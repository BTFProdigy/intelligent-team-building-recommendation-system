Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 31?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hybrid Multilingual Parsing with HPSG for SRL
Yi Zhang
Language Technology
DFKI GmbH, Germany
yzhang@coli.uni-sb.de
Rui Wang
Computational Linguistics
Saarland University, Germany
rwang@coli.uni-sb.de
Stephan Oepen
Informatics
University of Oslo, Norway
oe@ifi.uio.no
Abstract
In this paper we present our syntactic and se-
mantic dependency parsing system submitted
to both the closed and open challenges of the
CoNLL 2009 Shared Task. The system ex-
tends the system of Zhang, Wang, & Uszko-
reit (2008) in the multilingual direction, and
achieves 76.49 average macro F1 Score on the
closed joint task. Substantial improvements
to the open SRL task have been observed that
are attributed to the HPSG parses with hand-
crafted grammars. ?
1 Introduction
The CoNLL 2009 shared task (Hajic? et al, 2009)
continues the exploration on learning syntactic and
semantic structures based on dependency notations
in previous year?s shared task. The new addition
to this year?s shared task is the extension to mul-
tiple languages. Being one of the leading compe-
titions in the field, the shared task received sub-
missions from systems built on top of the state-
of-the-art data-driven dependency parsing and se-
mantic role labeling systems. Although it was
originally designed as a task for machine learning
approaches, CoNLL shared tasks also feature an
?open? track since 2008, which encourages the use
of extra linguistic resources to further improve the
?We are indebted to our DELPH-IN colleagues, specifi-
cally Peter Adolphs, Francis Bond, Berthold Crysmann, and
Montserrat Marimon for numerous hours of support in adapt-
ing their grammars and the PET software to parsing the CoNLL
data sets. The first author thanks the German Excellence Clus-
ter of Multimodal Computing and Interaction for the support
of the work. The second author is funded by the PIRE PhD
scholarship program. Participation of the third author in this
work was supported by the University of Oslo, as part of its re-
search partnership with the Center for the Study of Language
and Information at Stanford University. Our deep parsing ex-
perimentation was executed on the TITAN HPC facilities at the
University of Oslo.
performance. This makes the task a nice testbed for
the cross-fertilization of various language process-
ing techniques.
As an example of such work, Zhang et al (2008)
have shown in the past that deep linguistic parsing
outputs can be integrated to help improve the per-
formance of the English semantic role labeling task.
But several questions remain unanswered. First, the
integration only experimented with the semantic role
labeling part of the task. It is not clear whether
syntactic dependency parsing can also benefit from
grammar-based parsing results. Second, the English
grammar used to achieve the improvement is one of
the largest and most mature hand-crafted linguistic
grammars. It is not clear whether similar improve-
ments can be achieved with less developed gram-
mars. More specifically, the lack of coverage of
hand-crafted linguistic grammars is a major concern.
On the other hand, the CoNLL task is also a good
opportunity for the deep processing community to
(re-)evaluate their resources and software.
2 System Architecture
The overall system architecture is shown in Figure 1.
It is similar to the architecture used by Zhang et al
(2008). Three major components were involved.
The HPSG parsing component utilizes several hand-
crafted grammars for deep linguistic parsing. The
outputs of deep parsings are passed to the syntactic
dependency parser and semantic role labeler. The
syntactic parsing component is composed of a mod-
ified MST parser which accepts HPSG parsing re-
sults as extra features. The semantic role labeler is
comprised of a pipeline of 4 sub-components (pred-
icate identification is not necessary in this year?s
task). Comparing to Zhang et al (2008), this archi-
tecture simplified the syntactic component, and puts
more focus on the integration of deep parsing out-
puts. While Zhang et al (2008) only used seman-
31
SyntacticDependencyParsing
MST Parser
ERG
GG
JaCY
SRG
[incr tsdb()]
PET
HPSG Parsing
Argument Identification
Argument Classification
Predicate Classification
SemanticRoleLabeling
MRS
HPSG Syn.
Syn.Dep.
Figure 1: Joint system architecture.
tic features from HPSG parsing in the SRL task, we
added extra syntactic features from deep parsing to
help both tasks.
3 HPSG Parsing for the CoNLL Data
DELPH-IN (Deep Linguistic Processing with
HPSG) is a repository of open-source software and
linguistic resources for so-called ?deep? grammat-
ical analysis.1 The grammars are rooted in rela-
tively detailed, hand-coded linguistic knowledge?
including lexical argument structure and the linking
of syntactic functions to thematic arguments?and
are intended as general-purpose resources, applica-
ble to both parsing and generation. Semantics in
DELPH-IN is cast in the Minimal Recursion Seman-
tics framework (MRS; Copestake, Flickinger, Pol-
lard, & Sag, 2005), essentially predicate ? argument
structures with provision for underspecified scopal
relations. For the 2009 ?open? task, we used the
DELPH-IN grammars for English (ERG; Flickinger,
2000), German (GG; Crysmann, 2005), Japanese
(JaCY; Siegel & Bender, 2002), and Spanish (SRG;
Marimon, Bel, & Seghezzi, 2007). The grammars
vary in their stage of development: the ERG com-
prises some 15 years of continuous development,
whereas work on the SRG only started about five
years ago, with GG and JaCY ranging somewhere
inbetween.
3.1 Overall Setup
We applied the DELPH-IN grammars to the CoNLL
data using the PET parser (Callmeier, 2002) running
1See http://www.delph-in.net for background.
it through the [incr tsdb()] environment (Oepen &
Carroll, 2000), for parallelization and distribution.
Also, [incr tsdb()] provides facilities for (re-)training
the MaxEnt parse selection models that PET uses for
disambiguation.
The two main challenges in applying DELPH-
IN resources to parsing CoNLL data were (a) mis-
matches in basic assumptions, specifically tokeniza-
tion and the inventory of PoS tags provided as part of
the input, and (b) the need to adapt the resources for
new domains and genres?in particular in terms of
parse disambiguation?as the English and Spanish
grammars at least had not been previously applied
to the corpora used in the CoNLL shared task.
The importance of the first of these two aspects
is often underestimated. A detailed computational
grammar, inevitably, comes with its own assump-
tions about tokenization?the ERG, for example, re-
jects the conventional assumptions underlying the
PTB (and derived tools). It opts for an analysis of
punctuation akin to affixation (rather than as stand-
alone tokens), does not break up contracted negated
auxiliaries, and splits hyphenated words like ill-
advised into two tokens (the hyphen being part of
the first component). Thus, a string like Don?t you!
in the CoNLL data is tokenized as the four-element
sequence ?do, n?t, you, !?,2 whereas the ERG analy-
sis has only two leaf nodes: ?don?t, you!?.
Fortunately, the DELPH-IN toolchain recently
incorporated a mechanism called chart mapping
(Adolphs et al, 2008), which allows one to map
flexibly from ?external? input to grammar-internal
assumptions, while keeping track of external token
identities and their contributions to the final analysis.
The February 2009 release of the ERG already had
this machinery in place (with the goal of supporting
extant, PTB-trained PoS taggers in pre-processing
input to the deep parser), and we found that only a
tiny number of additional chart mapping rules was
required to ?fix up? CoNLL-specific deviations from
the PTB tradition. With the help of the original de-
velopers, we created new chart mapping configura-
tions for the German and Japanese grammars (with
17 and 16 such accomodation rules, respectively) in
a similar spirit. All four DELPH-IN grammars in-
2Note that the implied analogy to a non-contracted variant is
linguistically mis-leading, as ?Do not you! is ungrammatical.
32
clude an account of unknown words, based on un-
derspecified ?generic? lexical entries that are acti-
vated from PoS information.
The Japenese case was interesting, in that
the grammar assumes a different pre-processor
(ChaSen, rather than Juman), such that not only to-
ken boundaries but also PoS tags and morphological
features had to be mapped. From our limited ex-
perience to date, we found the chart mapping ap-
proach adequate in accomodating such discrepan-
cies, and the addition of this extra layer of input
processing gave substantial gains in parser cover-
age (see below). For the Spanish data, on the other
hand, we found it impossible to make effective use
of the PoS and morphological information in the
CoNLL data, due to more fundamental discrepan-
cies (e.g. the treatment of enclitics and multi-word
expressions).
3.2 Retraining Disambiguation Models
The ERG includes a domain-specific parse selection
model (for tourism instructions); GG only a stub
model trained on a handful of test sentences. For
use on the CoNLL data, thus, we had to train new
parse selections models, better adapted to the shared
task corpora. Disambiguation in PET is realized by
conditional MaxEnt models (Toutanova, Manning,
Flickinger, & Oepen, 2005), usually trained on full
HPSG treebanks. Lacking this kind of training ma-
terial, we utilized the CoNLL dependency informa-
tion instead, by defining an unlabeled dependency
accuracy (DA) metric for HPSG analyses, essen-
tially quantifying the degree of overlap in head ?
dependent relations against the CoNLL annotations.
Calculating DA for HPSG trees is similar to the
procedure commonly used for extracting bi-lexical
dependencies from phrase structure trees, in a sense
even simpler as HPSG analyses fully determine
headeness. Taking into account the technical com-
plication of token-level mismatches, our DA met-
ric loosely corresponds to the unlabeled attachment
score. To train CoNLL-specific parse selection mod-
els, we parsed the development sections in 500-best
mode (using the existing models) and then mechani-
cally ?annotated? the HPSG analyses with maximum
DA as preferred, all others as dis-preferred. In other
words, this procedure constructs a ?binarized? em-
pirical distribution where estimation of log-linear
Grammar Coverage Time
ERG 80.4% 10.06 s
GG 28.6% 3.41 s
JaCY 42.7% 2.13 s
SRG 7.5% 0.80 s
Table 1: Performance of the DELPH-IN grammars.
model parameters amounts to adjusting conditional
probabilities towards higher DA values.3
Using the [incr tsdb()] MaxEnt experimentation
facilities, we trained new parse selection models
for English and German, using the first 16,000 sen-
tences of the English training data and the full Ger-
man training corpus; seeing that only inputs that (a)
parse successfully and (b) have multiple readings,
with distinct DA values are relevant to this step, the
final models reflect close to 13,000 sentences for En-
glish, and a little more than 4,000 items for German.
Much like in the SRL component, these experiments
are carried out with the TADM software, using ten-
fold cross-validation and exact match ranking accu-
racy (against the binarized training distribution) to
optimize estimation hyper-parameters
3.3 Deep Parsing Features
HPSG parsing coverage and average cpu time per
input for the four languages with DELPH-IN gram-
mars are summarized in Table 1. The PoS-based
unknown word mechanism was active for all gram-
mars but no other robustness measures (which tend
to lower the quality of results) were used, i.e. only
complete spanning HPSG analyses were accepted.
Parse times are for 1-best parsing, using selective
unpacking (Zhang, Oepen, & Carroll, 2007).
HPSG parsing outputs are available in several dif-
ferent forms. We investigated two types of struc-
tures: syntactic derivations and MRS meaningrep-
resentations. Representative features were extracted
from both structures and selectively used in the sta-
tistical syntactic dependency parsing and semantic
role labeling modules for the ?open? challenge.
3We also experimented with using DA scores directly as em-
pirical probabilities in the training distribution (or some func-
tion of DA, to make it fall off more sharply), but none of
these methods seemed to further improve parse selection per-
formance.
33
Deep Semantic Features Similar to Zhang et al
(2008), we extract a set of features from the seman-
tic outputs (MRS) of the HPSG parses. These fea-
tures represent the basic predicate-argument struc-
ture, and provides a simplified semantic view on the
target sentence.
Deep Syntactic Dependency Features A HPSG
derivation is a tree structure. The internal nodes are
labeled with identifiers of grammar rules, and leaves
with lexical entries. The derivation tree provides
complete information about the actual HPSG anal-
ysis, and can be used together with the grammar to
reproduce complete feature structure and/or MRS.
Given that the shared task adopts dependency rep-
resentation, we further map the derivation trees into
token-token dependencies, labeled by corresponding
HPSG rules, by defining a set of head-finding rules
for each grammar. This dependency structure is dif-
ferent from the dependencies in CoNLL dataset, and
provides an alternative HPSG view on the sentences.
We refer to this structure as the dependency back-
bone (DB) of the HPSG anaylsis. A set of features
were extracted from the deep syntactic dependency
structures. This includes: i) the POS of the DB par-
ent from the predicate and/or argument; ii) DB la-
bel of the argument to its parent (only for AI/AC);
iii) labeled path from predicate to argument in DB
(only for AI/AC); iv) POSes of the predicate?s DB
dependents
4 Syntactic Dependency Parsing
For the syntactic dependency parsing, we use the
MST Parser (McDonald et al, 2005), which is a
graph-based approach. The best parse tree is ac-
quired by searching for a spanning tree which max-
imizes the score on either a partially or a fully con-
nected graph with all words in the sentence as nodes
(Eisner, 1996; McDonald et al, 2005). Based on our
experience last year, we use the second order setting
of the parser, which includes features over pairs of
adjacent edges as well as features over single edges
in the graph. For the projective or non-projective
setting, we compare the results on the development
datasets of different languages. According to the
parser performance, we decide to use non-projective
parsing for German, Japanese, and Czech, and use
projective parsing for the rest.
For the Closed Challenge, we first consider
whether to use the morphological features. We find
that except for Czech, parser performs better with-
out morphological features on other languages (En-
glish and Chinese have no morphological features).
As for the other features (i.e. lemma and pos) given
by the data sets, we also compare the gold standard
features and P-columns. For all languages, the per-
formance decreases in the following order: training
with gold standard features and evaluating with the
gold standard features, training with P-columns and
evaluating with P-columns, training with gold stan-
dard features and testing with P-columns. Conse-
quently, in the final submission, we take the second
combination.
The goal of the Open Challenge is to see whether
using external resources can be helpful for the pars-
ing performance. As we mentioned before, our
deep parser gives us both the syntactic analysis of
the input sentences using the HPSG formalism and
also the semantic analysis using MRS as the repre-
sentation. However, for the syntactic dependency
parsing, we only extract features from the syntac-
tic HPSG analyses and feed them into the MST
Parser. Although, when parsing with gold standard
lemma and POS features, our open system outper-
forms the closed system on out-domain tests (for En-
glish), when parsing with P-columns there is no sub-
stantial improvement observed after using the HPSG
features. Therefore, we did not include it in the final
submission.
5 Semantic Role Labeling
The semantic role labeling component used in the
submitted system is similar to the one described
by Zhang et al (2008). Since predicates are indi-
cated in the data, the predicate identification mod-
ule is removed from this year?s system. Argument
identification, argument classification and predicate
classification are the three sub-components in the
pipeline. All of them are MaxEnt-based classifiers.
For parameter estimation, we use the open source
TADM system (Malouf, 2002).
The active features used in various steps of SRL
are fine tuned separately for different languages us-
ing development datasets. The significance of fea-
ture types varies across languages and datasets.
34
ca zh cs en de ja es
SY
N Closed 82.67 73.63 75.58 87.90 84.57 91.47 82.69
ood - - 71.29 81.50 75.06 - -
SR
L
Closed 67.34 73.20 78.28 77.85 62.95 64.71 67.81
ood - - 77.78 67.07 54.87 - -
Open - - - 78.13 (?0.28) 64.31 (?1.36) 65.95 (?1.24) 68.24 (?0.43)
ood - - - 68.11 (?1.04) 58.42 (?3.55) - -
Table 2: Summary of System Performance on Multiple Languages
In the open challenge, two groups of extra fea-
tures from HPSG parsing outputs, as described in
Section 3.3, were used on languages for which we
have HPSG grammars, that is English, German,
Japanese, and Spanish.
6 Result Analysis
The evaluation results of the submitted system are
summarized in Table 2. The overall ranking of
the system is #7 in the closed challenge, and #2
in the open challenge. While the system achieves
mediocre performance, the clear performance dif-
ference between the closed and open challenges of
the semantic role labeler indicates a substantial gain
from the integration of HPSG parsing outputs. The
most interesting observation is that even with gram-
mars which only achieve very limited coverage, no-
ticeable SRL improvements are obtained. Con-
firming the observation of Zhang et al (2008), the
gain with HPSG features is more significant on out-
domain tests, this time on German as well.
The training of the syntactic parsing models for
all seven languages with MST parser takes about
100 CPU hours with 10 iterations. The dependency
parsing takes 6 ? 7 CPU hours. The training and test-
ing of the semantic role labeler is much more effi-
cient, thanks to the use of MaxEnt models and the
efficient parameter estimation software. The train-
ing of all SRL models for 7 languages takes about 3
CPU hours in total. The total time for semantic role
labeling on test datasets is less than 1 hour.
Figure 2 shows the learning curve of the syntactic
parser and semantic role labeler on the Czech and
English datasets. While most of the systems con-
tinue to improve when trained on larger datasets, an
exception was observed with the Czech dataset on
the out-domain test for syntactic accuracy. In most
of the cases, with the increase of training data, the
out-domain test performance of the syntactic parser
and semantic role labeler improves slowly relative
to the in-domain test. For the English dataset, the
SRL learning curve climbs more quickly than those
of syntactic parsers. This is largely due to the fact
that the semantic role annotation is sparser than the
syntactic dependencies. On the Czech dataset which
has dense semantic annotation, this effect is not ob-
served.
7 Conclusion
In this paper, we described our syntactic parsing and
semantic role labeling system participated in both
closed and open challenge of the (Joint) CoNLL
2009 Shared Task. Four hand-written HPSG gram-
mars of a variety of scale have been applied to parse
the datasets, and the outcomes were integrated as
features into the semantic role labeler of the sys-
tem. The results clearly show that the integration of
HPSG parsing results in the semantic role labeling
task brings substantial performance improvement.
The conclusion of Zhang et al (2008) has been re-
confirmed on multiple languages for which we hand-
built HPSG grammars exist, even where grammati-
cal coverage is low. Also, the gain is more signifi-
cant on out-of-domain tests, indicating that the hy-
brid system is more robust to cross-domain varia-
tion.
References
Adolphs, P., Oepen, S., Callmeier, U., Crysmann, B.,
Flickinger, D., & Kiefer, B. (2008). Some fine points
of hybrid natural language parsing. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation. Marrakech, Morocco.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado?, S.,
& Pinkal, M. (2006). The SALSA corpus: a German
corpus resource for lexical semantics. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation. Genoa, Italy.
35
 60
 65
 70
 75
 80
 85
 90
 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
Training Corpus Size (English)
Syn
SRL
Syn-ood
SRL-ood
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
Training Corpus Size (Czech)
Syn
SRL
Syn-ood
SRL-ood
Figure 2: Learning curves of syntactic dependency parser and semantic role labeler on Czech and English datasets
Callmeier, U. (2002). Preprocessing and encoding tech-
niques in PET. In S. Oepen, D. Flickinger, J. Tsujii, &
H. Uszkoreit (Eds.), Collaborative language engineer-
ing. A case study in efficient grammar-based process-
ing. Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Journal of Research on Language and Computa-
tion, 3(4), 281 ? 332.
Crysmann, B. (2005). Relative clause extraposition
in German. An efficient and portable implementation.
Research on Language and Computation, 3(1), 61 ?
82.
Flickinger, D. (2000). On building a more efficient gram-
mar by exploiting types. Natural Language Engineer-
ing, 6 (1), 15 ? 28.
Hajic?, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart??, M. A., Ma`rquez, L., Meyers, A., Nivre, J., Pado?,
S., S?te?pa?nek, J., Stran?a?k, P., Surdeanu, M., Xue, N.,
& Zhang, Y. (2009). The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning. Boulder,
CO, USA.
Hajic?, J., Panevova?, J., Hajic?ova?, E., Sgall, P., Pa-
jas, P., S?te?pa?nek, J., Havelka, J., Mikulova?, M., &
Z?abokrtsky?, Z. (2006). Prague Dependency Treebank
2.0 (Nos. Cat. No. LDC2006T01, ISBN 1-58563-370-
4). Philadelphia, PA, USA: Linguistic Data Consor-
tium.
Kawahara, D., Kurohashi, S., & Hasida, K. (2002). Con-
struction of a Japanese relevance-tagged corpus. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation (pp. 2008?2013).
Las Palmas, Canary Islands.
Malouf, R. (2002). A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings
of the 6th conferencde on natural language learning
(CoNLL 2002) (pp. 49?55). Taipei, Taiwan.
Marimon, M., Bel, N., & Seghezzi, N. (2007). Test suite
construction for a Spanish grammar. In T. H. King &
E. M. Bender (Eds.), Proceedings of the Grammar En-
gineering Across Frameworks workshop (p. 250-264).
Stanford, CA: CSLI Publications.
Oepen, S., & Carroll, J. (2000). Performance profiling for
parser engineering. Natural Language Engineering, 6
(1), 81 ? 97.
Palmer, M., Kingsbury, P., & Gildea, D. (2005). The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1), 71?106.
Palmer, M., & Xue, N. (2009). Adding semantic roles
to the Chinese Treebank. Natural Language Engineer-
ing, 15(1), 143?172.
Siegel, M., & Bender, E. M. (2002). Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd work-
shop on asian language resources and international
standardization at the 19th international conference
on computational linguistics. Taipei, Taiwan.
Surdeanu, M., Johansson, R., Meyers, A., Ma`rquez, L.,
& Nivre, J. (2008). The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proceedings of the 12th Conference on Computa-
tional Natural Language Learning. Manchester, UK.
Taule?, M., Mart??, M. A., & Recasens, M. (2008). An-
Cora: Multilevel Annotated Corpora for Catalan and
Spanish. In Proceedings of the 6th International Con-
ference on Language Resources and Evaluation. Mar-
rakesh, Morroco.
Toutanova, K., Manning, C. D., Flickinger, D., & Oepen,
S. (2005). Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on Language
and Computation, 3(1), 83 ? 105.
Zhang, Y., Oepen, S., & Carroll, J. (2007). Efficiency in
unification-based n-best parsing. In Proceedings of the
10th International Conference on Parsing Technolo-
gies (pp. 48 ? 59). Prague, Czech Republic.
Zhang, Y., Wang, R., & Uszkoreit, H. (2008). Hy-
brid Learning of Dependency Structures from Hetero-
geneous Linguistic Resources. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL 2008) (pp. 198?202). Manch-
ester, UK.
36
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36?44,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automated Multiword Expression Prediction for Grammar Engineering
Yi Zhang & Valia Kordoni
Dept. of Computational Linguistics
Saarland University
D-66041 Saarbru?cken, Germany
{yzhang,kordoni}@coli.uni-sb.de
Aline Villavicencio & Marco Idiart
Institutes of Informatics & Physics
Federal University of Rio Grande do Sul
Av. Bento Gonc?alves, 9500
Porto Alegre - RS, Brazil
avillavicencio@inf.ufrgs.br
idiart@if.ufrgs.br
Abstract
However large a hand-crafted wide-
coverage grammar is, there are always go-
ing to be words and constructions that
are not included in it and are going to
cause parse failure. Due to their hetero-
geneous and flexible nature, Multiword
Expressions (MWEs) provide an endless
source of parse failures. As the number
of such expressions in a speaker?s lexi-
con is equiparable to the number of single
word units (Jackendoff, 1997), one ma-
jor challenge for robust natural language
processing systems is to be able to deal
with MWEs. In this paper we propose
to semi-automatically detect MWE can-
didates in texts using some error mining
techniques and validating them using a
combination of the World Wide Web as a
corpus and some statistical measures. For
the remaining candidates possible lexico-
syntactic types are predicted, and they are
subsequently added to the grammar as new
lexical entries. This approach provides
a significant increase in the coverage of
these expressions.
1 Introduction
Hand-crafted large-scale grammars like the En-
glish Resource Grammar (Flickinger, 2000), the
Pargram grammars (Butt et al, 1999) and the
Dutch Alpino Grammar (Bouma et al, 2001)
are extremely valuable resources that have been
used in many NLP applications. However, due
to the open-ended and dynamic nature of lan-
guages, and the difficulties of grammar engineer-
ing, such grammars are likely to contain errors
and be incomplete. An error can be roughly clas-
sified as under-generating (if it prevents a gram-
matical sentence to be generated/parsed) or over-
generating (if it allows an ungrammatical sen-
tence to be generated/parsed). In the context of
wide-coverage parsing, we focus on the under-
generating errors which normally lead to parsing
failure.
Traditionally, the errors of the grammar are to
be detected manually by the grammar develop-
ers. This is usually done by running the grammar
over a carefully designed test suite and inspecting
the outputs. This procedure becomes less reliable
as the grammar gets larger, and is especially dif-
ficult when the grammar is developed in a dis-
tributed manner. Baldwin et al (2004), among
many others, for instance, have investigated the
main causes of parse failure, parsing a random
sample of 20,000 strings from the written com-
ponent of the British National Corpus (hencefor-
ward BNC) using the English Resource Gram-
mar (Flickinger, 2000), a broad-coverage preci-
sion HPSG grammar for English. They have found
that the large majority of failures are caused by
missing lexical entries, with 40% of the cases, and
missing constructions, with 39%.
To this effect, as mentioned above, in recent
years, some approaches have been developed in
order to (semi)automatically detect and/or repair
the errors in linguistic grammars. van Noord
(2004), for instance, takes a statistical approach
towards semi-automated error detection using the
parsability metric for word sequences. He reports
on a simple yet practical way of identifying gram-
mar errors. The method is particularly useful for
discovering systematic problems in a large gram-
mar with reasonable coverage. The idea behind it
is that each (under-generating) error in the gram-
36
mar leads to the parsing failure of some specific
grammatical sentences. By running the grammar
over a large corpus, the corpus can be split into
two subsets: the set of sentences covered by the
grammar and the set of sentences that failed to
parse. The errors can be identified by comparing
the statistical difference between these two sets
of sentences. By statistical difference, any kind
of uneven distribution of linguistic phenomena is
meant. In the case of van Noord (2004), the word
sequences are used, mainly because the cost to
compute and count the word sequences is mini-
mum. The parsability of a sequence wi . . . wj is
defined as:
R(wi . . . wj) =
C(wi . . . wj, OK)
C(wi . . . wj)
(1)
where C(wi . . . wj) is the number of sentences
in which the sequence wi . . . wj occurs, and
C(wi . . . wj , OK) is the number of sentences with
a successful parse which contain the sequence.
A frequency cut is used to eliminate the infre-
quent sequences. With suffix arrays and perfect
hashing automata, the parsability of all word se-
quences (with arbitrary length) can be computed
efficiently. The word sequences are then sorted
according to their parsabilities. Those sequences
with the lowest parsabilities are taken as direct in-
dication of grammar errors.
Among them, one common error, and sub-
sequently very common cause of parse failure
is due to Multiword Expressions (MWEs), like
phrasal verbs (break down), collocations (bread
and butter), compound nouns (coffee machine),
determiner-less PPs (in hospital), as well as so-
called ?frozen expressions? (by and large), as dis-
cussed by both Baldwin et al (2004) and van No-
ord (2004). Indicatively, in the experiments re-
ported in Baldwin et al (2004), for instance, from
all the errors due to missing lexical entries, one
fifth were due to missing MWEs (8% of total er-
rors). If an MWE is syntactically marked, the stan-
dard grammatical rules and lexical entries cannot
generate the string, as for instance in the case of
a phrasal verb like take off, even if the individual
words that make up the MWE are contained in the
lexicon.
In this paper we investigate semi-automatic
methods for error mining and detection of miss-
ing lexical entries, following van Noord (2004),
with the subsequent handling of the MWEs among
them. The output of the error mining phase pro-
poses a set of n-grams, which also contain MWEs.
Therefore, the task is to distinguish the MWEs
from the other cases. To do this, first we propose
to use the World Wide Web as a very large corpus
from which we collect evidence that enables us to
rule out noisy cases (due to spelling errors, for in-
stance), following Grefenstette (1999), Keller et
al. (2002), Kilgarriff and Grefenstette (2003) and
Villavicencio (2005). The candidates that are kept
can be semi-automatically included in the gram-
mar, by employing a lexical type predictor, whose
output we use in order to add lexical entries to the
lexicon, with a possible manual check by a gram-
mar writer. This procedure significantly speeds up
the process of grammar development, relieving the
grammar developer of some of the burden by au-
tomatically detecting parse failures and providing
semi-automatic means for handling them.
The paper starts with a discussion of MWEs and
of some of the characteristics that make them so
challenging for NLP, in section 2. This is followed
by a more detailed discussion of the technique
employed for error detection, in section 3. The
approach used for distinguishing noisy sequences
from MWE-related constructions using the World
Wide Web is then presented. How this information
is used for extending the grammar and the results
obtained are then addressed in section 5.
2 Multiword Expressions
The term Multiword Expressions (MWEs) has
been used to describe expressions for which the
syntactic or semantic properties of the whole ex-
pression cannot be derived from its parts ((Sag et
al., 2002), (Villavicencio et al, 2005)), including
a large number of related but distinct phenomena,
such as phrasal verbs (e.g. come along), nomi-
nal compounds (e.g. frying pan), institutionalised
phrases (e.g. bread and butter), and many oth-
ers. They are used frequently in language, and
in English, Jackendoff (1997) estimates the num-
ber of MWES in a speaker?s lexicon to be com-
parable to the number of single words. This is re-
flected in several existing grammars and lexical re-
sources, where almost half of the entries are Mul-
tiword Expressions. However, due to their hetero-
geneous characteristics, MWEs present a tough
challenge for both linguistic and computational
work (Sag et al, 2002). Some MWEs are fixed,
and do not present internal variation, such as ad
37
hoc, while others allow different degrees of inter-
nal variability and modification, such as touch a
nerve (touch/find a nerve) and spill beans (spill
several/musical/mountains of beans). In terms of
semantics, some MWEs are more opaque in their
meaning (e.g. to kick the bucket as to die), while
others have more transparent meanings that can be
inferred from the words in the MWE (e.g. eat up,
where the particle up adds a completive sense to
eat). Therefore, to provide a unified account for
the detection of these distinct but related phenom-
ena is a real challenge for NLP systems.
3 Detection of Errors: Overview
van Noord (2004) reports on various errors that
have been discovered for the Dutch Alpino Gram-
mar (Bouma et al, 2001) semi-automatically, us-
ing the Twente Nieuws Corpus. The idea pur-
sued by van Noord (2004) has been to locate those
n-grams in the input that might be the cause of
parsing failure. By processing a huge amount
of data, the parsability metrics briefly presented
in section 1 have been used to successfully lo-
cate various errors introduced by the tokenizer,
erroneous/incomplete lexical descriptions, frozen
expressions with idiosyncratic syntax, or incom-
plete grammatical descriptions. However, the re-
covery of these errors has been shown to still re-
quire significant efforts from the grammar devel-
oper. Moreover, there is no concrete data given
about the distribution of the different types of er-
rors discovered.
As also mentioned before, among the n-grams
that usually cause parse failures, there is a large
number of missing MWEs in the lexicon such
as phrasal verbs, collocations, compound nouns,
frozen expressions (e.g. by and large, centre of
attention, put forward by, etc).
For the purpose of the detection of MWEs, we
are interested in seeing what the major types of er-
ror for a typical large-scale deep grammar are. In
this context, we have run the error mining experi-
ment reported by van Noord with the English Re-
source Grammar (ERG; (Flickinger, 2000))1 and
the British National Corpus 2.0 (BNC; (Burnard,
2000)).
We have used a subset of the BNC written com-
ponent. The sentences in this collection contain
no more than 20 words and only ASCII characters.
1ERG is a large-scale HPSG grammar for English. In this
paper, we have used the January 2006 release of the grammar.
That is about 1.8M distinct sentences.
These sentences have then be fed into an effi-
cient HPSG parser (PET; (Callmeier, 2000)) with
ERG loaded. The parser has been configured with
a maximum edge number limit of 100K and has
run in the best-only mode so that it does not ex-
haustively find all the possible parses. The result
of each sentence is marked as one of the following
four cases:
? P means at least one parse is found for the
sentence;
? L means the parser halted after the morpho-
logical analysis and has not been able to con-
struct any lexical item for the input token;
? N means the search has finished normally
and there is no parse found for the sentence;
? E means the search has finished abnormally
by exceeding the edge number limit.
It is interesting to notice that when the ambigu-
ity packing mechanism (Oepen and Carroll, 2000)
is used and the unpacking is turned off 2, E does
not occur at all for our test corpus. Running the
parsability checking over the entire collection of
sentences has taken the parser less than 2 days on
a 64bit machine with 3GHz CPU. The results are
shown in Table 1.
Result # Sentences Percentage
P 644,940 35.80%
L 969,452 53.82%
N 186,883 10.38%
Table 1: Distribution of Parsing Results
?From the results shown in Table 1, one can see
that ERG has full lexical span for less than half of
the sentences. For these sentences, about 80% are
successfully parsed. These numbers show that the
grammar coverage has a significant improvement
as compared to results reported by Baldwin et al
(2004) and Zhang and Kordoni (2006), mainly at-
tributed to the increase in the size of the lexicon
and the new rules to handle punctuations and frag-
ments.
Obviously, L indicates the unknown words in
the input sentence. But for N , it is not clear where
2For the experiment of error mining, only the parsability
checking is necessary. There is no need to record the exact
parses.
38
and what kind of error has occurred. In order
to pinpoint the errors, we used the error mining
techniques proposed by van Noord (2004) on the
grammar and corpus. We have taken the sentences
marked as N (because the errors in L sentences
are already determined) and calculate the word se-
quence parsabilities against the sentences marked
as P . The frequency cut is set to be 5. The whole
process has taken no more than 20 minutes, result-
ing in total the parsability scores for 35K n-grams
(word sequences). The distribution of n-grams in
length with parsability below 0.1 is shown in Ta-
ble 2.
Number Percentage
uni-gram 798 20.84%
bi-gram 2,011 52.52%
tri-gram 937 24.47%
Table 2: Distribution of N-gram in Length in Error
Mining Results (R(x) < 0.1)
Although pinpointing the problematic n-grams
still does not tell us what the exact errors are, it
does shed some light on the cause. From Table 2
we see quite a lot of uni-grams with low parsabil-
ities. Table 3 gives some examples of the word
sequences. By intuition, we make the bold as-
sumption that the low parsability of uni-grams is
caused by the missing appropriate lexical entries
for the corresponding word.3
For the bi-grams and tri-grams, we do see a lot
of cases where the error can be repaired by just
adding a multiword lexical entry into the grammar.
N-gram Count
professionals 248
the flat 62
indication of 21
tone of voice 19
as always is 7
Table 3: Some Examples of the N-grams in Error
Mining Results
In order to distinguish those n-grams that can
be added into the grammar as MWE lexical en-
tries from the other cases, we propose to vali-
date them using evidence collected from the World
Wide Web.
3It has later been confirmed with the grammar developer
that almost all of the errors detected by these low parsability
uni-grams can be fixed by adding correct lexical entries.
4 Detection of MWEs and related
constructions
Recently, many researchers have started using the
World Wide Web as an extremely large corpus,
since, as pointed out by Grefenstette (1999), the
Web is the largest data set available for NLP
((Grefenstette, 1999), (Keller et al, 2002), (Kil-
garriff and Grefenstette, 2003) and (Villavicencio,
2005)). For instance, Grefenstette employs the
Web to do example-based machine translation of
compounds from French into English. The method
he employs would suffer considerably from data
sparseness, if it were to rely only on corpus data.
So for compounds that are sparse in the BNC he
also obtains frequencies from the Web. The scale
of the Web can help to minimise the problem of
data sparseness, that is especially acute for MWEs,
and Villavicencio (2005) uses the Web to find ev-
idence to verify automatically generated VPCs.
This work is built on these, in that we propose
to employ the Web as a corpus, using frequencies
collected from the Web to detect MWEs among
the n-grams that cause parse failure. We concen-
trate on the 482 most frequent candidates, to verify
t he method.
The candidate list has been pre-processed to re-
move systematic unrelated entries, like those in-
cluding acronyms, names, dates and numbers, fol-
lowing Bouma and Villada (2002). Using Google
as a search engine, we have looked for evidence
on the Web for each of the candidate MWEs, that
have occurred as an exact match in a webpage. For
each candidate searched, Google has provided us
with a measure of frequency in the form of the
number of pages in which it appears. Table 4
shows the 10 most frequent candidates, and among
these there are parts of formulae, frozen expres-
sions and collocations. Table 5 on the other hand,
shows the 10 least frequent candidates. From the
total of candidates, 311 have been kept while the
other have been discarded as noise.
A manual inspection of the candidates has re-
vealed that indeed the list contains a large amount
of MWEs and frozen expressions like taking into
account the, good and evil, by and large, put for-
ward by and breach of contract. Some of these
cases, like come into effect in, have very spe-
cific subcategorisation requirements, and this is re-
flected by the presence of the prepositions into and
in in the ngram. Other cases seem to be part of
formulae, like but also in, as part of not only X but
39
Table 4: Top 10 Candidate Multiword Expressions
MWE Pages Entropy Prob(%)
the burden of 36600000 0.366 79.4
and cost effective 34400000 0.372 70.7
the likes of 34400000 0.163 93.1
but also in 27100000 0.038 98.9
to bring together 25700000 0.086 96.6
points of view 24500000 0.017 99.6
and the more 23700000 0.512 61.5
with and without 23100000 0.074 97.4
can do for 22300000 0.003 99.9
taking into account the 22100000 0.009 99.6
but what about 21000000 0.045 98.7
the ultimate in 17400000 0.199 90.0
Table 5: Bottom 10 Candidate Multiword Expressions
MWE Pages Entropy Prob (%)
stand by and 1350000 0.399 65.5
discharged from hospital 553000 0.001 99.9
shock of it 92300 0.541 44.6
was woken by 91400 0.001 99.9
telephone rang and 43700 0.026 99.2
glanced across at 36900 0.003 99.9
the citizens charter 22900 0.070 97.9
input is complete 13900 0.086 97.2
from of government 706 0.345 0.1
the to infinitive 561 0.445 1.4
40
also Y, but what about, and the more the (part of
the more the Yer).
However, among the candidates there still re-
main those that are not genuine MWEs, like of al-
cohol and and than that in, which contain very fre-
quent words that enable them to obtain a very high
frequency count without being an MWE. There-
fore, to detect these cases, the remainder of the
candidates could be further analysed using some
statistical techniques to try to distinguish them
from the more likely MWEs among the candi-
dates. This is done by Bouma and Villada (2002)
who investigated some measures that have been
used to identify certain kinds of MWEs, focusing
on collocational prepositional phrases, and on the
tests of mutual information, log likelihood and ?2.
One significant difference here is that this work is
not constrained to a particular type of MWEs, but
has to deal with them in general. Moreover, the
statistical measures used by Bouma and Villada
demand the knowledge of single word frequencies
which can be a problem when using Google espe-
cially for common words like of and a.
In Tables 4 and 5 we present two alternative
measures that combined can help to detect false
candidates. The rational is similar to the statis-
tical tests, without the need of searching for the
frequency of each of the words that make up the
MWE. We assume that if a candidate is just a
result of the random occurrence of very frequent
words most probably the order of the words in the
ngram is not important. Therefore, given a can-
didate, such as the likes of, we measure the fre-
quency of occurrence of all its permutations (e.g.
the of likes, likes the of, etc) and we calculate the
candidate?s entropy as
S = ? 1logN
N
?
k=1
Pi logPi (2)
where Pi is the probability of occurrence of a
given permutation, and N the total number of per-
mutations. The entropy above defined has its max-
imum at S = 1 when all permutations are equally
probably, which indicates a clear signature of a
random nature. On the other hand, when order is
very important and only a single configuration is
allowed the entropy has its minimum, S = 0. An
ngram with low entropy has good chances of being
an MWE. A close inspection on Table 4 shows that
the top two candidate ngrams have relatively high
entropies ( here we consider high entropy when
S > 0.3 ). In the first case this can be explained
by the fact that the word the can appear after the
word of without compromising the MWE mean-
ing as in the burden of the job. In the second case
it shows that the real MWE is cost effective and
the word and can be either in the beginning or in
the end of the trigram. In fact for a trigram with
only two acceptable permutations the entropy is
S = log 2/ log 6 ' 0.39, very close to what is
obtained .
We also show the probability of occurrence
of each candidate ngram among its permutations
(P1). Most of the candidates in the list are more
frequent than their permutations. In Table 4 we
find two exceptions which are clearly spelling er-
rors in the last 2 ngrams. Therefore low P1 can
be a good indicative of a noisy candidate. Another
good predictor is the relative frequency between
the candidates. Given the occurrence values for
the most frequent candidates, we consider that by
using a threshold of 20,000 occurrences, it is pos-
sible to remove the more noisy cases.
We note that the grammar can also impose some
restrictions in the order of the elements in the
ngram, in the sense that some of the generated
permutations are ungrammatical (e.g. the of likes)
and will most probably have null or very low fre-
quencies. Therefore, on top of the constraints on
the lexical order there are also constraints on the
constituent order of a candidate which will be re-
flected in these measures.4
The remainder candidates can be semi-
automatically included in the grammar, by using
a lexical type predictor, as described in the next
section. With this information, each candidate is
added as a lexical entry, with a possible manual
check by a grammar writer prior to inclusion in
the grammar.
4Google ignores punctuation between the elements of the
ngram. This can lead to some hits being returned for some
of the ungrammatical permuted ngrams, such as one one by
in the sentence We?re going to catch people one by one. One
day,... from www.beertravelers.com/lists/drafttech.html. On
the other hand, Google only returns the number of pages
where a given ngram occurred, but not the number of times it
occurred in that page. This can result in a huge underestima-
tion especially for very frequent ngrams and words, which
can be used mo re than once in a given page. Therefore,
a conservative view of these frequencies must be adopted,
given that for some ngrams they might be inflated and for
others deflated.
41
5 Automated Deep Lexical Acquisition
In section (3), we have seen that more than 50%
of the sentences contain one or more unknown
words. And about half of the other parsing failures
are also due to lexicon missing. In this section, we
propose a statistical approach towards lexical type
prediction for unknown words, including multi-
word expressions.
5.1 Atomic Lexical Types
Lexicalist grammars are normally composed of a
limited number of rules and a lexicon with rich
linguistic features attached to each entry. Some
grammar formalisms have a type inheriting system
to encode various constraints, and a flat structure
of the lexicon with each entry mapped onto one
type in the inheritance hierarchy. The following
discussion is based on Head-driven Phrase Struc-
ture Grammar (HPSG) (Pollard and Sag, 1994),
but should be easily adapted to other formalisms,
as well.
The lexicon of HPSG consists of a list of well-
formed Typed Feature Structures (TFSs) (Carpen-
ter, 1992), which convey the constraints on spe-
cific words by two ways: the type compatibility,
and the feature-value consistency. Although it is
possible to use both features and types to con-
vey the constraints on lexical entries, large gram-
mars prefer the use of types in the lexicon because
the inheritance system prevents the redundant def-
inition of feature-values. And the feature-value
constraints in the lexicon can be avoided by ex-
tending the types. Say we have n lexical entries
Li :t
[
F a1
] . . . Ln :t
[
F an
]
. They share the same
lexical type t, but take different values for the fea-
ture F . If a1, . . . , an are the only possible values
for F in the context of type t, we can extend the
type t with subtypes ta1 :t
[
F a1
] . . . tan :t
[
F an
]
and modify the lexical entries to use these new
types, respectively. Based on the fact that large
grammars normally have a very restricted num-
ber of feature-values constraints for each lexical
type, the increase of the types is acceptable. It is
also typical that the types assigned to lexical en-
tries are maximum on the type hierarchy, which
means that they have no further subtypes. We will
call the maximum lexical types after extension the
atomic lexical types. Then the lexicon will be a
multi-valued mapping from the word stems to the
atomic lexical types.
Needless to underline here that all we have
mentioned above is not applicable exclusively to
HPSG, but to many other formalisms based on
TFSs, which makes our assumptions about atomic
lexical types all the more relevant for a wide range
of systems and applications.
5.2 Statistical Lexical Type Predictor
Given that the lexicon of deep grammars can be
modelled by a mapping from word stems to atomic
lexical types, we now go on designing the statisti-
cal methods that can automatically ?guess? such
mappings for unknown words.
Similar to Baldwin (2005), we also treat the
problem as a classification task. But there is an im-
portant difference. While Baldwin (2005) makes
predictions for each unknown word, we create a
new lexical entry for each occurrence of the un-
known word. The assumption behind this is that
there should be exactly one lexical entry that cor-
responds to the occurrence of the word in the given
context5.
We use a single classifier to predict the atomic
lexical type. There are normally hundreds of
atomic lexical types for a large grammar. So the
classification model should be able to handle a
large number of output classes. We choose the
Maximum Entropy-based model because it can
easily handle thousands of features and a large
number of possible outputs. It also has the ad-
vantages of general feature representation and no
independence assumption between features. With
the efficient parameter estimation algorithms dis-
cussed by Malouf (2002), the training of the model
is now very fast.
For our prediction model, the probability of a
lexical type t given an unknown word and its con-
text c is:
p(t|c) = exp(
?
i ?ifi(t, c))
?
t??T exp(
?
i ?ifi(t?, c))
(3)
where feature fi(t, c) may encode arbitrary char-
acteristics of the context. The parameters <
?1, ?2, . . . > can be evaluated by maximising the
pseudo-likelihood on a training corpus (Malouf,
2002). The detailed design and feature selec-
tion for the lexical type predictor are described in
Zhang and Kordoni (2006).
5Lexical ambiguity is not considered here for the un-
knowns. In principle, this constraint can be relaxed by allow-
ing the classifier to return more than one results by, setting a
confidence threshold, for example.
42
In the experiment described here, we have used
the latest version of the Redwoods Treebank in or-
der to train the lexical type predictor with morpho-
logical features and context words/POS tags fea-
tures 6. We have then extracted from the BNC
6248 sentences, which contain at least one of the
311 MWE candidates verified with World Wide
Web in the way described in the previous section.
For each occurrence of the MWE candidates in
this set of sentences, our lexical type predictor has
predicted a lexical entry candidate. This has re-
sulted in 1936 distinct entries. Only those entries
with at least 5 counts have been added into the
grammar. This has resulted in an extra 373 MWE
lexical entries for the grammar.
This addition to the grammar has resulted in a
significant increase in coverage (table 6) of 14.4%.
This result is very promising, as only a subset of
the candidate MWEs has been analysed, and could
result in an even greater increase in coverage, if
these techniques were applied to the complete set
of candidates.
However, we should also point out that the cov-
erage numbers reported in Table 6 are for a set
of ?difficult? sentences which contains a lot of
MWEs. When compared to the numbers reported
in Table 1, the coverage of the parser on this data
set after adding the MWE entries is still signifi-
cantly lower. This indicates that not all the MWEs
can be correctly handled by simply adding more
lexical entries. Further investigation is still re-
quired.
6 Conclusions
One of the important challenges for robust natural
language processing systems is to be able to deal
with the systematic parse failures caused in great
part by Multiword Expressions and related con-
structions. Therefore, in this paper we have pro-
posed an approach for the semi-automatic exten-
sion of grammars by using an error mining tech-
nique for the detection of MWE candidates in texts
and for predicting possible lexico-syntactic types
for them. The approach presented is based on that
of van Noord (2004) and proposes a set of MWE
candidates. For this set of candidates, using the
World Wide Web as a large corpus, frequencies are
gathered for each candidate. These in conjunction
with some statistical measures are employed for
ruling out noisy cases like spelling mistakes (from
6The POS tags are produced with the TnT tagger.
of government) and frequent non-MWE sequences
like input is complete.
With this information the remaining sequences
are analysed by a statistical type predictor that as-
signs the most likely lexical type for each of the
candidates in a given context. By adding these to
the grammar as new lexical entries, a considerable
increase in coverage of 14.4% was obtained.
The approach proposed employs simple and
self-contained techniques that are language-
independent and can help to semi-automatically
extend the coverage of a grammar without rely-
ing on external resources, like electronic dictio-
naries and ontologies that are expensive to obtain
and not available for all languages. Therefore, it
provides an inexpensive and reusable manner of
helping and speeding up the grammar engineer-
ing process, by relieving the grammar developer
of some of the burden of extending the coverage
of the grammar.
As future work we intend to investigate further
statistical measures that can be applied robustly to
different types of MWEs for refining even more
the list of candidates and distinguishing false pos-
itives, like of alcohol and from MWEs, like put
forward by. The high frequency with which the
former occur in corpora and the more accute prob-
lem of data sparseness that affects the latter make
this a difficult task.
References
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal.
Timothy Baldwin. 2005. Bootstrapping deep lexical
resources: Resources for courses. In Proceedings
of the ACL-SIGLEX Workshop on Deep Lexical Ac-
quisition, pages 67?76, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Gosse Bouma and Begon?a Villada. 2002. Corpus-
based acquisition of collocational prepositional
phrases. In Proceedings of the Computational Lin-
guistics in the Netherlands (CLIN) 2001, University
of Twente.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational anal-
ysis of dutch. In Computational Linguistics in The
Netherlands 2000.
43
Entries Added Item # Covered # Coverage
ERG 0 6246 268 4.3%
ERG+MWE(Web) 373 6246 1168 18.7%
Table 6: Parser coverage on ?difficult? sentences before/after adding MWE lexical entries
Lou Burnard. 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services.
M. Butt, S. Dipper, A. Frank, and T.H. King. 1999.
Writing large-scale parallel grammars for english,
french, and german. In Proceedings of the LFG99
Conference. CSLI Publications.
Ulrich Callmeier. 2000. PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. Journal of Natural Language Engineering,
6(1):99?108.
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press, Cam-
bridge, England.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Gregory Grefenstette. 1999. The World Wide Web
as a resource for example-based machine transla-
tion tasks. In Proceedings of ASLIB, Conference on
Translating and the Computer, London.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Frank Keller, Maria Lapata, and Olga Ourioupina.
2002. Using the Web to overcome data sparse-
ness. In Jan Hajic? and Yuji Matsumoto, editors, Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 230?237,
Philadelphia.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on web as corpus.
Computational Linguistics, 29.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conferencde on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Stephan Oepen and John Carroll. 2000. Ambiguity
packing in constraint-based parsing ? practical re-
sults. In Proceedings of the 1st Conference of the
North American Chapter of the ACL, pages 162?
169, Seattle, WA.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago, Illinois.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceed-
ings of the 3rd International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15, Mexico City, Mexico.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the spe-
cial issue on multiword expressions: having a crack
at a hard nut. Journal of Computer Speech and Lan-
guage Processing, 19.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech and
Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
44
Proceedings of the 10th Conference on Parsing Technologies, pages 48?59,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Efficiency in Unification-Based N -Best Parsing
Yi Zhang?, Stephan Oepen?, and John Carroll?
?Saarland University, Department of Computational Linguistics, and DFKI GmbH (Germany)
?University of Oslo, Department of Informatics (Norway)
?University of Sussex, Department of Informatics (UK)
Abstract
We extend a recently proposed algorithm for
n-best unpacking of parse forests to deal ef-
ficiently with (a) Maximum Entropy (ME)
parse selection models containing important
classes of non-local features, and (b) forests
produced by unification grammars contain-
ing significant proportions of globally incon-
sistent analyses. The new algorithm empir-
ically exhibits a linear relationship between
processing time and the number of analyses
unpacked at all degrees of ME feature non-
locality; in addition, compared with agenda-
driven best-first parsing and exhaustive pars-
ing with post-hoc parse selection it leads to
improved parsing speed, coverage, and ac-
curacy.?
1 Background?Motivation
Technology for natural language analysis using lin-
guistically precise grammars has matured to a level
of coverage and efficiency that enables parsing of
large amounts of running text. Research groups
working within grammatical frameworks like CCG
(Clark & Curran, 2004), LFG (Riezler et al, 2002),
and HPSG (Malouf & van Noord, 2004; Oepen,
Flickinger, Toutanova, & Manning, 2004; Miyao,
Ninomiya, & Tsujii, 2005) have successfully in-
tegrated broad-coverage computational grammars
with sophisticated statistical parse selection models.
The former delineate the space of possible analy-
ses, while the latter provide a probability distribu-
?The first author warmly acknowledges the guidance of his
PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grate-
ful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger,
and Erik Velldal for many discussions and their support. We
thank Ron Kaplan, Martin Kay, and Bob Moore for provid-
ing insightful information about related approaches, notably the
XLE and CLE parsers.
tion over competing hypotheses. Parse selection ap-
proaches for these frameworks often use discrimi-
native Maximum Entropy (ME) models, where the
probability of each parse tree, given an input string,
is estimated on the basis of select properties (called
features) of the tree (Abney, 1997; Johnson, Ge-
man, Canon, Chi, & Riezler, 1999). Such features,
in principle, are not restricted in their domain of
locality, and enable the parse selection process to
take into account properties that extend beyond lo-
cal contexts (i.e. sub-trees of depth one).
There is a trade-off in this set-up between the ac-
curacy of the parse selection model, on the one hand,
and the efficiency of the search for the best solu-
tion(s), on the other hand. Extending the context size
of ME features, within the bounds of available train-
ing data, enables increased parse selection accuracy.
However, the interplay of the core parsing algo-
rithm and the probabilistic ranking of alternate (sub-
)hypotheses becomes considerably more complex
and costly when the feature size exceeds the domain
of locality (of depth-one trees) that is characteristic
of phrase structure grammar-based formalisms. One
current line of research focuses on finding the best
balance between parsing efficiency and parse selec-
tion techniques of increasing complexity, aiming to
identify the most probable solution(s) with minimal
effort.
This paper explores a range of techniques, com-
bining a broad-coverage, high-efficiency HPSG
parser with a series of parse selection models with
varying context size of features. We sketch three
general scenarios for the integration: (a) a baseline
sequential configuration, where all results are enu-
merated first, and subsequently ranked; (b) an in-
terleaved but approximative solution, performing a
greedy search for an n-best list of results; and (c) a
two-phase approach, where a complete packed for-
48
est is created and combined with a specialized graph
search procedure to selectively enumerate results in
(globally) correct rank order. Although conceptu-
ally simple, the second technique has not previously
been evaluated for HPSG parsing (to the best of our
knowledge). The last of these techniques, which we
call selective unpacking, was first proposed by Car-
roll & Oepen (2005) in the context of chart-based
generation. However, they only provide an account
of the algorithm for local ME properties and assert
that the technique should generalize to larger con-
texts straightforwardly. This paper describes this
generalization of selective unpacking, in its appli-
cation to parsing, and demonstrates that the move
from features that resemble a context-free domain
of locality to features of, in principle, arbitrary con-
text size can indeed be based on the same algorithm,
but the required extensions are non-trivial.
The structure of the paper is as follows. Sec-
tion 2 summarizes our formalism, grammars used,
parse selection approach, and training and test data.
Section 3 discusses the range of possibilities for
structuring the process of statistical, grammar-based
parsing, and Sections 4 to 6 describe our approach
to efficient n-best parsing. We present experimental
results in Section 7, compare our approach to previ-
ous ones (Section 8), and finally conclude.
2 Overall Set-up
While couched in the HPSG framework, the tech-
niques explored here are applicable to the larger
class of unification-based grammar formalisms. We
make use of the DELPH-IN1 reference formalism,
as implemented by a variety of systems, including
the LKB (Copestake, 2002) and PET (Callmeier,
2002). For the experiments discussed here, we
adapted the open-source PET parsing engine in
conjunction with two publicly available grammars,
the English Resource Grammar (ERG; Flickinger,
2000) and the DFKI German Grammar (GG; Mu?ller
& Kasper, 2000, Crysmann, 2005). Our parse se-
lection models were trained and evaluated on HPSG
treebanks that are distributed with these grammars.
The following paragraphs summarize relevant prop-
erties of the structures manipulated by the parser,
1Deep Linguistic Processing with HPSG, an open-
source repository of grammars and processing tools; see
?http://www.delph-in.net/?.
subjh
hspec
det the le
the
sing noun
n intr le
dog
third sg fin verb
v unerg le
barks
Figure 1: Sample HPSG derivation tree for the sentence the
dog barks. Phrasal nodes are labeled with identifiers of gram-
mar rules, and (pre-terminal) lexical nodes with class names for
types of lexical entries.
followed by relevant background on parse selection.
Figure 1 shows an example ERG derivation tree.
Internal tree nodes are labeled with identifiers of
grammar rules, and leaves with lexical entries. The
derivation tree provides complete information about
the actual HPSG analysis, in the sense that it can be
viewed as a recipe for computing it. Lexical entries
and grammar rules alike are ultimately just feature
structures, complex and highly-structured linguistic
categories. When unified together in the configura-
tion depicted by the derivation tree, the resulting fea-
ture structure yields an HPSG sign, a detailed repre-
sentation of the syntactic and semantic properties of
the input string. Just as the full derivation denotes a
feature structure, so do its sub-trees, and for gram-
mars like the ERG and GG each such structure will
contain hundreds of feature ? value pairs.
Because of the lexicalized nature of HPSG (and
similar frameworks) our parsers search for well-
formed derivations in a pure bottom-up fashion.
Other than that, there are no hard-wired assumptions
about the order of computation, i.e. the specific pars-
ing strategy. Our basic set-up closely mimics that of
Oepen & Carroll (2002), where edges indexed by
sub-string positions in a chart represent the nodes of
the tree, recording both a feature structure (as its cat-
egory label) and the identity of the underlying lexi-
cal entry or rule in the grammar. Multiple edges de-
rived for identical sub-strings can be ?packed? into a
single chart entry in case their feature structures are
compatible, i.e. stand in an equivalence or subsump-
tion relation. By virtue of having each edge keep
back-pointers to its daughter edges?the immediate
sub-nodes in the tree whose combination resulted in
49
the mother edge?the parse forest provides a com-
plete and explicit encoding of all possible results in a
maximally compact form.2 A simple unpacking pro-
cedure is obtained from the cross-multiplication of
all local combinatorics, which is directly amenable
to dynamic programming.
Figure 2 shows a hypothetical forest (on the left),
where sets of edges exhibiting local ambiguity have
been packed into a single ?representative? edge, viz.
the one in each set with one or more incoming dom-
inance arcs. Confirming the findings of Oepen &
Carroll (2002), in our experiments packing under
feature structure subsumption is much more effec-
tive than packing under mere equivalence, i.e. for
each pair of edges (over identical sub-strings) that
stand in a subsumption relation, a technique that
Oepen & Carroll (2002) termed retro-active pack-
ing ensures that the more general of the two edges
remains in the chart. When packing under subsump-
tion, however, some of the cross-product of local
ambiguities in the forest may not be globally con-
sistent. Assume for example that, in Figure 2, edges
6 and 8 subsume 7 and 9 , respectively; combining
7 and 9 into the same tree during unpacking can in
principle fail. Thus, unpacking effectively needs to
deterministically replay unifications, but this extra
expense in our experience is negligible when com-
pared to the decreased cost of constructing the for-
est under subsumption. In Section 3 we argue that
this very property, in addition to increasing parsing
efficiency, interacts beneficially with parse selection
and on-demand enumeration of results in rank order.
Following (Johnson et al, 1999), a conditional
ME model of the probabilities of trees {t1 . . . tn}
for a string s, and assuming a set of feature
functions {f1 . . . fm} with corresponding weights
{?1 . . . ?m}, is defined as:
p(ti|s) =
exp?j ?jfj(ti)
?n
k=1 exp
?
j ?jfj(tk)
(1)
2This property of parse forests is not a prerequisite of the
chart parsing framework. The basic CKY procedure (Kasami,
1965), for example, as well as many unification-based adapta-
tions (e.g. the Core Language Engine; Moore & Alshawi, 1992)
merely record the local category of each edge, which is suffi-
cient for the recognition task and simplifies the search. How-
ever, reading out complete trees from the chart, then, amounts
to a limited form of search, going back to the rules of the gram-
mar itself to (re-)discover decomposition relations among chart
entries.
Type Sample Features
1 ?0 subjh hspec third sg fin verb?
1 ?1 ? subjh hspec third sg fin verb?
1 ?0 hspec det the le sing noun?
1 ?1 subjh hspec det the le sing noun?
1 ?2 ? subjh hspec det the le sing noun?
2 ?0 subjh third sg fin verb?
2 ?0 subjh hspce?
2 ?1 subjh hspec det the le?
2 ?1 subjh hspec sing noun?
3 ?1 n intr le dog?
3 ?2 det the le n intr le dog?
3 ?3  det the le n intr le dog?
Table 1: Examples of structural features extracted from the
derivation tree in Figure 1. The Type column indicates the
template corresponding to each sample feature; the integer that
starts each feature indicates the degree of grandparenting (in the
case of type 1 and 2 features) or n-gram size (type 3 features).
The symbols ? and  denote the root of the tree and left pe-
riphery of the yield, respectively.
Feature functions fj can test for arbitrary structural
properties of analyses ti, and their value typically is
the number of times a specific property is present
in ti. Toutanova, Manning, Flickinger, & Oepen
(2005) propose an inventory of features that per-
form well in HPSG parse selection; currently we re-
strict ourselves to the best-performing of these, of
the form illustrated in Table 1, comprising depth-
one sub-trees (or portions of these) with grammar-
internal identifiers as node labels, plus optionally
a chain of one or more dominating nodes (i.e. lev-
els of grandparents). If a grandparents chain is
present then the feature is non-local. For expository
purposes, Table 1 includes another feature type, n-
grams over leaf nodes of the derivation; in Section 5
below we speculate about the incorporation of these
(and similar) features in our algorithm.
3 Interleaving Parsing and Ranking
At an abstract level, given a grammar and an associ-
ated ME parse selection model, there are three basic
ways of combining them in order to find the single
?best? or small set of n-best results.
The first way is a na??ve sequential set-up, in which
the parser first enumerates the full set of analyses,
computes a score for each using the model, and re-
turns the highest-ranking n results. For carefully
50
1 ?
?
2 3
?
|
?
4 3
?
2 ?
?
5 6
?
|
?
5 7
?
4 ?
?
8 6
?
|
?
8 7
?
|
?
9 6
?
|
?
9 7
?
6 ?
?
10
?
|
?
11
?
Figure 2: Sample forest and sub-node decompositions: ovals in the forest (on the left) indicate packing of edges under subsump-
tion, i.e. edges 4 , 7 , 9 , and 11 are not in the chart proper. During unpacking, there will be multiple ways of instantiating a
chart edge, each obtained from cross-multiplying alternate daughter sequences locally. The elements of this cross-product we call
decomposition, and they are pivotal points both for stochastic scoring and dynamic programming in selective unpacking. The table
on the right shows all non-leaf decompositions for our example packed forest: given two ways of decomposing 6 , there will be
three candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees.
crafted grammars and inputs of average complexity
the approach can perform reasonably well.
Another mode of operation is to organize the
parser?s search according to an agenda (i.e. priority
queue) that assigns numeric scores to parsing moves
(Erbach, 1991). Each such move is an application of
the fundamental rule of chart parsing, combining an
active and a passive edge, and the scores represent
the expected ?figure of merit? (Caraballo & Char-
niak, 1998) of the resulting structure. Assuming a
parse selection model of the type sketched in Sec-
tion 2, we can determine the agenda priority for a
parsing move according to the (unnormalized) ME
score of the derivation (sub-)tree that would result
from its successful execution. Note that, unlike in
probabilistic context-free grammars (PCFGs), ME
scores of partial trees do not necessarily decrease as
the tree size increases; instead, the distribution of
feature weights is in the range (??,+?), centered
around 0, where negative weights intuitively corre-
spond to dis-preferred properties.
This lack of monotonicity in the scores associated
with sub-trees, on the one hand, is beneficial, in that
performing a greedy best-first search becomes prac-
tical: in contrast, with PCFGs and their monoton-
ically decreasing probabilities on larger sub-trees,
once the parser finds the first full tree the chart nec-
essarily has been instantiated almost completely. On
the other hand, the same property prohibits the appli-
cation of exact best-first techniques like A? search,
because there is no reliable future cost estimate; in
this respect, our set-up differs fundamentally from
that of Klein & Manning (2003) and related PCFG
parsing work. Using the unnormalized sum of ME
weights on a partial solution as its agenda score, ef-
fectively, means that sub-trees with low scores ?sink?
to the bottom of the agenda; highly-ranked partial
constituents, in turn, instigate the immediate cre-
ation of larger structures, and ideally the bottom-up
agenda-driven search will greedily steer the parser
towards full analyses with high scores. Given its
heuristic nature, this procedure cannot guarantee
that its n-best list of results corresponds to the glob-
ally correct rank order, but it may in practice come
reasonably close to it. While conceptually simple,
greedy best-first search does not combine easily with
ambiguity packing in the chart: (a) at least when
packing under subsumption, it is not obvious how
to accurately compute the agenda score of packed
nodes, and (b) to the extent that the greedy search
avoids exploration of dis-preferred local ambigu-
ity, the need for packing should be greatly reduced.
Unfortunately, in scoring bottom-up parsing moves,
ME features involving grandparenting are not ap-
plicable, leading to a second potential source of re-
duced parse selection accuracy. In Section 7 below,
we provide an empirical evaluation of both the na??ve
sequential and greedy best-first approaches.
4 Selective Unpacking
Carroll & Oepen (2005) observe that, at least for
grammars like the ERG, the construction of the
parse forest can be very efficient (with observed
polynomial complexity), especially when packing
edges under subsumption. Their selective unpacking
procedure, originally proposed for the forest created
by a chart generator, aims to unpack the n-best set
51
1 procedure selectively-unpack-edge(edge , n) ?
2 results? ??; i? 0;
3 do
4 hypothesis? hypothesize-edge(edge , i); i? i + 1;
5 if (new? instantiate-hypothesis(hypothesis)) then
6 n? n ? 1; results? results ? ?new?;
7 while (hypothesis and n ? 1)
8 return results;
9 procedure hypothesize-edge(edge , i) ?
10 if (edge.hypotheses[i]) return edge.hypotheses[i];
11 if (i = 0) then
12 for each (decomposition in decompose-edge(edge)) do
13 daughters? ? ?; indices? ? ?
14 for each (edge in decomposition.rhs) do
15 daughters? daughters ? ?hypothesize-edge(edge, 0)?;
16 indices? indices ? ?0?;
17 new-hypothesis(edge, decomposition, daughters, indices);
18 if (hypothesis? edge.agenda.pop()) then
19 for each (indices in advance-indices(hypothesis.indices)) do
20 if (indices ? hypothesis.decomposition.indices) then continue
21 daughters? ? ?;
22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do
23 daughter? hypothesize-edge(edge, i);
24 if (not daughter) then daughters? ??; break
25 daughters? daughters ? ?daughter?;
26 if (daughters) then new-hypothesis(edge, hypothesis.decomposition, daughters, indices)
27 edge.hypotheses[i]? hypothesis;
28 return hypothesis;
29 procedure new-hypothesis(edge , decomposition , daughters , indices) ?
30 hypothesis? new hypothesis(decomposition, daughters, indices);
31 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);
32 decomposition.indices? decomposition.indices? {indices};
Figure 3: Selective unpacking procedure, enumerating the n best realizations for a top-level result edge from a packed forest. An
auxiliary function decompose-edge() performs local cross-multiplication as shown in the examples in Figure 2. Another utility
function not shown in pseudo-code is advance-indices(), a ?driver? routine searching for alternate instantiations of daughter edges,
e.g. advance-indices(?0 2 1?)? {?1 2 1? ?0 3 1? ?0 2 2?}. Finally, instantiate-hypothesis() is the function that actually builds
result trees, replaying the unifications of constructions from the grammar (as identified by chart edges) with the feature structures
of daughter constituents.
of full trees from the forest, guaranteeing the glob-
ally correct rank order according to the probability
distribution, with a minimal amount of search. The
basic algorithm is a specialized graph search through
the forest, with local contexts of optimization corre-
sponding to packed nodes.
Each such node represents local combinatorics,
and two key notions in the selective unpacking pro-
cedure are the concepts of (a) decomposing an edge
locally into candidate ways of instantiating it, and
of (b) nested contexts of local search for ranked
hypotheses (i.e. uninstantiated edges) about candi-
date subtrees. See Figure 2 for examples of the de-
composition of edges. Given one decomposition?
i.e. a vector of candidate daughters for a particu-
lar rule?there can be multiple ways of instanti-
ating each daughter: a parallel index vector ~I =
?i0 . . . in? serves to keep track of ?vertical? search
among daughter hypotheses, where each index ij
denotes the i-th best instantiation (hypothesis) of
the daughter at position j. If we restrict ME fea-
tures to a depth of one (i.e. without grandparent-
ing), then given the additive nature of ME scores
on complete derivations, it can be guaranteed that
hypothesized trees including an edge e as an im-
mediate daughter must use the best instantiation of
e in their own best instantiation. Assuming a bi-
nary rule, the corresponding hypothesis would use
daughter indices of ?0 0?. The second-best instan-
tiation, in turn, can be obtained from moving to the
second-best hypothesis for one of the elements in the
(right-hand side of the) decomposition, e.g. indices
52
?0 1? or ?1 0? in the binary example. Hypotheses are
associated with ME scores and ordered within each
nested context by means of a local priority queue
(stored in the original representative edge, for con-
venience). Therefore, nested local optimizations re-
sult in a top-down, breadth-first, exact n-best search
through the packed forest, while avoiding exhaustive
cross-multiplication of packed nodes.
Figure 3 shows the unchanged pseudo-code of
Carroll & Oepen (2005). The main function
hypothesize-edge() controls both the ?horizontal? and
?vertical? search, initializing the set of decompo-
sitions and pushing initial hypotheses onto the lo-
cal agenda when called on an edge for the first
time (lines 11 ? 17). For each call, the procedure
retrieves the current next-best hypothesis from the
agenda (line 18), generates new hypotheses by ad-
vancing daughter indices (while skipping over con-
figurations seen earlier) and calling itself recursively
for each new index (lines 19 ? 26), and, finally, ar-
ranging for the resulting hypothesis to be cached for
later invocations on the same edge and i values (line
27). Note that unification (in instantiate-hypothesis())
is only invoked on complete, top-level hypotheses,
as our structural ME features can actually be eval-
uated prior to building each full feature structure.
However, as Carroll & Oepen (2005) suggest, the
procedure could be adapted to perform instantiation
of sub-hypotheses within each local search, should
additional features require it. For better efficiency,
the instantiate-hypothesis() routine applies dynamic
programming (i.e. memoization) to intermediate re-
sults.
5 Generalizing the Algorithm
Carroll & Oepen (2005) offer no solution for selec-
tive unpacking with larger context ME features. Yet,
both Toutanova et al (2005) and our own experi-
ments (described in Section 7 below) suggest that
properties of larger contexts and especially grand-
parenting can greatly improve parse selection ac-
curacy. The following paragraphs outline how to
generalize the basic selective unpacking procedure,
while retaining its key properties: exact n-best enu-
meration with minimal search. Our generalization of
the algorithm distinguishes between ?upward? con-
texts, with grandparenting with dominating nodes as
a representative feature type, and ?downward? exten-
sions, which we discuss for the example of lexical
n-gram features.
A na??ve approach to selective unpacking with
grandparenting might be extending the cross-
multiplication of local ambiguity to trees of more
than depth one. However, with multiple levels of
grandparenting this approach would greatly increase
the combinatorics to be explored, and it would pose
the puzzle of overlapping local contexts of opti-
mization. Choices made among the alternates for
one packed node would interact with other ambi-
guity contexts in their internal nodes, rather than
merely at the leaves of their decompositions. How-
ever, it is sufficient to keep the depth of decompo-
sitions to minimal sub-trees and rather contextual-
ize each decomposition as a whole. Assuming our
sample forest and set of decompositions from Fig-
ure 2, let ?1 4 ? : 6 ??10 ? denote the decomposi-
tion of node 6 in the context of 4 and 1 as its
immediate parents. When descending through the
forest, hypothesize-edge() can, without significant ex-
tra cost, maintain a vector ~P = ?pn . . . p0? of par-
ents of the current node, for n-level grandparenting.
For each packed node, the bookkeeping elements of
the graph search procedure need to be contextual-
ized on ~P , viz. (a) the edge-local priority queue,
(b) the record of index vectors hypothesized already,
and (c) the cache of previous instantiations. Assum-
ing each is stored in an associative array, then all
references to edge.agenda in the original procedure
can be replaced by edge.agenda[~P], and likewise for
other slots. With these extensions in place, the orig-
inal control structure of nested, on-demand creation
of hypotheses and dynamic programming of partial
results can be retained, and for each packed node
with multiple parents ( 6 in our sample forest) there
will be parallel, contextualized partitions of opti-
mization. Thus, extra combinatorics introduced in
this generalized procedure are confined to only such
nodes, which (intuitively at least) appears to estab-
lish the lower bound of added search needed?while
keeping the algorithm non-approximative. Section 7
provides empirical data on the degradation of the
procedure in growing levels of grandparenting and
the number of n-best results to be extracted from the
forest.
Finally, we turn to enlarged feature contexts that
53
capture information from nodes below the elements
of a local decomposition. Consider the example
of feature type 3 in Table 1, n-grams (of vari-
ous size) over properties of the yield of the parse
tree. For now we only consider lexical bi-grams.
For an edge e dominating a sub-string of n words
?wi . . . wi+n?1? there will be n? 1 bi-grams inter-
nal to e, and two bi-grams that interact with wi?1
and wi+n?which will be determined by the left-
and right-adjacent edges to e in a complete tree. The
internal bi-grams are unproblematic, and we can as-
sume that ME weights corresponding to these fea-
tures have been included in the sum of weights as-
sociated to e. Seeing that e may occur in multiple
trees, with different sister edges, the selective un-
packing procedure has to take this variation into ac-
count when evaluating local contexts of optimiza-
tion.
Let xey denote an edge e, with x and y as the
lexical types of its leftmost and rightmost daugh-
ters, respectively. Returning to our sample forest,
assume lexicalizations ? 10? and ? 11 ? (each span-
ning only one word), with ? 6= ?. Obviously, when
decomposing 4 as ?8 6 ?, its ME score, in turn, will
depend on the choice made in the expansion of 6 :
the sequences
?
? 8? ? 6?
?
and
?
? 8? ? 6 ?
?
will dif-
fer in (at least) the scores associated with the bi-
grams ???? vs. ????. Accordingly, when evalu-
ating candidate decompositions of 4 , the number of
hypotheses that need to be considered is doubled;
as an immediate consequence, there can be up to
eight distinct lexicalized variants for the decompo-
sition 1 ??4 3 ? further up in the tree. It may look
as if combinatorics will cross-multiply throughout
the tree?in the worst case returning us to an ex-
ponential number of hypotheses?but this is fortu-
nately not the case: regarding the external bi-grams
of 1 , node 6 no longer participates in its left- or
rightmost periphery, so variation internal to 6 is not
a multiplicative factor at this level. This is essen-
tially the observation of Langkilde (2000), and her
bottom-up factoring of n-gram computation is eas-
ily incorporated into our top-down selective unpack-
ing control structure. At the point where hypothesize-
edge() invokes itself recursively (line 23 in Figure 3),
its return value is now a set of lexicalized alternates,
and hypothesis creation (in line 26) can take into ac-
count the local cross-product of all such alternation.
Including additional properties from non-local sub-
trees (for example higher-order n-grams and head
lexicalization) is a straightforward extension of this
scheme, replacing our per-edge left- and rightmost
periphery symbols with a generalized vector of ex-
ternally relevant, internal properties. In addition
to traditional (head) lexicalization as we have just
discussed it, such extended ?downward? properties
on decompositions?percolated from daughters to
mothers and cross-multiplied as appropriate?could
include metrics of constituent weight too, for exam-
ple to enable the ME model to prefer ?balanced? co-
ordination structures.
However, given that Toutanova et al (2005) ob-
tain only marginally improved parse selection accu-
racy from the inclusion of n-gram (and other lexical)
ME features, we have left the implementation of lex-
icalization and empirical evaluation for future work.
6 Failure Caching and Propagation
As we pointed out at the end of Section 4, during
the unpacking phase, unification is only replayed in
instantiate-hypothesis() on the top-level hypotheses. It
is only at this step that inconsistencies in the local
combinatorics are discovered. However, such a dis-
covery can be used to improve the unpacking rou-
tine by (a) avoiding further unification on hypothe-
ses that have already failed to instantiate, (b) avoid-
ing creating new hypotheses based on failed sub-
hypotheses. This requires some changes to the rou-
tines instantiate-hypothesis() and hypothesize-edge(), as
well as an extra boolean marker for each hypothesis.
The extended instantiate-hypothesis() starts by
checking whether the hypothesis is already marked
as failed. If it is not so marked, the routine recur-
sively instantiates all sub-hypotheses. Any failure
will again lead to instant return. Otherwise, unifica-
tion is used to create a new edge from the outcome of
the sub-hypothesis instantiations. If this unification
fails, the current hypothesis is marked. Moreover,
all its ancestor hypotheses are also marked (by re-
cursively following the pointers to the direct parent
hypotheses) as they are also guaranteed to fail.
Correspondingly, hypothesize-edge() needs to
check the instantiation failure marker to avoid re-
turning hypotheses that are guaranteed to fail. If
a hypothesis coming out of the agenda is already
54
marked as failed, it will be used to create new hy-
potheses (with advance-indices()), but dropped af-
terward. Subsequent hypotheses will be popped
from the agenda until either a hypothesis that is not
marked as failed is returned, or the agenda is empty.
Moreover, hypothesize-edge() also needs to avoid
creating new hypotheses based on failed sub-
hypotheses. When a failed sub-hypothesis is found,
the creation of the new hypothesis is skipped. But
the index vector ~I may not be simply discarded.
Otherwise hypotheses based on advance-indices(~I)
will not be reachable in the search. On the other
hand, simply adding every advance-indices(~I) on to
the pending creation list is not efficient either in the
case where multiple sub-hypotheses fail.
To solve the problem, we compute a failure vec-
tor ~F = ?f0 . . . fn?, where fj is 1 when the sub-
hypothesis at position j is known as failed, and 0
otherwise. If a sub-hypothesis at position j is failed
then all the index vectors having value ij at posi-
tion j must also fail. By putting the result of ~I + ~F
on the pending creation list, we can safely skip the
failed rows of sub-hypotheses, while not losing the
reachability of the others. As an example, suppose
we have a ternary index vector ?3 1 2? for which a
new hypothesis is to be created. By checking the in-
stantiation failure marker of the sub-hypotheses, we
find that the first and the third sub-hypotheses are al-
ready marked. The failure recording vector will then
be ?1 0 1?. By putting ?4 1 3? = ?3 1 2? + ?1 0 1?
on to the pending hypothesis creation list, the failed
sub-hypotheses are skipped.
We evaluate the effects of instantiation failure
caching and propagation below in Section 7.
7 Empirical Results
To evaluate the performance of the selective unpack-
ing algorithm, we carried out a series of empirical
evaluations with the ERG and GG, in combination
with a modified version of the PET parser. When
running the ERG we used as our test set the JH4
section of the LOGON treebank3, which contains
1603 items with an average sentence length of 14.6
words. The remaining LOGON treebank (of around
3The treebank is comprised of several booklets of
edited, instructional texts on backcountry activities in Nor-
way. The data is available from the LOGON web site at
?http://www.emmtee.net?.
Configuration GP Coverage Time (s)
greedy best-first 0 91.6% 3889
exhaustive unpacking 0 84.5% 4673
selective unpacking
0 94.3% 2245
1 94.3% 2529
2 94.3% 3964
3 94.2% 3199
4 94.2% 3502
Table 2: Coverage on the ERG for different configurations, with
fixed resource consumption limits (of 100k passive edges or 300
seconds). In all cases, up to ten ?best? results were searched,
and Coverage shows the percentage of inputs that succeed to
parse within the available resource. Time shows the end-to-end
processing time for each batch.
5 15 25 35
String Length (Number of Input Tokens)
0
1
2
3
4
5
6
(s)
(generated by [incr tsdb()] at 23-mar-2007 (12:44 h))?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? gready best-first
? exhaustive unpacking
? selective unpacking
? forest creation
Figure 4: Parsing times for different configurations using the
ERG, in all three cases searching for up to ten results, without
the use of grandparenting.
8,000 items) was used in training the various ME
parse disambiguation models. For the experiment
with GG, we designated a 2825-item portion of the
DFKI Verbmobil treebank4 for our tests, and trained
ME models on the remaining 10,000 utterances. At
only 7.4 words, the average sentence length is much
shorter in the Verbmobil data.
We ran seven different configurations of the parser
with different search strategies and (un-)packing
mechanisms:
? Agenda driven greedy n-best parsing using the
ME score without grandparenting features; no
local ambiguity packing;
? Local ambiguity packing with exhaustive un-
packing, without grandparenting features;
4The data in this treebank is taken from transcribed appoint-
ment scheduling dialogues; see ?http://gg.dfki.de/?
for further information on GG and its treebank.
55
1 10 20 30 40 50 60 70 80 90 100
Maximum Number of Trees to Unpack (n)
0.00
0.02
0.04
0.06
0.08
0.10
(s)
? ? ? ? ?
? ?
?? ? ? ?
? ? ?
?
? ? ? ?
? ? ?
?? ? ?
? ? ?
?
?? ?
? ?
? ? ?
?
GP=0
GP=1
GP=2
GP=3
GP=4
Figure 5: Mean times for selective unpacking of all test items
for n-best parsing with the ERG, for varying n and grandpar-
enting (GP) levels
? Local ambiguity packing and selective unpack-
ing for n-best parsing, with 0 through 4 levels
of grandparenting (GP) features.
As a side-effect of differences in efficiency, some
configurations could not complete parsing all sen-
tences given reasonable memory constraints (which
we set at a limit of 100k passive edges or 300 sec-
onds processing time per item). The overall cover-
age and processing time of different configurations
on JH4 are given in Table 2.
The correlation between processing time and cov-
erage is interesting. However, it makes the efficiency
comparison difficult as parser behavior is not clearly
defined when the memory limit is exceeded. To cir-
cumvent this problem, in the following experiments
we average only over those 1362 utterances from
JH4 that complete parsing within the resource limit
in all seven configurations. Nevertheless, it must
be noted that this restriction potentially reduces effi-
ciency differences between configurations, as some
of the more challenging inputs (which typically lead
to the largest differences) are excluded.
Figure 4 compares the processing time of differ-
ent configurations. The difference is much more
significant for longer sentences (i.e. with more than
15 words). If the parser unpacks exhaustively, the
time for unpacking grows with sentence length at a
quickly increasing rate. In such cases, the efficiency
gain with ambiguity packing in the parsing phase
is mostly lost in the unpacking phase. The graph
shows that greedy best-first parsing without packing
outperforms exhaustive unpacking for sentences of
Configuration Exact Match Top Ten
random choice 11.34 43.06
no grandparenting 52.52 68.38
greedy best-first 51.79 69.48
grandparenting[1] 56.83 85.33
grandparenting[2] 56.55 84.14
grandparenting[3] 56.37 84.14
grandparenting[4] 56.28 84.51
Table 3: Parse selection accuracy for various levels of grandpar-
enting. The exact match column shows the percentage of cases
in which the correct tree, according to the treebank, was ranked
highest by the model; conversely, the top ten column indicates
how often the correct tree was among the ten top-ranking re-
sults.
less than 25 words. With sentences longer than 25
words, the packing mechanism helps the parser to
overtake greedy best-first parsing, although the ex-
haustive unpacking time also grows fast.
With the selective unpacking algorithm presented
in the previous sections, unpacking time is reduced,
and grows only slowly as sentence length increases.
Unpacking up to ten results, when contrasted with
the timings for forest creation (i.e. the first parsing
phase) in Figure 4, adds a near-negligible extra cost
to the total time required for both phases. Moreover,
Figure 5 shows that with selective unpacking, as n
is increased, unpacking time grows roughly linearly
for all levels of grandparenting (albeit always with
an initial delay in unpacking the first result).
Table 4 summarizes a number of internal parser
measurements using the ERG with different pack-
ing/unpacking settings. Besides the difference in
processing time, we also see a significant difference
in ?space? between exhaustive and selective un-
packing. Also, the difference in ?unifications? and
?copies? indicates that with our selective unpacking
algorithm, these expensive operations on typed fea-
ture structures are significantly reduced.
In return for increased processing time (and
marginal loss in coverage) when using grandparent-
ing features, Table 3 shows some large improve-
ments in parse selection accuracy (although the pic-
ture is less clear-cut at higher-order levels of grand-
parenting5). A balance point between efficiency
5The models were trained using the open-source TADM pack-
age (Malouf, 2002), using default hyper-parameters for all con-
figurations, viz. a convergence threshold of 10?8, variance of
the prior of 10?4, and frequency cut-off of 5. It is likely that
56
Configuration GP Unifications Copies Space Unpack Total(#) (#) (kbyte) (s) (s)
? 15
greedy best-first 0 1845 527 2328 ? 0.12
words
exhaustive unpacking 0 2287 795 8907 0.01 0.12
selective unpacking
0 1912 589 8109 0.00 0.12
1 1913 589 8109 0.01 0.12
2 1914 589 8109 0.01 0.12
3 1914 589 8110 0.01 0.12
4 1914 589 8110 0.02 0.13
> 15
greedy best-first 0 25233 5602 24646 ? 1.66
words
exhaustive unpacking 0 39095 15685 80832 0.85 1.95
selective unpacking
0 17489 4422 33326 0.03 1.17
1 17493 4421 33318 0.05 1.21
2 17493 4421 33318 0.09 1.25
3 17495 4422 33321 0.13 1.27
4 17495 4422 33320 0.21 1.34
Table 4: Contrasting the efficiency of various (un-)packing settings in use with ERG on short (top) and medium-length (bottom)
inputs; in each configuration, up to ten trees are extracted. Unification and Copies is the count of top-level FS operations, where
only successful unifications require a subsequent copy (when creating a new edge). Unpack and Total are unpacking and total parse
time, respectively.
and accuracy can be made according to application
needs.
Finally, we compare the processing time of the
selective unpacking algorithm with and without in-
stantiation failure caching and propagation (as de-
scribed in Section 4 above). The empirical results
for GG are summarized in Table 5, showing clearly
that the technique reduced unnecessary hypotheses
and instantiation failures. The design philosophy of
the ERG and GG differ. During the first, forest cre-
ation phase, GG suppresses a number of features (in
the HPSG sense, not the ME sense) that can actually
constrain the combinatorics of edges. This move
makes the packed forest more compact, but it im-
plies that unification failures will be more frequent
during unpacking. In a sense, GG thus moves part
of the search for globally consistent derivations into
the second phase, and it is possible for the forest to
contain ?result? trees that ultimately turn out to be
incoherent. Dynamic programming of instantiation
failures makes this approach tractable, while retain-
ing the general breadth-first characteristic of the se-
lective unpacking regime.
further optimization of hyper-parameters for individual config-
urations would moderately improve model performance, espe-
cially for higher-order grandparenting levels with large numbers
of features.
8 Discussion
The approach to n-best parsing described in this pa-
per takes as its point of departure recent work of Car-
roll & Oepen (2005), which describes an efficient al-
gorithm for unpacking n-best trees from a forest pro-
duced by a chart-based sentence generator and con-
taining local ME properties with associated weights.
In an almost contemporaneous study, but in the con-
text of parsing with treebank grammars, Huang &
Chiang (2005) develop a series of increasingly effi-
cient algorithms for unpacking n-best results from
a weighted hypergraph representing a parse forest.
The algorithm of Carroll & Oepen (2005) and the
final one of Huang & Chiang (2005) are essentially
equivalent, and turn out to be reformulations of an
approach originally described by Jime?nez & Marzal
(2000) (although expressed there only for grammars
in Chomsky Normal Form).
In this paper we have considered ME properties
that extend beyond immediate dominance relations,
extending up to 4 levels of grandparenting. Pre-
vious work has either assumed properties that are
restricted to the minimal parse fragments (i.e. sub-
trees of depth one) that make up the packed repre-
sentation (Geman & Johnson, 2002), or has taken a
more relaxed approach by allowing non-local prop-
57
Configuration Unifications Copies Hypotheses Space Unpack Total(#) (#) (#) (kbyte) (ms) (ms)
greedy best-first 5980 1447 ? 9202 ? 400
selective, no caching 5535 1523 1245 27188 70 410
selective, with cache 4915 1522 382 27176 10 350
Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting. All statistics are
averages over the 1941 items that complete within the resource bounds in all three configurations. Unification, Copies, Unpack,
and Total have the same interpretation as in Table 4, and Hypotheses is the average count of hypothesized sub-trees.
erties but without addressing the problem of how to
efficiently extract the top-ranked trees from a packed
forest (Miyao & Tsujii, 2002).
Probably the work closest in spirit to our approach
is that of Malouf & van Noord (2004), who use an
HPSG grammar comparable to the ERG and GG,
non-local ME features, and a two-phase parse for-
est creation and unpacking approach. However, their
unpacking phase uses a beam search to find a good
(single) candidate for the best parse; in contrast?
for ME models containing the types of non-local
features that are most important for accurate parse
selection?we avoid an approximative search and ef-
ficiently identify exactly the n-best parses.
When parsing with context free grammars, a (sin-
gle) parse can be retrieved from a parse forest in
time linear in the length of the input string (Bil-
lot & Lang, 1989). However, as discussed in Sec-
tion 2, when parsing with a unification-based gram-
mar and packing under feature structure subsump-
tion, the cross-product of some local ambiguities
may not be globally consistent. This means that ad-
ditional unifications are required at unpacking time.
In principle, when parsing with a pathological gram-
mar with a high rate of failure, extracting a single
consistent parse from the forest could take exponen-
tial time (see Lang (1994) for a discussion of this is-
sue with respect to Indexed Grammars). In the case
of GG, a high rate of unification failure in unpacking
is dramatically reduced by our instantiation failure
caching and propagation mechanism.
9 Conclusions and Future Work
We have described and evaluated an algorithm for
efficiently computing the n-best analyses from a
parse forest produced by a unification grammar, with
respect to a Maximum Entropy (ME) model con-
taining two classes of non-local features. The al-
gorithm is efficient in that it empirically exhibits a
linear relationship between processing time and the
number of analyses unpacked, at all degrees of ME
feature non-locality. It improves over previous work
in providing the only exact procedure for retrieving
n-best analyses from a packed forest that can deal
with features with extended domains of locality and
with forests created under subsumption. Our algo-
rithm applies dynamic programming to intermediate
results and local failures in unpacking alike.
The experiments compared the new algorithm
with baseline systems representing other possible
approaches to parsing with ME models: (a) a single
phase of agenda-driven parsing with on-line prun-
ing based on intermediate ME scores, and (b) two-
phase parsing with exhaustive unpacking and post-
hoc ranking of complete trees. The new approach
showed better speed, coverage, and accuracy than
the baselines.
Although we have dealt with the non-local ME
features that in previous work have been found to be
the most important for parse selection (i.e. grand-
parenting and n-grams), this does not exhaust the
full range of features that could possibly be useful.
For example, it may be the case that accurately re-
solving some kinds of ambiguities can only be done
with reference to particular parts?or combinations
of parts?of the HPSG feature structures represent-
ing the analysis of a complete constituent. To deal
with such cases we are currently designing an exten-
sion to the algorithms described here which would
add a ?controlled? beam search, in which the size of
the beam was limited by the interval of score adjust-
ments for ME features that could only be evaluated
once the full linguistic structure became available.
This approach would involve a constrained amount
of extra search, but would still produce the exact n-
best trees.
58
References
Abney, S. P. (1997). Stochastic attribute-value grammars. Com-
putational Linguistics, 23, 597 ? 618.
Billot, S., & Lang, B. (1989). The structure of shared forests
in ambiguous parsing. In Proceedings of the 27th Meeting
of the Association for Computational Linguistics (pp. 143 ?
151). Vancouver, BC.
Callmeier, U. (2002). Preprocessing and encoding techniques
in PET. In S. Oepen, D. Flickinger, J. Tsujii, & H. Uszkor-
eit (Eds.), Collaborative language engineering. A case study
in efficient grammar-based processing. Stanford, CA: CSLI
Publications.
Caraballo, S. A., & Charniak, E. (1998). New figures of merit
for best-first probabilistic chart parsing. Computational Lin-
guistics, 24(2), 275 ? 298.
Carroll, J., & Oepen, S. (2005). High-efficiency realization for
a wide-coverage unification grammar. In R. Dale & K. F.
Wong (Eds.), Proceedings of the 2nd International Joint
Conference on Natural Language Processing (Vol. 3651, pp.
165 ? 176). Jeju, Korea: Springer.
Clark, S., & Curran, J. R. (2004). Parsing the WSJ using CCG
and log-linear models. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics (pp. 104 ?
111). Barcelona, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Crysmann, B. (2005). Relative clause extraposition in German.
An efficient and portable implementation. Research on Lan-
guage and Computation, 3(1), 61 ? 82.
Erbach, G. (1991). A flexible parser for a linguistic develop-
ment environment. In O. Herzog & C.-R. Rollinger (Eds.),
Text understanding in LILOG (pp. 74 ? 87). Berlin, Ger-
many: Springer.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1),
15 ? 28.
Geman, S., & Johnson, M. (2002). Dynamic programming for
parsing and estimation of stochastic unification-based gram-
mars. In Proceedings of the 40th Meeting of the Association
for Computational Linguistics. Philadelphia, PA.
Huang, L., & Chiang, D. (2005). Better k-best parsing. In
Proceedings of the 9th International Workshop on Parsing
Technologies (pp. 53 ? 64). Vancouver, Canada.
Jime?nez, V. M., & Marzal, A. (2000). Computation of the
n best parse trees for weighted and stochastic context-free
grammars. In Proceedings of the Joint International Work-
shops on Advances in Pattern Recognition (pp. 183 ? 192).
London, UK: Springer-Verlag.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Association
for Computational Linguistics (pp. 535 ? 541). College Park,
MD.
Kasami, T. (1965). An efficient recognition and syntax al-
gorithm for context-free languages (Technical Report # 65-
758). Bedford, MA: Air Force Cambrige Research Labora-
tory.
Klein, D., & Manning, C. D. (2003). A* parsing. Fast exact
Viterbi parse selection. In Proceedings of the 4th Confer-
ence of the North American Chapter of the ACL. Edmonton,
Canada.
Lang, B. (1994). Recognition can be harder than parsing. Com-
putational Intelligence, 10(4), 486 ? 494.
Langkilde, I. (2000). Forest-based statistical sentence gener-
ation. In Proceedings of the 1st Conference of the North
American Chapter of the ACL. Seattle, WA.
Malouf, R. (2002). A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
6th Conference on Natural Language Learning. Taipei, Tai-
wan.
Malouf, R., & van Noord, G. (2004). Wide coverage parsing
with stochastic attribute value grammars. In Proceedings of
the IJCNLP workshop Beyond Shallow Analysis. Hainan,
China.
Miyao, Y., Ninomiya, T., & Tsujii, J. (2005). Corpus-oriented
grammar development for acquiring a Head-Driven Phrase
Structure Grammar from the Penn Treebank. In K.-Y. Su,
J. Tsujii, J.-H. Lee, & O. Y. Kwong (Eds.), Natural language
processing (Vol. 3248, pp. 684 ? 693). Hainan Island, China.
Miyao, Y., & Tsujii, J. (2002). Maximum entropy estimation
for feature forests. In Proceedings of the Human Language
Technology Conference. San Diego, CA.
Moore, R. C., & Alshawi, H. (1992). Syntactic and semantic
processing. In H. Alshawi (Ed.), The Core Language Engine
(pp. 129 ? 148). Cambridge, MA: MIT Press.
Mu?ller, S., & Kasper, W. (2000). HPSG analysis of German.
In W. Wahlster (Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Carroll, J. (2002). Efficient parsing for
unification-based grammars. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collaborative language en-
gineering. A case study in efficient grammar-based process-
ing (pp. 195 ? 225). Stanford, CA: CSLI Publications.
Oepen, S., Flickinger, D., Toutanova, K., & Manning, C. D.
(2004). LinGO Redwoods. A rich and dynamic treebank for
HPSG. Journal of Research on Language and Computation,
2(4), 575 ? 596.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell III,
J. T., & Johnson, M. (2002). Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discriminative es-
timation techniques. In Proceedings of the 40th Meeting of
the Association for Computational Linguistics. Philadelphia,
PA.
Toutanova, K., Manning, C. D., Flickinger, D., & Oepen, S.
(2005). Stochastic HPSG parse selection using the Red-
woods corpus. Journal of Research on Language and Com-
putation, 3(1), 83 ? 105.
59
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1561?1570,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Chinese Novelty Mining
Yi Zhang
Nanyang Technological University
50 Nanyang Avenue
Singapore 639798
yizhang@ntu.edu.sg
Flora S. Tsai
Nanyang Technological University
50 Nanyang Avenue
Singapore 639798
fst1@columbia.edu
Abstract
Automated mining of novel documents
or sentences from chronologically ordered
documents or sentences is an open chal-
lenge in text mining. In this paper, we
describe the preprocessing techniques for
detecting novel Chinese text and discuss
the influence of different Part of Speech
(POS) filtering rules on the detection per-
formance. Experimental results on AP-
WSJ and TREC 2004 Novelty Track data
show that the Chinese novelty mining per-
formance is quite different when choosing
two dissimilar POS filtering rules. Thus,
the selection of words to represent Chinese
text is of vital importance to the success of
the Chinese novelty mining. Moreover, we
compare the Chinese novelty mining per-
formance with that of English and investi-
gate the impact of preprocessing steps on
detecting novel Chinese text, which will
be very helpful for developing a Chinese
novelty mining system.
1 Introduction
The bloom of information nowadays brings us rich
useful information as well as tons of redundant in-
formation in news articles, social networks (Tsai et
al., 2009), and blogs (Chen et al, 2008). Novelty
mining (NM), or novelty detection, aims at mining
novel information from a chronologically ordered
list of relevant documents/sentences. It can facil-
itate users to quickly get useful information with-
out going through a lot of redundant information,
which is usually a tedious and time-consuming
task.
The process of detecting novel text contains
three main steps, (i) preprocessing, (ii) cate-
gorization, and (iii) novelty mining. The first
step preprocesses the text documents/sentences
by removing stop words, performing word stem-
ming, implementing POS tagging etc. Categoriza-
tion classifies each incoming document/sentence
into its relevant topic bin. Then, within each
topic bin containing a group of relevant docu-
ments/sentences, novelty mining searches through
the time sequence of documents/sentences and re-
trieves only those with ?novel? information. This
paper focuses on applying document/sentence-
level novelty mining on Chinese. In this task,
we need to identify all novel Chinese text given
groups of relevant documents/sentences.
Novelty mining has been performed at three dif-
ferent levels: event level, sentence level and doc-
ument level (Li and Croft, 2005). Works on nov-
elty mining at the event level originated from re-
search on Topic Detection and Tracking (TDT),
which is concerned with online new event detec-
tion/first story detection (Allan et al, 1998; Yang
et al, 2002; Stokes and Carthy, 2001; Franz et
al., 2001; Brants et al, 2003). Research on doc-
ument and sentence-level novelty mining aims to
find relevant and novel documents/sentences given
a stream of documents/sentences. Previous stud-
ies on document and sentence-level novelty min-
ing tend to apply some promising content-oriented
techniques (Li and Croft, 2005; Allan et al, 1998;
Yang et al, 1998; Zhang and Tsai, 2009). Simi-
larity metrics that can be used for detecting novel
text are word overlap, cosine similarity (Yang et
al., 1998), new word count (Brants et al, 2003),
etc. Other works utilize ontological knowledge,
especially taxonomy, such as WordNet (Zhang et
al., 2002; Allan et al, 2003), synonym dictionary
(Franz et al, 2001), HowNet (Eichmann and Srini-
vasan, 2002), etc.
Previous studies for novelty mining have been
conducted on the English and Malay languages
(Kwee et al, 2009; Tang et al, 2009; Tang and
Tsai, 2009). Novelty mining studies on the Chi-
nese language have been performed on topic de-
1561
tection and tracking, which identifies and collects
relevant stories on certain topics from information
stream (Zheng et al, 2008; Hong et al, 2008).
Also many works have discussed the issues, such
as word segmentation, POS tagging etc, between
English and Chinese (Wang et al, 2006; Wu et
al., 2003). However, to the best of our knowledge,
no studies have been reported on discussing pre-
processing techniques on Chinese document and
sentence-level novelty mining, which is the focus
of our paper.
The rest of this paper is organized as follows.
Section 2 gives a brief overview of related work on
detecting novel documents and sentences on En-
glish and Chinese. Section 3 introduces the details
of preprocessing steps for English and Chinese.
A general novelty mining algorithm is described
in Section 4. Section 5 reports experimental re-
sults. Section 6 summarizes the research findings
and discusses issues for further research.
2 Related Work
In the pioneering work for detecting novel doc-
uments (Zhang et al, 2002), document novelty
was predicted based on the distance between the
new document and the previously delivered doc-
uments in history. The detected document which
is very similar to any of its history documents is
regarded as a redundant document. To serve users
better, it could be more helpful to further highlight
novel information at the sentence level. Therefore,
later studies focused on detecting novel sentences,
such as those reported in TREC 2002-2004 Nov-
elty Tracks (Harman, 2002; Soboroff and Harman,
2003; Soboroff, 2004), which compared various
novelty metrics (Allan et al, 2003), and integrated
different natural language techniques (Ng et al,
2007; Li and Croft, 2008).
Although novelty mining studies have mainly
been conducted on the English language, stud-
ies on the Chinese language have been performed
on topic detection and tracking. A prior study
(Zheng et al, 2008) proposed an improved rel-
evance model to detect the novelty information
in topic tracking feedback and modified the topic
model based on this information. Experimental
results on Chinese datasets TDT4 and TDT2003
proved the effectiveness in topic tracking. Another
study proposed a method of applying semantic do-
main language model to link detection, based on
the structure relation among contents and the se-
mantic distribution in a story (Hong et al, 2008).
3 Preprocessing for English and Chinese
3.1 English
Since the focus of this paper is on novelty min-
ing, we begin from a list of relevant documents or
sentences that have already undergone the catego-
rization process.
The first step for English preprocessing is to re-
move all stop words from documents or sentences,
such as conjunctions, prepositions, and articles.
Stop words are words that are too common to
be informative. These words should be removed,
otherwise it will influence the novelty prediction
of documents or sentences. After stop words re-
moval, the remaining words are then stemmed.
The inflected (or sometimes derived) words are
reduced to their root forms. This paper used
Porter stemming algorithm (Porter, 1997) for En-
glish word stemming. This algorithm removes the
commoner morphological and inflexional endings
from the words in English. The entire preprocess-
ing steps in English novelty mining can be seen in
Figure 1.
3.2 Chinese
In Chinese, the word is the smallest independent
meaningful element. There is no obvious bound-
ary between words so that Chinese lexical anal-
ysis, such as Chinese word segmentation, is the
prerequisite for novelty mining.
Unlike English, Chinese word segmentation
is a very challenging problem because of the
difficulties in defining what constitutes a word
(Gao et al, 2005). While each criteria pro-
vides valuable insights into ?word-hood? in Chi-
nese, they do not consistently lead us to the
same conclusions. Moreover, there is no white
space between Chinese words or expressions
and there are many ambiguities in the Chinese
language, such as: ???????? (means
?mainboard and server? in English) might be ??
?/?/???? (means ?mainboard/and/server? in
English) or ???/??/?/?? (means ?main-
board/kimono/task/utensil? in English). This am-
biguity is a great challenge for Chinese word seg-
mentation. In addition, there is no obvious in-
flected or derived words in Chinese so that word
stemming is not applicable.
Therefore, in order to reduce the noise brought
by Chinese word segmentation and get a better
1562
Remove Stop 
Words
Stem Words
Novelty Mining
Relevant documents / 
sentences
Novel documents /
sentences
Preprocessing 
steps
Figure 1: Preprocessing steps on English.
word list for one document or sentence, we firstly
apply word segmentation on the Chinese text and
then utilize Part-of-Speech (POS) tagging to se-
lect the meaningful candidate words. Figure 2
shows the preprocessing steps on the Chinese text
for novelty mining. POS tagging is a process of
marking up the word in a text as corresponding
to a particular part of speech. It is learnt that the
idea of a text mainly relies on some meaningful
words, such as nouns and verbs, so that we can get
the main content by extracting these meaningful
words. Moreover, it will decrease the impact of
the errors in Chinese word segmentation on nov-
elty mining because only meaningful words are
considered and other words (including stop words)
such as ???? (means ?although? in English) will
not appear in the word list for the following sim-
ilarity computation in novelty mining. Losee also
mentioned that POS tagging shows a great poten-
tial to avoid lexical ambiguity and it can help to
improve the performance of information retrieval
(Losee, 2001).
ICTCLAS is used when performing word seg-
mentation and POS tagging in our experiments
(ICTCLAS, 2008). It is an open source project
and achieves a better precision in Chinese word
segmentation and POS tagging than other Chi-
nese POS tagging softwares (ICTCLAS, 2008).
First, we apply word segmentation on the relevant
Chinese documents/sentences. Chinese word seg-
mentation includes atom segmentation, N-shortest
path based rough segmentation and unknown
words recognition (see Figure 3). Atom segmen-
tation is an initial step of the Chinese language
segmentation process, where atom is defined to
be the minimal unit that cannot be split further.
The atom can be a Chinese character, punctua-
tion, symbol string, etc. Then, rough segmentation
tries to discover the correct segmentation with as
few candidates as possible. The N-Shortest Path
(NSP) method (Zhang and Liu, 2002) is applied
for rough segmentation. Next, we detect some un-
known words such as person name, location name
so as to optimize the segmentation result. Finally,
we POS tag the words and keep some kinds of
words in the word list according to the selective
rule, which are used in novelty mining.
4 Novelty Mining
From the output of preprocessing, we can obtain a
bag of words. The corresponding term-document
matrix (TDM)/term-sentence matrix (TSM) can be
constructed by counting the term frequency (TF)
of each word. The novelty mining system predicts
any incoming document/sentence by comparing it
with its history documents/sentences in this vector
space. Therefore, given a Chinese TDM/TSM, the
novelty mining system designed for English can
also be applied to Chinese.
In novelty mining, the novelty of a docu-
ment/sentence can be quantitatively measured by a
novelty metric and represented by a novelty score.
The most popular novelty metric, i.e. cosine sim-
ilarity (see (Allan et al, 2003)), is adopted. This
metric first calculates the similarities between the
current document/sentence d
t
and each of its his-
1563
Word
Segmentation
POS Tagging
Novelty Mining
Relevant documents / 
sentences
Novel documents /
sentences
Preprocessing 
steps
Figure 2: Preprocessing steps on Chinese.
tory documents/sentences d
i
(1 ? i ? t ? 1).
Then, the novelty score is simply one minus the
maximum of these cosine similarities, as shown in
Eq.(1).
Novelty Score(d
t
) = 1? max
1?i?t?1
cos(d
t
, d
i
) (1)
cos(d
t
, d
i
) =
?
n
k=1
w
k
(d
t
) ? w
k
(d
i
)
?d
t
? ? ?d
i
?
where N
cos
(d) denotes the cosine similarity score
of the document/sentence d and w
k
(d) is the
weight of k
th
element in the document/sentence
weighted vector d. The term weighting function
used in our work is TF(term frequency).
The final decision on whether a docu-
ment/sentence is novel or not depends on whether
the novelty score falls above or below a thresh-
old. The document/sentence predicted as ?novel?
will be placed into the list of history docu-
ments/sentences.
5 Experiments and Results
5.1 Datasets
Two public datasets APWSJ (Zhang et al, 2002)
and TREC Novelty Track 2004 (Soboroff, 2004)
are selected as our experimental datasets for the
document-level and the sentence-level novelty
mining respectively. APWSJ data consists of
news articles from Associated Press (AP) and Wall
Street Journal (WSJ). There are 50 topics from
Q101 to Q150 in APWSJ and 5 topics (Q131,
Q142, Q145, Q147, Q150) are excluded from the
Table 1: Statistics of experimental data
Dataset Novel Non-novel
APWSJ 10839(91.10%) 1057(8.90%)
TREC2004 3454(41.40%) 4889(58.60%)
experiments because they lack non-novel docu-
ments (Zhao et al, 2006). The assessors provide
two degrees of judgements on non-novel docu-
ments, absolute redundant and somewhat redun-
dant. In our experiments, we adopt the strict defi-
nition used in (Zhang et al, 2002) where only ab-
solute redundant documents are regarded as non-
novel. TREC 2004 Novelty Track data is devel-
oped from AQUAINT collection. Both relevant
and novel sentences are selected by TREC?s asses-
sors. The statistics of these two datasets are sum-
marized in Table 1.
5.2 Evaluation Measures
From many previous works, redundancy precision
(RP ), redundancy recall (RR) and redundancy F
Score (RF ) are used to evaluate the performance
of document-level novelty mining (Zhang et al,
2002). Precision (P ), recall (R) and F Score (F )
are mainly used in evaluating the performance for
sentence-level novelty mining (Allan et al, 2003).
Therefore, we use RP , RR, RF and redundancy
precision-recall (R-PR) curve to evaluate our ex-
perimental results on the document level. P , R, F
and precision-recall (PR) curve are used to eval-
uate the performance on the sentence-level nov-
elty mining. The larger the area under the R-PR
1564
Document/ Sentence String
AtomSegmentation
NSP-basedRough Segmentation
UnknownWord Recognition
POS Tagging
Atom Sequence
Top N Sequence
Revised N Result 
POS Sequence
Novelty Mining
Figure 3: Word segmentation on Chinese.
curve/PR curve, the better the algorithm. Also
we drew the standard redundancy F Score/F Score
contours (Soboroff, 2004), which indicate the F
Score values when setting precision and recall
from 0 to 1 with a step of 0.1. These contours can
facilitate us to compare redundancy F Scores/F
Scores in R-PR curves/PR curves. Redundancy
precision, redundancy recall, precision and recall
on a certain topic are defined as:
Redundancy Precision =
R
?
R
?
+N
?
(2)
Redundancy Recall =
R
?
R
?
+R
+
(3)
Precision =
N
+
N
+
+R
+
(4)
Recall =
N
+
N
+
+N
?
(5)
where R
+
,R
?
,N
+
,N
?
correspond to the number
of documents/sentences that fall into each cate-
gory (see Table 2).
Based on all the topics? RP /P and RR/R, we
could get the average RP /P and average RR/R by
calculating the arithmetic mean of these scores on
all topics. Then, the average redundancy F Score
(RF )/F Score (F ) is obtained by the harmonic av-
erage of the average RP /P and average RR/R.
Table 2: Categories for evaluation
Non-novel Novel
Delivered R
+
N
+
Not Delivered R
?
N
?
5.3 Experimental Results
In this experimental study, the focus was novelty
mining rather than relevant documents/sentences
categorization. Therefore, our experiments started
with all given relevant Chinese text, from which
the novel text should be identified.
Since the datasets that we used for document-
level novelty mining and sentence-level novelty
mining both were written in English, we first trans-
lated them into Chinese. During this process,
we investigated issues on machine translation vs.
manually corrected translation.
We compared the novelty mining performance
on 107 text in TREC 2004 Novelty Track between
automatically translated using Google Translate
API
1
and the manually corrected translation. For
example, here is an English sentence in Topic 51:
According to a Chilean government report, a
total of 4,299 political opponents died or disap-
peared during Pinochet?s term.
After machine translation using Google Trans-
lator, the above sentence is translated as:
????????????4299?????
??????????????
1
http://code.google.com/p/google-api-translate-java
1565
Then we manually corrected the machine trans-
lation and obtained the corrected translation:
???????????????????
????4299????????
After novelty mining on the machine transla-
tion sentences and the humanly corrected transla-
tion sentences individually, we found that there is
a slight difference (<2%) in precision and F Score.
Thus, we used machine translation to translate the
remaining documents/sentences to Chinese. This
indicates that the noise in machine translation for
Chinese had little impact on our actual results.
Then on English text, we applied the prepro-
cessing steps discussed in Section 3.1, includ-
ing stop word removing and word stemming.
For Chinese datasets, we segmented the docu-
ments/sentences into words and then performed
POS filtering to acquire the candidate words for
the space vector.
Based on the vectors of Chinese text, we
calculated the similarities between docu-
ments/sentences and predicted the novelty
for each document/sentence in the Chinese and
English datasets. An incoming Chinese/English
document will be compared with all the system
delivered 10 novel documents. If the novelty score
is above the novelty score threshold, the document
is considered to be novel. Thresholds used were
between 0.05 and 0.65. We also performed
Chinese/English sentence-level novelty mining.
Whether an incoming Chinese/English sentence
is novel is predicted by comparing with the most
recent system-delivered 1000 novel sentences.
Thresholds adopted were between 0.05 and 0.95
with an equal step of 0.10. Then, we evaluated the
Chinese/English novel text detection performance
by setting a series of novelty score thresholds.
5.3.1 POS Filtering Rule
We adopted two different rules to select the can-
didate words to represent one document/sentence
and investigated the POS filtering influence on de-
tecting the novel Chinese text.
? Rule1: only some non-meaningful words,
including pronouns (?r? in Peking Univer-
sity/Chinese Academy of Sciences Chinese
POS tagging criterions (PKU and CAS,
1999)), auxiliary words (?u?), tone words
(?y?), conjunctions (?c?), prepositions (?p?)
and punctuation words (?w?) are removed.
? Rule2: fewer kinds of words are selected to
represent a document/sentence. Only nouns
(including ?n? short for common nouns, ?nr?
short for person name, ?ns? short for location
name, ?nt? short for organization name, ?nz?
short for other proper nouns), verbs (?v?), ad-
jectives (?a?) and adverbs (?d?) are kept.
For example, here is a simple Chinese sen-
tence: ?????????? (There is a picture
on the wall). After POS filtering using Rule1,
the words we keep are: ??(?n?),?(?v?),?(?v?),
?(?m?), ?(?q?), ?(?n?)?. After POS filtering
using Rule2, the remaining words are: ??(?n?),
?(?v?), ?(?v?), ?(?n?)?. It is noticed that by
using Rule2, we can remove more non-important
words.
Figure 4 and Figure 5 show the performances
on the document and sentence-level novelty min-
ing when choosing the stricter rule (Rule2) and
the less strict rule (Rule1) in POS filtering. The
grey dashed lines show contours at intervals of 0.1
points of F Score.
From Figure 4 and Figure 5, we learn that the
Chinese novelty mining performance varies when
choosing the stricter rule (Rule2) and the less strict
rule (Rule1) in POS filtering. We can obtain a
better performance when choosing a stricter rule
(Rule2). Therefore, it is necessary to perform POS
filtering in the preprocessing steps on Chinese and
just removing some non-meaningful words (like
stop words) may not be enough. POS filtering
can help to remove the less meaningful words so
that each vector is represented better. Compared to
choosing more kinds of words (Rule1), only keep-
ing nouns, verbs, adjectives and adverbs (Rule2)
will be a better choice for novelty mining. We
also noticed that the selection of words to repre-
sent Chinese text is of vital importance to the suc-
cess of Chinese novelty mining.
5.3.2 Comparison with English
We compared the novelty mining performance
on the English and Chinese documents/sentences
datasets. For Chinese, we chose Rule2 to select
the candidate words. Figure 6 and Figure 7 show
the R-PR and PR curves of document/sentence-
level novelty mining in English and Chinese when
given a series of novelty score thresholds.
From Figure 6 and Figure 7, we observe that
the performance on detecting novel Chinese docu-
ments is slightly lower than that on English. This
may be due to the different linguistical characteris-
1566
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Redundancy?Recall
Re
dun
dan
cy?
Pre
cis
ion
Document?Level Novelty Mining on APWSJ
 
 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
RF score 
0.1 
Chinese D_NM_Rule2
Chinese D_NM_Rule1
0.25
0.35
0.45 0.55
0.15
0.25
0.2 0.35
0.45 0.55
0.2
Threshold=0.15
Threshold=0.05
Figure 4: R-PR curves for document-level novelty mining on Chinese when choosing different rules on
APWSJ. The grey dashed lines show contours at intervals of 0.1 points of RF .
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.45
0.5
0.55
0.6
0.65
0.7
Recall
Pre
cis
ion
Sentence?Level Novelty Mining on TREC 2004
 
 
F score 
0.8 
0.7 
0.6 
0.5 0.4 0.3 0.2 
Chinese S_NM_Rule2
Chinese S_NM_Rule1
0.65
0.55
Threshold=0.95
Threshold=0.95
0.85
0.85
0.75
0.75 0.65
0.55
Figure 5: PR curves for sentence-level novelty mining on Chinese when choosing different rules on
TREC 2004. The grey dashed lines show contours at intervals of 0.1 points of F .
1567
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Redundancy?Recall
Re
dun
dan
cy?
Pre
cis
ion
 
 RF score 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
Document?Level Novelty Mining on APWSJ
Chinese D_NM_Rule2
English D_NM
Threshold
=0.05 0.15
0.25
0.35
0.45 0.65
0.55
Figure 6: R-PR curves for document-level novelty mining on Chinese and English on APWSJ. The grey
dashed lines show contours at intervals of 0.1 points of RF .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cis
ion
 
 F score 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
Sentence?Level Novelty Mining on TREC 2004
Chinese S_NM_Rule2
E gli h S_NM
Threshold
=0.95 0.85 0.75
0.550.65
Figure 7: PR curves for sentence-level novelty mining on Chinese and English on TREC 2004. The grey
dashed lines show contours at intervals of 0.1 points of F .
1568
tics of each language so that the preprocessing in-
fluence on each language?s novelty mining is dis-
similar. Furthermore, the Chinese preprocessing
quality is not as good as that on English so that it
is difficult to obtain a good ?bag of words? from
a document. Moreover, the errors in word seg-
mentation will influence the result of POS tagging.
These issues make tokenizing and POS tagging
extremely difficult for the Chinese text.
However, the performance of Chinese sentence-
level novelty mining is almost the same as that on
English. The reason is that the novelty mining per-
formance at the sentence level is not so sensitive
to the preprocessing steps as that at the document
level. If the similarity computation is based on the
sentence level, the word segmentation and POS
tagging errors actually will not have a big influ-
ence on the result as that on documents.
6 Conclusion
This paper studied the preprocessing issues on
mining novel Chinese text, which, to the best
of our knowledge, have not been sufficiently
addressed in previous studies. We described
the Chinese preprocessing steps and discussed
the influence when choosing different Part-of-
Speech (POS) filtering rules. Then we applied
novelty mining on Chinese and English docu-
ments/sentences and compared their performance.
The experimental results on APWSJ and TREC
2004 Novelty Track showed that after adopting
a stricter POS filtering rule, the Chinese nov-
elty mining performed better on both documents
and sentences. This is because non-meaningful
words have a negative influence on detecting novel
text. However, compared to English, Chinese per-
formed worse on the document level and similarly
on the sentence level. The reason may be due to
the lower sensitivity of preprocessing at the sen-
tence level. The main contributions of this work
are as follows:
1) We investigated the preprocessing techniques
for detecting novel Chinese text on both doc-
ument and sentence level.
2) The POS filtering rule, telling how to select
words to represent one document/sentence,
was discussed.
3) Several experiments were conducted to com-
pare the novelty mining performance be-
tween Chinese and English. The novelty
mining performance on Chinese can be im-
proved as good as that on English if we can
increase the preprocessing precision on Chi-
nese text.
Our findings will be very helpful for develop-
ing a real-time Chinese novelty mining system at
both the document and sentence level. In future
work, we will try other word combinations and in-
vestigate better ways to represent the Chinese text.
In addition, we will explore how to utilize the bet-
ter Chinese sentence-level novelty mining result to
improve the detection performance on documents.
1569
References
James Allan, Ron Papka, and Victor Lavrenko. 1998. On-
line new event detection and tracking. In SIGIR 1998,
Melbourne, Australia, pages 37?45.
James Allan, Courtney Wade, and Alvaro Bolivar. 2003. Re-
trieval and novelty detection at the sentence level. In SI-
GIR 2003, Toronto, Canada, pages 314?321. ACM, Au-
gust.
Thorsten Brants, Francine Chen, and Ayman Farahat. 2003.
A system for new event detection. In SIGIR 2003,
Toronto, Canada, pages 330?337.
Yun Chen, Flora S. Tsai, and Kap Luk Chan. 2008. Machine
learning techniques for business blog search and mining.
Expert Syst. Appl., 35(3):581?590.
D. Eichmann and P. Srinivasan. 2002. Novel results and
some answers. In TREC 2002 - the 11th Text REtrieval
Conference.
Martin Franz, Abraham Ittycheriah, J.Scott McCarley, and
Todd Ward. 2001. First story detection: combining simi-
larity and novelty based approach. In Topic Detection and
Tracking Workshop.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.
2005. Chinese word segmentation and named entity
recognition: A pragmatic approach. Computational Lin-
guistics, 31(4):531?574, December.
D. Harman. 2002. Overview of the TREC 2002 Novelty
Track. In TREC 2002 - the 11th Text Retrieval Confer-
ence, pages 46?55.
Yu Hong, Yu Zhang, Jili Fan, Ting Liu, and Sheng Li. 2008.
Chinese topic link detection based on semantic domain
language model. Journal of Software, 19(9):2265?2275.
ICTCLAS. 2008. http://ictclas.org/index.html.
Agus Trisnajaya Kwee, Flora S Tsai, and Wenyin Tang.
2009. Sentence-level novelty detection in English and
Malay. In Lecture Notes in Computer Science (LNCS),
volume 5476, pages 40?51.
Xiaoyong Li and W. Bruce Croft. 2005. Novelty detection
based on sentence level patterns. In CIKM 2005, pages
744?751.
Xiaoyong Li and W. Bruce Croft. 2008. An information-
pattern-based approach to novelty detection. Information
Processing and Management: an International Journal,
44(3):1159?1188, May.
Robert M. Losee. 2001. Natural language processing in sup-
port of decision making: Phrases and part-of-speech tag-
ging. Information Processing and Management: an Inter-
national Journal, 37(6).
Kok Wah Ng, Flora S. Tsai, Kiat Chong Goh, and Lihui Chen.
2007. Novelty detection for text documents using named
entity recognition. In Information, Communications and
Signal Processing, 2007 6th International Conference on,
pages 1?5, December.
PKU and CAS. 1999. Chinese POS tagging criterion.
http://icl.pku.edu.cn/icl groups/corpus/addition.htm.
M.F. Porter. 1997. An algorithm for suffix stripping. Read-
ings in information retrieval, pages 313?316.
Ian Soboroff and D. Harman. 2003. Overview of the TREC
2003 Novelty Track. In TREC 2003 - the 12th Text Re-
trieval Conference.
Ian Soboroff. 2004. Overview of the TREC 2004 Novelty
Track. In TREC 2004 - the 13th Text Retrieval Confer-
ence.
N. Stokes and J. Carthy. 2001. First story detection using a
composite document representation. In HLT 2001, pages
134?141.
Wenyin Tang and Flora S Tsai. 2009. Threshold setting and
performance monitoring for novel text mining. In SIAM
International Conference on Data Mining Workshop on
Text Mining.
Wenyin Tang, Agus Trisnajaya Kwee, and Flora S Tsai.
2009. Accessing contextual information for interactive
novelty detection. In European Conference on Informa-
tion Retrieval (ECIR) Workshop on Contextual Informa-
tion Access, Seeking and Retrieval Evaluation.
Flora S. Tsai, Wenchou Han, Junwei Xu, and Hock Chuan
Chua. 2009. Design and development of a mobile peer-
to-peer social networking application. Expert Syst. Appl.,
36(8):11077 ? 11087.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006.
A fast, accurate deterministic parser for Chinese. In ACL
2006, Sydney, Australia, pages 425 ? 432.
Youzheng Wu, Jun Zhao, and Bo Xu. 2003. Chinese named
entity recognition combining a statistical model with hu-
man knowledge. In ACL 2003 workshop on Multilingual
and mixed-language named entity recognition, pages 65?
72.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998. A
study on retrospective and on-line event detection. pages
28?36. ACM Press.
Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun Jin.
2002. Topic-conditioned novelty detection. In SIGKDD
2002, pages 688 ? 693.
Huaping Zhang and Qun Liu. 2002. Model of Chinese words
rough segmentation based on n-shortest paths method.
Journal of Chinese Information Processing, 15:1?7.
Yi Zhang and Flora S. Tsai. 2009. Combining named enti-
ties and tags for novel sentence detection. In ESAIR ?09:
Proceedings of the WSDM ?09 Workshop on Exploiting
Semantic Annotations in Information Retrieval, pages 30?
34.
Yi Zhang, Jamie Callan, and Thomas Minka. 2002. Novelty
and redundancy detection in adaptive filtering. In ACM
SIGIR 2002, Tampere, Finland, pages 81?88.
Le Zhao, Min Zheng, and Shaoping Ma. 2006. The nature of
novelty detection. Information Retrieval, 9:527?541.
Wei Zheng, Yu Zhang, Bowei Zou, Yu Hong, and Ting Liu.
2008. Research of Chinese topic tracking based on rele-
vance model.
1570
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 244?252,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Non-negative Matrix Tri-factorization Approach to
Sentiment Classification with Lexical Prior Knowledge
Tao Li Yi Zhang
School of Computer Science
Florida International University
{taoli,yzhan004}@cs.fiu.edu
Vikas Sindhwani
Mathematical Sciences
IBM T.J. Watson Research Center
vsindhw@us.ibm.com
Abstract
Sentiment classification refers to the task
of automatically identifying whether a
given piece of text expresses positive or
negative opinion towards a subject at hand.
The proliferation of user-generated web
content such as blogs, discussion forums
and online review sites has made it possi-
ble to perform large-scale mining of pub-
lic opinion. Sentiment modeling is thus
becoming a critical component of market
intelligence and social media technologies
that aim to tap into the collective wis-
dom of crowds. In this paper, we consider
the problem of learning high-quality senti-
ment models with minimal manual super-
vision. We propose a novel approach to
learn from lexical prior knowledge in the
form of domain-independent sentiment-
laden terms, in conjunction with domain-
dependent unlabeled data and a few la-
beled documents. Our model is based on a
constrained non-negative tri-factorization
of the term-document matrix which can
be implemented using simple update rules.
Extensive experimental studies demon-
strate the effectiveness of our approach on
a variety of real-world sentiment predic-
tion tasks.
1 Introduction
Web 2.0 platforms such as blogs, discussion fo-
rums and other such social media have now given
a public voice to every consumer. Recent sur-
veys have estimated that a massive number of in-
ternet users turn to such forums to collect rec-
ommendations for products and services, guid-
ing their own choices and decisions by the opin-
ions that other consumers have publically ex-
pressed. Gleaning insights by monitoring and an-
alyzing large amounts of such user-generated data
is thus becoming a key competitive differentia-
tor for many companies. While tracking brand
perceptions in traditional media is hardly a new
challenge, handling the unprecedented scale of
unstructured user-generated web content requires
new methodologies. These methodologies are
likely to be rooted in natural language processing
and machine learning techniques.
Automatically classifying the sentiment ex-
pressed in a blog around selected topics of interest
is a canonical machine learning task in this dis-
cussion. A standard approach would be to manu-
ally label documents with their sentiment orienta-
tion and then apply off-the-shelf text classification
techniques. However, sentiment is often conveyed
with subtle linguistic mechanisms such as the use
of sarcasm and highly domain-specific contextual
cues. This makes manual annotation of sentiment
time consuming and error-prone, presenting a bot-
tleneck in learning high quality models. Moreover,
products and services of current focus, and asso-
ciated community of bloggers with their idiosyn-
cratic expressions, may rapidly evolve over time
causing models to potentially lose performance
and become stale. This motivates the problem of
learning robust sentiment models from minimal
supervision.
In their seminal work, (Pang et al, 2002)
demonstrated that supervised learning signifi-
cantly outperformed a competing body of work
where hand-crafted dictionaries are used to assign
sentiment labels based on relative frequencies of
positive and negative terms. As observed by (Ng et
al., 2006), most semi-automated dictionary-based
approaches yield unsatisfactory lexicons, with ei-
ther high coverage and low precision or vice versa.
However, the treatment of such dictionaries as
forms of prior knowledge that can be incorporated
in machine learning models is a relatively less ex-
plored topic; even lesser so in conjunction with
semi-supervised models that attempt to utilize un-
244
labeled data. This is the focus of the current paper.
Our models are based on a constrained non-
negative tri-factorization of the term-document
matrix, which can be implemented using simple
update rules. Treated as a set of labeled features,
the sentiment lexicon is incorporated as one set of
constraints that enforce domain-independent prior
knowledge. A second set of constraints introduce
domain-specific supervision via a few document
labels. Together these constraints enable learning
from partial supervision along both dimensions of
the term-document matrix, in what may be viewed
more broadly as a framework for incorporating
dual-supervision in matrix factorization models.
We provide empirical comparisons with several
competing methodologies on four, very different
domains ? blogs discussing enterprise software
products, political blogs discussing US presiden-
tial candidates, amazon.com product reviews and
IMDB movie reviews. Results demonstrate the ef-
fectiveness and generality of our approach.
The rest of the paper is organized as follows.
We begin by discussing related work in Section 2.
Section 3 gives a quick background on Non-
negative Matrix Tri-factorization models. In Sec-
tion 4, we present a constrained model and compu-
tational algorithm for incorporating lexical knowl-
edge in sentiment analysis. In Section 5, we en-
hance this model by introducing document labels
as additional constraints. Section 6 presents an
empirical study on four datasets. Finally, Section 7
concludes this paper.
2 Related Work
We point the reader to a recent book (Pang and
Lee, 2008) for an in-depth survey of literature on
sentiment analysis. In this section, we briskly
cover related work to position our contributions
appropriately in the sentiment analysis and ma-
chine learning literature.
Methods focussing on the use and generation of
dictionaries capturing the sentiment of words have
ranged from manual approaches of developing
domain-dependent lexicons (Das and Chen, 2001)
to semi-automated approaches (Hu and Liu, 2004;
Zhuang et al, 2006; Kim and Hovy, 2004), and
even an almost fully automated approach (Turney,
2002). Most semi-automated approaches have met
with limited success (Ng et al, 2006) and super-
vised learning models have tended to outperform
dictionary-based classification schemes (Pang et
al., 2002). A two-tier scheme (Pang and Lee,
2004) where sentences are first classified as sub-
jective versus objective, and then applying the sen-
timent classifier on only the subjective sentences
further improves performance. Results in these
papers also suggest that using more sophisticated
linguistic models, incorporating parts-of-speech
and n-gram language models, do not improve over
the simple unigram bag-of-words representation.
In keeping with these findings, we also adopt a
unigram text model. A subjectivity classification
phase before our models are applied may further
improve the results reported in this paper, but our
focus is on driving the polarity prediction stage
with minimal manual effort.
In this regard, our model brings two inter-
related but distinct themes from machine learning
to bear on this problem: semi-supervised learn-
ing and learning from labeled features. The goal
of the former theme is to learn from few labeled
examples by making use of unlabeled data, while
the goal of the latter theme is to utilize weak
prior knowledge about term-class affinities (e.g.,
the term ?awful? indicates negative sentiment and
therefore may be considered as a negatively la-
beled feature). Empirical results in this paper
demonstrate that simultaneously attempting both
these goals in a single model leads to improve-
ments over models that focus on a single goal.
(Goldberg and Zhu, 2006) adapt semi-supervised
graph-based methods for sentiment analysis but
do not incorporate lexical prior knowledge in the
form of labeled features. Most work in machine
learning literature on utilizing labeled features has
focused on using them to generate weakly labeled
examples that are then used for standard super-
vised learning: (Schapire et al, 2002) propose one
such framework for boosting logistic regression;
(Wu and Srihari, 2004) build a modified SVM
and (Liu et al, 2004) use a combination of clus-
tering and EM based methods to instantiate simi-
lar frameworks. By contrast, we incorporate lex-
ical knowledge directly as constraints on our ma-
trix factorization model. In recent work, Druck et
al. (Druck et al, 2008) constrain the predictions of
a multinomial logistic regression model on unla-
beled instances in a Generalized Expectation for-
mulation for learning from labeled features. Un-
like their approach which uses only unlabeled in-
stances, our method uses both labeled and unla-
beled documents in conjunction with labeled and
245
unlabeled words.
The matrix tri-factorization models explored in
this paper are closely related to the models pro-
posed recently in (Li et al, 2008; Sindhwani et al,
2008). Though, their techniques for proving algo-
rithm convergence and correctness can be readily
adapted for our models, (Li et al, 2008) do not
incorporate dual supervision as we do. On the
other hand, while (Sindhwani et al, 2008) do in-
corporate dual supervision in a non-linear kernel-
based setting, they do not enforce non-negativity
or orthogonality ? aspects of matrix factorization
models that have shown benefits in prior empirical
studies, see e.g., (Ding et al, 2006).
We also note the very recent work of (Sind-
hwani and Melville, 2008) which proposes a dual-
supervision model for semi-supervised sentiment
analysis. In this model, bipartite graph regulariza-
tion is used to diffuse label information along both
sides of the term-document matrix. Conceptually,
their model implements a co-clustering assump-
tion closely related to Singular Value Decomposi-
tion (see also (Dhillon, 2001; Zha et al, 2001) for
more on this perspective) while our model is based
on Non-negative Matrix Factorization. In another
recent paper (Sandler et al, 2008), standard regu-
larization models are constrained using graphs of
word co-occurences. These are very recently pro-
posed competing methodologies, and we have not
been able to address empirical comparisons with
them in this paper.
Finally, recent efforts have also looked at trans-
fer learning mechanisms for sentiment analysis,
e.g., see (Blitzer et al, 2007). While our focus
is on single-domain learning in this paper, we note
that cross-domain variants of our model can also
be orthogonally developed.
3 Background
3.1 Basic Matrix Factorization Model
Our proposed models are based on non-negative
matrix Tri-factorization (Ding et al, 2006). In
these models, an m? n term-document matrix X
is approximated by three factors that specify soft
membership of terms and documents in one of k-
classes:
X ? FSGT . (1)
where F is an m? k non-negative matrix repre-
senting knowledge in the word space, i.e., i-th row
of F represents the posterior probability of word
i belonging to the k classes, G is an n? k non-
negative matrix representing knowledge in docu-
ment space, i.e., the i-th row of G represents the
posterior probability of document i belonging to
the k classes, and S is an k? k nonnegative matrix
providing a condensed view of X .
The matrix factorization model is similar to
the probabilistic latent semantic indexing (PLSI)
model (Hofmann, 1999). In PLSI, X is treated
as the joint distribution between words and doc-
uments by the scaling X ? X? = X/?i j Xi j thus
?i j X?i j = 1). X? is factorized as
X? ?WSDT ,?
k
Wik = 1,?
k
D jk = 1,?
k
Skk = 1.
(2)
where X is the m ? n word-document seman-
tic matrix, X = WSD, W is the word class-
conditional probability, and D is the document
class-conditional probability and S is the class
probability distribution.
PLSI provides a simultaneous solution for the
word and document class conditional distribu-
tion. Our model provides simultaneous solution
for clustering the rows and the columns of X . To
avoid ambiguity, the orthogonality conditions
FT F = I, GT G = I. (3)
can be imposed to enforce each row of F and G
to possess only one nonzero entry. Approximating
the term-document matrix with a tri-factorization
while imposing non-negativity and orthogonal-
ity constraints gives a principled framework for
simultaneously clustering the rows (words) and
columns (documents) of X . In the context of co-
clustering, these models return excellent empiri-
cal performance, see e.g., (Ding et al, 2006). Our
goal now is to bias these models with constraints
incorporating (a) labels of features (coming from
a domain-independent sentiment lexicon), and (b)
labels of documents for the purposes of domain-
specific adaptation. These enhancements are ad-
dressed in Sections 4 and 5 respectively.
4 Incorporating Lexical Knowledge
We used a sentiment lexicon generated by the
IBM India Research Labs that was developed for
other text mining applications (Ramakrishnan et
al., 2003). It contains 2,968 words that have been
human-labeled as expressing positive or negative
sentiment. In total, there are 1,267 positive (e.g.
?great?) and 1,701 negative (e.g., ?bad?) unique
246
terms after stemming. We eliminated terms that
were ambiguous and dependent on context, such
as ?dear? and ?fine?. It should be noted, that this
list was constructed without a specific domain in
mind; which is further motivation for using train-
ing examples and unlabeled data to learn domain
specific connotations.
Lexical knowledge in the form of the polarity
of terms in this lexicon can be introduced in the
matrix factorization model. By partially specify-
ing term polarities via F , the lexicon influences
the sentiment predictions G over documents.
4.1 Representing Knowledge in Word Space
Let F0 represent prior knowledge about sentiment-laden words in the lexicon, i.e., if word i is a
positive word (F0)i1 = 1 while if it is negative
(F0)i2 = 1. Note that one may also use soft sen-timent polarities though our experiments are con-
ducted with hard assignments. This information
is incorporated in the tri-factorization model via a
squared loss term,
min
F,G,S
?X ?FSGT?2 +?Tr[(F?F0)TC1(F?F0)]
(4)
where the notation Tr(A) means trace of the matrix
A. Here, ? > 0 is a parameter which determines
the extent to which we enforce F ? F0, C1 is a m?
m diagonal matrix whose entry (C1)ii = 1 if thecategory of the i-th word is known (i.e., specified
by the i-th row of F0) and (C1)ii = 0 otherwise.The squared loss terms ensure that the solution for
F in the otherwise unsupervised learning problem
be close to the prior knowledge F0. Note that if
C1 = I, then we know the class orientation of allthe words and thus have a full specification of F0,Eq.(4) is then reduced to
min
F,G,S
?X?FSGT?2 +??F?F0?2 (5)
The above model is generic and it allows certain
flexibility. For example, in some cases, our prior
knowledge on F0 is not very accurate and we usesmaller ? so that the final results are not depen-
dent on F0 very much, i.e., the results are mostlyunsupervised learning results. In addition, the in-
troduction of C1 allows us to incorporate partialknowledge on word polarity information.
4.2 Computational Algorithm
The optimization problem in Eq.( 4) can be solved
using the following update rules
G jk? G jk
(XT FS) jk
(GGT XT FS) jk
, (6)
Sik ? Sik
(FT XG)ik
(FT FSGT G)ik
. (7)
Fik? Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
. (8)
The algorithm consists of an iterative procedure
using the above three rules until convergence. We
call this approach Matrix Factorization with Lex-
ical Knowledge (MFLK) and outline the precise
steps in the table below.
Algorithm 1 Matrix Factorization with Lexical
Knowledge (MFLK)
begin
1. Initialization:
Initialize F = F0
G to K-means clustering results,
S = (FT F)?1FT XG(GT G)?1.
2. Iteration:
Update G: fixing F,S, updating G
Update F: fixing S,G, updating F
Update S: fixing F,G, updating S
end
4.3 Algorithm Correctness and Convergence
Updating F,G,S using the rules above leads to an
asymptotic convergence to a local minima. This
can be proved using arguments similar to (Ding
et al, 2006). We outline the proof of correctness
for updating F since the squared loss term that in-
volves F is a new component in our models.
Theorem 1 The above iterative algorithm con-
verges.
Theorem 2 At convergence, the solution satisfies
the Karuch, Kuhn, Tucker optimality condition,
i.e., the algorithm converges correctly to a local
optima.
Theorem 1 can be proved using the standard
auxiliary function approach used in (Lee and Se-
ung, 2001).
Proof of Theorem 2. Following the theory of con-
strained optimization (Nocedal and Wright, 1999),
247
we minimize the following function
L(F)= ||X?FSGT ||2 +?Tr[(F?F0)TC1(F?F0)]
Note that the gradient of L is,
?L
?F =?2XGS
T +2FSGT GST +2?C1(F?F0).
(9)
The KKT complementarity condition for the non-
negativity of Fik gives
[?2XGST +FSGT GST +2?C1(F?F0)]ikFik = 0.(10)
This is the fixed point relation that local minima
for F must satisfy. Given an initial guess of F , the
successive update of F using Eq.(8) will converge
to a local minima. At convergence, we have
Fik = Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
.
which is equivalent to the KKT condition of
Eq.(10). The correctness of updating rules for G in
Eq.(6) and S in Eq.(7) have been proved in (Ding
et al, 2006). u?Note that we do not enforce exact orthogonality
in our updating rules since this often implies softer
class assignments.
5 Semi-Supervised Learning With
Lexical Knowledge
So far our models have made no demands on hu-
man effort, other than unsupervised collection of
the term-document matrix and a one-time effort in
compiling a domain-independent sentiment lexi-
con. We now assume that a few documents are
manually labeled for the purposes of capturing
some domain-specific connotations leading to a
more domain-adapted model. The partial labels
on documents can be described using G0 where
(G0)i1 = 1 if the document expresses positive sen-timent, and (G0)i2 = 1 for negative sentiment. Aswith F0, one can also use soft sentiment labelingfor documents, though our experiments are con-
ducted with hard assignments.
Therefore, the semi-supervised learning with
lexical knowledge can be described as
min
F,G,S
?X?FSGT?2 +?Tr[(F?F0)TC1(F?F0)]+
?Tr[(G?G0)TC2(G?G0)]
Where ? > 0,? > 0 are parameters which deter-
mine the extent to which we enforce F ? F0 and
G ? G0 respectively, C1 and C2 are diagonal ma-trices indicating the entries of F0 and G0 that cor-respond to labeled entities. The squared loss terms
ensure that the solution for F,G, in the otherwise
unsupervised learning problem, be close to the
prior knowledge F0 and G0.
5.1 Computational Algorithm
The optimization problem in Eq.( 4) can be solved
using the following update rules
G jk? G jk
(XT FS+?C2G0) jk
(GGT XT FS+?GGTC2G0) jk (11)
Sik ? Sik
(FT XG)ik
(FT FSGT G)ik
. (12)
Fik? Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
. (13)
Thus the algorithm for semi-supervised learning
with lexical knowledge based on our matrix fac-
torization framework, referred as SSMFLK, con-
sists of an iterative procedure using the above three
rules until convergence. The correctness and con-
vergence of the algorithm can also be proved using
similar arguments as what we outlined earlier for
MFLK in Section 4.3.
A quick word about computational complexity.
The term-document matrix is typically very sparse
with z nm non-zero entries while k is typically
also much smaller than n,m. By using sparse ma-
trix multiplications and avoiding dense intermedi-
ate matrices, the updates can be very efficiently
and easily implemented. In particular, updating
F,S,G each takes O(k2(m + n) + kz) time per it-
eration which scales linearly with the dimensions
and density of the data matrix. Empirically, the
number of iterations before practical convergence
is usually very small (less than 100). Thus, com-
putationally our approach scales to large datasets
even though our experiments are run on relatively
small-sized datasets.
6 Experiments
6.1 Datasets Description
Four different datasets are used in our experi-
ments.
Movies Reviews: This is a popular dataset in
sentiment analysis literature (Pang et al, 2002).
It consists of 1000 positive and 1000 negative
movie reviews drawn from the IMDB archive of
the rec.arts.movies.reviews newsgroups.
248
Lotus blogs: The data set is targeted at detect-
ing sentiment around enterprise software, specif-
ically pertaining to the IBM Lotus brand (Sind-
hwani and Melville, 2008). An unlabeled set
of blog posts was created by randomly sampling
2000 posts from a universe of 14,258 blogs that
discuss issues relevant to Lotus software. In ad-
dition to this unlabeled set, 145 posts were cho-
sen for manual labeling. These posts came from
14 individual blogs, 4 of which are actively post-
ing negative content on the brand, with the rest
tending to write more positive or neutral posts.
The data was collected by downloading the lat-
est posts from each blogger?s RSS feeds, or ac-
cessing the blog?s archives. Manual labeling re-
sulted in 34 positive and 111 negative examples.
Political candidate blogs: For our second blog
domain, we used data gathered from 16,742 polit-
ical blogs, which contain over 500,000 posts. As
with the Lotus dataset, an unlabeled set was cre-
ated by randomly sampling 2000 posts. 107 posts
were chosen for labeling. A post was labeled as
having positive or negative sentiment about a spe-
cific candidate (Barack Obama or Hillary Clinton)
if it explicitly mentioned the candidate in posi-
tive or negative terms. This resulted in 49 posi-
tively and 58 negatively labeled posts. Amazon
Reviews: The dataset contains product reviews
taken from Amazon.com from 4 product types:
Kitchen, Books, DVDs, and Electronics (Blitzer
et al, 2007). The dataset contains about 4000 pos-
itive reviews and 4000 negative reviews and can
be obtained from http://www.cis.upenn.
edu/?mdredze/datasets/sentiment/.
For all datasets, we picked 5000 words with
highest document-frequency to generate the vo-
cabulary. Stopwords were removed and a nor-
malized term-frequency representation was used.
Genuinely unlabeled posts for Political and Lo-
tus were used for semi-supervised learning experi-
ments in section 6.3; they were not used in section
6.2 on the effect of lexical prior knowledge. In the
experiments, we set ?, the parameter determining
the extent to which to enforce the feature labels,
to be 1/2, and ?, the corresponding parameter for
enforcing document labels, to be 1.
6.2 Sentiment Analysis with Lexical
Knowledge
Of course, one can remove all burden on hu-
man effort by simply using unsupervised tech-
niques. Our interest in the first set of experi-
ments is to explore the benefits of incorporating a
sentiment lexicon over unsupervised approaches.
Does a one-time effort in compiling a domain-
independent dictionary and using it for different
sentiment tasks pay off in comparison to simply
using unsupervised methods? In our case, matrix
tri-factorization and other co-clustering methods
form the obvious unsupervised baseline for com-
parison and so we start by comparing our method
(MFLK) with the following methods:
? Four document clustering methods: K-
means, Tri-Factor Nonnegative Ma-
trix Factorization (TNMF) (Ding et al,
2006), Information-Theoretic Co-clustering
(ITCC) (Dhillon et al, 2003), and Euclidean
Co-clustering algorithm (ECC) (Cho et al,
2004). These methods do not make use of
the sentiment lexicon.
? Feature Centroid (FC): This is a simple
dictionary-based baseline method. Recall
that each word can be expressed as a ?bag-
of-documents? vector. In this approach, we
compute the centroids of these vectors, one
corresponding to positive words and another
corresponding to negative words. This yields
a two-dimensional representation for docu-
ments, on which we then perform K-means
clustering.
Performance Comparison Figure 1 shows the
experimental results on four datasets using accu-
racy as the performance measure. The results are
obtained by averaging 20 runs. It can be observed
that our MFLK method can effectively utilize the
lexical knowledge to improve the quality of senti-
ment prediction.
Movies Lotus Political Amazon
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
ECC
ITCC
K?Means
Figure 1: Accuracy results on four datasets
249
Size of Sentiment Lexicon We also investigate
the effects of the size of the sentiment lexicon on
the performance of our model. Figure 2 shows
results with random subsets of the lexicon of in-
creasing size. We observe that generally the per-
formance increases as more and more lexical su-
pervision is provided.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Fraction of sentiment words labeled
Ac
cu
ra
cy
 
 
Movies
Lotus
Political
Amazon
Figure 2: MFLK accuracy as size of sentiment
lexicon (i.e., number of words in the lexicon) in-
creases on the four datasets
Robustness to Vocabulary Size High dimen-
sionality and noise can have profound impact on
the comparative performance of clustering and
semi-supervised learning algorithms. We simu-
late scenarios with different vocabulary sizes by
selecting words based on information gain. It
should, however, be kept in mind that in a tru-
ely unsupervised setting document labels are un-
available and therefore information gain cannot
be practically computed. Figure 3 and Figure 4
show results for Lotus and Amazon datasets re-
spectively and are representative of performance
on other datasets. MLFK tends to retain its po-
sition as the best performing method even at dif-
ferent vocabulary sizes. ITCC performance is also
noteworthy given that it is a completely unsuper-
vised method.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Fraction of Original Vocabulary 
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
K?Means
ITCC
ECC
Figure 3: Accuracy results on Lotus dataset with
increasing vocabulary size
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
Fraction of Original Vocabulary
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
K?Means
ITCC
ECC
Figure 4: Accuracy results on Amazon dataset
with increasing vocabulary size
6.3 Sentiment Analysis with Dual
Supervision
We now assume that together with labeled features
from the sentiment lexicon, we also have access to
a few labeled documents. The natural question is
whether the presence of lexical constraints leads
to better semi-supervised models. In this section,
we compare our method (SSMFLK) with the fol-
lowing three semi-supervised approaches: (1) The
algorithm proposed in (Zhou et al, 2003) which
conducts semi-supervised learning with local and
global consistency (Consistency Method); (2) Zhu
et al?s harmonic Gaussian field method coupled
with the Class Mass Normalization (Harmonic-
CMN) (Zhu et al, 2003); and (3) Green?s function
learning algorithm (Green?s Function) proposed
in (Ding et al, 2007).
We also compare the results of SSMFLK with
those of two supervised classification methods:
Support Vector Machine (SVM) and Naive Bayes.
Both of these methods have been widely used in
sentiment analysis. In particular, the use of SVMs
in (Pang et al, 2002) initially sparked interest
in using machine learning methods for sentiment
classification. Note that none of these competing
methods utilizes lexical knowledge.
The results are presented in Figure 5, Figure 6,
Figure 7, and Figure 8. We note that our SSMFLK
method either outperforms all other methods over
the entire range of number of labeled documents
(Movies, Political), or ultimately outpaces other
methods (Lotus, Amazon) as a few document la-
bels come in.
Learning Domain-Specific Connotations In
our first set of experiments, we incorporated the
sentiment lexicon in our models and learnt the
sentiment orientation of words and documents via
F,G factors respectively. In the second set of
250
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 5: Accuracy results with increasing number
of labeled documents on Movies dataset
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function 
SVM
Naive Bayes
Figure 6: Accuracy results with increasing number
of labeled documents on Lotus dataset
experiments, we additionally introduced labeled
documents for domain-specific adjustments. Be-
tween these experiments, we can now look for
words that switch sentiment polarity. These words
are interesting because their domain-specific con-
notation differs from their lexical orientation. For
amazon reviews, the following words switched
polarity from positive to negative: fan, impor-
tant, learning, cons, fast, feature, happy, memory,
portable, simple, small, work while the following
words switched polarity from negative to positive:
address, finish, lack, mean, budget, rent, throw.
Note that words like fan, memory probably refer
to product or product components (i.e., computer
fan and memory) in the amazon review context
but have a very different connotation say in the
context of movie reviews where they probably re-
fer to movie fanfare and memorable performances.
We were surprised to see happy switch polarity!
Two examples of its negative-sentiment usage are:
I ended up buying a Samsung and I couldn?t be
more happy and BORING, not one single exciting
thing about this book. I was happy when my lunch
break ended so I could go back to work and stop
reading.
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 7: Accuracy results with increasing number
of labeled documents on Political dataset
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 8: Accuracy results with increasing number
of labeled documents on Amazon dataset
7 Conclusion
The primary contribution of this paper is to pro-
pose and benchmark new methodologies for sen-
timent analysis. Non-negative Matrix Factoriza-
tions constitute a rich body of algorithms that have
found applicability in a variety of machine learn-
ing applications: from recommender systems to
document clustering. We have shown how to build
effective sentiment models by appropriately con-
straining the factors using lexical prior knowledge
and document annotations. To more effectively
utilize unlabeled data and induce domain-specific
adaptation of our models, several extensions are
possible: facilitating learning from related do-
mains, incorporating hyperlinks between docu-
ments, incorporating synonyms or co-occurences
between words etc. As a topic of vigorous current
activity, there are several very recently proposed
competing methodologies for sentiment analysis
that we would like to benchmark against. These
are topics for future work.
Acknowledgement: The work of T. Li is par-
tially supported by NSF grants DMS-0844513 and
CCF-0830659. We would also like to thank Prem
Melville and Richard Lawrence for their support.
251
References
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-phies, bollywood, boom-boxes and blenders: Do-main adaptation for sentiment classification. In Pro-
ceedings of ACL, pages 440?447.
H. Cho, I. Dhillon, Y. Guan, and S. Sra. 2004. Mini-
mum sum squared residue co-clustering of gene ex-pression data. In Proceedings of The 4th SIAM Data
Mining Conference, pages 22?24, April.
S. Das and M. Chen. 2001. Yahoo! for amazon:Extracting market sentiment from stock messageboards. In Proceedings of the 8th Asia Pacific Fi-
nance Association (APFA).
I. S. Dhillon, S. Mallela, and D. S. Modha. 2003.Information-theoretical co-clustering. In Proceed-
ings of ACM SIGKDD, pages 89?98.
I. S. Dhillon. 2001. Co-clustering documents andwords using bipartite spectral graph partitioning. In
Proceedings of ACM SIGKDD.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogo-
nal nonnegative matrix tri-factorizations for cluster-ing. In Proceedings of ACM SIGKDD, pages 126?135.
C. Ding, R. Jin, T. Li, and H.D. Simon. 2007. Alearning framework using green?s function and ker-nel regularization with application to recommender
system. In Proceedings of ACM SIGKDD, pages260?269.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-tion criteria. In SIGIR.
A. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In
HLT-NAACL 2006: Workshop on Textgraphs.
T. Hofmann. 1999. Probabilistic latent semantic in-dexing. Proceeding of SIGIR, pages 50?57.
M. Hu and B. Liu. 2004. Mining and summarizingcustomer reviews. In KDD, pages 168?177.
S.-M. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of International
Conference on Computational Linguistics.
D.D. Lee and H.S. Seung. 2001. Algorithms for non-negative matrix factorization. In Advances in Neural
Information Processing Systems 13.
T. Li, C. Ding, Y. Zhang, and B. Shao. 2008. Knowl-edge transformation from word space to documentspace. In Proceedings of SIGIR, pages 187?194.
B. Liu, X. Li, W.S. Lee, and P. Yu. 2004. Text classifi-cation by labeling words. In AAAI.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-amining the role of linguistic knowledge sources inthe automatic identification and classification of re-
views. In COLING & ACL.
J. Nocedal and S.J. Wright. 1999. Numerical Opti-
mization. Springer-Verlag.
B. Pang and L. Lee. 2004. A sentimental education:sentiment analysis using subjectivity summarizationbased on minimum cuts. In ACL.
B. Pang and L. Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trendsin Information Retrieval: Vol. 2: No 12, pp
1-135 http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learningtechniques. In EMNLP.
G. Ramakrishnan, A. Jadhav, A. Joshi, S. Chakrabarti,and P. Bhattacharyya. 2003. Question answeringvia bayesian inference on lexical relations. In ACL,pages 1?10.
T. Sandler, J. Blitzer, P. Talukdar, and L. Ungar. 2008.Regularized learning with networks of features. In
NIPS.
R.E. Schapire, M. Rochery, M.G. Rahim, andN. Gupta. 2002. Incorporating prior knowledge into
boosting. In ICML.
V. Sindhwani and P. Melville. 2008. Document-word co-regularization for semi-supervised senti-
ment analysis. In Proceedings of IEEE ICDM.
V. Sindhwani, J. Hu, and A. Mojsilovic. 2008. Regu-larized co-clustering with dual supervision. In Pro-
ceedings of NIPS.
P. Turney. 2002. Thumbs up or thumbs down? Se-mantic orientation applied to unsupervised classifi-
cation of reviews. Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
X. Wu and R. Srihari. 2004. Incorporating priorknowledge with weighted margin support vector ma-chines. In KDD.
H. Zha, X. He, C. Ding, M. Gu, and H.D. Simon.2001. Bipartite graph partitioning and data cluster-ing. Proceedings of ACM CIKM.
D. Zhou, O. Bousquet, T.N. Lal, J. Weston, andB. Scholkopf. 2003. Learning with local and globalconsistency. In Proceedings of NIPS.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using gaussian fields and har-monic functions. In Proceedings of ICML.
L. Zhuang, F. Jing, and X. Zhu. 2006. Movie reviewmining and summarization. In CIKM, pages 43?50.
252
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1034?1043, Prague, June 2007. c?2007 Association for Computational Linguistics
Validation and Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering
Aline Villavicencio??, Valia Kordoni?, Yi Zhang?,
Marco Idiart? and Carlos Ramisch?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Department of Computer Sciences, Bath University (UK)
?Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de
idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br
Abstract
This paper focuses on the evaluation of meth-
ods for the automatic acquisition of Multiword
Expressions (MWEs) for robust grammar engi-
neering. First we investigate the hypothesis that
MWEs can be detected by the distinct statistical
properties of their component words, regardless
of their type, comparing 3 statistical measures:
mutual information (MI), ?2 and permutation
entropy (PE). Our overall conclusion is that at
least two measures, MI and PE, seem to differen-
tiate MWEs from non-MWEs. We then investi-
gate the influence of the size and quality of differ-
ent corpora, using the BNC and the Web search
engines Google and Yahoo. We conclude that, in
terms of language usage, web generated corpora
are fairly similar to more carefully built corpora,
like the BNC, indicating that the lack of con-
trol and balance of these corpora are probably
compensated by their size. Finally, we show a
qualitative evaluation of the results of automat-
ically adding extracted MWEs to existing lin-
guistic resources. We argue that such a process
improves qualitatively, if a more compositional
approach to grammar/lexicon automated exten-
sion is adopted.
1 Introduction
The task of automatically identifying Multiword
Expressions (MWEs) like phrasal verbs (break
down) and compound nouns (coffee machine)
using statistical measures has been the focus
of considerable investigative effort, (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Given the heterogeneousness of
the different phenomena that are considered to
be MWEs, there is no consensus about which
method is best suited for which type of MWE,
and if there is a single method that can be suc-
cessfully used for any kind of MWE.
Another difficulty for work on MWE identifi-
cation is that of the evaluation of the results ob-
tained (Pearce, 2002; Evert and Krenn, 2005),
starting from the lack of consensus about a pre-
cise definition for MWEs (Villavicencio et al,
2005).
In this paper we investigate some of the is-
sues involved in the evaluation of automatically
extracted MWEs, from their extraction to their
subsequent use in an NLP task. In order to do
that, we present a discussion of different statisti-
cal measures, and the influence that the size and
quality of different data sources have. We then
perform a comparison of these measures and dis-
cuss whether there is a single measure that has
good overall performance for MWEs in general,
regardless of their type. Finally, we perform a
qualitative evaluation of the results of adding
automatically extracted MWEs to a linguistic
resource, taking as basis for the evaluation the
approach proposed by Zhang et al (2006). We
argue that such results can improve in quality
if a more compositional approach to MWE en-
coding is adopted for the grammar extension.
Having more accurate means of deciding for an
appropriate method for identifying and incor-
porating MWEs is critical for maintaining the
quality of linguistic resources for precise NLP.
This paper starts with a discussion of MWEs
(? 2), of their coverage in linguistic resources
(? 3), and of some methods proposed for auto-
matically identifying them (? 4). This is fol-
lowed by a detailed investigation and compar-
ison of measures for MWE identification (? 5).
1034
After that we present an approach for predicting
appropriate lexico-syntactic categories for their
inclusion in a linguistic resource, and an evalu-
ation of the results in a parsing task(? 7). We
finish with some conclusions and discussion of
future work.
2 Multiword Expressions
The term Multiword Expressions has been used
to describe expressions for which the syntactic or
semantic properties of the whole expression can-
not be derived from its parts (Sag et al, 2002),
including a large number of related but distinct
phenomena, such as phrasal verbs (e.g. come
along), nominal compounds (e.g. frying pan),
institutionalised phrases (e.g. bread and butter),
and many others. Jackendoff (1997) estimates
the number of MWEs in a speaker?s lexicon to
be comparable to the number of single words.
However, due to their heterogeneous character-
istics,MWEs present a tough challenge for both
linguistic and computational work (Sag et al,
2002). For instance, some MWEs are fixed, and
do not present internal variation, such as ad hoc,
while others allow different degrees of internal
variability and modification, such as spill beans
(spill several/musical/mountains of beans).
Sag et al (2002) discuss two main ap-
proaches commonly employed in NLP for treat-
ing MWEs: the words-with-spaces approach
models an MWE as a single lexical entry and it
can adequately capture fixed MWEs like by and
large. A compositional approach treats MWEs
by general and compositional methods of lin-
guistic analysis, being able to capture more syn-
tactically flexible MWEs, like rock boat, which
cannot be satisfactorily captured by a words-
with-spaces approach, since it would require lex-
ical entries to be added for all the possible
variations of an MWE (e.g. rock/rocks/rocking
this/that/his... boat). Therefore, to provide a
unified account for the detection and encoding
of these distinct but related phenomena is a real
challenge for NLP systems.
3 Grammar and Lexicon Coverage in
Deep Processing
Many NLP tasks and applications, like Parsing
and Machine Translation, depend on large-scale
linguistic resources, such as electronic dictionar-
ies and grammars for precise results. Several
substantial resources exist: e.g., hand-crafted
large-scale grammars like the English Resource
Grammar (ERG - Flickinger (2000)) and the
Dutch Alpino Grammar (Bouma et al, 2001).
Unfortunately, the construction of these re-
sources is the manual result of human efforts and
therefore likely to contain errors of omission and
commission (Briscoe and Carroll, 1997). Fur-
thermore, due to the open-ended and dynamic
nature of languages, such linguistic resources are
likely to be incomplete, and manual encoding of
new entries and constructions is labour-intensive
and costly.
Take, for instance, the coverage test results
for the ERG (a broad-coverage precision HPSG
grammar for English) on the British National
Corpus (BNC). Baldwin et al (2004), among
many others, have investigated the main causes
of parse failure, parsing a random sample of
20,000 strings from the written component of
the BNC using the ERG. They have found that
the large majority of failures is caused by miss-
ing lexical entries, with 40% of the cases, and
missing constructions, with 39%, where missing
MWEs accounted for 8% of total errors. That is,
even by a margin, the lexical coverage is lower
than the grammar construction coverage.
This indicates the acute need for robust (semi-
)automated ways of acquiring lexical informa-
tion for MWEs, and this is the one of the goals
of this work. In the next section we discuss
some approaches that have been developed in re-
cent years to (semi-)automatically detect and/or
repair lexical and grammar errors in linguistic
grammars and/or extend their coverage.
4 Acquiring MWEs
The automatic acquisition of specific types of
MWE has attracted much interest (Pearce,
2002; Baldwin and Villavicencio, 2002; Evert
and Krenn, 2005; Villavicencio, 2005; van der
1035
Beek, 2005; Nicholson and Baldwin, 2006). For
instance, Baldwin and Villavicencio (2002) pro-
posed a combination of methods to extract Verb-
Particle Constructions (VPCs) from unanno-
tated corpora, that in an evaluation on the
Wall Street Journal achieved 85.9% precision
and 87.1% recall. Nicholson and Baldwin (2006)
investigated the prediction of the inherent se-
mantic relation of a given compound nominaliza-
tion using as statistical measure the confidence
interval.
On the other hand, Zhang et al (2006) looked
at MWEs in general investigating the semi-
automated detection of MWE candidates in
texts using error mining techniques and vali-
dating them using a combination of the World
Wide Web as a corpus and some statistical mea-
sures. 6248 sentences were then extracted from
the BNC; these contained at least one of the 311
MWE candidates verified with World Wide Web
in the way described in Zhang et al (2006). For
each occurrence of the MWE candidates in this
set of sentences, the lexical type predictor pro-
posed in Zhang and Kordoni (2006) predicted a
lexical entry candidate. This resulted in 373 ad-
ditional MWE lexical entries for the ERG gram-
mar using a words-with-spaces approach. As re-
ported in Zhang et al (2006), this addition to
the grammar resulted in a significant increase in
grammar coverage of 14.4%. However, no fur-
ther evaluation was done of the results of the
measures used on the identification of MWEs or
of the resulting grammar, as not all MWEs can
be correctly handled by the simple words-with-
spaces approach (Sag et al, 2002). And these
are the starting points of the work we are re-
porting on here.
5 Evaluation of the Identification of
MWEs
One way of viewing the MWE identification task
is, given a list of sequences of words, to distin-
guish those that are genuine MWEs (e.g. in the
red), from those that are just sequences of words
that do not form any kind of meaningful unit
(e.g. of alcohol and). In order to do that, one
commonly used approach is to employ statisti-
cal measures (e.g. Pearce (2002) for collocations
and Zhang et al (2006) for MWEs in general).
When dealing with statistical analysis there are
two important statistical questions that should
be addressed: How reliable is the corpus used?
and How precise is the chosen statistical measure
to distinguish the phenomena studied?.
In this section we look at these issues, for the
particular case of trigrams, by testing different
corpora and different statistical measures. For
that we use 1039 trigrams that are the output
of Zhang et al (2006) error mining system, and
frequencies collected from the BNC and from
the World Wide Web. The former were col-
lected from two different portions of the BNC,
namely the fragment of the BNC (BNCf ) used
in the error-mining experiments, and the com-
plete BNC (from the site http://pie.usna.edu/),
to test whether a larger sample of a more ho-
mogeneous and well balanced corpus improves
results significantly. For the latter we used two
different search engines: Google and Yahoo, and
the frequencies collected reflect the number of
pages that had exact matches of the n-grams
searched, using the API tools for each engine.
5.1 Comparing Corpora
A corpus for NLP related work should be a re-
liable sample of the linguistic output of a given
language. For this work in particular, we expect
that the relative ordering in frequency for differ-
ent n-grams is preserved across corpora, in the
same domain (e.g. a corpus of chemistry arti-
cles). For, if this is not the case, different con-
clusions are certain to be drawn from different
corpora.
The first test we performed was a direct com-
parison of the rank plots of the relative fre-
quency of trigrams for the four corpora. We
ranked 1039 MWE-candidate trigrams accord-
ing to their occurrence in each corpus and we
normalised this value by the total number of
times any one of the 1039 trigrams appeared
for each corpus. These normalisation values
were: 66,101 times in BNCf , 322,325 in BNC,
224,479,065 in Google and 6,081,786,313 in Ya-
hoo. It is possible to have an estimate of the size
of each corpus from these numbers: the trigrams
1036
account for something like 0.3% of the BNC cor-
pora, while for Google and Yahoo nothing can
be said since their sizes are not reliable numbers.
Figure 1 displays the results. The overall rank-
ing distribution is very similar for these corpora
showing the expected Zipf like behaviour in spite
of their different sizes.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000
re
la
tiv
e 
fre
qu
en
cy
rank
BNCf
BNC
Google
Yahoo
Figure 1: Relative frequency rank for the 1039
trigrams analysed.
Of course, the information coming from Fig-
ure 1 is not sufficient for our purposes. The or-
der of the trigrams could be very different inside
each corpus. Therefore a second test is needed
to compare the rankings of the n-grams in each
corpus. In order to do that we measure the
Kendall?s ? scores between corpora. Kendall?s ?
is a non-parametric method for estimating cor-
relation between datasets (Press et al, 1992).
For the number of trigrams studied here the
Kendall?s scores obtained imply a significant cor-
relation between the corpora with p<0.000001.
The significance indicates that the data are cor-
related and the null hypothesis of statistical
independence is certainly disproved. Unfortu-
nately disproving the null hypothesis does not
give much information about the degree of cor-
relation; it only asserts that it exists. Thus, it
could be a very insignificant correlation. In ta-
ble 1, we display a more intuitive measure to
estimate the correlation, the probability Q that
any 2 trigrams chosen from two corpora have
the same relative ordering in frequency. This
probability is related to Kendall?s ? through the
expression Q = (1 + ?)/2 .
BNC Google Yahoo
BNCf 0.81 0.73 0.78
BNC 0.73 0.77
Google 0.86
Table 1: The probability Q of 2 trigrams hav-
ing the same frequency rank order for different
corpora.
The results show that the four corpora are
certainly correlated, and can probably be used
interchangeably to access most of the statisti-
cal properties of the trigrams. Interestingly, a
higher correlation was observed between Yahoo
and Google than between BNCf and BNC, even
though BNCf is a fragment of BNC, and there-
fore would be expected to have a very high cor-
relation. This suggests that as corpora sizes
increase, so do the correlations between them,
meaning that they are more likely to agree on
the ranking of a given MWE.
5.2 Comparing statistical measures -
are they equivalent?
Here we concentrate on a single corpus, BNCf ,
and compare the three statistical measures for
MWE identification: Mutual Information (MI),
?2 and Permutation Entropy (PE)(Zhang et al,
2006), to investigate if they order the trigrams
in the same fashion.
MI and ?2 are typical measures of associa-
tion that compare the joint probability of occur-
rence of a certain group of events p(abc) with
a prediction derived from the null hypothesis
of statistical independence between these events
p?(abc) = p(a)p(b)p(c) (Press et al, 1992). In
our case the events are the occurrences of words
in a given position in an n-gram. For a trigram
with words w1w2w3, ?2 is calculated as:
?2 =
?
a,b,c
[ n(abc)? n?(abc) ]2
n?(abc)
where a corresponds either to the word w1 or to
?w1 (all but the word w1) and so on. n(abc)
is the number of trigrams abc in the corpus,
n?(abc) = n(a)n(b)n(c)/N2 is the predicted
number from the null hypothesis, n(a) is the
1037
number of unigrams a, and N the number of
words in the corpus. Mutual Information, in
terms of these numbers, is:
MI =
?
a,b,c
n(abc)
N log2
[ n(abc)
n?(abc)
]
The third measure, permutation entropy, is a
measure of order association. Given the words
w1, w2, and w3, PE is calculated in this work as:
PE = ?
?
(i,j,k)
p(wiwjwk) ln [ p(wiwjwk) ]
where the sum runs over all the permutations
of the indexes and, therefore, over all possible
positions of the selected words in the trigram.
The probabilities are estimated from the number
of occurrences of each permutation of a trigram
(e.g. by and large, large by and, and large by,
and by large, large and by, and by large and) as:
p(w1w2w3) =
n(w1w2w3)
?
(i,j,k)
n(wiwjwk)
PE was proposed by Zhang et al (2006) as a
possible measure to detect MWEs, under the
hypothesis that MWEs are more rigid to per-
mutations and therefore present smaller PEs.
Even though it is quite different from MI and
?2, PE can also be thought as an indirect mea-
sure of statistical independence, since the more
independent the words are the closer PE is from
its maximal value (ln 6, for trigrams). One pos-
sible advantage of this measure over the others
is that it does not rely on single word counts,
which are less accurate in Web based corpora.
Given the rankings produced for each one of
these three measures we again use Kendall?s ?
test to assess correlation and its significance.
Table 2 displays the Q probability of finding
the same ordering in these three measures. The
general conclusion from the table is that even
though there is statistical significance in the cor-
relations found (the p values are not displayed,
but they are very low as before) the differ-
ent measures order the trigrams very differently.
There is a 70% chance of getting the same order
from MI and ?2, but it is safe to say that these
measures are very different from the PE, since
their Q values are very close to pure chance.
MI??2 MI?PE ?2?PE
Q 0.71 0.55 0.45
Table 2: The probability Q of having 2 trigrams
with the same rank order for different statistical
measures.
5.3 Comparing Statistical Measures -
are they useful?
The use of statistical measures is widespread in
NLP but there is no consensus about how good
these measures are for describing natural lan-
guage phenomena. It is not clear what exactly
they capture when analysing the data.
In order to evaluate if they would make good
predictors for MWEs, we compare the measures
distributions for MWEs and non-MWEs. For
that we selected as gold standard a set of around
400 MWE candidates annotated by a native
speaker1 as MWEs or not. We then calculated
the histograms for the values of MI, ?2 and
PE for the two groups. MI and ?2 were cal-
culated only for BNCf . Table 3 displays the re-
sults of the Kolmogorov-Smirnof test (Press et
al., 1992) for these histograms, where the first
value is Kolmogorov-Smirnov D value (D?[0,1]
and large D values indicate large differences be-
tween distributions) and the second is the signif-
icance probability (p) associated to D given the
sizes of the data sets, in this case 90 for MWEs
and 292 for non-MWEs.
MIBNCf ?2BNCf PEY ahoo PEGoogle
D 0.27 0.13 0.27 0.24
p< 0.0001 0.154 0.0001 0.0005
Table 3: Comparison of MI, ?2 and PE
The surprising result is that there is no statis-
tical significance, at least using the Kolmogorov-
Smirnov test, that indicates that being or not
an MWE has some effect in the value of the tri-
gram?s ?2. The same does not happen for MI
or PE. They do seem to differentiate between
MWEs and non-MWEs. As discussed before the
statistical significance implies the existence of an
1The native speaker is a linguist expert in MWEs.
1038
effect but has very little to say about the inten-
sity of the effect. As in the case of this work our
interest is to use the effect to predict MWEs,
the intensity is very important. In the figures
that follow we show the normalised histograms
for MI, ?2(for the BNCf ) and PE (for the case
of Yahoo) for MWEs and non-MWEs. The ideal
scenario would be to have non overlapping dis-
tributions for the two cases, so a simple thresh-
old operation would be enough to distinguish
MWEs. This is not the case in any of the plots.
Starting from Figure 3 it clearly illustrates the
negative result for ?2 in table 3. The other two
distributions show a visible effect in the form of
a slight displacement of the distributions to the
left for MWEs. In particular for the distribution
of PE, the large peak on the right, representing
the n-grams whose word order is irrelevant with
respect to its occurrence, has an important re-
duction for MWEs.
The statistical measures discussed here are
all different forms of measuring correlations be-
tween the component words of MWEs. There-
fore, as some types of MWEs may have stronger
constraints on word order, we believe that more
visible effects can be seen in these measures if we
look at their application for individual types of
MWEs, which is planned for future work. This
will bring an improvement to the power of MWE
prediction of these measures.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
-5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
Pr
ob
ab
ilit
y
log(MI)
MWEs
non-MWEs
Figure 2: Normalised histograms of MI values
for MWEs and non-MWEs in BNCf .
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 2  3  4  5  6  7  8
Pr
ob
ab
ilit
y
log(?2)
MWEs
non-MWEs
Figure 3: Normalised histograms of ?2 values
for MWEs and non-MWEs in BNCf .
 0
 0.05
 0.1
 0.15
 0.2
 0.25
-3.5 -3 -2.5 -2 -1.5 -1 -0.5  0  0.5
Pr
ob
ab
ilit
y
log(PE(Yahoo))
MWEs
non-MWEs
Figure 4: Normalised histograms of PE values
for MWEs and non-MWEs in Yahoo.
6 Evaluation of the Extensions to
the Grammar
Our ultimate goal is to maximally automate
the process of discovering and handling MWEs.
With good statistical measures, we are able
to distinguish genuine MWE from non-MWEs
among the n-gram candidates. However, from
the perspective of grammar engineering, even
with a good candidate list of MWEs, great ef-
fort is still required in order to incorporate such
word units into a given grammar automatically
and in a precise way.
Zhang et al (2006) tried a simple ?word with
spaces? approach. By acquiring new lexical en-
tries for the MWEs candidates validated by the
statistical measures, the grammar coverage was
shown to improve significantly. However, no fur-
ther investigation on the parser accuracy was re-
ported there.
Taking a closer look at the MWE candidates
1039
proposed, we find that only a small proportion of
them can be handled appropriately by the?word
with spaces? approach of Zhang et al (2006).
Simply adding new lexical entries for all MWEs
can be a workaround for enhancing the parser
coverage, but the quality of the parser output is
clearly linguistically less interesting.
On the other hand, we also find that a large
proportion of MWEs that cannot be correctly
handled by the grammar can be covered prop-
erly in a constructional way by adding one lex-
ical entry for the head (governing) word of the
MWE. For example, the expression foot the bill
will be correctly handled with a standard head-
complement rule, if there is a transitive verb
reading for the word foot in the lexicon. Some
other examples are: to put forward, the good of,
in combination with, . . . , where lexical exten-
sion to the words in bold will allow the gram-
mar to cover these MWEs. In this paper, we
employ a constructional approach for the acqui-
sition of new lexical entries for the head words
of the MWEs.2
It is arguable that such an approach may lead
to some potential grammar overgeneration, as
there is no selectional restriction expressed in
the new lexical entry. However, as far as the
parsing task is concerned, such overgeneration
is not likely to reduce the accuracy of the gram-
mar significantly as we show later in this paper
through a thorough evaluation.
6.1 Experimental Setup
With the complete list of 1039 MWE candidates
discussed in section 5, we rank each n-gram
according to each of the three statistical mea-
sures. The average of all the rankings is used
as the combined measure of the MWE candi-
dates. Since we are only interested in acquiring
new lexical entries for MWEs which are not cov-
ered by the grammar, we used the error mining
results (Zhang et al, 2006; van Noord, 2004)
to only keep those candidates with parsability
? 0.1. The top 30 MWE candidates are used in
2The combination of the ?word with space? approach
of Zhang et al (2006) with the constructional approach
we propose here is an interesting topic that we want to
investigate in future research.
this experiment.
We used simple heuristics in order to extract
the head words from these MWEs:
? the n-grams are POS-tagged with an auto-
matic tagger;
? finite verbs in the n-grams are extracted as
head words;
? nouns are also extracted if there is no verb
in the n-gram.
Occasionally, the tagger errors might introduce
wrong head words. However, the lexical type
predictor of Zhang and Kordoni (2006) that we
used in our experiments did not generate inter-
esting new entries for them in the subsequent
steps, and they were thus discarded, as discussed
below.
With the 30 MWE candidates, we extracted
a sub-corpus from the BNC with 674 sentences
which included at least one of these MWEs. The
lexical acquisition technique described in Zhang
and Kordoni (2006) was used with this sub-
corpus in order to acquire new lexical entries for
the head words. The lexical acquisition model
was trained with the Redwoods treebank (Oepen
et al, 2002), following Zhang et al (2006).
The lexical prediction model predicted for
each occurrence of the head words a most plau-
sible lexical type in that context. Only those
predictions that occurred 5 times or more were
taken into consideration for the generation of the
new lexical entries. As a result, we obtained 21
new lexical entries.
These new lexical entries were later merged
into the ERG lexicon. To evaluate the grammar
performance with and without these new lexical
entries, we
1. parsed the sub-corpus with/without new
lexical entries and compared the grammar
coverage;
2. inspected the parser output manually and
evaluated the grammar accuracy.
In parsing the sub-corpus, we used the PET
parser (Callmeier, 2001). For the manual eval-
1040
uation of the parser output, we used the tree-
banking tools of the [incr tsdb()] system (Oepen,
2001).
6.2 Grammar Performance
Table 4 shows that the grammar coverage im-
proved significantly (from 7.1% to 22.7%) with
the acquired lexical entries for the head words
of the MWEs. This improvement in coverage
is largely comparable to the result reported in
(Zhang et al, 2006), where the coverage was re-
ported to raise from 5% to 18% with the ?word
with spaces? approach (see also section 4).
It is also worth mentioning that Zhang et al
(2006) added 373 new lexical entries for a to-
tal of 311 MWE candidates, with an average
of 1.2 entries per MWE. In our experiment, we
achieved a similar coverage improvement with
only 21 new entries for 30 different MWE candi-
dates, with an average of 0.7 entries per MWE.
This suggests that the lexical entries acquired
in our experiment are of much higher linguistic
generality.
To evaluate the grammar accuracy, we man-
ually checked the parser outputs for the sen-
tences in the sub-corpus which received at least
one analysis from the grammar before and af-
ter the lexical extension. Before the lexical ex-
tension, 48 sentences are parsed, among which
32 (66.7%) sentences contain at least one cor-
rect reading (table 4). After adding the 21 new
lexical entries, 153 sentences are parsed, out of
which 124 (81.0%) sentences contain at least one
correct reading.
Baldwin et al (2004) reported in an earlier
study that for BNC data, about 83% of the sen-
tences covered by the ERG have a correct parse.
In our experiment, we observed a much lower
accuracy on the sub-corpus of BNC which con-
tains a lot of MWEs. However, after the lexical
extension, the accuracy of the grammar recovers
to the normal level.
It is also worth noticing that we did not re-
ceive a larger average number of analyses per
sentence (table 4), as it was largely balanced
by the significant increase of sentences covered
by the new lexical entries. We also found
that the disambiguation model as described by
Toutanova et al (2002) performed reasonably
well, and the best analysis is ranked among top-
5 for 66% of the cases, and top-10 for 75%.
All of these indicate that our approach of lexi-
cal acquisition for head words of MWEs achieves
a significant improvement in grammar coverage
without damaging the grammar accuracy. Op-
tionally, the grammar developers can check the
validity of the lexical entries before they are
added into the lexicon. Nonetheless, even a
semi-automatic procedure like this can largely
reduce the manual work of grammar writers.
7 Conclusions
In this paper we looked at some of the issues
involved in the evaluation of the identification
of MWEs. In particular we evaluated the use
of three statistical measures for automatically
identifying MWEs. The results suggest that at
least two of them (MI and PE) can distinguish
MWEs. In terms of the corpora used, a sur-
prisingly higher level of agreement was found
between different corpora (Google and Yahoo)
than between two fragments of the same one.
This tells us two lessons. First that even though
Google and Yahoo were not carefully built to be
language corpora their sizes compensate for that
making them fairly good samples of language
usage. Second, a fraction of a smaller well bal-
anced corpus may not necessarily be as balanced
as the whole.
Furthermore, we argued that for precise gram-
mar engineering it is important to perform a
careful evaluation of the effects of including au-
tomatically acquired MWEs to a grammar. We
looked at the evaluation of the effects in cover-
age, size of the grammar and accuracy of the
parses after adding the MWE-candidates. We
adopted a compositional approach to the en-
coding of MWEs, using some heuristics to de-
tect the head of an MWE, and this resulted in
a smaller grammar than that by Zhang et al
(2006), still achieving a similar increase in cov-
erage and maintaining a high level of accuracy of
parses, comparable to that reported by Baldwin
et al (2004).
The statistical measures are currently only
1041
item # parsed # avg. analysis # coverage %
ERG 674 48 335.08 7.1%
ERG + MWE 674 153 285.01 22.7%
Table 4: ERG coverage with/without lexical acquisition for the head words of MWEs
used in a preprocessing step to filter the non-
MWEs for the lexical type predictor. Alterna-
tively, the statistical outcomes can be incorpo-
rated more tightly, i.e. to combine with the lex-
ical type predictor and give confidence scores on
the resulting lexical entries. These possibilities
will be explored in future work.
References
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proc. of the 6th Conference on Nat-
ural Language Learning (CoNLL-2002), Taipei,
Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British
National Corpus. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Alpino: Wide-coverage computational
analysis of dutch. In Computational Linguistics in
The Netherlands 2000.
Ted Briscoe and John Carroll. 1997. Automatic
extraction of subcategorization from corpora. In
Fifth Conference on Applied Natural Language
Processing, Washington, USA.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Uni-
versita?t des Saarlandes, Saarbru?cken, Germany.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech
and Language, 19(4):450?466.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Jeremy Nicholson and Timothy Baldwin. 2006. In-
terpretation of compound nominalisations using
corpus and web statistics. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 54?
61, Sydney, Australia. Association for Computa-
tional Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and
Thorsten Brants. 2002. The LinGO Redwoods
treebank: Motivation and preliminary applica-
tions. In Proceedings of COLING 2002: The 17th
International Conference on Computational Lin-
guistics: Project Notes, Taipei.
Stephan Oepen. 2001. [incr tsdb()] ? competence
and performance laboratory. User manual. Tech-
nical report, Computational Linguistics, Saarland
University, Saarbru?cken, Germany.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation, Las Palmas, Canary Islands, Spain.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing.
Second edition. Cambridge University Press.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City,
Mexico.
Kristina Toutanova, Christoper D. Manning, Stu-
art M. Shieber, Dan Flickinger, and Stephan
Oepen. 2002. Parse ranking for a rich HPSG
grammar. In Proceedings of the First Workshop
on Treebanks and Linguistic Theories (TLT2002),
pages 253?263, Sozopol, Bulgaria.
Leonoor van der Beek. 2005. The extraction of
determinerless pps. In Proceedings of the ACL-
SIGSEM Workshop on The Linguistic Dimensions
of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colch-
ester, UK.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
1042
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the
special issue on multiword expressions: having a
crack at a hard nut. Journal of Computer Speech
and Language Processing, 19(4):365?377.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech
and Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts process-
ing. In Proceedings of the Fifth International
Conference on Language Resources and Evalua-
tion (LREC 2006), Genoa, Italy.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated multiword ex-
pression prediction for grammar engineering. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying
Properties, pages 36?44, Sydney, Australia. Asso-
ciation for Computational Linguistics.
1043
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 784?792,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Recognizing Textual Relatedness with Predicate-Argument Structures
Rui Wang
Dept of Computational Linguistics
Saarland University
66123 Saarbr?ucken, Germany
rwang@coli.uni-sb.de
Yi Zhang
Dept of Computational Linguistics
Saarland University
LT-Lab, DFKI GmbH
D-66123 Saarbr?ucken, Germany
yzhang@coli.uni-sb.de
Abstract
In this paper, we first compare several
strategies to handle the newly proposed
three-way Recognizing Textual Entailment
(RTE) task. Then we define a new mea-
surement for a pair of texts, called Textual
Relatedness, which is a weaker concept
than semantic similarity or paraphrase. We
show that an alignment model based on the
predicate-argument structures using this
measurement can help an RTE system to
recognize the Unknown cases at the first
stage, and contribute to the improvement
of the overall performance in the RTE task.
In addition, several heterogeneous lexical
resources are tested, and different contri-
butions from them are observed.
1 Introduction
Recognizing Textual Entailment (RTE) (Dagan et
al., 2006) is a task to detect whether one Hypoth-
esis (H) can be inferred (or entailed) by a Text
(T). Being a challenging task, it has been shown
that it is helpful to applications like question an-
swering (Harabagiu and Hickl, 2006). The recent
research on RTE extends the two-way annotation
into three-way
1 2
, making it even more difficult,
but more linguistic-motivated.
The straightforward strategy is to treat it as a
three-way classification task, but the performance
suffers a significant drop even when using the
same classifier and the same feature model. In
fact, it can also be dealt with as an extension to the
traditional two-way classification, e.g., by identi-
1
http://nlp.stanford.edu/RTE3-pilot/
2
http://www.nist.gov/tac/tracks/2008/
rte/rte.08.guidelines.html
fying the Entailment (E) cases first and then fur-
ther label the Contradiction (C) and Unknown (U)
T-H pairs. Some other researchers also work on
detecting negative cases, i.e. contradiction, in-
stead of entailment (de Marneffe et al, 2008).
However, according to our best knowledge, the
detailed comparison between these strategies has
not been fully explored, let alne the impact of the
linguistic motivation behind the strategy selection.
This paper will address this issue.
Take the following example from the RTE-4 test
set (Giampiccolo et al, 2009) as an example,
T: At least five people have been killed in
a head-on train collision in north-eastern
France, while others are still trapped in the
wreckage. All the victims are adults.
H: A French train crash killed children.
This is a pair of two contradicting texts, the
mentioning of events (i.e. train crash) in both T
and H are assumed to refer the same event
3
. In
fact, the only contradicting part lies in the sec-
ond sentence of T against H, that is, whether
there are children among the victims. Therefore,
this pair could also be classified as a Known (K)
pair (=E?C) against Unknown (U) pairs, instead
of being classified as a Non-entailment (N) case
(=C?U) against E case in the traditional two-way
annotation.
Furthermore, many state-of-the-art RTE ap-
proaches which are based on overlapping informa-
tion or similarity functions between T and H, in
fact over-cover the E cases, and sometimes, cover
the C cases as well. Therefore, in this paper, we
3
See more details about the annotation guideline at
http://www.nist.gov/tac/tracks/2008/rte/
rte.08.guidelines.html
784
would like to test whether applying this style of
approaches to capture the K cases instead of E
cases is more effective. While in lexical seman-
tics, semantic relatedness is a weaker concept than
semantic similarity, there is no counterpart at the
sentence or text level. Therefore, in this paper, we
propose a Recognizing Textual Relatedness (RTR)
task as a subtask or the first step of RTE. By doing
so, we choose predicate-argument structure (PAS)
as the feature representation, which has already
been shown quite useful in the previous RTE chal-
lenges (Wang and Neumann, 2007).
In order to obtain the PAS, we utilize a Semantic
Role Labeling (SRL) system developed by Zhang
et al (2008). Although SRL has been shown to be
effective for many tasks, e.g. information extrac-
tion, question answering, etc., it has not been suc-
cessfully used for RTE, mainly due to the low cov-
erage of the verb frame or semantic role resources
or the low performance of the automatic SRL sys-
tems. The recent CoNLL shared tasks (Surdeanu
et al, 2008; Haji?c et al, 2009) have been focus-
ing on semantic dependency parsing along with
the traditional syntactic dependency parsing. The
PAS from the system output is almost ready for
use to build applications based on it. Therefore,
another focus of this paper will be to apply SRL to
the RTE task. In particular, it can improve the first
stage binary classification (K vs. U), and the final
result improves as well.
The rest of the paper will be organized as fol-
lows: Section 2 will give a brief literature review
on both RTE and SRL; Section 3 describes the se-
mantic parsing system, which includes a syntactic
dependency parser and an SRL system; Section 4
presents an algorithm to align two PASs to recog-
nize textual relatedness between T and H, using
several lexical resources; The experiments will be
described in Section 5, followed by discussions;
and the final section will conclude the paper and
point out directions to work on in the future.
2 Related Work
Although the term of Textual Relatedness has not
been widely used by the community (as far as
we know), many researchers have already incor-
porated modules to tackle it, which are usually
implemented as an alignment module before the
inference/learning module is applied. For exam-
ple, Pado et al (2009) mentioned two alignment
modules, one is a phrase-based alignment system
called MANLI (MacCartney et al, 2008), and the
other is a stochastic aligner based on dependency
graphs. Siblini and Kosseim (2009) performed the
alignment on top of two ontologies. In this paper,
we would like to follow this line of research but on
another level of representation, i.e. the predicate-
argument structures (PAS), together with different
lexical semantic resources.
As for the whole RTE task, many people di-
rectly do the three-way classification with selec-
tive features (e.g. Agichtein et al (2009)) or dif-
ferent inference rules to identify entailment and
contradiction simultaneously (e.g. Clark and Har-
rison (2009)); while other researchers also extend
their two-way classification system into three-way
by performing a second-stage classification after-
wards. An interesting task proposed by de Marn-
effe et al (2008) suggested an alternative way to
deal with the three-way classification, that is, to
split out the contradiction cases first. However,
it has been shown to be more difficult than the
entailment recognition. Based on these previous
works and our experimental observations, we pro-
pose an alternative two-stage binary classification
approach, i.e. to identify the unknown cases from
the known cases (entailment and contradiction)
first. And the results show that due to the nature
of these approaches based on overlapping infor-
mation or similarity between T and H, this way of
splitting is more reasonable.
However, RTE systems using semantic role la-
belers has not shown very promising results, al-
though SRL has been successfully used in many
other NLP tasks, e.g. information extraction,
question answering, etc. According to our anal-
ysis of the data, there are mainly three reasons: a)
the limited coverage of the verb frames or predi-
cates; b) the undetermined relationships between
two frames or predicates; and c) the unsatisfy-
ing performance of an automatic SRL system.
For instance, Burchardt et al (2007) attempted to
use FrameNet (Baker et al, 1998) for the RTE-3
challenge, but did not show substantial improve-
ment. With the recent CoNLL challenges, more
and more robust and accurate SRL systems are
ready for use, especially for the PAS identifica-
tion. For the lexical semantics, we also discover
that, if we relax the matching criteria (from simi-
larity to relatedness), heterougeous resources can
contribute to the coverage differently and then the
effectiveness of PAS will be shown as well.
785
3 Semantic Parsing
In order to obtain the predicate-argument struc-
tures for the textual entailment corpus, we use the
semantic role labeler described in (Zhang et al,
2008). The SRL system is trained on the Wall
Street Journal sections of the Penn Treebank us-
ing PropBank and NomBank annotation of ver-
bal and nominal predicates, and relations to their
arguments, and produces as outputs the semantic
dependencies. The head words of the arguments
(including modifiers) are annotated as a direct de-
pendent of the corresponding predicate words, la-
beled with the type of the semantic relation (Arg0,
Arg1 . . . , and various ArgMs). Note that for the
application of SRL in RTE task, the PropBank and
NomBank notation appears to be more accessible
and robust than the the FrameNet notation (with
much more detailed roles or frame elements bond
to specific verb frames).
As input, the SRL system requires syntactic
dependency analysis. We use the open source
MST Parser (McDonald et al, 2005), trained also
on the Wall Street Journal Sections of the Penn
Treebank, using a projective decoder with second-
order features. Then the SRL system goes through
a pipeline of 4-stage processing: predicate identifi-
cation (PI) identifies words that evokes a semantic
predicate; argument identification (AI) identifies
the arguments of the predicates; argument classifi-
cation (AC) labels the argument with the semantic
relations (roles); and predicate classification (PC)
further differentiate different use of the predicate
word. All components are built as maximal en-
tropy based classifiers, with their parameters es-
timated by the open source TADM system
4
, fea-
ture sets selected on the development set. Evalu-
ation results from previous years? CoNLL shared
tasks show that the system achieves state-of-the-
art performance, especially for its out-domain ap-
plications.
4 Textual Relatedness
As we mentioned in the introduction, we break
down the three-way classification into a two-stage
binary classification. Furthermore, we treat the
first stage as a subtask of the main task, which
determines whether H is relevant to T. Similar to
the probabilistic entailment score, we use a relat-
edness score to measure such relationship. Due
4
http://tadm.sourceforge.net/
to the nature of the entailment recognition that
H should be fully entailed by T, we also make
this relatedness relationship asymmetric. Roughly
speaking, this Relatedness function R(T,H) can
be described as whether or how relevant H is to
some part of T. The relevance can be realized as
string similarity, semantic similarity, or being co-
occurred in similar contexts.
Before we define the relatedness function for-
mally, let us look at the representation again. After
semantic parsing described in the previous section,
we obtain a PAS for each sentence. On top of it,
we define a predicate-argument graph (PAG), the
nodes of which are predicates, arguments or some-
times both, and the edges of which are labeled se-
mantic relations. Notice that each predicate can
dominate zero, one, or more arguments, and each
argument have one or more predicates which dom-
inate it. Furthermore, the graph is not necessar-
ily fully connected. Thus, the R(T,H) function
can be defined on the dependency representation
as follows: if the PAG of H is semantically rel-
evant to part of the PAG of T, H is semantically
relevant to T.
In order to compare the two graphs, we further
reduce the alignment complexity by breaking the
graphs into sets of trees. Two types of decomposed
trees are considered: one is to take each predicate
as the root of a tree and arguments as child nodes,
and the other is on the contrary, to take each ar-
gument as root and their governing predicates as
child nodes. We name them as Predicate Trees (P-
Trees) and Argument Trees (A-Trees) respectively.
To obtain the P-Trees, we enumerate each predi-
cate, find all the arguments which it directly dom-
inates, and then construct a P-Tree. The algorithm
to obtain A-Trees works in the similar way. Fi-
nally, we will have a set of P-Trees and a set of A-
Trees for each PAG, both of which are simple trees
with depth of one. Figure 1 shows an example of
such procedures. Notice that we do not consider
cross-sentential inference, instead, we simply take
the union of tree sets from all the sentences. Figure
2 illustrates the PAG for both T and H after seman-
tic parsing, and the resulting P-Trees and A-Trees
after applying the decomposition algorithm.
Formally, we define the relatedness function for
a T-H pair as the maximum value of the related-
ness scores of all pairs of trees in T and H (P-trees
and A-trees).
786
ad e f
b c e
ge
a
d e
a b a
f
e
g
e f
b ca
d
g
A?Tree(s)
P?Tree(s)
Figure 1: Decomposition of predicate-argument graphs (left) into P-Trees (right top) and A-Trees (right
bottom)
R(T,H) = max
1?i?r,1?j?s
{
R(Tree
T
i
, T ree
H
j
)
}
In order to compare two P-Trees or A-Trees,
we further define each predicate-argument pair
contained in a tree as a semantic dependency
triple. Each semantic dependency triple con-
tains a predicate, an argument, and the seman-
tic dependency label in between, in the form
of ?Predicate,Dependency,Argument?. Then
we define the relatedness function between two
trees as the minimum value of the relatedness
scores of all the triple pairs from the two trees.
R(Tree
T
, T ree
H
) = min
1?i?n,1?j?m
{
R(?P
T
, D
T
i
, A
T
i
?, ?P
H
, D
H
j
, A
H
j
?)
}
For the relatedness function between two se-
mantic dependency triples, we define the follow-
ing two settings: the FULL match and the NOT-
FULL match. Either match requires that the pred-
icates are related at the first place. The former
means both the dependencies and the arguments
are related; while the latter only requires the de-
pendencies to be related.
R(?P
T
, D
T
, A
T
?, ?P
H
, D
H
, A
H
?) =
?
?
?
Full R(P
T
,P
H
)=R(D
T
,D
H
)=R(A
T
,A
H
)=1
NotFull R(P
T
,P
H
)=R(D
T
,D
H
)=1
Other Otherwise
Now, the only missing components in our defi-
nition is the relatedness functions between pred-
icates, arguments, and semantic dependencies.
Fortunately, many people have done research on
semantic relatedness in lexical semantics that we
could use. Therefore, these functions can be
realized by different string matching algorithms
and/or lexical resources. Since the meaning of rel-
evance is rather wide, apart from the string match-
ing of the lemmas, we also incorporate various
resources, from distributionally collected ones to
hand-crafted ontologies. We choose VerbOcean
(Chklovski and Pantel, 2004) to obtain the relat-
edness between predicates (after using WordNet
(Fellbaum, 1998) to change all the nominal pred-
icates into verbs) and use WordNet for the argu-
ment alignment. For the verb relations in Ver-
bOcean, we consider all of them as related; and
for WordNet, we not only use the synonyms, hy-
ponyms, and hypernyms, but antonyms as well.
Consequently, we simplify these basic relatedness
functions into a binary decision. If the correspond-
ing strings are matched or the relations mentioned
above exist, the two predicates, arguments, or de-
pendencies are related; otherwise, not.
In addition, the Normalized Google Distance
(NGD) (Cilibrasi and Vitanyi, 2007) is applied to
both cases
5
. As for the comparison between de-
pendencies, we simply apply the string matching,
except for modifier labels, which we treat them as
the same
6
. In all, the main idea here is to incorpo-
rate both distributional semantics and ontological
semantics in order to see whether their contribu-
tions are overlapping or complementary. In prac-
tice, we use empirical value 0.5 as the threshold.
Below the threshold means they are related, oth-
5
You may find the NGD values of all the con-
tent word pairs in RTE-3 and RTE-4 datasets at
http://www.coli.uni-sb.de/
?
rwang/
resources/RTE3_RTE4_NGD.txt.
6
This is mainly because it is more difficult for the SRL
system to differentiate modifier labels than the complements.
787
crash
killed
train
children
A0 A1
A1whilepeople
A1
killed
... ... train
collision
still
A1
trapped
... ...
AM?ADV
AM?ADV
A1
crash
train
killed
children
A0 A1
crash
P?Trees
A?Trees
whilepeople
A1
killed
... ... train
collision
still
A1
trapped
... ...
AM?ADV
AM?ADV
killed killed killed collision trapped trapped
people while ... ... train still ... ...
A1 AM?ADV A1 AM?ADV
killed killed crash
traincrash children
A1A0 A1
T H
PAG
Figure 2: Predicate-argument graphs and corresponding P-Trees and A-trees of the T-H pair
erwise not. In order to achieve a better coverage,
we use the OR operator to connect all the related-
ness functions above, which means, if any of them
holds, the two items are related.
Notice that, although we define only the relat-
edness between T and H, in principle, the graph
representation can also be used for the entailment
relationship. However, since it needs more fine-
grained analysis and resources, we will leave it as
the future work.
5 Experiments
In order to evaluate our method, we setup several
experiments. The baseline system here is a simple
Naive Bayes classifier with a feature set contain-
ing the Bag-of-Words (BoW) overlapping ratio be-
tween T and H, and also the syntactic dependency
overlapping ratio. The feature model combines
two baseline systems proposed by previous work,
which gives out quite competitive performance.
Since the main goal of this paper is to show the
impact of the PAS-based alignment module, we
will not compare our results with other RTE sys-
tems (In fact, the baseline system already outper-
forms the average accuracy score of the RTE-4
challenge).
The main data set used for testing here is the
RTE-4 data set with three-way annotations (500
entailment T-H pairs (E), 150 contradiction pairs
(C), and 350 unknown pairs (U)). The results on
RTE-3 data set (combination of the development
set and test set, in all, 822 E pairs, 161 C pairs,
and 617 U pairs) is also shown, although the origi-
nal annotation is two-way and the three-way anno-
tation was done by different researchers after the
challenge
7
.
We will first show the performance of the base-
line systems, followed by the results of our PAS-
based alignment module and its impact on the
whole task. After that, we will also give more de-
tailed analysis of our alignment module, according
to different lexical relatedness measurements.
7
The annotation of the development set was done by stu-
dents at Stanford, and the annotation of the test set was done
as double annotation by NIST assessors, followed by adjudi-
cation of disagreements. Answers were kept consistent with
the two-way decisions in the main task gold answer file.
788
5.1 Baselines
The baseline systems used here are based on over-
lapping ratio of words and syntactic dependencies
between T and H. For the word overlapping ratio,
we calculate the number of overlapping tokens be-
tween T and H and normalize it by dividing it by
the number of tokens in H. The syntactic depen-
dency overlapping ratio works similarly: we cal-
culate the number of overlapping syntactic depen-
dencies and divide it by the number of syntactic
dependencies in H, i.e. the same as the number
of tokens. Enlightened by the relatedness func-
tion, we also allow either FULL match (meaning
both the dependencies and the parent tokens are
matched), and NOTFULL match (meaning only the
dependencies are matched). Here we only use
string match between lemmas and syntactic de-
pendencies. Table 1 presents the performance of
the baseline system.
The results show that, even with the same clas-
sifier and the same feature model, with a proper
two-stage strategy, it can already achieve better
results than the three-way classification. Note
that, the first strategy is not so successful, and
that is the traditional two-way annotation of the
RTE task. Our explanation here is that the BoW
method (even with syntactic dependency features)
is based on overlapping information shared by T
and H, which essentially means the more informa-
tion they share, the more relevant they are, instead
of being more similar or the same. Therefore, for
the ?ECU ? E/CU? setting, methods based on
overlapping information are not the best choice,
while for ?ECU ? U/EC?, they are more ap-
propriate.
In addition, the upper bound numbers show the
accuracy when the first-stage classification is per-
fect, which give us an indication of how far we
could go. The lower upper bound for the second
strategy is mainly due to the low proportion of the
C cases (15%) in the data set; while the other two
both show large space for improvement.
5.2 The PAS-based Alignment Module
In this subsection, we present a separate evalua-
tion of our PAS-based alignment module. As we
mentioned before (cf. Section 4), there are sev-
eral parameters to be tuned in our alignment algo-
rithm: a) whether the relatedness function between
P-Trees asks for the FULL match; b) whether the
function for A-Trees asks for the FULL match; and
c) whether both P-Trees and A-Trees being related
are required or either of them holds is enough.
Since they are all binary values, we use the 3-digit
code to represent each setting, e.g. [FFO]
8
means
either P-Trees are FULL matched or A-Trees are
FULL matched. The performances of different set-
tings of the module are shown in the following
Precision-Recall figure 3,
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  
68
 
70
 
72
 
74
 
76
 
78
 
80
 
82
 
84
 
86
Recall (%)
Pre
cis
ion
 (%)
[FFA
][N
FA]
[FN
A]
[FFO
]
[NN
A]
[NF
O]
[FN
O]
[NN
O]
Figure 3: Precision and recall of different align-
ment settings
Since we will combine this module with the
baseline system and it will be integrated as the
first-stage classification, the F1 scores are not in-
dicative for selecting the best setting. Intuitively,
we may prefer higher precision than recall.
One limitation of our method we need to point
out here is that, if some important predicates or ar-
guments in H are not (correctly) identified by the
SRL system, fewer P-Trees and A-Trees are re-
quired to be related to some part of T, thus, the
relatedness of the whole pair could easily be satis-
fied, leading to false positive cases.
5.3 Impact on the Final Results
The best settings for RTE-3 data set is [NNA] and
for RTE-4 data set is [NFO], which are both in the
middle of the setting range shown in the previous
figure 3.
As for the integration of the PAS-based align-
ment model with our BoW-based baseline, we
only consider the third two-stage classification
strategy in Table 1. Other strategies would also be
interesting to try, however, the proposed alignment
algorithm exploits relatedness between T and H,
which might not be fine-grained enough to detect
8
F stands for FULL, and O stands for OR. Other letters
are, N stands for NOTFULL, and A stands for AND.
789
Strategies Three-Way Two-Stage
E/C/U E/CU ? E/C/U C/EU ? C/E/U U/EC ? U/E/C
Accuray 53.20% 50.00% 53.50% 54.20%
Upper Bound / 82.80% 68.70% 84.90%
Table 1: Performances of the Baselines
entailment or contradiction. New alignment algo-
rithm has to be designed to explore other strate-
gies. Thus, in this work, we believe that the align-
ment algorithm based on PAS (and other methods
based on overlapping information between T and
H) is suitable for the U/EC ? U/E/C classifi-
cation strategy.
Table 2 shows the final results.
The first observation is that the improvement of
accuracy on the first stage of the classification can
be preserved to the final results. And our PAS-
based alignment module can help, though there
is still large space for improvement. Compared
with the significantly improved results on RTE-4,
the improvement on RTE-3 is less obvious, mainly
due to the relatively lower precision (70.33% vs.
79.67%) of the alignment module itself.
Also, we have to say that the improvement is not
as big as we expected. There are several reasons
for this. Besides the limitation of our approach
mentioned in the previous section, the predicates
and arguments themselves might be too sparse to
convey all the information we need for the en-
tailment detection. In addition, in some sense,
the baseline is quite strong for this comparison,
since the PAS-based alignment module relies on
the overlapping words at the first place, there are
quite a few pairs solved by both the main approach
and the baseline. Then, it would be interesting
to take a closer look at the lexical resources used
in the main system, which is another additional
knowledge it has, comparing with the baseline.
5.4 Impact of the Lexical Resources
We did an ablation test of the lexical resources
used in our alignment module. Recall that we
have applied three lexical resources, VerbOcean
for the predicate relatedness function, WordNet
for the argument relatedness function, and Nor-
malized Google Distance for both. Table 3 shows
the performances of the system without each of the
resources,
The results clearly show that each lexical re-
source does contribute some improvement to the
final performance of the system and it confirms
the idea of combining lexical resources being ac-
quired in different ways. For instance, at the
beginning, we expected that the relationship be-
tween ?people? and ?children? could be captured
by WordNet, but in fact not. Fortunately, the NGD
has a quite low value of this pair of words (0.21),
which suggests that they occur together quite of-
ten, or in other words, they are relevant.
One interesting future work on this aspect is to
substitute the OR connector between these lexical
resources with an AND operator. Thus, instead of
using them to achieve a higher coverage, whether
they could be filters for each other to increase the
precision will also be interesting to know.
6 Conclusion and Future Work
In this paper, we address the motivation and issues
of casting the three-way RTE problem into a two-
stage binary classification task. We apply an SRL
system to derive the predicate-argument structure
of the input sentences, and propose ways of cal-
culating semantic relatedness between the shallow
semantic structures of T and H. The experiments
show improvements in the first-stage classifica-
tion, which accordingly contribute to the final re-
sults of the RTE task.
For future work, we would like to see whether
the PAS can help the second-stage classification
as well, e.g. the semantic dependency of negation
(AM-NEG) could be helpful for the contraction
recognition. Furthermore, since the PAS is usu-
ally a bag of unconnected graphs, we could find
a way to joint them together, in order to consider
both inter- and intra- sentential inferences based
on it.
In addition, this approach has the potential to
be integrated with other RTE modules. For in-
stance, for the predicate alignment, we may con-
sider to use DIRT rules (Lin and Pantel, 2001)
or other paraphrase resources (Callison-Burch,
2008), and for the argument alignment, exter-
nal named-entity recognizer and anaphora resolver
would be very helpful. Even more, we also plan to
compare/combine it with other methods which are
not based on overlapping information between T
and H.
790
Systems Baseline1 Baseline2 SRL+Baseline2 The First Stage
Data Sets Three-Way Two-Stage Two-Stage Baseline2 SRL+Baseline2 SRL
RTE-3 [NNA] 52.19% 52.50% 53.69%(2.87%?) 59.50% 60.56%(1.78%?) 70.33%
RTE-4 [NFO] 53.20% 54.20% 56.60%(6.39%?) 67.10% 70.20%(4.62%?) 79.67%
Table 2: Results on the Whole Datasets
Data Sets SRL+Baseline SRL+Baseline - VO SRL+Baseline - NGD SRL+Baseline - WN
RTE-3 [NNA] 53.69% 53.19%(0.93%?) 53.50%(0.35%?) 52.88%(1.51%?)
RTE-4 [NFO] 56.60% 56.00%(1.06%?) 56.10%(0.88%?) 55.70%(1.59%?)
Table 3: Impact of the Lexical Resources
Acknowledgments
The first author is funded by the PIRE PhD
scholarship program sponsored by the German
Research Foundation (DFG). The second author
thanks the German Excellence Cluster of Multi-
modal Computing and Interaction for the support
of the work.
References
Eugene Agichtein, Walt Askew, and Yandong Liu.
2009. Combining Lexical, Syntactic, and Semantic
Evidence for Textual Entailment Classification. In
Proceedings of the First Text Analysis Conference
(TAC 2008).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
Volume 1, pages 86?90, Montreal, Canada.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, Prague, Czech
Republic.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04), Barcelona, Spain.
Rudi Cilibrasi and Paul M. B. Vitanyi. 2007. The
Google Similarity Distance. IEEE/ACM Trans-
actions on Knowledge and Data Engineering,
19(3):370?383.
Peter Clark and Phil Harrison. 2009. Recognizing
Textual Entailment with Logical Inference. In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Machine Learning Challenges,
volume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2009. The Fourth PASCAL Recognizing Textual
Entailment Challenge. In Proceedings of the First
Text Analysis Conference (TAC 2008).
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning, Boulder, CO, USA.
Sanda Harabagiu and Andrew Hickl. 2006. Meth-
ods for Using Textual Entailment in Open-Domain
Question Answering. In Proceedings of COLING-
ACL 2006, pages 905?912, Sydney, Australia.
Dekang Lin and Patrick Pantel. 2001. DIRT - Dis-
covery of Inference Rules from Text. In In Proceed-
ings of the ACM SIGKDD Conference on Knowledge
Discovery and Data Mining.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP 2008.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of hlt-emnlp 2005, pages 523?530, Vancouver,
Canada.
Sebastian Pado, Marie-Catherine de Marneffe, Bill
MacCartney, Anna N. Rafferty, Eric Yeh, and
791
Christopher D. Manning. 2009. Deciding en-
tailment and contradiction with stochastic and edit
distance-based alignment. In Proceedings of the
First Text Analysis Conference (TAC 2008).
Reda Siblini and Leila Kosseim. 2009. Using Ontol-
ogy Alignment for the TAC RTE Challenge. In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th conference on computational natural lan-
guage learning (CoNLL-2008), Manchester, UK.
Rui Wang and G?unter Neumann. 2007. Recog-
nizing textual entailment using a subsequence ker-
nel method. In Proceedings of the Twenty-Second
AAAI Conference on Artificial Intelligence (AAAI-
07), pages 937?942, Vancouver, Canada.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning (CoNLL 2008), pages 198?202,
Manchester, UK.
792
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 189?192,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mapping between Compositional Semantic Representations and Lexical
Semantic Resources: Towards Accurate Deep Semantic Parsing
Sergio Roa??, Valia Kordoni? and Yi Zhang?
Dept. of Computational Linguistics, Saarland University, Germany?
German Research Center for Artificial Intelligence (DFKI GmbH)?
Dept. of Computer Science, University of Freiburg, Germany?
{sergior,kordoni,yzhang}@coli.uni-sb.de
Abstract
This paper introduces a machine learning
method based on bayesian networks which
is applied to the mapping between deep se-
mantic representations and lexical semantic
resources. A probabilistic model comprising
Minimal Recursion Semantics (MRS) struc-
tures and lexicalist oriented semantic features
is acquired. Lexical semantic roles enrich-
ing the MRS structures are inferred, which are
useful to improve the accuracy of deep seman-
tic parsing. Verb classes inference was also
investigated, which, together with lexical se-
mantic information provided by VerbNet and
PropBank resources, can be substantially ben-
eficial to the parse disambiguation task.
1 Introduction
Recent studies of natural language parsing have
shown a clear and steady shift of focus from pure
syntactic analyses to more semantically informed
structures. As a result, we have seen an emerging
interest in parser evaluation based on more theory-
neutral and semantically informed representations,
such as dependency structures. Some approaches
have even tried to acquire semantic representations
without full syntactic analyses. The so-called shal-
low semantic parsers build basic predicate-argument
structures or label semantic roles that reveal the par-
tial meaning of sentences (Carreras and Ma`rquez,
2005). Manually annotated lexical semantic re-
sources like PropBank (Palmer et al, 2005), Verb-
Net (Kipper-Schuler, 2005), or FrameNet (Baker et
al., 1998) are usually used as gold standards for
training and evaluation of such systems. In the
meantime, various existing parsing systems are also
adapted to provide semantic information in their out-
puts. The obvious advantage in such an approach
is that one can derive more fine-grained represen-
tations which are not typically available from shal-
low semantic parsers (e.g., modality and negation,
quantifiers and scopes, etc.). To this effect, var-
ious semantic representations have been proposed
and used in different parsing systems. Generally
speaking, such semantic representations should be
capable of embedding shallow semantic information
(i.e., predicate-argument or semantic roles). How-
ever, it is non-trivial to map even the basic predicate-
arguments between different representations. This
becomes a barrier to both sides, making the cross-
fertilization of systems and resources using different
semantic representations very difficult.
In this paper, we present a machine learning ap-
proach towards mapping between deep and shallow
semantic representations. More specifically, we use
Bayesian networks to acquire a statistical model that
enriches the Minimal Recursion Semantics struc-
tures produced by the English Resource Grammar
(ERG) (Flickinger, 2002) with VerbNet-like seman-
tic roles. Evaluation results show that the mapping
from MRS to semantic roles is reliable and benefi-
cial to deep parsing.
2 Minimal Recursion Semantics
The semantic representation we are interested in
in this paper is the Minimal Recursion Semantics
(MRS). Because of its underspecifiability, it has
been widely used in many deep and shallow pro-
cessing systems. The main assumption behind MRS
is that the interesting linguistic units for compu-
tational semantics are the elementary predications
(EPs), which are single relations with associated ar-
guments (Copestake et al, 2006). In this paper,
the MRS structures are created with the English Re-
source Grammar (ERG), a HPSG-based broad cov-
erage precision grammar for English. The seman-
189
tic predicates and their linguistic behaviour (includ-
ing the set of semantic roles, indication of optional
arguments, and their possible value constraints are
specified by the grammar as its semantic interface
(SEM-I) (Flickinger et al, 2005).
3 Relating MRS structures to lexical
semantic resources
3.1 Feature extraction from linguistic resources
The first set of features used to find corresponding
lexical semantic roles for the MRS predicate argu-
ments are taken from Robust MRS (RMRS) struc-
tures (Copestake, 2006). The general idea of the
process is to traverse the bag of elementary predi-
cations looking for the verbs in the parsed sentence.
When a verb is found, then its arguments are taken
from the rarg tags and alternatively from the in-g
conjunctions related to the verb. So, given the sen-
tence:
(1) Yields on money-market mutual funds contin-
ued to slide, amid signs that portfolio managers
expect further declines in interest rates.
the obtained features for expect are shown in Table
1.
SEM-I roles Features Words
ARG1 manager n of managers
ARG2 propositional m rel further declines
Table 1: RMRS features for the verb expect
The SEM-I role labels are based mainly on syn-
tactic characteristics of the verb. We employed
the data provided by the PropBank and VerbNet
projects to extract lexical semantic information. For
PropBank, the argument labels are named ARG1,...,
ARGN and additionally ARGM for adjuncts. In the
case of VerbNet, 31 different thematic roles are pro-
vided, e.g. Actor, Agent, Patient, Proposition, Predi-
cate, Theme, Topic. A treebank of RMRS structures
and derivations was generated by using the Prop-
Bank corpus. The process of RMRS feature extrac-
tion was applied and a new verb dependency trees
dataset was created.
To obtain a correspondence between the SEM-I
role labels and the PropBank (or VerbNet) role la-
bels, a procedure which maps these labellings for
each utterance and verb found in the corpus was im-
plemented. Due to the possible semantic roles that
subjects and objects in a sentence could bear, the
mapping between SEM-I roles and VerbNet role la-
bels is not one-to-one. The general idea of this align-
ment process is to use the words in a given utterance
which are selected by a given role label, both a SEM-
I and a PropBank one. With these words, a naive as-
sumption was applied that allows a reasonable com-
parison and alignment of these two sources of infor-
mation. The naive assumption considers that if all
the words selected by some SEM-I label are found in
a given PropBank (VerbNet) role label, then we can
deduce that these labels can be aligned. An impor-
tant constraint is that all the SEM-I labels must be
exhausted. An additional constraint is that ARG1,
ARG2 or ARG3 SEM-I labels cannot be mapped to
ARGM PropBank labels. When an alignment be-
tween a SEM-I role and a corresponding lexical se-
mantic role is found, no more mappings for these
labels are allowed. For instance, given the example
in Table 1, with the following Propbank (VerbNet)
labelling:
(2) [Arg0(Experiencer) Portfolio managers] expect
[Arg1(Theme) further declines in interest rates.]
the alignment shown in Table 2 is obtained.
SEM-I roles Mapped roles Features
ARG1 Experiencer manager n of
ARG2 Theme propositional m rel
Table 2: Alignment instance obtained for the verb expect
Since the use of fine-grained features can make
the learning process very complex, the WordNet
semantic network (Fellbaum, 1998) was also em-
ployed to obtain generalisations of nouns. The al-
gorithm described in (Pedersen et al, 2004) was
used to disambiguate the sense, given the heads
of the verb arguments and the verb itself (by us-
ing the mapping from VerbNet senses to WordNet
verb senses (Kipper-Schuler, 2005)). Alternatively,
a naive model has also been proposed, in which
these features are simply generalized as nouns. For
prepositions, the ontology provided by the SEM-I
was used. Other words like adjectives or verbs in
arguments were simply generalised as their corre-
sponding type (e.g., adjectival rel or verbal rel).
190
3.2 Inference of semantic roles with Bayesian
Networks
The inference of semantic roles is based on train-
ing of BNs by presenting instances of the features
extracted, during the learning process. Thus, a train-
ing example corresponding to the features shown in
Table 2 might be represented as Figure 1 shows, us-
ing a first-order approach. After training, the net-
work can infer a proper PropBank (VerbNet) seman-
tic role, given some RMRS role corresponding to
some verb. The use of some of these features can
be relaxed to test different alternatives.VerbNet classwish?62
ARG1 ARG3ARG2propositional_m_rel
RMRS Features
ARGMExperiencer nullThemepropositional_m_rel
PropBank/VerbNet Features
nullthing_nliving_
living_thing_n
Figure 1: A priori structure of the BN for lexical semantic
roles inference.
Two algorithms are used to train the BNs. The
Maximum Likelihood (ML) estimation procedure is
used when the structure of the model is known. In
our experiments, the a priori structure shown in Fig-
ure 1 was employed. In the case of the Structural Ex-
pectation Maximization (SEM) Algorithm, the ini-
tial structure assumed for the ML algorithm serves
as an initial state for the network and then the learn-
ing phase is executed in order to learn other con-
ditional dependencies and parameters as well. The
training procedure is described in Figure 2.
procedure Train (Model)
1: for all Verbs do
2: for all Sentences and Parsings which include the current verb
do
3: Initialize vertices of the network with SEM-I labels and fea-
tures.
4: Initialize optionally vertices with the corresponding VerbNet
class.
5: Initialize edges connecting corresponding features.
6: Append the current features as evidence for the network.
7: end for
8: Start Training Model for the current Verb, where Model is ML
or SEM.
9: end for
Figure 2: Algorithm for training Bayesian Networks for
inference of lexical semantic roles
After the training phase, a testing procedure using
the Markov Chain Monte Carlo (MCMC) inference
engine can be used to infer role labels. Since it is
reasonable to think that in some cases the VerbNet
class is not known, the presentation of this feature as
evidence can be left as optional. Thus, after present-
ing as evidence the SEM-I related features, a role
label with highest probability is obtained after using
the MCMC with the current evidence.
4 Experimental results
The experiment uses 10370 sentences from the
PropBank corpus which have a mapping to Verb-
Net (Loper et al, 2007) and are successfully parsed
by the ERG (December 2006 version). Up to 10
best parses are recorded for each sentence. The to-
tal number of instances, considering that each sen-
tence contains zero or more verbs, is 13589. The
algorithm described in section 3.1 managed to find
at least one mapping for 10960 of these instances
(1020 different verb lexemes). If the number of pars-
ing results is increased to 25 the results are improved
(1460 different verb lexemes were found). In the
second experiment, the sentences without VerbNet
mappings were also included.
The results for the probabilistic models for in-
fering lexical semantic roles are shown in Table 3,
where the term naive means that no WordNet fea-
tures were included in the training of the models, but
only simple features like noun rel for nouns. On the
contrary, when mode is complete, WordNet hyper-
nyms up to the 5th level in the hierarchy were used.
In this set of experiments the VerbNet class was also
included (in the marked cases) during the learning
and inference phases.
Corpus Nr. iter. Mode Model Verb Accuracy %
MCMC classes
PropBank with 1000 ML naive 78.41
VerbNet labels 10000 ML naive 84.48
10000 ML naive ? 87.92
1000 ML complete 84.74
10000 ML complete 86.79
10000 ML complete ? 87.76
1000 SEM naive 84.25
1000 SEM complete 87.26
PropBank with 1000 ML naive 87.46
PropBank labels 1000 SEM naive 90.27
Table 3: Results of role mapping with probabilistic model
In Table 3, the errors are due to the problems in-
troduced by the alternation behaviour of the verbs,
which are not encoded in the SEM-I labelling and
191
also some contradictory annotations in the mapping
between PropBank and VerbNet. Furthermore, the
use of the WordNet features may also generate a
more complex model or problems derived from the
disambiguation process and hence produce errors in
the inference phase. In addition, it is reasonable
to use the VerbNet class information in the learn-
ing and inference phases, which in fact improves
slightly the results. The outcomes also show that
the use of the SEM algorithm improves accuracy
slightly, meaning that the conditional dependency
assumptions were reasonable, but still not perfect.
The model can be slightly modified for verb class
inference, by adding conditional dependencies be-
tween the VerbNet class and SEM-I features, which
can potentially improve the parse disambiguation
task, in a similar way of thinking to (Fujita et al,
2007). For instance, for the following sentence, we
derive an incorrect mapping for the verb stay to the
VerbNet class EXIST-47.1-1 with the (falsely) fa-
vored parse where the PP ?in one place? is treated as
an adjunct/modifier. For the correct reading where
the PP is a complement to stay, the mapping to the
correct VerbNet class LODGE-46 is derived, and the
correct LOCATION role is identified for the PP.
(3) Regardless of whether [Theme you] hike from
lodge to lodge or stayLODGE-46 [Location in one
place] and take day trips, there are plenty of
choices.
5 Conclusions and Future Work
In this paper, we have presented a study of mapping
between the HPSG parser semantic outputs in form
of MRS structures and lexical semantic resources.
The experiment result shows that the Bayesian net-
work reliably maps MRS predicate-argument struc-
tures to semantic roles. The automatic mapping en-
ables us to enrich the deep parser output with seman-
tic role information. Preliminary experiments have
also shown that verb class inference can potentially
improve the parse disambiguation task. Although
we have been focusing on improving the deep pars-
ing system with the mapping to annotated semantic
resources, it is important to realise that the mapping
also enables us to enrich the shallow semantic an-
notations with more fine-grained analyses from the
deep grammars. Such analyses can eventually be
helpful for applications like question answering, for
instance, and will be investigated in the future.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Proceedings of
the 36th Annual Meeting of the ACL and 17th In-
ternational Conference on Computational Linguistics,
pages 86?90, San Francisco, CA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 152?164, Ann Arbor, Michigan.
Ann Copestake, Dan P. Flickinger, and Ivan A. Sag.
2006. Minimal recursion semantics: An introduction.
Research on Language and Computation, 3(4):281?
332.
Ann Copestake. 2006. Robust minimal recursion seman-
tics. Working Paper.
Christiane D. Fellbaum. 1998. WordNet ? An Electronic
Lexical Database. MIT Press.
Dan Flickinger, Jan T. L?nning, Helge Dyvik, Stephan
Oepen, and Francis Bond. 2005. SEM-I rational MT.
Enriching deep grammars with a semantic interface for
scalable machine translation. In Proceedings of the
10th Machine Translation Summit, pages 165 ? 172,
Phuket, Thailand.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. CSLI Publications.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for
hpsg parse selection. In ACL 2007 Workshop on Deep
Linguistic Processing, pages 25?32, Prague, Czech
Republic.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between Prop-
bank and Verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the Nine-
teenth National Conference on Artificial Intelligence
(AAAI-04).
192
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 378?386,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Cross-Domain Dependency Parsing Using a Deep Linguistic Grammar
Yi Zhang
LT-Lab, DFKI GmbH and
Dept of Computational Linguistics
Saarland University
D-66123 Saarbru?cken, Germany
yzhang@coli.uni-sb.de
Rui Wang
Dept of Computational Linguistics
Saarland University
66123 Saarbru?cken, Germany
rwang@coli.uni-sb.de
Abstract
Pure statistical parsing systems achieves
high in-domain accuracy but performs
poorly out-domain. In this paper, we
propose two different approaches to pro-
duce syntactic dependency structures us-
ing a large-scale hand-crafted HPSG gram-
mar. The dependency backbone of an
HPSG analysis is used to provide general
linguistic insights which, when combined
with state-of-the-art statistical dependency
parsing models, achieves performance im-
provements on out-domain tests.?
1 Introduction
Syntactic dependency parsing is attracting more
and more research focus in recent years, par-
tially due to its theory-neutral representation, but
also thanks to its wide deployment in various
NLP tasks (machine translation, textual entailment
recognition, question answering, information ex-
traction, etc.). In combination with machine learn-
ing methods, several statistical dependency pars-
ing models have reached comparable high parsing
accuracy (McDonald et al, 2005b; Nivre et al,
2007b). In the meantime, successful continuation
of CoNLL Shared Tasks since 2006 (Buchholz and
Marsi, 2006; Nivre et al, 2007a; Surdeanu et al,
2008) have witnessed how easy it has become to
train a statistical syntactic dependency parser pro-
vided that there is annotated treebank.
While the dissemination continues towards var-
ious languages, several issues arise with such
purely data-driven approaches. One common
observation is that statistical parser performance
drops significantly when tested on a dataset differ-
ent from the training set. For instance, when using
?The first author thanks the German Excellence Cluster
of Multimodal Computing and Interaction for the support of
the work. The second author is funded by the PIRE PhD
scholarship program.
the Wall Street Journal (WSJ) sections of the Penn
Treebank (Marcus et al, 1993) as training set, tests
on BROWN Sections typically result in a 6-8%
drop in labeled attachment scores, although the av-
erage sentence length is much shorter in BROWN
than that in WSJ. The common interpretation is
that the test set is heterogeneous to the training set,
hence in a different ?domain? (in a loose sense).
The typical cause of this is that the model overfits
the training domain. The concerns over random
choice of training corpus leading to linguistically
inadequate parsing systems increase over time.
While the statistical revolution in the field
of computational linguistics gaining high pub-
licity, the conventional symbolic grammar-based
parsing approaches have undergone a quiet pe-
riod of development during the past decade, and
reemerged very recently with several large scale
grammar-driven parsing systems, benefiting from
the combination of well-established linguistic the-
ories and data-driven stochastic models. The ob-
vious advantage of such systems over pure statis-
tical parsers is their usage of hand-coded linguis-
tic knowledge irrespective of the training data. A
common problem with grammar-based parser is
the lack of robustness. Also it is difficult to de-
rive grammar compatible annotations to train the
statistical components.
2 Parser Domain Adaptation
In recent years, two statistical dependency parsing
systems, MaltParser (Nivre et al, 2007b) and
MSTParser (McDonald et al, 2005b), repre-
senting different threads of research in data-driven
machine learning approaches have obtained high
publicity, for their state-of-the-art performances in
open competitions such as CoNLL Shared Tasks.
MaltParser follows the transition-based ap-
proach, where parsing is done through a series
of actions deterministically predicted by an oracle
model. MSTParser, on the other hand, follows
378
the graph-based approach where the best parse
tree is acquired by searching for a spanning tree
which maximizes the score on either a partially
or a fully connected graph with all words in the
sentence as nodes (Eisner, 1996; McDonald et al,
2005b).
As reported in various evaluation competitions,
the two systems achieved comparable perfor-
mances. More recently, approaches of combining
these two parsers achieved even better dependency
accuracy (Nivre and McDonald, 2008). Granted
for the differences between their approaches, both
systems heavily rely on machine learning methods
to estimate the parsing model from an annotated
corpus as training set. Due to the heavy cost of
developing high quality large scale syntactically
annotated corpora, even for a resource-rich lan-
guage like English, only very few of them meets
the criteria for training a general purpose statisti-
cal parsing model. For instance, the text style of
WSJ is newswire, and most of the sentences are
statements. Being lack of non-statements in the
training data could cause problems, when the test-
ing data contain many interrogative or imperative
sentences as in the BROWN corpus. Therefore, the
unbalanced distribution of linguistic phenomena
in the training data leads to inadequate parser out-
put structures. Also, the financial domain specific
terminology seen in WSJ can skew the interpreta-
tion of daily life sentences seen in BROWN.
There has been a substantial amount of work on
parser adaptation, especially from WSJ to BROWN.
Gildea (2001) compared results from different
combinations of the training and testing data to
demonstrate that the size of the feature model
can be reduced via excluding ?domain-dependent?
features, while the performance could still be pre-
served. Furthermore, he also pointed out that if the
additional training data is heterogeneous from the
original one, the parser will not obtain a substan-
tially better performance. Bacchiani et al (2006)
generalized the previous approaches using a maxi-
mum a posteriori (MAP) framework and proposed
both supervised and unsupervised adaptation of
statistical parsers. McClosky et al (2006) and Mc-
Closky et al (2008) have shown that out-domain
parser performance can be improved with self-
training on a large amount of unlabeled data. Most
of these approaches focused on the machine learn-
ing perspective instead of the linguistic knowledge
embraced in the parsers. Little study has been re-
ported on approaches of incorporating linguistic
features to make the parser less dependent on the
nature of training and testing dataset, without re-
sorting to huge amount of unlabeled out-domain
data. In addition, most of the previous work have
been focusing on constituent-based parsing, while
the domain adaptation of the dependency parsing
has not been fully explored.
Taking a different approach towards parsing,
grammar-based parsers appear to have much
linguistic knowledge encoded within the gram-
mars. In recent years, several of these linguisti-
cally motivated grammar-driven parsing systems
achieved high accuracy which are comparable to
the treebank-based statistical parsers. Notably are
the constraint-based linguistic frameworks with
mathematical rigor, and provide grammatical anal-
yses for a large variety of phenomena. For in-
stance, the Head-Driven Phrase Structure Gram-
mar (Pollard and Sag, 1994) has been success-
fully applied in several parsing systems for more
than a dozen of languages. Some of these gram-
mars, such as the English Resource Grammar
(ERG; Flickinger (2002)), have undergone over
decades of continuous development, and provide
precise linguistic analyses for a broad range of
phenomena. These linguistic knowledge are en-
coded in highly generalized form according to lin-
guists? reflection for the target languages, and tend
to be largely independent from any specific do-
main.
The main issue of parsing with precision gram-
mars is that broad coverage and high precision on
linguistic phenomena do not directly guarantee ro-
bustness of the parser with noisy real world texts.
Also, the detailed linguistic analysis is not always
of the highest interest to all NLP applications. It
is not always straightforward to scale down the
detailed analyses embraced by deep grammars to
a shallower representation which is more acces-
sible for specific NLP tasks. On the other hand,
since the dependency representation is relatively
theory-neutral, it is possible to convert from other
frameworks into its backbone representation in de-
pendencies. For HPSG, this is further assisted by
the clear marking of head daughters in headed
phrases. Although the statistical components of
the grammar-driven parser might be still biased
by the training domain, the hand-coded grammar
rules guarantee the basic linguistic constraints to
be met. This not to say that domain adaptation is
379
HPSG DBExtraction
HPSG DBFeature Models
MSTParserFeature Model
MaltParserFeature Model
Section 3.1
Section 3.3
McDonald
et al, 2005
Nivre
et al, 2007
Nivre andMcDonald,2008
Section 4.2
Section 4.3
Figure 1: Different dependency parsing models
and their combinations. DB stands for dependency
backbone.
not an issue for grammar-based parsing systems,
but the built-in linguistic knowledge can be ex-
plored to reduce the performance drop in pure sta-
tistical approaches.
3 Dependency Parsing with HPSG
In this section, we explore two possible applica-
tions of the HPSG parsing onto the syntactic de-
pendency parsing task. One is to extract depen-
dency backbone from the HPSG analyses of the
sentences and directly convert them into the tar-
get representation; the other way is to encode the
HPSG outputs as additional features into the ex-
isting statistical dependency parsing models. In
the previous work, Nivre and McDonald (2008)
have integrated MSTParser and MaltParser
by feeding one parser?s output as features into the
other. The relationships between our work and
their work are roughly shown in Figure 1.
3.1 Extracting Dependency Backbone from
HPSG Derivation Tree
Given a sentence, each parse produced by the
parser is represented by a typed feature structure,
which recursively embeds smaller feature struc-
tures for lower level phrases or words. For the
purpose of dependency backbone extraction, we
only look at the derivation tree which corresponds
to the constituent tree of an HPSG analysis, with
all non-terminal nodes labeled by the names of the
grammar rules applied. Figure 2 shows an exam-
ple. Note that all grammar rules in ERG are ei-
ther unary or binary, giving us relatively deep trees
when compared with annotations such as Penn
Treebank. Conceptually, this conversion is sim-
ilar to the conversions from deeper structures to
GR reprsentations reported by Clark and Curran
(2007) and Miyao et al (2007).
np_title_cmpnd
ms_n2 proper_np
subjh
generic_proper_ne
Haag
play_v1
hcomp
proper_np
generic_proper_ne
Elianti.
playsMs.
Figure 2: An example of an HPSG derivation tree
with ERG
Ms. Haag plays Elianti.
hcompnp_title_cmpnd subjh
Figure 3: An HPSG dependency backbone struc-
ture
The dependency backbone extraction works by
first identifying the head daughter for each bi-
nary grammar rule, and then propagating the head
word of the head daughter upwards to their par-
ents, and finally creating a dependency relation, la-
beled with the HPSG rule name of the parent node,
from the head word of the parent to the head word
of the non-head daughter. See Figure 3 for an ex-
ample of such an extracted backbone.
For the experiments in this paper, we used July-
08 version of the ERG, which contains in total
185 grammar rules (morphological rules are not
counted). Among them, 61 are unary rules, and
124 are binary. Many of the binary rules are
clearly marked as headed phrases. The gram-
mar also indicates whether the head is on the left
(head-initial) or on the right (head-final). How-
ever, there are still quite a few binary rules which
are not marked as headed-phrases (according to
the linguistic theory), e.g. rules to handle coor-
dinations, appositions, compound nouns, etc. For
these rules, we refer to the conversion of the Penn
Treebank into dependency structures used in the
CoNLL 2008 Shared Task, and mark the heads of
these rules in a way that will arrive at a compat-
ible dependency backbone. For instance, the left
most daughters of coordination rules are marked
as heads. In combination with the right-branching
analysis of coordination in ERG, this leads to the
same dependency attachment in the CoNLL syn-
tax. Eventually, 37 binary rules are marked with
a head daughter on the left, and 86 with a head
daughter on the right.
Although the extracted dependency is similar to
380
the CoNLL shared task dependency structures, mi-
nor systematic differences still exist for some phe-
nomena. For example, the possessive ??s? is an-
notated to be governed by its preceding word in
CoNLL dependency; while in HPSG, it is treated as
the head of a ?specifier-head? construction, hence
governing the preceding word in the dependency
backbone. With several simple tree rewriting
rules, we are able to fix the most frequent inconsis-
tencies. With the rule-based backbone extraction
and repair, we can finally turn our HPSG parser
outputs into dependency structures1. The unla-
beled attachment agreement between the HPSG
backbone and CoNLL dependency annotation will
be shown in Section 4.2.
3.2 Robust Parsing with HPSG
As mentioned in Section 2, one pitfall of using a
precision-oriented grammar in parsing is its lack
of robustness. Even with a large scale broad cover-
age grammar like ERG, using our settings we only
achieved 75% of sentential coverage2. Given that
the grammar has never been fine-tuned for the fi-
nancial domain, such coverage is very encourag-
ing. But still, the remaining unparsed sentences
comprise a big coverage gap.
Different strategies can be taken here. One
can either keep the high precision by only look-
ing at full parses from the HPSG parser, of which
the analyses are completely admitted by gram-
mar constraints. Or one can trade precision for
extra robustness by looking at the most proba-
ble incomplete analysis. Several partial parsing
strategies have been proposed (Kasper et al, 1999;
Zhang and Kordoni, 2008) as the robust fallbacks
for the parser when no available analysis can be
derived. In our experiment, we select the se-
quence of most likely fragment analyses accord-
ing to their local disambiguation scores as the par-
tial parse. When combined with the dependency
backbone extraction, partial parses generate dis-
joint tree fragments. We simply attach all frag-
ments onto the virtual root node.
1It is also possible map from HPSG rule names (together
with the part-of-speech of head and dependent) to CoNLL
dependency labels. This remains to be explored in the future.
2More recent study shows that with carefully designed
retokenization and preprocessing rules, over 80% sentential
coverage can be achieved on the WSJ sections of the Penn
Treebank data using the same version of ERG. The numbers
reported in this paper are based on a simpler preprocessor,
using rather strict time/memory limits for the parser. Hence
the coverage number reported here should not be taken as an
absolute measure of grammar performance.
3.3 Using Feature-Based Models
Besides directly using the dependency backbone
of the HPSG output, we could also use it for build-
ing feature-based models of statistical dependency
parsers. Since we focus on the domain adapta-
tion issue, we incorporate a less domain dependent
language resource (i.e. the HPSG parsing outputs
using ERG) into the features models of statistical
parsers. As mordern grammar-based parsers has
achieved high runtime efficency (with our HPSG
parser parsing at an average speed of?3 sentences
per second), this adds up to an acceptable over-
head.
3.3.1 Feature Model with MSTParser
As mentioned before, MSTParser is a graph-
based statistical dependency parser, whose learn-
ing procedure can be viewed as the assignment
of different weights to all kinds of dependency
arcs. Therefore, the feature model focuses on each
kind of head-child pair in the dependency tree, and
mainly contains four categories of features (Mc-
donald et al, 2005a): basic uni-gram features, ba-
sic bi-gram features, in-between POS features, and
surrounding POS features. It is emphasized by the
authors that the last two categories contribute a
large improvement to the performance and bring
the parser to the state-of-the-art accuracy.
Therefore, we extend this feature set by adding
four more feature categories, which are similar to
the original ones, but the dependency relation was
replaced by the dependency backbone of the HPSG
outputs. The extended feature set is shown in Ta-
ble 1.
3.3.2 Feature Model with MaltParser
MaltParser is another trend of dependency
parser, which is based on transitions. The learning
procedure is to train a statistical model, which can
help the parser to decide which operation to take at
each parsing status. The basic data structures are a
stack, where the constructed dependency graph is
stored, and an input queue, where the unprocessed
data are put. Therefore, the feature model focuses
on the tokens close to the top of the stack and also
the head of the queue.
Provided with the original features used in
MaltParser, we add extra ones about the top
token in the stack and the head token of the queue
derived from the HPSG dependency backbone.
The extended feature set is shown in Table 2 (the
new features are listed separately).
381
Uni-gram Features: h-w,h-p; h-w; h-p; c-w,c-p; c-w; c-p
Bi-gram Features: h-w,h-p,c-w,c-p; h-p,c-w,c-p; h-w,c-w,c-p; h-w,h-p,c-p; h-w,h-p,c-w; h-w,c-w; h-p,c-p
POS Features of words in between: h-p,b-p,c-p
POS Features of words surround: h-p,h-p+1,c-p-1,c-p; h-p-1,h-p,c-p-1,c-p; h-p,h-p+1,c-p,c-p+1; h-p-1,h-p,c-p,c-p+1
Table 1: The Extra Feature Set for MSTParser. h: the HPSG head of the current token; c: the current
token; b: each token in between; -1/+1: the previous/next token; w: word form; p: POS
POS Features: s[0]-p; s[1]-p; i[0]-p; i[1]-p; i[2]-p; i[3]-p
Word Form Features: s[0]-h-w; s[0]-w; i[0]-w; i[1]-w
Dependency Features: s[0]-lmc-d; s[0]-d; s[0]-rmc-d; i[0]-lmc-d
New Features: s[0]-hh-w; s[0]-hh-p; s[0]-hr; i[0]-hh-w; i[0]-hh-p; i[0]-hr
Table 2: The Extended Feature Set for MaltParser. s[0]/s[1]: the first and second token on the top of
the stack; i[0]/i[1]/i[2]/i[3]: front tokens in the input queue; h: head of the token; hh: HPSG DB head of
the token; w: word form; p: POS; d: dependency relation; hr: HPSG rule; lmc/rmc: left-/right-most child
With the extra features, we hope that the train-
ing of the statistical model will not overfit the in-
domain data, but be able to deal with domain in-
dependent linguistic phenomena as well.
4 Experiment Results & Error Analyses
To evaluate the performance of our different
dependency parsing models, we tested our ap-
proaches on several dependency treebanks for En-
glish in a similar spirit to the CoNLL 2006-2008
Shared Tasks. In this section, we will first de-
scribe the datasets, then present the results. An
error analysis is also carried out to show both pros
and cons of different models.
4.1 Datasets
In previous years of CoNLL Shared Tasks, sev-
eral datasets have been created for the purpose
of dependency parser evaluation. Most of them
are converted automatically from existing tree-
banks in various forms. Our experiments adhere
to the CoNLL 2008 dependency syntax (Yamada
et al 2003, Johansson et al 2007) which was
used to convert Penn-Treebank constituent trees
into single-head, single-root, traceless and non-
projective dependencies.
WSJ This dataset comprises of three portions.
The larger part is converted from the Penn Tree-
bank Wall Street Journal Sections #2?#21, and
is used for training statistical dependency parsing
models; the smaller part, which covers sentences
from Section #23, is used for testing.
Brown This dataset contains a subset of con-
verted sentences from BROWN sections of the
Penn Treebank. It is used for the out-domain test.
PChemtb This dataset was extracted from the
PennBioIE CYP corpus, containing 195 sentences
from biomedical domain. The same dataset has
been used for the domain adaptation track of the
CoNLL 2007 Shared Task. Although the original
annotation scheme is similar to the Penn Treebank,
the dependency extraction setting is slightly dif-
ferent to the CoNLLWSJ dependencies (e.g. the
coordinations).
Childes This is another out-domain test set from
the children language component of the TalkBank,
containing dialogs between parents and children.
This is the other datasets used in the domain adap-
tation track of the CoNLL 2007 Shared Task. The
dataset is annotated with unlabeled dependencies.
As have been reported by others, several system-
atic differences in the original CHILDES annota-
tion scheme has led to the poor system perfor-
mances on this track of the Shared Task in 2007.
Two main differences concern a) root attach-
ments, and b) coordinations. With several sim-
ple heuristics, we change the annotation scheme of
the original dataset to match the Penn Treebank-
based datasets. The new dataset is referred to as
CHILDES*.
4.2 HPSG Backbone as Dependency Parser
First we test the agreement between HPSG depen-
dency backbone and CoNLL dependency. While
approximating a target dependency structure with
rule-based conversion is not the main focus of this
work, the agreement between two representations
gives indication on how similar and consistent the
two representations are, and a rough impression of
whether the feature-based models can benefit from
the HPSG backbone.
382
# sentence ? w/s DB(F)% DB(P)%
WSJ 2399 24.04 50.68 63.85
BROWN 425 16.96 66.36 76.25
PCHEMTB 195 25.65 50.27 61.60
CHILDES* 666 7.51 67.37 70.66
WSJ-P 1796 (75%) 22.25 71.33 ?
BROWN-P 375 (88%) 15.74 80.04 ?
PCHEMTB-P 147 (75%) 23.99 69.27 ?
CHILDES*-P 595 (89%) 7.49 73.91 ?
Table 3: Agreement between HPSG dependency
backbone and CoNLL 2008 dependency in unla-
beled attachment score. DB(F): full parsing mode;
DB(P): partial parsing mode; Punctuations are ex-
cluded from the evaluation.
The PET parser, an efficient parser HPSG parser
is used in combination with ERG to parse the
test sets. Note that the training set is not used.
The grammar is not adapted for any of these spe-
cific domain. To pick the most probable read-
ing from HPSG parsing outputs, we used a dis-
criminative parse selection model as described
in (Toutanova et al, 2002) trained on the LOGON
Treebank (Oepen et al, 2004), which is signifi-
cantly different from any of the test domain. The
treebank contains about 9K sentences for which
HPSG analyses are manually disambiguated. The
difference in annotation make it difficult to sim-
ply merge this HPSG treebank into the training set
of the dependency parser. Also, as Gildea (2001)
suggests, adding such heterogeneous data to the
training set will not automatically lead to perfor-
mance improvement. It should be noted that do-
main adaptation also presents a challenge to the
disambiguation model of the HPSG parser. All
datasets we use in our should be considered out-
domain to the HPSG disambiguation model.
Table 3 shows the agreement between the HPSG
backbone and CoNLL dependency in unlabeled at-
tachment score (UAS). The parser is set in either
full parsing or partial parsing mode. Partial pars-
ing is used as a fallback when full parse is not
available. UAS are reported on all complete test
sets, as well as fully parsed subsets (suffixed with
?-p?).
It is not surprising to see that, without a de-
cent fallback strategy, the full parse HPSG back-
bone suffers from insufficient coverage. Since the
grammar coverage is statistically correlated to the
average sentence length, the worst performance is
observed for the PCHEMTB. Although sentences
in CHILDES* are significantly shorter than those
in BROWN, there is a fairly large amount of less
well-formed sentences (either as a nature of child
language, or due to the transcription from spoken
dialogs). This leads to the close performance be-
tween these two datasets. PCHEMTB appears to be
the most difficult one for the HPSG parser. The
partial parsing fallback sets up a good safe net for
sentences that fail to parse. Without resorting to
any external resource, the performance was sig-
nificantly improved on all complete test sets.
When we set the coverage of the HPSG gram-
mar aside and only compare performance on the
subsets of these datasets which are fully parsed
by the HPSG grammar, the unlabeled attachment
score jumps up significantly. Most notable is
that the dependency backbone achieved over 80%
UAS on BROWN, which is close to the perfor-
mance of state-of-the-art statistical dependency
parsing systems trained on WSJ (see Table 5 and
Table 4). The performance difference across data
sets correlates to varying levels of difficulties in
linguists? view. Our error analysis does confirm
that frequent errors occur in WSJ test with finan-
cial terminology missing from the grammar lexi-
con. The relative performance difference between
the WSJ and BROWN test is contrary to the results
observed for statistical parsers trained on WSJ.
To further investigate the effect of HPSG parse
disambiguation model on the dependency back-
bone accuracy, we used a set of 222 sentences
from section of WSJ which have been parsed with
ERG and manually disambiguated. Comparing
to the WSJ-P result in Table 3, we improved the
agreement with CoNLL dependency by another
8% (an upper-bound in case of a perfect disam-
biguation model).
4.3 Statistical Dependency Parsing with
HPSG Features
Similar evaluations were carried out for the statis-
tical parsers using extra HPSG dependency back-
bone as features. It should be noted that the per-
formance comparison between MSTParser and
MaltParser is not the aim of this experiment,
and the difference might be introduced by the spe-
cific settings we use for each parser. Instead, per-
formance variance using different feature models
is the main subject. Also, performance drop on
out-domain tests shows how domain dependent
the feature models are.
For MaltParser, we use Arc-Eager algo-
383
rithm, and polynomial kernel with d = 2. For
MSTParser, we use 1st order features and a pro-
jective decoder (Eisner, 1996).
When incorporating HPSG features, two set-
tings are used. The PARTIAL model is derived by
robust-parsing the entire training data set and ex-
tract features from every sentence to train a uni-
fied model. When testing, the PARTIAL model is
used alone to determine the dependency structures
of the input sentences. The FULL model, on the
other hand is only trained on the full parsed subset
of sentences, and only used to predict dependency
structures for sentences that the grammar parses.
For the unparsed sentences, the original models
without HPSG features are used.
Parser performances are measured using
both labeled and unlabeled attachment scores
(LAS/UAS). For unlabeled CHILDES* data, only
UAS numbers are reported. Table 4 and 5 summa-
rize results for MSTParser and MaltParser,
respectively.
With both parsers, we see slight performance
drops with both HPSG feature models on in-
domain tests (WSJ), compared with the original
models. However, on out-domain tests, full-parse
HPSG feature models consistently outperform the
original models for both parsers. The difference is
even larger when only the HPSG fully parsed sub-
sets of the test sets are concerned. When we look
at the performance difference between in-domain
and out-domain tests for each feature model, we
observe that the drop is significantly smaller for
the extended models with HPSG features.
We should note that we have not done any
feature selection for our HPSG feature models.
Nor have we used the best known configurations
of the existing parsers (e.g. second order fea-
tures in MSTParser). Admittedly the results on
PCHEMTB are lower than the best reported results
in CoNLL 2007 Shared Task, we shall note that we
are not using any in-domain unlabeled data. Also,
the poor performance of the HPSG parser on this
dataset indicates that the parser performance drop
is more related to domain-specific phenomena and
not general linguistic knowledge. Nevertheless,
the drops when compared to in-domain tests are
constantly decreased with the help of HPSG analy-
ses features. With the results on BROWN, the per-
formance of our HPSG feature models will rank
2nd on the out-domain test for the CoNLL 2008
Shared Task.
Unlike the observations in Section 4.2, the par-
tial parsing mode does not work well as a fall-
back in the feature models. In most cases, its
performances are between the original models and
the full-parse HPSG feature models. The partial
parsing features obscure the linguistic certainty of
grammatical structures produced in the full model.
When used as features, such uncertainty leads
to further confusion. Practically, falling back to
the original models works better when HPSG full
parse is not available.
4.4 Error Analyses
Qualitative error analysis is also performed. Since
our work focuses on the domain adaptation, we
manually compare the outputs of the original sta-
tistical models, the dependency backbone, and the
feature-based models on the out-domain data, i.e.
the BROWN data set (both labeled and unlabeled
results) and the CHILDES* data set (only unlabeled
results).
For the dependency attachment (i.e. unlabeled
dependency relation), fine-grained HPSG features
do help the parser to deal with colloquial sen-
tences, such as ?What?s wrong with you??. The
original parser wrongly takes ?what? as the root of
the dependency tree and ??s? is attached to ?what?.
The dependency backbone correctly finds out the
root, and thus guide the extended model to make
the right prediction. A correct structure of ?...,
were now neither active nor really relaxed.? is also
predicted by our model, while the original model
wrongly attaches ?really? to ?nor? and ?relaxed?
to ?were?. The rich linguistic knowledge from
the HPSG outputs also shows its usefulness. For
example, in a sentence from the CHILDES* data,
?Did you put dolly?s shoes on??, the verb phrase
?put on? can be captured by the HPSG backbone,
while the original model attaches ?on? to the adja-
cent token ?shoes?.
For the dependency labels, the most diffi-
culty comes from the prepositions. For example,
?Scotty drove home alone in the Plymouth?, all
the systems get the head of ?in? correct, which
is ?drove?. However, none of the dependency la-
bels is correct. The original model predicts the
?DIR? relation, the extended feature-based model
says ?TMP?, but the gold standard annotation is
?LOC?. This is because the HPSG dependency
backbone knows that ?in the Plymouth? is an ad-
junct of ?drove?, but whether it is a temporal or
384
Original PARTIAL FULL
LAS% UAS% LAS% UAS% LAS% UAS%
WSJ 87.38 90.35 87.06 90.03 86.87 89.91
BROWN 80.46 (-6.92) 86.26 (-4.09) 80.55 (-6.51) 86.17 (-3.86) 80.92 (-5.95) 86.58 (-3.33)
PCHEMTB 53.37 (-33.8) 62.11 (-28.24) 54.69 (-32.37) 64.09 (-25.94) 56.45 (-30.42) 65.77 (-24.14)
CHILDES* ? 72.17 (-18.18) ? 74.91 (-15.12) ? 75.64 (-14.27)
WSJ-P 87.86 90.88 87.78 90.85 87.12 90.25
BROWN-P 81.58 (-6.28) 87.41 (-3.47) 81.92 (-5.86) 87.51 (-3.34) 82.14 (-4.98) 87.80 (-2.45)
PCHEMTB-P 56.32 (-31.54) 65.26 (-25.63) 59.36 (-28.42) 69.20 (-21.65) 60.69 (-26.43) 70.45 (-19.80)
CHILDES*-P ? 72.88 (-18.00) ? 76.02 (-14.83) ? 76.76 (-13.49)
Table 4: Performance of the MSTParser with different feature models. Numbers in parentheses are
performance drops in out-domain tests, comparing to in-domain results. The upper part represents the
results on the complete data sets, and the lower part is on the fully parsed subsets, indicated by ?-P?.
Original PARTIAL FULL
LAS% UAS% LAS% UAS% LAS% UAS%
WSJ 86.47 88.97 85.39 88.10 85.66 88.40
BROWN 79.41 (-7.06) 84.75 (-4.22) 79.10 (-6.29) 84.58 (-3.52) 79.56 (-6.10) 85.24 (-3.16)
PCHEMTB 61.05 (-25.42) 71.32 (-17.65) 61.01 (-24.38) 70.99 (-17.11) 60.93 (-24.73) 70.89 (-17.51)
CHILDES* ? 74.97 (-14.00) ? 75.64 (-12.46) ? 76.18 (-12.22)
WSJ-P 86.99 89.58 86.09 88.83 85.82 88.76
BROWN-P 80.43 (-6.56) 85.78 (-3.80) 80.46 (-5.63) 85.94 (-2.89) 80.62 (-5.20) 86.38 (-2.38)
PCHEMTB-P 63.33 (-23.66) 73.54 (-16.04) 63.27 (-22.82) 73.31 (-15.52) 63.16 (-22.66) 73.06 (-15.70)
CHILDES*-P ? 75.95 (-13.63) ? 77.05 (-11.78) ? 77.30 (-11.46)
Table 5: Performance of the MaltParser with different feature models.
locative expression cannot be easily predicted at
the pure syntactic level. This also suggests a joint
learning of syntactic and semantic dependencies,
as proposed in the CoNLL 2008 Shared Task.
Instances of wrong HPSG analyses have also
been observed as one source of errors. For most of
the cases, a correct reading exists, but not picked
by our parse selection model. This happens more
often with the WSJ test set, partially contributing
to the low performance.
5 Conclusion & Future Work
Similar to our work, Sagae et al (2007) also con-
sidered the combination of dependency parsing
with an HPSG parser, although their work was to
use statistical dependency parser outputs as soft
constraints to improve the HPSG parsing. Nev-
ertheless, a similar backbone extraction algorithm
was used to map between different representa-
tions. Similar work also exists in the constituent-
based approaches, where CFG backbones were
used to improve the efficiency and robustness of
HPSG parsers (Matsuzaki et al, 2007; Zhang and
Kordoni, 2008).
In this paper, we restricted our investigation on
the syntactic evaluation using labeled/unlabeled
attachment scores. Recent discussions in the
parsing community about meaningful cross-
framework evaluation metrics have suggested to
use measures that are semantically informed. In
this spirit, Zhang et al (2008) showed that the se-
mantic outputs of the same HPSG parser helps in
the semantic role labeling task. Consistent with
the results reported in this paper, more improve-
ment was achieved on the out-domain tests in their
work as well.
Although the experiments presented in this pa-
per were carried out on a HPSG grammar for En-
glish, the method can be easily adapted to work
with other grammar frameworks (e.g. LFG, CCG,
TAG, etc.), as well as on langugages other than
English. We chose to use a hand-crafted grammar,
so that the effect of training corpus on the deep
parser is minimized (with the exception of the lex-
ical coverage and disambiguation model).
As mentioned in Section 4.4, the performance
of our HPSG parse selection model varies across
different domains. This indicates that, although
the deep grammar embraces domain independent
linguistic knowledge, the lexical coverage and the
disambiguation process among permissible read-
ings is still domain dependent. With the map-
ping between HPSG analyses and their depen-
dency backbones, one can potentially use existing
dependency treebanks to help overcome the insuf-
ficient data problem for deep parse selection mod-
els.
385
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. Map adaptation of stochastic grammars.
Computer speech and language, 20(1):41?68.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the 10th Conference on Computational Natural Lan-
guage Learning (CoNLL-X), New York City, USA.
Stephen Clark and James Curran. 2007. Formalism-
independent parser evaluation with ccg and depbank. In
Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 248?255, Prague,
Czech Republic.
Jason Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of the
16th International Conference on Computational Linguis-
tics (COLING-96), pages 340?345, Copenhagen, Den-
mark.
Dan Flickinger. 2002. On building a more efficient grammar
by exploiting types. In Stephan Oepen, Dan Flickinger,
Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative
Language Engineering, pages 1?17. CSLI Publications.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Language Processing, pages
167?202, Pittsburgh, USA.
Walter Kasper, Bernd Kiefer, Hans-Ulrich Krieger, C.J.
Rupp, and Karsten Worm. 1999. Charting the depths of
robust speech processing. In Proceedings of the 37th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL 1999), pages 405?412, Maryland, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of english: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Efficient HPSG parsing with supertagging and CFG-
filtering. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI 2007), pages
1671?1676, Hyderabad, India.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics, pages
337?344, Sydney, Australia.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 561?568,
Manchester, UK.
Ryan Mcdonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL?05),
pages 91?98, Ann Arbor, Michigan.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005b. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007. To-
wards framework-independent evaluation of deep linguis-
tic parsers. In Proceedings of the GEAF07 Workshop,
pages 238?258, Stanford, CA.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In Pro-
ceedings of ACL-08: HLT, pages 950?958, Columbus,
Ohio, June.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007a.
The CoNLL 2007 shared task on dependency parsing.
In Proceedings of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov, and
Erwin Marsi. 2007b. Maltparser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(1):1?41.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer,
Torbj?rn Nordga?rd, and Victoria Rose?n. 2004. Som a?
kapp-ete med trollet? Towards MRS-Based Norwegian?
English Machine Translation. In Proceedings of the 10th
International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, USA.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007. Hpsg
parsing with shallow dependency constraints. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 624?631, Prague,
Czech Republic.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th Conference
on Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Kristina Toutanova, Christoper D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse
ranking for a rich HPSG grammar. In Proceedings of the
1st Workshop on Treebanks and Linguistic Theories (TLT
2002), pages 253?263, Sozopol, Bulgaria.
Yi Zhang and Valia Kordoni. 2008. Robust Parsing with a
Large HPSG Grammar. In Proceedings of the Sixth Inter-
national Language Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CoNLL 2008), pages 198?202, Manchester, UK.
386
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 128?135,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Partial Parse Selection for Robust Deep Processing
Yi Zhang? and Valia Kordoni? and Erin Fitzgerald?
? Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
? Center for Language & Speech Processing,
Dept of Electrical & Computer Engineering, Johns Hopkins University, USA
{yzhang,kordoni}@coli.uni-sb.de
erin@clsp.jhu.edu
Abstract
This paper presents an approach to partial
parse selection for robust deep processing.
The work is based on a bottom-up chart
parser for HPSG parsing. Following the def-
inition of partial parses in (Kasper et al,
1999), different partial parse selection meth-
ods are presented and evaluated on the basis
of multiple metrics, from both the syntactic
and semantic viewpoints. The application
of the partial parsing in spontaneous speech
texts processing shows promising compe-
tence of the method.
1 Introduction
Linguistically deep processing is of high theoret-
ical and application interest because of its ability
to deliver fine-grained accurate analyses of natu-
ral language sentences. Unlike shallow methods
which usually return analyses for any input, deep
processing methods with precision grammars nor-
mally make a clear grammaticality judgment on in-
puts, therefore avoiding the generation of erroneous
analyses for less well-formed inputs. This is a desir-
able feature, for it allows for a more accurate mod-
eling of language itself.
However, this feature largely limits the robustness
of deep processing, for when a sentence is judged
to be ungrammatical, normally no analysis is gen-
erated. When faced with the noisy inputs in real
applications (e.g., input errors introduced by speech
recognizers or other pre-processors, mildly ungram-
matical sentences with fragmental utterances, self-
editing chunks or filler words in spoken texts, and
so forth), lack of robustness means poor coverage,
and makes deep processing less competitive as com-
pared to shallow methods.
Take the English Resource Grammar
(ERG; Flickinger (2000)), a large-scale accu-
rate HPSG for English, for example. (Baldwin et
al., 2004) reported coverage of 57% of the strings
with full lexical span from the British National
Corpus (BNC). Although recent extensions to the
grammar and lexicon have improved the coverage
significantly, full coverage over unseen texts by the
grammar is still not anywhere in sight.
Other domains are even more likely to not fit
into ERG?s universe, such as transcripts of sponta-
neously produced speech where speaker errors and
disfluencies are common. Using a recent version of
the ERG, we are not able to parse 22.6% of a ran-
dom sample of 500 utterances of conversational tele-
phone speech data. 76.1% of the unparsed data was
independently found to contain speaker errors and
disfluencies, and the remaining data either contained
filled pauses or other structures unaccounted for in
the grammar. Correctly recognizing and interpreting
the substrings in the utterance which have coherent
deep syntax is useful both for semantic analysis and
as building blocks for attempts to reconstruct the dis-
fluent spontaneously produced utterances into well-
formed sentences.
For these reasons, it is preferable to exploit the
intermediate syntactic and semantic analysis even if
the full analysis is not available. Various efforts have
been made on the partiality of language processing.
In bottom-up chart parsing, the passive parser edges
licensed by the grammar can be taken as partial anal-
yses. However, as pointed out in (Kasper et al,
1999), not all passive edges are good candidates, as
not all of them provide useful syntactic/semantic in-
formation. Moreover, the huge amount of passive
edges suggests the need for a technique of select-
ing an optimal subset of them. During recent devel-
opment in statistical parse disambiguation, the use
of log-linear models has been pretty much standard-
ized. However, it remains to be explored whether the
techniques can be adapted for partial parse selection.
In this paper, we adopt the same definition for
partial parse as in (Kasper et al, 1999) and de-
fine the task of partial parse selection. Several dif-
128
ferent partial parse selection models are presented
and implemented for an efficient HPSG parser ?
PET (Callmeier, 2001).
One of the main difficulties in the research of par-
tial analyses is the lack of good evaluation measure-
ments. Pure syntactic comparisons for parser eval-
uation are not good as they are very much specific
to the annotation guidelines. Also, the deep gram-
mars we are working with are not automatically ex-
tracted from annotated corpora. Therefore, unless
there are partial treebanks built specifically for the
deep grammars, there is simply no ?gold? standard
for non-golden partial analyses.
Instead, in this paper, we evaluate the partial anal-
yses results on the basis of multiple metrics, from
both the syntactic and semantic point of views. Em-
pirical evaluation has been done with the ERG on a
small set of texts from the Wall Street Journal Sec-
tion 22 of the Penn Treebank (Marcus et al, 1993).
A pilot study of applying partial parsing in sponta-
neous speech text processing is also carried out.
The remainder of the paper is organized as fol-
low. Section 2 provides background knowledge
about partial analysis. Section 3 presents various
partial parse selection models. Section 4 describes
the evaluation setup and results. Section 5 concludes
the paper.
2 Partial Parsing
2.1 HPSG Parsing
Our work on partial parsing is done with the
DELPH-IN HPSG grammars. Many of these gram-
mars can be used for both parsing and generation.
In this paper, we only focus on the parsing task. For
efficient parsing, we use PET.1 The parsing module
in PET is essentially a bottom-up chart parser. The
parsing process is guided by the parsing tasks on an
agenda. A parsing task represents the combination
of a passive chart edge and an active chart edge or
a rule. When the combination succeeds, new tasks
are generated and put on to the agenda. The parser
terminates either when the task agenda is empty or
when a specific number of full analyses has been
found (only in the no-packing best-first mode).
HPSG grammars use typed feature structures (TF-
Ses) as their background formalism. The TFSes rep-
resent various linguistic objects with a set of fea-
1LKB (Copestake, 2002) has a similar chart-based parser,
being less efficient mainly due to its implementation in Lisp
rather than C/C++.
tures (attribute value pairs) and a type inheritance
system. Therefore, each passive edge on the parsing
chart corresponds to a TFS. A relatively small set of
highly generalized rules are used to check the com-
patibility among smaller TFSes and build up larger
ones.
2.2 Partial Parses
Based on the bottom-up chart parsing, we use the
term Partial Parse to describe a set of intermediate
passive parsing edges whose spans (beginning and
end positions) are non-overlapping between each
other, and together they cover the entire input se-
quence (i.e., no skipped input tokens).
In a graph view, the intermediate results of a chart
parser can be described as a directed graph, where
all positions between input tokens/words are ver-
tices, and all the passive edges derived during pars-
ing are the directed graph arcs. Obviously such a
graph is acyclic and therefore topologically sorted.
A partial parse is then a path from the source vertex
(the beginning position of the input) to the terminal
vertex (the end position of the input).
Suppose in chart parsing, we derived the interme-
diate results as in Figure 1. There are in total 4 pos-
sible partial parses: {a, b, c, d}, {a, b, f}, {a, e, d}
and {a, g}.
1w 2w 3w 4w0 1 2 3 4
ba c d
e
g
f
Figure 1: Graph representation of intermediate chart
parsing results
Note that each passive edge is a sub-structure li-
censed by the grammar. A derivation tree or TFS can
be reconstructed for it if required. This definition of
partial parse is effectively the same to the view of
partial analyses in (Kasper et al, 1999).
2.3 Local Ambiguity Packing
There is one more complication concerning the par-
tial parses when the local ambiguity packing is used
in the parser.
Due to the inherent ambiguity of natural lan-
guage, the same sequence of input may be ana-
lyzed as the same linguistic object in different ways.
Such intermediate analyses must be recorded dur-
ing the processing and recovered in later stages.
129
Without any efficient processing technique, parsing
becomes computationally intractable with the com-
binatory explosion of such local ambiguities. In
PET, the subsumption-based ambiguity packing al-
gorithm proposed in (Oepen and Carroll, 2000) is
used. This separates the parsing into two phases:
forest creation phase and read-out/unpacking phase.
In relation to the work on partial parsing in this
paper, the local ambiguity packing poses an effi-
ciency and accuracy challenge, as not all the inter-
mediate parsing results are directly available as pas-
sive edges on the chart. Without unpacking the am-
biguity readings, interesting partial analyses might
be lost.2 But exhaustively unpacking all the readings
will pay back the efficiency gain by ambiguity pack-
ing, and eventually lead to computational intractable
results.
To efficiently recover the ambiguous readings
from packed representations, the selective unpack-
ing algorithm has been recently implemented as an
extension to the algorithm described in (Carroll and
Oepen, 2005). It is able to recover the top-n best
readings of a given passive parser edge based on the
score assigned by a maximum entropy parse rank-
ing model. This neat feature largely facilitates the
efficient searching for best partial parses described
in later sections.
3 Partial Parse Selection
A partial parse is a set of partial analyses licensed
by the grammar which cover the entire input without
overlapping. As shown in the previous section, there
are usually more than one possible partial parses
for a given input. For deep linguistic processing, a
high level of local ambiguity means there are even
more partial parses due to the combinatory explo-
sion. However, not all the possible partial parses are
equally good. Some partial parses partition the in-
put into fragments that do not correspond to linguis-
tic constituents. Even if the bracketing is correct,
the different edges with the same span represent sig-
nificantly different linguistic objects, and their sub-
structures can be completely different, as well. All
these indicate the need for methods that can appro-
priately select the best partial parses from all the
possible ones.
In this section, we review some of the previous
2More informative analyses are subsumed by less informa-
tive ones. In subsumption-based packing, such analyses are
packed and are not directly accessible.
approaches to partial parse selection, as well as new
partial parse ranking models.
3.1 Longest Edge
One of the simplest and most commonly used cri-
terion in selecting the best partial parse is to prefer
the partial parses which contain an edge that covers
the largest fragment of the input. For example, un-
der such a criterion, the best partial parse in Figure 1
will be {a, g}, since edge g has the largest span. The
logic behind this criterion is that such largest frag-
ments should preserve the most interesting linguistic
analysis of the input. As an added incentive, finding
the longest edge does not involve much search.
The limitations of such an approach are obvious.
There is no guarantee that the longest edge will be
significantly better than shorter edges, or that it will
even correspond to a valid constituent. Moreover,
when there are multiple edges with the same length
(which is often the case in parsing), the criterion
does not suffice for the choice of the best partial
parse.
3.2 Shortest Path
(Kasper et al, 1999) proposed an alternative solu-
tion to the problem. If the preference of each edge
as a part of the partial parse can be quantitatively de-
cided as a weight of the edge (with smaller weights
assigned to better candidates), then the problem of
finding the best partial parse is to find the shortest
path from the start vertex to the end vertex. Since
the graph is completely connected (by the lexical
edges spanning all the input tokens) and topolog-
ically sorted, such a path always exists. The dis-
covery of such a path can be done in linear time
(O(|V | + |E|)) with the DAG-shortest-path algo-
rithm (Cormen et al, 1990). Though not explic-
itly pointed out by (Kasper et al, 1999), such an
algorithm allows the weights of the edges to be of
any real value (no assumption of positive weights)
as long as the graph is a Directed Acyclic Graph
(DAG).
(Kasper et al, 1999) did point out that the weights
of the edges can be assigned by an estimation func-
tion. For example, the implementation of the al-
gorithm in PET preferred phrasal edges over lexi-
cal edges. Other types of edges are not allowed in
the partial parse. Suppose that we assign weight 1
to phrasal edges, 2 to lexical edges, and inf to all
other edges. Then for the graph in 2, the best par-
tial parses are {e, g} and {f, g}, both of which have
130
the path length of 2. It should be noted that such an
approach does not always favor the paths with the
longest edges (i.e., path {h, d} is not preferred in
the given example).
1w 2w 3w 4w0 1 2 3 4
b c
e g
h
d
f
a :2 :2 :2:2
:1 :1
:1
i 8:1 :
Figure 2: Shortest path partial parses with heuristi-
cally assigned edge weights
However, (Kasper et al, 1999) did not pro-
vide any sophisticated estimation functions based
on the shortest path approach. Using the heuristic
weight described above, usually thousands of differ-
ent paths are found with the same weight. (Kasper
et al, 1999) rely on another scoring function in or-
der to re-rank the partial parses. Although different
requirements for the scoring function are discussed,
no further details have been defined.
It should be noted that different variations of the
shortest path approach are widely in use in many ro-
bust deep parsing systems. For instance, (Riezler et
al., 2002) uses the fewest chunk method to choose
the best fragment analyses for sentences without
full analysis. The well-formed chunks are preferred
over token chunks. With this partial parse selection
method, the grammar achieves 100% coverage on
unseen data. A similar approach is also used in (van
Noord et al, 1999).
3.3 Alternative Estimation Functions
Generally speaking, the weights of the edges in the
shortest path approach represent the quality of the
local analyses and their likelihood of appearing in
the analysis of the entire input.
This is an interesting parallel to the parse selec-
tion models for the full analyses, where a goodness
score is usually assigned to the full analysis. For
example, the parse disambiguation model described
in (Toutanova et al, 2002) uses a maximum entropy
approach to model the conditional probability of a
parse for a given input sequence P (t|w). A similar
approach has also been reported in (Johnson et al,
1999; Riezler et al, 2002; Malouf and van Noord,
2004).
For a given partial parse ? = {t1, . . . , tk}, ? =
{w1, . . . , wk} is a segmentation of the input se-
quence so that each local analysis ti ? ? corre-
sponds to a substring wi ? ? of the input sequence
w. Therefore, the probability of the partial parse ?
given an input sequence w is:
P (?|w) = P (?|w) ? P (?|?) (1)
With the bold assumption that P (ti|wi) are mutually
independent for different i, we can derive:
P (?|w) ? P (?|w) ?
k
?
i=1
P (ti|wi) (2)
Therefore, the log-probability will be
logP (?|w) ? logP (?|w) +
k
?
i=1
logP (ti|wi) (3)
Equation 3 indicates that the log-probability of a
partial parse for a given input is the sum of the log-
probability of local analyses for the sub-strings, with
an additional component ? log P (?|w) represent-
ing the conditional log-probability of the segmen-
tation. If we use ? logP (ti|wi) as the weight for
each local analysis, then the DAG shortest path al-
gorithm will quickly find the partial parse that max-
imizes log P (?|w) ? logP (?|w).
The probability P (ti|wi) can be modeled in a sim-
ilar way to the maximum entropy based full parse
selection models:
P (ti|wi) =
exp ?nj=1 ?jfj(ti, wi)
?
t??T exp
?n
j=1 ?jfj(t?, wi)
(4)
where T is the set of all possible structures that
can be assigned to wi, f1 . . . fn are the features and
?1 . . . ?n are the parameters. The parameters can
be efficiently estimated from a treebank, as shown
by (Malouf, 2002). The only difference from the
full parse selection model is that here intermediate
results are used to generate events for training the
model (i.e. the intermediate nodes are used as posi-
tive events if it occurs on one of the active tree, or as
negative events if not). Since there is a huge number
of intermediate results availalbe, we only randomly
select a part of them as training data. This is es-
sentially similar to the approach in (Osborne, 2000),
where there is an infeasibly large number of training
events, only part of which is used in the estimation
step. The exact features used in the log-linear model
can significantly influence the disambiguation accu-
racy. In this experiment we used the same features
131
as those used in the PCFG-S model in (Toutanova et
al., 2002) (i.e., depth-1 derivation trees).
The estimation of P (?|w) is more difficult. In
a sense it is similar to a segmentation or chunking
model, where the task is to segment the input into
fragments. However, it is difficult to collect train-
ing data to directly train such a model for the deep
grammar we have. Here we take a simple rough es-
timation:
P? (?|w) = |Y (?)||Z(w)| (5)
where Y (?) is the set of all partial parses that have
the segmentation ?; Z(w) is the set of all partial
parses for the input w.
Unfortunately, the shortest path algorithm is not
able to directly find the maximized P (?|w). Fully
searching all the paths is not practical, since there
are usually tens of thousands of passive edges. In
order to achieve a balance between accuracy and ef-
ficiency, two different approximation approaches are
taken.
One way is to assume that the component
log P (?|w) in Equation 3 has less significant ef-
fect on the quality of the partial parse. If this is
valid, then we can simply use ? log P (ti|wi) as edge
weights, and use the shortest path algorithm to ob-
tain the best ?. This will be referred to as model
I.
An alternative way is to first retrieve several
?good? ? with relatively high P (?|w), and then se-
lect the best edges ti that maximize P (ti|wi) for
each wi in ?. We call this approach the model II.
How well these strategies work will be evaluated
in Section 4. Other strategies or more sophisticated
searching algorithms (e.g., genetic algorithm) can
also be used, but we will leave that to future re-
search.
3.4 Partial Semantic Construction
For each local analysis on the partial parse derived in
the above steps, a semantic fragment can be derived.
The HPSG grammars we use take a compositional
approach to semantic construction. Minimal Re-
cursion Semantics (MRS; Copestake et al (2006))
is used for semantic representation. MRS can be
easily converted to (Robust) MRS (RMRS; Copes-
take (2006)), which allows further underspecifica-
tion, and can be used for integration of deep and/or
shallow processing tools.
For robust deep processing, the ability to gener-
ate partial semantics is very important. Moreover, it
also provides us with a way to evaluate the partial
parses which is more or less independent from the
syntactic analysis.
4 Evaluation
The evaluation of partial parses is not as easy as the
evaluation of full parses. For full parsers, there are
generally two ways of evaluation. For parsers that
are trained on a treebank using an automatically ex-
tracted grammar, an unseen set of manually anno-
tated data is used as the test set. The parser out-
put on the test set is compared to the gold standard
annotation, either with the widely used PARSEVAL
measurement, or with more annotation-neutral de-
pendency relations. For parsers based on manually
compiled grammars, more human judgment is in-
volved in the evaluation. With the evolution of the
grammar, the treebank as the output from the gram-
mar changes over time (Oepen et al, 2002). The
grammar writer inspects the parses generated by the
grammar and either ?accepts? or ?rejects? the anal-
ysis.
In partial parsing for manually compiled gram-
mars, the criterion for acceptable analyses is less
evident. Most current treebanking tools are not de-
signed for annotating partial analyses. Large-scale
manually annotated treebanks do have the annota-
tion for sentences that deep grammars are not able
to fully analyze. And the annotation difference in
other language resources makes the comparison less
straightforward. More complication is involved with
the platform and resources used in our experiment.
Since the DELPH-IN grammars (ERG, JaCY, GG)
use MRS for semantics representation, there is no
reliable way of evaluating the output with traditional
metrics, i.e., dependency relations.
In this paper, we use both manual and automatic
evaluation methods on the partial parsing results.
Different processing resources are used to help the
evaluation from the syntactic, as well as the seman-
tic point of view.
4.1 Syntactic Evaluation
In order to evaluate the quality of the syntactic struc-
tures of the partial parses, we implemented the par-
tial parse models described in the previous section
in the PET parser. The Nov-06 version of the ERG
is used for the experiment. As test set, we used a
132
subset of sentences from the Wall Street Journal Sec-
tion 22 from the Penn Treebank. The subset contains
143 sentences which do not receive any full analysis
licensed by the grammar, and do not contain lexi-
cal gaps (input tokens for which the grammar can-
not create any lexical edge). The average sentence
length is 24 words.
Due to the inconsistency of the tokenisation,
bracketing and branching between the Penn Tree-
bank annotation and the handling in ERG, we manu-
ally checked the partial parse derivation trees. Each
output is marked as one of the three cases: GBL if
both the bracketing and the labeling of the partial
parse derivation trees are good (with no more than
two brackets crossing or four false labelings); GB if
the bracketings of the derivation trees are good (with
no more than two brackets crossing), but the label-
ing is bad (with more than four false labelings); or E
if otherwise.
The manual evaluation results are listed in Ta-
ble 1. The test set is processed with two models
presented in Section 3.3 (M-I for model I, M-II
for model II). For comparison, we also evaluate for
the approach using the shortest path with heuristic
weights (denoted by SP). In case there are more than
one path found with the same weight, only the first
one is recorded and evaluated.
GBL GB E
# % # % # %
SP 55 38.5% 64 44.8% 24 16.8%
M-I 61 42.7% 46 32.2% 36 25.2%
M-II 74 51.7% 50 35.0% 19 13.3%
Table 1: Syntactic Evaluation Results
The results show that the na??ve shortest path ap-
proach based on the heuristic weights works pretty
well at predicting the bracketing (with 83.3% of the
partial parses having less than two brackets cross-
ing). But, when the labeling is also evaluated it is
worse than model I, and even more significantly out-
performed by model II.
4.2 Semantic Evaluation
Evaluation of the syntactic structure only reflects the
partial parse quality from some aspects. In order
to get a more thorough comparison between differ-
ent selection models, we look at the semantic output
generated from the partial parses.
The same set of 143 sentences from the Wall
Street Journal Section 22 of the Penn Treebank is
used. The RMRS semantic representations are gen-
erated from the partial parses with different selection
models. To compare with, we used RASP 2 (Briscoe
et al, 2006), a domain-independent robust parsing
system for English. According to (Briscoe and Car-
roll, 2006), the parser achieves fairly good accuracy
around 80%. The reasons why we choose RASP
for the evaluation are: i) RASP has reasonable cov-
erage and accuracy; ii) its output can be converted
into RMRS representation with the LKB system.
Since there is no large scale (R)MRS treebank with
sentences not covered by the DELPH-IN precision
grammars, we hope to use the RASP?s RMRS out-
put as a standalone annotation to help the evaluation
of the different partial parse selection models.
To compare the RMRS from the RASP and the
partial parse selection models, we used the simi-
larity measurement proposed in (Dridan and Bond,
2006). The comparison outputs a distance value be-
tween two different RMRSes. We normalized the
distance value to be between 0 and 1. For each se-
lection model, the average RMRS distance from the
RASP output is listed in Table 2.
RMRS Dist.(?)
SP 0.674
M-I 0.330
M-II 0.296
Table 2: RMRS distance to RASP outputs
Again, we see that the outputs of model II
achieve the highest similarity when compared with
the RASP output. With some manual validation,
we do confirm that the different similarity does im-
ply a significant difference in the quality of the out-
put RMRS. The shortest path with heuristic weights
yielded very poor semantic similarity. The main rea-
son is that not every edge with the same span gen-
erates the same semantics. Therefore, although the
SP receives reasonable bracketing accuracy, it has
less idea of the goodness of different edges with the
same span. By incorporating P (ti|wi) in the scoring
model, the model I and II can produce RMRSes with
much higher quality.
4.3 Evaluating partial parses on spontaneous
speech text
The above evaluation shows in a comparative way
that model II outperforms other selection models
from both syntactic and semantic points of view. In
order to show its competence in real applications,
133
we applied the best performing model II on sponta-
neous speech transcripts, which have a high level of
informality and irregularity not available in newspa-
per texts such as the Wall Street Journal.
To evaluate the accuracy and potential interpre-
tational value of partial parsing on spontaneous
speech transcripts, we considered a 100-sentence
random sample of the Fisher Conversational Tele-
phone Speech 2004 development subcorpus (Cieri
et al, 2004), used in the fall 2004 NIST Rich Tran-
scription task.
Of these 100 sentences, six utterances received
neither full nor partial parses due to lexical gaps cre-
ated by words not found in the grammar?s lexicon.3
75 utterances produced full HPSG parses. For the
remaining 19 utterances, the one best partial parse is
found for each using model II.
According to manual evaluation of the output, se-
mantically and syntactically cohesive partial analy-
ses were successfully assigned to 9 of the 19 par-
tially parsed utterances. 3 of the 19 received incom-
plete semantics. The remaining 7 were judged to
be poor due to false segmentation, the syntax and
semantics within those parsed fragments, or both.
In one instance, the interpretation was plausible but
viewed as far less likely by the evaluator than the
preferable interpretation (?. . . [i think you know it it ?s]
[court]?4). It is likely that n-best partial parsing could
help us in most cases. This would only require a
straightforward extension of the current partial pars-
ing models.
Current partial parsing models do not use any con-
fidence thresholds. Therefore, any input will receive
some full or partial analysis (ignoring the case of
unknown words), together with semantics. Seman-
tic completeness is not checked in partial parsing. In
future research, we may consider finding a sophisti-
cated solution of assigning confidence scores to the
output RMRS fragments.
Overall though, we believe that the current 50%
acceptability of segmentation is reasonable perfor-
mance considering the types of noise in the speech
transcript input.
As a further step to show the competence of par-
tial parsing, we briefly investigated its application
in capturing disfluent regions in speech texts. The
state of the art approach in identifying disfluent re-
3Lexical prediction was not used here to avoid obfuscating
the quality of partial parsing by introducing lexical type predic-
tion errors.
4The repetition error of ?it? is interpreted as a topicalization.
gions and potentially capturing meaningful text is a
shallow parsing method described in (Johnson and
Charniak, 2004), which searches the text string for
approximately repeated constituents. We ran their
system on our random sample of the Fisher data, and
compared its results to the partial parse output of the
nine well-segmented partial parses analyses (every
utterance of which contained some speaker-induced
disfluency) to see how well partial parsing could po-
tentially fare as an approach for identifying disfluent
regions of speech text.
Often the (Johnson and Charniak, 2004) method
identified disfluent regions overlapped with identi-
fied fragments found in the partial parse, the removal
of which would yield a fluent sentence. As we hope
to learn confidence measures to determine which
fragments are contentless or repetitive in the fu-
ture, we identified those partial parses where whole
fragments could be deleted to obtain a fluent and
meaning-preserving sentence.
In three cases, simple repeated phrases caught by
(Johnson and Charniak, 2004) were also caught in
some form by the partial parse partitioning. In an-
other case, the speaker interrupts one thought to say
another, and both approaches identify in a single
fragment the final fluent statement. Finally, of the
nine well-segmented utterances, two partial parses
potentially catch deeper speaker errors that cannot
be caught by (Johnson and Charniak, 2004).
5 Conclusion and Future Work
In this paper, we have presented work on partial
parse selection. Different selection models have
been presented and evaluated from syntactic and
semantic viewpoints. In the application of spon-
taneous speech text processing, the method shows
promising competence, as well as a few problems
for further study.
One thing we did not do is a systematic compar-
ison on the efficiency of different partial parse se-
lection models. Although it is clear that less search-
ing is involved with the shortest path approach and
model I comparing to model II, a scientific bench-
marking of such difference will be helpful for the
choice between efficiency and accuracy. Also, a
more sophisticated estimation of P (?|w) can poten-
tially help the accuracy of the selection models.
Another alternative way of evaluation would be
to generate an ungrammatical corpus by randomly
introducing grammar errors. The performance of the
134
partial parse selection models can be measured by
evaluating how much of the parsing results can be
recovered from original sentences.
In the study with spontaneous speech text pro-
cessing, we see a need for confidence measurement
for partial analyses. We also see that the conditional
probability P (ti|wi) does not serve as a good mea-
surement, for it largely depends on the structures
that can be licensed to wi by the grammar. This
should be explored in future studies, as well.
References
Timothy Baldwin, Emily M. Bender, Dan Flickinger, Ara Kim,
and Stephan Oepen. 2004. Road-testing the English Re-
source Grammar over the British National Corpus. In Pro-
ceedings of the Fourth International Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lisbon.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC DepBank.
In Proceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 41?48, Sydney, Australia.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of the
COLING/ACL 2006 Interactive Presentation Sessions, pages
77?80, Sydney, Australia.
Ulrich Callmeier. 2001. Efficient parsing with large-scale uni-
fication grammars. Master?s thesis, Universita?t des Saarlan-
des, Saarbru?cken, Germany.
John Carroll and Stephan Oepen. 2005. High efficiency realiza-
tion for a wide-coverage unification grammar. In Proceed-
ings of the Second International Joint Conference on Natu-
ral Language Processing (IJCNLP05), pages 165?176, Jeju
Island, Korea.
Christopher Cieri, Stephanie Strassel, Mohamed Maamouri,
Shudong Huang, James Fiumara, David Graff, Kevin
Walker, and Mark L iberman. 2004. Linguistic resource
creation and distribution for EARS. In Proceedings of the
Rich Transcription Fall Workshop (RT-04F).
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2006. Minimal Recursion Semantics: an Introduction. Re-
search on Language and Computation, 3(4):281?332.
Ann Copestake. 2002. Implementing Typed Feature Structure
Grammars. CSLI, Stanford, CA.
Ann Copestake. 2006. Robust Minimal Recursion Se-
mantics. Working Paper, Unpublished Draft 2004/2006,
http://www.cl.cam.ac.uk/ aac10/papers.html.
Thomas H. Cormen, Charles E. Leiserson, and Ronald L.
Rivest. 1990. Introduction to Algorithms. MIT Press, MA.
Rebecca Dridan and Francis Bond. 2006. Sentence compari-
son using Robust Minimal Recursion Semantics and an on-
tology. In Proceedings of the ACL Workshop on Linguistic
Distances, pages 35?42, Sydney, Australia.
Dan Flickinger. 2000. On building a more efficient grammar by
exploiting types. Natural Language Engineering, 6(1):15?
28.
Mark Johnson and Eugene Charniak. 2004. A tag-based noisy-
channel model of speech repairs. In Proceedings of the 42nd
Meeting of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 33?39, Barcelona, Spain.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic unifcation-
based grammars. In Proceedings of the 37th Annual Meeting
of the ACL, pages 535?541, Maryland.
Walter Kasper, Bernd Kiefer, Hans-Ulrich Krieger, C.J. Rupp,
and Karsten Worm. 1999. Charting the depths of robust
speech processing. In Proceedings of the 37th Meeting of the
Association for Computational Linguistics (ACL?99), Main
Volume, pages 405?412, Maryland, USA, June.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of the
Sixth Conferencde on Natural Language Learning (CoNLL-
2002), pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English. The Penn Treebank. Computational Linguistics,
19:313?330.
Stephan Oepen and John Carroll. 2000. Ambiguity packing in
constraint-based parsing ? practical results. In Proceedings
of the 1st Conference of the North American Chapter of the
ACL, pages 162?169, Seattle, WA.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher
Manning, Dan Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods treebank: Motivation and preliminary ap-
plications. In Proceedings of COLING 2002: The 17th Inter-
national Conference on Computational Linguistics: Project
Notes, Taipei.
Miles Osborne. 2000. Estimation of Stochastic Attribute-Value
Grammars using an Informative Sample. In The 18th In-
ternational Conference on Computational Linguistics (COL-
ING 2000), volume 1, pages 586?592, Saarbru?cken.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. III Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and Discriminative Estimation Techniques. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 271?278, Philadelphia.
Kristina Toutanova, Christoper D. Manning, Stuart M. Shieber,
Dan Flickinger, and Stephan Oepen. 2002. Parse rank-
ing for a rich HPSG grammar. In Proceedings of the First
Workshop on Treebanks and Linguistic Theories (TLT2002),
pages 253?263, Sozopol, Bulgaria.
Gertjan van Noord, Gosse Bouma, Rob Koeling, and Mark-Jan
Nederhof. 1999. Robust grammatical analysis for spoken
dialogue systems. Natural language engineering, 5(1):45?
93.
135
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 152?159,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Corpus and the Lexicon: Standardising Deep Lexical Acquisition
Evaluation
Yi Zhang? and Timothy Baldwin? and Valia Kordoni?
? Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
{yzhang,kordoni}@coli.uni-sb.de
tim@csse.unimelb.edu.au
Abstract
This paper is concerned with the standard-
isation of evaluation metrics for lexical ac-
quisition over precision grammars, which
are attuned to actual parser performance.
Specifically, we investigate the impact that
lexicons at varying levels of lexical item
precision and recall have on the perfor-
mance of pre-existing broad-coverage pre-
cision grammars in parsing, i.e., on their
coverage and accuracy. The grammars used
for the experiments reported here are the
LinGO English Resource Grammar (ERG;
Flickinger (2000)) and JACY (Siegel and
Bender, 2002), precision grammars of En-
glish and Japanese, respectively. Our re-
sults show convincingly that traditional F-
score-based evaluation of lexical acquisition
does not correlate with actual parsing per-
formance. What we argue for, therefore, is a
recall-heavy interpretation of F-score in de-
signing and optimising automated lexical ac-
quisition algorithms.
1 Introduction
Deep processing is the process of applying rich lin-
guistic resources within NLP tasks, to arrive at a
detailed (=deep) syntactic and semantic analysis of
the data. It is conventionally driven by deep gram-
mars, which encode linguistically-motivated predic-
tions of language behaviour, are usually capable of
both parsing and generation, and generate a high-
level semantic abstraction of the input data. While
enjoying a resurgence of interest due to advances
in parsing algorithms and stochastic parse prun-
ing/ranking, deep grammars remain an underutilised
resource predominantly because of their lack of cov-
erage/robustness in parsing tasks. As noted in previ-
ous work (Baldwin et al, 2004), a significant cause
of diminished coverage is the lack of lexical cover-
age.
Various attempts have been made to ameliorate
the deficiencies of hand-crafted lexicons. More
recently, there has been an explosion of interest
in deep lexical acquisition (DLA; (Baldwin, 2005;
Zhang and Kordoni, 2006; van de Cruys, 2006))
for broad-coverage deep grammars, either by ex-
ploiting the linguistic information encoded in the
grammar itself (in vivo), or by using secondary lan-
guage resources (in vitro). Such approaches provide
(semi-)automatic ways of extending the lexicon with
minimal (or no) human interference.
One stumbling block in DLA research has been
the lack of standardisation in evaluation, with
commonly-used evaluation metrics including:
? Type precision: the proportion of correctly hy-
pothesised lexical entries
? Type recall: the proportion of gold-standard
lexical entries that are correctly hypothesised
? Type F-measure: the harmonic mean of the
type precision and type recall
? Token Accuracy: the accuracy of the lexical en-
tries evaluated against their token occurrences
in gold-standard corpus data
It is often the case that the different measures lead
to significantly different assessments of the quality
of DLA, even for a given DLA approach. Addi-
tionally, it is far from clear how the numbers gen-
erated by these evaluation metrics correlate with ac-
tual parsing performance when the output of a given
DLA method is used. This makes standardised com-
parison among the various different approaches to
DLA very difficult, if not impossible. It is far from
clear which evaluation metrics are more indicative of
the true ?goodness? of the lexicon. The aim of this
research, therefore, is to analyse how the different
evaluation metrics correlate with actual parsing per-
formance using a given lexicon, and to work towards
152
a standardised evaluation framework for future DLA
research to ground itself in.
In this paper, we explore the utility of different
evaluation metrics at predicting parse performance
through a series of experiments over two broad cov-
erage grammars: the English Resource Grammar
(ERG; Flickinger (2000)) and JACY (Siegel and
Bender, 2002). We simulate the results of DLA
by generating lexicons at different levels of preci-
sion and recall, and test the impact of such lexicons
on grammar coverage and accuracy related to gold-
standard treebank data. The final outcome of this
analysis is a proposed evaluation framework for fu-
ture DLA research.
The remainder of the paper is organised as fol-
lows: Section 2 reviews previous work on DLA for
the robust parsing task; Section 3 describes the ex-
perimental setup; Section 4 presents the experiment
results; Section 5 analyses the experiment results;
Section 6 concludes the paper.
2 Lexical Acquisition in Deep Parsing
Hand-crafted large-scale grammars are error-prone.
An error can be roughly classified as undergenerat-
ing (if it prevents a grammatical sentence from be-
ing generated/parsed) or overgenerating (if it allows
an ungrammatical sentence to be generated/parsed).
Hence, errors in deep grammar lexicons can be clas-
sified into two categories: i) a lexical entry is miss-
ing for a specific lexeme; and ii) an erroneous lexical
entry enters the lexicon. The former error type will
cause the grammar to fail to parse/generate certain
sentences (i.e. undergenerate), leading to a loss in
coverage. The latter error type will allow the gram-
mar to parse/generate inappropriate sentences (i.e.
overgenerate), potentially leading to a loss in ac-
curacy. In the first instance, we will be unable to
parse sentences involving a given lexical item if it is
missing from our lexicon, i.e. coverage will be af-
fected assuming the lexical item of interest occurs
in a given corpus. In the second instance, the im-
pact is indeterminate, as certain lexical items may
violate constraints in the grammar and never be li-
cenced, whereas others may be licenced more lib-
erally, generating competing (incorrect) parses for a
given input and reducing parse accuracy. It is these
two competing concerns that we seek to quantify in
this research.
Traditionally, errors in the grammar are detected
manually by the grammar developers. This is usu-
ally done by running the grammar over a carefully
designed test suite and inspecting the outputs. This
procedure becomes less reliable as the grammar gets
larger. Also we can never expect to attain complete
lexical coverage, due to language evolution and the
effects of domain/genre. A static, manually com-
piled lexicon, therefore, becomes inevitably insuffi-
cient when faced with open domain text.
In recent years, some approaches have been de-
veloped to (semi-)automatically detect and/or repair
the lexical errors in linguistic grammars. Such ap-
proaches can be broadly categorised as either sym-
bolic or statistical.
Erbach (1990), Barg and Walther (1998) and
Fouvry (2003) followed a unification-based sym-
bolic approach to unknown word processing for
constraint-based grammars. The basic idea is to
use underspecified lexical entries, namely entries
with fewer constraints, to parse whole sentences,
and generate the ?real? lexical entries afterwards by
collecting information from the full parses. How-
ever, lexical entries generated in this way may be ei-
ther too general or too specific. Underspecified lex-
ical entries with fewer constraints allow more gram-
mar rules to be applied while parsing, and fully-
underspecified lexical entries are computationally
intractable. The whole procedure gets even more
complicated when two unknown words occur next
to each other, potentially allowing almost any con-
stituent to be constructed. The evaluation of these
proposals has tended to be small-scale and some-
what brittle. No concrete results have been pre-
sented relating to the improvement in grammar per-
formance, either for parsing or for generation.
Baldwin (2005) took a statistical approach to au-
tomated lexical acquisition for deep grammars. Fo-
cused on generalising the method of deriving DLA
models on various secondary language resources,
Baldwin used a large set of binary classifiers to pre-
dict whether a given unknown word is of a particular
lexical type. This data-driven approach is grammar
independent and can be scaled up for large gram-
mars. Evaluation was via type precision, type recall,
type F-measure and token accuracy, resulting in dif-
ferent interpretations of the data depending on the
evaluation metric used.
Zhang and Kordoni (2006) tackled the robustness
problem of deep processing from two aspects. They
employed error mining techniques in order to semi-
automatically detect errors in deep grammars. They
then proposed a maximum entropy model based lex-
153
ical type predictor, to generate new lexical entries
on the fly. Evaluation focused on the accuracy of
the lexical type predictor over unknown words, not
the overall goodness of the resulting lexicon. Simi-
larly to Baldwin (2005), the methods are applicable
to other constraint-based lexicalist grammars, but no
direct measurement of the impact on grammar per-
formance was attempted.
van de Cruys (2006) took a similar approach over
the Dutch Alpino grammar (cf. Bouma et al (2001)).
Specifically, he proposed a method for lexical ac-
quisition as an extension to automatic parser error
detection, based on large amounts of raw text (cf.
van Noord (2004)). The method was evaluated us-
ing type precision, type recall and type F-measure.
Once again, however, these numbers fail to give us
any insight into the impact of lexical acquisition on
parser performance.
Ideally, we hope the result of DLA to be both ac-
curate and complete. However, in reality, there will
always be a trade-off between coverage and parser
accuracy. Exactly how these two concerns should be
balanced up depends largely on what task the gram-
mar is applied to (i.e. parsing or generation). In this
paper, we focus exclusively on the parsing task.1
3 Experimental Setup
In this research, we wish to evaluate the impact
of different lexicons on grammar performance. By
grammar performance, we principally mean cov-
erage and accuracy. However, it should be noted
that the efficiency of the grammar?e.g. the aver-
age number of edges in the parse chart, the average
time to parse a sentence and/or the average number
of analyses per sentence?is also an important per-
formance measurement which we expect the quality
of the lexicon to impinge on. Here, however, we
expect to be able to call on external processing opti-
misations2 to dampen any loss in efficiency, in a way
which we cannot with coverage and accuracy.
3.1 Resources
In order to get as representative a set of results as
possible, we choose to run the experiment over two
1In generation, we tend to have a semantic representation
as input, which is linked to pre-existing lexical entries. Hence,
lexical acquisition has no direct impact on generation.
2For example, (van Noord, 2006) shows that a HMM POS
tagger trained on the parser outputs can greatly reduce the lexi-
cal ambiguity and enhance the parser efficiency, without an ob-
servable decrease in parsing accuracy.
large-scale HPSGs (Pollard and Sag, 1994), based
on two distinct languages.
The LinGO English Resource Grammar (ERG;
Flickinger (2000)) is a broad-coverage, linguis-
tically precise HPSG-based grammar of English,
which represents the culmination of more than 10
person years of (largely) manual effort. We use the
jan-06 version of the grammar, which contains about
23K lexical entries and more than 800 leaf lexical
types.
JACY (Siegel and Bender, 2002) is a broad-
coverage linguistically precise HPSG-based gram-
mar of Japanese. In our experiment, we use the
November 2005 version of the grammar, which con-
tains about 48K lexical entries and more than 300
leaf lexical types.
It should be noted in HPSGs, the grammar is
made up of two basic components: the grammar
rules/type hierarchy, and the lexicon (which inter-
faces with the type hierarchy via leaf lexical types).
This is different to strictly lexicalised formalisms
like LTAG and CCG, where essentially all linguistic
description resides in individual lexical entries in the
lexicon. The manually compiled grammars in our
experiment are also intrinsically different to gram-
mars automatically induced from treebanks (e.g. that
used in the Charniak parser (Charniak, 2000) or the
various CCG parsers (Hockenmaier, 2006)). These
differences sharply differentiate our work from pre-
vious research on the interaction between lexical ac-
quisition and parse performance.
Furthermore, to test the grammar precision and
accuracy, we use two treebanks: Redwoods (Oepen
et al, 2002) for English and Hinoki (Bond et al,
2004) for Japanese. These treebanks are so-called
dynamic treebanks, meaning that they can be (semi-
)automatically updated when the grammar is up-
dated. This feature is especially useful when we
want to evaluate the grammar performance with dif-
ferent lexicon configurations. With conventional
treebanks, our experiment is difficult (if not impos-
sible) to perform as the static trees in the treebank
cannot be easily synchronised to the evolution of the
grammar, meaning that we cannot regenerate gold-
standard parse trees relative to a given lexicon (es-
pecially when for reduced recall where there is no
guarantee we will be able to produce all of the parses
in the 100% recall gold-standard). As a result, it is
extremely difficult to faithfully update the statistical
models.
The Redwoods treebank we use is the 6th growth,
154
which is synchronised with the jan-06 version of the
ERG. It contains about 41K test items in total.
The Hinoki treebank we use is updated for the
November 2005 version of the JACY grammar. The
?Rei? sections we use in our experiment contains
45K test items in total.
3.2 Lexicon Generation
To simulate the DLA results at various levels of pre-
cision and recall, a random lexicon generator is used.
In order to generate a new lexicon with specific pre-
cision and recall, the generator randomly retains a
portion of the gold-standard lexicon, and generates a
pre-determined number of erroneous lexical entries.
More specifically, for each grammar we first ex-
tract a subset of the lexical entries from the lexicon,
each of which has at least one occurrence in the tree-
bank. This subset of lexical entries is considered to
be the gold-standard lexicon (7,156 entries for the
ERG, 27,308 entries for JACY).
Given the gold-standard lexicon L, the target pre-
cision P and recall R, a new lexicon L? is created,
which is composed of two disjoint subsets: the re-
tained part of the gold-standard lexicon G, and the
erroneous entries E. According to the definitions of
precision and recall:
P = |G||L?| (1) R =
|G|
|L| (2)
and the fact that:
|L?| = |G| + |E| (3)
we get:
|G| = |L| ? R (4)
|E| = |L| ? R ? ( 1P ? 1) (5)
To retain a specific number of entries from the
gold-standard lexicon, we randomly select |G| en-
tries based on the combined probabilistic distribu-
tion of the corresponding lexeme and lexical types.3
We obtain the probabilistic distribution of lexemes
from large corpora (BNC for English and Mainichi
Shimbun [1991-2000] for Japanese), and the distri-
bution of lexical types from the corresponding tree-
banks. For each lexical entry e(l, t) in the gold-
standard lexicon with lexeme l and lexical type t,
3For simplicity, we assume mutual independence of the lex-
emes and lexical types.
the combined probability is:
p(e(l, t)) = CL(l) ? CT (t)?
e?(l?,t?)?L CL(l?) ? CT (t?)
(6)
The erroneous entries are generated in the same
way among all possible combinations of lexemes
and lexical types. The difference is that only open
category types and less frequent lexemes are used
for generating new entries (e.g. we wouldn?t expect
to learn a new lexical item for the lexeme the or the
lexical type d - the le in English). In our ex-
periment, we consider lexical types with more than
a predefined number of lexical entries (20 for the
ERG, 50 for JACY) in the gold-standard lexicon to
be open-class lexical types; the upper-bound thresh-
old on token frequency is set to 1000 for English and
537 for Japanese, i.e. lexemes which occur more fre-
quently than this are excluded from lexical acquisi-
tion under the assumption that the grammar develop-
ers will have attained full coverage of lexical items
for them.
For each grammar, we then generate 9 differ-
ent lexicons at varying precision and recall levels,
namely 60%, 80%, and 100%.
3.3 Parser Coverage
Coverage is an important grammar performance
measurement, and indicates the proportion of inputs
for which a correct parse was obtained (adjudged
relative to the gold-standard parse data in the tree-
banks). In our experiment, we adopt a weak defini-
tion of coverage as ?obtaining at least one spanning
tree?. The reason for this is that we want to obtain
an estimate for novel data (for which we do not have
gold-standard parse data) of the relative number of
strings for which we can expect to be able to produce
at least one spanning parse. This weak definition of
coverage actually provides an upper bound estimate
of coverage in the strict sense, and saves the effort to
manually evaluate the correctness of the parses. Past
evaluations (e.g. Baldwin et al (2004)) have shown
that the grammars we are dealing with are relatively
precise. Based on this, we claim that our results for
parse coverage provide a reasonable estimate indica-
tion of parse coverage in the strict sense of the word.
In principle, coverage will only decrease when
the lexicon recall goes down, as adding erroneous
entries should not invalidate the existing analy-
ses. However, in practice, the introduction of er-
roneous entries increases lexical ambiguity dramati-
155
0.6 0.8 1.0P \ R C E A C E A C E A
0.6 4294 2862 7156 5725 3817 9542 7156 4771 11927
0.8 4294 1073 5367 5725 1431 7156 7156 1789 8945
1.0 4294 0 4294 5725 0 5725 7156 0 7156
Table 1: Different lexicon configurations for the ERG with the number of correct (C), erroneous (E) and
combined (A) entries at each level of precision (P) and recall (R)
0.6 0.8 1.0P \ R C E A C E A C E A
0.6 16385 10923 27308 21846 14564 36410 27308 18205 45513
0.8 16385 4096 20481 21846 5462 27308 27308 6827 34135
1.0 16385 0 16385 21846 0 21846 27308 0 27308
Table 2: Different lexicon configurations for JACY with the number of correct (C), erroneous (E) and
combined (A) entries at each level of precision (P) and recall (R)
cally, readily causing the parser to run out of mem-
ory. Moreover, some grammars use recursive unary
rules which are triggered by specific lexical types.
Here again, erroneous lexical entries can lead to ?fail
to parse? errors.
Given this, we run the coverage tests for the two
grammars over the corresponding treebanks: Red-
woods and Hinoki. The maximum number of pas-
sive edges is set to 10K for the parser. We used
[incr tsdb()] (Oepen, 2001) to handle the dif-
ferent lexicon configurations and data sets, and PET
(Callmeier, 2000) for parsing.
3.4 Parser Accuracy
Another important measurement of grammar perfor-
mance is accuracy. Deep grammars often generate
hundreds of analyses for an input, suggesting the
need for some means of selecting the most probable
analysis from among them. This is done with the
parse disambiguation model proposed in Toutanova
et al (2002), with accuracy indicating the proportion
of inputs for which we are able to accurately select
the correct parse.
The disambiguation model is essentially a maxi-
mum entropy (ME) based ranking model. Given an
input sentence s with possible analyses t1 . . . tk, the
conditional probability for analysis ti is given by:
P (ti|s) =
exp ?mj=1 fj(ti)?j
?k
i?=1 exp
?m
j=1 fj(ti?)?j
(7)
where f1 . . . fm are the features and ?1 . . . ?m
are the corresponding parameters. When ranking
parses,
?m
j=1 fj(ti)?j is the indicator of ?good-
ness?. Drawing on the discriminative nature of the
ME models, various feature types can be incor-
porated into the model. In combination with the
dynamic treebanks where the analyses are (semi-
)automatically disambiguated, the models can be
easily re-trained when the grammar is modified.
For each lexicon configuration, after the cover-
age test, we do an automatic treebank update. Dur-
ing the automatic treebank update, only those new
parse trees which are comparable to the active trees
in the gold-standard treebank are marked as cor-
rect readings. All other trees are marked as in-
active and deemed as overgeneration of the gram-
mar. The ME-based parse disambiguation models
are trained/evaluated using these updated treebanks
with 5-fold cross validation. Since we are only in-
terested in the difference between different lexicon
configurations, we use the simple PCFG-S model
from (Toutanova et al, 2002), which incorporates
PCFG-style features from the derivation tree of the
parse. The accuracy of the disambiguation model
is calculated by top analysis exact matching (i.e. a
ranking is only considered correct if the top ranked
analysis matches the gold standard prefered reading
in the treebank).
All the Hinoki Rei noun sections (about 25K
items) were used in the accuracy evaluation for
JACY. However, due to technical limitations, only
the jh sections (about 6K items) of the Redwoods
Treebank were used for training/testing the disam-
biguation models for the ERG.
4 Experiment Results
The experiment consumes a considerable amount of
computational resources. For each lexicon config-
156
P \ R 0.6 0.8 1.0
0.6 44.56% 66.88% 75.51%
0.8 42.18% 65.82% 75.86%
1.0 40.45% 66.19% 76.15%
Table 3: Parser coverage of JACY with different lex-
icons
P \ R 0.6 0.8 1.0
0.6 27.86% 39.17% 79.66%
0.8 27.06% 37.42% 79.57%
1.0 26.34% 37.18% 79.33%
Table 4: Parser coverage of the ERG with different
lexicons
uration of a given grammar, we need to i) process
(parse) all the items in the treebank, ii) compare the
resulting trees with the gold-standard trees and up-
date the treebank, and iii) retrain the disambiguation
models over 5 folds of cross validation. Given the
two grammars with 9 configurations each, the en-
tire experiment takes over 1 CPU month and about
120GB of disk space.
The coverage results are shown in Table 3 and
Table 4 for JACY and the ERG, respectively.4 As
expected, we see a significant increase in grammar
coverage when the lexicon recall goes up. This in-
crease is more significant for the ERG than JACY,
mainly because the JACY lexicon is about twice as
large as the ERG lexicon; thus, the most frequent
entries are still in the lexicons even with low recall.
When the lexicon recall is fixed, the grammar cov-
erage does not change significantly at different lev-
els of lexicon precision. Recall that we are not eval-
uating the correctness of such parses at this stage.
It is clear that the increase in lexicon recall boosts
the grammar coverage, as we would expect. The
precision of the lexicon does not have a large in-
fluence on coverage. This result confirms that with
DLA (where we hope to enhance lexical coverage
relative to a given corpus/domain), the coverage of
the grammar can be enhanced significantly.
The accuracy results are obtained with 5-fold
cross validation, as shown in Table 5 and Table 6
4Note that even with the lexicons at 100% precision and re-
call level, there is no guarantee of 100% coverage. As the con-
tents of the Redwoods and Hinoki treebanks were determined
independently of the respective grammars, rather than the gram-
mars being induced from the treebanks e.g., they both still con-
tain significant numbers of strings for which the grammar can-
not produce a correct analysis.
P-R #ptree Avg. ?
060-060 13269 62.65% 0.89%
060-080 19800 60.57% 0.83%
060-100 22361 59.61% 0.63%
080-060 14701 63.27% 0.62%
080-080 23184 60.97% 0.48%
080-100 27111 60.04% 0.56%
100-060 15696 63.91% 0.64%
100-080 26859 61.47% 0.68%
100-100 31870 60.48% 0.71%
Table 5: Accuracy of disambiguation models for
JACY with different lexicons
P-R #ptree Avg. ?
060-060 737 71.11% 3.55%
060-080 1093 63.94% 2.75%
060-100 3416 60.92% 1.23%
080-060 742 70.07% 1.50%
080-080 1282 61.81% 3.60%
080-100 3842 59.05% 1.30%
100-060 778 69.76% 4.62%
100-080 1440 60.59% 2.64%
100-100 4689 57.03% 1.36%
Table 6: Accuracy of disambiguation models for the
ERG with different lexicons
for JACY and the ERG, respectively. When the lex-
icon recall goes up, we observe a small but steady
decrease in the accuracy of the disambiguation mod-
els, for both JACY and ERG. This is generally a side
effect of change in coverage: as the grammar cover-
age goes up, the parse trees become more diverse,
and are hence harder to discriminate.
When the recall is fixed and the precision of the
lexicon goes up, we observe a very small accuracy
gain for JACY (around 0.5% for each 20% increase
in precision). This shows that the grammar accu-
racy gain is limited as the precision of the lexicon
increases, i.e. that the disambiguation model is re-
markably robust to the effects of noise.
It should be noted that for the ERG we failed to
observe any accuracy gain at all with a more pre-
cise lexicon. This is partly due to the limited size
of the updated treebanks. For the lexicon config-
uration 060 ? 060, we obtained only 737 preferred
readings/trees to train/test the disambiguation model
over. The 5-fold cross validation results vary within
a margin of 10%, which means that the models are
still not converging. However, the result does con-
firm that there is no significant gain in grammar ac-
curacy with a higher precision lexicon.
Finally, we combine the coverage and accuracy
scores into a single F-measure (? = 1) value. The
results are shown in Figure 1. Again we see that
157
the difference in lexicon recall has a more signif-
icant impact on the overall grammar performance
than precision.
0.4
0.5
0.6
0.7
F-
sc
or
e 
(Ja
CY
)
R=0.6
R=0.8
R=1.0
P=0.6
P=0.8
P=1.0
0.4
0.5
0.6
0.7
0.6 0.8 1.0
F-
sc
or
e 
(E
RG
)
Lex. Precision
R=0.6
R=0.8
R=1.0
0.6 0.8 1.0
Lex. Recall
P=0.6
P=0.8
P=1.0
Figure 1: Grammar performance (F-score) with dif-
ferent lexicons
5 Discussion
5.1 Is F-measure a good metric for DLA
evaluation?
As mentioned in Section 2, a number of relevant ear-
lier works have evaluated DLA results via the un-
weighted F-score (relative to type precision and re-
call). This implicitly assumes that the precision and
recall of the lexicon are equally important. How-
ever, this is clearly not the case as we can see in the
results of the grammar performance. For example,
the lexicon configurations 060 ? 100 and 100 ? 060
of JACY (i.e. 60% precision, 100% recall vs. 100%
precision, 60% recall, respectively) have the same
unweighted F-scores, but their corresponding over-
all grammar performance (parser F-score) differs by
up to 17%.
5.2 Does precision matter?
The most interesting finding in our experiment is
that the precision of the deep lexicon does not ap-
pear to have a significant impact on grammar accu-
racy. This is contrary to the earlier predominant be-
lief that deep lexicons should be as accurate as pos-
sible. This belief is derived mainly from observa-
tion of grammars with relatively small lexicons. In
such small lexicons, the closed-class lexical entries
and frequent entries (which comprise the ?core? of
the lexicon) make up a large proportion of lexical
entries. Hence, any loss in precision means a signif-
icant degradation of the ?core? lexicon, which leads
to performance loss of the grammar. For example,
we find that the inclusion of one or two erroneous
entries for frequent closed-class lexical type words
(such as the, or of in English, for instance) may eas-
ily ?break? the parser.
However, in state-of-the-art broad-coverage deep
grammars such as JACY and ERG, the lexicons are
much larger. They usually have more or less similar
?cores? to the smaller lexicons, but with many more
open-class lexical entries and less frequent entries,
which compose the ?peripheral? parts of the lexi-
cons. In our experiment, we found that more than
95% of the lexical entries belong to the top 5% of
the open-class lexical types. The bigger the lexicon
is, the larger the proportion of lexical entries that be-
long to the ?peripheral? lexicon.
In our experiment, we only change the ?periph-
eral? lexicon by creating/removing lexical entries
for less frequent lexemes and open-class lexical
types, leaving the ?core? lexicon intact. Therefore, a
more accurate interpretation of the experimental re-
sults is that the precision of the open type and less
frequent lexical entries does not have a large impact
on the grammar performance, but their recall has a
crucial effect on grammar coverage.
The consequence of this finding is that the bal-
ance between precision and recall in the deep lexi-
con should be decided by their impact on the task to
which the grammar is applied. In research on auto-
mated DLA, the motivation is to enhance the robust-
ness/coverage of the grammars. This work shows
that grammar performance is very robust over the
inevitable errors introduced by the DLA, and that
more emphasis should be placed on recall.
Again, caution should be exercised here. We
do not mean that by blindly adding lexical entries
without worrying about their correctness, the per-
formance of the grammar will be monotonically en-
hanced ? there will almost certainly be a point at
which noise in the lexicon swamps the parse chart
and/or leads to unacceptable levels of spurious am-
biguity. Also, the balance between precision and re-
call of the lexicon will depend on various expecta-
tions of the grammarians/lexicographers, i.e. the lin-
guistic precision and generality, which is beyond the
scope of this paper.
As a final word of warning, the absolute gram-
mar performance change that a given level of lexi-
158
con type precision and recall brings about will obvi-
ously depend on the grammar. In looking across two
grammars from two very different languages, we are
confident of the robustness of our results (at least for
grammars of the same ilk) and the conclusions that
we have drawn from them. For any novel grammar
and/or formalism, however, the performance change
should ideally be quantified through a set of exper-
iments with different lexicon configurations, based
on the procedure outlined here. Based on this, it
should be possible to find the optimal balance be-
tween the different lexicon evaluation metrics.
6 Conclusion
In this paper, we have investigated the relationship
between evaluation metrics for deep lexical acquisi-
tion and grammar performance in parsing tasks. The
results show that traditional DLA evaluation based
on F-measure is not reflective of grammar perfor-
mance. The precision of the lexicon appears to have
minimal impact on grammar accuracy, and therefore
recall should be emphasised more greatly in the de-
sign of deep lexical acquisition techniques.
References
Timothy Baldwin, Emily Bender, Dan Flickinger, Ara Kim, and
Stephan Oepen. 2004. Road-testing the English Resource
Grammar over the British National Corpus. In Proc. of the
fourth international conference on language resources and
evaluation (LREC 2004), pages 2047?2050, Lisbon, Portu-
gal.
Timothy Baldwin. 2005. Bootstrapping deep lexical resources:
Resources for courses. In Proc. of the ACL-SIGLEX 2005
workshop on deep lexical acquisition, pages 67?76, Ann Ar-
bor, USA.
Petra Barg and Markus Walther. 1998. Processing unknown
words in HPSG. In Proc. of the 36th Conference of the
ACL and the 17th International Conference on Computa-
tional Linguistics, pages 91?95, Montreal, Canada.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: a treebank for text understanding. In Proc. of the
first international joint conference on natural language pro-
cessing (IJCNLP04), pages 554?562, Hainan, China.
Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001.
Alpino: wide-coverage computational analysis of Dutch. In
Computational linguistics in the Netherlands 2000, pages
45?59, Tilburg, the Netherlands.
Ulrich Callmeier. 2000. PET ? a platform for experimentation
with efficient HPSG processing techniques. Natural Lan-
guage Engineering, 6(1):99?107.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proc. of the 1st Annual Meeting of the North Ameri-
can Chapter of Association for Computational Linguistics
(NAACL2000), Seattle, USA.
Gregor Erbach. 1990. Syntactic processing of unknown words.
IWBS Report 131, IBM, Stuttgart, Germany.
Dan Flickinger. 2000. On building a more efficient grammar by
exploiting types. Natural Language Engineering, 6(1):15?
28.
Frederik Fouvry. 2003. Lexicon acquisition with a large-
coverage unification-based grammar. In Proc. of the 10th
Conference of the European Chapter of the Association for
Computational Linguistics (EACL 2003), pages 87?90, Bu-
dapest, Hungary.
Julia Hockenmaier. 2006. Creating a CCGbank and a wide-
coverage CCG lexicon for German. In Proc. of the 21st
International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational
Linguistics, pages 505?512, Sydney, Australia.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher
Manning, Dan Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods treebank: Motivation and preliminary ap-
plications. In Proc. of the 17th international conference on
computational linguistics (COLING 2002), Taipei, Taiwan.
Stephan Oepen. 2001. [incr tsdb()] ? competence and perfor-
mance laboratory. User manual. Technical report, Compu-
tational Linguistics, Saarland University, Saarbru?cken, Ger-
many.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago, USA.
Melanie Siegel and Emily Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proc. of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion, Taipei, Taiwan.
Kristina Toutanova, Christoper Manning, Stuart Shieber, Dan
Flickinger, and Stephan Oepen. 2002. Parse ranking for
a rich HPSG grammar. In Proc. of the First Workshop on
Treebanks and Linguistic Theories (TLT2002), pages 253?
263, Sozopol, Bulgaria.
Tim van de Cruys. 2006. Automatically extending the lexicon
for parsing. In Proc. of the eleventh ESSLLI student session,
pages 180?191, Malaga, Spain.
Gertjan van Noord. 2004. Error mining for wide-coverage
grammar engineering. In Proc. of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04), Main
Volume, pages 446?453, Barcelona, Spain.
Gertjan van Noord. 2006. At Last Parsing Is Now Operational.
In Actes de la 13e conference sur le traitement automatique
des langues naturelles (TALN06), pages 20?42, Leuven, Bel-
gium.
Fei Xia, Chung-Hye Han, Martha Palmer, and Aravind Joshi.
2001. Automatically extracting and comparing lexicalized
grammars for different languages. In Proc. of the 17th Inter-
national Joint Conference on Artificial Intelligence (IJCAI-
2001), pages 1321?1330, Seattle, USA.
Yi Zhang and Valia Kordoni. 2006. Automated deep lexical
acquisition for robust open texts processing. In Proc. of
the fifth international conference on language resources and
evaluation (LREC 2006), pages 275?280, Genoa, Italy.
159
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Combining Multi-Engine Translations with Moses
Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,
Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de
{eisele,cfedermann,uszkoreit}@dfki.de
Abstract
We present a simple method for generating
translations with the Moses toolkit (Koehn
et al, 2007) from existing hypotheses pro-
duced by other translation engines. As
the structures underlying these translation
engines are not known, an evaluation-
based strategy is applied to select sys-
tems for combination. The experiments
show promising improvements in terms of
BLEU.
1 Introduction
With the wealth of machine translation systems
available nowadays (many of them online and
for free), it makes increasing sense to investigate
clever ways of combining them. Obviously, the
main objective lies in finding out how to integrate
the respective advantages of different approaches:
Statistical machine translation (SMT) and rule-
based machine translation (RBMT) systems of-
ten have complementary characteristics. Previous
work on building hybrid systems includes, among
others, approaches using reranking, regeneration
with an SMT decoder (Eisele et al, 2008; Chen
et al, 2007), and confusion networks (Matusov et
al., 2006; Rosti et al, 2007; He et al, 2008).
The approach by (Eisele et al, 2008) aimed
specifically at filling lexical gaps in an SMT sys-
tem with information from a number of RBMT
systems. The output of the RBMT engines was
word-aligned with the input, yielding a total of
seven phrase tables which where simply concate-
nated to expand the phrase table constructed from
the training corpus. This approach differs from the
confusion network approaches mainly in that the
final hypotheses do not necessarily follow any of
the input translations as the skeleton. On the other
hand, it emphasizes that the additional translations
should be produced by RBMT systems with lexi-
cons that cannot be learned from the data.
The present work continues on the same track
as the paper mentioned above but implements a
number of important changes, most prominently
a relaxation of the restrictions on the number and
type of input systems. These differences are de-
scribed in more detail in Section 2. Section 3 ex-
plains the implementation of our system and Sec-
tion 4 its application in a number of experiments.
Finally, Section 5 concludes this paper with a sum-
mary and some thoughts on future work.
2 Integrating Multiple Systems of
Unknown Type and Quality
When comparing (Eisele et al, 2008) to the
present work, our proposal is more general in a
way that the requirement for knowledge about the
systems is minimum. The types and the identities
of the participated systems are assumed unknown.
Accordingly, we are not able to restrict ourselves
to a certain class of systems as (Eisele et al, 2008)
did. We rely on a standard phrase-based SMT
framework to extract the valuable pieces from the
system outputs. These extracted segments are also
used to improve an existing SMT system that we
have access to.
While (Eisele et al, 2008) included translations
from all of a fixed number of RBMT systems
and added one feature to the translation model for
each system, integrating all given system outputs
in this way in our case could expand the search
space tremendously. Meanwhile, we cannot rely
on the assumption that all candidate systems ac-
tually have the potential to improve our baseline.
This implies the need for a first step of system se-
lection where the best candidate systems are iden-
tified and a limited number of them is chosen to be
included in the combination. Our approach would
not work without a small set of tuning data being
available so that we can evaluate the systems for
later selection and adjust the weights of our sys-
tems. Such tuning data is included in this year?s
42
task.
In this paper, we use the Moses decoder to con-
struct translations from the given system outputs.
We mainly propose two slightly different ways:
One is to construct translation models solely from
the given translations and the other is to extend
an existing translation model with these additional
translations.
3 Implementation
Despite the fact that the output of current MT sys-
tems is usually not comparable in quality to hu-
man translations, the machine-generated transla-
tions are nevertheless ?parallel? to the input so
that it is straightforward to construct a translation
model from data of this kind. This is the spirit
behind our method for combining multiple trans-
lations.
3.1 Direct combination
Clearly, for the same source sentence, we expect
to have different translations from different trans-
lation systems, just like we would expect from hu-
man translators. Also, every system may have its
own advantages. We break these translations into
smaller units and hope to be able to select the best
ones and form them into a better translation.
One single translation of a few thousand sen-
tences is normally inadequate for building a re-
liable general-purpose SMT system (data sparse-
ness problem). However, in the system combina-
tion task, this is no longer an issue as the system
only needs to translate sentences within the data
set.
When more translation engines are available,
the size of this set becomes larger. Hence,
we collect translations from all available systems
and pair them with the corresponding input text,
thus forming a medium-sized ?hypothesis? cor-
pus. Our system starts processing this corpus
with a standard phrase-based SMT setup, using the
Moses toolkit (Koehn et al, 2007).
The hypothesis corpus is first tokenized and
lowercased. Then, we run GIZA++ (Och and
Ney, 2003) on the corpus to obtain word align-
ments in both directions. The phrases are extracted
from the intersection of the alignments with the
?grow? heuristics. In addition, we also generate
a reordering model with the default configuration
as included in the Moses toolkit. This ?hypothe-
sis? translation model can already be used by the
Moses decoder together with a language model to
perform translations over the corresponding sen-
tence set.
3.2 Integration into existing SMT system
Sometimes, the goal of system combination is not
only to produce a translation but also to improve
one of the systems. In this paper, we aim at incor-
porating the additional system outputs to improve
an out-of-domain SMT system trained on the Eu-
roparl corpus (Koehn, 2005). Our hope is that the
additional translation hypotheses could bring in
new phrases or, more generally, new information
that was not contained in the Europarl model. In
order to facilitate comparisons, we use in-domain
LMs for all setups.
We investigate two alternative ways of integrat-
ing the additional phrases into the existing SMT
system: One is to take the hypothesis translation
model described in Section 3.1, the other is to
construct system-specific models constructed with
only translations from one system at a time.
Although the Moses decoder is able to work
with two phrase tables at once (Koehn and
Schroeder, 2007), it is difficult to use this method
when there is more than one additional model.
The method requires tuning on at least six more
features, which expands the search space for the
translation task unnecessarily. We instead inte-
grate the translation models from multiple sources
by extending the phrase table. In contrast to the
prior approach presented in (Chen et al, 2007) and
(Eisele et al, 2008) which concatenates the phrase
tables and adds new features as system markers,
our extension method avoids duplicate entries in
the final combined table.
Given a set of hypothesis translation models
(derived from an arbitrary number of system out-
puts) and an original large translation model to be
improved, we first sort the models by quality (see
Section 3.3), always assigning the highest priority
to the original model. The additional phrase tables
are appended to the large model in sorted order
such that only phrase pairs that were never seen
before are included. Lastly, we add new features
(in the form of additional columns in the phrase ta-
ble) to the translation model to indicate each pair?s
origin.
3.3 System evaluation
Since both the system translations and the ref-
erence translations are available for the tuning
43
set, we first compare each output to the reference
translation using BLEU (Papineni et al, 2001)
and METEOR (Banerjee and Lavie, 2005) and a
combined scoring scheme provided by the ULC
toolkit (Gimenez and Marquez, 2008). In our ex-
periments, we selected a subset of 5 systems for
the combination, in most cases, based on BLEU.
On the other hand, some systems may be de-
signed in a way that they deliver interesting unique
translation segments. Therefore, we also measure
the similarity among system outputs as shown in
Table 2 in a given collection by calculating aver-
age similarity scores across every pair of outputs.
de-en fr-en es-en en-de en-fr en-es
Num. 20 23 28 15 16 9
Median 19.87 26.55 22.50 13.78 24.76 23.70
Range 16.37 17.06 9.74 4.75 11.05 13.94
Top 5 de-en fr-en es-en en-de en-fr en-es
Median 22.26 27.93 26.43 15.21 26.62 26.61
Range 4.31 4.76 5.71 1.71 0.68 5.56
Table 1: Statistics of system outputs? BLEU scores
The range of BLEU scores cannot indicate the
similarity of the systems. The direction with the
most systems submitted is Spanish-English but
their respective performances are very close to
each other. As for the selected subset, the English-
French systems have the most similar performance
in terms of BLEU scores. The French-English
translations have the largest range in BLEU but the
similarity in this group is not the lowest.
de-en fr-en es-en en-de en-fr en-es
All 34.09 46.48 61.83 31.74 44.95 38.11
Selected 36.65 56.16 56.06 33.92 52.78 57.25
Table 2: Similarity of the system outputs
Ideally, we should select systems with highest
quality scores and lowest similarity scores. For
German-English, we selected the three with the
highest METEOR scores and another two with
high METEOR scores but low similarity scores to
the first three. For the other language directions,
we chose five systems from different institutions
with the highest scores.
3.4 Language models
We use a standard n-gram language model for
each target language using the monolingual train-
ing data provided in the translation task. These
LMs are thus specific to the same domain as the
input texts. Moreover, we also generate ?hypoth-
esis? LMs solely based on the given system out-
puts, that is, LMs that model how the candidate
systems convey information in the target language.
These LMs do not require any additional training
data. Therefore, we do not require any training
data other than the given system outputs by using
the ?hypothesis? language model and the ?hypoth-
esis? translation model.
3.5 Tuning
After building the models, it is essential to tune
the SMT system to optimize the feature weights.
We use Minimal Error Rate Training (Och, 2003)
to maximize BLEU on the complete development
data. Unlike the standard tuning procedure, we do
not tune the final system directly. Instead, we ob-
tain the weights using models built from the tuning
portion of the system outputs.
For each combination variant, we first train
models on the provided outputs corresponding to
the tuning set. This system, called the tuning sys-
tem, is also tuned on the tuning set. The initial
weights of any additional features not included in
the standard setting are set to 0. We then adapt the
weights to the system built with translations cor-
responding to the test set. The procedure and the
settings for building this system must be identical
to that of the tuning system.
4 Experiments
The purpose of this exercise is to understand the
nature of the system combination task in prac-
tice. Therefore, we restrict ourselves to the train-
ing data and system translations provided by the
shared task. The types of the systems that pro-
duced the translations are assumed to be unknown.
We report results for six translation directions be-
tween four languages.
4.1 Data and baseline
We build an SMT system from release v4 of the
Europarl corpus (Koehn, 2005), following a stan-
dard routine using the Moses toolkit. The sys-
tem also includes 5-gram language models trained
on in-domain corpora of the respective target lan-
guages using SRILM (Stolcke, 2002).
The systems in this paper, including the base-
line, are all tuned on the same 501-sentence tuning
set. Note also that the provided n-best outputs are
excluded in our experiments.
44
4.2 Results
The experiments include three different setups for
direct system combination, involving only hypoth-
esis translation models. System S0, the baseline
for this group, uses a hypothesis translation model
built with all available system translations and a
hypothesis LM (also from the machine-generated
outputs). S1 differs from S0 in that the LM in S1 is
generated from a large news corpus. S2 consists of
translation models built with only the five selected
systems. The BLEU scores of these systems are
shown in Table 3.
de-en fr-en es-en en-de en-fr en-es
Top 1 21.16 30.91 28.54 14.96 26.55 27.84
Mean 17.29 23.78 21.39 12.76 22.96 21.43
S0 20.46 27.50 23.35 13.95 27.29 25.59
S1 21.76 28.05 25.49 15.16 27.70 26.09
S2 21.71 24.98 27.26 15.62 24.28 25.22
Table 3: BLEU scores of direct system combina-
tion
When all outputs are included, the combined
system can always produce translations better than
most of the systems. When only a hypothesis LM
is used, the BLEU scores are always higher than
the average BLEU scores of the outputs. It even
outperforms the top system for English-French.
This simple setup (S0) is certainly a feasible so-
lution when no additional data is available and no
system evaluation is possible. This approach ap-
pears to be more effective on typically difficult
language pairs that involve German.
As for the systems with normal language mod-
els, neither of the systems ensure better transla-
tions. The translation quality is not completely
determined by the number of included translations
and their quality. On the other hand, the output
set with higher diversity (Table 2) usually leads
to better combination results. This observation is
consistent with the results from the system inte-
gration experiments shown in Table 4.
de-en fr-en es-en en-de en-fr en-es
Bas 19.13 25.07 24.55 13.59 23.67 23.67
Med 17.99 24.56 20.70 13.19 24.19 22.12
All 21.40 28.00 27.75 15.21 27.20 26.41
Top5 21.70 26.01 28.53 15.52 27.87 27.92
Table 4: BLEU scores of integrated SMT systems
(Bas: Baseline, Med: Median)
There are two variants in our experiments on
system integration. All in Table 4 represents the
system that integrates the complete hypothesis
translation model with the Europarl model, while
Top 5 refers to the system that incorporates the five
system-specific models separately. Both setups re-
sult in an improvement over the baseline Europarl-
based SMT system. BLEU scores increase by up
to 4.25 points. The integrated SMT system some-
times produces translations better than the best
system (7 out of 12 cases).
5 Conclusion
This work uses the Moses toolkit to combine
translations from multiple engines in a simple way.
The experiments on six translation directions show
interesting results: The final translations are al-
ways better than the majority of the given systems,
while the combination performs better than the
best system in half the cases. A similar approach
was applied to improve an existing SMT system
which was built in a domain different from the test
task. We achieved improvements in all cases.
There are many possible future directions to
continue this work. As we have shown, the qual-
ity of the combined system is more related to the
diversity of the involved systems than to the num-
ber of the systems or their quality. Hand-picked
systems lead to better combinations than those se-
lected by BLEU scores. It would be interesting
to develop a more comprehensive system selection
strategy.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
45
WMT07, pages 193?196, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to integrate mul-
tiple rule-based machine translation engines into a
hybrid system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 179?
182, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jesus Gimenez and Lluis Marquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 195?198, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbs. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of Annual meet-
ing of the Association for Computation Linguis-
tics (acl), demonstration session, pages 177?180,
Prague, Czech, June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Antti-Veikko I. Rosti, Spyridon Matsoukas, and
Richard M. Schwartz. 2007. Improved word-level
system combination for machine translation. In
ACL.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
46
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 37?45,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Construction of a German HPSG grammar from a detailed treebank
Bart Cramer? and Yi Zhang??
Department of Computational Linguistics & Phonetics, Saarland University, Germany?
LT-Lab, German Research Center for Artificial Intelligence, Germany?
{bcramer,yzhang}@coli.uni-saarland.de
Abstract
Grammar extraction in deep formalisms
has received remarkable attention in re-
cent years. We recognise its value, but try
to create a more precision-oriented gram-
mar, by hand-crafting a core grammar, and
learning lexical types and lexical items
from a treebank. The study we performed
focused on German, and we used the Tiger
treebank as our resource. A completely
hand-written grammar in the framework of
HPSG forms the inspiration for our core
grammar, and is also our frame of refer-
ence for evaluation. 1
1 Introduction
Previous studies have shown that treebanks can
be helpful when constructing grammars. The
most well-known example is PCFG-based statis-
tical parsing (Charniak and Johnson, 2005), where
a PCFG is induced from, for instance, the Penn
Treebank. The underlying statistical techniques
have been refined in the last decade, and previ-
ous work indicates that the labelled f-score of this
method converges to around 91%.
An alternative to PCFGs, with more linguistic
relevance, is formed by deeper formalisms, such
as TAG (Joshi and Schabes, 1997), CCG (Steed-
man, 1996), LFG (Kaplan and Bresnan, 1995)
and HPSG (Pollard and Sag, 1994). For LFG
(Butt et al, 2002) and HPSG (Flickinger, 2000;
Mu?ller, 2002), large hand-written grammars have
been developed. In the case of HPSG, the gram-
mar writers found the small number of principles
too restrictive, and created more rules (approxi-
mately 50 to 300) to accommodate for phenomena
1The research reported in this paper has been carried out
with financial support from the Deutsche Forschungsgemein-
schaft and the German Excellence Cluster of Multimodal
Computing & Interaction.
that vanilla HPSG cannot describe correctly. The
increased linguistic preciseness comes at a cost,
though: such grammars have a lower out-of-the-
box coverage, i.e. they will not give an analysis on
a certain portion of the corpus.
Experiments have been conducted, where a
lexicalised grammar is learnt from treebanks, a
methodology for which we coin the name deep
grammar extraction. The basic architecture of
such an experiment is to convert the treebank to
a format that is compatible with the chosen lin-
guistic formalism, and read off the lexicon from
that converted treebank. Because all these for-
malisms are heavily lexicalised, the core gram-
mars only consist of a small number of principles
or operators. In the case of CCG (Hockenmaier
and Steedman, 2002), the core grammar consists
of the operators that CCG stipulates: function ap-
plication, composition and type-raising. Standard
HPSG defines a few schemata, but these are usu-
ally adapted for a large-scale grammar. Miyao et
al. (2004) tailor their core grammar for optimal use
with the Penn Treebank and the English language,
for example by adding a new schema for relative
clauses.
Hockenmaier and Steedman (2002), Miyao et
al. (2004) and Cahill et al (2004) show fairly good
results on the Penn Treebank (for CCG, HPSG and
LFG, respectively): these parsers achieve accura-
cies on predicate-argument relations between 80%
and 87%, which show the feasibility and scalabil-
ity of this approach. However, while this is a sim-
ple method for a highly configurational language
like English, it is more difficult to extend to lan-
guages with more complex morphology or with
word orders that display more freedom. Hocken-
maier (2006) is the only study known to the au-
thors that applies this method to German, a lan-
guage that displays these properties.
This article reports on experiments where the
advantages of hand-written and derived grammars
37
are combined. Compared to previous deep gram-
mar extraction approaches, a more sophisticated
core grammar (in the framework of HPSG) is cre-
ated. Also, more detailed syntactic features are
learnt from the resource treebank, which leads to
a more precise lexicon. Parsing results are com-
pared with GG (German Grammar), a previously
hand-written German HPSG grammar (Mu?ller,
2002; Crysmann, 2003; Crysmann, 2005).
2 Core grammar
2.1 Head-driven phrase structure grammar
This study has been entirely embedded in the
HPSG framework (Pollard and Sag, 1994). This
is a heavily lexicalised, constraint-based theory of
syntax, and it uses typed feature structures as its
representation. HPSG introduces a small num-
ber of principles (most notably, the Head Feature
Principle) that guide the construction of a few Im-
mediate Dominance schemata. These schemata
are meant to be the sole basis to combine words
and phrases. Examples of schemata are head-
complement, head-subject, head-specifier, head-
filler (for long-distance dependencies) and head-
modifier.
In this study, the core grammar is an extension
of the off-the-shelf version of HPSG. The type hi-
erarchy is organised by a typed feature structure
hierarchy (Carpenter, 1992), and can be read by
the LKB system (Copestake, 2002) and the PET
parser (Callmeier, 2000). The output is given in
Minimal Recursion Semantics (Copestake et al,
2005) format, which can be minimally described
as a way to include scope information in depen-
dency output.
2.2 The German language
Not unlike English, German uses verb position
to distinguish between different clause types. In
declarative sentences, verbs are positioned in the
second position, while subordinate classes are
verb-final. Questions and imperatives are verb-
initial. However, German displays some more
freedom with respect to the location of subjects,
complements and adjuncts: they can be scram-
bled rather freely. The following sentences are
all grammatical, and have approximately the same
meaning:
(1) a. Der
The.NOM
Pra?sident
President.NOM
hat
has
gestern
yesterday
das
the.ACC
Buch
book.ACC
gelesen.
read.PERF.
?The president read the book yester-
day?
b. Gestern hat der Pra?sident das Buch
gelesen.
c. Das Buch hat der Pra?sident gestern
gelesen.
As can be seen, the main verb is placed at sec-
ond position (the so-called ?left bracket?), but all
other verbs remain at the end of the sentence,
in the ?right bracket?. Most linguistic theories
about German recognise the existence of topolog-
ical fields: the Vorfeld before the left bracket, the
Mittelfeld between both brackets, and the Nach-
feld after the right bracket. The first two are
mainly used for adjuncts and arguments, whereas
the Nachfeld is typically, but not necessarily, used
for extraposed material (e.g. relative clauses or
comparative phrases) and some VPs. Again, the
following examples mean roughly the same:
(2) a. Er
He
hat
has
das
the.ACC
Buch,
Book.ACC,
das
that
sie
she
empfohlen
recommended
hat,
has,
gelesen.
read.PERF.
He has read the book that she recom-
mended.
b. Er hat das Buch gelesen, das sie emp-
fohlen hat.
c. Das Buch hat er gelesen, das sie emp-
fohlen hat.
Another distinctive feature of German is its rela-
tively rich morphology. Nominals are marked with
case, gender and number, and verbs with number,
person, tense and mood. Adjectives and nouns
have to agree with respect to gender, number and
declension type, the latter being determined by
the (non-)existence and type of determiner used
in the noun phrase. Verbs and subjects have to
agree with respect to number and person. Ger-
man also displays highly productive noun com-
pounding, which amplifies the need for effective
unknown word handling. Verb particles can ei-
ther be separated from or concatenated to the verb:
compare ?Er schla?ft aus? (?He sleeps in?) and ?Er
38
Amerikaner
?
?
no-det
VAL
[
SPEC ??
SUBCAT ??
]
?
?
?
?
noun
VAL
[
SPEC ?det?
SUBCAT ??
]
?
?
mu?ssen
?
?
?
?
?
?
?
?
verb
VAL 1
SLASH 2
XCOMP
?
?
verb
VAL 1
SLASH 2
?
?
?
?
?
?
?
?
?
?
hart
[
adverb
MOD verb
]
arbeiten
?
?
verb-inf
VAL
[
SUBJ ?np-nom?
]
SLASH ??
?
?
?
?
slash-subj
VAL
[
SUBJ ??
]
SLASH ?np-nom?
?
?
?
?
mod-head
VAL
[
SUBJ ??
]
SLASH ?np-nom?
?
?
?
?
head-cluster
VAL
[
SUBJ ??
]
SLASH ?np-nom?
?
?
?
?
filler-head
VAL
[
SUBJ ??
]
SLASH ??
?
?
Figure 1: This figure shows a (simplified) parse tree of the sentence ?Amerikaner mu?ssen hart arbeiten?
(?Americans have to work hard?).
wird ausschlafen? (?He will sleep in?). In such
verbs, the word ?zu? (which translates to the En-
glish ?to? in ?to sleep?) can be infixed as well: ?er
versucht auszuschlafen? (?He tries to sleep in?).
These characteristics make German a compar-
atively complex language to parse with CFGs:
more variants of the same lemma have to be mem-
orised, and the expansion of production rules will
be more diverse, with a less peaked statistical dis-
tribution. Efforts have been made to adapt existing
CFG models to German (Dubey and Keller, 2003),
but the results still don?t compare to state-of-the-
art parsing of English.
2.3 Structure of the core grammar
The grammar uses the main tenets from Head-
driven Phrase Structure Grammar (Pollard and
Sag, 1994). However, different from earlier deep
grammar extraction studies, more sophisticated
structures are added. Mu?ller (2002) proposes a
new schema (head-cluster) to account for verb
clusters in the right bracket, which includes the
possibility to merge subcategorisation frames of
e.g. object-control verbs and its dependent verb.
Separate rules for determinerless NPs, genitive
modification, coordination of common phrases,
relative phrases and direct speech are also created.
The free word order of German is accounted for
by scrambling arguments with lexical rules, and
by allowing adjuncts to be a modifier of unsat-
urated verb phrases. All declarative phrases are
considered to be head-initial, with an adjunct or
argument fronted using the SLASH feature, which
is then discharged using the head-filler schema.
The idea put forward by, among others, (Kiss and
Wesche, 1991) that all sentences should be right-
branching is linguistically pleasing, but turns out
be computationally very expensive (Crysmann,
2003), and the right-branching reading should be
replaced by a left-branching reading when the
right bracket is empty (i.e. when there is no auxil-
iary verb present).
An example of a sentence is presented in fig-
ure 1. It receives a right-branching analysis, be-
cause the infinitive ?arbeiten? resides in the right
bracket. The unary rule slash-subj moves the re-
quired subject towards the SLASH value, so that it
can be discharged in the Vorfeld by the head-filler
schema. ?mu?ssen? is an example of an argument
attraction verb, because it pulls the valence fea-
ture (containing SUBJ, SUBCAT etc; not visible
in the diagram) to itself. The head-cluster rule as-
sures that the VAL value then percolates upwards.
Because ?Amerikaner? does not have a specifier, a
separate unary rule (no-det) takes care of discharg-
ing the SPEC feature, before it can be combined
with the filler-head rule.
As opposed to (Hockenmaier, 2006), this study
39
(a)
teure Detektive kann sich der Supermarkt nicht leisten
NP
MO HD
NP
DET HD
VP
HDNGOA DA
S
SBOCHD
(b)
teure Detektive kann sich der Supermarkt nicht leisten
NP
MO HD
NP
DET HD
S
OA HD REFL SB MO VC
Figure 2: (a) shows the original sentence, whereas (b) shows the sentence after preprocessing. Note that
NP is now headed, that the VP node is deleted, and that the verbal cluster is explicitly marked in (b). The
glossary of this sentence is ?Expensive.ACC detectives.ACC can REFL the.NOM supermarket.NOM not
afford?
employs a core lexicon for words that have marked
semantic behaviour. These are usually closed
word classes, and include items such as raising
and auxiliary verbs, possessives, reflexives, arti-
cles, complementisers etc. The size of this core
lexicon is around 550 words. Note that, because
the core lexicon only contains function words, its
coverage is negligible without additional entries.
3 Derivation of the lexicon
3.1 The Tiger treebank
The Tiger treebank (Brants et al, 2002) is a tree-
bank that embraces the concept of constituency,
but can have crossing branches, i.e. the tree might
be non-projective. This allowed the annotators to
capture the German free word order. Around one-
third of the sentences received a non-projective
analysis. An example can be found in figure 2.
Additionally, it annotates each branch with a syn-
tactic function.
The text comes from a German newspaper
(Frankfurter Rundschau). It was annotated semi-
automatically, using a cascaded HMM model. Af-
ter each phase of the HMM model, the output was
corrected by human annotators. The corpus con-
sists of over 50,000 sentences, with an average
sentence length of 17.6 tokens (including punc-
tuation). The treebank employs 26 phrase cate-
gories, 56 PoS tags and 48 edge labels. It also en-
codes number, case and gender at the noun termi-
nals, and tense, person, number and mood at verbs.
Whether a verb is finite, an infinitive or a partici-
ple is encoded in the PoS tag. A peculiarity in the
annotation of noun phrases is the lack of headed-
ness, which was meant to keep the annotation as
theory-independent as reasonably possible.
3.2 Preprocessing
A number of changes had to be applied to the tree-
bank to facilitate the read-off procedure:
? A heuristic head-finding procedure is applied
in the spirit of (Magerman, 1995). We use
priority lists to find the NP?s head, deter-
miner, appositions and modifiers. PPs and
CPs are also split into a head and its depen-
dent.
? If a verb has a separated verb particle, this
particle is attached to the lemma of the verb.
For instance, if the verb ?schlafen? has a parti-
cle ?aus?, the lemma will be turned into ?auss-
chlafen? (?sleep in?). If this is not done, sub-
categorisation frames will be attributed to the
wrong lemma.
? Sentences with auxiliaries are non-projective,
if the adjunct of the embedded VP is in the
Vorfeld. This can be solved by flattening the
tree (removing the VP node), and marking
the verbal cluster (VC) explicitly. See fig-
ure 2 for an example. 67.6% of the origi-
nal Tiger treebank is projective, and with this
procedure, this is lifted to 80.1%.
? The Tiger treebank annotates reflexive pro-
nouns with the PoS tag PRF, but does not
distinguish the syntactic role. Therefore, if a
verb has an object that has PRF as its part-of-
speech, the label of that edge is changed into
REFL, so that reflexive verbs can be found.
40
(a)
0 10 20 30 40 50
0
10000
20000
30000
40000
50000
(b)
0 10 20 30 40 50
0
200
400
600
800
1000
(c)
0 10 20 30 40 50
0
0.1
0.2
0.3
0.4
0.5
Figure 3: These graphs show learning curves of the algorithm on the first 45,000 sentences of the Tiger
treebank. Graph (a) indicates the amount of lemmas learnt (from top to bottom: nouns, names, adjec-
tives, verbs and adverbs). The graph in (b) shows the number of distinct lexical types for verbs that are
learnt. Graph (c) shows the average proportion of morphological forms that is observed per verb lemma,
assuming that each verb has 28 different forms: infinitive, zu (to) + infinitive, participle, imperative and
24 finite forms (3 (person) * 2 (number) * 2 (tense) * 2 (mood)).
The preprocessing stage failed in 1.1% of the
instances.
3.3 Previous work
The method described in Hockenmaier (2006) first
converts the Tiger analysis to a tree, after which
the lexical types were derived. Because it was
the author?s goal to convert all sentences, some
rather crude actions had to be taken to render
non-projective trees projective: whenever a cer-
tain node introduces non-projectivity, some of its
daughters are moved to the parent tree, until that
node is projective. Below, we give two examples
where this will lead to incorrect semantic compo-
sition, with the consequence of flawed lexicon en-
tries. We argue that it is questionable whether the
impressive conversion scores actually represent a
high conversion quality. It would be interesting to
see how this grammar performs in a real parsing
task, but no such study has been carried out so far.
The first case deals with extraposed material in
the Nachfeld. Typical examples include relative
phrases, comparatives and PH/RE constructions2.
2NPs, AVPs and PPs can, instead of their usual headed
structure, be divided in two parts: a ?placeholder? and
a ?repeated element?. These nodes often introduce non-
projectivity, and it is not straightforward to create a valid lin-
guistic analysis for these phenomena. Example sentences of
these categories (NPs, AVPs and PPs, respectively) are:
(1) [ PH Es ] ist wirklich schwer zu sagen, [ RE welche
Positionen er einnimmt ]
(2) Man mu? sie also [ PH so ] behandeln , [ RE wie man
eine Weltanschauungsbewegung behandelt ]
(3) Alles deutet [ PH darauf ] hin [ RE da? sie es nicht
schaffen wird ]
These examples all have the RE in the Nachfeld, but their
placement actually has a large variety.
The consequence is that the head of the extraposed
material will be connected to the verb, instead of
to the genuine head.
Another example where Hockenmaier?s algo-
rithm will create incorrect lexical entries is when
the edge label is PAR (for ?parentheses?) or in the
case of appositions. Consider the following sen-
tence:
(3) mit
with
160
160
Planstellen
permanent posts
(etliche
(several
sind
are
allerdings
however
noch
still
unbesetzt)
unoccupied)
The conclusion that will be drawn from this sen-
tence is that ?sind? can modify nouns, which is
only true due to the parentheses, and has no re-
lation with the specific characteristics of ?sind?.
Similarly, appositions will act as modifiers of
nouns. Although one might argue that this is the
canonical CCG derivation for these phenomena, it
is not in the spirit of the HPSG grammars, and we
believe that these constructions are better handled
in rules than in the lexicon.
3.4 Procedure
In our approach, we will be more conservative,
and the algorithm will only add facts to its knowl-
edge base if the evidence is convincing. That
means that less Tiger graphs will get projective
analyses, but that doesn?t have to be a curse: we
can derive lexical types from non-projective anal-
yses just as well, and leave the responsibility for
solving the more complex grammatical phenom-
ena to the core grammar. For example, lexical
rules will deal with fronting and Mittelfeld scram-
bling, as we have stated before. This step of the
41
procedure has indeed strong affinity with deep lex-
ical acquisition, except for the fact that in DLA all
lexical types are known, and this is not the case in
this study: the hand-written lexical type hierarchy
is still extended with new types that are derived
from the resource treebank, mostly for verbs.
The basic procedure is as follows:
? Traverse the graph top-down.
? For each node:
? Identify the node?s head (or the deepest
verb in the verb cluster3);
? For each complement of this node, add
this complement to the head?s subcate-
gorisation frame.
? For each modifier, add this head to the
possible MOD values of the modifier?s
head.
? For each lexical item, a mapping of (lemma,
morphology)? word form is created.
After this procedure, the following information
is recorded for the verb lemma ?leisten? from fig-
ure 2:
? It has a subcategorisation frame ?npnom-refl-
npacc?.
? Its infinitive form is ?leisten?.
The core grammar defines that possible sub-
jects are nominative NPs, expletive ?es? and CPs.
Expletives are considered to be entirely syntac-
tic (and not semantic), so they will not receive a
dependency relation. Complements may include
predicative APs, predicative NPs, genitive, dative
and accusative NPs, prepositional complements,
CPs, reflexives, separable particles (also purely
syntactic), and any combination of these. For non-
verbs, the complements are ordered (i.e. it is a
list, and not a verb). Verb complementation pat-
terns are sets, which means that duplicate com-
plements are not allowed. For verbs, it is also
recorded whether the auxiliary verb to mark the
perfect tense should be either ?haben? (default) or
?sein? (mostly verbs that have to do with move-
ment). Nouns are annotated with whether they can
have appositions or not.
3That means that the head of a S/VP-node is assumed
to be contained in the lexicon, as it must be some sort of
auxiliary.
Results from the derivation procedure are
graphed in figure 3. The number of nouns and
names is still growing after 45,000 sentences,
which is an expected result, given the infinite na-
ture of names and frequent noun compounding.
However, it appears that verbs, adjectives and ad-
verbs are converging to a stable level. On the other
hand, lexical types are still learnt, and this shows a
downside of our approach: the deeper the extrac-
tion procedure is, the more data is needed to reach
the same level of learning.
The core grammar contains a little less than 100
lexical types, and on top of that, 636 lexical types
are learnt, of which 579 are for verbs. It is inter-
esting to see that the number of lexical types is
considerably lower than in (Hockenmaier, 2006),
where around 2,500 lexical types are learnt. This
shows that our approach has a higher level of gen-
eralisation, and is presumably a consequence of
the fact that the German CCG grammar needs dis-
tinct lexical types for verb-initial and verb-final
constructions, and for different argument scram-
blings in the Mittelfeld, whereas in our approach,
hand-written lexical rules are used to do the scram-
bling.
The last graph shows that the number of word
forms is still insufficient. We assume that each
verb can have 28 different word forms. As can be
seen, it is clear that only a small part of this area
is learnt. One direction for future research might
be to find ways to automatically expand the lexi-
con after the derivation procedure, or to hand-code
morphological rules in the core grammar.
4 Parsing
4.1 Methodology
All experiments in this article use the first 45,000
sentences as training data, and the consecutive
5,000 sentences as test data. The remaining 472
sentences are not used. We used the PET parser
(Callmeier, 2000) to do all parsing experiments.
The parser was instructed to yield a parse error af-
ter 50,000 passive edges were used. Ambiguity
packing (Oepen and Carroll, 2000) and selective
unpacking (Zhang et al, 2007) were used to re-
duce memory footprint and speed up the selection
of the top-1000 analyses. The maximum entropy
model, used for selective unpacking, was based on
200 treebanked sentences of up to 20 words from
the training set. Part-of-speech tags delivered by
the stock version of the TnT tagger (Brants, ) were
42
Tiger T.+TnT GG
Out of vocabulary 71.9 % 5.2 % 55.6 %
Parse error 0.2 % 1.5 % 0.2 %
Unparsed 7.9 % 37.7 % 28.2 %
Parsed 20.0 % 55.6 % 16.0 %
Total 100.0 % 100.0 % 100.0 %
Avg. length 8.6 12.8 8.0
Avg. nr. of parses 399.0 573.1 19.2
Avg. time (s) 9.3 15.8 11.6
Table 1: This table shows coverage results on the held-out test set. The first column denotes how the
extracted grammar performs without unknown word guessing. The second column uses PoS tags and
generic types to guide the grammar when an unknown word is encountered. The third column is the
performance of the fully hand-written HPSG German grammar by (Mu?ller, 2002; Crysmann, 2003).
OOV stands for out-of-vocabulary. A parse error is recorded when the passive edge limit (set to 50,000)
has been reached. The bottom three rows only gives information about the sentences where the grammar
actually returns at least one parse.
Training set Test set
All 100.0 % 100.0 %
Avg. length 14.2 13.5
Coverage 79.0 % 69.0 %
Avg. length 13.2 12.8
Correct (top-1000) 52.0% 33.5 %
Avg. length 10.4 8.5
Table 2: Shown are the treebanking results, giv-
ing an impression of the quality of the parses.
The ?training set? and ?test set? are subsets of 200
sentences from the training and test set, respec-
tively. ?Coverage? means that at least one analysis
is found, and ?correct? indicates that the perfect
solution was found in the top-1000 parses.
used when unknown word handling was turned
on. These tags were connected to generic lexical
types by a hand-written mapping. The version of
GG that was employed (Mu?ller, 2002; Crysmann,
2003) was dated October 20084.
4.2 Results
Table 1 shows coverage figures in three different
settings. It is clear that the resulting grammar has
a higher coverage than the GG, but this comes at a
cost: more ambiguity, and possibly unnecessary
ambiguity. Remarkably, the average processing
time is lower, even when the sentence lengths and
4It should be noted that little work has gone in to provid-
ing unknown word handling mechanisms, and that is why we
didn?t include it in our results. However, in a CoNLL-2009
shared task paper (Zhang et al, 2009), a coverage of 28.6%
was reported when rudimentary methods were used.
ambiguity rates are higher. We attribute this to
the smaller feature structure geometry that is in-
troduced by the core grammar (compared to the
GG). Using unknown word handling immediately
improved the coverage, by a large margin. Larger
ambiguity rates were recorded, and the number of
parser errors slightly increased.
Because coverage does not imply quality, we
wanted to look at the results in a qualitative fash-
ion. We took a sample of 200 sentences from
both the training and the test set, where the ones
from the training set did not overlap with the set
used to train the MaxEnt model, so that both set-
tings were equally influenced by the rudimentary
MaxEnt model. We evaluated for how many sen-
tences the exactly correct parse tree could be found
among the top-1000 parses (see table 2). The dif-
ference between the performance on the training
and test set give an idea of how well the gram-
mar performs on unknown data: if the difference
is small, the grammar extends well to unseen data.
Compared to evaluating on lexical coverage, we
believe this is a more empirical estimation of how
close the acquisition process is to convergence.
Based on the kind of parse trees we observed,
the impression was that on both sets, performance
was reduced due to the limited predictive power
of the disambiguation model. There were quite
a few sentences for which good parses could be
expected, because all lexical entries were present.
This experiment also showed that there were sys-
tematic ambiguities that were introduced by in-
consistent annotation in the Tiger treebank. For in-
43
stance, the word ?ein? was learnt as both a number
(the English ?one?) and as an article (?a?), leading
to spurious ambiguities for each noun phrase con-
taining the word ?ein?, or one of its morphological
variants. These two factors reinforced each other:
if there is spurious ambiguity, it is even harder for
a sparsely trained disambiguation model to pull
the correct parse inside the top-1000.
The difference between the two ?correct? num-
bers in table 2 is rather large, meaning that the
?real? coverage might seem disappointingly low.
Not unexpectedly, we found that the generic lex-
ical types for verbs (transitive verb, third person
singular) and nouns (any gender, no appositions
allowed) was not always correct, harming the re-
sults considerably.
A quantitative comparison between deep gram-
mars is always hard. Between DELPH-IN gram-
mars, coverage has been the main method of eval-
uation. However, this score does not reward rich-
ness of the semantic output. Recent evidence from
the ERG (Ytrest?l et al, 2009) suggests that the
ERG reaches a top-500 coverage of around 70%
on an unseen domain, a result that this experiment
did not approximate. The goal of GG is not com-
putational, but it serves as a testing ground for lin-
guistic hypotheses. Therefore, the developers have
never aimed at high coverage figures, and crafted
the GG to give more detailed analyses and to be
suited for both parsing and generation. We are
happy to observe that the coverage figures in this
study are higher than GG?s (Zhang et al, 2009),
but we realise the limited value of this evaluation
method. Future studies will certainly include a
more granular evaluation of the grammar?s perfor-
mance.
5 Conclusion and discussion
We showed how a precise, wide-coverage HPSG
grammar for German can be created successfully,
by constructing a core grammar by hand, and ap-
pending it with linguistic information from the
Tiger treebank. Although this extracted gram-
mar suffers considerably more from overgenera-
tion than the hand-written GG, we argue that our
conservative derivation procedure delivers a more
detailed, compact and correct compared to pre-
vious deep grammar extraction efforts. The use
of the core lexicon allows us to have more lin-
guistically motivated analyses of German than ap-
proaches where the core lexicon only comprises
the textbook principles/operators. We compared
our lexicon extraction results to those from (Hock-
enmaier, 2006). Also, preliminary parsing exper-
iments are reported, in which we show that this
grammar produces reasonable coverage on unseen
text.
Although we feel confident about the successful
acquisition of the grammar, there still remain some
limiting factors in the performance of the grammar
when actually parsing. Compared to coverage fig-
ures of around 80%, reported by (Riezler et al,
2001), the proportion of parse forests containing
the correct parse in this study is rather low. The
first limit is the constructional coverage, mean-
ing that the core grammar is not able to construct
the correct analysis, even though all lexical en-
tries have been derived correctly before. The most
frequent phenomena that are not captured yet are
PH/RE constructions and extraposed clauses, and
we plan to do an efficient implementation (Crys-
mann, 2005) of these in a next version of the gram-
mar. Second, as shown in figure 3, data scarcity in
the learning of the surface forms of lemmas neg-
atively influences the parser?s performance on un-
seen text.
In this paper, we focused mostly on the cor-
rectness of the derivation procedure. We would
like to address the real performance of the gram-
mar/parser combination in future work, which can
only be done when parses are evaluated according
to a more granular method than we have done in
this study. Furthermore, we ran into the issue that
there is no straightforward way to train larger sta-
tistical models automatically, which is due to the
fact that our approach does not convert the source
treebank to the target formalism?s format (in our
case HPSG), but instead reads off lexical types
and lexical entries directly. We plan to investigate
possibilities to have the annotation be guided auto-
matically by the Tiger treebank, so that the disam-
biguation model can be trained on a much larger
amount of training data.
Acknowledgements
We would like to thank Rebecca Dridan, Antske
Fokkens, Stephan Oepen and the anonymous re-
viewers for their valuable contributions to this pa-
per.
44
References
T. Brants. TnT: a statistical part-of-speech tagger. In
Proceedings of the Sixth Conference on Applied Nat-
ural Language Processing.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, pages 24?41.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In International Conference On Computational Lin-
guistics, pages 1?7.
A. Cahill, M. Burke, R. ODonovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL-2004, pages 320?327.
U. Callmeier. 2000. PET?a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99?107.
B. Carpenter. 1992. The Logic of Typed Feature Struc-
tures: With Applications to Unification Grammars,
Logic Programs, and Constraint Resolution. Cam-
bridge University Press, Cambridge, UK.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL-2005, pages 173?180.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2005. Minimal Recursion Semantics: An Intro-
duction. Research on Language & Computation,
3(4):281?332.
A. Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA, USA.
B. Crysmann. 2003. On the efficient implementation
of German verb placement in HPSG. In Proceedings
of RANLP-2003, pages 112?116.
B. Crysmann. 2005. Relative Clause Extraposition in
German: An Efficient and Portable Implementation.
Research on Language & Computation, 3(1):61?82.
A. Dubey and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 96?103.
D. Flickinger. 2000. On building a more effcient gram-
mar by exploiting types. Natural Language Engi-
neering, 6(1):15?28.
J. Hockenmaier and M. Steedman. 2002. Acquiring
compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of LREC-2002, pages 1974?
1981.
J. Hockenmaier. 2006. Creating a CCGbank and a
Wide-Coverage CCG Lexicon for German. In Pro-
ceedings of ACL-2006, pages 505?512.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. Handbook of formal languages, 3:69?
124.
R.M. Kaplan and J. Bresnan. 1995. Lexical-Functional
Grammar: A formal system for grammatical rep-
resentation. Formal Issues in Lexical-Functional
Grammar, pages 29?130.
T. Kiss and B. Wesche. 1991. Verb order and head
movement. Text Understanding in LILOG, Lecture
Notes in Artificial Intelligence, 546:216?242.
D. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL-1995, pages
276?283.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpus-
oriented grammar development for acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank. In Proceedings of IJCNLP-2004.
S. Mu?ller. 2002. Complex Predicates: Verbal
Complexes, Resultative Constructions, and Particle
Verbs in German. CSLI Publications, Stanford, CA,
USA.
S. Oepen and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing: practical results. In Pro-
ceedings of NAACL-2000, pages 162?169.
C.J. Pollard and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University Of Chicago Press,
Chicago, IL, USA.
S. Riezler, T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell III, and M. Johnson. 2001. Parsing
the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques.
In Proceedings of ACL-2001, pages 271?278.
M. Steedman. 1996. Surface structure and interpreta-
tion. MIT Press, Cambridge, MA, USA.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and Annotating Wikipedia Sub-Domains.
In Proceedings of the Seventh International Work-
shop on Treebanks and Linguistic Theories, pages
185?197.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency
in Unification-Based N-Best Parsing. In Proceed-
ings of the Tenth International Conference on Pars-
ing Technologies, pages 48?59.
Y. Zhang, R. Wang, and S.. Oepen. 2009. Hybrid Mul-
tilingual Parsing with HPSG for SRL. In Proceed-
ings of CoNLL-2009, to appear.
45
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 170?173,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Wall Street Journal Texts Using a Hand-Crafted Deep
Linguistic Grammar
Valia Kordoni & Yi Zhang
DFKI GmbH and Dept. of Computational Linguistics, Saarland University
66041 Saarbru?cken, GERMANY
{kordoni,yzhang}@coli.uni-sb.de
Abstract
This paper presents an on-going effort
which aims to annotate the Wall Street
Journal sections of the Penn Treebank with
the help of a hand-written large-scale and
wide-coverage grammar of English. In do-
ing so, we are not only focusing on the
various stages of the semi-automated an-
notation process we have adopted, but we
are also showing that rich linguistic anno-
tations, which can apart from syntax also
incorporate semantics, ensure that the tree-
bank is guaranteed to be a truly sharable,
re-usable and multi-functional linguistic
resource?.
1 Introduction
The linguistic annotation of a corpus is the prac-
tice of adding interpretative linguistic information
in order to give ?added value? to the corpus. Lin-
guistically annotated corpora have been shown to
help in many kinds of automatic language pro-
cessing or analysis. For example, corpora which
have been POS-tagged can automatically yield fre-
quency lists or frequency dictionaries with gram-
matical classification. Another important use for
linguistically annotated corpora is in the area of
automatic parsing. In terms of re-usability of lin-
guistic annotations, what is to be advocated here is
that ? as long as the annotation provided is a kind
useful to many users - an annotated corpus gives
?value added? because it can be readily shared by
others, apart from those who originally added the
annotation. In short, a linguistically annotated cor-
pus is a sharable resource, an example of the elec-
tronic resources increasingly relied on for research
and study in the humanities and social sciences.
In this paper, we present an on-going project
whose aim is to produce rich syntactic and se-
?We thank Dan Flickinger and Stephan Oepen for their
support with the grammar and treebanking software used in
this project. The second author is supported by the German
Excellence Cluster: Multimodal Computing & Interaction.
mantic annotations for the Wall Street Journal
(henceforward WSJ) sections of the Penn Tree-
bank (henceforward PTB; Marcus et al (1993)).
The task is being carried out with the help of the
English Resource Grammar (henceforward ERG;
Flickinger (2002)), which is a hand-written gram-
mar for English in the spirit of the framework of
Head-driven Phrase Structure Grammar (hence-
forward HPSG; Pollard and Sag (1994)).
2 Background & Motivation
The past two decades have seen the development
of many syntactically annotated corpora. There is
no need to defend the importance of treebanks in
the study of corpus linguistics or computational
linguistics here. Evidently, the successful devel-
opment of many statistical parsers is attributed
to the development of large treebanks. But for
parsing systems based on hand-written grammars,
treebanks are also important resources on the base
of which statistical parse disambiguation models
have been developed.
The early treebanking efforts started with man-
ual annotations which are time-consuming and
error-prone procedures. For instance, the WSJ
sections of the PTB has taken many person years
to get annotated. Similar efforts have been car-
ried out in many more languages, as can be seen
in the cases of the German Negra/Tiger Treebank
(Brants et al, 2002), the Prague Dependency Tree-
bank (Hajic? et al, 2000), Tu?Ba-D/Z1, etc. Al-
though many of these projects have stimulated re-
search in various sub-fields of computational lin-
guistics where corpus-based empirical methods
are used, there are many known shortcomings of
the manual corpus annotation approach.
Many of the limitations in the manual treebank-
ing approach have led to the development of sev-
eral alternative approaches. While annotating lin-
guistically rich structures from scratch is clearly
inpractical, it has been shown that the different
1http://www.sfs.nphil.uni-tuebingen.de/en tuebadz.shtml
170
structures in various linguistic frameworks can be
converted from annotated treebanks to a differ-
ent format. And the missing rich annotations can
be filled in incrementally and semi-automatically.
This process usually involves careful design of
the conversion program, which is a non-trivial
task. In very recent years, based on the treebank
conversion approach and existing manually anno-
tated treebanks, various ?new? annotations in dif-
ferent grammar frameworks have been produced
for the same set of texts. For example, for the
WSJ sections of the PTB, annotations in the style
of dependency grammar, CCG, LFG and HPSG
have become available. Such double annotations
have helped the cross-framework development and
evaluation of parsing systems. However, it must
be noted that the influence of the original PTB an-
notations and the assumptions implicit in the con-
version programs have made the independence of
such new treebanks at least questionable. To our
knowledge, there is no completely independent
annotation of the WSJ texts built without conver-
sion from the original PTB trees.
Another popular alternative way to aid treebank
development is to use automatic parsing outputs
as guidance. Many state-of-the-art parsers are
able to efficiently produce large amount of anno-
tated syntactic structures with relatively high ac-
curacy. This approach has changed the role of
human annotation from a labour-intensive task of
drawing trees from scratch to a more intelligence-
demanding task of correcting parsing errors, or
eliminating unwanted ambiguities (cf., the Red-
woods Treebank (Oepen et al, 2002)). It is our
aim in this on-going project to build a HPSG tree-
bank for the WSJ sections of the PTB based on the
hand-written ERG for English.
3 The Annotation Scheme
3.1 Grammars & Tools
The treebank under construction in this project
is in line with the so-called dynamic treebanks
(Oepen et al, 2002). We rely on the HPSG anal-
yses produced by the ERG, and manually dis-
ambiguate the parsing outputs with multiple an-
notators. The development is heavily based on
the DELPH-IN2 software repository and makes
use of the English Resource Grammar (ERG;
Flickinger (2002), PET (Callmeier, 2001), an ef-
ficient unification-based parser which is used in
2http://www.delph-in.net/
our project for parsing the WSJ sections of the
PTB, and [incr tsdb()] (Oepen, 2001), the gram-
mar performance profiling system we are using,
which comes with a complete set of GUI-based
tools for treebanking. Version control system also
plays an important role in this project.
3.2 Preprocessing
The sentences from the Wall Street Journal Sec-
tions of the Penn Treebank are extracted with their
original tokenization, with each word paired with
a part-of-speech tag. Each sentence is given a
unique ID which can be used to easily look up its
origin in the PTB.
3.3 Annotation Cycles
The annotation is organised into iterations of
parsing, treebanking, error analysis and gram-
mar/treebank update cycles.
Parsing Sentences from the WSJ are first parsed
with the PET parser using the ERG. Up to
500 top readings are recorded for each sentence.
The exact best-first parsing mode guarantees that
these recorded readings are the ones that have
?achieved? highest disambiguation scores accord-
ing to the current parse selection model, without
enumerating through all possible analyses.
Treebanking The parsing results are then man-
ually disambiguated by the annotators. However,
instead of looking at individual trees, the annota-
tors spend most of their effort making binary de-
cisions on either accepting or rejecting construc-
tions. Each of these decisions, called discrim-
inants, reduces the number of the trees satisfy-
ing the constraints (see Figure 1). Every time a
decision is made, the remaining set of trees and
discriminants are updated simultaneously. This
continues until one of the following conditions is
met: i) if there is only one remaining tree and it
represents a correct analysis of the sentence, the
tree is marked as gold; ii) if none of the remain-
ing trees represents a valid analysis, the sentence
will be marked as ?rejected?, indicating an error
in the grammar3; iii) if the annotator is not sure
about any further decision, a ?low confidence?
3In some cases, the grammar does produce a valid read-
ing, but the disambiguation model fails to rank it among the
top 500 recorded candidates. In practice, we find such er-
rors occuring frequently during the first annotation circle, but
they diminish quickly when the disambiguation model gets
updated.
171
Figure 1: Treebanking Interface with an example sentence, candidate readings, discriminants and the MRS. The top row of
the interface is occupied by a list of functional buttons, followed by a line indicating the sentence ID, number of remaining
readings, number of eliminated readings, annotator confidence level, and the original PTB bracket annotation. The left part
displays the candidate readings, and their corresponding IDs (ranked by the disambiguation model). The right part lists all the
discriminants among the remaining readings. The lower part shows the MRS of one candicate reading.
state will be marked on the sentence, saved to-
gether with the partial disambiguation decisions.
Generally speaking, given n candidate trees, on
average log2 n decisions are needed in order to
fully disambiguate. Given that we set a limit of
500 candidate readings per sentence, the whole
process should require no more than 9 decisions.
If both the syntactic and the MRS analyses look
valid, the tree will be recorded as the gold read-
ing for the sentence. It should be noted here that
the tree displayed in the treebanking window is
an abbreviated representation of the actual HPSG
analysis, which is much more informative than the
phrase-structure tree shown here.
Grammar & Treebank Update While the
grammar development is independent to the tree-
banking progress, we periodically incorporate the
recent changes of the grammar into the treebank
annotation cycle. When a grammar update is in-
corporated, the treebank will be updated accord-
ingly by i) parsing all the sentences with the new
grammar; ii) re-applying the recorded annotation
decisions; iii) re-annotating those sentences which
are not fully disambiguated after step ii. The ex-
tra manual annotation effort in treebank update is
usually small when compared to the first round an-
notation.
Another type of update happens more fre-
quently without extra annotation cost. When a
new portion of the corpus is annotated, this is used
to retrain the parse disambiguation model. This
improves the parse selection accuracy and reduces
the annotation workload.
3.4 Grammar coverage & robust parsing
Not having been specifically tuned for the newspa-
per texts, the ERG achieved out-of-box coverage
of over 80% on the WSJ dataset. While this is a re-
spectably high coverage for a hand-written preci-
sion grammar, the remaining 20% of the data is not
covered by the first round of annotation. We plan
to parse the remaining data using a less-restrictive
probabilistic context-free grammar extracted from
the annotated part of the treebank. The PCFG
parser will produce a pseudo-derivation tree, with
which robust unifications can be applied to con-
struct the semantic structures (Zhang and Kordoni,
172
the
DET
nation
N
N
N
NP
?s
DET
DET
largest
ADJ
pension
N
fund,
N
N
N
N
which
N
NP
oversees
V/NP
V/NP
$
N
N
NP
80
ADJ
billion
ADJ
ADJ
N
N
NP
VP/NP
for
P
college
N
N
employees,
N
N
N
N
N
NP
PP
VP/NP
S/NP
S
N
N
NP
plans
V
V
VP
to
P
offer
V
V
two
ADJ
DET
new
AP
investment
N
options
N
N
N
N
NP
VP
to
P
its
DET
1.2
ADJ
million
ADJ
ADJ
participants.
N
N
N
N
NP
PP
VP
PP
VP
S
Figure 2: An example tree including a ?heavy? NP-subject, a relative clause, and noun-noun compounds
2008).
3.5 Multiple annotations
To speed up the annotation, the project employs
three annotators. They are assigned with slightly
overlapping sections of the WSJ dataset. The
overlapping part allows us to measure the inter-
annotator agreement for the purpose of quality
control. To estimate the agreement level, the WSJ
Section 02 has been completely annotated by all
three annotators. Analysis shows that the annota-
tors reach exact match agreement for around 50%
of the sentences. Many disagreements are re-
lated to subtle variations in the linguistic analy-
ses. The agreement level shows improvement af-
ter several treebanker meetings. For future devel-
opment, a more fine-grained disagreement assess-
ment is planned.
4 Discussion
The WSJ section of the PTB is not only a chal-
lenging corpus to parse with a hand-written gram-
mar. It also contains various interesting and chal-
lenging linguistic phenomena. Figure 2, for in-
stance, shows the syntactic analysis that the ERG
produces for a sentence which includes a ?heavy?
NP (noun phrase) containing a relative clause in-
troduced by which in the subject position, as well
as many interesting compound nouns whose inter-
pretations are missing from the PTB annotation.
The newly annotated data will be also very im-
portant for the cross-framework parser develop-
ment and evaluation. While almost all of the state-
of-the-art statistical parsers for English use PTB
annotations for training and testing, it would be
interesting to see whether a comparable level of
parsing accuracy can be reproduced on the same
texts when re-annotated independently.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The tiger treebank. In
Proceedings of the workshop on treebanks and linguistic
theories, pages 24?41.
Ulrich Callmeier. 2001. Efficient parsing with large-scale
unification grammars. Master?s thesis, Universita?t des
Saarlandes, Saarbru?cken, Germany.
Dan Flickinger. 2002. On building a more efficient grammar
by exploiting types. In Stephan Oepen, Dan Flickinger,
Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative
Language Engineering, pages 1?17. CSLI Publications.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora Vi-
dova?-Hladka?. 2000. The Prague Dependency Treebank:
A Three-Level Annotation Scenario. In A. Abeille?, editor,
Treebanks: Building and Using Parsed Corpora, pages
103?127. Amsterdam:Kluwer.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of english: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christo-
pher Manning, Dan Flickinger, and Thorsten Brants.
2002. The LinGO Redwoods treebank: motivation and
preliminary applications. In Proceedings of COLING
2002: The 17th International Conference on Computa-
tional Linguistics: Project Notes, Taipei, Taiwan.
Stephan Oepen. 2001. [incr tsdb()] ? competence
and performance laboratory. User manual. Technical
report, Computational Linguistics, Saarland University,
Saarbru?cken, Germany.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Yi Zhang and Valia Kordoni. 2008. Robust Parsing with a
Large HPSG Grammar. In Proceedings of the Sixth Inter-
national Language Resources and Evaluation (LREC?08),
Marrakech, Morocco.
173
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 226?229,
Paris, October 2009. c?2009 Association for Computational Linguistics
Using Treebanking Discriminants as Parse Disambiguation Features
Md. Faisal Mahbub Chowdhury? and Yi Zhang? and Valia Kordoni?
? Dept of Computational Linguistics, Saarland University
? Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
{chowd,yzhang,kordoni}@coli.uni-sb.de
Abstract
This paper presents a novel approach of in-
corporating fine-grained treebanking deci-
sions made by human annotators as dis-
criminative features for automatic parse
disambiguation. To our best knowledge,
this is the first work that exploits treebank-
ing decisions for this task. The advan-
tage of this approach is that use of human
judgements is made. The paper presents
comparative analyses of the performance
of discriminative models built using tree-
banking decisions and state-of-the-art fea-
tures. We also highlight how differently
these features scale when these models are
tested on out-of-domain data. We show
that, features extracted using treebanking
decisions are more efficient, informative
and robust compared to traditional fea-
tures.
1 Introduction
State-of-the-art parse disambiguation models are
trained on treebanks, which are either fully hand-
annotated or manually disambiguated from the
parse forest produced by the parser. While most
of the hand-annotated treebanks contain only gold
trees, treebanks constructed from parser outputs
include both preferred and non-preferred analy-
ses. Some treebanking environments (such as
the SRI Cambridge TreeBanker (Carter, 1997) or
[incr tsdb()] (Oepen, 2001)) even record
the treebanking decisions (see section 2) that the
annotators take during manual annotation. These
treebanking decisions are, usually, stored in the
database/log files and used later for dynamic prop-
agation if a newer version of the grammar on the
same corpus is available (Oepen et al, 2002). But
until now, to our best knowledge, no research has
been reported on exploiting these decisions for
building a parse disambiguation model.
Previous research has adopted two approaches
to use treebanks for disambiguation models. One
approach, known as generative, uses only the gold
parse trees (Ersan and Charniak, 1995; Charniak,
2000). The other approach, known as discrimi-
native, uses both preferred trees and non-preferred
trees (Johnson et al, 1999; Toutanova et al, 2005).
In this latter approach, features such as local con-
figurations (i.e., local sub-trees), grandparents, n-
grams, etc., are extracted from all the trees and
are utilized to build the model. Neither of the ap-
proaches considers cognitive aspects of treebank-
ing, i.e. the fine-grained decision-making process
of the human annotators.
In this paper, we present our ongoing study of
using treebanking decisions for building a parse
disambiguation model. We present comparative
analyses among the features extracted using tree-
banking decisions and the state-of-the-art feature
types. We highlight how differently these features
scale when they are tested on out-of-domain data.
Our results demonstrate that features extracted us-
ing treebanking decisions are more efficient, in-
formative and robust, despite the total number of
these features being much less than that of the tra-
ditional feature types.
The rest of this paper is organised as follows
? section 2 presents some motivation along with
definition of treebanking decisions. Section 3 de-
scribes the feature extraction templates that have
been used for treebanking decisions. Section 4 ex-
plains the experimental data, results and analyses.
Section 5 concludes the paper with an outline of
our future research.
2 Treebanking decisions
One of the defining characteristics of Redwoods-
style treebanks1 (Oepen et al, 2002) is that the
candidate trees are constructed automatically by
1More details available in http://redwoods.stanford.edu.
226
D1 SUBJH the dog || barks
D2 HSPEC the || dog barks
D3 FRAG_NP the dog barks
D4 HSPEC the || dog
D5 NOUN_N_CMPND dog || barks
. . . . . .
D6 PLUR_NOUN_ORULE barks
D7 v_-_le barks
D8 n_-_mc_le barks
Figure 1: Example forest and discriminants
the grammar, and then manually disambiguated by
human annotators. In doing so, linguistically rich
annotation is built efficiently with minimum man-
ual labor. In order to further improve the manual
disambiguation efficiency, systems like [incr
tsdb()] computes the difference between can-
didate analyses. Instead of looking at the huge
parse forest, the treebank annotators select or re-
ject the features that distinguish between different
parses, until only one parse remains. The number
of decisions for each sentence is normally around
log2(n) where n is the total number of candidate
trees. For a sentence with 5000 candidate read-
ings, only about 12 treebanking decisions are re-
quired for a complete disambiguation. A similar
method was also proposed in (Carter, 1997).
Formally, a feature that distinguishes between
different parses is called a discriminant. For
Redwoods-style treebanks, this is usually ex-
tracted from the syntactic derivation tree of the
Head-driven Phrase Structure Grammar (HPSG)
analyses. Figure 1 shows a set of example dis-
criminants based on the two candidate trees.
A choice (acceptance or rejection, either manu-
ally annotated or inferred by the system) made on
a discriminant is called a decision. In the above
example, suppose the annotator decides to accept
the binary structure the dog || barks as a subject-
head construction and assigns a value yes to dis-
criminant D1, the remaining discriminants will
also receive inferred values by deduction (no for
D2, no for D3, yes for D4, etc). These decisions
are stored and used for dynamic evolution of the
treebank along with the grammar development.
Treebank decisions (especially those made by
annotators) are of particular interest to our study
of parse disambiguation. The decisions record the
fine-grained human judgements in the manual dis-
ambiguation process. This is different from the
traditional use of treebanks to build parse selec-
tion models, where a marked gold tree is picked
from the parse forest without concerning detailed
selection steps. Recent study on double annotated
treebanks (Kordoni and Zhang, 2009) shows that
annotators tend to start with the decisions with the
most certainty, and delay the ?hard? decisions as
much as possible. As the decision process goes,
many of the ?hard? discriminants will receive an
inferred value from the certain decisions. This
greedy approach helps to guarantee high inter-
annotator agreement. Concerning the statistical
parse selection models, the discriminative nature
of these treebanking decisions suggests that they
are highly effective features, and if properly used,
they will contribute to an efficient disambiguation
model.
3 Treebanking Decisions as
Discriminative Disambiguation
Features
We use three types of feature templates for tree-
banking decisions for feature extraction. We refer
to the features extracted using these templates as
TDF (Treebanking Decision Feature) in the rest of
this paper. The feature templates are
T1: discriminant + lexical types of the yield
T2: discriminant + rule(left-child)2 + rule(right-child)
T3: instances of T2 + rule(parent) + rule(siblings)
TDFs of T1, T2 and T3 in combination are re-
ferred to as TDFC or TDFs with context. For
example in Figure 1, instance of T1 for the
discriminant D4 is ?HSPEC3 + le_type(the)4 +
le_type(dog)"; instance of T2 is ?HSPEC + rule(
DET) + rule(N) "; and instance of T3 is ?HSPEC +
rule(DET ) + rule(N) + rule(S) + rule(VP)".
A TDF represents partial information about the
right parse tree (as most usual features). But in
some way, it also indicates that it was a point of
a decision (point of ambiguity with respect to the
underlying pre-processing grammar), hence carry-
ing some extra bit of information. TDFs allow to
2rule(X) represents the HPSG rule, applied on X, ex-
tracted from the corresponding derivation tree.
3HSPEC is the head-specifier rule in HPSG
4le_type(X) denotes the abstract lexical type of word X
inside the grammar.
227
omit certain details inside the features by encod-
ing useful purposes of relationships between lexi-
cal types of the words and their distant grandpar-
ents without considering nodes in the intermediate
levels (allowing some kind of underspecification).
In contrast, state-of-the-art feature types contain
all the nodes in the corresponding branches of
the tree. While they encode ancestor information
(through grandparenting), but they ignore siblings.
TDFs include siblings along with ancestor. Unlike
traditional features, which are generated from all
possible matches (which is huge) of feature types
followed by some frequency cut-offs, the selection
of TDFs is directly restricted by the small num-
ber of treebanking decisions themselves and ex-
haustive search is not needed. It should be noted
that, we do not use treebanking decisions made for
the parse forest of one sentence to extract features
from the parse forest of another sentence. That is
why, the number of TDFs is much smaller than
that of traditional features. This also ensures that
TDFs are highly correlated to the corresponding
constructions and corresponding sentences from
where they are extracted.
4 Experiment
4.1 Data
We use a collection of 8593 English sentences
from the LOGON corpus (Oepen et al, 2004) for
our experiment. 874 of them are kept as test items
and the remaining 7719 items are used for train-
ing. The sentences have an average length of 14.68
and average number of 203.26 readings per sen-
tence. The out-of-domain data are a set of 531
English Wikipedia sentences from WeScience cor-
pus (Ytrest?l et al, 2009).
Previous studies (Toutanova et al, 2005; Os-
borne and Baldridge, 2004) have reported rela-
tively high exact match accuracy with earlier ver-
sions of ERG (Flickinger, 2000) on datasets with
very short sentences. With much higher structural
ambiguities in LOGON and WeScience sentences,
the overall disambiguation accuracy drops signifi-
cantly.
4.2 Experimental setup and evaluation
measures
The goal of our experiments is to compare var-
ious types of features (with TDF) in terms of
efficiency, informativeness, and robustness. To
compare among the feature types, we build log-
linear training models (Johnson et al, 1999) for
parse selection (which is standard for unification-
based grammars) for TDFC, local configurations,
n-grams and active edges5. For each model, we
calculate the following evaluation metrics ?
? Exact (match) accuracy: it is simply the percentage
of times that the top-ranked analysis for each test sen-
tences is identical with the gold analysis of the same
sentence.
? 5-best (match) accuracy: it is the percentage of times
that the five top-ranked analyses for each of the sen-
tences contain the gold analysis.
? Feature Hit Count (FHC): it is the total number of oc-
currences of the features (of a particular feature type)
inside all the syntactic analyses for all the test sen-
tences. So, for example, if a feature (of a particular
feature type) is observed 100 times, then these 100 oc-
currences are added to the total FHC.
? Feature Type Hit Count (FTHC): it is the total num-
ber of distinct features (of the corresponding feature
type) observed inside the syntactic analyses of all the
test sentences.
While exact and 5-best match measures show
relative informativeness and robustness of the fea-
ture types, FHC and FTHC provide a more com-
prehensive picture of relative efficiencies.
4.3 Results and discussion
As we can see in Table 1, local configurations
achieve highest accuracy among the traditional
feature types. They also use higher number of fea-
tures (almost 2.7 millions). TDFC do better than
both n-grams and active edges, even with a lower
number of features. Though, local configurations
gain more accuracy than TDFC, but they do so at
a cost of 50 times higher number of features. This
indicates that features extracted using treebanking
decisions are more informative.
For out-of-domain data (Table 1), there is a big
drop of accuracy for local configurations. Active
edges and TDFC also have some accuracy drop.
Surprisingly, n-grams do better with our out-of-
domain data than in-domain, but still that accuracy
is close to that of TDFC. Note that n-grams have
8 times higher number of features than TDFC.
Hence, according to these results, TDFC are more
robust, for out-of-domain data, than local config-
urations and active edges, and almost as good as
n-grams.
5Active edges correspond to the branches (i.e. one daugh-
ter in turn) of the local sub-trees.
228
Feature Total 5-best accuracy 5-best accuracy Exact accuracy Exact accuracy
template features (in-domain) (out-of-domain) (in-domain) (out-of-domain)
n-gram 438,844 68.19% 62.71% 41.30% 42.37%
local configuration 2,735,486 75.51% 64.22% 50.69% 44.44%
active edges 89,807 68.99% 61.77% 41.88% 39.92%
TDFC 53,362 70.94% 62.71% 43.59% 41.05%
Table 1: Accuracies obtained on both in-domain and out-of-domain data using n-grams (n=4), local
configurations (with grandparenting level 3), active edges and TDFC.
Feature FHC FTHC Active
template features
n-gram 18,245,558 32,425 7.39%
local config. 62,060,610 357,150 13.06%
active edges 22,902,404 27,540 30.67%
TDFC 21,719,698 17,818 33.39%
Table 2: FHC and FTHC calculated for in-domain
data.
The most important aspect of TDFC is that they
are more efficient than their traditional counter-
parts (Table 2). They have significantly higher
number of active features ( FTHCTotalFeature# ) than n-grams and local configurations.
5 Future work
The results of the experiments described in this pa-
per indicate a good prospect for utilizing treebank-
ing decisions, although, we think that the types of
feature templates that we are using for them are
not yet fully conveying cognitive knowledge of the
annotators, in which we are specifically interested
in. For instance, we expect to model human dis-
ambiguation process more accurately by focusing
only on human annotators? decisions (instead of
only inferred decisions). Such a model will not
only improve the performance of the parsing sys-
tem at hand, but can also be applied interactively
in treebanking projects to achieve better annota-
tion speed (e.g., by ranking the promising discrim-
inants higher to help annotators make correct de-
cisions). Future experiments will also investigate
whether any pattern of discriminant selection by
the humans can be learnt from these decisions.
References
David Carter. 1997. The treebanker: A tool for supervised
training of parsed corpora. In Proceedings of the Work-
shop on Computational Environments for Grammar De-
velopment and Linguistic Engineering, Madrid, Spain.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational Lin-
guistics (NAACL 2000), pages 132?139, Seattle, USA.
Murat Ersan and Eugene Charniak. 1995. A statistical syn-
tactic disambiguation program and what it learns. pages
146?159.
Dan Flickinger. 2000. On building a more efficient grammar
by exploiting types. 6(1):15?28.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unifcation-based grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL 1999), pages 535?541, Maryland, USA.
Valia Kordoni and Yi Zhang. 2009. Annotating wall street
journal texts using a hand-crafted deep linguistic gram-
mar. In Proceedings of The Third Linguistic Annotation
Workshop (LAW III), Singapore.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christo-
pher Manning, Dan Flickinger, and Thorsten Brants.
2002. The LinGO Redwoods treebank: motivation and
preliminary applications. In Proceedings of COLING
2002: The 17th International Conference on Computa-
tional Linguistics: Project Notes, Taipei, Taiwan.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer, Tor-
bj?rn Nordg?rd, and Victoria Ros?n. 2004. Som ? kapp-
ete med trollet? towards mrs-based norwegian-english
machine translation. In Proceedings of the 10th Interna-
tional Conference on Theoretical and Methodological Is-
sues in Machine Translation, pages 11?20, MD, USA.
Stephan Oepen. 2001. [incr tsdb()] ? competence and
performance laboratory. User manual. Technical report,
Computational Linguistics, Saarland University, Saar-
br?cken, Germany.
Miles Osborne and Jason Baldridge. 2004. Ensemble-based
active learning for parse selection. In HLT-NAACL 2004:
Main Proceedings, pages 89?96, Boston, USA.
Kristina Toutanova, Christoper D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse selec-
tion using the Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83?105.
Gisle Ytrest?l, Stephan Oepen, and Daniel Flickinger. 2009.
Extracting and annotating wikipedia sub-domains. In Pro-
ceedings of the 7th International Workshop on Treebanks
and Linguistic Theories, pages 185?197, Groningen, the
Netherlands.
229
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 587?595, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Combining Multiple Forms of Evidence While Filtering
Yi Zhang ?
Information System and Technology Management
School of Engineering
University of California, Santa Cruz
Santa Cruz, CA 95064, USA
yiz@soe.ucsc.edu
Jamie Callan
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
callan@cs.cmu.edu
Abstract
This paper studies how to go beyond relevance
and enable a filtering system to learn more in-
teresting and detailed data driven user models
from multiple forms of evidence. We carry out
a user study using a real time web based per-
sonal news filtering system, and collect exten-
sive multiple forms of evidence, including ex-
plicit and implicit user feedback. We explore
the graphical modeling approach to combine
these forms of evidence. To test whether the ap-
proach can help us understand the domain bet-
ter, we use graph structure learning algorithm
to derive the causal relationships between dif-
ferent forms of evidence. To test whether the
approach can help the system improve the per-
formance, we use the graphical inference algo-
rithms to predict whether a user likes a docu-
ment based on multiple forms of evidence. The
results show that combining multiple forms
of evidence using graphical models can help
us better understand the filtering problem, im-
prove filtering system performance, and handle
various data missing situations naturally.
1 Introduction
An adaptive personal information filtering system is an
autonomous agent that delivers information to the user in
a dynamic environment over a period of time. A com-
mon filtering approach is adapting existing text classi-
fication/retrieval algorithms to classify incoming docu-
ments as either relevant or non relevant using user pro-
files learned from explicit user feedback on documents
the user has seen. However, there are other important
criteria for the user besides relevance, such as readabil-
ity (Collins-Thompson and Callan, 2004), novelty (Har-
man, 2003), and authority (Kleinberg, 1998). Besides,
much information about the user and the document can
be collected by a filtering system. These suggest a way to
improve the current filtering system: going beyond rele-
vance and using multiple forms of evidence.
?This research was done while at the Language Technolo-
gies Institute, Carnegie Mellon University.
Unfortunately, there is no standard evaluation data set
for this research, and there is not much work on finding
a good theory to combine various forms of evidence. To
solve the first problem, we designed a user study and col-
lect thousands of cases with multiple forms of evidence,
including the content of a document, explicit and im-
plicit user feedback, such as a user?s mouse usage, key
board usage, document length, novelty, relevance, read-
ability, authority, user profile characteristics, news source
information, and whether a user likes a document or not.
Solving the second problem is very challenging. A good
model should have the representation power to combine
multiple forms of evidence; it should be able to help us
understand the relationships between various forms of ev-
idence; it should use the evidence to improve filtering
system performance; and it should handle various prob-
lems like missing data in an operational environment ro-
bustly.
On the other hand, researchers have identified three
major advantages of graphical modeling approach: 1) it
provides inference tools to naturally handle situations of
missing data entry because of the conditional dependen-
cies encoded in the graph structure; 2) it can learn causal
relationships in the domain, thus help us to understand
the problem and to predict the consequences of interven-
tion; and 3) it can easily combine prior knowledge (such
as partial information about the causal relationship) with
data in this framework. This approach has been applied
to model computer software users (Horvitz et al, 1998),
car drivers (Pynadath and Wellman, 1995), and students
(Conati et al, 1997). Motivated by the prior work, we
choose to use graphical models as our solution. To under-
stand relationships between various forms of evidence,
we use the causal graph structure learning algorithms (ad-
vantage 2), together with some prior knowledge of the
domain (advantage 3), to derive the causal relationships
between different user feedback, actions and user con-
text. To improve the existing filtering system, especially
in the situation of missing data, we use statistical infer-
ence tools to predict how a user will like a document,
using information available in different missing evidence
situations (advantage 1). We also try linear regression as
an alternative approach.
The following sections describe our efforts towards
587
Figure 1: The user study system structure. The structured
information, such as user feedback and crawler statistics,
are kept in the database. The content of each web page
crawled is saved in the news repository.
collecting data and customizing the graphical modeling
approach to combine multiple forms of evidence for fil-
tering. We begin with a description of the user study in
Section 2, followed by some preliminary data analysis
on the data collected in Section 3. Section 4 explores
causal structure learning algorithm to understand the re-
lationships between various forms of evidence from the
data and Section 5 explores how to improve the system
performance using multiple forms of evidence. Section
6 discusses related work and how this work differs from
existing work, and Section 7 concludes.
2 User Study
No existing filtering database contains the level of detail
that we needed for our study, so we developed a web
based news story filtering system to collect an evalua-
tion data set (Figure 1). This system constantly gathers
and recommends information to the users. The system
includes a crawler with 8000 candidate RSS news feeds
(Pilgrim, 2002) to crawl every day. The Lemur indexer
indexes the crawled document stream incrementally, and
an adaptive filtering system recommends documents to
the users using a modified logistic regression algorithm
(Zhang, 2004). Users read and evaluate what the system
has delivered to them. An example of the web interface
after user login is in Figure 2.
More than 20 paid subjects from 19 different programs
at Carnegie Mellon University, who are otherwise not af-
filiated with our research, participated in the study for 4
weeks. We expected to collect enough data for evalua-
tion over this period of time. The subjects were required
to read the news for about 1 hour per day and provide
explicit feedback for each page they visited. 1 28 users
1In the last week of the study, some subjects read 2 hours
per day. They are encouraged but not required to do so.
Figure 2: Web interface after a user logged in.
Figure 3: Evaluation user interface. The interface for user
to give their explicit feedback of the current news story.
tried this system. However, only 21 users are official paid
subjects, among which one worked only for 2 weeks and
20 worked for about 4 weeks.
2.1 Data collected
We have collected 7881 feedback entries from all 28
users, among which 7839 were from the 21 official par-
ticipants. Each entry contains several different forms of
evidence for a news story a user clicked.2 Our intention
to collect the evidence is not to be exhaustive, but repre-
sentative. The evidence can be roughly classified into the
following five categories listed in Tables 1 to 5.3
Explicit user feedback After finishing reading a news
story, a user clicks a button on the toolbar of the
browser to bring up an evaluation interface shown in
Figure 3. Through this interface, the user provided
the explicit feedback to tell the hidden properties
about current story, including the topics the news
belongs to (classes), how the user likes this news
2Each entry is for a <document, user class, time> tuple.
3The forms of evidence are listed in the first column and we
will get the the other columns later in Section 3.
588
(user likes), how relevant the news is related to the
class(es) (relevant), how novel the news is (novel),
whether the news matches the readability level of
the user (readable), and whether the news is au-
thoritative (authoritative). user likes, relevant
and novel are recorded as integers ranging from 1
(least) to 5 (most). readable and authoritative are
recorded as 0 or 1. A user has the option to provide
partial instead of all explicit feedback. A user can
create new classes, and choose multiple classes for
one documents.
User actions The browser adapted from (Claypool et al,
2001) recorded some user actions, such as mouse ac-
tivities, scroll bar activities, and keyboard activities
(Table 2). TimeOnPage is the number of seconds the
user spent on a page, and EventOnScroll is the num-
ber of clicks on the scroll bars. When the mouse
is out of the browser window or when the browser
window is not focused, the browser does not capture
any activities. More details about the actions are in
(Le and Waseda, 2000).
Topic information Each participant filled out an exit
questionnaire and answered several topic/class4 spe-
cific questions for each of his/her most popular 10
topics and other topics with more than 20 evalu-
ated documents each (Table 3). The questions in-
clude how familiar the user is with the topic be-
fore the study (topic familiar before), how the user
likes this topic (topic like), and how confident the
user is with respect to the answers he/she provided
(topic confidence). We include this information
as evidence, because they may be collected when
a topic is created and used by filtering systems.
Whether collecting them in exit questionnaire af-
fects the answers needs further investigation.
News Source Information For each news source (RSS
feed), we collected the number of web pages that
link to it (RSS link), the number of pages that link to
the server that provided it (host link), and the speed
of the server that hosts it.
Content based evidence Three pieces of evidence are
collected to represent the content of each document:
the relevance score, the readability score and the
number of words in the document (doc len) (Table
5). To estimate the relevance score of a document,
the system processes all the documents a user put
into a class ordered by the feedback time and adap-
tively learns a topic specific relevance model using
the relevance feedback the user provided. The rel-
evance score of a documents is estimated using a
4
?topic? and ?class? are used interchangeably in the paper.
Table 1: Basic descriptive statistics about explicit feed-
backs.
Variable Mean variance corr miss
user likes 3.5 1.2 1 0.05
relevant 3.5 1.3 0.73 0.005
novel 3.6 1.33 0.70 0.008
authoritative 0.88 0.32 0.50 0.065
readable 0.90 0.30 0.54 0.012
modified logistic regression model learned from all
feedback before it (Zhang, 2004). To estimate the
readability score of document, the system processes
all the documents in all users? classes ordered by the
feedback time and adaptively learns a user indepen-
dent readability model using a logistic regression al-
gorithm.
3 Preliminary data analysis
The means and variances of all variables are in Tables 1 to
5. These basic descriptive statistics are very diverse. The
values of some evidence may be missing; only the user
actions and news source information were always col-
lected. Out of the 7991 entries, only 4522 (57%) entries
contain no missing value. The missing rate of each form
of evidence is also reported in the tables. There are sev-
eral reasons for missing data. For example, the explicit
feedback is missing because users didn?t always follow
instructions, the relevance score is missing for the first
story in a class, and the topic familiar before values
for many topics are missing because we only collected
the topic specific answers for larger topics. We expect
missing data to be common in operational environments.
The correlation coefficient between each evidence and
the explicit feedback user likes is also listed (corr).
The high correlation coefficients between user likes and
other forms of explicit feedback are not very interest-
ing because we can only get explicit feedback after a
user reads the document. The correlation coefficient be-
tween relevance score and user likes is 0.37, the highest
among all forms of evidence that the system can get be-
fore delivering a document. This is not surprising since
most filtering systems only consider relevance and use
relevance score to make decisions.
The correlation coefficients between user likes and
the topic information (Table 3) are relatively high.
This suggests collecting topic familiar before or
topic like in a real filtering system, since they are in-
formative and collecting them requires less user effort (a
user only needs to provide information on the class level
instead of document level). Section 5 will show how to
use it with other forms of evidence in a filtering system.
The correlation coefficients between the news source in-
589
Table 2: Basic descriptive statistics about user actions.
The unit for time is second.
Variable Mean variance corr
TimeOnPage 7.2? 104 1.3? 105 0.14
EventOnScroll 1 3.6 0.1
ClickOnWindow 0.93 2.5 0.05
TimeOnMouse 2? 103 5.8? 103 0.02
MSecForDownArrow 211 882 0.08
NumOfDownArrow 1.1 4.7 0.09
MSecForUpArrow 29 240 0.03
NumOfUpArrow 0.10 0.8 0.04
NumOfPageUp 0.12 0.9 ' 0
NumOfPageDown 0.14 1 ' 0
MSecForPageUp 22 202 ' 0
MSecForPageDown 28 251 ' 0
Table 3: Basic descriptive statistics about topics. Each
variable ranges from 1 to 7.
variable Mean variance corr miss
topic familiar before 3.6 1.9 0.30 0.27
topic like 4.9 2.0 0.30 0.27
topic confidence 4.7 2.0 0.34 0.27
Table 4: Basic descriptive statistics about news sources.
variable Mean variance corr
RSS link 90.35 4.89 0.14
host link 4.41? 104 7.5? 107 0.08
RSS SPEED 3.92? 105 3.7? 109 -0.08
Table 5: Basic descriptive statistics about documents.
The length of the document does not include HTML tags.
variable mean variance corr miss
doc length 837 1.2? 103 0.04 0.05
relevant score 0.49 0.42 0.37 0.18
readability score 0.52 0.16 0.25 0.11
formation and user likes are weaker (Table 4). The cor-
relation coefficient between user likes and each user ac-
tion (Table 2) is even lower (Table 1). Some actions, such
as TimeOnPage, are more correlated with user likes
than other refined actions, such as NumOfPageDown.
This finding agrees with (Claypool et al, 2001).
4 Understanding the domain using causal
structure learning
Correlation analysis in Section 3 has helped us to get
some initial idea about the data collected. However, in
order to better understand the underlying truth of the do-
main, we need to go beyond correlation and uncover the
causal relationships between different variables.
To do that, we first specify N nodes, one for each form
of evidence to be included in the model. Then PC algo-
rithm is used (Spirtes et al, 2000) to search the causal
relationships between multiple forms of evidence from
the data collected. To make the search space smaller,
some prior domain knowledge, such as forbidden edges,
required edges or temporal tiers, can be introduced be-
fore searching. In our experiments, we manually spec-
ified some prior knowledge based on the first authors?
experience and intuition as the following 5-tier tempo-
ral tier: 5 1) Topic info = (familiar topic before),
RSS info =(RSS link, host link), document length
(doc len); 2) hidden criteria, such as relevant, novel,
authoritative, and readable; 3) system generated
scores, such as relevance score and readability score; 4)
user likes; 5) user actions, such as seconds spent on a
page (TimeOnPage) or the number of clicks on the ? key
(NumOfDownArrow). This informs the learning algo-
rithm that? from a higher level to lower level is prohib-
ited.
It is very encouraging to see that the structure learned
automatically looks reasonable (Figure 4). Accord-
ing to the graph, novel, relevant, authoritative,
readabilty of a document and whether a user is
familiar with the topic before using the system
(familar topic before) are direct causes of the user?s
preference for a document (user likes) . How fa-
miliar with this topic a user is before participating
the study (topic familiar before) and the number of
web links to the news source (RSS link) directly af-
fect the user?s relevant and authoritative feedback
and readability score. Relevant, authoritative,
familiar topic before and host link influence a user?s
actions, such as the EventOnScroll.
Comparing Tables 2 to 5 with Figure 4, one may ask
why some variables are correlated with user likes al-
though there is no direct links between them and user
likes. For example, why the correlation between rele-
vance score and user likes is 0.39, while there is no di-
rect link between them. Does Figure 4 contradict Ta-
ble 5? The answer is ?no?. In fact the indirect causal
relationship between them tells us why relevance score
and user likes are correlated: relevance score and user
likes have a common cause relevant. Most of the re-
fined actions, such as the number of pressing page up key
(NumOfPageUp), are far away from user likes. This
implies that these refined actions are not very informative
if we want to use the learned model to predict whether
a user likes a document or not. This finding agree with
(Claypool et al, 2001) and Table 2.
The node authoritative is directly linked to
readability score and host link. The link between
host link and authoritative confirms the existing ap-
proaches that use the web link structure to estimate the
5Other priors are also possible.
590
Figure 4: User independent causal graphical structure
learned using PC algorithm. X ? Y means X is a di-
rect cause of Y. X ?Y means the algorithm cannot tell if
X causes Y or if Y causes X. X ?? Y means the algo-
rithm found some problem, which may happen due to a
latent common cause of X and Y, a chance pattern in the
sample, or other violations of assumptions.
Figure 5: Structure of GM complete.
Figure 6: Structure of GM causal.
authority of a page (Kleinberg, 1998). The links between
readability score, readable and authoritative are very
interesting. They suggest the difficulty to understand a
page may make the user feel it is not authoritative. Fur-
ther investigation shows that although the percentage of
un-authoritative news is less than 15% in general, among
the 187 news stories some users identified as ?difficult?
using class labels, 73% were also rated as not authorita-
tive. Besides some successful web page authority algo-
rithms that only use hyper links, the estimation of author-
ity may be further improved using the content of a page.
There are links among relevant, novel, readable and
authoritative. Although the algorithm failed to tell the
causal direction between some pairs of variables, it sug-
gests that the four variables influence each other. This
may be an inherent property of the document; or because
a user is likely to rate one aspect of the document higher
than he/she should if the other aspects are good.
One may ask why the structure in Figure 4 contains
no link between readable and readability score, since
intuitively it should exist. To answer this question, one
needs to understand that the causal relationships learned
automatically are what the algorithm ?believes? based on
the evidence of the data, the assumptions it makes, and
the prior constraints we engineered. They may have er-
rors, because the data is noisy, or the assumptions and
the prior constraints may be wrong. For example, the PC
algorithm do statistical test about the independence re-
lationships among variables using the data and the final
results are subject to the error of the statistical test. The
PC algorithm assumes no hidden variables, however be-
sides relevant, novel, authoritative, and readable, other
hidden variables, such as whether a document is up-to-
date, interesting, misleading, etc. (Schamber and Bate-
man, 1996), may exist and influence a user?s preference
for a document. Thus it is not surprising that some of the
causal relationships, such as the link between readable
and readability score, are missed in the final graph be-
cause of the limitation of the learning algorithms. The
model learned only sheds some light on the relationships
between the variables instead of uncovering the whole
truth. It only serves as a starting point for us. To further
understand the domain, we may want to break down some
variables in the current graph further and relate them to
either the user or document properties. In general, causal
discovery is inherently difficult and far from solved.
5 Improving system performance using
inference algorithms
A primary task of a filtering system is to predict user
preference (user likes) for a document so that the sys-
tem can decide whether to deliver it to the user. To
tell whether combining multiple forms of evidence using
591
graphical models can improve system performance, we
evaluate the proposed solution on the task of predicting
user likes while filtering.
To predict user likes, the system needs to learn a
graphical model: the combination of a graph structure
and a set of local conditional probability functions or po-
tential functions. Doing inference over the causal struc-
ture learned in the previous section is difficult because of
the circles and a mixture of directed and undirected links
on the graph. So, we tried the following directed acyclic
graphical models.
GM complete, an almost complete Bayesian network:
In this graph, we order the nodes from top to bot-
tom, and the parents of a node are all the nodes
above it, such as in Figure 5. For this structure, the
order of the nodes is not very important when using
Gaussian distributions.
GM causal, a graphical model inspired by causal models:
We manually modify the causal structure in Figure
4 to make it a directed acyclic graph as in Figure 6.
In the graphs, RSS info=(RSS link, host link) and Topic
info=topic familiar before, topic like) are 2 dimensional
vectors representing the information about the news
source and the topic in Table 4 and Table 3. actions =
(TimeOnPage, ...) is a 12 dimensional vector repre-
senting the user actions in Table 1. user likes is the
target variable the system wants to predict.
Before learning the parameters of the model, we need
to choose a specific conditional form for the probability
function associated with each node. We chose Gaussian
distributions. If the parents of node X are Y, P (X|Y ) =
N(m + W ? Y,?), where N(?,?) is a gaussian distri-
bution with mean ? and covariance ?. This is a com-
monly used distribution for continuous valued nodes. It
assumes the joint distribution of these variables is mul-
tivariate Gaussian, which may be wrong. Nevertheless,
because of the mathematical convenience, the existence
of efficient learning and inference algorithms for Gaus-
sian networks, and the availability of modeling tools, we
chose this distribution. Using the BNT Toolbox (Mur-
phy, 2001), the maximum likelihood estimations of the
parameters (m,W,?) were learned using EM algorithm
and junction tree inference engine(Cowell et al, 1999)
over the graphical models, with whatever information
was available on the first 2/3 of the data.
An alternative approach to combine multiple forms of
evidence is linear regression. We tried two special meth-
ods to solve the missing evidence problem while using
linear regression: 1) building a model that does not use
the evidence that is missing for each missing situation
(LR different); or 2)mean substitution: replacing each
missing value for an evidence with the average of the
Figure 7: Comparison of the prediction power of differ-
ent models using 7952 cases for evaluation. The vertical
axis is the correlation coefficient between the predicted
value of user likes using the model and the true explicit
feedback provided by the users. The order of different
forms of evidence is set manually, based on how easy it
is to collect each evidence.
observed evidence (LR mean). For K different forms
of evidence, the system may need to handle 2K differ-
ent evidence missing situations. A large number of linear
regression models need to be learned if we use the first
approach, considering K is higher than 15 in some of our
experiments. Building 215 models is almost impossible
for us, so a heuristic approach, which is discussed later,
was used to make the experiments possible.
Not all 7991 cases collected in the user study were
used in the experiments. We conducted two sets of ex-
periments. For the first set of experiments, we use 7952
cases for which user likes is not missing. For the other
set of runs, we use only cases without missing value. In
this task, the value of each variable is continuous and nor-
malized to variance one. Each model is learned using all
information available on the first 2/3 of the cases, and
tested on the remaining 1/3 of the cases. The correlation
coefficient between the predicted value of user likes and
the true explicit user likes feedback provided by the
users is used as the evaluation measure. Our baseline is
using relevance score alone, which has a correlation co-
efficient of 0.367 with 95% confidence interval 0.33-0.40
on the last 1/3 of the 7952 cases.
5.1 Experimental results and discussions
Figure 7 shows the effectiveness of different models
at different testing conditions as indicated by the hor-
izontal axis. From left to right, additional sources
of evidence are given when testing. At the very left
of the figure (x=RSS info), a model predicts the
592
value of user likes only given the value of RSS info
at testing time. ?+explicit? means the explicit feed-
back (except user likes) about the current document is
given besides the value of actions, relevance score,
readability score, RSS info, and TopicInfo. The
graphical models and LR mean model were trained with
all evidence/features, and the learned models are inde-
pendent of the testing condition. LR different models
were only trained with features that are also provided at
testing time, so there is one model per testing condition.
6
The results show that GM complete performs sim-
ilarly to LR different. This is not surprising. Theo-
retically, if there is no missing entries in training data,
GM complete?s estimation of the conditional distribu-
tion of P (user likes|available evidence) would be the
same as that of LR different on a testing case with miss-
ing evidence.
Comparing the correlation coefficients under dif-
ferent testing conditions when using LR different or
GM complete, we can see that as more forms of ev-
idence are available, the performance improves. If
only the news source information of a document
(RSS info) is given, all models perform poorly. The
readability score improves the system performance sig-
nificantly. This is nice and interesting, because the evi-
dence is user independent and can be estimated efficiently
for each document. The performance keeps improving
as topic info and relevance score were added. To
collect them, we needs user feedback on previous doc-
uments. The performance improvement is not very ob-
vious with actions added. This means that given other
evidence (RSS info, topic info, relevance score and
readability score), the system won?t improve its predic-
tion of the document much by observing these actions.
However, this is only true when we use a model learned
for all users and other forms of evidence are available. It
does not mean the actions are useless if we learn user
specific model, or if other forms of evidence (such as
relevance score) are not available. All models perform
very good with explicit feedback added. However, this is
a ?cheating? condition of less interest to us.
The performances of LR mean and GM causal
do not increase monotonically as more forms of ev-
idence are added. They perform much worse than
LR different and GM complete. Why does a structure
that looks more causally reasonable not perform well
6However, for a specific testing condition, the training data
and testing data contain cases where some evidence that is sup-
posed to be available is missing. These cases in training data
were ignored and not used to learn a LR different model.
However, ignoring such kind of cases in testing data makes
comparison of different runs difficult. So we used mean sub-
stitution approach to fill the required missing features in testing
data while using LR different.
Model Cond. corr RLow RUp
LR mean +R 0.2783 0.2426 0.3132
LR different +R 0.4372 0.4058 0.4677
GM complete +R 0.4247 0.3928 0.4555
GM causal +R 0.3078 0.2728 0.342
LR mean +A 0.2646 0.2286 0.2998
LR different +A 0.4375 0.406 0.4679
GM complete +A 0.4315 0.3999 0.4622
GM causal +A 0.3086 0.2736 0.3428
Table 6: A comparison of different models on all data un-
der the +relevance score (+R) and +action (+A) con-
ditions. Corr is the correlation coefficient between the
predicted value of user likes using the model and the
true explicit feedback provided by the users. RLO and
RUP are the lower and upper bounds for a 95% confi-
dence interval for each coefficient.
as the simple GM complete? We may answer this
question better by comparing the underlying assumptions
of these algorithms. GM complete only assumes the
joint distribution of all variables is multivariate Gaus-
sian. GM causal makes much stronger independence
assumptions by removing some links between variables.
As mentioned before, the causal relationships learned au-
tomatically are not perfect, which may cause the poor
performance of GM causal. LR mean also suffers
from the strong conditional independent assumptions.
Table 6 reports the performance together with the
confidence intervals of all the models under the
+relevance score and +actions conditions. Under
both conditions, GM complete and LR different are
statistically significantly better than the baseline 0.367.
LR mean and GM causal are significantly worse. It
means using multiple forms of evidence may hurt some
models and benefit others. Further analysis about the
+actions runs shows that LR mean gave explicit feed-
back too much weight and overlooked other less strong
evidence. At testing time, it did not handle the problem of
missing explicit feedback well and thus performed poorly.
Although GM complete also gave very high weights to
explicit feedback, it could infer the missing values based
on other available evidence at testing time, thus per-
formed better than LR mean. LR different didn?t con-
sider explicit feedback for training, thus it didn?t over-
look other forms of evidence and suffer from the problem
less. LR mean may work reasonably if explicit vari-
ables are not included, however the large difference on
how informative each evidence is will still hurt the per-
formance of LR mean to some extent when some strong
evidence is missing. For GM complete approach, a sin-
gle model is needed to handle various evidence missing
situations. If we use LR different approach, several mod-
els are needed. As we mentioned before, there are 2K
593
Model Cond. Corr RLow RUp
LR mean +R 0.13 0.08 0.18
LR different +R 0.41 0.37 0.45
GM complete +R 0.41 0.37 0.45
GM causal +R 0.41 0.375 0.45
LR mean +A 0.11 0.061 0.16
LR different +A 0.42 0.38 0.46
GM complete +A 0.42 0.38 0.46
GM causal +A 0.38 0.33 0.42
Table 7: The performance on 4522 no missing value cases
under the +relevance score (+R) and +action (+A)
conditions.
different evidence missing combinations, and 2K linear
regression models are needed in order to handle all these
situations using LR different approach. LR different may
be preferred if K is small, while graphical modeling us-
ing GM complete may be a better approach to handle
different data missing situations if K is big.
So far, all results are based on 7952 cases where
some evidence may be missing. We also compared
the models under different testing conditions using the
4522 cases that do not have any missing value (Table
7). GM causal performs significantly better than be-
fore. We need to be very careful with the structures while
using the graphical modeling approach, since a structure
that looks more reasonable may work poorly on the in-
ference task. However, we couldn?t not draw any con-
clusion on whether GM complete is better in general,
because the answer may be different with different con-
ditional probability distributions, different data sets, or a
better structure learning algorithm.
6 Related Work
There has been some research on news filtering using
time-coded implicit feedback (Lang, 1995; Morita and
Shinoda, 1994). We noticed that an independent work
uses a different graphical modeling approach, depen-
dency network, to understand the relationships between
implicit measures and explicit satisfaction while user
were conducting their web searches and viewing results,
and then uses decision tree to predict user satisfaction
with results (Fox et al, 2005). Our work differs from
the previous work in the goal of the task, the range of ev-
idence considered, the modeling approach we took, and
the findings reached.
There has been a lot of related research on using im-
plicit feedback (Kelly and Teevan, 2003). The user
actions we collected are based on (Claypool et al,
2001). There is much work about how to handle miss-
ing data. (Schafer and Graham, 2002) discussed several
approaches such as case deletion, mean substitution, and
recommended maximum likelihood (ML) and Bayesian
multiple imputation (MI). LR mean uses mean substitu-
tion, LR different uses case deletion, and graphical mod-
els follow the ML approach.
There has been some research on criteria beyond topic
relevance (Carbonell and Goldstein, 1998) (Zhang et al,
2002) (Collins-Thompson and Callan, 2004) (Kleinberg,
1998). (Schamber and Bateman, 1996) identified crite-
ria underlying users? relevance judgements and explored
how users employed the criteria in making evaluations
by asking users to interpret and sort criteria independent
of document manually. In the literature, the word ?rele-
vant? is used ambiguously, either as a narrow definition
of ?related to the matter at hand (aboutness)? or a broader
definition of ?having the ability to satisfy the needs of the
user?. When it is used by the second definition, such as
in (Schamber and Bateman, 1996), researchers are usu-
ally studying what we refer to as user likes. In this paper,
we use ?relevant? as is defined in the first definition and
use the phrase ?user likes? for the second definition. De-
spite the vocabulary difference, our work is motivated by
the early research. The major contributions of our work
in this area are: 1) we model the user likes and other cri-
teria as hidden variables; 2) we quantify the importance
of various criteria based on probabilistic reasoning; and
3) we have explored the new methodology for combining
these criteria with implicit and explicit user feedback.
7 CONCLUSION
We have explored how to combine multiple forms of evi-
dence using the graphical modeling approach. This work
is significant because it addresses some long-standing is-
sues in the adaptive information filtering community: the
integration of a wider range of user-specific and user-
independent evidence, and handling situations like miss-
ing data that occur in operational environments.
We have analyzed the user study data using graphical
models, as well as linear regression algorithms. The ex-
perimental results show that the graphical modeling ap-
proach can help us to understand the causal relationships
between multiple forms of evidence in the domain and
explain the real world scenario better. It can also help
the filtering system to predict user preference more accu-
rately with multiple forms of evidence compared to using
a relevance model only.
As more forms of evidence are added, missing data is a
common problem because of system glitches or because
users will not behave as desired. A real system needs to
handle missing data by either ignoring it or by estimat-
ing it based on what is known. The graphical modeling
approach addresses this problem naturally. LR different
handles the problem by building many different models to
be used at different data missing conditions. LR different
and GM complete perform similarly. When the types
594
of evidence is few, LR different probably is preferable
because of the simplicity. However, as more forms of
evidence are added, a more powerful model, such as
GM complete, may be preferred because of the com-
putation and space efficiency.
We only collected data for documents users clicked.
Further investigation is needed to look at data not clicked,
which is a critical step to see whether the improvement on
prediction accuracy of user preference will help the sys-
tem serve the user better in a real system. This is the first
step towards using graphical models to combine multiple
forms of evidence while filtering. The proposed solution,
especially the data analyzing methodology used in this
paper, can also be used in other IR tasks besides filtering,
such as context-based retrieval.
8 Acknowledgments
We thank Jaime Carbonell, Tom Minka, Stephen Robert-
son, Yiming Yang, Wei Xu, Peter Spirtes, Diane Kelley,
Paul Ogilvie, Kevyn Collins-Thompson, Luo Si, Joemon
Jose for valuable discussions about the work described in
this paper.
This research was funded in part by a fellowship from
IBM and a grant from National Science Foundation. Any
opinions, findings, conclusions or recommendations ex-
pressed in this paper are the authors?, and do not neces-
sarily reflect those of the sponsors.
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference.
Mark Claypool, Phong Le, Makoto Wased, and David
Brown. 2001. Implicit interest indicators. In Intel-
ligent User Interfaces.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of the HLT/NAACL 2004 Conference.
C. Conati, A. S. Gertner, K. VanLehn, and M. J.
Druzdzel. 1997. On-line student modeling for
coached problem solving using Bayesian networks. In
Proceedings of the Sixth International Conference on
User Modeling, pages 231?242.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen,
and David J. Spiegelhalter. 1999. Probabilistic Net-
works and Expert Systems. Springer.
Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan
Dumais, and Thomas White. 2005. Evaluating im-
plicit measures to improve web search. In ACM Trans.
Information Systems, volume 23.
Donna Harman. 2003. Overview of the TREC 2002 nov-
elty track. In The Eleventh Text REtrieval Conference
(TREC-11). NIST 500-251.
E. Horvitz, J. Breese, D. Heckerman, D. Hovel, and
K. Rommelse. 1998. The Lumiere project: Bayesian
user modeling for inferring the goals and needs of soft-
ware users. In Proceedings of the Fourteenth Confer-
ence on Uncertainty in Artificial Intelligence, July.
Diane Kelly and Jaime Teevan. 2003. Implicit feedback
for inferring user preference: a bibliography. SIGIR
Forum, 37(2):18?28.
J. Kleinberg. 1998. Authoritative sources in a hyper-
linked environment. In Proc. 9th ACM-SIAM Sympo-
sium on Discrete Algorithms.
Ken Lang. 1995. Newsweeder: Learning to filter news.
In Proceedings of the Twelfth International Conference
on Machine Learning.
Phong Le and Makoto Waseda. 2000. A curious browser:
Implicit ratings. http://www.cs.wpi.edu/ clay-
pool/mqp/iii/.
Masahiro Morita and Yoichi Shinoda. 1994. Informa-
tion filtering based on user behavior analysis and best
match text retrieval. In Proceedings of the 17th ACM
SIGIR conference.
Kevyn Murphy. 2001. The Bayes net toolbox for matlab.
In Computing Science and Statistics.
Mark Pilgrim. 2002. What is RSS.
http://www.xml.com/pub/a/2002/12/18/dive-into-
xml.html.
D.V. Pynadath and W.P. Wellman. 1995. Accounting for
context in plan recognition, with application to traffic
monitoring. In Proceedings of the Eleventh Confer-
ence on Uncertainty in Artificial Intelligence.
Joseph L. Schafer and John W. Graham. 2002. Missing
data: Our view of the state of art. In Psychological
Methods, volume 7, No 2.
Linda Schamber and Judy Bateman. 1996. User crite-
ria in relevance evaluation: Toward development of a
measurement scale. In ASIS 1996 Annual Conference
Proceedings, October.
Perter Spirtes, Clark Glymour, and Richard Scheines.
2000. Causation, Prediction, and Search. The MIT
Press.
Yi Zhang, Jamie Callan, and Tom Minka. 2002. Novelty
and redundancy detection in adaptive filtering. In Pro-
ceedings of the 25th Annual International ACM SIGIR
Conference.
Yi Zhang. 2004. Using Bayesian priors to combine clas-
sifiers for adaptive filtering. In Proceedings of the 27th
Annual International ACM SIGIR Conference.
595
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 223?231,
Beijing, August 2010
Constraining robust constructions for broad-coverage parsing with
precision grammars
Bart Cramer? and Yi Zhang??
Department of Computational Linguistics & Phonetics, Saarland University?
LT-Lab, German Research Center for Artificial Intelligence (DFKI)?
{bcramer,yzhang}@coli.uni-saarland.de
Abstract
This paper addresses two problems that
commonly arise in parsing with precision-
oriented, rule-based models of grammar:
lack of speed and lack of robustness. First,
we show how we can reduce parsing times
by restricting the number of tasks the
parser will carry out, based on a gener-
ative model of rule applications. Sec-
ond, we show that a combination of search
space restriction and radically overgen-
erating robustness rules lead to a more
robust parser, with only a small penalty
in precision. Applying both the robust-
ness rules and a fragment fallback strat-
egy showed better recall than just giving
fragment analyses, with equal precision.
Results are reported on a medium-sized
HPSG grammar for German. 1
1 Introduction
In the field of natural language processing, it
is common wisdom that handwritten, rule-based
models generally perform poorly on complex
problems, mainly due to the knowledge acquisi-
tion bottleneck: it is hard for the human modeller
to conceive of all possible scenarios the model
has to cope with. In parsing, many approaches
have relied on hand-written grammars, and their
fragility is one of their largest weaknesses. Such
models can fail due to insufficiency of lexical en-
tries or grammatical constructions, but also due
1The research reported on in this paper has been carried
out with financial support from the Deutsche Forschungs-
gemeinschaft and the German Excellence Cluster of Multi-
modal Computing & Interaction.
to creative or ungrammatical input. In any case,
the parser should always return a reasonable out-
put. A very simple technique is partial or fragment
parsing (Kiefer et al, 1999; Riezler et al, 2001;
Zhang et al, 2007a): if there is no item in the chart
that both spans the complete sentence and fulfills
the root condition, several chunks that do conform
to a root condition are combined by minimising a
certain cost function (for instance to favour larger
chunks, or more probable chunks).
A second problem with deep parsers is their rel-
atively low efficiency. For online applications, it is
impermissible to wait for longer than a minute be-
fore the system responds. Apart from studies that
were aimed at increasing the efficiency of deep
parsers by using smarter algorithms (e.g. using
left-corner relations (Van Noord, 1997)), several
studies in recent years have suggested that search
space restriction can offer a beneficial balance be-
tween speed and accuracy as well. Techniques
that have been proposed are, among others, su-
pertagging (Clark and Curran, 2007), CFG filter-
ing (Matsuzaki et al, 2007) and beam threshold-
ing (Ninomiya et al, 2005).
A potential disadvantage of the latter technique
is that the unifications have taken place by the
time the value of the resulting chart item is in-
vestigated. One strategy that tries to prevent ex-
ecution of unlikely tasks altogether is presented
by van Noord (2009). In this method, the parser
learns from an unannotated corpus which parse
steps contributed to the solution as preferred by
the disambiguation model (as opposed to a cer-
tain gold standard). Hence, this approach is self-
learning.
Another study that is close to our approach
223
to search space restriction is c-structure pruning
(Cahill et al, 2008). The authors show that a
large, hand-written, unification-based parser (the
XLE LFG parser for English) can perform reason-
ably faster (18%) without losing accuracy, by not
allowing the parser to unify if the resulting item
will have a span that does not conform to a CFG
tree that was generated from the sentence before-
hand by a PCFG parser. Much better results (67%
speed-up) are obtained by pruning chart items lo-
cally, based on their relative probabilities (Cahill
et al, 2008). This is the approach that is closest to
the one we present in this paper.
In this paper, we introduce a method that ad-
dresses robustness and efficiency concurrently.
The search space is restricted by setting a maxi-
mum on the number of tasks per chart cell. Be-
cause tasks are carried out according to a prior-
ity model based on the generative probabilities of
the rule applications, it is unlikely that good read-
ings are dropped. More robustness is achieved by
adding radically overgenerating rules to the gram-
mar, which could cover all sentences, given an dis-
proportionate amount of time and memory. By
strongly restricting the search space, however, the
computation requirements remains within bounds.
Because the robustness rules are strongly dispre-
ferred by both the priority model and the dis-
ambiguation model, all sentences that would be
covered by the ?restricted? grammar remain high-
precision, but sentences that are not covered will
get an additional push from the robustness rules.
1.1 An HPSG grammar for German
The grammar we use (Cramer and Zhang, 2009)
is the combination of a hand-written, constraint-
based grammar in the framework of HPSG and an
open word class lexicon extracted from the Tiger
treebank (Brants et al, 2002) in a deep lexical ac-
quisition step. One of the aims of this grammar
is to be precision-oriented: it tries to give detailed
analyses of the German language, and reject un-
grammatical sentences as much as possible. How-
ever, this precision comes at the cost of lower cov-
erage, as we will see later in this paper.
Along with the grammar, a treebank has been
developed by re-parsing the Tiger treebank, and
including those sentences for which the grammar
was able to reproduce the original Tiger depen-
dencies. The treebank?s size is just over 25k sen-
tences (only selected from the first 45k sentences,
so they don?t overlap with either the development
or test set), and contains the correct HPSG deriva-
tion trees. These (projective) derivation trees will
function as the training set for the statistical mod-
els we develop in this study.
2 Restriction of the search space
2.1 The PET parser
The parser we employ, the PET parser (Callmeier,
2000), is an agenda-driven, bottom-up,
unification-based parser. In order to reduce com-
putational demands, state-of-the-art techniques
such as subsumption-based packing (Oepen
and Carroll, 2000) and the quasi-destructive
unification operator (Tomabechi, 1991) have been
implemented.
A central component in the parser is the agenda,
implemented as a priority queue of parsing tasks
(unifications). Tasks are popped from the agenda,
until no task is left, after which all passive items
spanning the complete sentence are compared
with the root conditions as specified by the gram-
mar writer. The best parse is extracted from the
parse forest by a Maximum Entropy parse disam-
biguation model (Toutanova et al, 2002), using
selective unpacking (Zhang et al, 2007b).
Two different types of items are identified: pas-
sive items and active items. Passive items are
?normal? chart items, in the sense that they can
freely combine with other items. Active items
still need to combine with a passive item to be
complete. Hence, the parser knows two types of
tasks as well (see figure 1): rule+passive and ac-
tive+passive.
Each time a task succeeds, the following hap-
pens:
? For each inserted passive item, add
(rule+passive) tasks that combine the
passive item with each of the rules, and add
(active+passive) tasks that combine with
each of the neighbouring active items.
? For each inserted active item, add (ac-
tive+passive) tasks that combine the remain-
224
unary binary
rule+passive
binary
active+passive
R
+ P ?
R
P
R
+ P ?
R
P
R
P
1
+ P
2
?
R
P
1
P
2
Figure 1: Depicted are the different types of tasks in the PET parser. Not shown are the features
structures imposed by the rules and the chart items.
ing gaps in the active item with existing
neighbouring passive items in the chart.
2.2 Defining priorities
The priorities of the parsing tasks are calculated
based on a generative PCFG model extracted from
the treebank by maximum likelihood estimation,
smoothed by Lidstone smoothing. Each passive
chart item receives a score based on its generative
probability, calculated as the product of all applied
rule probabilities. For active parsing items, we set
the score to be the upper bound of this generative
probability, if the item succeeds later in combin-
ing with other passive edge(s) to build a complete
subtree. This is done by simply assuming the un-
determined subtree in the active item receiving a
generative score of 1.
The priorities that are assigned to both types of
tasks are not yet conditioned on the probability
of the topmost rule application. Hence, they are
computed using the following simple formula:
Pr = p(R) ? p(P )
where Pr is the task?s priority, p(R) the prior
probability of the rule category R; and p(P ) is
the highest possible generative probability of the
resulting passive item P .
2.3 Restriction strategies
It is a natural thought to allocate more computa-
tional resources to longer sentences, and this is
exactly what happens in the restriction strategies
we develop in this study. We define a cap on
the number of tasks for a certain cell/span (i, j),
which means that the number of cells is quadrati-
cally related to the number of words in a sentence:
ncells = n(n + 1)/2.
We define three task restriction strategies: all,
success, and passive. In all, the cap is defined
for all tasks, whether the unification is success-
ful or not. Success only counts tasks that are suc-
cessful (i.e. lead to either an active or a passive
item), and passive only counts tasks that lead to a
passive item. In all strategies, morphological and
lexical tasks are not counted, and hence not re-
stricted. Unary phrasal rules (such as empty-det)
are counted, though.
The implementation uses only one priority
queue. Each time a task is popped from the
agenda, it is checked whether the limit for this
span has been reached or not. If so, the task is
discarded; otherwise, it is executed.
2.4 Methodology
All our experiments are based on the Tiger tree-
bank (Brants et al, 2002). The grammar?s lex-
icon is based on the first 45k sentences in the
treebank, and so are the MaxEnt disambiguation
model (Toutanova et al, 2002) and the genera-
tive model we developed for this study. The de-
velopment set (s45001-s47500) was used to fine-
tune the methods, but all final results presented in
this paper are with respect to the test set (s47501-
s50000). The maximum time for building up the
packed parse forest is 60 seconds, after which un-
packing is started. Unpacking the first reading
usually has negligible computation costs, and is
not reported on. Along with the best reading?s
derivation, the dependencies are output, and com-
225
Strategy exhaustive all success passive
Cap size 3000 200 100
Time (s) 7.20 1.04 0.92 1.06
Coverage 59.4% 60.5% 60.0% 59.0%
Exact 17.6% 17.6% 17.4% 17.4%
Recall 37.6% 39.5% 38.9% 38.0%
Precision 80.7% 80.3% 80.1% 80.4%
F-score 51.3% 52.9% 52.4% 51.6%
Table 1: A more detailed look into some data points from figure 2. ?Coverage? and ?Exact? are sentential
percentages, showing how many sentences receive at least one or the exactly correct reading. Recall,
precision and f-score are on a per-dependency basis.
l
0 2 4 6 8
46
48
50
52
54
Time (s)
F?
sc
or
e
l exhaustive
all
success
passive
Figure 2: This figure shows the tradeoff between
speed and f-score for the standard grammar, using
the restriction strategies with different cap sizes.
pared to the gold standard dependencies from the
Tiger treebank.
2.5 Results
The results of the experiments, with different cap
sizes, are summarized in table 1 and figure 2.
As expected, for all strategies it holds that longer
computation times lead to higher coverage num-
bers. The interesting thing is that the restriction of
the search space doesn?t affect the parses? preci-
sion, indicating that the priorities work well: the
tasks leading to good solutions are indeed given
high priority scores.
A striking observation is that the coverage num-
bers go up by about 1%, with reductions in parse
times of more than 80%. This is due to the use of
the timeout, and the generic tendency of our defi-
nition of the priorities: because less rule applica-
tions lead to higher log probabilities, the agenda
will favour tasks with smaller span size. If the
agenda doesn?t apply too strong a restriction on
those tasks, the parser might not create any items
spanning the whole sentence after the full 60 sec-
onds, and hence produce no parse. This is miti-
gated by stronger restriction, leading to a quicker
path upwards in the chart.
No large differences of success are found be-
tween the different strategies. The intuition be-
hind the success and passive strategies was that
only more effort should be invested into a par-
ticular span if not enough chart items for that
span have been created. However, the time/quality
trade-offs are very similar for all strategies, as
shown in figure 22.
The strategies we have reported on have one
thing in common: their counters are with respect
to one particular span, and therefore, they have
a very local scope. We have tried other strate-
gies that would give the algorithm more flexibil-
ity by defining the caps on more global scale, for
instance per span length or for the entire chart.
However, this degraded the performance severely,
because the parser was not able to divide its atten-
tion properly.
2One might be tempted to consider the all strategy as
the best one. However, the time/f-score tradeoff curves look
slightly different on the development set.
226
3 Increasing robustness
For hand-written deep parsers, efficiency and cov-
erage are often competing factors: allowing more
items to be created might be beneficial for recall,
but the parser will also be too slow. However, be-
cause the search space can be restricted so rigidly,
we can make the grammar more permissive to ac-
cept more sentences, hopefully without a heavy
efficiency penalty. One way to do this is to re-
move constraints from the grammar rules. How-
ever, that would infringe on the precision-oriented
nature of the grammar. Instead, we will keep the
normal grammar rules as they are, and create a
small number of additional, super-accepting ro-
bustness rules. The intuition is that when the re-
stricted part of the grammar can find a solution,
that solution will indeed be found, and preferred
by the statistical models. On the other hand, when
the sentence is extragrammatical, the robustness
rules may be able to overcome the barriers.
Let?s consider the following example, assuming
that the grammar only lists ?to run? as an intransi-
tive verb:
?John ran the marathon yesterday?
A fragment approach would come up with the
following solution:
John ran the marathon yesterday
subj-h
?John? will correctly be identified as the subject
of ?ran?, but that is all. No dependencies are estab-
lished between ?the marathon? and ?ran?, or ?yes-
terday? and ?ran?. The former is hard to establish,
because of the missing lexical item. However, the
latter should be doable: the lexicon knows that
?yesterday? is an adverb that modifies verbs. If
we could create a robustness rule that would ab-
sorb the object (?the marathon?) without assigning
a dependency, it would at least be able to identify
the modifier dependency between ?ran? and ?yes-
terday?.
John
ran the marathon
yesterdaym-robust
h-adjunct
subj-h
In other words, a fragment analysis solely com-
bines items at the top level, whereas a robust
parser would ideally be able to overcome barri-
ers in both the lower and the higher regions of the
chart, meaning that the damage can be localised
and thus minimised. The robustness rules we pro-
pose are intended to achieve that.
How does this idea interact with the restriction
mechanism explained in the previous section? Ro-
bustness rules get an inhibitively large, constant
penalty in both the priority model and the dis-
ambiguation model. That means that at first the
parser will try to build the parse forest with the re-
stricted set of rules, because tasks involving sub-
trees with only rules from the standard grammar
will always have a higher priority than tasks us-
ing an item with a robustness rule application in
its subtree. When this is finished, the robustness
rules try to fill the gaps. Especially in the suc-
cess and passive strategies, tasks with robustness
rules are discarded if already enough chart items
are found for a particular span, meaning that the
parser automatically focusses on those parts of the
chart that haven?t been filled before.
3.1 Defining robustness rules
Defining robustness rules is a sort of grammar
engineering, and it took a bit of experimentation
to find rules that worked well. One of the fac-
tors was the interaction between the subsumption-
based packing and the robustness rules. When the
chart is built up, items that are subsumed by an ex-
isting item are marked as ?frozen?, and the latter
(more general) item functions as the representa-
tive node in the remainder of the parsing process.
When unpacking the best solution, the best deriva-
tion tree is extracted from the packed forest, which
227
might include a frozen node. Because this frozen
node has more constraints than its representative,
this derivation tree is not guaranteed to be free of
unification failures, and hence, before outputting,
this is checked by replaying all the unifications in
the derivation tree. This procedure is repeated un-
til a sound derivation has been found.
So what happens when the representative nodes
are very general? Many nodes will be packed,
and hence the chart will remain compact. How-
ever, the unpacking process will become prob-
lematic, because many of the proposed derivation
trees during unpacking will be incorrect, leading
to excessive computation times (in the order of
minutes).
Therefore, we chose to define robustness rules
such, that the resulting chart items will be equally
constrained as their daughters. They are all bi-
nary, and have one common ancestor in the type
hierarchy:
?
?????????????
structure-robust
SYNSEM 1
ROBUST +
MN-DTR
?
?
sign
SYNSEM 1
[
LOCAL.CAT.HEAD verb
]
ROBUST -
?
?
RB-DTR
?
?
sign
SYNSEM
[
NONLOCAL no-nonlocal
]
ROBUST -
?
?
?
?????????????
All rules have a main daughter and a robust
daughter. The co-indexation of the SYNSEM of
the main daughter and the SYNSEM of the rule
itself has the effect that the resulting chart item
will have the exact same syntactic properties as its
main daughter, whereas the robust daughter does
not contribute to the syntactic properties of the
mother node. The ROBUST feature is used to
prevent the application of two robust rules con-
secutively. Additional constraints (not shown)
make sure that morphological processing is fin-
ished, and that both parts are not involved in a
coordination. Robustness rules do not yield a de-
pendency triple (although they mght be guessed
accurately by a few heuristics).
We define two pairs of robustness rules, each
pair consisting of a rule with MN-DTR first and
RB-DTR second, and one rule in the other order:
+V The robust daughter is a verb, which is still
allowed to have valence, but cannot have any
features in NONLOCAL.
+NV The robust daughter is anything but a verb,
cannot have any non-empty valence list, and
cannot have any features in NONLOCAL.
3.2 Fragment parsing
As a baseline for comparison, we investigate the
existing partial parsing algorithms that pick frag-
mented analyses from the parse forest as a fall-
back strategy when there is no full parse available.
Kiefer et al (1999) took a shortest-path approach
to find a sequence of fragment analysis that min-
imizes a heuristics-based cost function. Another
variation of the algorithm (Riezler et al, 2001)
is to pick fewest chunks that connect the entire
sentence. While these early approaches are based
on simple heuristics, more sophisticated parse se-
lection methods also use the statistical models to
rank the partial analyses. For example, Zhang et
al. (2007a) proposed several ways of integrating
discriminative parse ranking scores with the par-
tial parse selection algorithm.
In this experiment, we first use the shortest
path algorithm to find candidate chunks of par-
tial analysis. All phrasal constituents were given
equal weights, and preferred over input and lex-
ical edges. For each chunk (edges spanning the
same sub-string of the input sentence), the edge
with the highest generative probability is picked.
Consequently, the best partial reading (covering
that edge) is decoded by the selective unpacking
algorithm using the MaxEnt parse ranking model.
With each fragment, the partial semantic represen-
tations were extracted. Similar to the robustness
rules, no cross-fragment dependencies are recov-
ered in this approach. Due to the limited number
of chart items and the use of selective unpacking,
the computation times for the shortest-path algo-
rithm are marginal.
3.3 Results
The results of this experiment are listed in ta-
ble 2. For the robust versions of the grammar,
no exhaustive parsing results are reported, be-
cause they take too long to compute, as can be
expected. Coverage number are on a per-sentence
228
standard +V +NV +V+NV
exhaustive restricted restricted
time (s) 7.20 0.92 4.10 1.42 4.09
no fragment coverage 59.3% 60.0% 72.6% 69.9% 78.6%
recall 37.6% 38.9% 48.4% 47.0% 53.8%
precision 80.7% 80.1% 78.6% 78.2% 77.7%
f-score 51.3% 52.4% 59.9% 58.7% 63.6%
fragment coverage 94.3% 98.3% 98.5% 98.7% 98.5%
recall 50.4% 53.6% 59.5% 56.9% 61.3%
precision 75.4% 75.0% 75.0% 74.5% 74.7%
f-score 60.4% 62.5% 66.3% 64.5% 67.3%
Table 2: Results for experiments with different robustness rules, and with or without fragment fallback
strategy.
basis, whereas the other percentages are on a per-
dependency basis. Time denotes the average num-
ber of seconds it takes to build the parse forest. All
results under ?restricted? are carried out with the
success strategy, with a cap of 200 tasks (success-
200). ?(No) fragment? indicates whether a frag-
ment parse is returned when no results are ob-
tained after selective unpacking.
The robustness rules significantly increase the
sentential coverage, in the case of +V+NV almost
20 percent points. The gains of +V and +NV
are fairly additive: they seem to cover different
sets of extragrammatical sentences. In the most
permissive setting (+V+NV), dependency recall
goes up by 16 percent point, with only a 3 per-
cent point decrease of precision, showing that the
newly-covered sentences still receive fairly accu-
rate parses. Also, it can be seen that the +V pair of
rules is more effective than +NV to increase cov-
erage. The robust grammars are certainly slower
than the standard grammar, but still twice as fast
as the standard grammar in an exhaustive setting.
Coverage numbers are approximating 100%
when the fragment parsing fallback strategy is ap-
plied, in all settings. However, it is interesting
to see that the recall numbers are higher when
the robustness rules are more permissive, but that
no significant effect on the precision is observed.
This suggests that the lumps that are connected by
the fragment parsing mechanism are larger, due
to previous applications of the robustness rules.
From this, we conclude that the connections made
by the robustness rules are of relatively high qual-
ity.
We have also tried the all-3000 and passive-
100 settings (the same as listed in table 1). That
yielded very similar results, except on the gram-
mar with both +V and +NV enabled. With pas-
sive-100, there was a small decrease in cover-
age (76.0%), but this drop was much more pro-
nounced for all-3000: 72.0%. This suggests that,
if the pressure on the generative model is larger
due to heavier overgeneration, counting success-
ful tasks or passive items performs better than just
counting the number of executed tasks.
After manual inspection, we found out that the
kind of constructions the robustness rules created
were very diverse. Most of the rule applications
were not in the top of the tree, as was intended.
There also seemed to be a correlation between the
length of the robust daughter and the quality of the
parse. When the robust daughter of the rule was
large, the application of the robustness rule looked
like an emergency strategy, with a corresponding
quality of the parse. However, when the robust-
ness rule connects a verb to a relatively small con-
stituent (a particle or an NP, for example), the re-
sulting derivation tree was of reasonable quality,
keeping most of the other dependencies intact.
4 Discussion
Achieving broad coverage in deep parsing while
maintaining high precision is difficult. Until now,
most existing hand-written grammar-based pars-
ing systems rely on fragment analyses (or various
ways of putting fragments together to compose
229
partial readings), but we argued (with the exam-
ple in section 3) that such an approach delivers in-
ferior results when the tree falls apart at the very
bottom. The use of robust constructions offers a
way to keep the damage local, but can create an
intractable search space. The proposed pruning
strategies carefully control the bound of overgen-
eration, resulting in improvements on both pars-
ing efficiency and coverage, with a significantly
smaller degradation in f-score than a pure frag-
ment approach. The combination of grammar en-
gineering, statistical modelling and algorithmic
design in the parser brings the parser performance
to a new level.
Although the experiments were carried out on
a specific grammar framework, we consider the
techniques put forward in this paper to be applica-
ble to other linguistic frameworks. The robustness
rules are easy to construct (with the precautions
from section 3.1 in mind), and all modern deep
parsers have a treebank to their disposal, from
which the generative model can be learned.
There are still points that can be improved on.
Currently, there is no way to determine which of
the robust rule applications are more promising
than others, and the decision to try one before the
other is solely based on the the probabilities of the
passive items, and not on the generative model.
This can be inefficient: for instance, all robustness
rules presented in this paper (both +V and +NV)
requires the main daughter to be a verb. It would
be straightforward to learn from a small treebank
that trying to unify the main daughter of a robust-
ness rules (which should have a verbal head) with
a specifier-head rule application does not have a
high chance on succeeding.
Another possible improvement is to differenti-
ate between different robustness rules. We pre-
sented a two-tier system here, but the framework
lends itself naturally to more layers with differing
degrees of specificity, creating a smoother scale
from specific/prioritised to robust/non-prioritised.
References
Brants, S., S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Pro-
ceedings of the Workshop on Treebanks and Lin-
guistic Theories, pages 24?41.
Cahill, A., J.T. Maxwell III, P. Meurer, C. Rohrer, and
V. Rose?n. 2008. Speeding up LFG parsing using
c-structure pruning. In Proceedings of the Work-
shop on Grammar Engineering Across Frameworks,
pages 33?40. Association for Computational Lin-
guistics.
Callmeier, U. 2000. PET?a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99?107.
Clark, S. and J.R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?
552.
Cramer, B. and Y. Zhang. 2009. Construction of
a German HPSG grammar from a detailed tree-
bank. In Proceedings of the GEAF workshop ACL-
IJCNLP 2009, pages 37?45.
Kiefer, B., H.U. Krieger, J. Carroll, and R. Malouf.
1999. A bag of useful techniques for efficient and
robust parsing. In Proceedings of the 37th annual
meeting of the Association for Computational Lin-
guistics on Computational Linguistics, pages 473?
480. Association for Computational Linguistics.
Matsuzaki, T., Y. Miyao, and J. Tsujii. 2007. Ef-
ficient HPSG parsing with supertagging and CFG-
filtering. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI
2007), pages 1671?1676, Hyderabad, India.
Ninomiya, T., Y. Tsuruoka, Y. Miyao, and J. Tsujii.
2005. Efficacy of beam thresholding, unification
filtering and hybrid parsing in probabilistic HPSG
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 103?114.
Association for Computational Linguistics.
Oepen, S. and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing: practical results. In Pro-
ceedings of the first conference on North American
chapter of the Association for Computational Lin-
guistics, pages 162?169. Morgan Kaufmann Pub-
lishers Inc. San Francisco, CA, USA.
Riezler, S., T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell III, and M. Johnson. 2001. Parsing
the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 271?
278.
Tomabechi, H. 1991. Quasi-destructive graph unifi-
cation. In Proceedings of the 29th annual meet-
ing on Association for Computational Linguistics,
pages 315?322. Association for Computational Lin-
guistics.
230
Toutanova, K., C.D. Manning, S. Shieber,
D. Flickinger, and S. Oepen. 2002. Parse
disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253?263.
Van Noord, G. 1997. An efficient implementation of
the head-corner parser. Computational Linguistics,
23(3):425?456.
van Noord, G. 2009. Learning efficient parsing. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 817?825,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Zhang, Y., V. Kordoni, and E. Fitzgerald. 2007a. Par-
tial parse selection for robust deep processing. In
Proceedings of ACL 2007 Workshop on Deep Lin-
guistic Processing, pages 128?135, Prague, Czech.
Zhang, Y., S. Oepen, and J. Carroll. 2007b. Effi-
ciency in Unification-Based N-Best Parsing. In Pro-
ceedings of the Tenth International Conference on
Parsing Technologies, pages 48?59. Association for
Computational Linguistics.
231
Coling 2010: Poster Volume, pages 692?700,
Beijing, August 2010
Contextual Recommendation based on Text Mining
Yize Li, Jiazhong Nie, Yi Zhang
School of Engineering
University of California Santa Cruz
{yize,niejiazhong,yiz}@soe.ucsc.edu
Bingqing Wang
School of Computer Science Technology
Fudan University
wbq@fudan.edu.cn
Baoshi Yan, Fuliang Weng
Research and Technology Center
Robert Bosch LLC
Baoshi.Yan@us.bosch.com
Fuliang.Weng@us.bosch.com
Abstract
The potential benefit of integrating con-
textual information for recommendation
has received much research attention re-
cently, especially with the ever-increasing
interest in mobile-based recommendation
services. However, context based recom-
mendation research is limited due to the
lack of standard evaluation data with con-
textual information and reliable technol-
ogy for extracting such information. As
a result, there are no widely accepted con-
clusions on how, when and whether con-
text helps. Additionally, a system of-
ten suffers from the so called cold start
problem due to the lack of data for train-
ing the initial context based recommenda-
tion model. This paper proposes a novel
solution to address these problems with
automated information extraction tech-
niques. We also compare several ap-
proaches for utilizing context based on
a new data set collected using the pro-
posed solution. The experimental results
demonstrate that 1) IE-based techniques
can help create a large scale context data
with decent quality from online reviews,
at least for restaurant recommendations;
2) context helps recommender systems
rank items, however, does not help pre-
dict user ratings; 3) simply using context
to filter items hurts recommendation per-
formance, while a new probabilistic latent
relational model we proposed helps.
1 Introduction
In the information retrieval community, one ma-
jor research focus is developing proactive re-
trieval agent that acts in anticipation of informa-
tion needs of a user and recommends information
to the user without requiring him/her to issue an
explicit query. The most popular examples of such
kind of proactive retrieval agent are recommender
systems. Over the last several years, research
in standard recommender systems has been im-
proved significantly, largely due to the availability
of large scale evaluation data sets such as Netflix.
The current research focus goes beyond the stan-
dard user-item rating matrix. As researchers start
to realize that the quality of recommendations de-
pends on time, place and a range of other rele-
vant users? context, how to integrate contextual
information for recommendation is becoming an
ever increasingly important topic in the research
agenda (Adomavicius and Ricci, 2009).
One major challenge in context-aware recom-
mendation research is the lack of large scale an-
notated data set. Ideally, a good research data
set should contain contextual information besides
users? explicit ratings on items. However, such
kinds of data sets are not readily available for
researchers. Previous research work in context
based recommendation usually experiments on a
small data set collected through user studies. Al-
though undoubtedly useful, this approach is lim-
ited because 1) user studies are usually very ex-
pensive and their scales are small; 2) it is very hard
for the research community to repeat such study;
and 3) a personalized contextual system may not
692
1 I was very excited to try this place and my wife took me here on my birthday. . .
We ordered a side of the brussell sprouts and they were the highlight of the night.
2 A friend of mine suggested we meet up here for a night of drinks. . . This actually
a restaurant with a bar in it, but when we went it was 10pm and . . .
Table 1: Examples of the restaurant reviews
succeed until a user has interacted with it for a
long period of time to enable context based rec-
ommendation models well trained.
On the other hand, a large amount of re-
view documents from web sites such as tri-
padvisor.com, yelp.com, cnet.com, amazon.com,
are available with certain contextual information,
such as time and companion, implicitly in the re-
views (see Table 1 for examples). This offers us an
opportunity to apply information extraction tech-
niques for obtaining contextual information from
the review texts. Together with users? explicit rat-
ings on items, this might lead to a large research
data set for context based recommendation and
consequently address the cold start issue in the
recommender systems. This paper describes the
methods that extract the contextual information
from online reviews and their impact on the rec-
ommendation quality at different accuracy levels
of the extraction methods.
Another challenge is how to integrate contex-
tual information into existing recommendation al-
gorithms. Existing approaches can be classified
into three major categories: pre-filtering, post-
filtering and the modeling based approaches (Oku
et al, 2007; Adomavicius and Tuzhilin, 2008).
Pre-filtering approaches utilize contextual infor-
mation to select data for that context, and then pre-
dict ratings using a traditional recommendation
method on the selected data (Adomavicius et al,
2005). Post-filtering approaches first predict rat-
ings on the whole data using traditional methods,
then use the contextual information to adjust re-
sults. Both methods separate contextual informa-
tion from the rating estimation process and leads
to unsatisfying findings. For example, Adomavi-
cious et al (2005) found neither standard col-
laborative filtering nor contextual reduction-based
methods dominate each other in all the cases. In
the modeling based approaches, contextual infor-
mation is used directly in the rating prediction
process. For example, Oku et al (2007) propose
a context-aware SVM-based predictive model to
classify restaurants into ?positive? and ?negative?
classes, and contextual information is included as
additional input features for the SVM classifier.
However, treating recommendation as classifica-
tion is not a common approach, and does not take
advantage of the state of art collaborative filtering
techniques. In this paper, we propose a new prob-
abilistic model to integrate contextual information
into the state of art factorization based collabora-
tive filtering approach, and compare it with sev-
eral baselines.
2 Mining Contextual Information from
Textual Opinions
The context includes any information that can be
used to characterize the situation of entities. Ex-
amples of context are: location, identity and state
of people, companions, time, activities of the cur-
rent user, the devices being used etc. (Lee et
al., 2005). Without loss of generality, we looked
into widely available restaurant review data. More
specifically, we investigated four types of contex-
tual information for a dining event, as they might
affect users? dining decisions, and they have not
been studied carefully before. The four types of
contextual information are: Companion (whether
a dining event involves multiple people), Occa-
sion (for what occasions the event is), Time (what
time during the day) and Location (in which city
the event happens).
2.1 Text Mining Approaches
We developed a set of algorithms along with exist-
ing NLP tools (GATE (Cunningham et al, 2002)
etc.) for this task. More detailed description of
these algorithms is given below.
Time: we classified the meal time into the
following types: ?breakfast?, ?lunch?, ?dinner?,
?brunch?, ?morning tea?, ?afternoon tea?. We
693
compiled a list of lexicons for these different types
of meal times, and used a string matching method
to find the explicit meal times from reviews. Here,
the meal time with an expression, such as ?6pm?,
was extracted using ANNIE?s time named entity
recognition module from the GATE toolkit. For
example, if a user says, ?When we went there, it
was 10pm?, we infer that it was for dinner.
Occasion: The ANNIE?s time named en-
tity recognition module recognizes certain special
days from text. We augmented ANNIE?s lookup
function with a list of holidays in the United States
from Wikipedia1 as well as some other occasions,
such as birthdays and anniversaries.
Location: Ideally, a location context would be
a user?s departure location to the selected restau-
rant. However, such information rarely exists in
the review texts. Therefore, we used the location
information from a user?s profile to approximate.
Companion: Extracting a companion?s infor-
mation accurately from review data is more diffi-
cult. We utilized two methods to address the chal-
lenge:
Companion-Baseline: This is a string match-
ing based approach. First, we automatically gen-
erated a lexicon of different kinds of compan-
ion words/phrases by using prepositional patterns,
such as ?with my (our) NN (NNS)?. We extracted
the noun or noun phrases from the prepositional
phrases as the companion terms, which were then
sorted by frequency of occurrence and manually
verified. This led to a lexicon of 167 entries.
Next, we grouped these entries into 6 main cate-
gories of companions: ?family?, ?friend?, ?cou-
ple?, ?colleague?, ?food-buddy? and ?pet?. Fi-
nally, the review is tagged as one or more of the
companion categories if it contains a correspond-
ing word/phrase in that lexicon.
Companion-Classifier: In order to achieve bet-
ter precision, we sampled and annotated 1000
sentences with companion terms from the corpus
and built three classifiers: 1) a MaxEnt classi-
fier with bag-of-words features, 2) a rule-based
classifier, 3) a hybrid classifier. For the rule-
based classifier, we looked into the structural as-
pects of the window where companion terms oc-
1http://en.wikipedia.org/wiki/List of holidays by
country#United States of America
curred, specifically, the adjacent verbs and prepo-
sitions associated with those terms. We collected
high frequency structures including verbs, verb-
proposition combinations, and verb-genitive com-
binations from the whole corpus, and then con-
structed a list of rules to decide whether a compan-
ion context exists based on these structures. For
the hybrid classifier, we used the patterns identi-
fied by the rule-based classifier as features for the
MaxEnt model (Ratnaparkhi, 1998). To train the
classifier, we also included features such as POS
tags of the verb and of the candidate companion
term, the occurrence of a meal term (e.g. ?lunch?,
?dinner?), the occurrence of pronouns (e.g. ?we?
or ?us?) and the genitive of the companion term.
Based on the evaluation results (using 5-fold cross
validation) shown in Table 2, the hybrid classifier
is the best performing classifier and it is used for
the subsequent experiments in the paper.
Words Rule Hybrid
Precision 0.7181 0.7238 0.7379
Recall 0.8962 0.8947 0.9143
F-Score 0.7973 0.8003 0.8167
Table 2: Evaluation results for the bag-of-words-
based classifier (Words), the rule-based classifier
(Rule) and the hybrid classifier (Hybrid)
3 Recommendation based on Contextual
Information
Next we consider how to integrate various con-
textual information into recommender systems.
Assume there are N items and M users. Each
user reviews a set of items in the system. The
data set can be represented as a set of quadruplet
D = (y, i, j, c), where i is the index of user, j is
the index of item, c is a vector describing the con-
text of this rating data, and y is the rating value.
Let c = (c1, ..., ck), where each component ck
represents a type of context, such as ?dinner time?
or ?location=San Jose?. The observed features
(meta data) of user i and item j are represented
as vectors fi and fj respectively, where each com-
ponent in the vector represents a type of feature,
such as ?gender of the user? or ?price range of
the restaurant?. In the rest of this paper, we in-
694
tegrate context c into the user?s observed features
fi. This makes fi a dynamic feature vector, which
will change with different context. The goal is
to predict ratings for candidate items given user i
and context c, and recommend the top items. We
present two recommendation models for integrat-
ing contextual information in this section.
3.1 Boolean Model
The Boolean Model filters out items that do not
match the context. The Boolean model itself re-
turns an item set instead of a ranked list. We fur-
ther rank the items by predicted rating values. We
score items by the Boolean model as follows:
s(j) =
{
sm(j) if item j matches the context
?? otherwise
(1)
where sm(j) is the predicted rating computed us-
ing a rating prediction method m, such as a Col-
laborative Filtering model without using context.
3.2 Probabilistic Latent Relational Model
We propose a novel Probabilistic Latent Rela-
tional Model (PLRM) for integrating contextual
information. In a context-aware recommender
system, a user?s interest for item is influenced by
two factors: (1) the user?s long-term preference,
which can be learned from users? rating history;
(2) the current context (how the item matches the
current context). To capture the two factors si-
multaneously, we introduce a new probabilistic
model by assuming the rating value yi,j,c follows
a Gaussian distribution with mean ui,j,c and vari-
ance 1/?(y):
yi,j,c ? N (ui,j,c, 1/?(y)) (2)
ui,j,c = uTi Avj + (Wufi)T (Wvfj) (3)
where ui and vj are the hidden representations of
user i and item j to be learned from rating data,
and Wu and Wv are feature transformation matri-
ces for users and items respectively. In Equation
(3), the first term uTi Avj is the estimation based
on user? long term preferences, where A = {a} is
a matrix modeling the interaction between ui and
vj .2 The second term (Wufi)T (Wvfj) is the esti-
2We introduce A matrix so that the model can also
be used to model multiple different types of relation-
mation based on current context and the observed
features of users and items, since the context c is
integrated into user?s observed features fi.
{U, V,A,W} are the parameters of the model
to be estimated from the training data set D,
where W = {Wu,Wv} = {w} , U =
{u1,u2, ...uN} and V = {v1,v2, ...vM}. We as-
sume the prior distribution of the parameters fol-
low the Gaussian distributions centered on 0. We
use 1/?(u),1/?(v), 1/?(w) and 1/?(a) to represent
the variance of the corresponding Gaussian distri-
butions. The effect of the prior distribution is sim-
ilar to the ridge regression (norm-2 regularizer)
commonly used in machine learning algorithms to
control model complexity and avoid overfitting.
The proposed model is motivated by well per-
forming recommendation models in the literature.
It generalizes several existing models. If we set A
to the identity matrix and Wu,Wv to zero matri-
ces, the model presented in Equation (3) is equiv-
alent to the well known norm-2 regularized singu-
lar value decomposition, which performs well on
the Netflix competition(Salakhutdinov and Mnih,
2007). If we set A to zero matrix and Wu to iden-
tity matrix, the Model (3) becomes the bilinear
model that works well on Yahoo news recommen-
dation task (Chu and Park, 2009).
Based on the above model assumption, the joint
likelihood of all random variables (U , V , A, W
and D) in the system is:
P (U, V,A,W,D) =?
(i,j,c,y)?D
P (yi,j,c|ui,vj , fi, fj , A,Wu,Wv)
?
i
P (ui)
?
j
P (vj)P (A)P (Wu)P (Wv)(4)
3.3 Parameter Estimation
We use a modified EM algorithm for parame-
ter estimation to find the posterior distribution of
(U, V ) and max a posterior (MAP) of (A,W ).
The estimation can be used to make the final pre-
ships/interactions jointly, where each type of relationship
corresponds to a different A matrix. For the task in this pa-
per, A is not required and can be set to the identity matrix
for simplicity. However, we leave A as parameters to be es-
timated in the rest of this paper for generality.
695
dictions as follows:
y?i,j,c =
?
ui,vj
P (ui)P (vj)(uTi Avj
+(Wufi)TWvfj)duidvj
E Step: the Variational Bayesian approach is used
to estimate the posterior distributions of U and V .
Assuming (A,W ) are known, based on Equation
4, we have
P (U, V |A,W,D) ?
?
(y,i,j,c)?D
N (uTi Avj + (Wufi)TWvfj , 1/?(y))
?
M?
i=1
N (ui|0, 1/?(u)I)
N?
j=1
N (vj |0, 1/?(v)I)
Deriving the exact distribution and use it to predict
y will result in intractable integrals. Thus we ap-
proximate the posterior with a variational distribu-
tion Q(U, V ) = ?Mi=1Q(ui)
?N
j=1Q(vj). Q(ui)
and Q(vj) are restricted to Gaussian distributions
so that predicting y using Bayesian inference with
Q(U, V ) will be straightforward. Q(U, V ) can be
estimated by minimizing the KL-divergence be-
tween it and P (U, V |A,W,D). Since Q(U, V ) is
factorized into individual Q(ui) and Q(vj), we
can first focus on one Q(ui) (or Q(vj)) at a time
by fixing/ignoring other factors. For space consid-
erations, we omit the derivation in this paper. The
optimal Q(ui) is N (u?i,?i), where u?i = ?idi,
??1i =
?
(y,i,j,c)?D
?(y)A(v?jv?Tj + ?j)AT
+ ?(u)I
di =
?
(y,i,j,c)?D
?(y)y?Av?j
Similarly, the optimal Q(vj) isN (v?j,?j), where
v?j = ?jej ,
??1j =
?
(y,i,j,c)?D
?(y)AT (u?iu?Ti + ?i)A
+ ?(v)I
ej =
?
(y,i,j,c)?D
?(y)y?AT v?j
M Step: Based on the approximate pos-
terior estimation Q(U, V ) derived in the E
step, the maximum a posteriori estimation
of {A,W} can be found by maximizing
the expected posterior likelihood {A?, W?} =
argmaxA,W EQ(U,V )(logP (A,W,U, V |D)).
This can be done using the conjugate gradient
descent method, and the gradient of A,Wu,Wv
can be calculated as follows:
??
?A =
?
(y,i,j,c)?D
?(y)((y? ? y)u?iv?Tj
+ u?iu?Ti A?j + ?iAv?jv?Tj + ?iA?j)
+ ?(a)A
??
?Wu
=
?
(y,i,j,c)?D
?(y)(y? ? y)WvfjfTi
+ ?(w)Wu
??
?Wv
=
?
(y,i,j,c)?D
?(y)(y? ? y)WufifTj
+ ?(w)Wv
where ? = EQ(U,V )(logP (A,W,U, V |D)) and
y? = u?Ti Av?j + (Wufi)TWvfj .
4 Experimental Methodology
4.1 Data Collection
We collected an evaluation data set from a pop-
ular review web site where users review ser-
vices/products and provide integer ratings from 1
to 5. The user profile and the description of items,
such as user gender and the category of restau-
rants are also collected. The data set used in this
paper includes the restaurants in Silicon Valley
(Bay area) and the users who ever reviewed these
restaurants. We extract context from the review
texts. The four kinds of context considered in our
paper are described in Section 2.1. For each type
of context, we create a subset, in which all reviews
contain the corresponding contextual information.
Finally we construct four sub data sets and each
data set is described by the corresponding con-
text type: Time, Location, Occasion and Compan-
ion. We use ?All? to represent the whole data set.
Statistics about each data set are described in Ta-
ble 3.
696
(a) Time (b) Location (c) Occasion
(d) Companion (e) All
Figure 1: Performance on the top-K recommendation task. The plots focus on the top 20% ranking
region.
Dataset #Ratings #Users #Items
All 756,031 82,892 12,533
Location 583,051 56,026 12,155
Time 229,321 49,748 10,561
Occasion 22,732 12,689 4,135
Companion 196,000 47,545 10,246
Table 3: Statistics of data
4.2 Experimental Setup
We design the experiments to answer the follow-
ing questions: 1) Does including contextual in-
formation improve the recommendation perfor-
mance? 2) How does the probabilistic latent re-
lational modeling approach compare with pre-
filtering or post-filtering approaches? 3) How
does the extraction quality of the contextual infor-
mation affect the recommendation performance?
To answer the first question, we compare the
performance of the Probabilistic Latent Relational
Model on a standard collaborative filtering setting
where only rating information is considered, in-
dicated by Nocontext. We also evaluate the per-
formance of the Probabilistic Latent Relational
Model when integrating contextual information,
indicated by Context-X, where X represents the
type of contextual information considered. To
answer the second question, we compare the
performance of Context-X with the pre-filtering
Boolean Model, which first uses the context to se-
lect items and then ranks them using scores com-
puted by Nocontext. To answer the third question,
we compare the recommendation performance for
different extraction precision. The performance
on the following two recommendation tasks are
reported in this paper:
Top-K Recommendation: We rank the items
by the predicted rating values and retrieve the top
K items. This task simulates the scenario where
a real recommender system usually suggests a list
of ranked K items to a user. To simulate the sce-
nario that we only want to recommend the 5-star
items to users, we treat 5-star rating data in testing
data as relevant. Ideally, classic IR measures such
as Precision and Recall are used to evaluate the
recommendation algorithms. However, without
complete relevance judgements, standard IR eval-
uation is almost infeasible. Thus we use a varia-
tion of the evaluation method proposed by Koren
(Koren, 2008).
Rating Prediction: Given an active user i and a
target item j, the system predicts the rating of user
697
Training on Sub Data set Training on the Whole Data set
Testing Data ItemAvg Nocontext Context ItemAvg Nocontext Context
Time 1.1517 1.0067 1.0067 1.1052 0.9829 0.9822
Companion 1.2657 1.0891 1.0888 1.2012 1.0693 1.0695
Occasion 1.2803 1.1381 1.1355 1.2121 1.0586 1.0583
Location 1.1597 1.0209 1.0206 1.1597 1.0183 1.0183
All context - - - 1.1640 1.0222 1.0219
Table 4: RMSE on the rating prediction task
Time CompanionBaseline CompanionClassifier Occasion
#Reviews 300 300 300 200
#Contexts 115 148 114 207
Precision 84.4% 62.2% 77.1% -
Recall 80.2% 95.8% 91.7% -
F-Score 82.2% 75.4% 83.8% Accuracy 78.3%
Table 5: Performance of the context extraction module
i on item j. The prediction accuracy is measured
by Root Mean Square Error (RMSE), which is
commonly used in collaborative filtering research.
This task simulates the scenario that we need to
guess a user?s rating about an item, given that the
user has already purchased/selected the item.
For each data set (Time, Companion, Location,
Occasion and All), we randomly sample 10% for
testing, 80% for training and 10% for validation.
5 Experimental Results
5.1 Performance on Top-K Recommendation
Figure 1(a)-(e) shows the ranking performance on
each data set. The x-axis is the rank and the y-axis
is the portion of relevant products covered by this
level of rank. The results across all data sets are
consistent. With contextual information, PLRM
Context-X outperforms Nocontext, whereas using
context to pre-filter items (Boolean) does not help.
It means that contextual information can help if
used appropriately, however improperly utilizing
context, such as simply using it as a boolean filter,
may hurt the recommendation performance. Our
proposed PLRM is an effective way to integrate
contextual information.
5.2 Performance on Rating Prediction Task
Table 4 summaries the RMSE results of differ-
ent approaches on the rating prediction task. The
RMSE of simply using item?s average rating value
as the prediction is also reported as a reference
since it is a commonly used approach by non per-
sonalized recommender systems. For each con-
text, we can either train the model only on the sub-
set that consists of rating data with related context,
or train on a bigger data set by adding the rating
data without related context. The results on both
settings are reported here. Table 4 shows that uti-
lizing context does not affect the prediction accu-
racy. We may wonder why the effects of adding
context is so different on the rating task compared
with the ranking task. One possible explanation
is that the selection process of a user is influenced
by context, while how the user rates an item after
selecting it is less relevant to context. For exam-
ple, when a user wants to have a breakfast, he may
prefer a cafeteria rather than a formal restaurant.
However, how the user rates this cafeteria is more
based on user?s experiences in the cafeteria, such
as quality of services, food, price, environment,
etc.
5.3 How does Text Mining Accuracy Affect
Recommendation
To evaluate the extraction performance on ?Com-
panion?, ?Time? and ?Occasion?, we randomly
sample some reviews and evaluate the perfor-
698
mance on the samples3. The results are shown in
Table 5. Compared with other contexts, the ex-
traction of companion context is more challenging
and the string matching baseline algorithm pro-
duces significantly inferior results. However, by
using a MaxEnt classifier with features selection,
we can boost the precision of the companion con-
text extraction to a level comparable to other con-
texts.
To further investigate the relationship between
the quality of the extracted context and the perfor-
mance of the recommender system, we compare
the recommendation performance of Companion-
Baseline and Companion-Classifier in Figure
1(d). It shows that improving the quality of the
extraction task leads to a significant improvement
on the recommender systems? top-K ranking task.
6 Conclusions
Reviews widely available online contain a large
amount of contextual information. This paper
proposes to leverage information extraction tech-
niques to help recommender systems to train
better context-aware recommendation models by
mining reviews. We also introduce a probabilis-
tic latent relation model for integrating the cur-
rent context and the user?s long term preferences.
This model takes the advantages of traditional col-
laborative filtering approaches (CF). It also cap-
tures the interaction between contextual informa-
tion and item characteristics. The experimental
results demonstrate that context is an important
factor that affects user choices. If properly used,
contextual information helps ranking based rec-
ommendation systems, probably because context
influences users? purchasing decisions. Besides,
more accurate contextual information leads to bet-
ter recommendation models. However, contextual
information does not help the user rating predic-
tion task significantly, probably because context
doesn?t matter much given the user has already
chosen a restaurant.
As the first step towards using the information
3We sample 300 reviews for ?Time? and ?Companion?
evaluation. Due to the extremely low probability of occur-
rence of Occasion context, we futher sample 200 reviews
containing Occasion-related expressions and only evaluate
extraction accuracy on these samples
extraction techniques to help contextual recom-
mendation, the techniques used in this paper are
far from optimal. In the future, we will research
more effective text mining techniques for contex-
tual extraction(Mazur and Dale, 2008; McCallum
et al, 2000; Lafferty et al, 2001) at the same time
increasing the amount of annotated review data
for better classifier performance through actively
learning (Laws and Schu?tze, 2008). We also plan
to work towards a better understanding of con-
textual information in recommender systems, and
explore other types of contextual information in
different types of recommendation tasks besides
restaurant recommendations.
7 Acknowledgements
Part of this research is funded by National Sci-
ence Foundation IIS-0713111 and the Institute of
Education Science. Any opinions, findings, con-
clusions or recommendations expressed in this pa-
per are the authors?, and do not necessarily reflect
those of the sponsors. Bingqing Wang?s work is
done during his stay in the Research and Technol-
ogy Center, Robert Bosch LLC.
References
Adomavicius, Gediminas and Francesco Ricci. 2009.
Recsys?09 workshop 3: workshop on context-aware
recommender systems, cars-2009. In Proceedings
of the 3rd ACM Conference on Recommender Sys-
tems, RecSys 2009, pages 423?424.
Adomavicius, Gediminas and Alexander Tuzhilin.
2008. Context-aware recommender systems. In
Proceedings of the 2nd ACM Conference on Rec-
ommender Systems, RecSys 2008, pages 335?336.
Adomavicius, Gediminas, Ramesh Sankaranarayanan,
Shahana Sen, and Alexander Tuzhilin. 2005.
Incorporating contextual information in recom-
mender systems using a multidimensional approach.
ACM Transactions on Information Systems (TOIS),
23(1):103?145.
Chu, Wei and Seung-Taek Park. 2009. Personalized
recommendation on dynamic content using predic-
tive bilinear models. In Proceedings of the 18th In-
ternational Conference on World Wide Web, WWW
2009, pages 691?700.
Cunningham, Hamish, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. A frame-
work and graphical development environment for
699
robust nlp tools and applications. In Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics, ACL 2002, pages 168?
175.
Koren, Yehuda. 2008. Factorization meets the
neighborhood: a multifaceted collaborative filtering
model. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, SIGKDD 2008, pages 426?434.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML
2001, pages 282?289.
Laws, Florian and Hinrich Schu?tze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics, Coling 2008,
pages 465?472, August.
Lee, Hong Joo, Joon Yeon Choi, and Sung Joo Park.
2005. Context-aware recommendations on the mo-
bile web. In On the Move to Meaningful Internet
Systems 2005: OTM 2005 Workshops, pages 142?
151.
Mazur, Pawel and Robert Dale. 2008. What?s the
date? high accuracy interpretation of weekday
names. In Proceedings of the 22nd International
Conference on Computational Linguistics, Coling
2008, pages 553?560.
McCallum, Andrew, Dayne Freitag, and Fernando
C. N. Pereira. 2000. Maximum entropy markov
models for information extraction and segmenta-
tion. In Proceedings of the 17th International Con-
ference on Machine Learning, ICML 2000, pages
591?598.
Oku, Kenta, Shinsuke Nakajima, Jun Miyazaki, and
Shunsuke Uemura. 2007. Investigation for design-
ing of context-aware recommendation system using
svm. In Proceedings of the International MultiCon-
ference of Engineers and Computer Scientists 2007,
IMECS 2007, pages 970?975.
Ratnaparkhi, A. 1998. MAXIMUM ENTROPY MOD-
ELS FOR NATURAL LANGUAGE AMBIGUITY
RESOLUTION. Ph.D. thesis, University of Penn-
sylvania.
Salakhutdinov, Ruslan and Andriy Mnih. 2007. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems 20, Proceedings of
the 21st Annual Conference on Neural Information
Processing Systems, NIPS 2007, pages 1257?1264.
700
Coling 2010: Poster Volume, pages 1453?1461,
Beijing, August 2010
Discriminant Ranking for Efficient Treebanking
Yi Zhang Valia Kordoni
LT-Lab, German Research Center for Artificial Intelligence (DFKI GmbH)
Department of Computational Linguistics, Saarland University
{yzhang,kordoni}@coli.uni-sb.de
Abstract
Treebank annotation is a labor-intensive
and time-consuming task. In this paper,
we show that a simple statistical ranking
model can significantly improve treebank-
ing efficiency by prompting human an-
notators, well-trained in disambiguation
tasks for treebanking but not necessarily
grammar experts, to the most relevant lin-
guistic disambiguation decisions. Experi-
ments were carried out to evaluate the im-
pact of such techniques on annotation ef-
ficiency and quality. The detailed analysis
of outputs from the ranking model shows
strong correlation to the human annotator
behavior. When integrated into the tree-
banking environment, the model brings a
significant annotation speed-up with im-
proved inter-annotator agreement.?
1 Introduction
The development of a large-scale treebank (Mar-
cus et al, 1993; Hajic? et al, 2000; Brants et al,
2002) with rich syntactic annotations is a highly
rewarding task. But the huge amount of man-
ual labor required for the annotation task itself,
as well as the difficulties in standardizing linguis-
tic analyses, results in long development cycles
of such valuable language resources, which typ-
ically amounts to years or even decades. Despite
the profound scientific and practical value of de-
tailed syntactic treebanks, the requirement and ne-
cessity for long-term commitment raises the risk
?The first author thanks the German Excellence Cluster
of Multimodal Computing and Interaction for the support of
the work.
cost of such projects, a fact which often makes
them not feasible in many current economical en-
vironments.
In recent years, computational grammars have
been employed to assist the construction of such
language resources. A typical development model
involves a parser which generates candidate anal-
yses, and human annotators who manually iden-
tify the desired tree structure. This treebanking
method dramatically reduces the cost of train-
ing annotators, for they are not required to spon-
taneously produce linguistic solutions to vari-
ous phenomena. Instead, they are trained to
associate their language intuition with specific
linguistically-relevant decisions. How to select
and carefully present such decisions to the an-
notators is thus crucial for achieving high an-
notation speed and quality. On the other hand,
for large treebanking projects, parallel annota-
tion with multiple annotators is usually neces-
sary. Inter-annotator agreement is a crucial quality
measure in such cases. But improvements on an-
notation speed should not be achieved at expense
of the quality of the treebank.
With both speed and quality in mind, a good
treebank annotation method should acknowledge
the complexity of the decision-making process;
for instance, the same tree can be disambiguated
by different sets of individual decisions which
are mutually dependent. The annotation method
should also strive to create a distraction-free en-
vironment for annotators who can then focus on
making the judgments. To this effect, we present a
simple statistical model that learns from the anno-
tation history, and offers a ranking of disambigua-
tion decisions from the most to the least relevant
1453
ones, which enables well-trained human annota-
tors to speed-up treebanking without compromis-
ing on the quality of the linguistic decisions guid-
ing the annotation task.
The remaining of this paper is structured as fol-
lows: Section 2 gives an overview of the diffi-
culties in syntactic annotation, and the potential
ways of improving the annotation efficiency with-
out damaging the quality; Section 3 presents the
new annotation method which is based on a statis-
tical discriminant ranking model; Sections 4 and 5
describe the setup and results of a series of anno-
tation experiments; Section 6 concludes the paper
and proposes future research directions.
2 Background
Large-scale full syntactic annotation has for quite
some time been approached with mixed feelings
by researchers. On the one hand, detailed syn-
tactic annotation serves as a basis for corpus-
linguistic study and data-driven NLP methods.
Especially, when combined with popular super-
vised machine learning methods, richly annotated
language resources, like, for instance, treebanks,
play a key role in modern computational linguis-
tics. The public availability of large-scale tree-
banks in recent years has stimulated the blossom-
ing of data-driven approaches to syntactic and se-
mantic parsing.
On the other hand, though, the creation of de-
tailed syntactic structures turns out to be an ex-
tremely challenging task. From the choice of
the appropriate linguistic framework and the de-
sign of the annotation scheme to the choice of the
text source and the working protocols on the syn-
cronization of the parallel development, as well as
the quality assurance, none of these steps in the
entire annotation procedure is considered a solved
issue. Given the vast design choices, very few
of the treebanking projects have made it through
all these difficult annotation stages. Even the
most outstanding projects have not been com-
pleted without receiving criticisms.
Our treebanking project is no exception. The
aim of the project is to provide annotations
of the Wall Street Journal (henceforward WSJ)
sections of the Penn Treebank (henceforward
PTB (Marcus et al, 1993)) with the help of
the English Resource Grammar (henceforward
ERG; (Flickinger, 2002)), a hand-written large-
scale and wide-coverage grammar of English in
the framework of Head-Driven Phrase Structure
Grammar (HPSG; (Pollard and Sag, 1994)). Such
annotations are very rich linguistically, since apart
from syntax they also incorporate semantic infor-
mation. The annotation cycle is organized into
iterations of parsing, treebanking in the sense of
disambiguating syntactic and semantic analyses
of the various linguistic phenomena contained in
the corpus, error analysis and grammar/treebank
update cycles. That is, sentences from the WSJ
are first parsed with the PET parser (Callmeier,
2001), an efficient unification-based parser, using
the ERG. The parsing results are then manually
disambiguated by human annotators. However,
instead of considering individual trees, the annota-
tion process is mostly invested on binary decisions
which are made on either accepting or rejecting
constructions or lexical types. Each of such deci-
sions, called discriminants, as we will also see in
the following, reduces the number of the trees sat-
isfying the constraints. The process is presented
in the next section in more detail. What should,
though, be clear is that the aforementioned multi-
cycle annotation procedure is as time-consuming
and human-error prone as any other, despite the
fact that at the center of the entire annotation cy-
cle lies a valuable linguistic resource, which has
been developed with a lot of effort over many
years, namely the ERG. For the first period of this
project, we have established an average speed of
40 sentences per annotator hour, meaning a total
of ?1200 hours of annotation for the entire WSJ.
Including the long training period at the beginning
of the project, and periodical grammar and tree-
bank updates, the project period is roughly two
years with two part-time annotators employed.
3 Statistical Discriminant Ranking
3.1 Discriminants & Decisions
One common characteristic of modern treebank-
ing efforts ? especially, in so-called dynamic tree-
banking platforms (cf., for instance, (Oepen et al,
2002) and http://redwoods.stanford.
edu), like the one we are describing and referring
1454
to extensively in the following, is that the can-
didate trees are constructed automatically by the
grammar, and then manually disambiguated by
human annotators. In doing so, linguistically rich
annotation is built efficiently with minimum man-
ual labor. In order to further improve the manual
disambiguation efficiency, systems like [incr
tsdb()] (Oepen, 2001) compute the difference
between candidate analyses. Instead of looking at
the huge parse forest, the treebank annotators se-
lect or reject the features that distinguish between
different parses, until no ambiguity remains (ei-
ther one analysis is accepted from the parse forest,
or all of them are rejected). The number of deci-
sions for each sentence is normally around log2n
where n is the total number of candidate trees.
For a sentence with 5000 candidate readings, only
about 12 treebanking decisions are required for a
complete disambiguation. A similar method was
also proposed in (Carter, 1997).
Formally, an attribute that distinguishes be-
tween different parses is called a discriminant.
For Redwoods-style treebanks, this is extracted ei-
ther from the syntactic derivation trees or the se-
mantic representations (in the form of Minimal
Recursion Semantics (MRS; (Copestake et al,
2005))).
Figure 1 shows an example graphical annota-
tion interface. At the top of the window, a list
of action buttons shows the operations permitted
on the sentence level. Then the sentence in its
original PTB bracket format is shown. 15 : 0
indicates that at the current disambiguation state,
15 trees remain to be disambiguated while 0 has
been eliminated. On the left large panel, the can-
didate trees are shown in their simplified phrase-
structure representation. Note that the actual
HPSG analyses are not shown in the screenshot
and can be displayed on request. On the right large
panel, the list of effective discriminants (see Sec-
tion 3.2) up to this disambiguation state is shown.
The highlighted discriminant in Figure 1 suggests
a possibility of constructing the entire sentence by
choosing a subject-head rule (SUBJH), taking ?ms.
Haag? as the subject and ?plays Elianti.? as the
head daughter. When the discriminant is clicked,
the annotator can say yes or no to it, hence nar-
rowing the remaining trees to the In Parses or Out
Parses. The unknown button is used to mark the
uncertainties and is rarely used.
Note that in this interface, the discriminants
are sorted in descending order according to their
length, meaning that the discriminants related to
higher level constructions are shown before the
lexical type choices. When up to 500 parses
are stored in the forest, the average number of
discriminants per forest is about 100. Scan-
ning through the long list manually can be time-
consuming and distracting.
Kordoni and Zhang (2009) show that annota-
tors tend to start with the decisions with the most
certainty, and delay the ?hard? decisions as much
as possible. As the decision progresses, many of
the ?hard? discriminants will receive an inferred
value from the certain decisions. Our annotation
guideline only describes specific decisions. The
order in which discriminants are chosen is left un-
derspecified and very much depends on personal
styles. In practice, we see that our annotators
gradually developed complex strategies which in-
volve both top-down and bottom-up pruning.
One potential drawback of such a discriminant-
based treebanking method is that the process is
very sensitive to decision errors. One wrong judg-
ment can rule out the correct tree and ruin the
analysis of the sentence. In such a case, the an-
notators usually resort to backtracking to previous
decisions they had made. To compensate for this,
we ask our annotators to double-check the tree-
banked analysis before saving the disambiguation
result. And in case of doubt, they are instructed to
avoid ambivalent decisions as much as possible.
3.2 Maximum-Entropy-Based Discriminant
Ranking Model
Suppose for a sentence ?, a parse forest Y was
generated by the grammar. Note that for effi-
ciency reasons, the parse forest might have been
trimmed to only contain up to n top readings
ranked by the parse disambiguation model. For
convenience, we note the parse forest Y as a set
of parses {y1, y2, . . . , yn}. Each discriminant d
defines a binary valued function ? on the parse
forest (? : Y 7? {0, 1}) , which can be inter-
preted as whether a parse yi has attribute d or not.
By the nature of this definition, each discriminant
1455
Figure 1: Screenshot of the discriminant-based treebanking graphical annotator interface
function defines a bi-partition of the parse forest.
When both subsets of the partition are non-empty,
i.e., there exists at least one yp and yq such that
?(yp) = 0 and ?(yq) = 1, the discriminant is con-
sidered effective on the forest Y . In the following
discussion, we are only considering the set of ef-
fective discriminants D for parse forest Y .
Instead of directly predicting the outcome of
disambiguation decision on each discriminant
(i.e., whether the GOLD tree has discriminant
function value 0 or 1), our model tries to measure
the probability of a discriminant being chosen by
human annotators, regardless of the yes/no deci-
sion. For each discriminant d, and the parse forest
Y , a set of feature functions f1, f2, . . . , fk receive
real values, and contribute to the following log-
linear model:
P (d|Y,D) = exp(
?k
i=1 ?ifi(d, Y ))?
d??D exp(
?k
i=1 ?ifi(d?, Y ))(1)
where ?1, ?2, . . . , ?k are the parameters of the
model.
To estimate these model parameters, we gather
the annotation logs from our treebank annotators
on the completed datasets with detailed informa-
tion about each discriminant. Apart from the
necessary information to reconstruct the discrim-
inants from the forest, the log also contains the
status information of i) whether the discriminant
takes value 0 or 1 on the gold tree; ii) whether
the human annotator has said yes or no to the dis-
criminant. Note that the human annotator does not
need to manually decide on the value of each dis-
criminant. Whenever a new decision is made, the
forest will be pruned to the subset of trees compat-
ible with the decision. And all remaining discrim-
inants are checked for effectiveness on the pruned
forest. Discriminants which become ineffective
from previous decisions are said to have received
inferred values.
The parameters of the model are estimated by
the open-source maximum entropy parameter es-
1456
timation toolkit TADM1. For training, we use
all the manually disambiguated discriminants as
positive instances, and automatically inferred dis-
criminants as negative instances.
The discriminant ranking model is applied dur-
ing the manual annotation sessions. When a parse
forest is loaded and the discriminants are con-
structed, each discriminant is assigned an (un-
normalized) score ?ki=1 ?ifi(d, Y ), and the list
of discriminants is sorted by descending order
of the score accordingly. The scoring and sort-
ing adds negligible additional computation on the
treebanking software, and is not noticeable to the
human annotators. By putting those discriminants
that are potentially to be manually judged near the
top of the list, the model saves manual labor on
scanning through the lengthy list by filtering out
ambivalent discriminants.
Note that this discriminant ranking model pre-
dicts the possibility of a discriminant being man-
ually disambiguated. It is not modeling the spe-
cific decision that the human annotator makes on
the discriminant. Including the decision outcome
in the model can potentially damage the annota-
tion quality if annotators develop a habit of over-
trusting the model prediction, making the whole
manual annotation pointless. A discriminant rank-
ing model, however, only suggestively re-orders
the discriminants on the presentation level, which
are much safer when the annotation quality is con-
cerned.
3.3 Feature Model for Syntactic
Discriminants
In practice, there are different ways of finding dis-
criminants from the parse forest. For instance, the
[incr tsdb()] system supports both syntax-
based and semantics-based discriminants. The
syntax-based discriminants are extracted from the
derivation trees of the HPSG analyses. All HPSG
rule applications (unary or binary) and choices
of lexical entries are picked as candidate dis-
criminants and checked for effectiveness. The
semantics-based discriminants, on the other hand,
represent the differences on the semantic struc-
tures (MRS in the cases of DELPH-IN2 gram-
1http://tadm.sourceforge.net/
2http://www.delph-in.net/
mars). With a few exceptions, many DELPH-IN
HPSG treebanks choose to use the syntactic dis-
criminants which allow human annotators to pick
the low-level constructions. The above proposed
ranking model works for different types of dis-
criminants (and potentially a mixture of different
discriminant types). But for the evaluation of this
paper, we show the feature model designed for the
syntactic discriminants only.
The syntactic discriminants record the differ-
ences between derivation trees by memorizing di-
rect rule applications and lexical choices. Beside
the rule or lexical entry name, the discriminant
also records the information concerning the corre-
sponding constituent, e.g., the category and span-
ning of the constituent, the parent and daughters
of the constituent, etc. Furthermore, given the dis-
criminant d and the parse forest Y , we can calcu-
late the distribution of parses over the value of the
discriminant function ?, which can be character-
ized by ?y?Y ?(y)/|Y |. This numeric feature in-
dicates how many parses can be ruled out with the
given discriminant.
As example, for the highlighted discriminant in
Figure 1, the extracted features are listed in Ta-
ble 1.
4 Experiment Setup
To test the effectiveness of the discriminant rank-
ing models, we carried out a series of experi-
ments, investigating their effects on both annota-
tion speed and quality. The experiment was done
in the context of our ongoing annotation project
of the WSJ sections of the PTB described in Sec-
tion 2. Despite sharing the source of texts, the
new project aims to create an independently an-
notated corpus. Therefore, the trees from the PTB
were not used to guide the disambiguation pro-
cess. In this annotation project, two annotators
(both graduate students, referred to as A and B
below) are employed to manually disambiguate
the parsing outputs of the ERG. For quality con-
trol and adjudication in case of disagreement, a
third linguist/grammarian annotates parts of the
treebank in parallel.
With the help of our annotation log files, which
record in details the manual decision-making pro-
cess, we trained three discriminant ranking mod-
1457
Feature Possible Values Example
discriminant type RULE/LEX RULE
edge position FULL/FRONT/BACK FULL
edge span length(constituent)/length(sentence) 4/4
edge category rule or lexical type name SUBJH
level of discrimination ?y?Y ?(y)/|Y | 4/15
branch splitting length(left-dtr)/length(constituent) 2/4
Table 1: Features for syntactic discriminant ranking model and example values for the highlighted
discriminant in Figure 1
els with the datasets completed so far: MODEL-
A and MODEL-B trained with annotation logs
from two annotators separately, and MODEL-
BOTH trained jointly with data from both annota-
tors. For each annotator?s model (MODEL-A and
MODEL-B), we used about 6,000 disambiguated
parse forests for training. For each of these 6,000
forests, the log file contains about 600,000 effec-
tive discriminants, among which only ?6% re-
ceived a manual decision.
To evaluate the treebanking speed, we have
the annotators work under a distraction-free en-
vironment and record their annotation speed. The
speed is averaged over several 1-hour annotation
sessions. Different discriminant ranking models
were used without the annotators being informed
of the details of the setting.
As testing dataset, we use the texts from the
PARC 700 Dependency Bank (King et al, 2003),
which include 700 carefully selected sentences
from the WSJ sections of the PTB. These sen-
tences were originally chosen for the purpose of
parser evaluation. Many linguistically challeng-
ing phenomena are included in these sentences,
although the sentence length is shorter in average
than the sentence length in the entire WSJ. The
language is also less related to the financial do-
main specific language observed in the WSJ. We
parsed the dataset with the Feb. 2009 version of
the ERG, and recorded up to 500 trees per sen-
tence (ranked by a MaxEnt parse selection model
trained on previously treebanked WSJ sections).
5 Results
Although we employed a typical statistical rank-
ing model in our system, it is difficult to directly
evaluate the absolute performance of the predicted
ranking. Annotators only annotate a very small
subset of the discriminants, and their order is not
fully specified. To compare the behavior of mod-
els trained with data annotated by different anno-
tators, we plot the relative ranking (normalized to
[0, 1] for each sentence, with 0 being the highest
rank and 1 the lowest) of discriminants for 50 sen-
tences in Figure 2.
The plot shows a strong positive linear correla-
tion between the two ranking models. The partic-
ularly strong correlation at the low and high ends
of the ranking shows that the two annotators share
a similar behavior pattern concerning the most and
least preferred discriminants. The correlation is
slightly weaker in the middle ranking zone, where
different preferences or annotation styles can be
observed.
To further visualize the effect of the ranking
model, we highlighted with color the discrimi-
nants which are manually annotated by annotator
B under a basic setting without using the ranking
models. 75% of these ?prominent? discriminants
are grouped within the top-25% region of the plot.
Without surprise, the model B gives an average
relative ranking of 0.18 as oppose to 0.21 with
model A. The overall distribution of rankings for
manually disambiguated discriminants are shown
in Figure 3.
In Table 2, the average treebanking speed of
two annotators over multiple annotation sessions
is shown. The baseline model ranks the discrim-
inants by the spanning length of the correspond-
ing constituent, and uses the alphabetical order
of the rule or lexical type names as tie-breaker.
The own-model refers to the annotation sessions
which have been carried out by the annotators us-
1458
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Mo
del
 B 
Ra
nki
ng
Model A Ranking
Figure 2: Correlation of discriminant ranks with
different models and manual annotation
 0
 10
 20
 30
 40
 50
 60
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Nu
mb
er o
f D
isc
rim
ina
nts
Relative Ranking
Model AModel B
Figure 3: Histogram of rankings given by two
models on discriminants manually picked by an-
notator B
ing their own ranking model. The peer-model
refers to the annotation sessions where the annota-
tors use their peer colleague?s model. And finally,
the joint-model refers to the annotations done by
the jointly trained model.
The annotation efficiency was boosted by over
50% with all the discriminant ranking models.
The own-model setting achieved best speed. This
is probably due to the fact that the model most
closely reflects the annotation habit of the annota-
tor. But the advantage over other models is very
small.
To measure the inter-annotator agreement, we
calculate the Cohen?s KAPPA (Carletta, 1996) on
the constituents of the derivation trees:
? = Pr(a)? Pr(e)1? Pr(e) (2)
Ranking Model Speed (s/h) Speed-up (%)
Baseline 61.9 ?
Own-model 96.1 55%
Peer-model 94.6 53%
Joint-model 95.0 53%
Table 2: Average annotation speed with different
discriminant ranking models
where Pr(a) is the relative observed agreement
between annotators, and Pr(e) is the probability
of two annotators agreeing by chance. The calcu-
lation of Pr(a) can be done in a similar way to
the calculation of PARSEVAL labeled bracketing
accuracy, while Pr(e) is estimated by averaging
the agreement over a large set of tree pairs ran-
domly sampled from the parse forest. Since the
calculation of ? takes into account the agreement
occurring by chance, it is a safer (though has the
tendency of being overly conservative) measure of
agreement.
Ranking Model Cohen?s KAPPA (?)
Baseline 0.5404
Own-model 0.5798
Peer-model 0.5567
Joint-model 0.5536
Table 3: Inter-annotator agreement measured by
constituent-level Cohen?s KAPPA
The numbers in Table 3 show that the use of
discriminant ranking models results in a small im-
provement to the inter-annotator agreement, with
the best agreement achieved by each annotator us-
ing the model trained with their own annotation
records. These numbers are comforting in that the
annotation quality is not damaged by our new way
to present the linguistic decisions.
Note that the relatively low inter-annotator
agreement in this experiment is due to the fact that
we used a dataset which involves non-trivial lin-
guistic phenomena that are on average more dif-
ficult than the texts in the WSJ corpus. Another
fact is that these annotations were done under time
pressure. The annotators are not encouraged to go
backwards to check and correct the previous sen-
tences during these sessions. On the entire WSJ,
we have recorded a stable and persistently higher
1459
agreement level at ? = 0.6. Given the highly de-
tailed linguistic annotations specified by the gram-
mar (over 260 rules and 800 lexical types), this
figure indicates a very substantial agreement be-
tween our annotators. Our further investigation
has shown that the agreement figure hits the ceil-
ing at around ? = 0.65. Further training and dis-
cussion is not rewarded with sustainable improve-
ment of annotation quality.
Apart from the numerical evaluation, we also
interview our annotators for subjective feelings
about the various ranking models. There is gen-
erally a very positive attitude towards all the rank-
ing models over the baseline. An easily decid-
able discriminant is usually found within the top-3
with very few exceptions, which leads to a self-
noticeable speed-up that confirms our numeric
findings. It is also interesting to note that, de-
spite the substantial difference between the statis-
tical models, the difference is hardly noticed by
the annotators. And the results only show small
variations in both the annotation speed, as well as
the inter-annotator agreement.
The annotators also claim that the speed-up
is somewhat diminished over the ?rejected? sen-
tences, for which none of the candidate trees are
acceptable. In such cases, the annotators still have
to go through a long sequence of discriminants,
and sometimes have to redo the previous steps in
fear of the chain-effect of wrong decisions. How
to compensate for the psychological insatisfaction
of rejecting all analyses while maintaining good
annotation speed and quality is a new topic for our
future research.
6 Conclusion & Future Work
We propose to use a statistical ranking model to
assist the discriminant-based treebank annotation.
Our experiment shows that such a model, trained
on annotation history, brings a huge efficiency im-
provement together with slightly improved inter-
annotator agreement.
Although the reported experiments were car-
ried out on the specific HPSG treebank, we be-
lieve that the proposed ranked discriminant-based
annotation method can be applied in annotation
tasks concerning different linguistic frameworks,
or even different layers of linguistic representa-
tion. Apart from the specific features presented
in Section 3.3, the model itself does not assume a
phrase-structure tree annotation, and the discrimi-
nants can take various forms. Assuming a ?gram-
mar? produces a number of candidate analyses,
the annotators can rely on the ranking model to ef-
ficiently pick relevant discriminants, and focus on
making linguistically relevant decisions. This is
especially suitable for large annotation tasks aim-
ing for parallel rich annotation by multiple anno-
tators, where fully manual annotation is not fea-
sible and high inter-annotator agreement hard to
achieve.
The ranking model is based on annotation his-
tory and influences the future progress of tree-
banking. It can be dynamically integrated into the
treebank development cycles in which the anno-
tation habit evolves over time. Such a model can
also shorten the training period for new annota-
tors, which is an interesting aspect for our future
investigation.
From a different point of view, the rankings of
the discriminants show annotators? confidence on
various ambiguities. The clearly uneven distri-
bution over discriminants can also provide gram-
mar writers with interesting feedback, helping
with the improvement of the linguistic analysis.
We would also like to integrate confidence mea-
sures into the computer-assisted treebank annota-
tion process, which could potentially help annota-
tors make difficult decisions, such as whether to
reject all trees for a sentence.
References
[Brants et al2002] Brants, Sabine, Stefanie Dipper,
Silvia Hansen, Wolfgang Lezius, and George Smith.
2002. The tiger treebank. In Proceedings of
the workshop on treebanks and linguistic theories,
pages 24?41.
[Callmeier2001] Callmeier, Ulrich. 2001. Effi-
cient parsing with large-scale unification gram-
mars. Master?s thesis, Universita?t des Saarlandes,
Saarbru?cken, Germany.
[Carletta1996] Carletta, Jean. 1996. Assessing agree-
ment on classification tasks: the kappa statistic.
Computational Linguistics, 22(2):249?254.
[Carter1997] Carter, David. 1997. The treebanker: a
tool for supervised training of parsed corpora. In
1460
Proceedings of the ACL Workshop on Computa-
tional Environments for Grammar Development and
Linguistic Engineering, pages 9?15, Madrid, Spain.
[Copestake et al2005] Copestake, Ann, Dan
Flickinger, Carl J. Pollard, and Ivan A. Sag.
2005. Minimal recursion semantics: an introduc-
tion. Research on Language and Computation,
3(4):281?332.
[Flickinger2002] Flickinger, Dan. 2002. On build-
ing a more efficient grammar by exploiting types.
In Oepen, Stephan, Dan Flickinger, Jun?ichi Tsu-
jii, and Hans Uszkoreit, editors, Collaborative Lan-
guage Engineering, pages 1?17. CSLI Publications.
[Hajic? et al2000] Hajic?, Jan, Alena Bo?hmova?, Eva
Hajic?ova?, and Barbora Vidova?-Hladka?. 2000. The
Prague Dependency Treebank: A Three-Level An-
notation Scenario. In Abeille?, A., editor, Treebanks:
Building and Using Parsed Corpora, pages 103?
127. Amsterdam:Kluwer.
[King et al2003] King, Tracy H., Richard Crouch, Ste-
fan Riezler, Mary Dalrymple, and Ronald M. Ka-
plan. 2003. The PARC 700 Dependency Bank. In
Proceedings of the 4th International Workshop on
Linguistically Interpreted Corpora, held at the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL?03), Bu-
dapest, Hungary.
[Kordoni and Zhang2009] Kordoni, Valia and
Yi Zhang. 2009. Annotating wall street jour-
nal texts using a hand-crafted deep linguistic
grammar. In Proceedings of The Third Linguistic
Annotation Workshop (LAW III), Singapore.
[Marcus et al1993] Marcus, Mitchell P., Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of english: The penn
treebank. Computational Linguistics, 19(2):313?
330.
[Oepen et al2002] Oepen, Stephan, Kristina
Toutanova, Stuart Shieber, Christopher Man-
ning, Dan Flickinger, and Thorsten Brants. 2002.
The LinGO Redwoods treebank: motivation and
preliminary applications. In Proceedings of COL-
ING 2002: The 17th International Conference on
Computational Linguistics: Project Notes, Taipei,
Taiwan.
[Oepen2001] Oepen, Stephan. 2001. [incr tsdb()] ?
competence and performance laboratory. User man-
ual. Technical report, Computational Linguistics,
Saarland University, Saarbru?cken, Germany.
[Pollard and Sag1994] Pollard, Carl J. and Ivan A. Sag.
1994. Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago, USA.
1461
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397?408,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Parser Evaluation over
Local and Non-Local Deep Dependencies in a Large Corpus
Emily M. Bender?, Dan Flickinger?, Stephan Oepen?, Yi Zhang?
?Dept of Linguistics, University of Washington, ?CSLI, Stanford University
?Dept of Informatics, Universitetet i Oslo, ?Dept of Computational Linguistics, Saarland University
ebender@uw.edu, danf@stanford.edu, oe@ifi.uio.no, yzhang@coli.uni-sb.de
Abstract
In order to obtain a fine-grained evaluation of
parser accuracy over naturally occurring text,
we study 100 examples each of ten reason-
ably frequent linguistic phenomena, randomly
selected from a parsed version of the En-
glish Wikipedia. We construct a correspond-
ing set of gold-standard target dependencies
for these 1000 sentences, operationalize map-
pings to these targets from seven state-of-the-
art parsers, and evaluate the parsers against
this data to measure their level of success in
identifying these dependencies.
1 Introduction
The terms ?deep? and ?shallow? are frequently used
to characterize or contrast different approaches to
parsing. Inevitably, such informal notions lack a
clear definition, and there is little evidence of com-
munity consensus on the relevant dimension(s) of
depth, let alne agreement on applicable metrics. At
its core, the implied dichotomy of approaches al-
ludes to differences in the interpretation of the pars-
ing task. Its abstract goal, on the one hand, could
be pre-processing of the linguistic signal, to enable
subsequent stages of analysis. On the other hand, it
could be making explicit the (complete) contribution
that the grammatical form of the linguistic signal
makes to interpretation, working out who did what
to whom. Stereotypically, one expects correspond-
ing differences in the choice of interface representa-
tions, ranging from various levels of syntactic anal-
ysis to logical-form representations of semantics.
In this paper, we seek to probe aspects of variation
in automated linguistic analysis. We make the as-
sumption that an integral part of many (albeit not all)
applications of parsing technology is the recovery of
structural relations, i.e. dependencies at the level of
interpretation. We suggest a selection of ten linguis-
tic phenomena that we believe (a) occur with reason-
ably high frequency in running text and (b) have the
potential to shed some light on the depths of linguis-
tic analysis. We quantify the frequency of these con-
structions in the English Wikipedia, then annotate
100 example sentences for each phenomenon with
gold-standard dependencies reflecting core proper-
ties of the phenomena of interest. This gold standard
is then used to estimate the recall of these dependen-
cies by seven commonly used parsers, providing the
basis for a qualitative discussion of the state of the
art in parsing for English.
In this work, we answer the call by Rimell et
al. (2009) for ?construction-focused parser evalua-
tion?, extending and complementing their work in
several respects: (i) we investigate both local and
non-local dependencies which prove to be challeng-
ing for many existing state-of-the-art parsers; (ii) we
investigate a wider range of linguistic phenomena,
each accompanied with an in-depth discussion of
relevant properties; and (iii) we draw our data from
the 50-million sentence English Wikipedia, which
is more varied and a thousand times larger than the
venerable WSJ corpus, to explore a more level and
ambitious playing field for parser comparison.
2 Background
All parsing systems embody knowledge about possi-
ble and probable pairings of strings and correspond-
ing linguistic structure. Such linguistic and proba-
bilistic knowledge can be hand-coded (e.g., as gram-
mar rules) or automatically acquired from labeled or
397
unlabeled training data. A related dimension of vari-
ation is the type of representations manipulated by
the parser. We briefly review some representative
examples along these dimensions, as these help to
position the parsers we subsequently evaluate.1
2.1 Approaches to parsing
Source of linguistic knowledge At one end of this
dimension, we find systems whose linguistic knowl-
edge is encoded in hand-crafted rules and lexical en-
tries; for English, the ParGram XLE system (Rie-
zler et al, 2002) and DELPH-IN English Resource
Grammar (ERG; Flickinger (2000))?each reflect-
ing decades of continuous development?achieve
broad coverage of open-domain running text, for ex-
ample. At the other end of this dimension, we find
fully unsupervised approaches (Clark, 2001; Klein
and Manning, 2004), where the primary source of
linguistic knowledge is co-occurrence patterns of
words in unannotated text. As Haghighi and Klein
(2006) show, augmenting this knowledge with hand-
crafted prototype ?seeds? can bring strong improve-
ments. Somewhere between these poles, a broad
class of parsers take some or all of their linguistic
knowledge from annotated treebanks, e.g. the Penn
Treebank (PTB), which encodes ?surface grammati-
cal analysis? (Marcus et al, 1993). Such approaches
include those that directly (and exclusively) use the
information in the treebank (e.g. Charniak (1997),
Collins (1999), Petrov et al (2006), inter alios) as
well as those that complement treebank structures
with some amount of hand-coded linguistic knowl-
edge (e.g. O?Donovan et al (2004), Miyao et al
(2004), Hockenmaier and Steedman (2007), inter
alios). Another hybrid in terms of its acquisition of
linguistic knowledge is the RASP system of Briscoe
et al (2006), combining a hand-coded grammar over
PoS tag sequences with a probabilistic tagger and
statistical syntactic disambiguation.
Design of representations Approaches to parsing
also differ fundamentally in the style of represen-
tation assigned to strings. These vary both in their
1Additional sources of variation among extant parsing tech-
nologies include (a) the behavior with respect to ungrammatical
inputs and (b) the relationship between probabilistic and sym-
bolic knowledge in the parser, where parsers with a hand-coded
grammar at their core typically also incorporate an automati-
cally trained probabilistic disambiguation component.
formal nature and the ?granularity? of linguistic in-
formation (i.e. the number of distinctions assumed),
encompassing variants of constituent structure, syn-
tactic dependencies, or logical-form representations
of semantics. Parser interface representations range
between the relatively simple (e.g. phrase structure
trees with a limited vocabulary of node labels as in
the PTB, or syntactic dependency structures with a
limited vocabulary of relation labels as in Johansson
and Nugues (2007)) and the relatively complex, as
for example elaborate syntactico-semantic analyses
produced by the ParGram or DELPH-IN grammars.
There tends to be a correlation between the
methodology used in the acquisition of linguistic
knowledge and the complexity of representations: in
the creation of a mostly hand-crafted treebank like
the PTB, representations have to be simple enough
for human annotators to reliably manipulate. Deriv-
ing more complex representations typically presup-
poses further computational support, often involv-
ing some hand-crafted linguistic knowledge?which
can take the form of mappings from PTB-like repre-
sentations to ?richer? grammatical frameworks (as
in the line of work by O?Donovan et al (2004), and
others; see above), or can be rules for creating the
parse structures in the first place (i.e. a computa-
tional grammar), as for example in the treebanks of
van der Beek et al (2002) or Oepen et al (2004).2
In principle, one might expect that richer repre-
sentations allow parsers to capture complex syntac-
tic or semantic dependencies more explicitly. At the
same time, such ?deeper? relations may still be re-
coverable (to some degree) from comparatively sim-
ple parser outputs, as demonstrated for unbounded
dependency extraction from strictly local syntactic
dependency trees by Nivre et al (2010).
2.2 An armada of parsers
Stanford Parser (Klein and Manning, 2003) is a
probabilistic parser which can produce both phrase
structure trees and grammatical relations (syntactic
dependencies). The parsing model we evaluate is the
2A noteworthy exception to this correlation is the annotated
corpus of Zettlemoyer and Collins (2005), which pairs sur-
face strings from the realm of natural language database inter-
faces directly with semantic representations in lambda calculus.
These were hand-written on the basis of database query state-
ments distributed with the original datasets.
398
English factored model which combines the prefer-
ences of unlexicalized PCFG phrase structures and
of lexical dependencies, trained on sections 02?21
of the WSJ portion of the PTB. We chose Stanford
Parser from among the state-of-the-art PTB-derived
parsers for its support for grammatical relations as
an alternate interface representation.
Charniak&Johnson Reranking Parser (Char-
niak and Johnson, 2005) is a two-stage PCFG parser
with a lexicalized generative model for the first-
stage, and a discriminative MaxEnt reranker for the
second-stage. The models we evaluate are also
trained on sections 02?21 of the WSJ. Top-50 read-
ings were used for the reranking stage. The output
constituent trees were then converted into Stanford
Dependencies. According to Cer et al (2010), this
combination gives the best parsing accuracy in terms
of Stanford dependencies on the PTB.
Enju (Miyao et al, 2004) is a probabilistic HPSG
parser, combining a hand-crafted core grammar with
automatically acquired lexical types from the PTB.3
The model we evaluate is trained on the same ma-
terial from the WSJ sections of the PTB, but the
treebank is first semi-automatically converted into
HPSG derivations, and the annotation is enriched
with typed feature structures for each constituent.
In addition to HPSG derivation trees, Enju also pro-
duces predicate argument structures.
C&C (Clark and Curran, 2007) is a statistical
CCG parser. Abstractly similar to the approach of
Enju, the grammar and lexicon are automatically
induced from CCGBank (Hockenmaier and Steed-
man, 2007), a largely automatic projection of (the
WSJ portion of) PTB trees into the CCG framework.
In addition to CCG derivations, the C&C parser can
directly output a variant of grammatical relations.
RASP (Briscoe et al, 2006) is an unlexicalized
robust parsing system, with a hand-crafted ?tag se-
quence? grammar at its core. The parser thus anal-
yses a lattice of PoS tags, building a parse forest
from which the most probable syntactic trees and
sets of corresponding grammatical relations can be
extracted. Unlike other parsers in our mix, RASP
did not build on PTB data in either its PoS tagging
3This hand-crafted grammar is distinct from the ERG, de-
spite sharing the general framework of HPSG. The ERG is not
included in our evaluation, since it was used in the extraction of
the original examples and thus cannot be fairly evaluated.
or syntactic disambiguation components.
MSTParser (McDonald et al, 2005) is a data-
driven dependency parser. The parser uses an edge-
factored model and searches for a maximal span-
ning tree that connects all the words in a sentence
into a dependency tree. The model we evaluate
is the second-order projective model trained on the
same WSJ corpus, where the original PTB phrase
structure annotations were first converted into de-
pendencies, as established in the CoNLL shared task
2009 (Johansson and Nugues, 2007).
XLE/ParGram (Riezler et al, 2002, see also
Cahill et al, 2008) applies a hand-built Lexical
Functional Grammar for English and a stochastic
parse selection model. For our evaluation, we used
the Nov 4, 2010 release of XLE and the Nov 25,
2009 release of the ParGram English grammar, with
c-structure pruning turned off and resource limita-
tions set to the maximum possible to allow for ex-
haustive search. In particular, we are evaluating the
f-structures output by the system.
Each parser, of course, has its own requirements
regarding preprocessing of text, especially tokeniza-
tion. We customized the tokenization to each parser,
by using the parser?s own internal tokenization or
pre-tokenizing to match the parser?s desired input.
The evaluation script is robust to variations in tok-
enization across parsers.
3 Phenomena
In this section we summarize the ten phenomena we
explore and our motivations for choosing them. Our
goal was to find phenomena where the relevant de-
pendencies are relatively subtle, such that more lin-
guistic knowledge is beneficial in order to retrieve
them. Though this set is of course only a sampling,
these phenomena illustrate the richness of structure,
both local and non-local, involved in the mapping
from English strings to their meanings. We discuss
the phenomena in four sets and then briefly review
their representation in the Penn Treebank.
3.1 Long distance dependencies
Three of our phenomena can be classified as involv-
ing long-distance dependencies: finite that-less rel-
atives clauses (?barerel?), tough adjectives (?tough?)
and right node raising (?rnr?). These are illustrated
399
in the following examples:4
(1) barerel: This is the second time in a row Aus-
tralia has lost their home tri-nations? series.
(2) tough: Original copies are very hard to find.
(3) rnr: Ilu?vatar, as his names imply, exists before
and independently of all else.
While the majority of our phenomena involve lo-
cal dependencies, we include these long-distance
dependency types because they are challenging for
parsers and enable more direct comparison with the
work of Rimell et al (2009), who also address right
node raising and bare relatives. Our barerel category
corresponds to their ?object reduced relative? cate-
gory with the difference that we also include adverb
relatives, where the head noun functions as a modi-
fier within the relative clause, as does time in (1). In
contrast, our rnr category is somewhat narrower than
Rimell et al (2009)?s ?right node raising? category:
where they include raised modifiers, we restrict our
category to raised complements.
Part of the difficulty in retrieving long-distance
dependencies is that the so-called extraction site is
not overtly marked in the string. In addition to this
baseline level of complication, these three construc-
tion types present further difficulties: Bare relatives,
unlike other relative clauses, do not carry any lexi-
cal cues to their presence (i.e., no relative pronouns).
Tough adjective constructions require the presence
of specific lexical items which form a subset of a
larger open class. They are rendered more difficult
by two sources of ambiguity: alternative subcatego-
rization frames for the adjectives and the purposive
adjunct analysis (akin to in order to) for the infiniti-
val VP. Finally, right node raising often involves co-
ordination where one of the conjuncts is in fact not
a well-formed phrase (e.g., independently of in (3)),
making it potentially difficult to construct the correct
coordination structure, let alne associate the raised
element with the correct position in each conjunct.
3.2 Non-dependencies
Two of our phenomena crucially look for the lack of
dependencies. These are it expletives (?itexpl?) and
verb-particle constructions (?vpart?):
4All examples are from our data. Words involved in the rel-
evant dependencies are highlighted in italics (dependents) and
boldface (heads).
(4) itexpl: Crew negligence is blamed, and it is sug-
gested that the flight crew were drunk.
(5) vpart: He once threw out two baserunners at
home in the same inning.
The English pronoun it can be used as an ordi-
nary personal pronoun or as an expletive: a place-
holder for when the language demands a subject (or
occasionally object) NP but there is no semantic role
for that NP. The expletive it only appears when it
is licensed by a specific construction (such as ex-
traposition, (4)) or selecting head. If the goal of
parsing is to recover from the surface string the de-
pendencies capturing who did what to whom, exple-
tive it should not feature in any of those dependen-
cies. Likewise, instances of expletive it should be
detected and discarded in reference resolution. We
hypothesize that detecting expletive it requires en-
coding linguistic knowledge about its licensers.
The other non-dependency we explore is between
the particle in verb-particle constructions and the
direct object. Since English particles are almost
always homophonous with prepositions, when the
object of the verb-particle pair follows the par-
ticle, there will always be a competing analysis
which analyses the sequence as V+PP rather than
V+particle+NP. Furthermore, since verb-particle
pairs often have non-compositional semantics (Sag
et al, 2002), misanalyzing these constructions could
be costly to downstream components.
3.3 Phrasal modifiers
Our next category concerns modifier phrases:
(6) ned: Light colored glazes also have softening
effects when painted over dark or bright images.
(7) absol: The format consisted of 12 games, each
team facing the other teams twice.
The first, (?ned?), is a pattern which to our knowl-
edge has not been named in the literature, where a
noun takes the typically verbal -ed ending, is modi-
fied by another noun or adjective, and functions as a
modifier or a predicate. We believe this phenomenon
to be interesting because its unusual morphology is
likely to lead PoS-taggers astray, and because the
often-hyphenated Adj+N-ed constituent has produc-
tive internal structure constraining its interpretation.
The second phrasal modifier we investigate is the
absolutive construction. An absolutive consists of an
400
NP followed by a non-finite predicate (such as could
appear after the copula be). The whole phrase mod-
ifies a verbal projection that it attaches to. Absolu-
tives may be marked with with or unmarked. Here,
we focus on the unmarked type as this lack of lexical
cue can make the construction harder to detect.
3.4 Subtle arguments
Our final three phenomena involve ways in which
verbal arguments can be more difficult to identify
than in ordinary finite clauses. These include de-
tecting the arguments of verbal gerunds (?vger?), the
interleaving of arguments and adjuncts (?argadj?) and
raising/control (?control?) constructions.
(8) vger: Accessing the website without the ?www?
subdomain returned a copy of the main site for
?EP.net?.
(9) argadj: The story shows, through flashbacks, the
different histories of the characters.
(10) control: Alfred ?retired? in 1957 at age 60 but
continued to paint full time.
In a verbal gerund, the -ing form a verb retains
verbal properties (e.g., being able to take NP com-
plements, rather than only PP complements) but
heads a phrase that fills an NP position in the syn-
tax (Malouf, 2000). Since gerunds have the same
morphology as present participle VPs, their role in
the larger clause is susceptible to misanalysis.
The argadj examples are of interest because En-
glish typically prefers to have direct objects directly
adjacent to the selecting verb. Nonetheless, phe-
nomena such as parentheticals and heavy-NP shift
(Arnold et al, 2000), in which ?heavy? constituents
appear further to the right in the string, allow for
adjunct-argument order in a minority of cases. We
hypothesize that the relative infrequency of this con-
struction will lead parsers to prefer incorrect analy-
ses (wherein the adjunct is picked up as a comple-
ment, the complement as an adjunct or the structure
differs entirely) unless they have access to linguis-
tic knowledge providing constraints on possible and
probable complementation patterns for the head.
Finally, we turn to raising and control verbs (?con-
trol?) (e.g., Huddleston and Pullum (2002, ch. 14)).
These verbs select for an infinitival VP complement
and stipulate that another of their arguments (sub-
ject or direct object in the examples we explore) is
identified with the unrealized subject position of the
infinitival VP. Here it is the dependency between
the infinitval VP and the NP argument of the ?up-
stairs? verb which we expect to be particularly sub-
tle. Getting this right requires specific lexical knowl-
edge about which verbs take these complementation
patterns. This lexical knowledge needs to be repre-
sented in such a way that it can be used robustly even
in the case of passives, relative clauses, etc.5
3.5 Penn Treebank representations
We investigated the representation of these 10 phe-
nomena in the PTB (Marcus et al, 1993) in two
steps: First we explored the PTB?s annotation guide-
lines (Bies et al, 1995) to determine how the rele-
vant dependencies were intended to be represented.
We then used Ghodke and Bird?s (2010) Treebank
Search to find examples of the intended annotations
as well as potential examples of the phenomena an-
notated differently, to get a sense of the consistency
of the annotation from both precision and recall per-
spectives. In this study, we take the phrase structure
trees of the PTB to represent dependencies based on
reasonable identification of heads.
The barerel, vpart, and absol phenomena are com-
pletely unproblematic, with their relevant dependen-
cies explicitly and reliably represented. In addition,
the tough construction is reliably annotated, though
one of the dependencies we take to be central is not
directly represented: The missing argument is linked
to a null wh head at the left edge of the comple-
ment of the tough predicate, rather than to its sub-
ject. Two further phenomena (rnr and vger) are es-
sentially correctly represented: the representations
of the dependencies are explicit and mostly but not
entirely consistently applied. Two out of a sample of
20 examples annotated as containing rnr did not, and
two out of a sample of 35 non-rnr-annotated coordi-
nations actually contained rnr. For vger the primary
problem is with the PoS tagging, where the gerund
is sometimes given a nominal tag, contrary to PTB
guidelines, though the structure above it conforms.
The remaining four constructions are more prob-
lematic. In the case of object control, while the guide-
5Distinguishing between raising and control requires fur-
ther lexical knowledge and is another example of a ?non-
dependency? (in the raising examples). We do not draw that
distinction in our annotations.
401
lines specify an analysis in which the shared NP is
attached as the object of the higher verb, the PTB
includes not only structures conforming to that anal-
ysis but also ?small clause? structures, with the latter
obscuring the relationship of the shared argument to
the higher verb. In the case of itexpl, the adjoined
(S(-NONE- *EXP*)) indicating an expletive use of
it is applied consistently for extraposition (as pre-
scribed in the guidelines). However, the set of lex-
ical licensers of the expletive is incomplete. For ar-
gadj we run into the problem that the PTB does not
explicitly distinguish between post-verbal modifiers
and verbal complements in the way that they are at-
tached. The guidelines suggest that the function tags
(e.g., PP-LOC, etc.) should allow one to distinguish
these, but examination of the PTB itself suggests
that they are not consistently applied. Finally, the
ned construction is not mentioned in the PTB guide-
lines nor is its internal structure represented in the
treebank. Rather, strings like gritty-eyed are left un-
segmented and tagged as JJ.
We note that the PTB representations of many of
these phenomena (barerel, tough, rnr, argadj, control,
itexpl) involve empty elements and/or function tags.
Systems that strip these out before training, as is
common practice, will not benefit from the informa-
tion that is in the PTB.
Our purpose here is not to criticize the PTB,
which has been a tremendously important resource
to the field. Rather, we have two aims: The first is
to provide context for the evaluation of PTB-derived
parsers on these phenomena. The second is to high-
light the difficulty of producing consistent annota-
tions of any complexity as well as the hurdles faced
by a hand-annotation approach when attempting to
scale a resource to more complex representations
and/or additional phenomena (though cf. Vadas and
Curran (2008) on improving PTB representations).
4 Methodology
4.1 Data extraction
We processed 900 million tokens of Wikipedia text
using the October 2010 release of the ERG, follow-
ing the work of the WikiWoods project (Flickinger
et al, 2010). Using the top-ranked ERG deriva-
tion trees as annotations over this corpus and sim-
ple patterns using names of ERG-specific construc-
Phenomenon Frequency Candidates
barerel 2.12% 546
tough 0.07% 175
rnr 0.69% 1263
itexpl 0.13% 402
vpart 4.07% 765
ned 1.18% 349
absol 0.51% 963
vger 5.16% 679
argadj 3.60% 1346
control 3.78% 124
Table 1: Relative frequencies of phenomena matches in
Wikipedia, and number of candidate strings vetted.
tions or lexical types, we randomly selected a set
of candidate sentences for each of our ten phenom-
ena. These candidates were then hand-vetted in se-
quence by two annotators to identify, for each phe-
nomenon, 100 examples that do in fact involve the
phenomenon in question and which are both gram-
matical and free of typos. Examples that were ei-
ther deemed overly basic (e.g. plain V+V coordi-
nation, which the ERG treats as rnr) or inappropri-
ately complex (e.g. non-constituent coordination ob-
scuring the interleaving of arguments and adjuncts)
were also discarded at this step. Table 1 summarizes
relative frequencies of each phenomenon in about
47 million parsed Wikipedia sentences, as well as
the total size of the candidate sets inspected. For
the control and tough phenomena hardly any filtering
for complexity was applied, hence these can serve
as indicators of the rate of genuine false positives.
For phenomena that partially overlap with those of
Rimell et al (2009), it appears our frequency es-
timates are comparable to what they report for the
Brown Corpus (but not the WSJ portion of the PTB).
4.2 Annotation format
We annotated up to two dependency triples per phe-
nomenon instance, identifying the heads and depen-
dents by the surface form of the head words in the
sentence suffixed with a number indicating word po-
sition (see Table 2).6 Some strings contain more
than one instance of the phenomenon they illustrate;
in these cases, multiple sets of dependencies are
6As the parsers differ in tokenization strategies, our evalua-
tion script treats these position IDs as approximate indicators.
402
Item ID Phenomenon Polarity Dependency
1011079100200 absol 1 having-2|been-3|passed-4 ARG act-1
1011079100200 absol 1 withdrew-9 MOD having-2|been-3|passed-4
1011079100200 absol 1 carried+on-12 MOD having-2|been-3|passed-4
Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7
Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.
Phenomenon Head Type Dependent Distance
Bare relatives gapped predicate in relative ARG2/MOD modified noun 3.0 (8)
(barerel) modified noun MOD top predicate of relative 3.3 (8)
Tough adjectives tough adjective ARG2 to-VP complement 1.7 (5)
(tough) gapped predicate in to-VP ARG2 subject/modifiee of adjective 6.4 (21)
Right Node Raising verb/prep2 ARG2 shared noun 2.8 (9)
(rnr) verb/prep1 ARG2 shared noun 6.1 (12)
Expletive It it-subject taking verb !ARG1 it 1.2 (3)
(itexpl) raising-to-object verb !ARG2 it ?
Verb+particle constructions particle !ARG2 complement 2.7 (9)
(vpart) verb+particle ARG2 complement 3.7 (10)
Adj/Noun2 + Noun1-ed head noun MOD Noun1-ed 2.4 (17)
(ned) Noun1-ed ARG1/MOD Adj/Noun2 1.0 (1.5)
Absolutives absolutive predicate ARG1 subject of absolutive 1.7 (12)
(absol) main clause predicate MOD absolutive predicate 9.8 (26)
Verbal gerunds selecting head ARG[1,2] gerund 1.9 (13)
(vger) gerund ARG2/MOD first complement/modifier of gerund 2.3 (8)
Interleaved arg/adj selecting verb MOD interleaved adjunct 1.2 (7)
(argadj) selecting verb ARG[2,3] displaced complement 5.9 (26)
Control ?upstairs? verb ARG[2,3] ?downstairs? verb 2.4 (23)
(control) ?downstairs? verb ARG1 shared argument 4.8 (17)
Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.
recorded. In addition, some strings evince more than
one of the phenomena we are studying. However,
we only annotate the dependencies associated with
the phenomenon the string was selected to repre-
sent. Finally, in examples with coordinated heads or
dependents, we recorded separate dependencies for
each conjunct. In total, we annotated 2127 depen-
dency triples for the 1000 sentences, including 253
negative dependencies (see below). Table 3 outlines
the dependencies annotated for each phenomenon.
To allow for multiple plausible attachment sites,
we give disjunctive values for heads or dependents
in several cases: (i) with auxiliaries, (ii) with com-
plementizers (that or to, as in Table 2), (iii) in cases
of measure or classifier nouns or partitives, (iv) with
multi-word proper names and (v) where there is
genuine attachment ambiguity for modifiers. As
these sets of targets are disjunctive, these conven-
tions should have the effect of increasing measured
parser performance. 580 (27%) of the annotated de-
pendencies had at least one disjunction.
4.3 Annotation and reconciliation process
The entire data set was annotated independently by
two annotators. Both annotators were familiar with
the ERG, used to identify these sentences in the
WikiWoods corpus, but the annotation was done
without reference to the ERG parses. Before begin-
ning annotation on each phenomenon, we agreed on
which dependencies to annotate. We also communi-
cated with each other about annotation conventions
as the need for each convention became clear. The
annotation conventions address how to handle co-
ordination, semantically empty auxiliaries, passives
and similar orthogonal phenomena.
Once the entire data set was dual-annotated, we
compared annotations, identifying the following
sources of mismatch: typographical errors, incom-
pletely specified annotation conventions, inconsis-
tent application of conventions (101 items, dropping
in frequency as the annotation proceeded), and gen-
uine disagreement about what to annotate, either dif-
ferent numbers of dependencies of interest identified
403
in an item (59 items) or conflicting elements in a de-
pendency (54 items).7 Overall, our initial annotation
pass led to agreement on 79% of the items, and a
higher per-dependency level of agreement. Agree-
ment could be expected to approach 90% with more
experience in applying annotation conventions.
We then reconciled the annotations, using the
comparison to address all sources of difference. In
most cases, we readily agreed which annotation was
correct and which was in error. In a few cases, we
decided that both annotations were plausible alter-
natives (e.g., in terms of alternative attachment sites
for modifiers) and so created a single merged anno-
tation expressing the disjunction of both (cf. ? 4.2).
5 Evaluation
With the test data consisting of 100 items for each of
our ten selected phenomena, we ran all seven pars-
ing systems and recorded their dependency-style
outputs for each sentence. While these outputs
are not directly comparable with each other, we
were able to associate our manually-annotated tar-
get dependencies with parser-specific dependencies,
by defining sets of phenomenon-specific regular ex-
pressions for each parser. In principle, we allow this
mapping to be somewhat complex (and forgiving to
non-contentful variation), though we require that it
work deterministically and not involve specific lexi-
cal information. An example set is given in Fig. 2.
"absol" =>
{?ARG1? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W2}\W*_(\d+) \W*{W1}\W*_(\d+)\)?],
?ARG? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?],
?MOD? => [
?\(xmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(cmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?]}
Figure 2: Regexp set to evaluate C&C for absol.
These expressions fit the output that we got from the
C&C parser, illustrated in Fig. 3 with a relevant por-
tion of the dependencies produced for the example
in Table 2. Here the C&C dependency (ncsubj
passed 4 Act 1 ) matches the first target in the
7We do not count typographical errors or incompletely spec-
ified conventions as failures of inter-annotator agreement.
gold-standard (Table 2), but no matching C&C de-
pendency is found for the other two targets.
(xmod _ Act_1 passed_4)
(ncsubj passed_4 Act_1 _)
(ncmod _ withdrew,_9 Jessop_8)
(dobj year,_7 withdrew,_9)
Figure 3: Excerpts of C&C output for item in Table 2.
The regular expressions operate solely on the de-
pendency labels and are not lexically-specific. They
are specific to each phenomenon, as we did not at-
tempt to write a general dependency converter, but
rather to discover what patterns of dependency rela-
tions describe the phenomenon when it is correctly
identified by each parser. Thus, though we did not
hold out a test set, we believe that they would gener-
alize to additional gold standard material annotated
in the same way for the same phenomena.8
In total, we wrote 364 regular expressions to han-
dle the output of the seven parsers, allowing some
leeway in the role labels used by a parser for any
given target dependency. The supplementary mate-
rials for this paper include the test data, parser out-
puts, target annotations, and evaluation script.
Fig. 1 provides a visualization of the results of our
evaluation. Each column of points represents one
dependency type. Dependency types for the same
phenomenon are represented by adjacent columns.
The order of the columns within a phenomenon fol-
lows the order of the dependency descriptions in
Table 3: For each pair, the dependency type with
the higher score for the majority of the parsers is
shown first (to the left). The phenomena them-
selves are also arranged according to increasing (av-
erage) difficulty. itexpl only has one column, as we
annotated just one dependency per instance here.
(The two descriptions in Table 3 reflect different,
mutually-incompatible instance types.) Since ex-
pletive it should not be the semantic dependent of
any head, the targets are generalized for this phe-
nomenon and the evaluation script counts as incor-
8In the case of the XLE, our simplistic regular-expression
approach to the interpretation of parser outputs calls for much
more complex patterns than for the other parsers. This is owed
to the rich internal structure of LFG f-structures and higher
granularity of linguistic analysis, where feature annotations on
nodes as well as reentrancies need to be taken into account.
Therefore, our current results for the XLE admit small amounts
of both over- and under-counting.
404
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
vger
vpart
control
argadj
barerel
rnr tough
ned itexpl
absol
enju
xle
c&j
c&c
stanford
mst
rasp
Figure 1: Individual dependency recall for seven parsers over ten phenomena.
rect any dependency involving referential it.
We observe fairly high recall of the dependencies
for vpart and vger (with the exception of RASP), and
high recall for both dependencies representing con-
trol for five systems. While Enju, Stanford, MST,
and RASP all found between 70 and 85% of the de-
pendency between the adjective and its complement
in the tough construction, only Enju and XLE rep-
resented the dependency between the subject of the
adjective and the gap inside the adjective?s comple-
ment. For the remaining phenomena, each parser
performed markedly worse on one dependency type,
compared to the other. The only exceptions here
are XLE and C&C?s (and to a lesser extent, C&J?s)
scores for barerel. No system scored higher than
33% on the harder of the two dependencies in rnror
absol, and Stanford, MST, and RASP all scored be-
low 25% on the harder dependency in barerel. Only
XLE scored higher than 10% on the second depen-
dency for ned and higher than 50% for itexpl.
6 Discussion
From the results in Fig. 1, it is clear that even the best
of these parsers fail to correctly identify a large num-
ber of relevant dependencies associated with linguis-
tic phenomena that occur with reasonable frequency
in the Wikipedia. Each of the parsers attempts
with some success to analyze each of these phe-
nomena, reinforcing the claim of relevance, but they
vary widely across phenomena. For the two long-
distance phenomena that overlap with those studied
in Rimell et al (2009), our results are comparable.9
Our evaluation over Wikipedia examples thus shows
the same relative lack of success in recovering long-
distance dependencies that they found for WSJ sen-
tences. The systems did better on relatively well-
studied phenomena including control, vger, and vpart,
but had less success with the rest, even though all but
two of those remaining phenomena involve syntac-
tically local dependencies (as indicated in Table 3).
Successful identification of the dependencies in
these phenomena would, we hypothesize, benefit
from richer (or deeper) linguistic information when
parsing, whether it is lexical (tough, control, itexpl,
and vpart), or structural (rnr, absol, vger, argadj, and
barerel), or somewhere in between, as with ned. In
the case of treebank-trained parsers, for the informa-
tion to be available, it must be consistently encoded
in the treebank and attended to during training. As
9Other than Enju, which scores 16 points higher in the eval-
uation of Rimell et al, our average scores for each parser across
the dependencies for these phenomena are within 12 points of
those reported by Rimell et al (2009) and Nivre et al (2010).
405
noted in Sections 2.1 and 3.5, there is tension be-
tween developing sufficiently complex representa-
tions to capture linguistic phenomena and keeping
an annotation scheme simple enough that it can be
reliably produced by humans, in the case of hand-
annotation.
7 Related Work
This paper builds on a growing body of work which
goes beyond (un)labeled bracketing in parser evalua-
tion, including Lin (1995), Carroll et al (1998), Ka-
plan et al (2004), Rimell et al (2009), and Nivre et
al. (2010). Most closely related are the latter two of
the above, as we adopt their ?construction-focused
parser evaluation methodology?.
There are several methodological differences be-
tween our work and that of Rimell et al First, we
draw our evaluation data from a much larger and
more varied corpus. Second, we automate the com-
parison of parser output to the gold standard, and we
distribute the evaluation scripts along with the anno-
tated corpus, enhancing replicability. Third, where
Rimell et al extract evaluation targets on the basis
of PTB annotations, we make use of a linguistically
precise broad-coverage grammar to identify candi-
date examples. This allows us to include both local
and non-local dependencies not represented or not
reliably encoded in the PTB, enabling us to evalu-
ate parser performance with more precision over a
wider range of linguistic phenomena.
These methodological innovations bring two em-
pirical results. The first is qualitative: Where previ-
ous work showed that overall Parseval numbers hide
difficulties with long-distance dependencies, our re-
sults show that there are multiple kinds of reason-
ably frequent local dependencies which are also dif-
ficult for the current standard approaches to pars-
ing. The second is quantitative: Where Rimell et
al. found two phenomena which were virtually un-
analyzed (recall below 10%) for one or two parsers
each, we found eight phenomena which were vir-
tually unanalyzed by at least one system, includ-
ing two unanalyzed by five and one by six. Every
system had at least one virtually unanalyzed phe-
nomenon. Thus we have shown that the dependen-
cies being missed by typical modern approaches to
parsing are more varied and more numerous than
previously thought.
8 Conclusion
We have presented a detailed construction-focused
evaluation of seven parsers over 10 phenomena,
with 1000 examples drawn from English Wikipedia.
Gauging recall of such ?deep? dependencies, in our
view, can serve as a proxy for downstream pro-
cessing involving semantic interpretation of parser
outputs. Our annotations and automated evaluation
script are provided in the supplementary materials,
for full replicability. Our results demonstrate that
significant opportunities remain for parser improve-
ment, and highlight specific challenges that remain
invisible in aggregate parser evaluation (e.g. Parse-
val or overall dependency accuracy). These results
suggest that further progress will depend on train-
ing data that is both more extensive and more richly
annotated than what is typically used today (seeing,
for example, that a large part of more detailed PTB
annotation remains ignored in much parsing work).
There are obvious reasons calling for diversity in
approaches to parsing and for different trade-offs
in, for example, the granularity of linguistic analy-
sis, average accuracy, cost of computation, or ease
of adaptation. Our proposal is not to substitute
construction-focused evaluation on Wikipedia data
for widely used aggregate metrics and reference cor-
pora, but rather to augment such best practices in
the spirit of Rimell et al (2009) and expand the
range of phenomena considered in such evaluations.
Across frameworks and traditions (and in principle
languages), it is of vital importance to be able to
evaluate the quality of parsing (and grammar induc-
tion) algorithms in a maximally informative manner.
Acknowledgments
We are grateful to Tracy King for her assistance in
setting up the XLE system and to three anonymous
reviewers for helpful comments. The fourth author
thanks DFKI and the DFG funded Excellence Clus-
ter on MMCI for their support of the work. Data
preparation on the scale of Wikipedia was made pos-
sible through access to large-scale HPC facilities,
and we are grateful to the Scientific Computing staff
at UiO and the Norwegian Metacenter for Computa-
tional Science.
406
References
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76(1):28?55.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style Penn treebank project. Technical report, Univer-
sity of Pennsylvania.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, pages 77?80, Sydney, Australia.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2008. Speeding
up LFG parsing using c-structure pruning. In Coling
2008: Proceedings of the workshop on Grammar En-
gineering Across Frameworks, pages 33?40, Manch-
ester, England, August. Coling 2008 Organizing Com-
mittee.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference on
Language Resources and Evaluation, pages 447?454,
Granada.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Malta.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 598 ? 603, Providence, RI.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. In Proceedings of the 5th Confer-
ence on Natural Language Learning, pages 105?112.
Toulouse, France.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods. Syntacto-semantic annotation for
English Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation, Valletta, Malta.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Processing
with HPSG):15 ? 28.
Sumukh Ghodke and Steven Bird. 2010. Fast query for
large treebanks. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 267?275, Los Angeles, California, June.
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881?888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
In Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 97?104, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10, Cambridge, MA. MIT
Press.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, pages 478?485, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI-95, pages 1420?1425, Montreal, Canada.
Robert Malouf. 2000. Verbal gerunds as mixed cate-
gories in HPSG. In Robert Borsley, editor, The Nature
407
and Function of Syntactic Categories, pages 133?166.
Academic Press, New York.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of the 2005 Conference on Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancouver,
Canada.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
the Penn Treebank. In Proceedings of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing, pages 684?693, Hainan Island, China.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 833?841, Beijing, China.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
Van Genabith, and Andy Way. 2004. Large-scale in-
duction and evaluation of lexical resources from the
penn-ii treebank. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 367?374, Barcelona, Spain.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575 ? 596.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 813?821, Singapore. Association for Computa-
tional Linguistics.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions. A pain in the neck for NLP. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, volume 2276 of Lecture Notes in
Computer Science, pages 189?206. Springer, Berlin,
Germany.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of ACL-
08: HLT, pages 335?343, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
L. van der Beek, Gosse Bouma, Robert Malouf, and Gert-
jan van Noord. 2002. The Alpino Dependency Tree-
bank. In Mariet Theune, Anton Nijholt, and Hen-
dri Hondorp, editors, Computational Linguistics in the
Netherlands, Amsterdam, The Netherlands. Rodopi.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Twenty-First Annual Conference on
Uncertainty in Artificial Intelligence, pages 658?666,
Arlington, Virginia. AUAI Press.
408
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Chart Mining-based Lexical Acquisition with Precision Grammars
Yi Zhang,? Timothy Baldwin,?? Valia Kordoni,? David Martinez? and Jeremy Nicholson??
? DFKI GmbH and Dept of Computational Linguistics, Saarland University, Germany
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
yzhang@coli.uni-sb.de, tb@ldwin.net, kordoni@dfki.de,
{davidm,jeremymn}@csse.unimelb.edu.au
Abstract
In this paper, we present an innovative chart
mining technique for improving parse cover-
age based on partial parse outputs from preci-
sion grammars. The general approach of min-
ing features from partial analyses is applica-
ble to a range of lexical acquisition tasks, and
is particularly suited to domain-specific lexi-
cal tuning and lexical acquisition using low-
coverage grammars. As an illustration of the
functionality of our proposed technique, we
develop a lexical acquisition model for En-
glish verb particle constructions which oper-
ates over unlexicalised features mined from
a partial parsing chart. The proposed tech-
nique is shown to outperform a state-of-the-art
parser over the target task, despite being based
on relatively simplistic features.
1 Introduction
Parsing with precision grammars is increasingly
achieving broad coverage over open-domain texts
for a range of constraint-based frameworks (e.g.,
TAG, LFG, HPSG and CCG), and is being used in
real-world applications including information ex-
traction, question answering, grammar checking and
machine translation (Uszkoreit, 2002; Oepen et al,
2004; Frank et al, 2006; Zhang and Kordoni, 2008;
MacKinlay et al, 2009). In this context, a ?preci-
sion grammar? is a grammar which has been engi-
neered to model grammaticality, and contrasts with
a treebank-induced grammar, for example.
Inevitably, however, such applications demand
complete parsing outputs, based on the assumption
that the text under investigation will be completely
analysable by the grammar. As precision grammars
generally make strong assumptions about complete
lexical coverage and grammaticality of the input,
their utility is limited over noisy or domain-specific
data. This lack of complete coverage can make
parsing with precision grammars less attractive than
parsing with shallower methods.
One technique that has been successfully applied
to improve parser and grammar coverage over a
given corpus is error mining (van Noord, 2004;
de Kok et al, 2009), whereby n-grams with low
?parsability? are gathered from the large-scale out-
put of a parser as an indication of parser or (pre-
cision) grammar errors. However, error mining is
very much oriented towards grammar engineering:
its results are a mixture of different (mistreated) lin-
guistic phenomena together with engineering errors
for the grammar engineer to work through and act
upon. Additionally, it generally does not provide
any insight into the cause of the parser failure, and it
is difficult to identify specific language phenomena
from the output.
In this paper, we instead propose a chart min-
ing technique that works on intermediate parsing re-
sults from a parsing chart. In essence, the method
analyses the validity of different analyses for words
or constructions based on the ?lifetime? and prob-
ability of each within the chart, combining the con-
straints of the grammar with probabilities to evaluate
the plausibility of each.
For purposes of exemplification of the proposed
technique, we apply chart mining to a deep lexical
acquisition (DLA) task, using a maximum entropy-
based prediction model trained over a seed lexicon
and treebank. The experimental set up is the fol-
lowing: given a set of sentences containing puta-
tive instances of English verb particle constructions,
10
extract a list of non-compositional VPCs optionally
with valence information. For comparison, we parse
the same sentence set using a state-of-the-art statisti-
cal parser, and extract the VPCs from the parser out-
put. Our results show that our chart mining method
produces a model which is superior to the treebank
parser.
To our knowledge, the only other work that has
looked at partial parsing results of precision gram-
mars as a means of linguistic error analysis is that of
Kiefer et al (1999) and Zhang et al (2007a), where
partial parsing models were proposed to select a set
of passive edges that together cover the input se-
quence. Compared to these approaches, our pro-
posed chart mining technique is more general and
can be adapted to specific tasks and domains. While
we experiment exclusively with an HPSG grammar
in this paper, it is important to note that the proposed
method can be applied to any grammar formalism
which is compatible with chart parsing, and where it
is possible to describe an unlexicalised lexical entry
for the different categories of lexical item that are to
be extracted (see Section 3.2 for details).
The remainder of the paper is organised as fol-
lows. Section 2 defines the task of VPC extraction.
Section 3 presents the chart mining technique and
the feature extraction process for the VPC extraction
task. Section 4 evaluates the model performance
with comparison to two competitor models over sev-
eral different measures. Section 5 further discusses
the general applicability of chart mining. Finally,
Section 6 concludes the paper.
2 Verb Particle Constructions
The particular construction type we target for DLA
in this paper is English Verb Particle Constructions
(henceforth VPCs). VPCs consist of a head verb
and one or more obligatory particles, in the form
of intransitive prepositions (e.g., hand in), adjec-
tives (e.g., cut short) or verbs (e.g., let go) (Villav-
icencio and Copestake, 2002; Huddleston and Pul-
lum, 2002; Baldwin and Kim, 2009); for the pur-
poses of our dataset, we assume that all particles are
prepositional?by far the most common and produc-
tive of the three types?and further restrict our atten-
tion to single-particle VPCs (i.e., we ignore VPCs
such as get alng together).
One aspect of VPCs that makes them a partic-
ularly challenging target for lexical acquisition is
that the verb and particle can be non-contiguous (for
instance, hand the paper in and battle right on).
This sets them apart from conventional collocations
and terminology (cf., Manning and Schu?tze (1999),
Smadja (1993) and McKeown and Radev (2000))
in that they cannot be captured effectively using n-
grams, due to their variability in the number and type
of words potentially interceding between the verb
and the particle. Also, while conventional colloca-
tions generally take the form of compound nouns
or adjective?noun combinations with relatively sim-
ple syntactic structure, VPCs occur with a range of
valences. Furthermore, VPCs are highly productive
in English and vary in use across domains, making
them a prime target for lexical acquisition (Dehe?,
2002; Baldwin, 2005; Baldwin and Kim, 2009).
In the VPC dataset we use, there is an addi-
tional distinction between compositional and non-
compositional VPCs. With compositional VPCs,
the semantics of the verb and particle both corre-
spond to the semantics of the respective simplex
words, including the possibility of the semantics be-
ing specific to the VPC construction in the case of
particles. For example, battle on would be clas-
sified as compositional, as the semantics of bat-
tle is identical to that for the simplex verb, and
the semantics of on corresponds to the continua-
tive sense of the word as occurs productively in
VPCs (cf., walk/dance/drive/govern/... on). With
non-compositional VPCs, on the other hand, the
semantics of the VPC is somehow removed from
that of the parts. In the dataset we used for eval-
uation, we are interested in extracting exclusively
non-compositional VPCs, as they require lexicalisa-
tion; compositional VPCs can be captured via lexi-
cal rules and are hence not the target of extraction.
English VPCs can occur with a number of va-
lences, with the two most prevalent and productive
valences being the simple transitive (e.g., hand in
the paper) and intransitive (e.g., back off ). For the
purposes of our target task, we focus exclusively on
these two valence types.
Given the above, we define the English VPC ex-
traction task to be the production of triples of the
form ?v, p, s?, where v is a verb lemma, p is a prepo-
sitional particle, and s ? {intrans , trans} is the va-
11
lence; additionally, each triple has to be semantically
non-compositional. The triples are extracted relative
to a set of putative token instances for each of the
intransitive and transitive valences for a given VPC.
That is, a given triple should be classified as positive
if and only if it is associated with at least one non-
compositional token instance in the provided token-
level data.
The dataset used in this research is the one used
in the LREC 2008 Multiword Expression Workshop
Shared Task (Baldwin, 2008).1 In the dataset, there
is a single file for each of 4,090 candidate VPC
triples, containing up to 50 sentences that have the
given VPC taken from the British National Cor-
pus. When the valence of the VPC is ignored,
the dataset contains 440 unique VPCs among 2,898
VPC candidates. In order to be able to fairly com-
pare our method with a state-of-the-art lexicalised
parser trained over the WSJ training sections of the
Penn Treebank, we remove any VPC types from the
test set which are attested in the WSJ training sec-
tions. This removes 696 VPC types from the test
set, and makes the task even more difficult, as the
remaining testing VPC types are generally less fre-
quent ones. At the same time, it unfortunately means
that our results are not directly comparable to those
for the original shared task.2
3 Chart Mining for Parsing with a Large
Precision Grammar
3.1 The Technique
The chart mining technique we use in this paper
is couched in a constituent-based bottom-up chart
parsing paradigm. A parsing chart is a data struc-
ture that records all the (complete or incomplete) in-
termediate parsing results. Every passive edge on
the parsing chart represents a complete local analy-
sis covering a sub-string of the input, while each ac-
tive edge predicts a potential local analysis. In this
view, a full analysis is merely a passive edge that
spans the whole input and satisfies certain root con-
1Downloadable from http://www.csse.unimelb.
edu.au/research/lt/resources/vpc/vpc.tgz.
2In practice, there was only one team who participated in
the original VPC task (Ramisch et al, 2008), who used a vari-
ety of web- and dictionary-based features suited more to high-
frequency instances in high-density languages, so a simplistic
comparison would not have been meaningful.
ditions. The bottom-up chart parser starts with edges
instantiated from lexical entries corresponding to the
input words. The grammar rules are used to incre-
mentally create longer edges from smaller ones until
no more edges can be added to the chart.
Standardly, the parser returns only outputs that
correspond to passive edges in the parsing chart that
span the full input string. For those inputs without a
full-spanning edge, no output is generated, and the
chart becomes the only source of parsing informa-
tion.
A parsing chart takes the form of a hierarchy of
edges. Where only passive edges are concerned,
each non-lexical edge corresponds to exactly one
grammar rule, and is connected with one or more
daughter edge(s), and zero or more parent edge(s).
Therefore, traversing the chart is relatively straight-
forward.
There are two potential challenges for the chart-
mining technique. First, there is potentially a huge
number of parsing edges in the chart. For in-
stance, when parsing with a large precision gram-
mar like the HPSG English Resource Grammar
(ERG, Flickinger (2002)), it is not unusual for a
20-word sentence to receive over 10,000 passive
edges. In order to achieve high efficiency in pars-
ing (as well as generation), ambiguity packing is
usually used to reduce the number of productive
passive edges on the parsing chart (Tomita, 1985).
For constraint-based grammar frameworks like LFG
and HPSG, subsumption-based packing is used to
achieve a higher packing ratio (Oepen and Carroll,
2000), but this might also potentially lead to an in-
consistent packed parse forest that does not unpack
successfully. For chart mining, this means that not
all passive edges are directly accessible from the
chart. Some of them are packed into others, and the
derivatives of the packed edges are not generated.
Because of the ambiguity packing, zero or more
local analyses may exist for each passive edge on
the chart, and the cross-combination of the packed
daughter edges is not guaranteed to be compatible.
As a result, expensive unification operations must be
reapplied during the unpacking phase. Carroll and
Oepen (2005) and Zhang et al (2007b) have pro-
posed efficient k-best unpacking algorithms that can
selectively extract the most probable readings from
the packed parse forest according to a discrimina-
12
tive parse disambiguation model, by minimising the
number of potential unifications. The algorithm can
be applied to unpack any passive edges. Because
of the dynamic programming used in the algorithm
and the hierarchical structure of the edges, the cost
of the unpacking routine is empirically linear in the
number of desired readings, and O(1) when invoked
more than once on the same edge.
The other challenge concerns the selection of in-
formative and representative pieces of knowledge
from the massive sea of partial analyses in the pars-
ing chart. How to effectively extract the indicative
features for a specific language phenomenon is a
very task-specific question, as we will show in the
context of the VPC extraction task in Section 3.2.
However, general strategies can be applied to gener-
ate parse ranking scores on each passive edge. The
most widely used parse ranking model is the log-
linear model (Abney, 1997; Johnson et al, 1999;
Toutanova et al, 2002). When the model does not
use non-local features, the accumulated score on a
sub-tree under a certain (unpacked) passive edge can
be used to approximate the probability of the partial
analysis conditioned on the sub-string within that
span.3
3.2 The Application: Acquiring Features for
VPC Extraction
As stated above, the target task we use to illustrate
the capabilities of our chart mining method is VPC
extraction.
The grammar we apply our chart mining method
to in this paper is the English Resource Grammar
(ERG, Flickinger (2002)), a large-scale precision
HPSG for English. Note, however, that the method
is equally compatible with any grammar or grammar
formalism which is compatible with chart parsing.
The lexicon of the ERG has been semi-
automatically extended with VPCs extracted
by Baldwin (2005). In order to show the effective-
ness of chart mining in discovering ?unknowns?
and remove any lexical probabilities associated
with pre-existing lexical entries, we block the
3To have a consistent ranking model on any sub-analysis,
one would have to retrain the disambiguation model on every
passive edge. In practice, we find this to be intractable. Also,
the approximation based on full-parse ranking model works rea-
sonably well.
lexical entries for the verb in the candidate VPC
by substituting the input token with a DUMMY-V
token, which is coupled with four candidate lexical
entries of type: (1) intransitive simplex verb (v - e),
(2) transitive simplex verb (v np le), (3) intransitive
VPC (v p le), and (4) transitive VPC (v p-np le),
respectively. These four lexical entries represent the
two VPC valences we wish to distinguish between
in the VPC extraction task, and the competing
simplex verb candidates. Based on these lexical
types, the features we extract with chart mining are
summarised in Table 1. The maximal constituent
(MAXCONS) of a lexical entry is defined to be the
passive edge that is an ancestor of the lexical entry
edge that: (i) must span over the particle, and (ii)
has maximal span length. In the case of a tie,
the edge with the highest disambiguation score is
selected as the MAXCONS. If there is no edge found
on the chart that spans over both the verb and the
particle, the MAXCONS is set to be NULL, with a
MAXSPAN of 0, MAXLEVEL of 0 and MAXCRANK
of 4 (see Table 1). The stem of the particle is also
collected as a feature.
One important characteristic of these features is
that they are completely unlexicalised on the verb.
This not only leads to a fair evaluation with the ERG
by excluding the influence from the lexical coverage
of VPCs in the grammar, but it also demonstrates
that complete grammatical coverage over simplex
verbs is not a prerequisite for chart mining.
To illustrate how our method works, we present
the unpacked parsing chart for the candidate VPC
show off and input sentence The boy shows off his
new toys in Figure 1. The non-terminal edges are
marked with their syntactic categories, i.e., HPSG
rules (e.g., subjh for the subject-head-rule, hadj for
the head-adjunct-rule, etc.), and optionally their dis-
ambiguation scores. By traversing upward through
parent edges from the DUMMY-V edge, all features
can be efficiently extracted (see the third column in
Table 1).
It should be noted that none of these features are
used to deterministically dictate the predicted VPC
category. Instead, the acquired features are used as
inputs to a statistical classifier for predicting the type
of the VPC candidate at the token level (in the con-
text of the given sentence). In our experiment, we
used a maximum entropy-based model to do a 3-
13
Feature Description Examples
LE:MAXCONS
A lexical entry together with the maximal constituent
constructed from it
v - le:subjh, v np le:hadj,
v p le:subjh, v p-np le:subj
LE:MAXSPAN
A lexical entry together with the length of the span of
the maximal constituent constructed from the LE
v - le:7, v np le:5, v p le:4,
v p-np le:7
LE:MAXLEVEL
A lexical entry together with the levels of projections
before it reaches its maximal constituent
v - le:2, v np le:1, v p le:2,
v p-np le:3
LE:MAXCRANK
A lexical entry together with the relative disambigua-
tion score ranking of its maximal constituent among
all MaxCons from different LEs
v - le:4, v np le:3, v p le:1,
v p-np le:2
PARTICLE The stem of the particle in the candidate VPC off
Table 1: Chart mining features used for VPC extraction
his new toysoffshows
PREPPRTL
v_?_le
NP1
VP4?hcomp
NP2
VP5?hcomp
PP?hcomp
0 2 3 4 7
DUMMY?V
S1?subjh(.125)
S3?subjh(.875)
VP1?hadj VP3?hcomp
S2?subjh(.925)
VP2?hadj(.325)
v_p?np_lev_np_le v_p_le
the boy
Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG
category classification: non-VPC, transitive VPC,
or intransitive VPC. For the parameter estimation
of the ME model, we use the TADM open source
toolkit (Malouf, 2002). The token-level predictions
are then combined with a simple majority voting to
derive the type-level prediction for the VPC candi-
date. In the case of a tie, the method backs off to
the na??ve baseline model described in Section 4.2,
which relies on the combined probability of the verb
and particle forming a VPC.
We have also experimented with other ways of de-
riving type-level predictions from token-level classi-
fication results. For instance, we trained a separate
classifier that takes the token-level prediction as in-
put in order to determine the type-level VPC predic-
tion. Our results indicate no significant difference
between these methods and the basic majority vot-
ing approach, so we present results exclusively for
this simplistic approach in this paper.
4 Evaluation
4.1 Experiment Setup
To evaluate the proposed chart mining-based VPC
extraction model, we use the dataset from the LREC
2008 Multiword Expression Workshop shared task
(see Section 2). We use this dataset to perform three
distinct DLA tasks, as detailed in Table 2.
The chart mining feature extraction is imple-
mented as an extension to the PET parser (Callmeier,
14
Task Description
GOLD VPC Determine the valence for a verb?preposition combination which is known to occur
as a non-compositional VPC (i.e. known VPC, with unknown valence(s))
FULL Determine whether each verb?preposition combination is a VPC or not, and further
predict its valence(s) (i.e. unknown if VPC, and unknown valence(s))
VPC Determine whether each verb?preposition combination is a VPC or not ignoring va-
lence (i.e. unknown if VPC, and don?t care about valence)
Table 2: Definitions of the three DLA tasks
2001). We use a slightly modified version of the
ERG in our experiments, based on the nov-06 re-
lease. The modifications include 4 newly-added
dummy lexical entries for the verb DUMMY-V and
the corresponding inflectional rules, and a lexical
type prediction model (Zhang and Kordoni, 2006)
trained on the LOGON Treebank (Oepen et al, 2004)
for unknown word handling. The parse disambigua-
tion model we use is also trained on the LOGON
Treebank. Since the parser has no access to any of
the verbs under investigation (due to the DUMMY-
V substitution), those VPC types attested in the
LOGON Treebank do not directly impact on the
model?s performance. The chart mining feature ex-
traction process took over 10 CPU days, and col-
lected a total of 44K events for 4,090 candidate VPC
triples.4 5-fold cross validation is used to train/test
the model. As stated above (Section 2), the VPC
triples attested in the WSJ training sections of the
Penn Treebank are excluded in each testing fold for
comparison with the Charniak parser-based model
(see Section 4.2).
4.2 Baseline and Benchmark
For comparison, we first built a na??ve baseline model
using the combined probabilities of the verb and par-
ticle being part of a VPC. More specifically, P (c|v)
and P (c|p) are the probabilities of a given verb
v and particle p being part of a VPC candidate
of type s ? {intrans , trans , null}, for transitive
4Not all sentences in the dataset are successfully chart-
mined. Due to the complexity of the precision grammar we
use, the parser is unlikely to complete the parsing chart for ex-
tremely long sentences (over 50 words). Moreover, sentences
which do not receive any spanning edge over the verb and the
particle are not considered as an indicative event. Nevertheless,
the coverage of the chart mining is much higher than the full-
parse coverage of the grammar.
VPC, intransitive VPC, and non-VPC, respectively.
P? (s|v, p) = P (s|v) ? P (s|p) is used to approxi-
mate the joint probability of verb-particle (v, p) be-
ing of type s, and the prediction type is chosen ran-
domly based on this probabilistic distribution. Both
P (s|v) and P (s|p) can be estimated from a list of
VPC candidate types. If v is unseen, P (s|v) is set to
be 1|V |
?
vi?V P (s|vi) estimated over all verbs |V |
seen in the list of VPC candidates. The na??ve base-
line performed poorly, mainly because there is not
enough knowledge about the context of use of VPCs.
This also indicates that the task of VPC extraction
is non-trivial, and that context (evidence from sen-
tences in which the VPC putatively occurs) must be
incorporated in order to make more accurate predic-
tions.
As a benchmark VPC extraction system, we use
the Charniak parser (Charniak, 2000). This sta-
tistical parser induces a context-free grammar and
a generative parsing model from a training set of
gold standard parse trees. Traditionally, it has been
trained over the WSJ component of the Penn Tree-
bank, and for this work we decided to take the same
approach and train over sections 1 to 22, and use sec-
tion 23 for parameter-tuning. After parsing, we sim-
ply search for the VPC triples in each token instance
with tgrep2,5 and decide on the classification of
the candidate by majority voting over all instances,
breaking ties randomly.
5Noting that the Penn POS tagset captures essentially the
compositional vs. non-compositional VPC distinction required
in the extraction task, through the use of the RP (prepositional
particle, for non-compositional VPCs) and RB (adverb, for com-
positional VPCs) tags.
15
4.3 Results
The results of our experiments are summarised in
Table 3. For the na??ve baseline and the chart mining-
based models, the results are averaged over 5-fold
cross validation.
We evaluate the methods in the form of the three
tasks described in Table 2. Formally, GOLD VPC
equates to extracting ?v, p, s? tuples from the sub-
set of gold-standard ?v, p? tuples; FULL equates to
extracting ?v, p, s? tuples for all VPC candidates;
and VPC equates to extracting ?v, p? tuples (ignor-
ing valence) over all VPC candidates. In each case,
we present the precision (P), recall (R) and F-score
(? = 1: F). For multi-category classifications (i.e.
the two tasks where we predict the valence s, indi-
cated as ?All? in Table 3), we micro-average the pre-
cision and recall over the two VPC categories, and
calculate the F-score as their harmonic mean.
From the results, it is obvious that the chart
mining-based model performs best overall, and in-
deed for most of the measures presented. The Char-
niak parser-based extraction method performs rea-
sonably well, especially in the VPC+valence extrac-
tion task over the FULL task, where the recall was
higher than the chart mining method. Although
not reported here, we observe a marked improve-
ment in the results for the Charniak parser when
the VPC types attested in the WSJ are not filtered
from the test set. This indicates that the statisti-
cal parser relies heavily on lexicalised VPC infor-
mation, while the chart mining model is much more
syntax-oriented. In error analysis of the data, we ob-
served that the Charniak parser was noticeably more
accurate at extracting VPCs where the verb was fre-
quent (our method, of course, did not have access
to the base frequency of the simplex verb), under-
lining again the power of lexicalisation. This points
to two possibilities: (1) the potential for our method
to similarly benefit from lexicalisation if we were to
remove the constraint on ignoring any pre-existing
lexical entries for the verb; and (2) the possibility
for hybridising between lexicalised models for fre-
quent verbs and unlexicalised models for infrequent
verbs. Having said this, it is important to reinforce
that lexical acquisition is usually performed in the
absence of lexicalised probabilities, as if we have
prior knowledge of the lexical item, there is no need
to extract it. In this sense, the first set of results in
Table 3 over Gold VPCs are the most informative,
and illustrate the potential of the proposed approach.
From the results of all the models, it would ap-
pear that intransitive VPCs are more difficult to ex-
tract than transitive VPCs. This is partly because the
dataset we use is unbalanced: the number of transi-
tive VPC types is about twice the number of intran-
sitive VPCs. Also, the much lower numbers over
the FULL set compared to the GOLD VPC set are due
to the fact that only 1/8 of the candidates are true
VPCs.
5 Discussion and Future Work
The inventory of features we propose for VPC ex-
traction is just one illustration of how partial parse
results can be used in lexical acquisition tasks.
The general chart mining technique can easily be
adapted to learn other challenging linguistic phe-
nomena, such as the countability of nouns (Bald-
win and Bond, 2003), subcategorization properties
of verbs or nouns (Korhonen, 2002), and general
multiword expression (MWE) extraction (Baldwin
and Kim, 2009). With MWE extraction, e.g., even
though some MWEs are fixed and have no internal
syntactic variability, such as ad hoc, there is a very
large proportion of idioms that allow various de-
grees of internal variability, and with a variable num-
ber of elements. For example, the idiom spill the
beans allows internal modification (spill mountains
of beans), passivisation (The beans were spilled in
the latest edition of the report), topicalisation (The
beans, the opposition spilled), and so forth (Sag et
al., 2002). In general, however, the exact degree of
variability of an idiom is difficult to predict (Riehe-
mann, 2001). The chart mining technique we pro-
pose here, which makes use of partial parse results,
may facilitate the automatic recognition task of even
more flexible idioms, based on the encouraging re-
sults for VPCs.
The main advantage, though, of chart mining is
that parsing with precision grammars does not any
longer have to assume complete coverage, as has
traditionally been the case. As an immediate con-
sequence, the possibility of applying our chart min-
ing technique to evolving medium-sized grammars
makes it especially interesting for lexical acquisi-
16
Task VPC Type Na??ve Baseline Charniak Parser Chart-Mining
P R F P R F P R F
GOLD VPC
Intrans-VPC 0.300 0.018 0.034 0.549 0.753 0.635 0.845 0.621 0.716
Trans-VPC 0.676 0.348 0.459 0.829 0.648 0.728 0.877 0.956 0.915
All 0.576 0.236 0.335 0.691 0.686 0.688 0.875 0.859 0.867
FULL
Intrans-VPC 0.060 0.018 0.028 0.102 0.593 0.174 0.153 0.155 0.154
Trans-VPC 0.083 0.348 0.134 0.179 0.448 0.256 0.179 0.362 0.240
All 0.080 0.236 0.119 0.136 0.500 0.213 0.171 0.298 0.218
VPC 0.123 0.348 0.182 0.173 0.782 0.284 0.259 0.332 0.291
Table 3: Results for the different methods over the three VPC extraction tasks detailed in Table 2
tion over low-density languages, for instance, where
there is a real need for rapid-prototyping of language
resources.
The chart mining approach we propose in this
paper is couched in the bottom-up chart parsing
paradigm, based exclusively on passive edges. As
future work, we would also like to look into the
top-level active edges (those active edges that are
never completed), as an indication of failed assump-
tions. Moreover, it would be interesting to investi-
gate the applicability of the technique in other pars-
ing strategies, e.g., head-corner or left-corner pars-
ing. Finally, it would also be interesting to in-
vestigate whether by using the features we acquire
from chart mining enhanced with information on the
prevalence of certain patterns, we could achieve per-
formance improvements over broader-coverage tree-
bank parsers such as the Charniak parser.
6 Conclusion
We have proposed a chart mining technique for lex-
ical acquisition based on partial parsing with preci-
sion grammars. We applied the proposed method
to the task of extracting English verb particle con-
structions from a prescribed set of corpus instances.
Our results showed that simple unlexicalised fea-
tures mined from the chart can be used to effec-
tively extract VPCs, and that the model outperforms
a probabilistic baseline and the Charniak parser at
VPC extraction.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram. The first was supported by the German Excellence
Cluster of Multimodal Computing and Interaction.
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23:597?618.
Timothy Baldwin and Francis Bond. 2003. Learning
the countability of English nouns from corpus data.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2003),
pages 463?470, Sapporo, Japan.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proceedings of the LREC 2008 Work-
shop: Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Morocco.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Univer-
sita?t des Saarlandes, Saarbru?cken, Germany.
John Carroll and Stephan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proceedings of the 2nd International Joint Confer-
ence on Natural LanguageProcessing (IJCNLP 2005),
pages 165?176, Jeju Island, Korea.
Eugene Charniak. 2000. A maximum entropy-based
parser. In Proceedings of the 1st Annual Meeting of
the North American Chapter of Association for Com-
putational Linguistics (NAACL2000), Seattle, USA.
Daniel de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the ACL2009
Workshop on Grammar Engineering Across Frame-
works (GEAF), Singapore.
17
Nicole Dehe?. 2002. Particle Verbs in English: Syn-
tax, Information, Structure and Intonation. John Ben-
jamins, Amsterdam, Netherlands/Philadelphia, USA.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. CSLI Publications.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte Jo?rg, and Ul-
rich Scha?fer. 2006. Question answering from struc-
tured knowledge sources. Journal of Applied Logic,
Special Issue on Questions and Answers: Theoretical
and Applied Perspectives., 5(1):20?48.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic unifcation-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1999), pages 535?541, Mary-
land, USA.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A Bag of Useful Techniques for
Efficient and Robust Parsing. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Maryland, USA.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Andrew MacKinlay, David Martinez, and Timothy Bald-
win. 2009. Biomedical event annotation with CRFs
and precision grammars. In Proceedings of BioNLP
2009: Shared Task, pages 77?85, Boulder, USA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conferencde on Natural Language
Learning (CoNLL 2002), pages 49?55, Taipei, Taiwan.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of Natural Lan-
guage Processing.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing ? practical results. In
Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational
Linguistics (NAACL 2000), pages 162?169, Seattle,
USA.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.
2004. Som a? kapp-ete med trollet? Towards MRS-
Based Norwegian?English Machine Translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, USA.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the
extraction of multiword expressions. In Proceedings
of the LREC 2008 Workshop: Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 50?53,
Marrakech, Morocco.
Susanne Riehemann. 2001. A Constructional Approach
to Idioms and Word Formation. Ph.D. thesis, Stanford
University, CA, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002), pages 1?15, Mexico City, Mexico.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?178.
Masaru Tomita. 1985. An efficient context-free parsing
algorithm for natural languages. In Proceedings of the
9th International Joint Conference on Artificial Intel-
ligence, pages 756?764, Los Angeles, USA.
Kristina Toutanova, Christoper D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse ranking for a rich HPSG grammar. In Proceed-
ings of the 1st Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 253?263, Sozopol, Bul-
garia.
Hans Uszkoreit. 2002. New chances for deep linguis-
tic processing. In Proceedings of the 19th interna-
tional conference on computational linguistics (COL-
ING 2002), Taipei, Taiwan.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics), pages 446?453, Barcelona, Spain.
Aline Villavicencio and Ann Copestake. 2002. Verb-
particle constructions in a computational grammar of
English. In Proceedings of the 9th International Con-
ference on Head-Driven Phrase Structure Grammar
(HPSG-2002), Seoul, Korea.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 275?280, Genoa, Italy.
Yi Zhang and Valia Kordoni. 2008. Robust parsing
with a large HPSG grammar. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007a.
Partial parse selection for robust deep processing. In
Proceedings of ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 128?135, Prague, Czech Repub-
lic.
Yi Zhang, Stephan Oepen, and John Carroll. 2007b. Ef-
ficiency in unification-based N-best parsing. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies (IWPT 2007), pages 48?59, Prague,
Czech.
18
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction
with Latent Structural SVM
Lizhen Qu
Max Planck Institute
for Informatics
lqu@mpi-inf.mpg.de
Yi Zhang
Nuance Communications
yi.zhang@nuance.com
Rui Wang
DFKI GmbH
mars198356@hotmail.com
Lili Jiang
Max Planck Institute
for Informatics
ljiang@mpi-inf.mpg.de
Rainer Gemulla
Max Planck Institute
for Informatics
rgemulla@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute
for Informatics
weikum@mpi-inf.mpg.de
Abstract
Extracting instances of sentiment-oriented re-
lations from user-generated web documents is
important for online marketing analysis. Un-
like previous work, we formulate this extrac-
tion task as a structured prediction problem
and design the corresponding inference as an
integer linear program. Our latent structural
SVM based model can learn from training cor-
pora that do not contain explicit annotations of
sentiment-bearing expressions, and it can si-
multaneously recognize instances of both bi-
nary (polarity) and ternary (comparative) re-
lations with regard to entity mentions of in-
terest. The empirical evaluation shows that
our approach significantly outperforms state-
of-the-art systems across domains (cameras
and movies) and across genres (reviews and
forum posts). The gold standard corpus that
we built will also be a valuable resource for
the community.
1 Introduction
Sentiment-oriented relation extraction (Choi et al.,
2006) is concerned with recognizing sentiment po-
larities and comparative relations between entities
from natural language text. Identifying such rela-
tions often requires syntactic and semantic analysis
at both sentence and phrase level. Most prior work
on sentiment analysis consider either i) subjective
sentence detection (Yu and K?bler, 2011), ii) po-
larity classification (Johansson and Moschitti, 2011;
Wilson et al., 2005), or iii) comparative relation
identification (Jindal and Liu, 2006; Ganapathib-
hotla and Liu, 2008). In practice, however, differ-
ent types of sentiment-oriented relations frequently
coexist in documents. In particular, we found that
more than 38% of the sentences in our test corpus
contain more than one type of relations. The iso-
lated analysis approach is inappropriate because i) it
sacrifices acuracy by ignoring the intricate interplay
among different types of relations; ii) it could lead to
conflicting predictions such as estimating a relation
candidate as both negative and comparative. There-
fore, in this paper, we identify instances of both sen-
timent polarities and comparative relations for enti-
ties of interest simultaneously. We assume that all
the mentions of entities and attributes are given, and
entities are disambiguated. It is a widely used as-
sumption when evaluating a module in a pipeline
system that the outputs of preceding modules are
error-free.
To the best of our knowledge, the only exist-
ing system capable of extracting both comparisons
and sentiment polarities is a rule-based system pro-
posed by Ding et al. (2009). We argue that it is
better to tackle the task by using a unified model
with structured outputs. It allows us to consider a
set of correlated relation instances jointly and char-
acterize their interaction through a set of soft and
hard constraints. For example, we can encode con-
straints to discourage an attribute to participate in
a polarity relation and a comparative relation at the
same time. As a result, the system extracts a set of
correlated instances of sentiment-oriented relations
from a given sentence. For example, with the sen-
tence about the camera Canon 7D, ?The sensor is
great, but the price is higher than Nikon D7000.?
the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155?168. Action Editor: Janyce Wiebe.
Submitted 6/2013; Revised 11/2013; Published 4/2014. c?2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textit-
price).
However, constructing a fully annotated train-
ing corpus for this task is labor-intensive and re-
quires strong linguistic background. We minimize
this overhead by applying a simplified annotation
scheme, in which annotators mark mentions of en-
tities and attributes, disambiguate the entities, and
label instances of relations for each sentence. Based
on the new scheme, we have created a small Senti-
ment Relation Graph (SRG) corpus for the domains
of cameras and movies, which significantly differs
from the corpora used in prior work (Wei and Gulla,
2010; Kessler et al., 2010; Toprak et al., 2010;
Wiebe et al., 2005; Hu and Liu, 2004) in the follow-
ing ways: i) both sentiment polarities and compar-
ative relations are annotated; ii) all mentioned en-
tities are disambiguated; and iii) no subjective ex-
pressions are annotated, unless they are part of entity
mentions.
The new annotation scheme raises a new chal-
lenge for learning algorithms in that they need to
automatically find textual evidences for each anno-
tated relation during training. For example, with the
sentence ?I like the Rebel a little better, but that is
another price jump?, simply assigning a sentiment-
bearing expression to the nearest relation candidate
is insufficient, especially when the sentiment is not
explicitly expressed.
In this paper, we propose SENTI-LSSVM, a latent
structural SVM based model for sentiment-oriented
relation extraction. SENTI-LSSVM is applied to find
the most likely set of the relation instances expressed
in a given sentence, where the latent variables are
used to assign the most appropriate textual evidences
to the respective instances.
In summary, the contributions of this paper are the
following:
? We propose SENTI-LSSVM: the first unified sta-
tistical model with the capability of extracting
instances of both binary and ternary sentiment-
oriented relations.
? We design a task-specific integer linear pro-
gramming (ILP) formulation for inference.
? We construct a new SRG corpus as a valuable
asset for the evaluation of sentiment relation
extraction.
? We conduct extensive experiments with on-
line reviews and forum posts, showing that
SENTI-LSSVM model can effectively learn from
a training corpus without explicitly annotated
subjective expressions and that its performance
significantly outperforms state-of-the-art sys-
tems.
2 Related Work
There are ample works on analyzing sentiment po-
larities and entity comparisons, but the majority of
them studied the two tasks in isolation.
Most prior approaches for fine-grained sentiment
analysis focus on polarity classification. Super-
vised approaches on expression-level analysis re-
quire the annotation of sentiment-bearing expres-
sions as training data (Jin et al., 2009; Choi
and Cardie, 2010; Johansson and Moschitti, 2011;
Yessenalina and Cardie, 2011; Wei and Gulla,
2010). However, the corresponding annotation pro-
cess is time-consuming. Although sentence-level
annotations are easier to obtain, the analysis at this
level cannot cope with sentences conveying relations
of multiple types (McDonald et al., 2007; T?ckstr?m
and McDonald, 2011; Socher et al., 2012). Lexicon-
based approaches require no training data (Ku et al.,
2006; Kim and Hovy, 2006; Godbole et al., 2007;
Ding et al., 2008; Popescu and Etzioni, 2005; Liu et
al., 2005) but suffer from inferior performance (Wil-
son et al., 2005; Qu et al., 2012). In contrast, our
method requires no annotation of sentiment-bearing
expressions for training and can predict both senti-
ment polarities and comparative relations.
Sentiment-oriented comparative relations have
been studied in the context of user-generated dis-
course (Jindal and Liu, 2006; Ganapathibhotla and
Liu, 2008). Approaches rely on linguistically moti-
vated rules and assume the existence of independent
keywords in sentences which indicate comparative
relations. Therefore, these methods fall short of ex-
tracting comparative relations based on domain de-
pendent information.
Both Johansson and Moschitti (2011) and Wu et
al. (2011) formulate fine-grained sentiment analy-
sis as a learning problem with structured outputs.
However, they focus only on polarity classification
156
of expressions and require annotation of sentiment-
bearing expressions for training as well.
While ILP has been previously applied for infer-
ence in sentiment analysis (Choi and Cardie, 2009;
Somasundaran and Wiebe, 2009; Wu et al., 2011),
our task requires a complete ILP reformulation due
to 1) the absence of annotated sentiment expressions
and 2) the constraints imposed by the joint extrac-
tion of both sentiment polarity and comparative re-
lations.
3 System Overview
This section gives an overview of the whole system
for extracting sentiment-oriented relation instances.
Prior to presenting the system architecture, we in-
troduce the essential concepts and the definitions of
two kinds of directed hypergraphs as the represen-
tation of correlated relation instances extracted from
sentences.
3.1 Concepts and Definitions
Entity. An entity is an abstract or concrete thing,
which needs not be of material existence. An entity
in this paper refers to either a product or a brand.
Attribute. An attribute is an object closely associ-
ated with or belonging to an entity, such as the lens
of digital camera.
Sentiment-Oriented Relation. A sentiment-
oriented relation is either a sentiment polarity or a
comparative relation, defined on tuples of entities
and attributes. A sentiment polarity relation conveys
either a positive or a negative attitude towards enti-
ties or their attributes, whereas a comparative rela-
tion indicates the preference of one entity over the
other entity w.r.t. an attribute.
Relation Instance. An instance of sentiment polar-
ity takes the form r(entity, attribute) with r ? {pos-
itive, negative}, such as positive(Canon 7D, sen-
sor). The polarity instances expressed in the form
of unary relations, such as ?Nikon D7000 is ex-
cellent.?, are denoted as binary relations r(entity,
whole), where the attribute whole indicates the en-
tity as a whole. In contrast, an instance of compar-
ative relation is in the form of preferred{entity, en-
tity, attribute}, e.g. preferred(Canon 7D, Nikon
D7000, price). For brevity, we refer to an instance
set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances
of the remaining relations, we represent them as
other{entity, attribute}, such as textitpartOf{wheel,
car}. These relations include objective relations
and the subjective relations other than sentiment-
oriented relations.
Mention-Based Relation Instances. A mention-
based relation instance refers to a tuple of entity
mentions with a certain relation. This concept is in-
troduced as the representation of instances in a sen-
tence by replacing entities with the corresponding
entity mentions, such as positive(?Canon SD880i?,
?wide angle view?).
Figure 1: An example of MRG.
Mention-Based Relation Graph. A mention-based
relation graph (or MRG ) represents a collection of
mention-based relation instances expressed in a sen-
tence. As illustrated in Figure 1, an MRG is a di-
rected hypergraph G = ?M,E? with a vertex set
M and an edge set E. A vertex mi ? M denotes
a mention of an entity or an attribute occurring ei-
ther within the sentence or in its context. We say
that a mention is from the context if it is mentioned
in the previous sentence or is an attribute implied
in the current sentence. An instance of a binary re-
lation in an MRG takes the form of a binary edge
el = (mi,ma), where mi and ma denote an en-
tity mention and an attribute mention respectively,
and the type l ? {positive, negative, other}. A
ternary edge el indicating comparative relation is
represented as el = (mi,mj ,ma), where two en-
tity mentions mi and mj are compared with respect
to the attribute mention ma. We define the type
l ? {better,worse} to indicate two possible direc-
tions of the relation and assume mi occurs before
mj . As a result, we have a set L of five relation
types: positive, negative, better, worse or other. Ac-
cording to these definitions, the annotations in the
SRG corpus are actually MRGs and disambiguated
entities. If there are multiple mentions referring to
the same entity, annotators are asked to choose the
157
most obvious one because it saves annotation time
and is less demanding for the entity recognition and
diambiguation modules.
Figure 2: An example of eMRG. The textual evi-
dences are wrapped by green dashed boxes.
Evidentiary Mention-Based Relation Graph. An
evidentiary mention-based relation graph, coined
eMRG , extends an MRG by associating each edge
with a textual evidence to support the corresponding
relation assertions (see Figure 2). Consequently, an
edge in an eMRG is denoted by a pair (a, c), where
a represents a mention-based relation instance and
c is the associated textual evidence. It is also re-
ferred to as an evidentiary edge. represented as
el = (mi,mj ,ma), an MRG as an evidentiary MRG
(eMRG) and the edges of eMRGs as evidentiary
edges, as shown in Figure 2.
3.2 System Architecture
Figure 3: System architecture.
As illustrated by Figure 3, at the core of our sys-
tem is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs
from sentences. For a given sentence with known
entity mentions, we select all possible mention sets
as relation candidates, where each set includes at
least one entity mention. Then we associate each
relation candidate with a set of constituents or the
whole sentence as the textual evidence candidates
(cf. Section 6.1). Subsequently, the inference com-
ponent aims to find the most likely eMRG from all
possible combinations of mention-based relation in-
stances and their textual evidences (cf. Section 6.2).
The representation eMRG is chosen because it char-
acterizes exactly the model outputs by letting each
edge correspond to an instance of mention-based re-
lation and the associated textual evidence. Finally,
the model parameters of this model are learned by
an online algorithm (cf. Section 7).
Since instance sets of sentiment-oriented relations
(sSoRs) are the expected outputs, we can obtain
sSoRs from MRGs by using a simple rule-based al-
gorithm. The algorithm essentially maps the men-
tions from an MRG into entities and attributes in an
sSoR and label the corresponding tuples with the re-
lation types of the edges from an MRG. For instances
of comparative relation, the label better or worse is
mapped to the relation type preferred.
4 SENTI-LSSVM Model
The task of sentiment-oriented relation extraction
is to determine the most likely sSoR in a sentence.
Since sSoRs are derived from the corresponding
MRGs as described in Section 3, the task is reduced
to find the most likely MRG for each sentence. Since
an MRG is created by assigning relation types to a
subset of all relation candidates, which are possible
tuples of mentions with unknown relation types, the
number of MRGs can be extremely high.
To tackle the task, one solution is to employ
an edge-factored linear model in the framework of
structural SVM (Martins et al., 2009; Tsochantaridis
et al., 2004). The model suggests that a bag of fea-
tures should be specified for each relation candidate,
and then the model predicts the most likely candi-
date sets along with their relation types to form the
optimal MRGs. As we observed, for a relation can-
didate, the most informative features are the words
near its entity mentions in the original text. How-
158
ever, if we represent a candidate by all these words,
it is very likely that the instances of different relation
types share overly similar features, because a men-
tion is often involved in more than one relation can-
didate, as shown in Figure 2. As a consequence, the
instances of different relations represented by overly
similar features can easily confuse the learning algo-
rithm. Thus, it is critical to select proper constituents
or sentences as textual evidences for each relation
candidate in both training and testing.
Consequently, we divide the task of sentiment-
oriented relation extraction into two subtasks : i)
identifying the most likely MRGs; ii) assigning
proper textual evidences to each edge of MRGs to
support their relation assertions. It is desirable to
carry out the two subtasks jointly as these two sub-
tasks could enhance each other. First, the identifi-
cation of relation types requires proper textual ev-
idences; second, the soft and hard constraints im-
posed by the correlated relation instances facilitate
the recognition of the corresponding textual evi-
dences. Since the eMRGs are created by attaching
every MRG with a set of textual evidences, tackling
the two subtasks simultaneously is equivalent to se-
lecting the most likely eMRG from a set of eMRG
candidates. It is challenging because our SRG corpus
does not contain any annotation of textual evidences.
Formally, let X denote the set of all available sen-
tences, and we define y ? Y(x)(x ? X ) as the set
of labeled edges of an MRG and Y = ?x?XY(x).
Since the assignments of textual evidences are not
observed, an assignment of evidences to y is de-
noted by a latent variable h ? H(x) and H =
?x?XH(x). Then (y, h) corresponds to an eMRG,
and (a, c) ? (y, h) is a labeled edge a attached
with a textual evidence c. Given a labeled dataset
D = {(x1, y1), ..., (xn, yn)} ? (X ? Y)n, we aim
to learn a discriminant function f : X ? Y?H that
outputs the optimal eMRG (y, h) ? Y(x)?H(x) for
a given sentence x.
Due to the introduction of latent variables, we
adopt the latent structural SVM (Yu and Joachims,
2009) for structural classification. Our discriminant
function is defined as
f(x) = argmax(y,h)?Y(x)?H(x)?>?(x, y, h) (1)
where ?(x, y, h) is the feature function of an eMRG
(y, h) and ? is the corresponding weight vector.
To ensure tractability, we also employ edge-based
factorization for our model. Let Mp denote a set of
entity mentions and yr(mi) be a set of edges labeled
with sentiment-oriented relations incident to mi, the
factorization of ?(x, y, h) is given as
?(x, y, h) =
?
(a,c)?(y,h)
?e(x, a, c) + (2)
?
mi?Mp
?
a,a??yr(mi),a 6=a?
?c(a, a?)
where ?e(x, a, c) is a local edge feature function
for a labeled edge a attached with a textual evidence
c and ?c(a, a?) is a feature function capturing co-
occurrence of two labeled edges ami and a?mi inci-dent to an entity mention mi.
5 Feature Space
The following features are used in the feature func-
tions (Equation 2):
Unigrams: As mentioned before, a textual evi-
dence attached to an edge in MRG is either a word,
phrase or sentence. We consider all lemmatized un-
igrams in the textual evidence as unigram features.
Context: Since web users usually express related
sentiments about the same entity across sentence
boundaries, we describe the sentiment flow using a
set of contextual binary features. For example, if en-
tity A is mentioned in both the previous sentence and
the current sentence, a set of contextual binary fea-
tures are used to indicate all possible combinations
of the current and the previous mentioned sentiment-
oriented relations regarding to entity A.
Co-occurrence: We have mentioned the co-
occurrence feature in Equation 2, indicated by
?c(a, a?). It captures the co-occurrence of two la-
beled edges incident to the same entity mention.
Note that the co-occurrence feature function is con-
sidered only if there is a contrast conjunction such as
?but? between the non-shared entity mentions inci-
dent to the two labeled edges.
Senti-predictors: Following the idea of (Qu et
al., 2012), we encode the prediction results from
the rule-based phrase-level multi-relation predic-
tor (Ding et al., 2009) and from the bag-of-opinions
predictor (Qu et al., 2010) as features based on the
textual evidence. The output of the first predictor
is an integer value, while the output of the second
predictor is a sentiment relation, such as ?positive?,
159
?negative?, ?better? or ?worse?. We map the rela-
tional outputs into integer values and then encode
the outputs from both predictors as senti-predictor
features.
Others: The commonly used part-of-speech tags
are also included as features. Moreover, for an edge
candidate, a set of binary features are used to denote
the types of the edge and its entity mentions. For in-
stance, a binary feature indicates whether an edge is
a binary edge related to an entity mentioned in con-
text. To characterize the syntactic dependencies be-
tween two adjacent entity mentions, we use the path
in the dependency tree between the heads of the cor-
responding constituents, the number of words and
other mentions in-between as features. Additionally,
if the textual evidence is a constituent, its feature
w.r.t. an edge is the dependency path to the clos-
est mention of the edge that does not overlap with
this constituent.
6 Structural Inference
In order to find the best eMRG for a given sentence
with a well trained model, we need to determine
the most likely relation type for each relation candi-
date and support the corresponding assertions with
proper textual evidences. We formulate this task
as an Integer Linear Programming (ILP). Instead of
considering all constituents of a sentence, we empir-
ically select a subset as textual evidences for each
relation candidate.
6.1 Textual Evidence Candidates Selection
Textual evidences are selected based on the con-
stituent trees of sentences parsed by the Stanford
parser (Klein and Manning, 2003). For each men-
tion in a sentence, we first locate a constituent in
the tree with the maximal overlap by Jaccard sim-
ilarity. Starting from this constituent, we consider
two types of candidates: type I candidates are con-
stituents at the highest level which contain neither
any word of another mention nor any contrast con-
junctions such as ?but?; type II candidates are con-
stituents at the highest level which cover exactly two
mentions of an edge and do not overlap with any
other mentions. For a binary edge connecting an en-
tity mention and an attribute mention, we consider
a type I candidate starting from the attribute men-
tion. For a binary edge connecting two entity men-
tions, we consider type I candidates starting from
both mentions. Moreover, for a comparative ternary
edge, we consider both type I and type II candidates
starting from the attribute mention. This strategy is
based on our observation that these candidates of-
ten cover the most important information w.r.t. the
covered entity mentions.
6.2 ILP Formulation
We formulate the inference problem of finding the
best eMRG as an ILP problem due to its convenient
integration of both soft and hard constraints.
Given the model parameters ?, we reformulate
the score of an eMRG in the discriminant function
(1) as follows,
?>?(x, y, h) =
?
(a,c)?(y,h)
saczac +
?
mi?Mp
?
a,a??yr(mi),a 6=a?
saa?zaa?
where sac = ?>?e(x, a, c) denotes the score of a
labeled edge a attached with a textual evidence c,
saa? = ?>?c(a, a?) is the edge co-occurrence score,
the binary variable zac indicates the presence or ab-
sence of the corresponding edge, and zaa? indicates
if two edges co-occurr. As not every edge set can
form an eMRG, we require that a valid eMRG should
satisfy a set of linear constraints, which form our
constraint space. Then function (1) is equivalent to
max
z?B s
>z + ?zd
s.t. A
?
?
z
?
?
?
? ? d
z,?, ? ? B
where B = 2S with S = {0, 1}, and ? and ? are
auxiliary binary variables that help define the con-
straint space. The above optimization problem takes
exactly the form of an ILP because both the con-
straints and the objective function are linear, and all
variables take only integer values.
In the following, we consider two types of con-
straint space, 1) an eMRG with only binary edges and
2) an eMRG with both binary and ternary edges.
160
eMRG with only Binary Edges: An eMRG has
only binary edges if a sentence contains no attribute
mention or at most one entity mention. We expect
that each edge has only one relation type and is sup-
ported by a single textual evidence. To facilitate the
formulation of constraints, we introduce ?el to de-
note the presence or absence of a labeled edge el,
and ?ec to indicate if a textual evidence c is assigned
to an unlabeled edge e. Then the binary variable for
the corresponding evidentiary edge zelc = ?ec ? ?el ,
where the ILP formulation of conjunction can be
found in (Martins et al., 2009).
Let Ce denote the set of textual evidence candi-
dates of an unlabeled edge e. The constraint of at
most one textual evidence per edge is formulated as:
?
c?Ce
?ec ? 1 (3)
Once a textual evidence is assigned to an edge,
their relation labels should match and the number
of labeled edges must agree with the number of at-
tached textual evidences. Further, we assume that a
textual evidence c conveys at most one relation so
that an evidence will not be assigned to the relations
of different types, which is the main problem for the
structural SVM based model. Let ?cl indicate that
the textual evidence c is labeled by the relation type
l. The corresponding constraints are expressed as,
?
l?Le
?el =
?
c?Ce
?ec; zelc ? ?cl;
?
l?L
?cl ? 1
where Le denotes the set of all possible labels for
an unlabeled edge e, and L is the set of all relation
types of MRGs (cf. Section 3).
In order to avoid a textual evidence being overly
reused by multiple relation candidates, we first pe-
nalize the assignment of a textual evidence c to a
labeled edge a by associating the corresponding zac
with a fixed negative cost ?? in the objective func-
tion. Then the selection of one textual evidence per
edge a is encouraged by associating ? to zdc in the
objective function, where zdc =
?
e?Sc ?ec and Sc isthe set of edges that the textual evidence c serves as
a candidate. The disjunction zdc is expressed as:
zdc ? ?e, e ? Sc
zdc ?
?
e?Sc
?e
(a) Binary edge structure
(b) Ternary edge structure
Figure 4: Alternative structures associated with an
attribute mention.
This soft constraint not only encourages one textual
evidence per edge, but also keeps it eligible for mul-
tiple assignments.
For any two labeled edge a and a? incident
to the same entity mention, the edge-to-edge co-
occurrence is described by zca,a? = za ? za? .
eMRG with both Binary and Ternary Edges: If
there are more than one entity mentions and at least
one attribute mention in a sentence, an eMRG can
potentially have both binary and ternary edges. In
this case, we assume that each mention of attributes
can participate either in binary relations or in ternary
relations. The assumption holds in more than 99.9%
of the sentences in our SRG corpus, thus we describe
it as a set of hard constraints. Geometrically, the as-
sumption can be visualized as the selection between
two alternative structures incident to the same at-
tribute mention, as shown in Figure 4. Note that,
in the binary edge structure, we include not only the
edges incident to the attribute mention but also the
edge between the two entity mentions.
Let Sbmi be the set of all possible labeled edgesin a binary edge structure of an attribute mention
mi. Variable ? bmi =
?
el?Sbmi
?el indicates whether
the attribute mention is associated with a binary
edge structure or not. In the same manner, we use
? tmi =
?
el?Stmi
?el to indicate the association of the
an attribute mention mi with an ternary edge struc-
ture from the set of all incident ternary edges Stmi .The selection between two alternative structures is
161
formulated as ? bmi + ? tmi = 1. As this influencesonly the edges incident to an attribute mention, we
keep all the constraints introduced in the previous
section unchanged except for constraint (3), which
is modified as
?
c?Ce
?ec ? ? bmi ;
?
c?Ce
?ec ? ? tmi
Therefore, we can have either binary edges or
ternary edges for an attribute mention.
7 Learning Model Parameters
Given a set of training sentences D =
{(x1, y1), . . . , (xn, yn)}, the best weight vec-
tor ? of the discriminant function (1) is found by
solving the following optimization problem:
min
?
1
n
n?
i=1
[ max
(y?,h?)?Y(x)?H(x)
(?>?(x, y?, h?)+?(h?, y?, y))
? max
h??H(x)
?>?(x, y, h?)] + ?|?|] (4)
where ?(h?, y?, y) is a loss function measuring the dis-
crepancies between an eMRG (y, h?) with gold stan-
dard edge labels y and an eMRG (y?, h?) with inferred
labeled edges y? and textual evidences h?. Due to the
sparse nature of the lexical features, we apply L1
regularizer to the weight vector ?, and the degree of
sparsity is controlled by the hyperparameter ?.
Since the L1 norm in the above optimization
problem is not differentiable at zero, we apply the
online forward-backward splitting (FOBOS) algo-
rithm (Duchi and Singer, 2009). It requires two steps
for updating the weight vector ? by using a single
training sentence x on each iteration t.
?t+ 12 = ?t ? ?t?t
?t+1 = arg min
?
1
2?? ? ?t?
2 + ?t?|?|
where ?t is the subgradient computed without con-
sidering the L1 norm and ?t is the learning rate.
For a labeled sentence x, ?t = ?(x, y??, h??) ?
?(x, y, h??), where the feature functions of the corre-
sponding eMRGs are inferred by solving (y??, h??) =
arg max(h?,y?)?H(x)?Y(x)[?
>?(x, y?, h?) + ?(h?, y?, y)]
and (y, h??) = arg maxh??H(x) ?>?(x, y, h?), as in-
dicated in the optimization problem (4).
The former inference problem is similar to the
one we considered in the previous section except
the inclusion of the loss function. We incorporate
the loss function into the ILP formulation by defin-
ing the loss between an MRG (y, h) and a gold stan-
dard MRG as the sum of per-edge costs. In our ex-
periments, we consider a positive cost ? for each
wrongly labeled edge a, so that if an edge a has a
different label from the gold standard, we add ? to
the coefficient sac of the corresponding variable zac
in the objective function of the ILP formulation.
In addition, since the non-positive weights of edge
labels in the initial learning phrase often lead to
eMRGs with many unlabeled edges, which harms the
learning performance, we fix it by adding a con-
straint for the minimal number of labeled edges in
an eMRG, ?
a?A
?
c?Ca
?ac ? ? (5)
where A is the set of all labeled edge candidates and
? denotes the minimal number of labeled edges.
Empirically, the best way to determine ? is to
make it equal to the maximal number of labeled
edges in an eMRG with the restriction that a tex-
tual evidence can be assigned to at most one edge.
By considering all the edge candidates A and all the
textual evidence candidates C as two vertex sets in a
bipartite graph G? = ?V = (A,C), E? (with edges in
E indicating which textual evidence can be assigned
to which edge), ? corresponds to exactly the size of
a maximum matching of the bipartite graph1.
To find the optimal eMRG (y, h??), for the gold la-
bel k of each edge, we consider the following set of
constraints for inference since the labels of the edges
are known for the training data,
?
c?Ce
?ec ? 1; ?ec ? lck
?
k??L
lck? ? 1;
?
e?Sc
?ec ? 1
We include also the soft constraints, which avoid
a textual evidence being overly reused by multiple
relations, and the constraints similar to (5) to ensure
a minimal number of labeled edges and a minimal
number of sentiment-oriented relations.
1It is computed by the Hopcroft-Karp algorithm (Hopcroft
and Karp, 1973) in our implementation.
162
8 SRG Corpus
For evaluation we constructed the SRG corpus,
which in total consists of 1686 manually annotated
online reviews and forum posts in the digital camera
and movie domains2. For each domain, we maintain
a set of attributes and a list of entity names.
The annotation scheme for the sentiment repre-
sentation asserts minimal linguistic knowledge from
our annotators. By focusing on the meanings of the
sentences, the annotators make decisions based on
their language intuition, not restricted by specific
syntactic structures. Taking the example in Figure
2, the annotators only need to mark the mentions of
entities and attributes from both the sentences and
the context, disambiguate them, and label (?Canon
7D?, ?Nikon D7000?, price) as worse and (?Canon
7D?, ?sensor?) as positive, whereas in prior work,
people have annotated the sentiment-bearing expres-
sions such as ?great? and link them to the respective
relation instances as well. This also enables them
to annotate instances of both sentiment polarity and
comparative relaton, which are conveyed by not only
explicit sentiment-bearing expressions like ?excel-
lent performance?, but also factual expressions im-
plying evaluations such as ?The 7V has 10x optical
zoom and the 9V has 16x.?.
Camera Movie
Reviews Forums Reviews Forums
positive 386 1539 879 905
negative 165 363 529 331
comparison 30 480 39 35
Table 1: Distribution of relation instances in SRG corpus.
14 annotators participated in the annotation
project. After a short training period, annotators
worked on randomly assigned documents one at a
time. For product reviews, the system lists all rel-
evant information about the entity and the prede-
fined attributes. For forum posts, the system shows
only the attribute list. For each sentence in a doc-
ument, the annotator first determines if it refers to
an entity of interest. If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Ama-
zon.com; the 667 camera forum posts are downloaded from fo-
rum.digitalcamerareview.com; the 138 movie reviews and 774
forum posts are from imdb.com and boards.ie respectively
as off-topic. Otherwise, the annotator will identify
the most obvious mentions, disambiguate them, and
mark the MRGs. We evaluate the inter-annotator
agreement on sSoRs in terms of Cohen?s Kappa
(?) (Cohen, 1968). An average Kappa value of 0.698
was achieved on a randomly selected set consisting
of 412 sentences.
Table 1 shows the corpus distribution after nor-
malizing them into sSoRs. Camera forum posts con-
tain the largest proportion of comparisons because
they are mainly about the recommendation of dig-
ital cameras. In contrast, web users are much less
interested in comparing movies, in both reviews and
forums. In all subsets, positive relations play a dom-
inant role since web users intend to express more
positive attitudes online than negative ones (Pang
and Lee, 2007).
9 Experiments
This section describes the empirical evaluation of
SENTI-LSSVM together with two competitive base-
lines on the SRG corpus.
9.1 Experimental Setup
We implemented a rule-based baseline (DING-
RULE) and a structural SVM (Tsochantaridis et
al., 2004) baseline (SENTI-SSVM) for comparison.
The former system extends the work of Ding et
al. (2009), which designed several linguistically-
motivated rules based on a sentiment polarity lexi-
con for relation identification and assumes there is
only one type of sentiment relation in a sentence. In
our implementation, we keep all the rules of (Ding et
al., 2009) and add one phrase-level rule when there
are more than one mention in a sentence. The ad-
ditional rule assigns sentiment-bearing words and
negators to its nearest relation candidates based on
the absolute surface distance between the words and
the corresponding mentions. In this case, the phrase-
level sentiment-oriented relations depend only on
the assigned sentiment words and negators. The lat-
ter system is based on a structural SVM and does
not consider the assignment of textual evidences to
relation instances during inference. The textual fea-
tures of a relation candidate are all lexical and sen-
timent predictor features within a surface distance
of four words from the mentions of the candidate.
163
Thus, this baseline does not need the inference con-
straints of SENTI-LSSVM for the selection of textual
evidences. To gain more insights into the model,
we also evaluate the contribution of individual fea-
tures of SENTI-LSSVM. In addition, to show if identi-
fying sentiment polarities and comparative relations
jointly works better than tackling each task on its
own, we train SENTI-LSSVM for each task separately
and combine their predictions according to compat-
ibility rules and the corresponding graph scores.
For each domain and text genre, we withheld 15%
documents for development and use the remaining
for cross validation. The hyperparameters of all sys-
tems are tuned on the development datasets. For all
experiments of SENTI-LSSVM, we use ? = 0.0001
for the L1 regularizer in Eq.(4) and ? = 0.05 for
the loss function; and for SENTI-SSVM, ? = 0.0001
and ? = 0.01. Since the relation type of off-topic
sentences is certainly other, we evaluate all systems
with 5-fold cross-validation only on the on-topic
sentences in the evaluation dataset. Since the same
sSoR can have several equivalent MRGs and the rela-
tion type other is not of our interest, we evaluate the
sSoRs in terms of precision, recall and F-measure.
All reported numbers are averages over the 5 folds.
9.2 Results
Table 2 shows the complete results of all sys-
tems. Here our model SENTI-LSSVM outperformed
all baselines in terms of the average F-measure
scores and recalls by a large margin. The F-measure
on movie reviews is about 14% over the best base-
line. The rule-based system has higher precision
than recall in most cases. However, simply increas-
ing the coverage of the domain independent senti-
ment polarity lexicon might lead to worse perfor-
mance (Taboada et al., 2011) because many sen-
timent oriented relations are conveyed by domain
dependent expressions and factual expressions im-
plying evaluations, such as ?This camera does not
have manual control.? Compared to DING-RULE,
SENTI-SSVM performs better in the camera domain
but worse for the movies due to many misclassi-
fication of negative relation instances as other. It
also wrongly predicted more positive instances as
other than SENTI-LSSVM. We found that the recalls
of these instances are low because they often have
overly similar features with the instances of the type
other linking to the same mentions. The problem
gets worse in the movie domain since i) many sen-
tences contain no explicit sentiment-bearing words;
ii) the prior polarity of the sentiment-bearing words
do not agree with their contextual polarity in the
sentences. Consider the following example from a
forum post about the movie ?Superman Returns?:
?Have a look at Superman: the Animated Series or
Justice League Unlimited . . . that is how the char-
acters of Superman and Lex Luthor should be.?. In
contrast, our model minimizes the overlapping fea-
tures by assigning them to the most likely relation
candidates. This leads to significantly better per-
formance. Although SENTI-SSVM has low recall for
both positive and negative relations, it achieves the
highest recall for the comparative relation among all
systems in the movie domain and camera reviews.
Since less than 1% of all instances are for compara-
tive relations in these document sets and all models
are trained to optimize the overall accuracy, SENTI-
LSSVM intends to trade off the minority class for the
overall better performance. This advantage disap-
pears on the camera forum posts, where the number
of instances of comparative relation is 12 times more
than that in the other data sets.
All systems perform better in predicting positive
relations than the negative ones. This corresponds
well to the empirical findings in (Wilson, 2008) that
people intend to use more complex expressions for
negative sentiments than their affirmative counter-
parts. It is also in accordance with the distribution of
these relations in our SRG corpus which is randomly
sampled from the online documents. For learning
systems, it can also be explained by the fact that the
training data for positive relations are considerably
more than those for negative ones. The comparative
relation is the hardest one to process since we found
that many corresponding expressions do not contain
explicit keywords for comparison.
To understand the performance of the key fea-
ture groups in our model better, we remove each
group from the full SENTI-LSSVM system and eval-
uate the variations with movie reviews and camera
forum posts, which have relatively balanced distri-
bution of relation types. As shown in Table 3, the
features from the sentiment predictors make signif-
icant contributions for both datasets. The differ-
ent drops of the performance indicate that the po-
164
Positive Negative Comparison Micro-average
P R F P R F P R F P R F
Ca
me
ra
Fo
rum
DING-RULE 56.4 39.0 46.1 46.2 24.0 31.6 42.6 14.0 21.0 53.4 30.8 39.0
SENTI-SSVM 60.2 35.6 44.8 44.2 38.5 41.2 28.0 40.1 32.9 43.7 36.7 39.9
SENTI-LSSVM 69.2 38.9 49.8 50.8 39.3 44.3 42.6 35.1 38.5 56.5 38.0 45.4
Ca
me
ra
Re
-
vie
w DING-RULE 83.6 69.0 75.6 68.6 38.8 49.6 30.0 16.9 21.6 81.1 58.6 68.1SENTI-SSVM 72.6 75.4 74.0 63.9 62.5 63.2 28.0 38.9 32.5 68.1 70.4 69.3
SENTI-LSSVM 77.3 85.4 81.2 68.9 61.3 64.9 22.3 20.7 21.6 73.1 73.4 73.7
Mo
vie
Fo
rum
DING-RULE 63.7 37.4 47.1 27.6 34.3 30.6 8.9 5.6 6.8 48.2 35.9 41.2
SENTI-SSVM 66.2 30.1 41.3 25.6 17.3 20.7 44.2 56.7 49.7 53.3 27.9 36.6
SENTI-LSSVM 63.3 44.2 52.1 29.7 45.6 36.0 40.1 45.0 42.4 49.7 44.6 47.0
Mo
vie Re
-
vie
w DING-RULE 66.5 47.2 55.2 42.0 39.1 40.5 31.4 12.0 17.4 56.2 44.0 49.4SENTI-SSVM 61.3 54.0 57.4 45.2 13.7 21.1 24.5 63.3 35.3 54.6 39.2 45.7
SENTI-LSSVM 59.0 79.1 67.6 53.3 51.4 52.3 28.3 34.0 30.9 57.9 68.8 62.9
Table 2: Evaluation results for DING-RULE, SENTI-SSVM and SENTI-LSSVM. Boldface figures are statistically
significantly better than all others in the same comparison group under t-test with p = 0.05.
Feature Models Movie Reviews Camera Forums
full system 62.9 45.4
?unigram 63.2 (+0.3) 41.2 (-4.2)
?context 54.5 (-8.4) 46.0 (+0.6)
?co-occurrence 62.6 (-0.3) 44.9 (-0.5)
?senti-predictors 61.3 (-1.6) 34.3 (-11.1)
Table 3: Micro-average F-measure of SENTI-LSSVM
with different feature models
larities predicted by rules are more consistent in
camera forum posts than in movie reviews. Due
to the complexity of expressions in the movie re-
views our model cannot benefit from the unigram
features but these features are a good compensation
for the sentiment predictor features in camera fo-
rum posts. The sharp drop by removing the context
features from our model on movie reviews indicates
that the sentiments in movie reviews depend highly
on the relations of the previous sentences. In con-
trast, the sentiment-oriented relations of the previ-
ous sentences could be a reason of overfitting for
camera forum data. The edge co-occurrence fea-
tures do not play an important role in our model
since the number of co-occurred sentiment-oriented
relations in the sentences with contrast conjunctions
like ?but? is small. However, we found that allow-
ing the co-occurrence of any sentiment-oriented re-
lations would harm the performance of the model.
In addition, our experiments showed that the sep-
arated approach, which trains a model for senti-
ment polarities and comparative relations respec-
tively, leads to a decrease by almost 1% in terms of
the F-measure averaged over all four datasets. The
largest drop of F-measure is 3% on camera forum
posts, since this dataset contains the largest propor-
tion of comparative relations. We found that the er-
rors are increased when the trained models make
conflicting predictions. In this case, the joint ap-
proach can take all factors into account and make
more consistent decisions than the separated ap-
proaches.
10 Conclusion
We proposed SENTI-LSSVM model for extracting in-
stances of both sentiment polarities and comparative
relations. For evaluating and training the model, we
created an SRG corpus by using a lightweight an-
notation scheme. We showed that our model can
automatically find textual evidences to support its
relation predictions and achieves significantly bet-
ter F-measure scores than alternative state-of-the-art
methods.
References
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
165
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 590?598, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the Annual meeting of
the Association for Computational Linguistics, pages
269?274. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 431?
439, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or Par-
tial Credit. Psychological bulletin, 70(4):213.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining, pages 231?240, New
York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Lei Zhang. 2009. Entity
discovery and assignment for opinion mining applica-
tions. In Proceedings of the ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining, pages
1125?1134.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
The Journal of Machine Learning Research, 10:2899?
2934.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, pages 241?248, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs (system demonstration). In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media.
John E Hopcroft and Richard M Karp. 1973. An n?5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on computing, 2(4):225?231.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Proceedings of the
ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 168?177, New York, NY,
USA. ACM.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: a novel machine learning system for
web opinion mining and extraction. In Proceedings
of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1195?
1204, New York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In Proceedings of the 21st In-
ternational Conference on Artificial Intelligence - Vol-
ume 2, AAAI?06, pages 1331?1336. AAAI Press.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities?
exploration of pipelines and joint models. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics, volume 11, pages 101?106.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sent-
ment corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, SST ?06, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006.
Opinion extraction, summarization and tracking in
news and blog corpora. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 100?107.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351, New
York, NY, USA. ACM.
Andr? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Annual meeting of the Association for Computational
Linguistics, pages 342?350.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
166
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In Chu-Ren
Huang and Dan Jurafsky, editors, Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), ACL Anthology, pages 913?
921, Beijing, China. Tsinghua University Press.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum. 2012.
A weakly supervised model for sentence-level seman-
tic orientation analysis with multiple experts. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 149?159,
Jeju Island, Korea, July. Proceedings of the Annual
meeting of the Association for Computational Linguis-
tics.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1201?1211.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, pages 226?234.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar T?ckstr?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the 33rd
European conference on Advances in information re-
trieval, ECIR?11, pages 368?374, Berlin, Heidelberg.
Springer-Verlag.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 575?584,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the International
Conference on Machine Learning, pages 104?112.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the Annual meeting of the Association
for Computational Linguistics, pages 404?413.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained subjectivity
and sentiment analysis: recognizing the intensity, po-
larity, and attitudes of private states. Ph.D. thesis,
UNIVERSITY OF PITTSBURGH.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2011. Structural opinion mining for graph-based sen-
timent representation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1332?1341.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of the International Conference on Machine
Learning, page 147.
Ning Yu and Sandra K?bler. 2011. Filling the gap:
Semi-supervised learning for opinion detection across
domains. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning, pages
200?209. Association for Computational Linguistics.
167
168
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 272?275,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
MARS: A Specialized RTE System for Parser Evaluation
Rui Wang
?
Yi Zhang
??
? Department of Computational Linguistics, Saarland University
? LT-Lab, German Research Center for Artificial Intelligence
Im Stadtwald, 66123 Saarbr?ucken, Germany
{rwang,yzhang}@coli.uni-sb.de
Abstract
This paper describes our participation
in the the SemEval-2010 Task #12,
Parser Evaluation using Textual Entail-
ment. Our system incorporated two depen-
dency parsers, one semantic role labeler,
and a deep parser based on hand-crafted
grammars. The shortest path algorithm
is applied on the graph representation of
the parser outputs. Then, different types
of features are extracted and the entail-
ment recognition is casted into a machine-
learning-based classification task. The
best setting of the system achieves 66.78%
of accuracy, which ranks the 3rd place.
1 Introduction
The SemEval-2010 Task #12, Parser Evaluation
using Textual Entailment (PETE) (Yuret et al,
2010), is an interesting task connecting two areas
of research, parsing and recognizing textual entail-
ment (RTE) (Dagan et al, 2005). The former is
usually concerned with syntactic analysis in spe-
cific linguistic frameworks, while the latter is be-
lieved to involve more semantic aspects of the hu-
man languages. However, no clear-cut boundary
can be drawn between syntax and semantics for
both tasks. In recent years, the parsing commu-
nity has been reaching beyond what was usually
accepted as syntactic structures. Many deep lin-
guistic frameworks allow the construction of se-
mantic representations in parallel to the syntactic
structure. Meanwhile, data-driven shallow seman-
tic parsers (or semantic role labelers) are another
popular type of extension to enrich the information
in the parser outputs.
Although entailment is described as a semantic
relation, RTE, in practice, covers linguistic phe-
nomena at various levels, from surface text to the
meaning, even to the context and discourse. One
proposal of solving the problem is to deal with dif-
ferent cases of entailment using different special-
ized RTE modules (Wang and Neumann, 2009).
Then, the PETE data can be naturally classified
into the syntactic and shallow semantic categories.
By participating in this shared task, we aim to
investigate whether different parsing outputs leads
to different RTE accuracy, and on the contrary,
whether the ?application?-based evaluation pro-
vides insights to the parser comparison. Further,
we investigate if strict grammaticality checking
with a linguistic grammar is helpful in this task.
2 System Description
The workflow of the system is shown in Figure
1 and the details of the three components will be
elaborated on in the following sections.
2.1 Preprocessing
In this paper, we generally refer all the linguistic
analyses on the text as preprocessing. The output
of this procedure is a graph representation, which
approximates the meaning of the input text. In par-
ticular, after tokenization and POS tagging, we did
dependency parsing and semantic role labeling. In
addition, HPSG parsing is a filter for ungrammat-
ical hypotheses.
Tokenization and POS Tagging We use the
Penn Treebank style tokenization throughout the
various processing stages. TnT, an HMM-based
POS tagger trained with Wall Street Journal sec-
tions of the PTB, was used to automatically pre-
dict the part-of-speech of each token in the texts
and hypotheses.
Dependency Parsing For obtaining the syntac-
tic dependencies, we use two dependency parsers,
MSTParser (McDonald et al, 2005) and Malt-
Parser (Nivre et al, 2007). MSTParser is a graph-
based dependency parser where the best parse
tree is acquired by searching for a spanning tree
272
Dependency Path
Extraction
Feature-based ClassificationPreprocessing
HPSG
Parsing
Dependency
Parsing
Semantic 
Role 
Labeling
T
H
Dependency
Triple
Extraction
Path 
Extraction
Feature 
Extraction
SVM-based
Classification
Yes/No
No
Figure 1: Workflow of the System
which maximize the score on an either partially or
fully connected dependency graph. MaltParser is
a transition-based incremental dependency parser,
which is language-independent and data-driven. It
contains a deterministic algorithm, which can be
viewed as a variant of the basic shift-reduce al-
gorithm. Both parsers can achieve state-of-the-art
performance and Figure 2 shows the resulting syn-
tactic dependency trees of the following T-H pair,
ID: 2036; Entailment: YES
T: Devotees of the market question the value of
the work national service would perform.
H: Value is questioned.
Semantic Role Labeling The statistical depen-
dency parsers provide shallow syntactic analyses
of the entailment pairs through the limited vocab-
ulary of the dependency relations. In our case, the
CoNLL shared task dataset from 2008 were used
to train the statistical dependency parsing mod-
els. While such dependencies capture interesting
syntactic relations, when compared to the parsing
systems with deeper representations, the contained
information is not as detailed. To compensate for
this, we used a shallow semantic parser to predict
the semantic role relations in the T and H of en-
tailment pairs. The shallow semantic parser was
also trained with CoNLL 2008 shared task dataset,
with semantic roles extracted from the Propbank
and Nombank annotations (Zhang et al, 2008).
Figure 3 shows the resulting semantic dependency
graphs of the T-H pair.
HPSG Parsing We employ the English Re-
source Grammar (Flickinger, 2000), a hand-
written linguistic grammar in the framework of
HPSG, and the PET HPSG parser (Callmeier,
2001) to check the grammaticality of each hy-
pothesis sentence. As the hypotheses in this
PETE shared task were automatically generated,
some ungrammatical hypotheses occur in non-
entailment pairs. the grammaticality checking al-
lows us to quickly identify these instances.
2.2 Dependency Path Extraction
According to the task definition, we need to ver-
ify whether those dependency relations in H also
appear in T. We firstly find out all the impor-
tant dependency triples in H, like <word, depen-
dency relation, word>, excluding those having
stop words. The extracted syntactic dependency
triples of the example T-H pair would be none,
since the only content words ?value? and ?ques-
tioned? have no direct syntactic dependency in-
between (Figure 2). The extracted semantic de-
pendency triples would be <?questioned?, ?A1?,
?value?> (Figure 3).
After that, we use the word pairs contained in
the extracted dependency triples as anchors to find
out the corresponding dependency relations in T.
Notice that it is not necessarily that we can al-
ways find a direct dependency relation in T be-
tween the same word pair, so we need to traverse
the dependency tree or graph to find the depen-
dency paths. In general, we treat all the depen-
dency trees and graphs as undirected graphs with
loops, but keep records for the directions of the
edges we traverse. For the following three repre-
sentations, we apply slightly different algorithms
to find the dependency path between two words,
Syntactic Dependency Tree We simply traverse
the tree and find the corresponding depen-
dency path connecting the two words;
Semantic Dependency Graph We apply Dijk-
stra?s algorithm (Dijkstra, 1959) to find the
shortest path between the two words;
Joint Dependency Graph We assign different
weights to syntactic and semantic dependen-
cies and apply Dijkstra?s algorithm to find the
shortest path (with the lowest cost)
1
.
2.3 Feature-based Classification
Based on the meaning representation we have dis-
cussed above (Section 2.1 and Section 2.2), we ex-
1
In practice, we simply give semantic dependencies 0.5
cost and syntactic dependencies 1.0 cost, to show the prefer-
ences on the former when both exist.
273
T:
H:
Figure 2: Syntactic dependency of the example T-H pair by MaltParser.
T:
H:
Figure 3: Semantic dependency of the example T-H pair by MaltParser and our SRL system.
tract features for the machine-learning-based clas-
sifier. First of all, we should check whether there
are dependency triples extracted from H, other-
wise for our system, there is no meaning repre-
sentation for that sentence. Then we also need to
check whether the same words can be found in T
as well. Only if the corresponding dependency
paths are successfully located in T, we could ex-
tract the following features.
The direction of each dependency relation or
path could be interesting. The direction of the
H-path is clear, so we only need to check the
direction of the T-path. In practice, we simply
use a boolean value to represent whether T-path
contains dependency relations with different di-
rections. For instance, in Figure 3, if we extract
the path from ?market? to ?value?, the directions
of the dependency relations contained in the path
would be? and?, one of which would be incon-
sistent with the dependency relation in H.
Notice that all the dependency paths from H
have length 1
2
, but the lengths of the dependency
paths from T are varied. If the latter length is also
1, we can simply compare the two dependency re-
lations; otherwise, we compare each of the depen-
2
The length of one dependency path is defined as the num-
ber of dependency relations contained in the path.
dency relation contained the T-path with H-path
one by one
3
. By comparison, we mainly focus on
two values, the category of the dependency rela-
tion (e.g. syntactic dependency vs. semantic de-
pendency) and the content of the dependency rela-
tion (e.g. A1 vs. AM-LOC).
We also incorporate the string value of the de-
pendency relation pair and make it boolean ac-
cording to whether it occurs or not. Table 1 shows
the feature types we extract from each T-H pair.
3 Experiments
As we mentioned in the preprocessing section
(Section 2.1), we utilize the open source depen-
dency parsers, MSTParser
4
and MaltParser
5
, our
own semantic role labeler (Zhang et al, 2008), and
the PET HPSG parser
6
. For the shortest path algo-
rithm, we use the jGraphT package
7
; and for the
machine learning toolkit, we use the UniverSVM
3
Enlightened by Wang and Neumann (2007), we ex-
clude some dependency relations like ?CONJ?, ?COORD?,
?APPO?, etc., heuristically, since in most of the cases, they
will not change the relationship between the two words at
both ends of the path.
4
http://sourceforge.net/projects/
mstparser/
5
http://maltparser.org/
6
http://heartofgold.dfki.de/PET.html
7
http://jgrapht.sourceforge.net/
274
H
N
u
l
l
?
T
N
u
l
l
?
D
i
r
M
u
l
t
i
?
D
e
p
S
a
m
e
?
R
e
l
S
i
m
?
R
e
l
S
a
m
e
?
R
e
l
P
a
i
r
Joint + + + + + + + +
No Sem + + + + +
No Syn + + + + + + +
Table 1: Feature types of different settings of the
system. H Null? means whether H has dependencies;
T Null? means whether T has the corresponding paths (us-
ing the same word pairs found in H); Dir is whether the di-
rection of the path T the same as H; Multi? adds a prefix,
m , to the Rel Pair features, if the T-path is longer than one
dependency relation; Dep Same? checks whether the two de-
pendency types are the same, i.e. syntactic and semantic de-
pendencies; Rel Sim? only occurs when two semantic depen-
dencies are compared, meaning whether they have the same
prefixes, e.g. C-, AM-, etc.; Rel Same? checks whether the
two dependency relations are the same; and Rel Pair simple
concatenates the two relation labels together. Notice that, the
first seven feature types all contain boolean values, and for the
last one, we make it boolean as well, by observing whether
that pair of dependency labels appear or not.
package
8
. We test different dependency graphs
and feature sets as mentioned before (Table 1), and
the results are shown in Table 2.
MSTParser+SRL MaltParser+SRL
Joint No Sem No Syn Joint No Sem No Syn
+GC 0.5249
0.5116 0.5050
0.6678
0.5282 0.6346
(-1.3%) (-2.0%) (-14.0%) (-3.3%)
-GC 0.5216 0.5050 0.4950 0.6545 0.5282 0.6179
Table 2: Experiment results of our system with
different settings.
First of all, in almost all the cases, the grammat-
icality checking based on HPSG parsing is help-
ful, if we compare each pair of results at the two
rows, +GC and -GC. In all cases, the joint graph
representation achieves better results. This in-
dicates that features extracted from both syntac-
tic dependency and shallow semantic dependency
are useful for the entailment recognition. For the
MaltParser case, the semantic features show great
importance. Notice that the performance of the
whole system does not necessarily reflect the per-
formance of the parser itself, since it also depends
on our entailment modules. In all, the best setting
of our system ranks the 3rd place in the evaluation.
4 Conclusion
In this paper, we present our system used in the
PETE task, which consists of preprocessing, de-
pendency path extraction, and feature-based clas-
sification. We use MSTParser and MaltParser as
8
http://www.kyb.mpg.de/bs/people/
fabee/universvm.html
dependency parsers, our SRL system as a shallow
semantic parser, and a deep parser based on hand-
crafted grammars for grammaticality checking.
The entailment recognition is done by an SVM-
based classifier using features extracted from the
graph representation of the parser outputs. Based
on the results, we tentatively conclude that both
the syntactic and the shallow semantic features are
useful. A detailed error analysis would be our on-
going work in the near future.
Acknowledgment
The authors thank the PIRE PhD scholarship and
the German Excellence Cluster of MMCI for the
support of the work.
References
Ulrich Callmeier. 2001. Efficient parsing with large-scale
unification grammars. Master?s thesis, Universit?at des
Saarlandes, Saarbr?ucken, Germany.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Qui?nonero-Candela et al, editor, MLCW 2005, volume
LNAI Volume 3944, pages 177?190. Springer-Verlag.
E. W. Dijkstra. 1959. A note on two problems in connexion
with graphs. Numerische Mathematik, 1:269?271.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(1):1?41.
Rui Wang and G?unter Neumann. 2007. Recognizing textual
entailment using a subsequence kernel method. In Pro-
ceedings of AAAI-07, Vancouver, Canada, July.
Rui Wang and G?unter Neumann. 2009. An accuracy-
oriented divide-and-conquer strategy for recognizing tex-
tual entailment. In Proceedings of TAC 2008, Gaithers-
burg, Maryland, USA.
Deniz Yuret, Ayd?n Han, and Zehra Turgut. 2010. Semeval-
2010 task 12: Parser evaluation using textual entailments.
In Proceedings of the SemEval-2010 Evaluation Exercises
on Semantic Evaluation.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hybrid
learning of dependency structures from heterogeneous lin-
guistic resources. In Proceedings of the Twelfth Con-
ference on Computational Natural Language Learning
(CoNLL 2008), pages 198?202, Manchester, UK.
275
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 57?64
Manchester, August 2008
Towards Domain-Independent Deep Linguistic Processing:
Ensuring Portability and Re-Usability of Lexicalised Grammars
Kostadin Cholakov?, Valia Kordoni??, Yi Zhang??
? Department of Computational Linguistics, Saarland University, Germany
? LT-Lab, DFKI GmbH, Germany
{kostadin,kordoni,yzhang}@coli.uni-sb.de
Abstract
In this paper we illustrate and underline
the importance of making detailed linguis-
tic information a central part of the pro-
cess of automatic acquisition of large-scale
lexicons as a means for enhancing robust-
ness and at the same time ensuring main-
tainability and re-usability of deep lexi-
calised grammars. Using the error mining
techniques proposed in (van Noord, 2004)
we show very convincingly that the main
hindrance to portability of deep lexicalised
grammars to domains other than the ones
originally developed in, as well as to ro-
bustness of systems using such grammars
is low lexical coverage. To this effect,
we develop linguistically-driven methods
that use detailed morphosyntactic informa-
tion to automatically enhance the perfor-
mance of deep lexicalised grammars main-
taining at the same time their usually al-
ready achieved high linguistic quality.
1 Introduction
We focus on enhancing robustness and ensur-
ing maintainability and re-usability for a large-
scale deep grammar of German (GG; (Crysmann,
2003)), developed in the framework of Head-
driven Phrase Structure Grammar (HPSG). Specif-
ically, we show that the incorporation of detailed
linguistic information into the process of auto-
matic extension of the lexicon of such a language
resource enhances its performance and provides
linguistically sound and more informative predic-
tions which bring a bigger benefit for the grammar
when employed in practical real-life applications.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
In recent years, various techniques and re-
sources have been developed in order to improve
robustness of deep grammars for real-life applica-
tions in various domains. Nevertheless, low cover-
age of such grammars remains the main hindrance
to their employment in open domain natural lan-
guage processing. (Baldwin et al, 2004), as well
as (van Noord, 2004) and (Zhang and Kordoni,
2006) have clearly shown that the majority of pars-
ing failures with large-scale deep grammars are
caused by missing or wrong entries in the lexicons
accompanying grammars like the aforementioned
ones. Based on these findings, it has become clear
that it is crucial to explore and develop efficient
methods for automated (Deep) Lexical Acquisition
(henceforward (D)LA), the process of automati-
cally recovering missing entries in the lexicons of
deep grammars.
Recently, various high-quality DLA approaches
have been proposed. (Baldwin, 2005), as well
as (Zhang and Kordoni, 2006), (van de Cruys,
2006) and (Nicholson et al, 2008) describe effi-
cient methods towards the task of lexicon acqui-
sition for large-scale deep grammars for English,
Dutch and German. They treat DLA as a classi-
fication task and make use of various robust and
efficient machine learning techniques to perform
the acquisition process.
However, it is our claim that to achieve bet-
ter and more practically useful results, apart from
good learning algorithms, we also need to incorpo-
rate into the learning process fine-grained linguis-
tic information which deep grammars inherently
include and provide for. As we clearly show in
the following, it is not sufficient to only develop
and use good and complicated classification algo-
rithms. We must look at the detailed linguistic in-
formation that is already included and provided for
by the grammar itself and try to capture and make
as much use of it as possible, for this is the infor-
mation we aim at learning when performing DLA.
57
In this way, the learning process is facilitated and
at the same time it is as much as possible ensured
that its outcome be linguistically more informative
and, thus, practically more useful.
We use the GG deep grammar for the work we
present in this paper because German is a language
with rich morphology and free word order, which
exhibits a range of interesting linguistic phenom-
ena, a fair number of which are already analysed in
the GG. Thus, the grammar is a valuable linguistic
resource since it provides linguistically sound and
detailed analyses of these phenomena. Apart from
the interesting syntactic structures, though, the lex-
ical entries in the lexicon of the aforementioned
grammar also exhibit a rich and complicated struc-
ture and contain various important linguistic con-
straints. Based on our claim above, in this pa-
per we show how the information these constraints
provide can be captured and used in linguistically-
motivated DLA methods which we propose here.
We then apply our approach on real-life data and
observe the impact it has on the the grammar cov-
erage and its practical application. In this way we
try to prove our assumption that the linguistic in-
formation we incorporate into our DLA methods
is vital for the good performance of the acquisition
process and for the maintainability and re-usability
of the grammar, as well for its successful practical
application.
The remainder of the paper is organised as fol-
lows. In Section 2 we show that low (lexical) cov-
erage is a serious issue for the GG when employed
for open domain natural language processing. Sec-
tion 3 presents the types in the lexical architecture
of the GG that are considered to be relevant for the
purposes of our experiments. Section 4 describes
the extensive linguistic analysis we perform in or-
der to deal with the linguistic information these
types provide and presents the target type inven-
tory for our DLA methods. Section 5 reports on
statistical approaches towards automatic DLA and
shows the importance of a good and linguistically-
motivated feature selection. Section 6 illustrates
the practical usage of the proposed DLA methods
and their impact on grammar coverage. Section 7
concludes the paper.
2 Coverage Test with the GG
We start off adopting the automated error mining
method described in (van Noord, 2004) for iden-
tification of the major type of errors in the GG.
As an HPSG grammar, the GG is based on typed
feature structures. The GG types are strictly de-
fined within a type hierarchy. The GG also con-
tains constructional and lexical rules and a lexicon
with its entries belonging to lexical types which
are themselves defined again within the type hier-
archy. The grammar originates from (Mu?ller and
Kasper, 2000), but continued to improve after the
end of the Verbmobil project (Wahlster, 2000) and
it currently consists of 5K types, 115 rules and the
lexicon contains approximately 35K entries. These
entries belong to 386 distinct lexical types.
In the experiments we report here two corpora
of different kind and size have been used. The
first one has been extracted from the Frankfurter
Rundschau newspaper and contains about 614K
sentences that have between 5 and 20 tokens. The
second corpus is a subset of the German part of the
Wacky project (Kilgarriff and Grefenstette, 2003).
The Wacky project aims at the creation of large
corpora for different languages, including German,
from various web sources, such as online news-
papers and magazines, legal texts, internet fora,
university and science web sites, etc. The Ger-
man part, named deWaC (Web as Corpus), con-
tains about 93M sentences and 1.65 billion tokens.
The subset used in our experiments is extracted
by randomly selecting 2.57M sentences that have
between 4 and 30 tokens. These corpora have
been chosen because it is interesting to observe
the grammar performance on a relatively balanced
newspaper corpus that does not include so many
long sentences and sophisticated linguistic con-
structions and to compare it with the performance
of the grammar on a random open domain text cor-
pus.
The sentences are fed into the PET HPSG parser
(Callmeier, 2000) with the GG loaded. The parser
has been configured with a maximum edge num-
ber limit of 100K and it is running in the best-only
mode so that it does not exhaustively find all pos-
sible parses. The result of each sentence is marked
as one of the following four cases:
? P means at least one parse is found for the
sentence;
? L means the parser halted after the morpho-
logical analysis and was not able to construct
any lexical item for the input token;
? N means that the parser exhausted the search-
ing and was not able to parse the sentence;
58
? E means the parser reached the maximum
edge number limit and was still not able to
find a parse.
Table 1 shows the results of the experiments
with the two corpora. From these results it can
FR deWaC
Result #Sentences % #Sentences %
P 62,768 10.22% 109,498 4.3%
L 464,112 75.55% 2,328,490 90.5%
N 87,415 14.23% 134,917 5.2%
E 3 ? 14 ?
Total: 614,298 100% 2,572,919 100%
Table 1: Parsing results with the GG and the test
corpora
be seen that the GG has full lexical span for only
a small portion of the sentences? about 25% and
10% for the Frankfurter Rundschau and the deWaC
corpora, respectively. The output of the error min-
ing confirms our assumption that missing lexical
entries are the main problem when it comes to
robust performance of the GG and illustrates the
need for efficient DLA methods.
3 Atomic Lexical Types
Before describing the proposed DLA algorithm,
we should define what exactly is being learnt.
Most of the so called deep grammars are strongly
lexicalised. As mentioned in the previous section,
the GG employs a type inheritance system and its
lexicon has a flat structure with each lexical entry
mapped onto one type in the inheritance hierarchy.
Normally, the types assigned to the lexical entries
are maximal on the type hierarchy, i.e., they do not
have any subtypes. They provide the most specific
information available for this branch of the hierar-
chy. These maximal types which the lexical entries
are mapped onto are called atomic lexical types.
Thus, in our experiment setup, we can define the
lexicon of the grammar as being a one-to-one map-
ping from word stems to atomic lexical types. It is
this mapping which must be automatically learnt
(guessed) by the different DLA methods.
We are interested in learning open-class words,
i.e., nouns, adjectives, verbs and adverbs. We as-
sume that the close-class words are already in the
lexicon or the grammar can handle them through
various lexical rules and they are not crucial for
the grammar performance in real life applications.
Thus, for the purposes of our experiments, we con-
sider only the open-class lexical types. Moreover,
we propose an inventory of open-class lexical types
with sufficient type and token frequency. The type
frequency of a given lexical type is defined as
the number of lexical entries in the lexicon of the
grammar that belong to this type and the token fre-
quency is the number of words in some corpus that
belong to this type.
We use sentences from the Verbmobil corpus
which have been treebanked with the GG in order
to determine the token frequency and to map the
lexemes to their correct entries in the lexicon for
the purposes of the experiment. This set contains
11K sentences and about 73K tokens; this gives an
average of 6.8 words per sentence. The sentences
are taken from spoken dialogues. Hence, they are
not long and most of them do not exhibit interest-
ing linguistic properties which is a clear drawback
but currently there is no other annotated data com-
patible with the GG.
We used a type frequency threshold of 10 entries
in the lexicon and a token frequency threshold of
3 occurrences in the treebanked sentences to form
a list of relevant open-class lexical types. The re-
sulting list contains 38 atomic lexical types with a
total of 32,687 lexical entries.
4 Incorporation of Linguistic Features
However, in the case of the GG this type inventory
is not a sufficient solution. As already mentioned,
in the lexicon of the grammar much of the relevant
linguistic information is encoded not in the type
definition itself but in the form of constraints in the
feature structures of the various types. Moreover,
given that German has a rich morphology, a given
attribute may have many different values among
lexical entries of the same type and it is crucial for
the DLA process to capture all the different com-
binations. That is why we expand the identified
38 atomic lexical type definitions by including the
values of various features into them.
By doing this, we are trying to facilitate the
DLA process because, in that way, it can ?learn?
to differentiate not only the various lexical types
but also significant morphosyntactic differences
among entries that belong to the same lexical type.
That gives the DLA methods access to much more
linguistic information and they are able to apply
more linguistically fine-tuned classification crite-
ria when deciding which lexical type the unknown
word must be assigned to. Furthermore, we en-
sure that the learning process deliver linguistically
59
Feature Values Meaning
SUBJOPT (subject options)
+ in some cases the article for the noun can be omitted
- the noun always goes with an article
+ raising verb
- non-raising verb
KEYAGR (key agreement)
? case-number-gender information for nouns
c-s-n underspecified-singular-neutral
c-p-g underspecified-plural-underspecified
... ...
(O)COMPAGR ((oblique) a-n-g, d-n-g, etc. case-number-gender information
complement ? for (oblique) verb complements
agreement ? case-number-gender of the modified noun (for adjectives)
(O)COMPTOPT ((oblique) ? verbs can take a different number of complements
complement + the respective (oblique) complement is present
options - the respective (oblique) complement is absent
KEYFORM
? the auxiliary verb used for the formation of perfect tense
haben the auxiliary verb is ?haben?
sein the auxiliary verb is ?sein?
Table 2: Relevant features used for type expansion
plausible, precise and more practically useful re-
sults. The more the captured and used linguistic
information is, the better and more useful the DLA
results will be.
However, we have to avoid creating data sparse
problems. We do so by making the assumption
that not every feature could really contribute to the
classification process and by filtering out these fea-
tures that we consider irrelevant for the enhance-
ment of the DLA task. Naturally, the question
which features are to be considered relevant arises.
After performing an extensive linguistic analysis,
we have decided to take the features shown in Ta-
ble 2 into account.
We have thoroughly analysed each of these fea-
tures and selected them on the basis of their lin-
guistic meaning and their significance and contri-
bution to the DLA process. The SUBJOPT fea-
ture can be used to differentiate among nouns that
have a similar morphosyntactic behaviour but dif-
fer only in the usage of articles; 4 out of the consid-
ered 9 noun atomic lexical types do not define this
feature. Furthermore, using this feature, we can
also refine our classification within a single atomic
lexical type. For example, the entry ?adresse-n?
(address) of the type ?count-noun-le?1 has ?-? for
the SUBJOPT value, whereas the value for the en-
try ?anbindung-n? (connection) of the same type is
?+?:
(1) a. Das
det.NEUT.NOM
Hotel
hotel
hat
have.3PER.SG
gute
good
Anbindung
connection
an
to
die
det.PL.ACC
o?ffentlichen
public
1count noun lexeme; all lexical entries in the lexicon end
with le which stands for lexeme.
Verkehrsmittel.
transportation means
?The hotel has a good connection to public
transportation.?
b. Die
det.FEM.NOM
Anbindung
connection
an
to
Rom
Rome
mit
with
dem
det.MASC.DAT
Zug
train
ist
be.3PER.SG
gut.
good
?The train connection to Rome is good.?
The distinction between raising and non-raising
verbs that this feature expresses is also an impor-
tant contribution to the classification process.
The case-number-gender data the KEYAGR and
(O)COMPAGR features provide allows for a bet-
ter usage of morphosyntactic information for the
purposes of DLA. Based on this data, the classifi-
cation method is able to capture words with sim-
ilar morphosyntactic behaviour and give various
indications for their syntactic nature; for instance,
if the word is a subject, direct or indirect object.
This is especially relevant and useful for languages
with rich morphology and relatively free word or-
der such as German. The same is also valid for
the (O)COMPOPT and KEYFORM features? they
allow the DLA method to successfully learn and
classify verbs with similar syntactic properties.
The values of the features are just attached to the
old type name to form a new type definition. In this
way, we ?promote? them and these features are now
part of the type hierarchy of the grammar which
makes them accessible for the DLA process since
this operates on the type level. For example, the
original type of the entry for the noun ?abenteuer?
(adventure):
abenteuer-n := count-noun-le &
[ [ --SUBJOPT -,
60
KEYAGR c-n-n,
KEYREL "_abenteuer_n_rel",
KEYSORT situation,
MCLASS nclass-2_-u_-e ] ].
will become abenteuer-n := count-noun-le - c-n-
n when we incorporate the values of the features
SUBJOPT and KEYAGR into the original type
definition. The new expanded type inventory is
shown in Table 3.
Original Expanded
lexicon lexicon
Number of lexical types 386 485
Atomic lexical types 38 137
-nouns 9 72
-verbs 19 53
-adjectives 3 5
-adverbs 7 7
Table 3: Expanded atomic lexical types
The features we have ignored do not contribute
to the learning process and are likely to cre-
ate sparse data problems. The (O)COMPFORM
((oblique) complement form) features which de-
note dependent to verbs prepositions are not con-
sidered to be relevant. An example of OCOMP-
FORM is the lexical entry ?begru?nden mit-v? (jus-
tify with) where the feature has the preposition
?mit? (with) as its value. Though for German
prepositions can be considered as case markers, the
DLA has already a reliable access to case informa-
tion through the (O)COMPAGR features. More-
over, a given dependent preposition is distributed
across many types and it does not indicate clearly
which type the respective verb belongs to.
The same is valid for the feature VCOPMFORM
(verb complement form) that denotes the separa-
ble particle (if present) of the verb in question.
An example of this feature is the lexical entry
?abdecken-v? (to cover) where VCOMPFORM has
the separable particle ?ab? as its value. However,
treating such discontinuous verb-particle combina-
tions as a lexical unit could help for the acquisi-
tion of subcategorizational frames. For example,
anho?ren (to listen to someone/something) takes an
accusative NP as argument, zuho?ren (to listen to)
takes a dative NP and aufho?ren (to stop, to termi-
nate) takes an infinitival complement. Thus, ignor-
ing VCOMPFORM could be a hindrance for the
acquisition of some verb types2.
We have also tried to incorporate some sort of
semantic information into the expanded atomic
2We thank the anonymous reviewer who pointed this out
for us.
lexical type definitions by also attaching the
KEYSORT semantic feature to them. KEYSORT
defines a certain situation semantics category
(?anything?, ?action sit?, ?mental sit?) which the
lexical entry belongs to. However, this has caused
again a sparse data problem because the semantic
classification is too specific and, thus, the number
of possible classes is too large. Moreover, seman-
tic classification is done based on completely dif-
ferent criteria and it cannot be directly linked to the
morphosyntactic features. That is why we have fi-
nally excluded this feature, as well.
Armed with this elaborate target type inventory,
we now proceed with the DLA experiments for the
GG.
5 DLA Experiments with the GG
For our DLA experiments, we adopted the Max-
imum Entropy based model described in (Zhang
and Kordoni, 2006), which has been applied to the
ERG (Copestake and Flickinger, 2000), a wide-
coverage HPSG grammar for English. For the pro-
posed prediction model, the probability of a lexical
type t given an unknown word and its context c is:
(2) p(t|c) = exp(
?
i
?
i
f
i
(t,c))
?
t
?
?T
exp(
?
i
?
i
f
i
(t
?
,c))
where f
i
(t, c) may encode arbitrary characteristics
of the context and ?
i
is a weighting factor esti-
mated on a training corpus. Our experiments have
been performed with the feature set shown in Table
4.
Features
the prefix of the unknown word
(length is less or equal 4)
the suffix of the unknown word
(length is less or equal 4)
the 2 words before and after the unknown word
the 2 types before and after the unknown word
Table 4: Features for the DLA experiment
We have also experimented with prefix and suf-
fix lengths up to 3. To evaluate the contribution
of various features and the overall precision of the
ME-based unknown word prediction model, we
have done a 10-fold cross validation on the Verb-
mobil treebanked data. For each fold, words that
do not occur in the training partition are assumed
to be unknown and are temporarily removed from
the lexicon.
For comparison, we have also built a baseline
model that always assigns a majority type to each
61
unknown word according to its POS tag. Specifi-
cally, we tag the input sentence with a small POS
tagset. It is then mapped to a most popular lexi-
cal type for that POS. Table 5 shows the relevant
mappings.
POS Majority lexical type
noun count-noun-le - c-n-f
verb trans-nerg-str-verb-le haben-auxf
adj adj-non-prd-le
adv intersect-adv-le
Table 5: POS tags to lexical types mapping
Again for comparison, we have built another
simple baseline model using the TnT POS tagger
(Brants, 2000). TnT is a general-purpose HMM-
based trigram tagger. We have trained the tagging
models with all the lexical types as the tagset. The
tagger tags the whole sentence but only the output
tags for the unknown words are taken to generate
lexical entries and to be considered for the eval-
uation. The precisions of the different prediction
models are given in Table 6.
The baseline achieves a precision of about 38%
and the POS tagger outperforms it by nearly 10%.
These results can be explained by the nature of the
Verbmobil data. The vast majority of the adjec-
tives and the adverbs in the sentences belong to
the majority types shown in Table 5 and, thus, the
baseline model assigns the correct lexical types to
almost every adjective and adverb, which brings
up the overall precision. The short sentence length
facilitates the tagger extremely, for TnT, as an
HMM-based tagger, makes predictions based on
the whole sentence. The longer the sentences are,
the more challenging the tagging task for TnT is.
The results of these models clearly show that the
task of unknown word type prediction for deep
grammars is non-trivial.
Our ME-based models give the best results in
terms of precision. However, verbs and adverbs
remain extremely difficult for classification. The
simple morphological features we use in the ME
model are not good enough for making good pre-
dictions for verbs. Morphology cannot capture
such purely syntactic features as subcategoriza-
tional frames, for example.
While the errors for verbs are pretty random,
there is one major type of wrong predictions for
adverbs. Most of them are correctly predicted as
such but they receive the majority type for adverbs,
namely ?intersect-adv-le?. Since most of the ad-
verbs in the Verbmobil data we are using belong
to the majority adverb type, the predictor is biased
towards assigning it to the unknown words which
have been identified as adverbs.
The results in the top half of the Table 6 show
that morphological features are already very good
for predicting adjectives. In contrast with ad-
verbs, adjectives occur in pretty limited number of
contexts. Moreover, when dealing with morpho-
logically rich languages such as German, adjec-
tives are typically marked by specific affixes cor-
responding to a specific case-number-gender com-
bination. Since we have incorporated this kind of
linguistic information into our target lexical type
definitions, this significantly helps the prediction
process based on morphological features.
Surprisingly, nouns seem to be hard to learn.
Apparently, the vast majority of the wrong pre-
dictions have been made for nouns that belong to
the expanded variants of the lexical type ?count-
noun-le? which is also the most common non-
expanded lexical type for nouns in the original lex-
icon. Many nouns have been assigned the right lex-
ical type except for the gender:
(3) Betrieb (business, company, enterprise)
prediction: count-noun-le - c-n-n
correct type: count-noun-le - c-n-m
According to the strict exact-match evaluate mea-
sure we use, such cases are considered to be errors
because the predicted lexical type does not match
the type of the lexical entry in the lexicon.
The low numbers for verbs and adverbs show
clearly that we also need to incorporate some sort
of syntactic information into the prediction model.
We adopt the method described in (Zhang and Ko-
rdoni, 2006) where the disambiguation model of
the parser is used for this purpose. We also believe
that the kind of detailed morphosyntactic informa-
tion which the learning process now has access
to would facilitate the disambiguation model be-
cause the input to the model is linguistically more
fine-grained. In another DLA experiment we let
PET use the top 3 predictions provided by the lex-
ical type predictor in order to generate sentence
analyses. Then we use the disambiguation model,
trained on the Verbmobil data, to choose the best
one of these analyses and the corresponding lexical
entry is taken to be the final result of the prediction
process.
As shown in the last line of Table 6, we achieve
an increase of 19% which means that in many
cases the correct lexical type has been ranked sec-
62
Model Precision Nouns Adjectives Verbs Adverbs
Baseline 37.89% 27.03% 62.69% 33.57% 67.14%
TnT 47.53% 53.76% 74.52% 26.94% 32.68%
ME(affix length=3) 51.2% 48.25% 75.41% 44.06% 44.13%
ME(affix length=4) 54.63% 53.55% 76.79% 47.10% 43.55%
ME + disamb. 73.54% 75% 88.24% 65.98% 65.90%
Table 6: Precision of unknown word type predictors
ond or third by the predictor. This proves that
the expanded lexical types improve also the perfor-
mance of the disambiguation model and allow for
its successful application for the purposes of DLA.
It also shows, once again, the importance of the
morphology in the case of the GG and proves the
rightness of our decision to expand the type defini-
tions with detailed linguistic information.3
6 Practical Application
Since our main claim in this paper is that for
good and practically useful DLA, which at the
same time may facilitate robustness and ensure
maintainability and re-usability of deep lexicalised
grammars, we do not only need good machine
learning algorithms but also classification and fea-
ture selection that are based on an extensive lin-
guistic analysis, we apply our DLA methods to real
test data. We believe that due to our expanded lex-
ical type definitions, we provide much more lin-
guistically accurate predictions. With this type of
predictions, we anticipate a bigger improvement of
the grammar coverage and accuracy for the pre-
diction process delivers much more linguistically
relevant information which facilitates parsing with
the GG.
We have conducted experiments with PET and
the two corpora we have used for the error mining
to determine whether we can improve coverage by
using our DLA method to predict the types of un-
known words online. We have trained the predic-
tor on the whole set of treebanked sentences and
extracted a subset of 50K sentences from each cor-
pus. Since lexical types are not available for these
sentences, we have used POS tags instead as fea-
tures for our prediction model. Coverage is mea-
sured as the number of sentences that received at
least one parse and accuracy is measured as the
number of sentences that received a correct analy-
sis. The results are shown in Table 7.
The coverage for FR improves with more than
12% and the accuracy number remains almost the
3Another reason for this high result is the short average
length of the treebanked sentences which facilitates the dis-
ambiguation model of the parser.
Parsed Corpus Coverage Accuracy
FR with the vanilla version GG 8.89% 85%
FR with the GG + DLA 21.08% 83%
deWaC with the vanilla version GG 7.46% ?
deWaC with the GG + DLA 16.95% ?
Table 7: Coverage results
same. Thus, with our linguistically-oriented DLA
method, we have managed to increase parsing cov-
erage and at the same time to preserve the high
accuracy of the grammar. It is also interesting to
note the increase in coverage for the deWaC cor-
pus. It is about 10%, and given the fact that deWaC
is an open and unbalanced corpus, this is a clear
improvement. However, we do not measure ac-
curacy on the deWaC corpus because many sen-
tences are not well formed and the corpus itself
contains much ?noise?. Still, these results show
that the incorporation of detailed linguistic infor-
mation in the prediction process contributed to the
parser performance and the robustness of the gram-
mar without harming the quality of the delivered
analyses.
7 Conclusion
In this paper, we have tackled from a more
linguistically-oriented point of view the lexicon
acquisition problem for a large-scale deep gram-
mar for German, developed in HPSG. We have
shown clearly that missing lexical entries are the
main cause for parsing failures and, thus, illus-
trated the importance of increasing the lexical cov-
erage of the grammar. The target type inventory
for the learning process has been developed in a
linguistically motivated way in an attempt to cap-
ture significant morphosyntactic information and,
thus, achieve a better performance and more prac-
tically useful results.
With the proposed DLA approach and our elab-
orate target type inventory we have achieved nearly
75% precision and this way we have illustrated the
importance of fine-grained linguistic information
for the lexical prediction process. In the end, we
have shown that with our linguistically motivated
DLA methods, the parsing coverage of the afore-
63
mentioned deep grammar improves significantly
while its linguistic quality remains intact.
The conclusion, therefore, is that it is vital to
be able to capture linguistic information and suc-
cessfully incorporate it in DLA processes, for it
facilitates deep grammars and makes processing
with them much more robust for applications. At
the same time, the almost self-evident portability
to new domains and the re-usability of the gram-
mar for open domain natural language processing
is significantly enhanced.
The DLA method we propose can be used as
an external module that can help the grammar be
ported and operate on different domains. Thus,
specifically in the case of HPSG, DLA can also
be seen as a way for achieving more modular-
ity in the grammar. Moreover, in a future re-
search, the proposed kind of DLA might also be
used in order to facilitate the division and transi-
tion from a core deep grammar with a core lex-
icon towards subgrammars with domain specific
lexicons/lexical constraints in a linguistically mo-
tivated way. The use of both these divisions nat-
urally leads to a highly modular structure of the
grammar and the system using the grammar, which
at the same time helps in controlling its complex-
ity.
Our linguistically motivated approach provides
fine-grained results that can be used in a number
of different ways. It is a valuable linguistic tool
and it is up to the grammar developer to choose
how to use the many opportunities it provides.
References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the English
Resource Grammar over the British National Corpus. In
Proceedings of the Fourth Internation Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lisbon,
Portugal.
Baldwin, Timothy. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Proceedings of the
ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisi-
tion, pages 67?76, Ann Arbor, USA.
Brants, Thorsten. 2000. TnT- a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Applied
Natural Language Processing ANLP-2000, Seattle, WA,
USA.
Callmeier, Ulrich. 2000. PET- a platform for experimenta-
tion with efficient HPSG processing techniques. In Jour-
nal of Natural Language Engineering, volume 6(1), pages
99?108.
Copestake, Ann and Dan Flickinger. 2000. An open-sourse
grammar development environment and broad-coverage
English grammar using HPSG. In Proceedings of the Sec-
ond conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Crysmann, Berthold. 2003. On the efficient implementation
of German verb placement in HPSG. In Proceedings of
RANLP 2003, pages 112?116, Borovets, Bulgaria.
Kilgarriff, Adam and G Grefenstette. 2003. Introduction to
the special issue on the web as corpus. Computational Lin-
guistics, 29:333?347.
Mu?ller, Stephan and Walter Kasper. 2000. HPSG analysis of
German. In Wahlster, Wolfgang, editor, Verbmobil: Foun-
dations of Speech-to-Speech Translation, pages 238?253.
Springer-Verlag.
Nicholson, Jeremy, Valia Kordoni, Yi Zhang, Timothy Bald-
win, and Rebecca Dridan. 2008. Evaluating and extend-
ing the coverage of HPSG grammars. In In proceedings of
LREC, Marrakesh, Marocco.
van de Cruys, Tim. 2006. Automatically extending the lexi-
con for parsing. In Huitink, Janneke and Sophia Katrenko,
editors, Proceedings of the Student Session of the Euro-
pean Summer School in Logic, Language and Information
(ESSLLI), pages 180?191, Malaga, Spain.
van Noord, Gertjan. 2004. Error mining for wide coverage
grammar engineering. In Proceedings of the 42nd Meeting
of the Assiciation for Computational Linguistics (ACL?04),
Main Volume, pages 446?453, Barcelona, Spain.
Wahlster, Wolfgang, editor. 2000. Verbmobil: Foundations
of Speech-to-Speech Translation. Artificial Intelligence.
Springer.
Zhang, Yi and Valia Kordoni. 2006. Automated deep lexical
acquisition for robust open text processing. In Proceed-
ings of the Fifth International Conference on Language
Resourses and Evaluation (LREC 2006), Genoa, Italy.
64
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 198?202
Manchester, August 2008
Hybrid Learning of Dependency Structures from Heterogeneous
Linguistic Resources
Yi Zhang
Language Technology Lab
DFKI GmbH
yzhang@coli.uni-sb.de
Rui Wang
Computational Linguistics
Saarland University, Germany
rwang@coli.uni-sb.de
Hans Uszkoreit
Language Technology Lab
DFKI GmbH
uszkoreit@dfki.de
Abstract
In this paper we present our syntactic and
semantic dependency parsing system par-
ticipated in both closed and open compe-
titions of the CoNLL 2008 Shared Task.
By combining the outcome of two state-of-
the-art syntactic dependency parsers, we
achieved high accuracy in syntactic de-
pendencies (87.32%). With MRSes from
grammar-based HPSG parsers, we achieved
significant performance improvement on
semantic role labeling (from 71.31% to
71.89%), especially in the out-domain
evaluation (from 60.16% to 62.11%).
1 Introduction
The CoNLL 2008 shared task (Surdeanu et al,
2008) provides a unique chance of comparing dif-
ferent syntactic and semantic parsing techniques
in one unified open competition. Our contribution
in this joint exercise focuses on the combination
of different algorithms and resources, aiming not
only for state-of-the-art performance in the com-
petition, but also for the dissemination of the learnt
lessons to related sub-fields in computational lin-
guistics.
The so-called hybrid approach we take has two
folds of meaning. For syntactic dependency pars-
ing, we build our system based on state-of-the art
algorithms. Past CoNLL share task results have
shown that transition-based and graph-based algo-
rithms started from radically different ideas, yet
achieved largely comparable results. One of the
question we would like investigate is whether the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
combination of the two approach on the output
level leads to even better results.
For the semantic role labeling (SRL) task, we
would like to build a system that allows us to test
the contribution of different linguistic resources.
To our special interest is to examine the deep
linguistic parsing systems based on hand-crafted
grammars. During the past decades, various large
scale linguistic grammars have been built, some
of which achieved both broad coverage and high
precision. In combination with other advances
in deep linguistic processing, e.g. efficient pars-
ing algorithms, statistical disambiguation models
and robust processing techniques, several systems
have reached mature stage to be deployed in ap-
plications. Unfortunately, due to the difficulties
in cross-framework evaluation, fair comparison of
these systems with state-of-the-art data-driven sta-
tistical parsers is still hard to achieve. More impor-
tantly, it is not even clear whether deep linguistic
analysis is necessary at all for tasks such as shallow
semantic parsing (also known as SRL). Drawing
a conclusion on this latter point with experiments
using latest deep parsing techniques is one of our
objective.
The remainder of the paper is structure as fol-
lows. Section 2 introduces the overall system ar-
chitecture. Section 3 explains the voting mecha-
nism used in the syntactic parser. Section 4 de-
scribes in detail the semantic role labeling com-
ponent. Section 5 presents evaluation results and
error analysis. Section 6 concludes the paper.
2 System Architecture
As shown in Figure 1, our system is a two-stage
pipeline. For the syntactic dependencies, we apply
two state-of-the-art dependency parsers and com-
bined their results based on a voting model. For
198
Parse Selector
(MaltParser)Transition?based DepParser (MST Parser)Graph?based DepParser
Deep Linguistic Parser(ERG/PET)Predicate Identification
Argument Identification
Argument Classification
Predicated Classification
SemanticRoleLabeling
Syn.Dep.
MRS
SyntacticDependencyParsing
Figure 1: System Architecture
the semantic roles, we extracted features from the
previous stage, combined with deep parsing results
(in MRS), and use statistical classification models
to make predictions. In particular, the second part
can be further divided into four stages: predicate
identification (PI), argument identification (AI), ar-
gument classification (AC), and predicate classi-
fication (PC). Maximum entropy-based machine
learning techniques are used in both components
which we will see in detail in the following sec-
tions.
3 Syntactic Dependency Parsing
For obtaining syntactic dependencies, we have
combined the results of two state-of-the-art depen-
dency parsers: the MST parser (McDonald et al,
2005) and the MaltParser (Nivre et al, 2007).
The MST parser formalizes dependency parsing
as searching for maximum spanning trees (MSTs)
in directed graphs. A major advantage of their
framework is the ability to naturally and efficiently
model both projective and non-projective parses.
To learn these structures they used online large-
margin learning that empirically provides state-of-
the-art performance.
The MaltParser is a transition-based incremental
dependency parser, which is language-independent
and data-driven. It contains a deterministic algo-
rithm, which can be viewed as a variant of the ba-
sic shift-reduce algorithm. The learning method
they applied is support vector machine and experi-
mental evaluation confirms that the MaltParser can
achieve robust, efficient and accurate parsing for a
wide range of languages.
Since both their parsing algorithms and machine
learning methods are quite different, we decide to
take advantages of them. After a comparison be-
tween the results of the two parsers
1
, we find that,
1. The MST parser is better at the whole struc-
ture. In several sentences, the MaltParser was
wrong at the root node, but the MST parser is
correct.
2. The MaltParser is better at some dependency
labels (e.g. TMP, LOC, etc.).
These findings motivate us to do a voting based
on both outputs. The features considered in the
voting model are as follows:
? Dependency path: two categories of depen-
dency paths are considered as features: 1)
the POS-Dep-POS style and 2) the Dep-Dep
style. The former consists of part-of-speech
(POS) tags and dependency relations appear-
ing in turns; and the latter only contains de-
pendency relations. The maximum length of
the dependency path is three dependency re-
lations.
? Root attachments: the number of tokens at-
tached to the ROOT node by the parser in one
sentence
? Sentence length: the number of tokens in
each input sentence
? Projectivity: whether the parse is projective
or not
With these features, we apply a statistical model
to predict, for each sentence, we choose the pars-
ing result from which parser. The voted result will
be our syntactic dependency output and be passed
to the later stages.
4 Semantic Role Labeling
4.1 Overview
The semantic role labeling component of our sys-
tem is comprised of a pipeline model with four
1
In this experiment, we use second order features and the
projective decoder for the MST parser trained with 10 iter-
ations, and Arc-eager algorithm with a quadric polynomial
kernel for the MaltParser.
199
sub-components that performs predicate identi-
fication (PI), argument identification (AI), argu-
ment classification (AC) and predicate classifica-
tion (PC) respectively. The output in previous
steps are taken as input information to the follow-
ing stages. All these components are essentially
based on a maximum entropy statistical classifier,
although with different task-specific optimizations
and feature configurations in each step. Depending
on the available information from the input data
structure, the same architecture is used for both
closed and open challenge runs, with different fea-
ture types. Note that our system does not make use
of or predict SU chains.
Predicate Identification The component makes
binary prediction on each input token whether it
forms a predicate in the input sentence. This pre-
dictor precedes other components because it is a
relatively easy task (comparing to the following
components). Also, making this prediction early
helps to cut down the search space in the follow-
ing steps. Based on the observation on the training
data, we limit the PI predictor to only predict for
tokens with certain POS types (POSes marked as
predicates for at least 50 times in the training set).
This helps to significantly improve the system effi-
ciency in both training and prediction time without
sacrificing prediction accuracy.
It should be noted that the prediction of nominal
predicates are generally much more difficult (based
on CoNLL 2008 shared task annotation). The PI
model achieved 96.32 F-score on WSJ with verbal
predicates, but only 84.74 on nominal ones.
Argument Identification After PI, the argu-
ments to the predicted predicates are identified
with the AI component. Similar to the approach
taken in Hacioglu (2004), we use a statistical clas-
sifier to select from a set of candidate nodes in a
dependency tree. However, instead of selecting
from a set of neighboring nodes from the predicate
word
2
, we define the concept of argument path as
a chain of dependency relations from the predicate
to the argument in the dependency tree. For in-
stance, an argument path [??? | ???] indicates that
if the predicate is syntactically depending as ??? on
a node which has a ??? child, then the ??? node
2
Hacioglu (2004) defines a tree-structured family of a
predicate as a measure of locality. It is a set of dependency
relation nodes that consists of the predicate?s parent, chil-
dren, grandchildren, siblings, siblings? children and siblings?
grandchildren with respect to its dependency tree
(sibling to the predicate) is an argument candidate.
While Hacioglu (2004)?s approach focus mainly
on local arguments (with respect to the syntactic
dependencies), our approach is more suitable of
capturing long distance arguments from the pred-
icate. Another minor difference is that we allow
predicate word to be its own argument (which is
frequently the case for nominal predicates) with an
empty argument path [ | ].
The set of effective argument paths are obtained
from the training set, sorted and filtered according
to their frequencies, and used in testing to obtain
the candidate arguments. By setting a frequency
threshold, we are able to select the most useful
argument paths. The lower the threshold is, the
higher coverage one might get in finding candi-
date arguments, accompanied with a higher aver-
age candidate number per predicate and potentially
a more difficult task for the statistical classifier.
By experimenting with different frequency thresh-
olds on the training set, we established a frequency
threshold of 40, which guarantees candidate argu-
ment coverage of 95%, and on average 5.76 candi-
dates per predicate. Given that for the training set
each predicate takes on average 2.13 arguments,
the binary classifier will have relatively balanced
prediction classes.
Argument Classification For each identified ar-
gument, an argument label will be assigned during
the argument classification step. Unlike the binary
classifiers in previous two steps, AC uses a multi-
class classifier that predicts from the inventory of
argument labels. For efficiency reasons, we only
concern the most frequent 25 argument labels.
Predicate Classification The final step in the
SRL component labels the predicted predicate with
a predicate name. Due to the lack of lexical
resources in the closed competition, this step is
scheduled for the last, in order to benefit from the
predictions made in the previous steps. Unlike the
previous steps, the statistical model used in this
step is a ranking model. We obtained a list of can-
didate frames and corresponding rolesets from the
provided PropBank and NomBank data. Each pre-
dicted predicate is mapped onto the potential role-
sets it may take. When the frame for the predicate
word is missing from the list, or there is only one
candidate roleset for it, the predicate name is as-
signed deterministically (word stem concatenated
with ?.01? for frame missing predicates, the unam-
200
biguous roleset name when there is only one can-
didate). When there are more than one candidate
rolesets, a ranking model is trained to select the
most probable roleset for a given predicate given
the syntactic and semantic context.
4.2 Features
The feature types used in our SRL component are
summarized in Table 1, with the configurations of
our submitted ?closed? and ?open? runs marked.
Numerous different configurations with these fea-
ture types have been experimented on training and
development data. The results show that feature
types 1?14 are the best performing ones. Fea-
tures related to the siblings of the predicate only
introduce minor performance variation. We also
find the named entity labels does not lead to im-
mediate improvement of SRL performance. The
WordNet sense feature does achieve minor perfor-
mance increase on PI and PC, although the signif-
icant remains to be further examined. Based on
the pipeline model, we find it difficult to achieve
further improvement by incorporate more features
types from provided annotation. And the vari-
ance of SRL performance with different open fea-
tures is usually less than 1%. To clearly show the
contribution of extra external resources, these less
contributing features (siblings, named entity labels
and WordNet sense) are not used in our submitted
?open? runs.
MRSes as features to SRL As a novel point of
our SRL system, we incorporate parsing results
from a linguistic grammar-based parsing system
in our ?open? competition run. In this experi-
ment, we used English Resource Grammar (ERG,
Flickinger (2000)), a precision HPSG grammar for
English. For parsing, we used PET (Callmeier,
2001), an efficient HPSG parser, together with ex-
tended partial parsing module (Zhang et al, 2007)
for maximum robustness. The grammar is hand-
crafted and the disambiguation models are trained
on Redwoods treebanks. They present general lin-
guistic knowledge, and are not tuned for the spe-
cific domains in this competition.
While the syntactic analysis of the HPSG gram-
mar is largely different from the dependency anno-
tation used in this shared task, the semantic rep-
resentations do share a similar view on predicate-
argument structures. ERG uses as its semantic
representation the Minimal Recursion Semantics
(MRS, Copestake et al (2005)), a non-recursive flat
structure that is suitable for underspecifying scope
ambiguities. A predicate-argument backbone of
MRS can be extracted by identifying shared vari-
ables between elementary predications (??s).
In order to align the HPSG parser?s I/O with
CoNLL?s annotation, extensive mapping scripts
are developed to preprocess the texts, and extract
backbone from output MRSes. The unknown word
handling techniques (Zhang and Kordoni, 2005)
are used to overcome lexical gaps. Only the best
analysis is used for MRS extraction. Without par-
tial parsing, the ERG achieves around 70% of raw
coverage on the training data. When partial pars-
ing is used, almost all the sentences received ei-
ther full or partial analysis (except for several cases
where computational resources are exhausted), and
the SRL performance improves by ?0.5%.
5 Results
Among 20+ participant groups, our system ranked
seventh in the ?closed? competition, and first in
the ?open? challenge. The performance of the syn-
tactic and semantic components of our system are
summarized in Table 2.
In-Domain Out-Domain
Lab. Unlab. Lab. Unlab.
Syntactic Dep. 88.14% 90.78% 80.80% 86.12%
S
R
L
Closed 72.67% 82.68% 60.16% 76.98%
Open 73.08% 83.04% 62.11% 78.48%
Table 2: Labeled and unlabeled attachment scores
in syntactic dependency parsing and F1 score for
semantic role labeling.
The syntactic voting and semantic labeling parts
of our system are implemented in Java together
with a few Perl scripts. Using the open source
TADM for parameter estimation, our the voting
component take no more than 1 minute to train and
10 seconds to run (on WSJ testset). The SRL com-
ponent takes about 1 hour for training, and no more
than 30 seconds for labeling (WSJ testset).
Result analysis shows that the combination of
the two state-of-the-art parsers delivers good syn-
tactic dependencies (ranked 2nd). Error analysis
shows most of the errors are related to preposi-
tions. One category is the syntactic ambiguity of
pp-attachment, e.g. in ?when trading was halted
in Philip Morris?, ?in? can be attached to either
?trading? or ?halted?. The other category is the
LOC and TMP tags in phrases like ?at the end of
the day?, ?at the point of departure?, etc.
201
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
P
l
e
m
m
a
P
P
O
S
P
r
e
l
P
-
p
a
r
e
n
t
P
O
S
A
l
e
m
m
a
A
P
O
S
A
r
e
l
P
-
c
h
i
l
d
r
e
n
P
O
S
e
s
P
-
c
h
i
l
d
r
e
n
r
e
l
P
-
A
p
a
t
h
A
-
c
h
i
l
d
r
e
n
P
O
S
e
s
A
-
c
h
i
l
d
r
e
n
r
e
l
s
P
p
r
e
c
e
d
e
s
A
?
A
?
s
p
o
s
i
t
i
o
n
P
-
s
i
b
l
i
n
g
s
P
O
S
e
s
P
-
s
i
b
l
i
n
g
s
r
e
l
s
P
N
E
l
a
b
e
l
P
W
N
s
e
n
s
e
P
M
R
S
?
?
-
n
a
m
e
P
M
R
S
-
a
r
g
s
l
a
b
e
l
s
P
M
R
S
-
a
r
g
s
P
O
S
e
s
A
M
R
S
?
?
-
n
a
m
e
A
M
R
S
-
p
r
e
d
s
l
a
b
e
l
s
A
M
R
S
-
p
r
e
d
s
P
O
S
e
s
PI ? ? ? ? ? ? ? ?   
AI ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?      
AC ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?      
PC ? ? ? ? ? ? ? ?   
Table 1: Feature types used in semantic role labeling sub-components. Feature types marked with ? are
used in the ?closed? run; feature types marked with  are used in the ?open? run; feature types marked
with ? are used in both runs. P denotes predicate; A denotes semantic argument.
The results on semantic role labeling show,
sometimes, even with syntactic errors of
LOC/TMP tags, the semantic role labeler can
still predict AM-LOC/AM-TMP correctly, which
indicates the robustness of our hybrid approach.
By comparing our ?closed? and ?open? runs, the
MRS features do introduce a clear performance
improvement. The performance gain is even
more significant in out-domain test, showing that
the MRS features from ERG are indeed much less
domain dependent. Another example worth men-
tioning is that, in the sentence ?Scotty regarded the
ear and the grizzled hair around it with a moment
of interest?, it is extremely difficult to know that
?Scotty? is a semantic role of ?interest?.
Also, we are the only group that submitted runs
for both tracks, and achieved better performance
in open competition. Although the best ways of
integrating deep linguistic processing techniques
remain as an open question, the achieved results
at least show that hand-crafted grammars like ERG
do provide heterogeneous linguistic insights that
can potentially find their usage in data-driven NLP
tasks as such.
6 Conclusion
In this paper, we described our hybrid system
on both syntactic and semantic dependencies la-
beling. We built a voting model to combine
the results of two state-of-the-art syntactic depen-
dency parsers, and a pipeline model to combine
deep parsing results for SRL. The experimental re-
sults showed the advantages of our hybrid strat-
egy, especially on the cross-domain data set. Al-
though the optimal ways of combining deep pro-
cessing techniques remains to be explored, the
performance gain achieved by incorporating hand-
crafted grammar outputs shows a promising direc-
tion of study for both fields.
References
Callmeier, Ulrich. 2001. Efficient parsing with large-scale
unification grammars. Master?s thesis, Universit?at des
Saarlandes, Saarbr?ucken, Germany.
Copestake, Ann, Dan Flickinger, Carl J. Pollard, and Ivan A.
Sag. 2005. Minimal recursion semantics: an introduction.
Research on Language and Computation, 3(4):281?332.
Flickinger, Dan. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28.
Hacioglu, Kadri. 2004. Semantic role labeling using de-
pendency trees. In Proceedings of COLING 2004, pages
1273?1276, Geneva, Switzerland, Aug 23?Aug 27.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Nivre, Joakim, Jens Nilsson, Johan Hall, Atanas Chanev,
G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and Er-
win Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(1):1?41.
Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu??s
M`arquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th Conference
on Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Zhang, Yi and Valia Kordoni. 2005. A statistical approach
towards unknown word type prediction for deep grammars.
In Proceedings of the Australasian Language Technology
Workshop 2005, pages 24?31, Sydney, Australia.
Zhang, Yi, Valia Kordoni, and Erin Fitzgerald. 2007. Partial
parse selection for robust deep processing. In Proceed-
ings of ACL 2007 Workshop on Deep Linguistic Process-
ing, pages 128?135, Prague, Czech.
202
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 11?18,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
An Extensible Crosslinguistic Readability Framework
Jesse Saba Kirchner
Department of Linguistics
UC Santa Cruz
1156 High Street
Santa Cruz, CA 95064
kirchner@ucsc.edu
Justin Nuger
Department of Linguistics
UC Santa Cruz
1156 High Street
Santa Cruz, CA 95064
jnuger@ucsc.edu
Yi Zhang
Baskin School of Engineering
UC Santa Cruz
1156 High Street, SOE 3
Santa Cruz, CA 95064
yiz@soe.ucsc.edu
Abstract
Automatic assessment of the readability
level (i.e., the relative linguistic complex-
ity) of documents in a large number of
languages is an important problem that
can be applied to many real-world appli-
cations, such as retrieving age-appropriate
search engine results for kids, construct-
ing automatic tutoring systems, and so on.
Unfortunately, existing readability label-
ing techniques have only been applied to
a very small number of languages. In this
paper, we present an extensible crosslin-
guistic readability framework based on the
use of parallel corpora to quickly create
readability software for thousands of lan-
guages, including languages for which no
linguists are available to define readability
rules or for which documents with read-
ability labels are lacking to train readabil-
ity models. To demonstrate our idea, we
developed a system based on the proposed
framework. This paper discusses the theo-
retical and practical issues involved in de-
signing such a system and presents the re-
sults of an experiment conducted with the
system.
1 Introduction
Automatically labeling the reading difficulty of an
arbitrary document is an important problem in sev-
eral human language technology applications. It
can, for example, be used in the next generation of
personalized information retrieval systems to find
documents tailored to children at different grade
levels. In a tutoring system, it can be used to find
online reading materials of the appropriate diffi-
culty level for students (Heilman et al, 2006).
Of the world?s more than 6,000 languages
(Grimes, 2005), readability classification software
exists for a striking few, and it is limited in cover-
age to languages spoken in countries with promi-
nent standing in global economics and politics.
A substantial number of the remaining languages
nevertheless have a sufficient corpus of digital
documents ? a number which may already be
in the hundreds and soon in the thousands (Pao-
lillo et al, 2005). A natural idea is to create
software to automatically predict readability lev-
els (henceforth ?RLs?) for these documents. Such
software has significant potential for applications
in different areas of research, such as creating web
search engines for kids speaking languages not
covered by existing readability software, as de-
scribed above.
There is much research on assessing the read-
ing difficulties of texts in a particular language,
and the existing work can be roughly classified as
falling under two approaches. The first approach
uses manually or semi-automatically crafted rules
designed by computational linguists who are fa-
miliar with the language in question (Anderson,
1981). The second approach learns readability
models for a particular language based on labeled
data (Collins-Thompson and Callan, 2004).
Unfortunately, existing approaches cannot be
easily extended to handle thousands of different
languages. The first approach, using rules de-
vised by computational linguists familiar with the
languages, is impractical because for many lan-
guages, especially minority or understudied lan-
guages, there are relatively few linguists suffi-
ciently familiar with the language to design such
software. Even if these linguists exist, it is un-
likely that a search engine company that wanted to
serve the whole world would have the resources to
11
hire all of them. The second approach, using ma-
chine learning techniques on labeled data, is very
expensive because it requires the support of edu-
cated speakers of each language to provide read-
ability labels for documents in the language. The
availability of such speakers cannot always be as-
sumed. Again, recruiting annotators for thousands
of different languages is not economically feasible
or practical for a company. An alternative strategy
that can scale to thousands of different languages
is needed.
In this paper, we propose a general framework
to solve this problem based on a parallel corpus
crawled from the web. To illustrate the idea, we
developed an Extensible Crosslinguistic Readabil-
ity system (henceforth ?ECR system?), which uses
a Cross-Lingual Information Retrieval (henceforth
?CLIR?) system that we call EXCLAIM. The ECR
system functions to create RL classification soft-
ware in any language with sufficient coverage in
the CLIR system. We also report the promising ?
though very preliminary ? results of an experi-
ment that tests a real-world application of this sys-
tem. Investigation of the basic assumptions and
generalization of parameters and evaluation met-
rics are left for future work.
The rest of this paper is organized as follows.
The problem setting is described in Section 2. The
architecture of our ECR system is explained in
Section 3. Our experimental design is laid out in
Section 4, followed by experimental result analy-
sis in Section 5. Section 6 gives an overview of
related work, and section 7 concludes.
2 Problem and Proposed Methodology
2.1 Existing Approaches to Readability
Classification
In traditional approaches to computational read-
ability classification, there is a variety of language-
specific system requirements needed in order to
perform the RL classification task. For some
languages, this task is relatively well-studied.
For example, the simple and widely-used Laes-
barhedsindex (henceforth ?LIX?) calculates RLs
for texts written in Western European languages1
with the following LIX formula:
RLD = wordssentences +
100 ? wordschar>6
words
1In practice, LIX may be substituted with other metrics,
such as Flesch-Kincaid.
where D is a document written in an unfamiliar
language, and RLD is the readability score of the
document D.
The above formula relies on specific parame-
ters which have been tuned to a certain set of lan-
guages. These include the total number of words
in D (words), the total number of sentences in D
(sentences), and the total number of words in D
with more than six characters (wordschar>6 ).
Although this formula may be successful in
RL classification for languages like English and
French (Bjo?rnsson and Ha?rd af Segerstad(1979),
Anderson (1981)), it remains essentially parochial
in the context of other languages because the pa-
rameters overfit the data from the Western Euor-
pean languages for which it was designed. Since
the LIX formula depends on measuring the num-
ber of characters in a word to find words greater
than 6, it is ineffective in determining the readabil-
ity of documents written in languages with differ-
ent writing systems, such as Chinese. This is due
to the fact that some languages, like Chinese, are
written with characters based on semantic mean-
ing rather than phonemes, as in English, and a
large number of Chinese words consist of just one
or two characters, regardless of semantic complex-
ity (Li and Thompson, 1981). In a similar vein,
many languages of the world (even some that use
phonemically-based writing systems) do not ad-
here to the implicit assumption of the LIX formula
that semantically ?complex? words are longer than
simpler words (Greenberg, 1954). In these lan-
guages, then, the same metric cannot be used as a
valid measure of RL difficulty of documents, since
word length does not correlate with semantic com-
plexity.
One recent alternative approach has been devel-
oped for readability labeling that uses multiple sta-
tistical language models (Collins-Thompson and
Callan, 2004). The idea is to train statistical lan-
guage models for each grade level automatically
from manually labeled training documents. How-
ever, even an approach like this is not scalable to
handle thousands of languages, since it is hard to
recruit annotators of all of these languages to man-
ually label the training data.
2.2 Proposed Solution
We propose a scalable solution to the problem of
labeling the readability of documents in many lan-
guages. The general idea is to combine CLIR
12
technology with off-the-shelf readability software
for at least one well-studied language, such as
English. First, off-the-shelf readability software
is used to assign RLs to a set of documents in
the source language, e.g. English, which serve as
training data. Second, a set of key terms is se-
lected from each group of documents correspond-
ing to a particular RL to construct a readability
model for that RL. Third, for each of these sets
of terms, the cross-lingual query-expansion com-
ponent of the CLIR system returns a semantically
relevant set of terms in the target language. Fi-
nally, these target-language term sets are used to
build the target-language RL models, which can
be used to assign RLs to documents in the tar-
get language, even if language-specific readability
classification software does not exist for that lan-
guage. This solution plausibly extends to any of
the languages covered by the CLIR system. It is
possible to create a CLIR system by crawling the
internet for parallel corpora, which exist for many
language pairs. As a result, the proposed solution
already has the potential to cover many different
languages.
The success of this method relies on the as-
sumption that readability levels remain fairly con-
stant across syntactically and semantically paral-
lel documents in the two languages in question,
or simply across documents typified by equivalent
key terms. This does not seem unreasonable: if the
same information is represented in two different
languages in semantically and structurally compa-
rable ways, it is likely that the reading difficulty of
the two texts should not differ much, if at all. If
this assumption is true, generation of readability
software really depends only on the availability of
a solid CLIR system, and the problem of requir-
ing trained computational linguists and native lan-
guage speakers to design the system is mitigated.
Figure 1 shows a simple process model of a sys-
tem for generating RL classifiers for various lan-
guages. A set of training documents from a source
language (i.e., the ?L1? in Figure 1) is assigned
RLs by the off-the-shelf RL classification soft-
ware R(L1). Using the source langauge files and
the RLs produced by R(L1), the ECR system pro-
duces a source language (L1) readability model.
Through the system interface, the CLIR system
(EXCLAIM) uses the L1 readability model to pro-
duce a target language (L2) readability model. The
system uses the L2 readability model to produce a
Figure 1: ECR Domain
new RL classifier R(L2) for the target language.
The newly developed classifier R(L2) can then be
used to classify documents in the L2.
3 System Architecture
To address any theoretical or empirical concerns
and questions about the proposed solution, includ-
ing those relating to the assumption that key term
equivalence correlates with RL equivalence, we
have developed an ECR system compatible with
an existing CLIR system and have proposed eval-
uation metrics for this system. We developed
the ECR system to meet the needs of two differ-
ent kinds of users. First, higher-level intermedi-
ate users can build RL classification software for
a given target language. Second, end users can
use the software to classify documents in that lan-
guage. In this section, we give a developer?s-eye
view of the system architecture (shown in Figure
2), making specific reference to the points at which
intermediate and end users may interact with the
system. For presentational clarity, we periodically
adopt the arbitrary assumption that the source lan-
guage is English, as this is the source language of
our experiment described in the following section.
The ECR system has three primary tasks. The
first task is to enable intermediate users to develop
RL classification model for the source language.
The second task is to provide the intermediate user
with a toolkit to construct language-specific soft-
ware that automatically tags documents in the tar-
get language with the appropriate RLs. The final
task is to provide an interface module for the end
13
Figure 2: ECR System design
user to utilize this software.
In order to approach the first task, one needs a
set of documents in a source language for which
off-the-shelf readability software is available. This
set of documents functions as a training data set;
if a user is trying to assign RLs to documents
in a particular domain ? e.g., forestry, medical,
leisure, etc. ? then (s)he can already help shape
the results of the system by providing domain-
relevant source langauge data at this stage. To
aid the intermediate user in obtaining RLs for this
set of data, the ECR system has a number of pa-
rameters that may be selected, based on different
models of RL-tagging ? for example, we selected
English as the source language and the aforemen-
tioned LIX formula due to its simplicity. The doc-
uments are then organized according to the gener-
ated RLs and separated into different RL groups.
At this point, the K most salient words are
extracted from each source language RL groups
(RLS) based on the following tf*idf term weight-
ing:2
wi ,j =
(
0.5 + 0.5 freqi,jmaxl freql,j
)
? logNni
2In principle, this choice is arbitrary and any other appro-
priate term-weighting formula could also be used.
The selected words RLS = {f1 , f2 , ...fK }
form the basis for constructing an RL classifica-
tion model for an unknown target language.
In order to construct a target language RL clas-
sification model, the cross-lingual query expan-
sion component of a CLIR system is necessary
to select semantically comparable and semanti-
cally related words in the target language. The
CLIR system we developed is called EXCLAIM,
or the EXtensible Cross-Linguistic Automatic
Information Machine. We constructed EXCLAIM
from a semantically (though not structurally) par-
allel corpus crawled from Wikipedia (Wikime-
dia Foundation, 1999). All Wikipedia articles with
both source and target language versions collec-
tively function as data to construct the CLIR com-
ponent. Due to Wikipedia?s coverage of a large
amount of languages (English being the language
with the largest collection of articles at the time
of writing), CLIR components for English paired
with a wide number of target languages was cre-
ated for EXCLAIM.
For each RLS, the query-expansion component
of EXCLAIM determines a set of corresponding
words for the target language RLT. Initially, each
word in RLS is matched with the source language
document in EXCLAIM for which it has the highest
tf*idf term weight. The M most salient terms in
the corresponding target language document (cal-
culated once again using the tf*idf formula) are
then added to RLT. Therefore, RLT contains no
more than K ? M terms. The total set of RLTs
form the base of the target language readability
classification model.
Using this model, the system generates target
language readability classification software on the
fly, which plugs into the system?s existing inter-
face module for end users. Through the module,
the end user can use the newly generated software
to determine RLs for a set of target language doc-
uments without requiring any specialized knowl-
edge of the languages or the software development
process.
4 Experimental Design
We conducted an experiment to demonstrate this
idea and to test our ECR system. Without loss
of generality, we chose English as our source lan-
guage and Chinese as our target language. While
Chinese is a major language for which it would
be relatively easy to find linguistic experts to write
14
readability rules and native speakers to label doc-
ument readability for training, our goal is not to
demonstrate that the proposed solution is the best
solution to build readability software for Chinese.
Instead, we chose these languages for the follow-
ing reasons. First, we are capable of reading both
languages and are thus able to judge the quality of
the ECR system. Second, publicly available En-
glish readability labeling software exists, and we
are not aware of such software for Chinese. Third,
we had access to a parallel set of documents that
could be used for the evaluation of our experiment.
Fourth, the many differences between English and
Chinese might demonstrate the applicability of our
system for a diverse set of languages. However,
the features that made Chinese a desirable target
language for us are not essential for the proposed
solution, and do not affect the extensibility of the
approach.
We created a test set using a collection of
Chinese-English parallel documents from the
medical domain (Chinese Community Health Re-
source Center, 2004). The set comprised 65 docu-
ments in English and their human-translated Chi-
nese translations. Although a typical user does
not need to have access to sets of bilingual doc-
uments for the system to run successfully, we cir-
cumvented both the lack of off-the-shelf Chinese
readability labeling software and the lack of la-
beled Chinese documents for the evaluation of the
results of our system by using a high quality trans-
lated parallel document set. Since RLs are rough
measures of semantic and structural complexity,
we assume they should be approximately if not
exactly the same for a given document and its
translation in a different language, an extension of
the ideas in Collins-Thompson and Callan (2004).
Based on this assumption, we can accurately com-
pare the RLs of the translated CCHRC Chinese
medical documents to the RLs of the original En-
glish documents, which we call the ?true RLs? of
the testing documents.
LIX-based RLs can be roughly mapped to grade
levels, e.g., a text that is classified with an RL of
8 is appropriate for the average 8th grade reader.
Since we can assign RLs to the English versions
of the 65 CCHRC documents, these RLs can serve
as targets to match when generating RLs for the
corresponding Chinese versions of the same docu-
ments.
An advantage of our system arises from a com-
plete vertical integration which allows a user with
knowledge of the eventual goal to help shape the
development of the target language RL classifica-
tion model and software. In our case, the target
language (Chinese) test set was from the medical
domain, so we selected the OHSU87 medical ab-
stract corpus as an English data set. We automati-
cally classified the OHSU87 documents using the
LIX mapping schema assigned by the UNIX Dic-
tion and Style tools,3 given in the following Table.
LIX Index RL LIX Index RL
Under 34.0 4 48.0-50.9 9
34.0-37.9 5 51.0-53.9 10
38.0-40.9 6 54.0-56.9 11
41.0-43.9 7 57.0 and over 12
44.0-47.9 8
Table 1: Mapping of LIX Index scores to RLs as
assigned by Diction
Then, we concatenated the English OHSU87 doc-
uments in each RL group. The tf*idf formula was
used to select the K English words most represen-
tative of each RL group.
Next, we automatically selected a set of Chi-
nese words for each RL class to create a corre-
sponding Chinese readability model by passing
each English word through the CLIR system, EX-
CLAIM, to retrieve the most relevant English doc-
ument in the Wikipedia corpus, where relevance
is measured using the tf*idf vector space model.
The top M Chinese words from the corresponding
Chinese document in the parallel Wikipedia cor-
pus were added to RLT. By repeating this pro-
cess for each word of each RL class, the Chinese
readability model was constructed. In our exper-
iment, we set K = 50 and M = 10 arbitrarily.
The ECR system then automatically generated the
subsequent RL classification software for Chinese.
Finally, we assigned a RL to each document in
the test set. At this point the procedure is essen-
tially similar to document retrieval task. Each RL
group?s set of words RLT was treated as a docu-
ment (dj ), and each test document to be labeled
was treated as a query (q). RLs were ranked based
on the cosine similarity between RLT and q. Fi-
nally, the top-ranked RL was assigned to each test
document.
3Available online at http://www.gnu.org/software
/diction/diction.html.
15
5 Empirical Results
The results are presented below in Table 2. The
RL assigned to each Chinese document is com-
pared to the ?true RL? of the English document, on
the assumption that translation does not affect the
readability level. Although only 7.8% of the RLs
were predicted accurately (i.e., the highest ranked
RL for the Chinese document corresponded iden-
tically to the RL of the translated English docu-
ment), over 50% were either perfectly accurate or
off by only one RL.
Correctly predicted RL 7.8%
RL off by 1 grade level 43.1%
RL off by 2 grade levels 18.4%
RL off by 3 grade levels 18.4%
RL off by 4 grade levels 6.1%
RL off by 5 grade levels 3.1%
RL off by 6 grade levels 0%
RL off by 7 grade levels 3.1%
RL off by 8 grade levels 0%
Table 2: Distribution of RLs as predicted by our
ECR system
This table motivates us to represent the results
in a more comprehensive fashion. Intuitively, the
system tends to succeed at assigning RLs near the
correct level, though not necessarily at the exact
level. To quantify this intuition, we used Root
Mean Squared Error (RMSE) to evaluate the ex-
perimental results. We compared our results to
two kinds of baseline RL assignments. The first
method was to randomly assign RLs 1000 times
and take the average of the RMSE obtained in
each assignment; this yielded an average RMSE
of 3.05. The second method used a fixed equal
distribution of the nine RLs, applying each RL to
each document an equal number of times, and tak-
ing the average of these results. This baseline re-
turned an average RMSE of 3.65. The average
RMSE of our ECR system?s performance on the
CCHRC Chinese documents is 2.48. This number
compares favorably against both of the baseline al-
gorithms.
Recall that the actual RL-tagging procedure has
been treated as a document retrieval task, using
Vector Space Cosine similarity. As such, RLs are
not simply ?picked out? for each document: each
document receives a cosine similarity score for
each RL, calculated on the basis of its similarity to
the language model word set constructed for each
RL. For the results above, only the top ranked RL
was considered, as this would be the RL yielded if
the user wanted a discrete numeric value to assign
to the text. If we allow for enough flexibility to se-
lect the better of the two top-ranked RLs assigned
to each document by our ECR system, the results
are as given in Table 3.
Correctly predicted RL 10.8%
RL off by 1 grade level 49.2%
RL off by 2 grade levels 27.7%
RL off by 3 grade levels 7.7%
RL off by 4 grade levels 1.5%
RL off by 5 grade levels 0%
RL off by 6 grade levels 3.1%
RL off by 7 grade levels 0%
RL off by 8 grade levels 0%
Table 3: RL Distribution (Best of Two Top-
Ranked RLs)
While this extra selection is certain to improve
the RMSE, what is surprising is the extent to
which the RMSE improves. Once again, RMSE
can be calculated in the following way. The two
top-ranked RLs for each document are taken into
consideration, and of these two RLs, the RL near-
est to the true RL is selected. Selecting the best of
the two top-ranked RLs causes the RMSE to drop
to 1.91.
6 Related Work
The method described above builds on recent work
that has exploited the web and parallel corpora to
develop language technologies for minority lan-
guages (Trosterud (2002), inter alia).
Yarowsky et al (2001) describe a system and
a set of algorithms for automatically deriving au-
tonomous monolingual POS-taggers, base noun-
phrase bracketers, named-entity taggers, and mor-
phological analyzers for an arbitrary target lan-
guage. Bilingual text corpora are treated with
existing text analysis tools for English, and their
output is projected onto the target language via
statistically derived word alignments. Their ap-
proach is especially interesting insofar as the sys-
tem does not require hand-annotation of target-
language training data or virtually any target-
language-specific knowledge or resources.
Martin et al (2003) present an English-Inuktitut
aligned parallel corpus, demonstrating superior
16
sentence alignment via Pointwise Mutual Informa-
tion (PMI). Their approach provides broad cov-
erage of cross-linguistic morphology, which has
implications for dictionary expansion tasks; prob-
lems encountered in dealing with the agglutina-
tive morphology of Inuktitut are suggestive of the
myriad issues arising from cross-language com-
parisons.
Rogati et al (2003) present an unsupervised
learning approach to building an Arabic stemmer,
modeled on statistical machine translation. The
authors use an English stemmer and a small par-
allel corpus as training resources, with no parallel
text necessary after the training phase. Additional
monolingual texts can be incorporated to improve
the stemmer by allowing it to adapt to a specific
domain.
While Yarowsky et al (2001), Martin et
al. (2003) and Rogati et al (2003) all focus
on aligned parallel corpora, our approach dif-
fers in that we use comparable documents from
Wikipedia are linked thematically on the basis
of semantic content alone: there is no presumed
structural or lexical alignment between parallel
documents. We have adapted the methods used
in conjunction with aligned parallel corpora for
use with non-aligned parallel corpora to handle
the task pursued by Collins-Thompson and Callan
(2004), which presents a new approach to predict-
ing the RLs of a document by evaluating readabil-
ity in terms of statistical language modeling. Their
approach employs multiple language models to es-
timate the most likely RL for each document.
This approach contrasts with other previous
monolingual methods of calculating readability,
such as Chall and Dale (1995), which assesses
the readability of texts by calculating the percent-
age of terms that do not appear on a 3,000 word
list that 80% of tested fourth-grade students were
able to read. Similarly, Stenner et al (1988) use
the word frequency information from a 5-million-
word corpus.
While our work has drawn from several tech-
niques employed in prior research, we have mainly
hybridized the technique of using parallel cor-
pus employed by Yarowsky (2001) and the lan-
guage modeling approach employed by Collins-
Thompson and Callan (2004). Our approach relies
on parallel corpora to build a readability classi-
fier for one language based on readability software
for another language. Rather than focusing on
language-specific readability classification based
on training data drawn from the same language
as the testing data (Collins-Thompson and Callan,
2004), we have constructed a radically extensible
tool that can easily create readability classifiers
for an arbitrary target language using training data
from a source language such as English. The result
is a system capable of allowing a user to construct
readability software for languages like Indonesian,
for example, even if that user does not speak In-
donesian ? this is possible due to the large paral-
lel English-Indonesian corpus on Wikipedia.
7 Conclusion
We have proposed a general framework to quickly
construct a standalone readability classifier for
an arbitrary (and possibly unfamiliar) language
using statistical language models based both on
monolingual and non-aligned parallel corpora. To
demonstrate the proposed idea, we developed an
Extensible Crosslingual Readability system. We
evaluated the system on the task of predicting
readability level of a set of Chinese medical docu-
ments. The experimental results show that the pre-
dicted RLs were correct or nearly correct for over
50% of the documents. This research is important
because it is the only technique we are aware of
that is capable of straightforwardly creating read-
ability labels for hundreds, or theoretically even
thousands, of different languages.
Although the general framework and architec-
ture of the proposed system are straightforward,
the details of implementation of the system mod-
ules could be further improved to achieve bet-
ter performance. For example, all target lan-
guage words are selected from a single ?best-
matching document? using EXCLAIM in this pa-
per. Further experimentation might discover a
better word selection module. Future work may
also reveal delineation points for over- and under-
specialized sets of training data. The OHSU87
data set was selected on the basis of its medical
domain coverage, however it may not have pro-
vided broad enough coverage of the appropriate
domain-independent vocabulary in the CCHRC
documents. And finally, we conducted the ex-
periment using our own CLIR system, EXCLAIM,
while other CLIR systems might yield better re-
sults.
17
Acknowledgements
The research reported here was partly supported
by NSF Grant #BCS-0846979 and the Institute of
Education Sciences, US Department of Education,
through Grant R305A00596 to the University of
California, Santa Cruz. Any opinions, findings,
conclusions or recommendations expressed in this
paper are the authors?, and do not necessarily re-
flect those of the sponsors.
References
Jonathan Anderson. 1981. Analysing the readability
of English and non-English texts in the classroom
with Lix. Paper presented at the Annual Meeting of
the Australian Reading Association.
C. H. Bjo?rnsson and Birgit Ha?rd af Segerstad. 1979.
Lix pa? Franska och tio andra spra?k. Pedagogiskt
centrum, Stockholms skolfo?rvaltning.
Jeanne S. Chall and Edgar Dale. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline, Cambridge, Mass.
Chinese Community Health Resource Center. 2004.
CCHRC Medical Documents. Retrieved Decem-
ber 9, 2006, fromhttp://www.cchphmo.com/
cchrchealth/index E.html.
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 2004.
ACL.
Joseph H. Greenberg. 1954. A quantitative approach
to the morphological typology of language. In
Method and Perspective in Anthropology: Papers
in Honor of Wilson D. Wallis, pages 192?220, Min-
neapolis. University of Minnesota Press.
Barbara Grimes. 2005. Ethnologue: Languages of the
World, 15th ed. Summer Institute of Linguistics.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom
success of an intelligent tutoring system for lexical
practice and reading comprehension. In Proceed-
ings of the Ninth International Conference on Spo-
ken Language Processing.
Charles N. Li and Sandra Thompson. 1981. Mandarin
Chinese: A Functional Reference Grammar. Uni-
versity of California Press.
Joel Martin, Howard Johnson, Benoit Farley, and Anna
Maclachlan. 2003. Aligning and using an English-
Inuktitut parallel corpus. In Proceedings of the HLT-
NAACL 2003 workshop on building and using par-
allel texts: Data driven machine translation and be-
yond. ACL.
John Paolillo, Daniel Pimienta, and Daniel Prado.
2005. Measuring Linguistic Diversity on the Inter-
net. UNESCO, France.
Monica Rogati, Scott McCarley, and Yiming Yang.
2003. Unsupervised learning of arabic stemming us-
ing a parallel corpus. In Proceedings of the 41st an-
nual meeting of the Association for Computational
Linguistics. ACL.
A.J. Stenner, I. Horabin, D.R. Smith, and M. Smith.
1988. The Lexile Framework. Metametrics,
Durham, NC.
Trond Trosterud. 2002. Parallel corpora as tools for
investigating and developing minority languages. In
Parallel corpora, parallel worlds, pages 111?122.
Rodopi.
Wikimedia Foundation. 1999. Wikipedia, the
free encyclopedia. Retrieved May 8, 2006, from
http://en.wikipedia.org/.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology Research, pages
161?168.
18
Discriminative Parse Reranking for Chinese with Homogeneous and
Heterogeneous Annotations
Weiwei Sun?? and Rui Wang? and Yi Zhang??
?Department of Computational Linguistics, Saarland University
?German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
{wsun,rwang,yzhang}@coli.uni-saarland.de
Abstract
Discriminative parse reranking has been
shown to be an effective technique to im-
prove the generative parsing models. In
this paper, we present a series of exper-
iments on parsing the Tsinghua Chinese
Treebank with hierarchically split-merge
grammars and reranked with a perceptron-
based discriminative model. In addition to
the homogeneous annotation on TCT, we
also incorporate the PCTB-based parsing
result as heterogeneous annotation into
the reranking feature model. The rerank-
ing model achieved 1.12% absolute im-
provement on F1 over the Berkeley parser
on a development set. The head labels in
Task 2.1 are annotated with a sequence
labeling model. The system achieved
80.32 (B+C+H F1) in CIPS-SIGHAN-
2010 Task 2.1 (Open Track) and 76.11
(Overall F1) in Task 2.2 (Open Track)1.
1 Introduction
The data-driven approach to syntactic analysis of
natural language has undergone revolutionary de-
velopment in the last 15 years, ever since the
first few large scale syntactically annotated cor-
pora, i.e. treebanks, became publicly available in
the mid-90s of the last century. One and a half
decades later, treebanks remain to be an expensive
type of language resources and only available for
1This result is achieved with a bug-fixed version of the
system and does not correspond to the numbers in the origi-
nal evaluation report.
a small number of languages. The main issue that
hinders large treebank development projects is
the difficulties in creating a complete and consis-
tent annotation guideline which then constitutes
the very basis for sustainable parallel annotation
and quality assurance. While traditional linguistic
studies typically focus on either isolated language
phenomena or limited interaction among a small
groups of phenomena, the annotation scheme in
treebanking project requires full coverage of lan-
guage use in the source media, and proper treat-
ment with an uniformed annotation format. Such
high demand from the practical application of lin-
guistic theory has given rise to a countless num-
ber of attempts and variations in the formaliza-
tion frameworks. While the harsh natural selec-
tion set the bar high and many attempts failed to
even reach the actual annotation phase, a hand-
ful highly competent grammar frameworks have
given birth to several large scale treebanks.
The co-existence of multiple treebanks with
heterogeneous annotation presents a new chal-
lenge to the consumers of such resources. The im-
mediately relevant task is the automated syntactic
analysis, or parsing. While many state-of-the-art
statistical parsing systems are not bound to spe-
cific treebank annotation (assuming the formal-
ism is predetermined independently), almost all
of them assume homogeneous annotation in the
training corpus. Therefore, such treebanks can not
be simply put together when training the parser.
One approach would be to convert them into an
uniformed representation, although such conver-
sion is usually difficult and by its nature error-
prune. The differences in annotations constitute
different generative stories: i.e., when the pars-
ing models are viewed as mechanisms to produce
structured sentences, each treebank model will as-
sociate its own structure with the surface string in-
dependently. On the other hand, if the discrimina-
tive view is adopted, it is possible to use annota-
tions in different treebanks as indication of good-
ness of the tree in the original annotation.
In this paper, we present a series of experi-
ments to improve the Chinese parsing accuracy
on the Tsinghua Chinese Treebank. First, we use
coarse-to-fine parsing with hierarchically split-
merge generative grammars to obtain a list of can-
didate trees in TCT annotation. A discriminative
parse selection model is then used to rerank the
list of candidates. The reranking model is trained
with both homogeneous (TCT) and heterogeneous
(PCTB) data. A sequence labeling system is used
to annotate the heads in Task 2-1.
The remaining part of the paper is organized as
follows. Section 2 reviews the relevant previous
study on generative split-merge parsing and dis-
criminative reranking models. Section 3 describes
the work flow of our system participated in the
CIPS-SIGHAN-2010 bake-off Task 2. Section 4
describes the detailed settings for the evaluation
and the empirical results. Section 5 concludes the
paper.
2 Background
Statistical constituent-based parsing is popular-
ized through the decade-long competition on pars-
ing the Wall Street Journal sections of the English
Penn Treebank. While the evaluation setup has
for long seen its limitation (a frustratingly low
of 2% overall improvement throughout a decade
of research), the value of newly proposed pars-
ing methods along the way has clearly much more
profound merits than the seemly trivial increase in
evaluation figures. In this section we review two
effective techniques in constituent-based statisti-
cal parsing, and their potential benefits in parsing
Chinese.
Comparing with many other languages, statisti-
cal parsing for Chinese has reached early success,
due to the fact that the language has relatively
fixed word order and extremely poor inflectional
morphology. Both facts allow the PCFG-based
statistical modeling to perform well. On the other
hand, the much higher ambiguity between basic
word categories like nouns and verbs makes Chi-
nese parsing interestingly different from the situ-
ation of English.
The type of treebank annotations also affects
the performance of the parsing models. Tak-
ing the Penn Chinese Treebank (PCTB; Xue
et al (2005)) and Tsinghua Chinese Treebank
(TCT; Zhou (2004)) as examples, PCTB is anno-
tated with a much more detailed set of phrase cat-
egories, while TCT uses a more fine-grained POS
tagset. The asymmetry in the annotation informa-
tion is partially due to the difference of linguis-
tic treatment. But more importantly, it shows that
both treebanks have the potential of being refined
with more detailed classification, on either phrasal
or word categories. One data-driven approach to
derive more fine-grained annotation is the hierar-
chically split-merge parsing (Petrov et al, 2006;
Petrov and Klein, 2007), which induces subcat-
egories from coarse-grained annotations through
an expectation maximization procedure. In com-
bination with the coarse-to-fine parsing strategy,
efficient inference can be done with a cascade
of grammars of different granularity. Such pars-
ing models have reached (close to) state-of-the-art
performance for many languages including Chi-
nese and English.
Another effective technique to improve parsing
results is discriminative reranking (Charniak and
Johnson, 2005; Collins and Koo, 2005). While
the generative models compose candidate parse
trees, a discriminative reranker reorders the list
of candidates in favor of those trees which max-
imizes the properties of being a good analysis.
Such extra model refines the original scores as-
signed by the generative model by focusing its de-
cisions on the fine details among already ?good?
candidates. Due to this nature, the set of features
in the reranker focus on those global (and poten-
tially long distance) properties which are difficult
to model with the generative model. Also, since
it is not necessary for the reranker to generate the
candidate trees, one can easily integrate additional
external information to help adjust the ranking of
the analysis. In the following section, we will de-
Berkeley 
Parser
...
Parse 
Reranker
TCT
Head
Classifier
...
H
H
H
A B
C D
C
D B
A
C
Task 2.1
Task 2.2
Open
e.g. ?? ??? ? ??
PCTB
Parser
Figure 1: Workflow of the System
scribe the reranking model we developed for the
CIPS-SIGHAN-2010 parsing tasks. We will also
show how the heterogeneous parsing results can
be integrated through the reranker to further im-
prove the performance of the system.
3 System Description
In this section, we will present our approach
in detail. The whole system consists of three
main components, the Berkeley Parser, the Parse
Reranker, and the Head Classifier. The workflow
is shown in Figure 1. Firstly, we use the Berke-
ley Parser trained on the TCT to parse the in-
put sentence and obtain a list of possible parses;
then, all the parses2 will be re-ranked by the Parse
Reranker; and finally, the Head Classifer will an-
notate the head information for each constituent
2In practice, we only take the top n parses. We have dif-
ferent n values in the experiment settings, and n is up to 50.
Algorithm 1: The Perptron learning proce-
dure.
input : Data {(xt, yt), t = 1, 2, ...,m}
Initialize: w? (0, ..., 0)1
for i = 1, 2, ..., I do2
for t =SHUFFLE (1, ...,m) do3
y?t =4
arg maxy?GENbestn (xt) w
>?(xt, y)
if y?t 6= yt then5
w? w+(?(xt, yt)??(xt, y?t ))6
end7
end8
wi ? w9
end10
return aw = 1I
?I
i=1 wi11
on the best parse tree. For parse reranking, we
can extract features either from TCT-style parses
or together with the PCTB-style parse of the same
sentence. For example, we can check whether
the boundary predictions given by the TCT parser
are agreed by the PCTB parser. Since the PCTB
parser is trained on a different treebank from TCT,
our reranking model can be seen as a method to
use a heterogenous resource. The best parse tree
given by the Parse Reranker will be the result for
Task 2.2; and the final output of the system will
be the result for Task 2.1. Since we have already
mentioned the Berkeley Parser in the related work,
we will focus on the other two modules in the rest
of this section.
3.1 Parse Reranker
We follow Collins and Koo (2005)?s discrimina-
tive reranking model to score possible parse trees
of each sentence given by the Berkeley Parser.
Previous research on English shows that struc-
tured perceptron (Collins, 2002) is one of the
strongest machine learning algorithms for parse
reranking (Collins and Duffy, 2002; Gao et al,
2007). In our system, we use the averaged per-
ceptron algorithm to do parameter estimation. Al-
gorithm 1 illustrates the learning procedure. The
parameter vector w is initialized to (0, ..., 0). The
learner processes all the instances (t is from 1 to
n) in each iteration (i). If current hypothesis (w)
fails to predict xt, the learner update w through
calculating the difference between ?(xt, y?t ) and
?(xt, yt). At the end of each iteration, the learner
save the current model as w + i, and finally all
these models will be added up to get aw.
3.2 Features
We use an example to show the features we extract
in Figure 2.
vp
v
?
eat
np
v
?
buy
uJDE
?
n
??
apple
Figure 2: An Example
Rules The context-free rule itself:
np? v + uJDE + np.
Grandparent Rules Same as the Rules, but
also including the nonterminal above the rule:
vp(np? v + uJDE + np)
Bigrams Pairs of nonterminals from the left to
right of the the rule. The example rule would con-
tribute the bigrams np(STOP, v), np(v,uJDE),
np(uJDE,np) and np(np, STOP).
Grandparent Bigrams Same as Bigrams, but
also including the nonterminal above the bigrams.
For instance, vp(np(STOP, v))
Lexical Bigrams Same as Bigrams, but with
the lexical heads of the two nonterminals also in-
cluded. For instance, np(STOP,?).
Trigrams All trigrams within the rule. The
example rule would contribute the trigrams
np(STOP, STOP, v), np(STOP, v,uJDE),
np(v,uJDE,np), np(uJDE,np,STOP) and
np(np,STOP,STOP).
Combination of Boundary Words and
Rules The first word and the rule (i.e.
?+(np? v + uJDE + np)), the last word
and the rule one word before and the rule, one
word after and the rule, the first word, the last
word and the rule, and the first word?s POS, last
word?s POS and the rule.
Combination of Boundary Words and Phrasal
Category : Same as combination of boundary
words and rules, but substitute the rule with the
category of current phrases.
Two level Rules Same as Rules, but also
including the entire rule above the rule:
vp? v + (np? v + uJDE + np)
Original Rank : The logarithm of the original
rank of n-best candidates.
Affixation features In order to better handle
unknown words, we also extract morphologi-
cal features: character n-gram prefixes and suf-
fixes for n up to 3. For example, for word/tag
pair ????/n, we add the following fea-
tures: (prefix1,?,n), (prefix2,??,n), (prefix3,?
??,n), (suffix1,?,n), (suffix2,??,n), (suf-
fix3,???,n).
Apart from training the reranking model using
the same dataset (i.e. the TCT), we can also use
another treebank (e.g. the PCTB). Although they
have quite different annotations as well as the data
source, it would still be interesting to see whether
a heterogenous resource is helpful with the parse
reranking.
Consist Category If a phrase is also analyzed
as one phrase by the PCTB parser, both the TCT
and PCTB categories are used as two individual
features. The combination of the two categories
are also used.
Inconsist Category If a phrase is not analyzed
as one phrase by the PCTB parser, the TCT cate-
gory is used as a feature.
Number of Consist and Inconsist phrases The
two number are used as two individual featuers.
We also use the ratio of the number of consist
phrases and inconsist phrase (we add 0.1 to each
number for smoothing), the ratio of the number
of consist/inconsist phrases and the length of the
current sentence.
POS Tags For each word, the combination of
TCT and PCTB POS tags (with or without word
content) are used.
3.3 Head Classifier
Following (Song and Kit, 2009), we apply a se-
quence tagging method to find head constituents.
We suggest readers to refer to the original paper
for details of the method. However, since the fea-
ture set is different, we give the discription of
them in this paper. To predict whether current
phrase is a head phrase of its parent, we use the
same example above (Figure 2) for convenience.
If we consider np as our current phrase, the fol-
lowing features are extracted,
Rules The generative rule, vp? v + (np).
Category of the Current Phrase and its Parent
np, vp, and (np, vp).
Bigrams and Trigrams (v, np), (np,STOP),
(STOP, v,np), and (np,STOP,STOP).
Parent Bigrams and Trigrams vp(v, np),
vp(np,STOP), vp(STOP, v, np),
vp(np,STOP,STOP).
Lexical Unigram The first word ?, the last
word ??, and together with the parent, (vp,?)
and (vp,??)
4 Evaluation
4.1 Datasets
The dataset used in the CIPS-ParsEval-2010 eval-
uation is converted from the Tsinghua Chinese
Treebank (TCT). There are two subtasks: (1)
event description sub-sentence analysis and (2)
complete sentence parsing. On the assumption
that the boundaries and relations between these
event description units are determined separately,
the first task aims to identify the local fine-grained
syntactic structures. The goal of the second task
is to evaluate the performance of the automatic
parsers on complete sentences in real texts. The
training dataset is a mixture of several genres, in-
cluding newspaper texts, encyclopedic texts and
novel texts.
The annotation in the dataset is different to
the other frequently used Chinese treebank (i.e.
PCTB) Whereas TCT annotation strongly reflects
early descriptive linguistics, PCTB draws primar-
ily on Government-Binding (GB) theory from
1980s. PCTB annotation differs from TCT anno-
tation from many perspectives:
? TCT and PCTB have different segmentation
standards.
? TCT is somehow branching-rich annota-
tion, while PCTB annotation is category-
rich. Specifically the topological tree struc-
tures is more detailed in TCT, and there
are not many flat structures. However con-
stituents are detailed classified, namely the
number of phrasal categories is small. On the
contrary, though flat structures are very com-
mon in PCTB, the categorization of phrases
is fine-grained. In addition, PCTB contains
functional information. Function tags ap-
pended to constituent labels are used to in-
dicate additional syntactic or semantic infor-
mation.
? TCT contains head indices, making head
identification of each constituent an impor-
tant goal of task 1.
? Following the GB theory, PCTB assume
there are movements, so there are empty cat-
egory annotation. Because of different theo-
retical foundations, there are different expla-
nations for a series of linguistic phenomena
such as the usage of function word ???.
In the reranking experiments, we also use a
parser trained on PCTB to provide more syntac-
tic clues.
4.2 Setting
In order to gain a representative set of training
data, we use cross-validation scheme described in
(Collins, 2000). The dataset is a mixture of three
genres. We equally split every genre data into 10
subsets, and collect three subset of different gen-
res as one fold of the whole data. In this way, we
can divide the whole data into 10 balanced sub-
sets. For each fold data, a complement parser is
trained using all other data to produce multiple hy-
potheses for each sentence. This cross-validation
n 1 2 5 10 20 30 40 50
F1 79.97 81.62 83.51 84.63 85.59 86.07 86.38 86.60
Table 1: Upper bound of f-score as a function of number n of n-best parses.
scheme can prevent the initial model from being
unrealistically ?good? on the training sentences.
We use the first 9 folds as training data and the last
fold as development data for the following exper-
iments. For the final submission of the evaluation
task, we re-train a reranking model using all 10
folds data. All reranking models are trained with
30 iterations.
For parsing experiments, we use the Berkeley
parser3. All parsers are trained with 5 iterations
of split, merge, smooth. To produce PCTB-style
analysis, we train the Berkeley parse with PCTB
5.0 data that contains 18804 sentences and 508764
words. For the evaluation of development experi-
ments, we used the EVALB tool4 for evaluation,
and used labeled recall (LR), labeled precision
(LP) and F1 score (which is the harmonic mean
of LR and LP) to measure accuracy.
For the head classification, we use SVMhmm5,
an implementation of structural SVMs for se-
quence tagging. The main setting of learning pa-
rameter is C that trades off margin size and train-
ing error. In our experiments, the head classifica-
tion is not sensitive to this parameter and we set
it to 1 for all experiments reported. For the kernel
function setting, we use the simplest linear kernel.
4.3 Results
4.3.1 Upper Bound of Reranking
The upper bound of n-best parse reranking is
shown in Table 1. From the 1-best result we see
that the base accuracy of the parser is 79.97. 2-
best and 10-best show promising oracle-rate im-
provements. After that things start to slow down,
and we achieve an oracle rate of 86.60 at 50-best.
4.3.2 Reranking Using Homogeneous Data
Table 2 summarizes the performance of the ba-
sic reranking model. It is evaluated on short sen-
3http://code.google.com/p/
berkeleyparser/
4http://nlp.cs.nyu.edu/evalb/
5http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
tences (less than 40 words) from the development
data of the task 2. When 40 reranking candidates
are used, the model gives a 0.76% absolute im-
provement over the basic Berkeley parser.
POS(%) LP(%) LR(%) F1
Baseline 93.59 85.60 85.36 85.48
n = 2 93.66 85.84 85.54 85.69
n = 5 93.62 86.04 85.73 85.88
n = 10 93.66 86.22 85.85 86.04
n = 20 93.70 86.19 85.87 86.03
n = 30 93.70 86.32 86.00 86.16
n = 40 93.76 86.40 86.09 86.24
n = 50 93.73 86.10 85.81 85.96
Table 2: Reranking performance with different
number of parse candidates on the sentences that
contain no more than 40 words in the development
data.
4.3.3 Reranking Using Heterogeneous Data
Table 3 summarizes the reranking performance
using PCTB data. It is also evaluated on short sen-
tences of the task 2. When 30 reranking candi-
dates are used, the model gives a 1.12% absolute
improvement over the Berkeley parser. Compar-
ison of Table 2 and 3 shows an improvement by
using heterogeneous data.
POS(%) LP(%) LR(%) F1
n = 2 93.70 85.98 85.67 85.82
n = 5 93.75 86.52 86.19 86.35
n = 10 93.77 86.64 86.29 86.47
n = 20 93.79 86.71 86.34 86.53
n = 30 93.80 86.72 86.48 86.60
n = 40 93.80 86.54 86.22 86.38
n = 50 93.89 86.73 86.41 86.57
Table 3: Reranking performance with different
number of parse candidates on the sentences that
contain no more than 40 words in the development
data.
Task 1 ?B+C?-P ?B+C?-R ?B+C?-F1 ?B+C+H?-P ?B+C+H?-R ?B+C+H?-F1 POS
Old data 82.37 83.05 82.71 79.99 80.65 80.32 81.87
Table 4: Final results of task 1.
Task 2 dj-P dj-R dj-F1 fj-P fj-R fj-F1 Avg. POS
Old data 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23
New data 79.60 79.13 79.36 70.01 75.94 72.85 76.11 89.05
Table 5: Final results of task 2.
4.3.4 Head Classification
The head classification performance is evalu-
ated using gold-standard syntactic trees. For each
constituent in a gold parse tree, a structured clas-
sifier is trained to predict whether it is a head con-
stituent of its parent. Table 6 shows the overall
performance of head classification. We can see
that the head classification can achieve a high per-
formance.
P(%) R(%) F?=1
98.59% 98.20% 98.39
Table 6: Head classification performance with
gold trees on the development data.
4.3.5 Final Result
Table 4 and 5 summarize the final results. Here
we use the reranking model with heterogeneous
data. The second line of Table 5 shows the offi-
cal final results. In this submission, we trained a
model using an old version of training data. Note
that, the standard of POS tags of the ?old? version
is different from the latest version which is also
used as test data. For example, the name of some
tags are changed. The third line of Table 46 shows
the results predicted by the newest data7. This re-
sult is comparable to other systems.
5 Conclusion
In this paper, we described our participation of
the CIPS-SIGHAN-2010 parsing task. The gen-
6There are two sentences that are not parsed by the Berke-
ley parser. We use a simple strategy to solve this problem:
We first roughly segment the sentence according to punctu-
ation; Then the parsed sub-sentences are merged as a single
zj.
7We would like to thank the organizer to re-test our new
submission.
erative coarse-to-fine parsing model is integrated
with a discriminative parse reranking model, as
well as a head classifier based on sequence la-
beling. We use the perceptron algorithm to train
the reranking models and experiment with both
homogenous and heterogenous data. The results
show improvements over the baseline in both
cases.
Acknowledgments
The first author is supported by the German Aca-
demic Exchange Service (DAAD). The second
author is supported by the PIRE scholarship pro-
gram; the third author thanks DFKI and the Clus-
ter of Excellence on Multimodal Computing and
Interaction for their support of the work.
References
Charniak, E. and M Johnson. 2005. oarse-to-fine n-
best parsing and maxent discriminative reranking.
In Proceedings of ACL, pages 173?180.
Collins, Michael and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In
Proceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Collins, Michael and Terry Koo. 2005. Discriminative
reranking for natural language parsing. In Compu-
tational Linguistics, volume 31(1), pages 25?69.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Computational Lin-
guistics, pages 175?182. Morgan Kaufmann.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, pages
1?8, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Gao, Jianfeng, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Petrov, S. and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL-2007, Rochester, NY, USA, April.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Song, Yan and Chunyu Kit. 2009. Pcfg parsing with
crf tagging for head recognition. In Proceedings of
the CIPS-ParsEval-2009.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Zhou, Qiang. 2004. Annotation scheme for chinese
treebank (in chinese). Journal of Chinese Informa-
tion Processing, 18(4):1?8.
