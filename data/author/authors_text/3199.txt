Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98?107,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Indirect-HMM-based Hypothesis Alignment for Combining Outputs 
from Machine Translation Systems 
 
Xiaodong He?, Mei Yang? *, Jianfeng Gao?, Patrick Nguyen?, and Robert Moore? 
 
?Microsoft Research ?Dept. of Electrical Engineering 
One Microsoft Way University of Washington 
Redmond, WA 98052 USA Seattle, WA 98195, USA 
{xiaohe,jfgao, panguyen, 
bobmoore}@microsoft.com 
yangmei@u.washington.edu 
 
 
Abstract 
This paper presents a new hypothesis alignment method 
for combining outputs of multiple machine translation 
(MT) systems. An indirect hidden Markov model 
(IHMM) is proposed to address the synonym matching 
and word ordering issues in hypothesis alignment.  
Unlike traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly from a 
variety of sources including word semantic similarity, 
word surface similarity, and a distance-based distortion 
penalty. The IHMM-based method significantly 
outperforms the state-of-the-art TER-based alignment 
model in our experiments on NIST benchmark 
datasets.  Our combined SMT system using the 
proposed method achieved the best Chinese-to-English 
translation result in the constrained training track of the 
2008 NIST Open MT Evaluation. 
1 Introduction* 
System combination has been applied successfully 
to various machine translation tasks. Recently, 
confusion-network-based system combination 
algorithms have been developed to combine 
outputs of multiple machine translation (MT) 
systems to form a consensus output (Bangalore, et 
al. 2001, Matusov et al, 2006, Rosti et al, 2007, 
Sim et al, 2007). A confusion network comprises a 
sequence of sets of alternative words, possibly 
including null?s, with associated scores. The 
consensus output is then derived by selecting one 
word from each set of alternatives, to produce the 
sequence with the best overall score, which could 
be assigned in various ways such as by voting, by 
                                                          
* Mei Yang performed this work when she was an intern with 
Microsoft Research. 
using posterior probability estimates, or by using a 
combination of these measures and other features. 
Constructing a confusion network requires 
choosing one of the hypotheses as the backbone 
(also called ?skeleton? in the literature), and other 
hypotheses are aligned to it at the word level. High 
quality hypothesis alignment is crucial to the 
performance of the resulting system combination. 
However, there are two challenging issues that 
make MT hypothesis alignment difficult. First, 
different hypotheses may use different 
synonymous words to express the same meaning, 
and these synonyms need to be aligned to each 
other. Second, correct translations may have 
different word orderings in different hypotheses 
and these words need to be properly reordered in 
hypothesis alignment.  
In this paper, we propose an indirect hidden 
Markov model (IHMM) for MT hypothesis 
alignment. The HMM provides a way to model 
both synonym matching and word ordering. Unlike 
traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly 
from a variety of sources including word semantic 
similarity, word surface similarity, and a distance-
based distortion penalty, without using large 
amount of training data. Our combined SMT 
system using the proposed method gave the best 
result on the Chinese-to-English test in the 
constrained training track of the 2008 NIST Open 
MT Evaluation (MT08). 
2 Confusion-network-based MT system 
combination 
The current state-of-the-art is confusion-network-
based MT system combination as described by 
98
 Rosti and colleagues (Rosti et al, 2007a, Rosti et 
al., 2007b). The major steps are illustrated in 
Figure 1. In Fig. 1 (a), hypotheses from different 
MT systems are first collected. Then in Fig. 1 (b), 
one of the hypotheses is selected as the backbone 
for hypothesis alignment. This is usually done by a 
sentence-level minimum Bayes risk (MBR) 
method which selects a hypothesis that has the 
minimum average distance compared to all 
hypotheses. The backbone determines the word 
order of the combined output. Then as illustrated in 
Fig. 1 (c), all other hypotheses are aligned to the 
backbone. Note that in Fig. 1 (c) the symbol ? 
denotes a null word, which is inserted by the 
alignment normalization algorithm described in 
section 3.4. Fig. 1 (c) also illustrates the handling 
of synonym alignment (e.g., aligning ?car? to 
?sedan?), and word re-ordering of the hypothesis. 
Then in Fig. 1 (d), a confusion network is 
constructed based on the aligned hypotheses, 
which consists of a sequence of sets in which each 
word is aligned to a list of alternative words 
(including null) in the same set. Then, a set of 
global and local features are used to decode the 
confusion network.  
  
E1 he have good car 
argmin ( , )B E EE TER E E?? ? ?? ?E E
 
E2 he has nice sedan 
E3 it a nice car        e.g., EB = E1 E4 a sedan he has 
(a)  hypothesis set                    (b) backbone selection 
 
EB he have ? good car      he  have   ?   good   car 
       he   has    ?   nice    sedan 
       it     ?       a   nice    car   
E4 a  ?  sedan  he   has      he   has    a     ?       sedan 
(c)  hypothesis alignment        (d) confusion network 
 
Figure 1: Confusion-network-based MT system 
combination.  
3 Indirect-HMM-based Hypothesis 
Alignment  
In confusion-network-based system combination 
for SMT, a major difficulty is aligning hypotheses 
to the backbone. One possible statistical model for 
word alignment is the HMM, which has been 
widely used for bilingual word alignment (Vogel et 
al., 1996, Och and Ney, 2003). In this paper, we 
propose an indirect-HMM method for monolingual 
hypothesis alignment. 
 
3.1 IHMM for hypothesis alignment  
 
Let 
1 1( ,..., )I Ie e e? denote the backbone, 
1 1( ,..., )J Je e e? ? ??  a hypothesis to be aligned to 1Ie , 
and 
1 1( ,..., )J Ja a a?  the alignment that specifies 
the position of the backbone word aligned to each 
hypothesis word. We treat each word in the 
backbone as an HMM state and the words in the 
hypothesis as the observation sequence. We use a 
first-order HMM, assuming that the emission 
probability 
( | )jj ap e e?
 depends only on the 
backbone word, and the transition probability 
1( | , )j jp a a I?
 depends only on the position of the 
last state and the length of the backbone. Treating 
the alignment as hidden variable, the conditional 
probability that the hypothesis is generated by the 
backbone is given by  
 
 
1
1 1 1
1
( | ) ( | , ) ( | )jJ
JJ I
j j j a
ja
p e e p a a I p e e?
?
? ?? ?? ? ???
 (1) 
  
As in HMM-based bilingual word alignment 
(Och and Ney, 2003), we also associate a null with 
each backbone word to allow generating 
hypothesis words that do not align to any backbone 
word.  
In HMM-based hypothesis alignment, emission 
probabilities model the similarity between a 
backbone word and a hypothesis word, and will be 
referred to as the similarity model. The transition 
probabilities model word reordering, and will be 
called the distortion model. 
 
3.2 Estimation of the similarity model 
 
The similarity model, which specifies the emission 
probabilities of the HMM, models the similarity 
between a backbone word and a hypothesis word. 
Since both words are in the same language, the 
similarity model can be derived based on both 
semantic similarity and surface similarity, and the 
overall similarity model is a linear interpolation of 
the two: 
 
( | ) ( | ) (1 ) ( | )j i sem j i sur j ip e e p e e p e e? ?? ? ?? ? ? ? ?  (2) 
 
99
 where ( | )sem j ip e e?
 and ( | )sur j ip e e?
 reflect the 
semantic and surface similarity between 
je?
 and  
ie , respectively, and ? is the interpolation factor. 
Since the semantic similarity between two 
target words is source-dependent, the semantic 
similarity model is derived by using the source 
word sequence as a hidden layer: 
 
0
( | )
( | ) ( | , )
sem j i
K
k i j k i
k
p e e
p f e p e f e
?
?
???
 
0
( | ) ( | )K k i j k
k
p f e p e f
?
???     (3) 
 
where 
1 1( ,..., )K Kf f f?  is the source sentence. 
Moreover, in order to handle the case that two 
target words are synonyms but neither of them has 
counter-part in the source sentence, a null is 
introduced on the source side, which is represented 
by f0. The last step in (3) assumes that first ei 
generates all source words including null. Then ej? 
is generated by all source words including null.  
In the common SMT scenario where a large 
amount of bilingual parallel data is available, we 
can estimate the translation probabilities from a 
source word to a target word and vice versa via 
conventional bilingual word alignment. Then both 
( | )k ip f e  and ( | )j kp e f?
 in (3) can be derived:  
 
2( | ) ( | )j k s t j kp e f p e f? ??
 
 
where 
2 ( | )s t j kp e f?
 is the translation model from 
the source-to-target word alignment model, and 
( | )k ip f e  , which enforces the sum-to-1 constraint 
over all words in the source sentence, takes the 
following form, 
 
2
2
0
( | )( | )
( | )
t s k i
k i K
t s k i
k
p f ep f e
p f e
?
?
?
 
 
where 
2 ( | )t s k ip f e  is the translation model from 
the  target-to-source word alignment model. In our 
method, 
2 ( | )t s ip null e  for all target words is 
simply a constant pnull, whose value is optimized 
on held-out data 1.  
The surface similarity model can be estimated 
in several ways. A very simple model could be 
based on exact match: the surface similarity model, 
( | )sur j ip e e?
, would take the value 1.0 if e?= e, and 
0 otherwise 2 . However, a smoothed surface 
similarity model is used in our method. If the target 
language uses alphabetic orthography, as English 
does, we treat words as letter sequences and the 
similarity measure can be the length of the longest 
matched prefix (LMP) or the length of the longest 
common subsequence (LCS) between them. Then, 
this raw similarity measure is transformed to a 
surface similarity score between 0 and 1 through 
an exponential mapping,  
 
? ?( | ) exp ( , ) 1sur j i j ip e e s e e?? ?? ?? ? ?? ?    (4) 
 
where ( , )j is e e?
 is computed as 
 
( , )( , ) max(| |,| |)
j i
j i
j i
M e es e e e e
?? ? ?
 
 
and ( , )j iM e e?
 is the raw similarity measure of ej? 
ei, which is the length of the LMP or LCS of ej? 
and ei. and ? is a smoothing factor that 
characterizes the mapping, Thus as ? approaches 
infinity, ( | )sur j ip e e?
 backs off to the exact match 
model. We found the smoothed similarity model of 
(4) yields slightly better results than the exact 
match model. Both LMP- and LCS- based methods 
achieve similar performance but the computation 
of LMP is faster. Therefore, we only report results 
of the LMP-based smoothed similarity model.  
 
3.3 Estimation of the distortion model 
 
The distortion model, which specifies the transition 
probabilities of the HMM, models the first-order 
dependencies of word ordering. In bilingual 
HMM-based word alignment, it is commonly 
assumed that transition probabilities 
                                                          
1  The other direction, 
2 ( | )s t ip e null? , is available from the 
source-to-target translation model. 
2 Usually a small back-off value is assigned instead of 0.  
100
 1( | , )? ?? ?j jp a i a i I
 depend only on the jump 
distance (i - i')  (Vogel et al, 1996):  
 
1
( )( | , )
( )
I
l
c i ip i i I
c l i
?
??? ?
???
             (5) 
 
As suggested by Liang et al (2006), we can 
group the distortion parameters {c(d)}, d= i - i', 
into a few buckets. In our implementation, 11 
buckets are used for c(?-4),  c(-3), ... c(0), ..., c(5), 
c(?6). The probability mass for transitions with 
jump distance larger than 6 and less than -4 is 
uniformly divided. By doing this, only a handful of 
c(d) parameters need to be estimated. Although it 
is possible to estimate them using the EM 
algorithm on a small development set, we found 
that a particularly simple model, described below, 
works surprisingly well in our experiments.  
Since both the backbone and the hypothesis are 
in the same language, It seems intuitive that the 
distortion model should favor monotonic 
alignment and only allow non-monotonic 
alignment with a certain penalty. This leads us to 
use a distortion model of the following form, 
where K is a tuning factor optimized on held-out 
data. 
 
? ? ? ?1 1c d d ??? ? ?, d= ?4, ?, 6   (6) 
 
As shown in Fig. 2, the value of distortion score 
peaks at d=1, i.e., the monotonic alignment, and 
decays for non-monotonic alignments depending 
on how far it diverges from the monotonic 
alignment. 
 
Figure 2, the distance-based distortion parameters 
computed according to (6), where K=2. 
 
Following Och and Ney (2003), we use a fixed 
value p0 for the probability of jumping to a null 
state, which can be optimized on held-out data, and 
the overall distortion model becomes 
 
0
0
              if     state( | , ) (1 ) ( | , )  otherwise
p i nullp i i I p p i i I
??? ? ? ?? ???
 
 
3.4 Alignment normalization 
 
Given an HMM, the Viterbi alignment algorithm 
can be applied to find the best alignment between 
the backbone and the hypothesis, 
 
1
1 1
1
? argmax ( | , ) ( | )jJ
JJ
j j j aa j
a p a a I p e e?
?
? ??? ? ??
  (7) 
 
However, the alignment produced by the 
algorithm cannot be used directly to build a 
confusion network. There are two reasons for this. 
First, the alignment produced may contain 1-N 
mappings between the backbone and the 
hypothesis whereas 1-1 mappings are required in 
order to build a confusion network. Second, if 
hypothesis words are aligned to a null in the 
backbone or vice versa, we need to insert actual 
nulls into the right places in the hypothesis and the 
backbone, respectively. Therefore, we need to 
normalize the alignment produced by Viterbi 
search. 
 
EB ? e2  ?2   ?   
   ?    ?      e2        ?     ?      ? 
           e1'    e2'    e3'   e4'    
Eh e1'    e2'    e3'   e4'  
(a) hypothesis words are aligned to the backbone null  
 
EB e1  ?1  e2  ?2  e3  ?3    
   ?    e1     e2        e3      ? 
           e2'    ?      e1'   
Eh e1'    e2'    ?  
(b) a backbone word is aligned to no hypothesis word 
 
Figure 3: illustration of alignment normalization 
 
First, whenever more than one hypothesis 
words are aligned to one backbone word, we keep 
the link which gives the highest occupation 
probability computed via the forward-backward 
algorithm. The other hypothesis words originally 
 -4                     1                      6  
 1.0 
 0.0 
   c(d) 
  d 
101
 aligned to the backbone word will be aligned to the 
null associated with that backbone word. 
Second, for the hypothesis words that are 
aligned to a particular null on the backbone side, a 
set of nulls are inserted around that backbone word 
associated with the null such that no links cross 
each other. As illustrated in Fig. 3 (a), if a 
hypothesis word e2? is aligned to the backbone 
word e2, a null is inserted in front of the backbone 
word e2 linked to the hypothesis word e1? that 
comes before e2?. Nulls are also inserted for other 
hypothesis words such as e3? and e4? after the 
backbone word e2. If there is no hypothesis word 
aligned to that backbone word, all nulls are 
inserted after that backbone word .3 
For a backbone word that is aligned to no 
hypothesis word, a null is inserted on the 
hypothesis side, right after the hypothesis word 
which is aligned to the immediately preceding 
backbone word. An example is shown in Fig. 3 (b). 
4 Related work 
The two main hypothesis alignment methods for 
system combination in the previous literature are 
GIZA++ and TER-based methods. Matusov et al 
(2006) proposed using GIZA++ to align words 
between different MT hypotheses, where all 
hypotheses of the test corpus are collected to create 
hypothesis pairs for GIZA++ training. This 
approach uses the conventional HMM model 
bootstrapped from IBM Model-1 as implemented 
in GIZA++, and heuristically combines results 
from aligning in both directions. System 
combination based on this approach gives an 
improvement over the best single system. 
However, the number of hypothesis pairs for 
training is limited by the size of the test corpus. 
Also, MT hypotheses from the same source 
sentence are correlated with each other and these 
hypothesis pairs are not i.i.d. data samples. 
Therefore, GIZA++ training on such a data set may 
be unreliable.  
Bangalore et al (2001) used a multiple string-
matching algorithm based on Levenshtein edit 
distance, and later Sim et al (2007) and Rosti et al 
(2007) extended it to a TER-based method for 
hypothesis alignment. TER (Snover et al, 2006) 
                                                          
3  This only happens if no hypothesis word is aligned to a 
backbone word but some hypothesis words are aligned to the 
null associated with that backbone word. 
measures the minimum number of edits, including 
substitution, insertion, deletion, and shift of blocks 
of words, that are needed to modify a hypothesis so 
that it exactly matches the other hypothesis. The 
best alignment is the one that gives the minimum 
number of translation edits. TER-based confusion 
network construction and system combination has 
demonstrated superior performance on various 
large-scale MT tasks (Rosti. et al 2007). However, 
when searching for the optimal alignment, the 
TER-based method uses a strict surface hard match 
for counting edits. Therefore, it is not able to 
handle synonym matching well. Moreover, 
although TER-based alignment allows phrase 
shifts to accommodate the non-monotonic word 
ordering, all non-monotonic shifts are penalized 
equally no matter how short or how long the move 
is, and this penalty is set to be the same as that for 
substitution, deletion, and insertion edits. 
Therefore, its modeling of non-monotonic word 
ordering is very coarse-grained.  
In contrast to the GIZA++-based method, our 
IHMM-based method has a similarity model 
estimated using bilingual word alignment HMMs 
that are trained on a large amount of bi-text data. 
Moreover, the surface similarity information is 
explicitly incorporated in our model, while it is 
only used implicitly via parameter initialization for 
IBM Model-1 training by Matusov et al (2006). 
On the other hand, the TER-based alignment 
model is similar to a coarse-grained, non-
normalized version of our IHMM, in which the 
similarity model assigns no penalty to an exact 
surface match and a fixed penalty to all 
substitutions, insertions, and deletions, and the 
distortion model simply assigns no penalty to a 
monotonic jump, and a fixed penalty to all other 
jumps, equal to the non-exact-match penalty in the 
similarity model. 
There have been other hypothesis alignment 
methods. Karakos, et al (2008) proposed an ITG-
based method for hypothesis alignment, Rosti et al 
(2008) proposed an incremental alignment method, 
and a heuristic-based matching algorithm was 
proposed by Jayaraman and Lavie (2005).  
5 Evaluation 
In this section, we evaluate our IHMM-based 
hypothesis alignment method on the Chinese-to-
English (C2E) test in the constrained training track 
102
 of the 2008 NIST Open MT Evaluation (NIST, 
2008). We compare to the TER-based method used 
by Rosti et al (2007). In the following 
experiments, the NIST BLEU score is used as the 
evaluation metric (Papineni et al, 2002), which is 
reported as a percentage in the following sections.  
 
5.1 Implementation details 
 
In our implementation, the backbone is selected 
with MBR. Only the top hypothesis from each 
single system is considered as a backbone. A 
uniform posteriori probability is assigned to all 
hypotheses. TER is used as loss function in the 
MBR computation.  
Similar to (Rosti et al, 2007), each word in the 
confusion network is associated with a word 
posterior probability. Given a system S, each of its 
hypotheses is assigned with a rank-based score of 
1/(1+r)?, where r is the rank of the hypothesis, and 
? is a rank smoothing parameter. The system 
specific rank-based score of a word w for a given 
system S is the sum of all the rank-based scores of 
the hypotheses in system S that contain the word w 
at the given position (after hypothesis alignment). 
This score is then normalized by the sum of the 
scores of all the alternative words at the same 
position and from the same system S to generate 
the system specific word posterior. Then, the total 
word posterior of w over all systems is a sum of 
these system specific posteriors weighted by 
system weights. 
Beside the word posteriors, we use language 
model scores and a word count as features for 
confusion network decoding. 
Therefore, for an M-way system combination 
that uses N LMs, a total of M+N+1 decoding 
parameters, including M-1 system weights, one 
rank smoothing factor, N language model weights, 
and one weight for the word count feature, are 
optimized using Powell?s method (Brent, 1973) to 
maximize BLEU score on a development set4 . 
Two language models are used in our 
experiments. One is a trigram model estimated 
from the English side of the parallel training data, 
and the other is a 5-gram model trained on the 
English GigaWord corpus from LDC using the 
MSRLM toolkit (Nguyen et al 2007). 
                                                          
4 The parameters of IHMM are not tuned by maximum-BLEU 
training. 
In order to reduce the fluctuation of BLEU 
scores caused by the inconsistent translation output 
length, an unsupervised length adaptation method 
has been devised. We compute an expected length 
ratio between the MT output and the source 
sentences on the development set after maximum- 
BLEU training. Then during test, we adapt the 
length of the translation output by adjusting the 
weight of the word count feature such that the 
expected output/source length ratio is met. In our 
experiments, we apply length adaptation to the 
system combination output at the level of the 
whole test corpus. 
 
5.2  Development and test data  
 
The development (dev) set used for system 
combination parameter training contains 1002 
sentences sampled from the previous NIST MT 
Chinese-to-English test sets: 35% from MT04, 
55% from MT05, and 10% from MT06-newswire. 
The test set is the MT08 Chinese-to-English 
?current? test set, which includes 1357 sentences 
from both newswire and web-data genres. Both 
dev and test sets have four references per sentence. 
As inputs to the system combination, 10-best 
hypotheses for each source sentence in the dev and 
test sets are collected from each of the eight single 
systems. All outputs on the MT08 test set were 
true-cased before scoring using a log-linear 
conditional Markov model proposed by Toutanova 
et al (2008). However, to save computation effort, 
the results on the dev set are reported in case 
insensitive BLEU (ciBLEU) score instead. 
 
5.3  Experimental results 
 
In our main experiments, outputs from a total of 
eight single MT systems were combined. As listed 
in Table 1, Sys-1 is a tree-to-string system 
proposed by Quirk et al, (2005); Sys-2 is a phrase-
based system with fast pruning proposed by Moore 
and Quirk (2008); Sys-3 is a phrase-based system 
with syntactic source reordering proposed by 
Wang et al (2007a); Sys-4 is a syntax-based pre-
ordering system proposed by Li et. al. (2007); Sys-
5 is a hierarchical system proposed by Chiang 
(2007); Sys-6 is a lexicalized re-ordering system 
proposed by Xiong et al (2006); Sys-7 is a two-
pass phrase-based system with adapted LM 
proposed by Foster and Kuhn (2007); and  Sys-8 is 
103
 a hierarchical system with two-pass rescoring 
using a parser-based LM proposed by Wang et al, 
(2007b). All systems were trained within the 
confines of the constrained training condition of 
NIST MT08 evaluation. These single systems are 
optimized with maximum-BLEU training on 
different subsets of the previous NIST MT test 
data. The bilingual translation models used to 
compute the semantic similarity are from the word-
dependent HMMs proposed by He (2007), which 
are trained on two million parallel sentence-pairs 
selected from the training corpus allowed by the 
constrained training condition of MT08.  
 
5.3.1 Comparison with TER alignment 
In the IHMM-based method, the smoothing 
factor for surface similarity model is set to ? = 3, 
the interpolation factor of the overall similarity 
model is set to ? = 0.3, and the controlling factor of 
the distance-based distortion parameters is set to 
K=2. These settings are optimized on the dev set. 
Individual system results and system combination 
results using both IHMM and TER alignment, on 
both the dev and test sets, are presented in Table 1. 
The TER-based hypothesis alignment tool used in 
our experiments is the publicly available TER Java 
program, TERCOM (Snover et al, 2006). Default 
settings of TERCOM are used in the following 
experiments. 
On the dev set, the case insensitive BLEU score 
of the IHMM-based 8-way system combination 
output is about 5.8 points higher than that of the 
best single system. Compared to the TER-based 
method, the IHMM-based method is about 1.5 
BLEU points better. On the MT08 test set, the 
IHMM-based system combination gave a case 
sensitive BLEU score of 30.89%. It outperformed 
the best single system by 4.7 BLEU points and the 
TER-based system combination by 1.0 BLEU 
points. Note that the best single system on the dev 
set and the test set are different. The different 
single systems are optimized on different tuning 
sets, so this discrepancy between dev set and test 
set results is presumably due to differing degrees 
of mismatch between the dev and test sets and the 
various tuning sets. 
 
 
 
 
 
Table 1. Results of single and combined systems 
on the dev set and the MT08 test set  
System Dev 
ciBLEU% 
MT08 
BLEU% 
System 1 34.08 21.75 
System 2 33.78 20.42 
System 3 34.75 21.69 
System 4 37.85 25.52 
System 5 37.80 24.57 
System 6 37.28 24.40 
System 7 32.37 25.51 
System 8 34.98 26.24 
TER 42.11 29.89 
IHMM 43.62 30.89 
 
In order to evaluate how well our method 
performs when we combine more systems, we 
collected MT outputs on MT08 from seven 
additional single systems as summarized in Table 
2. These systems belong to two groups. Sys-9 to 
Sys-12 are in the first group. They are syntax-
augmented hierarchical systems similar to those 
described by Shen et al (2008) using different 
Chinese word segmentation and language models. 
The second group has Sys-13 to Sys-15. Sys-13 is 
a phrasal system proposed by Koehn et al (2003), 
Sys-14 is a hierarchical system proposed by 
Chiang (2007), and Sys-15 is a syntax-based 
system proposed by Galley et al (2006). All seven 
systems were trained within the confines of the 
constrained training condition of NIST MT08 
evaluation.  
We collected 10-best MT outputs only on the 
MT08 test set from these seven extra systems. No 
MT outputs on our dev set are available from them 
at present. Therefore, we directly adopt system 
combination parameters trained for the previous 8-
way system combination, except the system 
weights, which are re-set by the following 
heuristics: First, the total system weight mass 1.0 is 
evenly divided among the three groups of single 
systems: {Sys-1~8}, {Sys-9~12}, and {Sys-
13~15}. Each group receives a total system weight 
mass of 1/3. Then the weight mass is further 
divided in each group: in the first group, the 
original weights of systems 1~8 are multiplied by 
1/3; in the second and third groups, the weight 
mass is evenly distributed within the group, i.e., 
1/12 for each system in group 2, and 1/9 for each 
104
 system in group 35.  Length adaptation is applied to 
control the final output length, where the same 
expected length ratio of the previous 8-way system 
combination is adopted. 
The results of the 15-way system combination 
are presented in Table 3. It shows that the IHMM-
based method is still about 1 BLEU point better 
than the TER-based method. Moreover, combining 
15 single systems gives an output that has a NIST 
BLEU score of 34.82%, which is 3.9 points better 
than the best submission to the NIST MT08 
constrained training track (NIST, 2008). To our 
knowledge, this is the best result reported on this 
task. 
 
Table 2. Results of seven additional single systems 
on the NIST MT08 test set 
System MT08 
BLEU% 
System 9 29.59 
System 10 29.57 
System 11 29.64 
System 12 29.85 
System 13 25.53 
System 14 26.04 
System 15 29.70 
 
Table 3. Results of the 15-way system combination 
on the NIST MT08 C2E test set 
Sys. Comb.  MT08 
BLEU% 
TER 33.81 
IHMM 34.82 
 
5.3.2 Effect of the similarity model  
In this section, we evaluate the effect of the 
semantic similarity model and the surface 
similarity model by varying the interpolation 
weight ? of (2). The results on both the dev and 
test sets are reported in Table 4. In one extreme 
case, ? = 1, the overall similarity model is based 
only on semantic similarity. This gives a case 
insensitive BLEU score of 41.70% and a case 
sensitive BLEU score of 28.92% on the dev and 
test set, respectively. The accuracy is significantly 
improved to 43.62% on the dev set and 30.89% on 
test set when ? = 0.3. In another extreme case, ? = 
                                                          
5 This is just a rough guess because no dev set is available. We 
believe a better set of system weights could be obtained if MT 
outputs on a common dev set were available. 
0, in which only the surface similarity model is 
used for the overall similarity model, the 
performance degrades by about 0.2 point. 
Therefore, the surface similarity information seems 
more important for monolingual hypothesis 
alignment, but both sub-models are useful.  
 
Table 4. Effect of the similarity model 
 Dev 
ciBLEU% 
Test 
BLEU% 
? = 1.0 41.70 28.92 
? = 0.7 42.86 30.50 
? = 0.5 43.11 30.94 
? = 0.3 43.62 30.89 
? = 0.0 43.35 30.73 
 
5.3.3 Effect of the distortion model  
We investigate the effect of the distance-based 
distortion model by varying the controlling factor 
K in (6). For example, setting K=1.0 gives a linear-
decay distortion model, and setting K=2.0 gives a 
quadratic smoothed distance-based distortion 
model. As shown in Table 5, the optimal result can 
be achieved using a properly smoothed distance-
based distortion model. 
 
Table 5. Effect of the distortion model 
 Dev 
ciBLEU% 
Test 
BLEU% 
K=1.0 42.94 30.44 
K=2.0 43.62 30.89 
K=4.0 43.17 30.30 
K=8.0 43.09 30.01 
6 Conclusion 
Synonym matching and word ordering are two 
central issues for hypothesis alignment in 
confusion-network-based MT system combination. 
In this paper, an IHMM-based method is proposed 
for hypothesis alignment. It uses a similarity model 
for synonym matching and a distortion model for 
word ordering. In contrast to previous methods, the 
similarity model explicitly incorporates both 
semantic and surface word similarity, which is 
critical to monolingual word alignment, and a 
smoothed distance-based distortion model is used 
to model the first-order dependency of word 
ordering, which is shown to be better than simpler 
approaches. 
105
 Our experimental results show that the IHMM-
based hypothesis alignment method gave superior 
results on the NIST MT08 C2E test set compared 
to the TER-based method. Moreover, we show that 
our system combination method can scale up to 
combining more systems and produce a better 
output that has a case sensitive BLEU score of 
34.82, which is 3.9 BLEU points better than the 
best official submission of MT08.  
Acknowledgement 
The authors are grateful to Chris Quirk, Arul 
Menezes, Kristina Toutanova, William Dolan, Mu 
Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, 
Ming Zhou, George Foster, Roland Kuhn, Jing 
Zheng, Wen Wang, Necip Fazil Ayan, Dimitra 
Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin 
Knight, Jens-Soenke Voeckler, Spyros Matsoukas, 
and Antti-Veikko Rosti for assistance with the MT 
systems and/or for the valuable suggestions and 
discussions.  
 
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In Proc. 
of IEEE ASRU, pp. 351?354. 
Richard Brent, 1973. Algorithms for Minimization 
without Derivatives. Prentice-Hall, Chapter 7. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
George Foster and Roland Kuhn. 2007. Mixture-Model 
Adaptation for SMT. In Proc. of the Second ACL 
Workshop on Statistical Machine Translation. pp. 
128 ? 136. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In Proc. 
of COLING-ACL, pp. 961?968. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. In Proc. of the 
Second ACL Workshop on Statistical Machine 
Translation. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word 
matching. In Proc. of EAMT. pp. 143 ? 152. 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proc. of ACL-HLT, pp. 81?84. 
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming 
Zhou, Yi Guan. 2007. A Probabilistic Approach to 
Syntax-based Reordering for Statistical Machine 
Translation. In Proc. of ACL. pp. 720 ? 727. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by Agreement. In Proc. of NAACL. pp 
104 ? 111.  
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing consensus translation from 
multiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL, pp. 33?40. 
Robert Moore and Chris Quirk. 2007. Faster Beam-
Search Decoding for Phrasal Statistical Machine 
Translation. In Proc. of MT Summit XI. 
Patrick Nguyen, Jianfeng Gao and Milind Mahajan. 
2007. MSRLM: a scalable language modeling 
toolkit. Microsoft Research Technical Report MSR-
TR-2007-144. 
NIST. 2008. The 2008 NIST Open Machine Translation 
Evaluation. www.nist.gov/speech/tests/mt/2008/doc/  
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of ACL, 
pp. 311?318. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase based translation. In Proc. of 
NAACL. pp. 48 ? 54. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL. pp. 271?
279. 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proc. of NAACL-
HLT, pp. 228?235. 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007b. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL, pp. 312?319. 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental Hypothesis 
Alignment for Building Confusion Networks with 
Application to Machine Translation System 
Combination, In Proc. of the Third ACL Workshop 
on Statistical Machine Translation, pp. 183?186. 
Libin Shen, Jinxi Xu, Ralph Weischedel. 2008. A New 
String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. In Proc. of ACL-HLT, pp. 577?585. 
106
 Khe Chai Sim, William J. Byrne, Mark J.F. Gales, 
Hichem Sahbi, and Phil C. Woodland. 2007. 
Consensus network decoding for statistical machine 
translation system combination. In Proc. of ICASSP, 
vol. 4. pp. 105?108. 
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea 
Micciulla, and John Makhoul. 2006. A study of 
translation edit rate with targeted human annotation. 
In Proc. of AMTA. 
Kristina Toutanova, Hisami Suzuki and Achim Ruopp. 
2008. Applying Morphology Generation Models to 
Machine Translation. In Proc. of ACL. pp. 514 ? 522. 
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 
1996. HMM-based Word Alignment In Statistical 
Translation. In Proc. of COLING. pp. 836-841. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007a. Chinese Syntactic Reordering for Statistical 
Machine Translation.  In Proc. of EMNLP-CoNLL. 
pp. 737-745. 
Wen Wang, Andreas Stolcke, Jing Zheng. 2007b. 
Reranking Machine Translation Hypotheses With 
Structured and Web-based Language Models. In 
Proc. of IEEE ASRU. pp. 159 ? 164. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL. 
pp. 521 ? 528. 
107
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1202?1211,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Joint Optimization for Machine Translation System Combination 
 
 
Xiaodong He 
Microsoft Research  
One Microsoft Way, Redmond, WA 
xiaohe@microsoft.com 
Kristina Toutanova 
Microsoft Research  
One Microsoft Way, Redmond, WA 
kristout@microsoft.com 
  
 
Abstract 
System combination has emerged as a 
powerful method for machine translation 
(MT). This paper pursues a joint optimization 
strategy for combining outputs from multiple 
MT systems, where word alignment, ordering, 
and lexical selection decisions are made 
jointly according to a set of feature functions 
combined in a single log-linear model. The 
decoding algorithm is described in detail and a 
set of new features that support this joint 
decoding approach is proposed. The approach 
is evaluated in comparison to state-of-the-art 
confusion-network-based system combination 
methods using equivalent features and shown 
to outperform them significantly.   
1 Introduction 
System combination for machine translation 
(MT) has emerged as a powerful method of 
combining the strengths of multiple MT systems 
and achieving results which surpass those of 
each individual system (e.g. Bangalore, et. al., 
2001, Matusov, et. al., 2006, Rosti, et. al., 
2007a). Most state-of-the-art system combination 
methods are based on constructing a confusion 
network (CN) from several input translation 
hypotheses, and choosing the best output from 
the CN based on several scoring functions (e.g. 
Rosti et. al., 2007a, He et. al., 2008, Matusov et 
al. 2008). Confusion networks allow word-level 
system combination, which was shown to 
outperform sentence re-ranking methods and 
phrase-level combination (Rosti, et. al. 2007a). 
We will review confusion-network-based 
system combination with the help of the 
examples in Figures 1 and 2. Figure 1 shows 
translation hypotheses from three Chinese-to-
English MT systems. The general idea is to 
combine hypotheses in a representation such as 
the ones in Figure 2, where for each word 
position there is a set of possible words, shown 
in columns.1The final output is determined by 
choosing one word from each column, which can 
be a real word or the empty word ?. For example, 
the CN in Figure 2a) can generate eight distinct 
sequences of words, including e.g. ?she bought 
the Jeep? and ?she bought the SUV Jeep?. The 
choice is performed to maximize a scoring 
function using a set of features and a log-linear 
model (Matusov, et. al 2006, Rosti, et al 2007a). 
We can view a confusion network as an 
ordered sequence of columns (correspondence 
sets). Each word from each input hypothesis 
belongs to exactly one correspondence set. Each 
correspondence set contains at most one word 
from each input hypothesis and contributes 
exactly one of its words (including the possible 
?) to the final output. Final words are output in 
the order of correspondence sets. In order to 
construct such a representation, we need to solve 
the following two sub-problems:  arrange words 
from all input hypotheses into correspondence 
sets (alignment problem) and order 
correspondence sets (ordering problem).  After 
constructing the confusion network we need to 
solve a third sub-problem: decide which words to 
output from each correspondence set (lexical 
choice problem). 
In current state-of-the-art approaches, the 
construction of the confusion network is 
performed as follows: first, a backbone 
hypothesis is selected. The backbone hypothesis 
determines the order of words in the final system 
output, and guides word-level alignments for 
construction of columns of possible words at 
each position. Let us assume that for our 
example in Figure 1, the second hypothesis is 
selected as a backbone. All other hypotheses are 
aligned to the backbone such that these 
alignments are one-to-one; empty words are 
inserted where necessary to make one-to-one 
                                                 
1 This representation is alternative to directed acyclic 
graph representations of confusion networks. 
1202
alignment possible. Words in all hypotheses are 
sorted by the position of the backbone word they 
align to and the confusion network is determined.  
It is clear that the quality of selection of the 
backbone and alignments has a large impact on 
the performance, because the word order is 
determined by the backbone, and the set of 
possible words at each position is determined by 
alignment. Since the space of possible 
alignments is extremely large, approximate and 
heuristic techniques have been employed to 
derive them. In pair-wise alignment, each 
hypothesis is aligned to the backbone in turn, 
with separate processing to combine the multiple 
alignments. Several models have been used for 
pair-wise alignment, starting with TER and 
proceeding with more sophisticated techniques 
such as HMM models, ITG, and IHMM (Rosti 
et. al 2007a, Matusov et al2008, Krakos et al 
2008, He et al 2008). A major problem with 
such methods is that each hypothesis is aligned 
to the backbone independently, leading to sub-
optimal behavior. For example, suppose that we 
use a state-of-the-art word alignment model for 
pairs of hypotheses, such as the IHMM. Figure 1 
shows likely alignment links between every pair 
of hypotheses. If Hypothesis 1 is aligned to 
Hypothesis 2 (the backbone), Jeep is likely to 
align to SUV because they express similar 
Chinese content. Hypothesis 3 is separately 
aligned to the backbone and since the alignment 
is constrained to be one-to-one, SUV is aligned to 
SUV and Jeep to an empty word which is 
inserted after SUV. The network in Figure 2a) is 
the result of this process. An undesirable 
property of this CN is that the two instances of 
Jeep are placed in separate columns and cannot 
vote to reinforce each other. 
Incremental alignment methods have been 
proposed to relax the independence assumption 
of pair-wise alignment (Rosti et al 2008, Li et al 
2009). Such methods align hypotheses to a 
partially constructed CN in some order. For 
example, if in such method, Hypothesis 3 is first 
aligned to the backbone, followed by Hypothesis 
1, we are likely to arrive at the CN in Figure 2b) 
in which the two instances of Jeep are aligned. 
However, if Hypothesis 1 is aligned to the 
backbone first, we would still get the CN in 
Figure 2a).  Notice that the desirable output ?She 
bought the Jeep SUV? cannot be generated from 
either of the confusion networks because a re-
reordering of columns would be required. 
A common characteristic of CN-based 
approaches is that the order of words (backbone) 
and the alignment of words (correspondence 
sets) are decided as greedy steps independently 
of the lexical choice for the final output. The 
backbone and alignment are optimized according 
to auxiliary scoring functions and heuristics 
which may or may not be optimal with respect to 
producing CNs leading to good translations. In 
some recent approaches, these assumptions are 
relaxed to allow each input hypothesis as a 
backbone. Each backbone produces a separate 
CN and the decision of which CN to choose is 
taken at a later decoding stage, but this still 
restricts the possible orders and alignments 
greatly (Rosti et al 2008, Matusov et al 2008). 
In this paper, we present a joint optimization 
method for system combination. In this method, 
the alignment, ordering and lexical selection sub-
problems are solved jointly in a single decoding 
framework based on a log-linear model. 
she
sh
she
bought
bought
buys
the
the
the
Jeep
SUV
SUV Jeep
?
? ?
 
 Figure 1. Three MT system hypotheses with pair-
wise alignments. 
 
she bought the Jeep ? 
she buys the SUV ? 
she bought the SUV Jeep 
 
a) Confusion network with pair-wise alignment. 
 
she bought the ? Jeep 
she buys the SUV ? 
she bought the SUV Jeep 
 
b) Confusion network with incremental alignment. 
 
Figure 2. Correspondence sets of confusion networks 
under pair-wise and incremental alignment, using the 
second hypothesis as a backbone. 
2 Related Work 
There has been a large body of work on MT 
system combination. Among confusion-network-
based algorithms, most relevant to our work are 
state-of-the-art methods for constructing word 
alignments (correspondence sets) and methods 
for improving the selection of a backbone 
hypothesis. We have already reviewed such work 
in the introduction and will note relation to 
1203
specific models throughout the paper as we 
discuss specifics of our scoring functions. 
In confusion network algorithms which use 
pair-wise (or incremental) word-level alignment 
algorithms for correspondence set construction, 
problems of converting many-to-many 
alignments and handling multiple insertions and 
deletions need to be addressed. Prior work has 
used a number of heuristics to deal with these 
problems (Matusov, et. al., 2006, He et al08). 
Some work has made such decisions in a more 
principled fashion by computing model-based 
scores (Matusov et al 2008), but still special-
purpose algorithms and heuristics are needed and 
a single alignment is fixed.  
In our approach, no heuristics are used to 
convert alignments and no concept of a backbone 
is used. Instead, the globally highest scoring 
combination of alignment, order, and lexical 
choice is selected (subject to search error). 
Other than confusion-network-based 
algorithms, work most closely related to ours is 
the method of MT system combination proposed 
in (Jayaraman and Lavie 2005), which we will 
refer to as J&L. Like our method, this approach 
performs word-level system combination and is 
not limited to following the word order of a 
single backbone hypothesis; it also allows more 
flexibility in the selection of correspondence sets 
during decoding, compared to a confusion-
network-based approach. Even though their 
algorithm and ours are broadly similar, there are 
several important differences.   
Firstly, the J&L approach is based on pair-
wise alignments between words in different 
hypotheses, which are hard and do not have 
associated probabilities. Every word in every 
hypothesis is aligned to at most one word from 
each of the remaining hypotheses. Thus there is 
no uncertainty about which words should belong 
to the correspondence set of an aligned word w, 
once that word is selected to extend a partial 
hypothesis during search. If words do not have 
corresponding matching words in some 
hypotheses, heuristic matching to currently 
unused words is attempted.  
In contrast, our algorithm is based on the 
definition of a joint scoring model, which takes 
into account alignment uncertainty and combines 
information from word-level alignment models, 
ordering and lexical selection models, to address 
the three sub-problems of word-level system 
combination. In addition to the language model 
and word-voting features used by the J&L 
model, we incorporate features which measure 
alignment confidence via word-level alignment 
models and features which evaluate re-ordering 
via distortion models with respect to original 
hypotheses. While the J&L search algorithm 
incorporates a number of special-purpose 
heuristics to address phenomena of unused words 
lagging behind the last used words, the goal in 
our work is to minimize heuristics and perform 
search to jointly optimize the assignment of 
hidden variables (ordered correspondence sets) 
and observed output variables (words in final  
translations). 
Finally, the J&L method has not been 
evaluated in comparison to confusion-network-
based methods to study the impact of performing 
joint decoding for the three sub-problems. 
3 Notation 
Before elaborating the models and decoding 
algorithms, we first clarify the notation that will 
be used in the paper.  
We denote by ? =  ?1 ,? ,?? the set of 
hypotheses from multiple MT systems, where ??  
is the hypothesis from the i-th system and ??  is a 
word sequence ??,1 ,? ,??,?(?)  with length ?(?) .  
For simplicity, we assume that each system 
contributes only its 1-best hypothesis for 
combination. Accordingly, the i-th hypothesis ??  
will be associated with a weight ?(?) which is 
the weight of the i-th system. In the scenario that 
N-best lists are available from individual systems 
for combination, the weight of each hypothesis 
can be computed based on its rank in the N-best 
list (Rosti et. al. 2007a).  
Like in CN-based system combination, we 
construct a set of ordered correspondence sets 
(CS) from input hypotheses, and select one word 
from each CS to form the final output. A CS is 
defined as a set of (possibly empty) words, one 
from each hypothesis, that implicitly align to 
each other and that contributes exactly one of its 
words to the final output. A valid complete set of 
CS includes each non-empty word from each 
hypothesis in exactly one CS. As opposed to CN-
based algorithms, our ordered correspondence 
sets are constructed during a joint decoding 
process which performs lexical selection at the 
same time. 
To facilitate the presentation of our features, 
we define notation for ordered CS. A sequence 
of correspondence sets is denoted by 
C=??1,? ,??? . Each correspondence set is 
specified by listing the positions of each of the 
words in the CS in their respective input 
1204
hypotheses. Each input hypothesis is assumed 
to have one special empty word ? at position 0. 
A CS is denoted by ?? ?1 ,? , ??  
= ?1,?1 ,? ,??,??  , where ??,??  is the li-th word in 
the i-th hypothesis and the word position vector                
? =  ?1 ,? , ?? 
?  specifies the position of each 
word in its original hypothesis. Correspondingly, 
word ?? ,??  has the same weight ?(?)  as its 
original hypothesis?? . As an example, the last 
two correspondence sets specified by the CN in 
Figure 2a) would be specified as ??4 =
?? 4,4,4 = {????, ???, ???}  and ??5 =
?? 0,0,5 = {?, ?, ????}. 
As opposed to the CS defined in a 
conventional CN, words that have the same 
surface form but come from different hypotheses 
are not collapsed to be one single candidate since 
they have different original word positions. We 
need to trace each of them separately during the 
decoding process.  
4 A Joint Optimization Framework For 
System Combination 
The joint decoding framework chooses optimal 
output according to the following log-linear 
model: 
 
?? =  argmax
???,???,???
???  ?? ? ??(?,?,?,?)
?
?=1
  
              
where we denote by C the set of all possible 
valid arrangements of CS, O the set of all 
possible orders of CS, W the set of all possible 
word sequences, consisting of words from the 
input hypotheses. {??(?,?,?,?)}  are the 
features and {??} are the feature weights in the 
log-linear model, respectively. 
4.1 Features  
A set of features are used in this framework. 
Each of them models one or more of the 
alignment, ordering, and lexical selection sub-
problems. Features are defined as follows.  
 
Word posterior model:  
The word posterior feature is the same as the 
one proposed by Rosti et. al. (2007a). i.e.,  
???  ?,?,?,? =  ??? ? ??  ???  
?
?=1
 
 
where the posterior of a single word in a CS is 
computed based on a weighted voting score: 
 
? ?? ,??  ?? = ?  ??,??  ?? ?1 ,? , ??   
=  ?(?)
?
?=1
?(?? ,?? = ?? ,??) 
 
and M is the number of CS generated. Note 
that M may be larger than the length of the 
output word sequence w since some CS may 
generate empty words. 
 
Bi-gram voting model: 
 The second feature we used is a bi-gram 
voting feature proposed by Zhao and He (2009), 
i.e., for each bi-gram  ?? ,??+1  , a weighted 
position-independent voting score is computed: 
?  ?? ,??+1  ? =  ?(?)
?
?=1
?( ?? ,??+1 ? ??) 
 
And the global bi-gram voting feature is 
defined as: 
????  ?,?,?,? =  ??? ?  ?? ,??+1  ?  
|? |?1
?=1
 
 
Distortion model: 
Unlike in the conventional CN-based system 
combination, flexible orders of CS are allowed in 
this joint decoding framework. In order to model 
the distortion of different orderings, a distortion 
model between two CS is defined as follows: 
First we define the distortion cost between two 
words at a single hypothesis. Similarly to the 
distortion penalty in the conventional phrase-
based decoder (Koehn 2004b), the distortion cost 
of jumping from a word at position i to another 
word at position j, d(i,j), is proportional to the 
distance between i and j, e.g., |i-j|. Then, the 
distortion cost of jumping from one CS, which 
has a position vector recording the original 
position of each word in that CS, to another CS 
is a weighted sum of single-hypothesis-based 
distortion costs: 
?(??? ,???+1)  =  ?(?)
?
?=1
? |?? ,? ? ??+1,? |  
 
where ?? ,?  and ??+1,?  are the k-th element of 
the word position vector of CSm and CSm+1, 
respectively. For the purpose of computing the 
distortion feature, the position of an empty 
word is taken to be the same as the position of 
1205
the last visited non-empty word from the same 
hypothesis. 
The overall ordering feature can then be 
computed based on ?(??? ,???+1): 
 
???? ?,?,?,? = ?  ?(??? ,???+1)
??1
?=1
 
 
It is worth noting that this is not the only 
feature modeling the re-ordering behavior.  
Under the joint decoding framework, other 
features such as the language model and bi-gram 
voting affect the ordering as well.  
 
Alignment model: 
Each CS consists of a set of words, one from 
each hypothesis, that are implicitly aligned to 
each other. Therefore, a valid complete set of CS 
defines the word alignment among different 
hypotheses. In this paper, we derive an alignment 
score of a CS based on alignment scores of word 
pairs in that CS. To compute scores for word 
pairs, we perform pair-wise hypothesis alignment 
using the indirect HMM (He et al 2008) for 
every pair of input hypotheses. Note that this 
involves a total of N by (N-1)/2 bi-directional 
hypothesis alignments. The alignment score for a 
pair of words  ?? ,??  and  ?? ,??  is defined as the 
average of posterior probabilities of alignment 
links in both directions and is thus direction 
independent: 
 
?  ?? ,??  ,?? ,??   =  
1
2
  ?(??? = ?? |?? ,??) +  ?(??? = ?? |?? ,?? )  
 
If one of the two words is ?, the posterior of 
aligning word ? to state j is computed as 
suggested by Liang et al (2006), i.e., 
 
? ?0 = ??  ?? ,??  =  1? ? ?? = ??  ?? ,??   
?(?)
?=1
 
 
And ?(??? = 0|?? ,??) can be computed by the 
HMM directly.  
If both words are ?, then a pre-defined  ???  is 
assigned, i.e., ? ?0 = 0 ?? ,??  = ??? , where ???  
can be optimized on a held-out validation set. 
For a CS of words, if we set the j-th word as 
an anchor word, the probability that all other 
words align to that word is: 
?(?|??)  =  ?  ?? ,??  ,?? ,??   
?
?=1
???
 
 
The alignment score of the whole CS is a 
weighted sum of the logarithm of the above 
alignment probabilities, i.e., 
 
???? (??)  = ?(?)
?
?=1
??? ?(?|??)  
 
and the global alignment score is computed as: 
 
????  ?,?,?,? =  ???? (???)
?
?=1
 
 
Entropy model: 
In general, it is preferable to align the same 
word from different hypotheses into a common 
CS. Therefore, we use entropy to model the 
purity of a CS. The entropy of a CS is defined as: 
 
??? ?? = ???(?? ?1 ,? , ?? )  = 
 ? ?? ,??  ?? ???? ??,??  ?? 
??
?=1
 
 
where the sum is taken over all distinct words in 
the CS. Then the global entropy score is 
computed as: 
 
????  ?,?,?,? =  ???(
?
?=1
???) 
 
Other features used in our log-linear model 
include the count of real words |w|, a n-gram 
language model, and the count M of CS sets. 
These features address one or more of the 
three sub-problems of MT system combination. 
By performing joint decoding with all these 
features working together, we hope to derive 
better decisions on alignment, ordering and 
lexical selection.  
5 Joint Decoding 
5.1 Core algorithm 
Decoding is based on a beam search algorithm 
similar to that of the phrase-based MT decoder 
(Koehn 2004b). The input is a set of translation 
hypotheses to be combined, and the final output 
1206
sentence is generated left to right. Figure 3 
illustrates the decoding process, using the 
example input hypotheses from Figure 1.  Each 
decoding state represents a partial sequence of 
correspondence sets covering some of the words 
in the input hypotheses and a sequence of words 
selected from the CS to form a partial output 
hypothesis. The initial decoding state has an 
empty sequence of CS and an empty output 
sequence. A state corresponds to a complete 
output candidate if its CS covers all input words.  
 
lm: ? bought the
 
    a) a decoding state 
lm: ? bought the lm: ? bought the
 
     b)  seed states         
lm: ? bought the lm: ? bought the
 
    c) correspondence set states 
lm: ? the Jeep lm: ? the Jeep
 
    d) decoding states 
Figure 3. Illustration of the decoding process. 
 
In practice, because the features over 
hypotheses can be decomposed, we do not need 
to encode all of this information in a decoding 
state. It suffices to store a few attributes. They 
include positions of words from each input 
hypothesis that have been visited, the last two 
non-empty words generated (if a tri-gram LM is 
used), and an "end position vector (EPV)" 
recording positions of words in the last CS, 
which were just visited. In the figure, the visited 
words are shown with filled circles and the EPV 
is shown with a dotted pattern in the filled 
circles. Words specified by the EPV are 
implicitly aligned. In the state in Figure 3 a) the 
first three words of each hypothesis have been 
visited, the third word of each hypothesis is the 
last word visited (in the EPV), and the last two 
words produced are ?bought the?. The states also 
record the decoding score accumulated so far and 
an estimated future score to cover words that 
have not been visited yet (not shown).  
The expansion from one decoding state to a 
set of new decoding states is illustrated in Figure 
3. The expansion is done in three steps with the 
help of intermediate states. Starting from a 
decoding state as shown in Figure 3a), first a set 
of ?seed states? as shown in Figure 3b) are 
generated. Each seed state represents a choice of 
one of unvisited words, called a ?seed word? 
which is selected and marked as visited. For 
example, the word Jeep from the first hypothesis 
and the word SUV from the second hypothesis 
are selected in the two seed states shown in 
Figure 3b), respectively. These seed states 
further expand into a set of "CS states" as shown 
in Figure 3c). I.e., a CS is formed by picking one 
word from each of the other hypotheses which is 
unvisited and has a valid alignment link to the 
seed word. Figure 3c) shows two CS states 
expanded from the first seed state of Figure 3b), 
using Jeep from the first hypothesis as a seed 
word. In one of them the empty word from the 
second hypothesis is chosen, and in the other, the 
word SUV is chosen. Both are allowed by the 
alignments illustrated in Figure 1. Finally, each 
CS state generates one or more complet  
decoding states, in which a word is chosen from 
the current CS and the EPV vector is advanced to 
reflect the last newly visited words. Figure 3d) 
shows two such states, descending from the 
corresponding CS states in 3c). After one more 
expansion the state in 3d) on the left can generate 
the translation ?She bought the Jeep SUV?, 
which cannot be produced by either confusion 
network in Figure 2. 
5.2 Pruning 
The full search space of joint decoding is a 
product of the alignment, ordering, and lexical 
selection spaces. Its size is exponential in the 
length of the sentence and the number of 
hypotheses involved in combination. Therefore, 
pruning techniques are necessary to reduce the 
search space.  
First we will prune down the alignment space. 
Instead of allowing any alignment link between 
1207
arbitrary words of two hypotheses, only links 
that have alignment score higher than a threshold 
are allowed, plus links in the union of the Viterbi 
alignments in both directions. In order to prevent 
the garbage collection problem where many 
words align to a rare word at the other side 
(Moore, 2004), we further impose the limit that if 
one word is aligned to more than T words, these 
links are sorted by their alignment score and only 
the top T links are kept. Meanwhile, alignments 
between a real word and ? are always allowed.  
We then prune down the ordering space by 
limiting the expansion of new states. Only states 
that are adjacent to their preceding states are 
created. Two states are called adjacent if their 
EPVs are adjacent, i.e., given the EPV of the 
preceding state m as  ?? ,1 ,? , ?? ,? 
?
 and the 
EPV of the next state m+1 as 
 ??+1,1,? , ??+1,? 
?
, if at least at one 
dimension k, ??+1,?  = ?? ,?+1, then these two 
states are adjacent. When checking the 
adjacency of two states, the position of an 
empty word is taken to be the same as the 
position of the last visited non-empty word 
from the same hypothesis.  
The number of possible CS states expanded 
from a decoding state is exponential in the 
number of hypotheses. In decoding, these CS 
states are sorted by their alignment scores and 
only the top K CS states are kept.  
The search space can be further pruned down 
by the widely used technique of path 
recombination and by best-first pruning.  
Path recombination is a risk-free pruning 
method. Two paths can be recombined if they 
agree on a) words from each hypothesis that have 
been visited so far, b) the last two real words 
generated, and c) their EPVs. In such case, we 
only need to keep the path with the higher score. 
Best-first pruning can help to reduce the 
search space even further. In the decoding 
process we compare paths that have generated 
the same number of words (both real and empty 
words) and only keep a certain number of most 
promising paths. Pruning is based on an 
estimated overall score of each path, which is the 
sum of the decoding score accumulated so far 
and an estimated future score to cover the words 
that have not been visited. Next we discuss the 
future score computation. 
5.3 Computing the future score 
In order to estimate the future cost of an 
unfinished path, we treat the unvisited words of 
one input hypothesis as a backbone, and apply a 
greedy search for alignment based on it; i.e., for 
each word of this backbone, the most likely 
words (based on the alignment link scores) from 
other hypotheses, one word from each 
hypothesis, are collected to form a CS. These CS 
are ordered according to the word order of the 
backbone and form a CN. Then, a light decoding 
process with a search beam of size one is applied 
to decode this CN and find the approximate 
future path, with future feature scores computed 
during the decoding process. If there are leftover 
words not included in this CN, they are treated in 
the way described in section 5.4. Additionally, 
caching techniques are applied to speed up the 
computation of future scores further.   
Given the method discussed above, we can 
estimate a future score based on each input 
hypothesis, and the final future score is estimated 
as the best of these hypothesis-dependent scores.  
5.4  Dealing with leftover input words  
At a certain point a path will reach the end, i.e., 
no more states can be generated from it 
according to the state expansion requirement. 
Then it is marked as a finished path. However, 
sometimes the state may contain a few input 
words that have not been visited. An example of 
this situation is the second state in Figure 3d). 
The word SUV in the third input hypothesis is 
left unvisited and it cannot be selected next 
because there is no adjacent state that can be 
generated. For such cases, we need to compute 
an extra score of covering these leftover words. 
Our approach is to create a state that produces 
the same output translation, but also covers all 
remaining words. For each leftover word, we 
create a pseudo CS that contains just that word 
plus ??s from all other hypotheses, and let it 
output ?. Moreover, that CS is inserted at a place 
such that no extra distortion cost is incurred. 
Figure 4 shows an example using the second 
state in Figure 3d). The last two words from the 
first two MT hypotheses ?the Jeep? and ?the 
SUV? align to the third and fifth words of the 
third hypothesis ?the Jeep?; the word w3,4 from 
the third hypothesis is left unvisited. The original 
path has two CS and one left-over word w3,4. It is 
expanded to have three CS, with a pseudo CS 
inserted between the two CS.  
It is worth noting that the new inserted pseudo 
CS will not affect the word count feature and 
contextually dependent feature scores such as the 
LM and bi-gram voting, since it only generates 
an empty word. Moreover, it will not affect the 
1208
distortion score either. For example, as shown in 
Figure 4, the distortion cost of jumping from 
word w2,3  to ?2  and then to w2,4 is the same as  
the cost of jumping from w2,3  to w2,4 given the 
way we assign position to empty word and the 
fact that the distortion cost is proportional to the 
difference between word positions.  
Scores of other features for this pseudo CS 
such as word posterior (of ?), alignment score, 
CS entropy, and CS count are all local scores and 
can be computed easily. Unlike future scores 
which are approximate, the score computed in 
this process is exact. Adding this extra score to 
the existing score accumulated in the final state 
gives the complete score of this finished path. 
When all paths are finished, the one with the best 
complete score is returned as the final output 
sentence. 
 
w1,3 w1,4   w1,3 ?1 w1,4  
w2,3 w2,4  =>  w2,3  ?2  w2,4  
w3,3 w3,4 w3,5  w3,3 w3,4 w3,5  
Figure 4. Expanding a leftover word to a pseudo 
correspondence set. 
6 Evaluation  
6.1 Experimental conditions 
For the joint decoding method, the threshold for 
alignment-score-based pruning is set to 0.25 and 
the maximum number of words that can align to 
the same word is limited to 3. We call this the 
standard setting. The joint decoding approach is 
evaluated on the Chinese-to-English (C2E) test 
set of the 2008 NIST Open MT Evaluation 
(NIST 2008). Results are reported in case 
insensitive BLEU score in percentages 
(Papineni et. al., 2002).  
The NIST MT08 C2E test set contains 691 
and 666 sentences of data from two genres, 
newswire and web-data, respectively. Each test 
sentence has four references provided by human 
translators. Individual systems in our 
experiments belong to the official submissions of 
the MT08 C2E constraint-training track. Each 
submission provides 1-best translation of the 
whole test set. In order to train feature weights, 
the original test set is divided into two parts, 
called the dev and test set, respectively. The dev 
set consists of the first half of both newswire and 
web-data, and the test set consists of the second 
half of data of both genres.   
There are 20 individual systems available. We 
ranked them by their BLEU score results on the 
dev set and picked the top five systems, 
excluding systems ranked 5th and 6th since they 
are subsets of the first entry (NIST 2008). 
Performance of these systems on the dev and test 
sets is shown in Table 1.  
The baselines include a pair-wise hypothesis 
alignment approach using the indirect HMM 
(IHMM) proposed by He et al (2008), and an 
incremental hypothesis alignment approach using 
the incremental HMM (IncHMM) proposed by 
Li et al (2009). The lexical translation model 
used to compute the semantic similarity is 
estimated from two million parallel sentence-
pairs selected from the training corpus of MT08. 
The backbone for the IHMM-based approach is 
selected based on Minimum Bayes Risk (MBR) 
using a BLEU-based loss function. The various 
parameters of the IHMM and the IncHMM are 
tuned on the dev set. The same IHMM is used to 
compute the alignment feature score for the joint 
decoding approach.  
The final combination output can be obtained 
by decoding the CN with a set of features. The 
features used for the baseline systems are the 
same as the features used by the joint decoding 
approach. Some of these features are constant 
across decoding hypotheses and can be ignored. 
The non-constant features are word posterior, bi-
gram voting, language model score, and word 
count. They are computed in the same way as for 
the joint decoding approach.  
System weights and feature weights are 
trained together using Powell's search for the 
IHMM-based approach. Then the same system 
weights are applied to both IncHMM and Joint 
Decoding -based approaches, and the feature 
weights of them are trained using the max-BLEU 
training method proposed by Och (2003) and 
refined by Moore and Quirk (2008).  
Table 1: Performance of individual systems on 
the dev and test set 
System ID dev test 
System A 32.88 31.81 
System B 32.82 32.03 
System C 32.16 31.87 
System D 31.40 31.32 
System E 27.44 27.67 
6.2 Comparison against baselines 
Table 2 lists the BLEU scores achieved by the 
two baselines and the joint decoding approach. 
Both baselines surpass the best individual system 
1209
significantly. However, the gain of incremental 
HMM over IHMM is smaller than that reported 
in Li et al (2009). One possible reason of such 
discrepancy could be that fewer hypotheses are 
used for combination in this experiment 
compared to that of Li et al (2009), so the 
performance difference between them is 
narrowed accordingly. Despite that, the proposed 
joint decoding method outperforms both IHMM 
and IncHMM baselines significantly.  
Table 2: Comparison between the joint decoding 
approach and the two baselines 
method dev test 
IHMM 36.91 35.85 
IncHMM 37.32 36.38 
Joint Decoding 37.94 37.20* 
* The gains of Joint Decoding over IHMM and 
IncHMM are both with a statistical significance level > 
99%, measured based on the paired bootstrap re-
sampling method (Koehn 2004a) 
6.3 Comparison of alignment pruning  
The effect of alignment pruning is also studied. 
We tested with limiting the allowable links to 
just those that in the union of bi-directional 
Viterbi alignments.  
The results are presented in Table 3. 
Compared to the standard setting, allowing only 
links in the union of the bi-directional Viterbi 
alignments causes slight performance 
degradation. On the other hand, it still 
outperforms the IHMM baseline by a fair margin. 
This is because the joint decoding approach is 
effectively resolving the ambiguous 1-to-many 
alignments and deciding proper places to insert 
empty words during decoding. 
Table 3: Comparison between different settings 
of alignment pruning 
Setting Test 
standard settings 37.20 
union of Viterbi 36.88 
6.4 Comparison of ordering constraints 
In order to investigate the effect of allowing 
flexible word ordering, we conducted 
experiments using different constraints on the 
ordering of CS in the decoding process. In the 
first case, we restrict the order of CS to follow 
the word order of a backbone, which is one of 
the input hypotheses selected by MBR-BLEU. In 
the second case, the order of CS is constrained to 
follow the word order of at least one of the input 
hypotheses.  As shown in Table 4, in comparison 
to the standard setting that allows backbone-free 
word ordering, the constrained settings did not 
lead to significant performance degradation. This 
indicates that most of the gain due to the joint 
decoding approach comes from the joint 
optimization of alignment and word selection. It 
is possible, though, that if we lift the CS 
adjacency constraint during search, we might 
derive more benefit from flexible word ordering.  
Table 4: Effect of ordering constraints 
Setting test 
standard settings 37.20 
monotone w.r.t. backbone 37.22 
monotone w.r.t. any hyp. 37.12 
7 Discussion 
This paper proposed a joint optimization 
approach for word-level combination of 
translation hypotheses from multiple machine 
translation systems. Unlike conventional 
confusion-network-based methods, alignments 
between words from different hypotheses are not 
pre-determined and flexible word orderings are 
allowed. Decisions on word alignment between 
hypotheses, word ordering, and the lexical choice 
of the final output are made jointly according to 
a set of features in the decoding process. A new 
set of features to model alignment and re-
ordering behavior is also proposed. The method 
is evaluated against state-of-the-art baselines on 
the NIST MT08 C2E task. The joint decoding 
approach is shown to outperform baselines 
significantly. 
Because of the complexity of search, a 
challenge for our approach is combining a large 
number of input hypotheses. When N-best 
hypotheses from the same system are added, it is 
possible to pre-compute and fix the one-to-one 
word alignment among the same-system 
hypotheses; such pre-computation is reasonable 
given our observation that the disagreement 
among hypotheses from different systems is 
larger than that among hypotheses from the same 
system. This will reduce the alignment search 
space to be the same as that for 1-best case. We 
plan to study this setting in future work. 
To further improve the performance of our 
approach we see the biggest opportunity in 
developing better estimates of future scores and 
incorporating additional features. Beside 
potential performance improvement, they may 
help on more effective pruning and speed up the 
overall decoding process as well. 
1210
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In 
Proceedings of IEEE ASRU. 
 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore 2008. Indirect HMM 
based Hypothesis Alignment for Combining 
Outputs from Machine Translation Systems. In 
Proceedings of EMNLP. 
 
Shyamsundar Jayaraman and Alon Lavie. 2005. 
Multi-engine machine translation guided by explicit 
word matching. In Proceedings of EAMT. 
 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proceedings of ACL. 
 
Philipp Koehn, 2004a, Statistical Significance Tests 
for Machine Translation Evaluation. In Proceedings 
of EMNLP. 
 
Philipp Koehn. 2004b. Pharaoh: A Beam Search 
Decoder For Phrase Based Statistical Machine 
Translation Models. In Proceedings of AMTA. 
 
Chi-Ho Li, Xiaodong He, Yupeng Liu and Ning Xi, 
2009. Incremental HMM Alignment for MT 
System Combination. In Proceedings of ACL. 
 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Personal Communication  
 
Evgeny Matusov, Nicola Ueffing and Hermann Ney. 
2006. Computing Consensus Translation from 
Multiple Machine Translation Systems using 
Enhanced Hypothesis Alignment. In Proceedings of 
EACL. 
 
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs, 
Nicola Bertoldi, Daniel D?chelotte, Marcello 
Federico, Muntsin Kolss, Young-Suk Lee, Jos? B. 
Mari?o, Matthias Paulik, Salim Roukos, Holger 
Schwenk, and Hermann Ney. 2008. System 
combination for  machine translation of spoken and 
written language. IEEE transactions on audio 
speech and language processing 16(7). 
 
Robert C. Moore and Chris Quirk. 2008. Random 
Restarts in Minimum Error Rate Training for 
Statistical Machine Translation, In Proceedings of 
COLING 
 
Robert C. Moore. 2004. Improving IBM Word 
Alignment Model 1, In Proceedings of ACL. 
 
NIST 2008. The NIST Open Machine Translation 
Evaluation.www.nist.gov/speech/tests/mt/2008/doc/ 
 
Franz J. Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL.  
 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei- Jing Zhu 2002. BLEU: a Method for 
Automatic Evaluation of Machine Translation. In 
Proceedings of ACL. 
 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proceedings of 
NAACL-HLT. 
 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz 2007b. Improved Word-level System 
Combination for Machine Translation. In 
Proceedings of ACL. 
 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz 2008. Incremental 
Hypothesis Alignment for Building Confusion 
Networks with Application to Machine Translation 
System Combination. In Proceedings of the 3rd 
ACL Workshop on SMT. 
 
Yong Zhao and Xiaodong He, 2009. Using N-gram 
based Features for Machine Translation System 
Combination. In Proceedings of NAACL-HLT 
 
 
 
1211
Proceedings of NAACL HLT 2009: Short Papers, pages 205?208,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using N-gram based Features for Machine Translation                    
System Combination 
 
Yong Zhao1 Xiaodong He 
Georgia Institute of Technology Microsoft Research 
Atlanta, GA 30332, USA Redmond, WA 98052, USA 
yongzhao@gatech.edu xiaohe@microsoft.com 
  
Abstract 
Conventional confusion network based 
system combination for machine translation 
(MT) heavily relies on features that are 
based on the measure of agreement of 
words in different translation hypotheses. 
This paper presents two new features that 
consider agreement of n-grams in different 
hypotheses to improve the performance of 
system combination. The first one is based 
on a sentence specific online n-gram 
language model, and the second one is 
based on n-gram voting. Experiments on a 
large scale Chinese-to-English MT task 
show that both features yield significant 
improvements on the translation 
performance, and a combination of them 
produces even better translation results. 
1 Introduction1 
In past years, the confusion network based system 
combination approach has been shown with 
substantial improvements in various machine 
translation (MT) tasks (Bangalore, et. al., 2001, 
Matusov, et. al., 2006, Rosti, et. al., 2007, He, 
et. al., 2008). Given hypotheses of multiple 
systems, a confusion network is built by aligning 
all these hypotheses. The resulting network 
comprises a sequence of correspondence sets, each 
of which contains the alternative words that are 
aligned with each other. To derive a consensus 
hypothesis from the confusion network, decoding 
is performed by selecting a path with the maximum 
overall confidence score among all paths that pass 
the confusion network (Goel, et. al., 2004).  
                                                          
1 The work was performed when Yong Zhao was an intern at 
Microsoft Research 
The confidence score of a hypothesis could be 
assigned in various ways. Fiscus (1997) used 
voting by frequency of word occurrences. Mangu 
et. al., (2000) computed a word posterior 
probability based on voting of that word in 
different hypotheses. Moreover, the overall 
confidence score is usually formulated as a log-
linear model including extra features including 
language model (LM) score, word count, etc.  
Features based on word agreement measure are 
extensively studied in past work (Matusov, et. al., 
2006, Rosti, et. al., 2007, He, et. al., 2008). 
However, utilization of n-gram agreement 
information among the hypotheses has not been 
fully explored yet. Moreover, it was argued that 
the confusion network decoding may introduce 
undesirable spur words that break coherent 
phrases (Sim, et. al., 2007). Therefore, we would 
prefer the consensus translation that has better n-
gram agreement among outputs of single systems.  
In the literature, Zens and Ney (2004) 
proposed an n-gram posterior probability based 
LM for MT. For each source sentence, a LM is 
trained on the n-best list produced by a single MT 
system and is used to re-rank that n-best list itself. 
On the other hand, Matusov et al (2008) proposed 
an ?adapted? LM for system combination, where 
this ?adapted? LM is trained on translation 
hypotheses of the whole test corpus from all single 
MT systems involved in system combination.  
Inspired by these ideas, we propose two new 
features based on n-gram agreement measure to 
improve the performance of system combination. 
The first one is a sentence specific LM built on 
translation hypotheses of multiple systems; the 
second one is n-gram-voting-based confidence. 
Experimental results are presented in the context of 
a large-scale Chinese-English translation task.  
205
2 System Combination for MT  
One of the most successful approaches for 
system combination for MT is based on 
confusion network decoding as described in 
(Rosti, et. al., 2007). Given translation 
hypotheses from multiple MT systems, one of 
the hypotheses is selected as the backbone for 
the use of hypothesis alignment. This is usually 
done by a sentence-level minimum Bayes risk 
(MBR) re-ranking method. The confusion 
network is constructed by aligning all these 
hypotheses against the backbone. Words that 
align to each other are grouped into a 
correspondence set, constituting competition 
links of the confusion network. Each path in the 
network passes exactly one link from each 
correspondence set. The final consensus output 
relies on a decoding procedure that chooses a 
path with the maximum confidence score among 
all paths that pass the confusion network. 
 The confidence score of a hypothesis is 
usually formalized as a log-linear sum of several 
feature functions. Given a source language 
sentence ? , the total confidence of a target 
language hypothesis ? = (?1 ,? , ??)  in the 
confusion network can be represented as:  
???? ? ? = ??? ? ?? ?,? 
?
?=1
+ ?1?????? ? 
+ ?2?????? (?) 
(1) 
where the feature functions include word 
posterior probability  ?(?? |?,?), LM probability 
???(?), and the number of real words ??????  in 
? . Usually, the model parameter ?i could be 
trained by optimizing an evaluation metric, e.g., 
BLEU score, on a held-out development set.  
3 N-gram Online Language Model  
Given a source sentence ?, the fractional count 
? ?1
? ?  of an n-gram ?1
?  is defined as: 
? ?1
?  ? =   ? ??  ? 
?
?=??????
?(? ? ???+1
?
, ?1
?) (2) 
where ??  denotes the hypothesis set, ? ?,?  
denotes the Kronecker function, and ?(?? |?) is 
the posterior probability of translation 
hypothesis ?? , which is expressed as the 
weighted sum of the system specific posterior 
probabilities through the systems that contains 
hypothesis ?? , 
? ? ? =  ???(?
?
?=1
 ?? ,? 1(? ? ??? ) (3) 
where ??  is the weight for the posterior 
probability of the kth system ?? , and 1 ?  is the 
indicator function.   
Follows Rosti, et. al. (2007), system specific 
posteriors are derived based on a rank-based 
scoring scheme. I.e., if translation hypothesis ??  
is the rth best output in the n-best list of system 
?? , posterior ? ??  ?? ,?  is approximated as:  
? ??  ?? ,? =
1/(1 + ?)?  
 1/(1 + ??)?
||??? ||
? ?=1
 (4) 
where ? is a rank smoothing parameter. 
Similar to (Zens and Ney, 2004), a 
straightforward approach of using n-gram 
fractional counts is to formulate it as a sentence 
specific online LM. Then the online LM score 
of a path in the confusion network will be added 
as an additional feature in the log-linear model 
for decoding. The online n-gram LM score is 
computed by: 
?(?? |????+1
??1 ,?) =
?(????+1
? |?)
?(????+1
??1 |?)
 (5) 
The LM score of hypothesis ? is obtained by: 
??? ? ? = ? ?? |????+1
??1 ,? 
?
?=?
 (6) 
Since new n-grams unseen in original 
translation hypotheses may be proposed by the 
CN decoder, LM smoothing is critical. In our 
approach, the score of the online LM is 
smoothed by taking a linear interpolation to 
combine scores of different orders.  
206
?????? ? ?? |????+1
??1 ,? 
=  ???(?? |????+1
??1 ,?)
?
?=1
 
(7) 
In our implementation, the interpolation weights 
{ ?? } can be learned along with other 
combination parameters in the same Max-BLEU 
training scheme via Powell's search.  
4 N-gram-Voting-Based Confidence  
Motivated by features based on voting of single 
word, we proposed new features based on N-
gram voting. The voting score ? ?1
?  ?  of an n-
gram ?1
?  is computed as: 
? ?1
?  ? =  ? ??  ? 1(?1
? ? ??)?????       (8) 
It receives a vote from each hypothesis that 
contains that n-gram, and weighted by the 
posterior probability of that hypothesis, where 
the posterior probability ? ??  ?  is computed by 
(3). Unlike the fractional count, each hypothesis 
can vote no more than once on an n-gram. 
? ?1
?  ?  takes a value between 0 and 1. It 
can be viewed as the confidence of the n-gram 
?1
? . Then the n-gram-voting-based confidence 
score of a hypothesis ?  is computed as the 
product of confidence scores of n-grams in E: 
??? ,? ? ? = ??? ,? ?1
?  ?,? =
                                  ?(??
?+??1|?)???+1?=1   
(9) 
where n can take the value of 2, 3, ?, N. In 
order to prevent zero confidence, a small back-
off confidence score is assigned to all n-grams 
unseen in original hypotheses.  
Augmented with the proposed n-gram based 
features, the final log-linear model becomes: 
???? ? ? 
= ???? ??  ?,? 
?
?=1
+ ?1?????? ? 
+ ?2??????  ? + ?3?????? ? ? 
+ ??+2?????? ,? ? ? 
?
?=2
              
(10) 
5 Evaluation 
We evaluate the proposed n-gram based features 
on the Chinese-to-English (C2E) test in the past 
NIST Open MT Evaluations. The experimental 
results are reported in case sensitive BLEU 
score (Papineni, et. al., 2002). 
The dev set, which is used for system 
combination parameter training, is the newswire 
and newsgroup parts of NIST MT06, which 
contains a total of 1099 sentences. The test set is 
the "current" test set of NIST MT08, which 
contains 1357 sentences of newswire and web-
blog data. Both dev and test sets have four 
reference translations per sentence.  
Outputs from a total of eight single MT 
systems were combined for consensus 
translations. These selected systems are based 
on various translation paradigms, such as 
phrasal, hierarchical, and syntax-based systems. 
Each system produces 10-best hypotheses per 
translation. The BLEU score range for the eight 
individual systems are from 26.11% to 31.09% 
on the dev set and from 20.42% to 26.24% on 
the test set. In our experiments, a state-of-the-art 
system combination method proposed by He, et. 
al. (2008) is implemented as the baseline. The 
true-casing model proposed by Toutanova et al 
(2008) is used. 
Table 1 shows results of adding the online 
LM feature. Different LM orders up to four are 
tested. Results show that using a 2-gram online 
LM yields a half BLEU point gain over the 
baseline. However, the gain is saturated after a 
LM order of three, and fluctuates after that.  
Table 2 shows the performance of using n-
gram-voting-based confidence features. The best 
result of 31.01% is achieved when up to 4-gram 
confidence features are used. The BLEU score 
keeps improving when longer n-gram 
confidence features are added. This indicates 
that the n-gram voting based confidence feature 
is robust to high order n-grams. 
We further experimented with incorporating 
both features in the log-linear model and 
reported the results in Table 3. Given the 
observation that the n-gram voting based 
confidence feature is more robust to high order 
n-grams, we further tested using different n-
gram orders for them. As shown in Table 3, 
using 3-gram online LM plus 2~4-gram voting 
207
based confidence scores yields the best BLEU 
scores on both dev and test sets, which are 
37.98% and 31.35%, respectively. This is a 0.84 
BLEU point gain over the baseline on the MT08 
test set.  
Table 1: Results of adding the n-gram online LM. 
BLEU % Dev  Test  
Baseline 37.34 30.51 
1-gram online LM 37.34 30.51 
2-gram online LM 37.86 31.02 
3-gram online LM 37.87 31.08 
4-gram online LM 37.86 31.01 
Table 2: Results of adding n-gram voting based 
confidence features. 
BLEU % Dev  Test  
Baseline 37.34 30.51 
+ 2-gram voting 37.58 30.88 
+ 2~3-gram voting 37.66 30.96 
+ 2~4-gram voting 37.77 31.01 
Table 3: Results of using both n-gram online LM 
and n-gram voting based confidence features 
BLEU % Dev  Test  
Baseline 37.34 30.51 
2-gram LM + 2-gram voting 37.78 30.98 
3-gram LM + 2~3-gram voting 37.89 31.21 
4-gram LM + 2~4-gram voting 37.93 31.08 
3-gram LM + 2~4-gram voting 37.98 31.35 
 
6 Conclusion 
This work explored utilization of n-gram 
agreement information among translation 
outputs of multiple MT systems to improve the 
performance of system combination. This is an 
extension of an earlier idea presented at the 
NIPS 2008 Workshop on Speech and Language 
(Yong and He 2008). Two kinds of n-gram based 
features were proposed. The first is based on an 
online LM using n-gram fractional counts, and 
the second is a confidence feature based on n-
gram voting scores. Our experiments on the 
NIST MT08 Chinese-English task showed that 
both methods yield nice improvements on the 
translation results, and incorporating both kinds 
of features produced the best translation result 
with a BLEU score of 31.35%, which is a 0.84% 
improvement. 
 
References  
J.G. Fiscus, 1997. A post-processing system to yield 
reduced word error rates: Recognizer Output Voting 
Error Reduction (ROVER), in Proc. ASRU. 
S. Bangalore, G. Bordel, and G. Riccardi, 2001. 
Computing consensus translation from multiple 
machine translation systems, in Proc. ASRU. 
E. Matusov, N. Ueffing, and H. Ney, 2006. 
Computing consensus translation from multiple 
machine translation systems using enhanced 
hypotheses alignment, in Proc. EACL. 
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz, 2007. 
Improved Word-Level System Combination for 
Machine Translation. In Proc. ACL. 
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore, 
2008. Indirect-HMM-based hypothesis alignment for 
combining outputs from machine translation 
systems, in Proc. EMNLP. 
L. Mangu, E. Brill, and A. Stolcke, 2000. Finding 
Consensus in Speech Recognition: Word Error 
Minimization and Other Applications of Confusion 
Networks, Computer Speech and Language, 
14(4):373-400. 
R. Zens and H. Ney, 2004. N-Gram posterior 
probabilities for statistical machine translation, in 
Proc. HLT-NAACL. 
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and 
P.C. Woodland, 2007. Consensus network decoding 
for statistical machine translation system 
combination. in Proc. ICASSP. 
V. Goel, S. Kumar, and W. Byrne, 2004. Segmental 
minimum Bayes-risk decoding for automatic speech 
recognition. IEEE transactions on Speech and Audio 
Processing, vol. 12, no. 3. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu, 2002. 
BLEU: a method for automatic evaluation of 
machine translation. in Proc. ACL. 
K. Toutanova, H. Suzuki and A. Ruopp. 2008. Applying 
Morphology Generation Models to Machine 
Translation. In Proc. of ACL. 
Yong Zhao and Xiaodong He. 2008. System 
Combination for Machine Translation Using N-Gram 
Posterior Probabilities. NIPS 2008 WORKSHOP on 
Speech and Language: Learning-based Methods and 
Systems. Dec. 2008 
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. 
Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B. 
Marino, M. Paulik, S. Roukos, H. Schwenk, and H. 
Ney. 2008. System Combination for Machine 
Translation of Spoken and Written Language. IEEE 
Transactions on Audio, Speech and Language 
Processing, Sept. 2008. 
208
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 949?957,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Incremental HMM Alignment for MT System Combination
Chi-Ho Li
Microsoft Research Asia
49 Zhichun Road, Beijing, China
chl@microsoft.com
Xiaodong He
Microsoft Research
One Microsoft Way, Redmond, USA
xiaohe@microsoft.com
Yupeng Liu
Harbin Institute of Technology
92 Xidazhi Street, Harbin, China
ypliu@mtlab.hit.edu.cn
Ning Xi
Nanjing University
8 Hankou Road, Nanjing, China
xin@nlp.nju.edu.cn
Abstract
Inspired by the incremental TER align-
ment, we re-designed the Indirect HMM
(IHMM) alignment, which is one of the
best hypothesis alignment methods for
conventional MT system combination, in
an incremental manner. One crucial prob-
lem of incremental alignment is to align a
hypothesis to a confusion network (CN).
Our incremental IHMM alignment is im-
plemented in three different ways: 1) treat
CN spans as HMM states and define state
transition as distortion over covered n-
grams between two spans; 2) treat CN
spans as HMM states and define state tran-
sition as distortion over words in compo-
nent translations in the CN; and 3) use
a consensus decoding algorithm over one
hypothesis and multiple IHMMs, each of
which corresponds to a component trans-
lation in the CN. All these three ap-
proaches of incremental alignment based
on IHMM are shown to be superior to both
incremental TER alignment and conven-
tional IHMM alignment in the setting of
the Chinese-to-English track of the 2008
NIST Open MT evaluation.
1 Introduction
Word-level combination using confusion network
(Matusov et al (2006) and Rosti et al (2007)) is a
widely adopted approach for combining Machine
Translation (MT) systems? output. Word align-
ment between a backbone (or skeleton) translation
and a hypothesis translation is a key problem in
this approach. Translation Edit Rate (TER, Snover
et al (2006)) based alignment proposed in Sim
et al (2007) is often taken as the baseline, and
a couple of other approaches, such as the Indi-
rect Hidden Markov Model (IHMM, He et al
(2008)) and the ITG-based alignment (Karakos et
al. (2008)), were recently proposed with better re-
sults reported. With an alignment method, each
hypothesis is aligned against the backbone and all
the alignments are then used to build a confusion
network (CN) for generating a better translation.
However, as pointed out by Rosti et al (2008),
such a pair-wise alignment strategy will produce
a low-quality CN if there are errors in the align-
ment of any of the hypotheses, no matter how good
the alignments of other hypotheses are. For ex-
ample, suppose we have the backbone ?he buys a
computer? and two hypotheses ?he bought a lap-
top computer? and ?he buys a laptop?. It will be
natural for most alignment methods to produce the
alignments in Figure 1a. The alignment of hypoth-
esis 2 against the backbone cannot be considered
an error if we consider only these two translations;
nevertheless, when added with the alignment of
another hypothesis, it produces the low-quality
CN in Figure 1b, which may generate poor trans-
lations like ?he bought a laptop laptop?. While it
could be argued that such poor translations are un-
likely to be selected due to language model, this
CN does disperse the votes to the word ?laptop? to
two distinct arcs.
Rosti et al (2008) showed that this problem can
be rectified by incremental alignment. If hypoth-
esis 1 is first aligned against the backbone, the
CN thus produced (depicted in Figure 2a) is then
aligned to hypothesis 2, giving rise to the good CN
as depicted in Figure 2b.1 On the other hand, the
1Note that this CN may generate an incomplete sentence
?he bought a?, which is nevertheless unlikely to be selected
as it leads to low language model score.
949
Figure 1: An example bad confusion network due
to pair-wise alignment strategy
correct result depends on the order of hypotheses.
If hypothesis 2 is aligned before hypothesis 1, the
final CN will not be good. Therefore, the obser-
vation in Rosti et al (2008) that different order
of hypotheses does not affect translation quality is
counter-intuitive.
This paper attempts to answer two questions: 1)
as incremental TER alignment gives better perfor-
mance than pair-wise TER alignment, would the
incremental strategy still be better than the pair-
wise strategy if the TER method is replaced by
another alignment method? 2) how does transla-
tion quality vary for different orders of hypotheses
being incrementally added into a CN? For ques-
tion 1, we will focus on the IHMM alignment
method and propose three different ways of imple-
menting incremental IHMM alignment. Our ex-
periments will also try several orders of hypothe-
ses in response to question 2.
This paper is structured as follows. After set-
ting the notations on CN in section 2, we will
first introduce, in section 3, two variations of the
basic incremental IHMM model (IncIHMM1 and
IncIHMM2). In section 4, a consensus decoding
algorithm (CD-IHMM) is proposed as an alterna-
tive way to search for the optimal alignment. The
issues of alignment normalization and the order of
hypotheses being added into a CN are discussed in
sections 5 and 6 respectively. Experiment results
and analysis are presented in section 7.
Figure 2: An example good confusion network
due to incremental alignment strategy
2 Preliminaries: Notation on Confusion
Network
Before the elaboration of the models, let us first
clarify the notation on CN. A CN is usually de-
scribed as a finite state graph with many spans.
Each span corresponds to a word position and con-
tains several arcs, each of which represents an al-
ternative word (could be the empty symbol , ?) at
that position. Each arc is also associated with M
weights in an M -way system combination task.
Follow Rosti et al (2007), the i-th weight of an
arc is ?r 11+r , where r is the rank of the hypothe-
sis in the i-th system that votes for the word repre-
sented by the arc. This conception of CN is called
the conventional or compact form of CN. The net-
works in Figures 1b and 2b are examples.
On the other hand, as a CN is an integration
of the skeleton and all hypotheses, it can be con-
ceived as a list of the component translations. For
example, the CN in Figure 2b can be converted
to the form in Figure 3. In such an expanded or
tabular form, each row represents a component
translation. Each column, which is equivalent to
a span in the compact form, comprises the alter-
native words at a word position. Thus each cell
represents an alternative word at certain word po-
sition voted by certain translation. Each row is as-
signed the weight 11+r , where r is the rank of the
translation of some MT system. It is assumed that
all MT systems are weighted equally and thus the
950
Figure 3: An example of confusion network in tab-
ular form
rank-based weights from different system can be
compared to each other without adjustment. The
weight of a cell is the same as the weight of the
corresponding row. In this paper the elaboration
of the incremental IHMM models is based on such
tabular form of CN.
Let EI1 = (E1 . . . EI) denote the backbone CN,
and e?J1 = (e?1 . . . e?J) denote a hypothesis being
aligned to the backbone. Each e?j is simply a word
in the target language. However, each Ei is a span,
or a column, of the CN. We will also use E(k) to
denote the k-th row of the tabular form CN, and
Ei(k) to denote the cell at the k-th row and the
i-th column. W (k) is the weight for E(k), and
Wi(k) = W (k) is the weight for Ei(k). pi(k)
is the normalized weight for the cell Ei(k), such
that pi(k) = Wi(k)?
i Wi(k)
. Note that E(k) contains
the same bag-of-words as the k-th original trans-
lation, but may have different word order. Note
also that E(k) represents a word sequence with
inserted empty symbols; the sequence with all in-
serted symbols removed is known as the compact
form of E(k).
3 The Basic IncIHMM Model
A na??ve application of the incremental strategy to
IHMM is to treat a span in the CN as an HMM
state. Like He et al (2008), the conditional prob-
ability of the hypothesis given the backbone CN
can be decomposed into similarity model and dis-
tortion model in accordance with equation 1
p(e?J1 |EI1) =
?
aJ1
J?
j=1
[p(aj |aj?1, I)p(e?j |eaj )] (1)
The similarity between a hypothesis word e?j and
a span Ei is simply a weighted sum of the similar-
ities between e?j and each word contained in Ei as
equation 2:
p(e?j |Ei) =
?
Ei(k)?Ei
pi(k) ? p(e?j |Ei(k)) (2)
The similarity between two words is estimated in
exactly the same way as in conventional IHMM
alignment.
As to the distortion model, the incremental
IHMM model also groups distortion parameters
into a few ?buckets?:
c(d) = (1 + |d? 1|)?K
The problem in incremental IHMM is when to ap-
ply a bucket. In conventional IHMM, the transi-
tion from state i to j has probability:
p?(j|i, I) = c(j ? i)?I
l=1 c(l ? i)
(3)
It is tempting to apply the same formula to the
transitions in incremental IHMM. However, the
backbone in the incremental IHMM has a special
property that it is gradually expanding due to the
insertion operator. For example, initially the back-
bone CN contains the option ei in the i-th span and
the option ei+1 in the (i+1)-th span. After the first
round alignment, perhaps ei is aligned to the hy-
pothesis word e?j , ei+1 to e?j+2, and the hypothesis
word e?j+1 is left unaligned. Then the consequent
CN have an extra span containing the option e?j+1
inserted between the i-th and (i + 1)-th spans of
the initial CN. If the distortion buckets are applied
as in equation 3, then in the first round alignment,
the transition from the span containing ei to that
containing ei+1 is based on the bucket c(1), but
in the second round alignment, the same transition
will be based on the bucket c(2). It is therefore not
reasonable to apply equation 3 to such gradually
extending backbone as the monotonic alignment
assumption behind the equation no longer holds.
There are two possible ways to tackle this prob-
lem. The first solution estimates the transition
probability as a weighted average of different dis-
tortion probabilities, whereas the second solution
converts the distortion over spans to the distortion
over the words in each hypothesis E(k) in the CN.
3.1 Distortion Model 1: simple weighting of
covered n-grams
Distortion Model 1 shifts the monotonic alignment
assumption from spans of CN to n-grams covered
by state transitions. Let us illustrate this point with
the following examples.
In conventional IHMM, the distortion probabil-
ity p?(i + 1|i, I) is applied to the transition from
state i to i+1 given I states because such transition
951
jumps across only one word, viz. the i-th word of
the backbone. In incremental IHMM, suppose the
i-th span covers two arcs ea and ?, with probabili-
ties p1 and p2 = 1? p1 respectively, then the tran-
sition from state i to i+ 1 jumps across one word
(ea) with probability p1 and jumps across nothing
with probability p2. Thus the transition probabil-
ity should be p1 ? p?(i+ 1|i, I) + p2 ? p?(i|i, I).
Suppose further that the (i + 1)-th span covers
two arcs eb and ?, with probabilities p3 and p4 re-
spectively, then the transition from state i to i+ 2
covers 4 possible cases:
1. nothing (??) with probability p2 ? p4;
2. the unigram ea with probability p1 ? p4;
3. the unigram eb with probability p2 ? p3;
4. the bigram eaeb with probability p1 ? p3.
Accordingly the transition probability should be
p2p4p?(i|i, I) + p1p3p?(i+ 2|i, I) +
(p1p4 + p2p3)p?(i+ 1|i, I).
The estimation of transition probability can be
generalized to any transition from i to i? by ex-
panding all possible n-grams covered by the tran-
sition and calculating the corresponding probabil-
ities. We enumerate all possible cell sequences
S(i, i?) covered by the transition from span i to
i?; each sequence is assigned the probability
P i?i =
i??1?
q=i
pq(k).
where the cell at the i?-th span is on some row
E(k). Since a cell may represent an empty word,
a cell sequence may represent an n-gram where
0 ? n ? i? ? i (or 0 ? n ? i ? i? in backward
transition). We denote |S(i, i?)| to be the length of
n-gram represented by a particular cell sequence
S(i, i?). All the cell sequences S(i, i?) can be clas-
sified, with respect to the length of corresponding
n-grams, into a set of parameters where each ele-
ment (with a particular value of n) has the proba-
bility
P i?i (n; I) =
?
|S(i,i?)|=n
P i?i .
The probability of the transition from i to i? is:
p(i?|i, I) =
?
n
[P i?i (n; I) ? p?(i+ n|i, I)]. (4)
That is, the transition probability of incremental
IHMM is a weighted sum of probabilities of ?n-
gram jumping?, defined as conventional IHMM
distortion probabilities.
However, in practice it is not feasible to ex-
pand all possible n-grams covered by any transi-
tion since the number of n-grams grows exponen-
tially. Therefore a length limit L is imposed such
that for all state transitions where |i? ? i| ? L, the
transition probability is calculated as equation 4,
otherwise it is calculated by:
p(i?|i, I) = maxq p(i
?|q, I) ? p(q|i, I)
for some q between i and i?. In other words, the
probability of longer state transition is estimated
in terms of the probabilities of transitions shorter
or equal to the length limit.2 All the state transi-
tions can be calculated efficiently by dynamic pro-
gramming.
A fixed value P0 is assigned to transitions to
null state, which can be optimized on held-out
data. The overall distortion model is:
p?(j|i, I) =
{
P0 if j is null state
(1? P0)p(j|i, I) otherwise
3.2 Distortion Model 2: weighting of
distortions of component translations
The cause of the problem of distortion over CN
spans is the gradual extension of CN due to the
inserted empty words. Therefore, the problem
will disappear if the inserted empty words are re-
moved. The rationale of Distortion Model 2 is
that the distortion model is defined over the ac-
tual word sequence in each component translation
E(k).
Distortion Model 2 implements a CN in such a
way that the real position of the i-th word of the k-
th component translation can always be retrieved.
The real position of Ei(k), ?(i, k), refers to the
position of the word represented by Ei(k) in the
compact form of E(k) (i.e. the form without any
inserted empty words), or, if Ei(k) represents an
empty word, the position of the nearest preceding
non-empty word. For convenience, we also denote
by ??(i, k) the null state associated with the state
of the real word ?(i, k). Similarly, the real length
2This limit L is also imposed on the parameter I in distor-
tion probability p?(i?|i, I), because the value of I is growing
larger and larger during the incremental alignment process. I
is defined as L if I > L.
952
of E(k), L(k), refers to the number of non-empty
words of E(k).
The transition from span i? to i is then defined
as
p(i|i?) = 1?
k W (k)
?
k
[W (k) ? pk(i|i?)] (5)
where k is the row index of the tabular form CN.
Depending on Ei(k) and Ei?(k), pk(i|i?) is
computed as follows:
1. if both Ei(k) and Ei?(k) represent real
words, then
pk(i|i?) = p?(?(i, k)|?(i?, k), L(k))
where p? refers to the conventional IHMM
distortion probability as defined by equa-
tion 3.
2. if Ei(k) represents a real word but Ei?(k) the
empty word, then
pk(i|i?) = p?(?(i, k)|??(i?, k), L(k))
Like conventional HMM-based word align-
ment, the probability of the transition from a
null state to a real word state is the same as
that of the transition from the real word state
associated with that null state to the other real
word state. Therefore,
p?(?(i, k)|??(i?, k), L(k)) =
p?(?(i, k)|?(i?, k), L(k))
3. if Ei(k) represents the empty word but
Ei?(k) a real word, then
pk(i|i?) =
{
P0 if?(i, k) = ?(i?, k)
P0P?(i|i?; k) otherwise
where P?(i|i?; k) = p?(?(i, k)|?(i?, k), L(k)).
The second option is due to the constraint that
a null state is accessible only to itself or the
real word state associated with it. Therefore,
the transition from i? to i is in fact composed
of the first transition from i? to ?(i, k) and the
second transition from ?(i, k) to the null state
at i.
4. if both Ei(k) and Ei?(k) represent the empty
word, then, with similar logic as cases 2
and 3,
pk(i|i?) =
{
P0 if?(i, k) = ?(i?, k)
P0P?(i|i?; k) otherwise
4 Incremental Alignment using
Consensus Decoding over Multiple
IHMMs
The previous section describes an incremental
IHMM model in which the state space is based on
the CN taken as a whole. An alternative approach
is to conceive the rows (component translations)
in the CN as individuals, and transforms the align-
ment of a hypothesis against an entire network to
that against the individual translations. Each in-
dividual translation constitutes an IHMM and the
optimal alignment is obtained from consensus de-
coding over these multiple IHMMs.
Alignment over multiple sequential patterns has
been investigated in different contexts. For ex-
ample, Nair and Sreenivas (2007) proposed multi-
pattern dynamic time warping (MPDTW) to align
multiple speech utterances to each other. How-
ever, these methods usually assume that the align-
ment is monotonic. In this section, a consensus
decoding algorithm that searches for the optimal
(non-monotonic) alignment between a hypothesis
and a set of translations in a CN (which are already
aligned to each other) is developed as follows.
A prerequisite of the algorithm is a function
for converting a span index to the corresponding
HMM state index of a component translation. The
two functions ? and ?? s defined in section 3.2 are
used to define a new function:
??(i, k) =
{
??(i, k) if Ei(k) is null
?(i, k) otherwise
Accordingly, given the alignment aJ1 = a1 . . . aJ
of a hypothesis (with J words) against a CN
(where each aj is an index referring to the span
of the CN), we can obtain the alignment a?k =
??(a1, k) . . . ??(aJ , k) between the hypothesis and
the k-th row of the tabular CN. The real length
function L(k) is also used to obtain the number of
non-empty words of E(k).
Given the k-th row of a CN, E(k), an IHMM
?(k) is formed and the cost of the pair-wise align-
ment, a?k, between a hypothesis h and ?(k) is de-
fined as:
C(a?k;h, ?(k)) = ? logP (a?k|h, ?(k)) (6)
The cost of the alignment of h against a CN is then
defined as the weighted sum of the costs of the K
alignments a?k:
C(a;h,?) =
?
k
W (k)C(a?k;h, ?(k))
953
= ?
?
k
W (k) logP (a?k|h, ?(k))
where ? = {?(k)} is the set of pair-wise IHMMs,
and W (k) is the weight of the k-th row. The op-
timal alignment a? is the one that minimizes this
cost:
a? = argmaxa
?
k
W (k) logP (a?k|h, ?(k))
= argmaxa
?
k
W (k)[
?
j
[
logP (??(aj , k)|??(aj?1, k), L(k)) +
logP (ej |Ei(k))]]
= argmaxa
?
j
[
?
k
W (k) logP (??(aj , k)|??(aj?1, k), L(k)) +
?
k
W (k) logP (ej |Ei(k))]
= argmaxa
?
j
[logP ?(aj |aj?1) +
logP ?(ej |Eaj )]
A Viterbi-like dynamic programming algorithm
can be developed to search for a? by treating CN
spans as HMM states, with a pseudo emission
probability as
P ?(ej |Eaj ) =
K?
k=1
P (ej |Eaj (k))W (k)
and a pseudo transition probability as
P ?(j|i) =
K?
k=1
P (??(j, k)|??(i, k), L(k))W (k)
Note that P ?(ej |Eaj ) and P ?(j|i) are not true
probabilities and do not have the sum-to-one prop-
erty.
5 Alignment Normalization
After alignment, the backbone CN and the hypoth-
esis can be combined to form an even larger CN.
The same principles and heuristics for the con-
struction of CN in conventional system combina-
tion approaches can be applied. Our incremen-
tal alignment approaches adopt the same heuris-
tics for alignment normalization stated in He et al
(2008). There is one exception, though. All 1-
N mappings are not converted to N ? 1 ?-1 map-
pings since this conversion leads to N ? 1 inser-
tion in the CN and therefore extending the net-
work to an unreasonable length. The Viterbi align-
ment is abandoned if it contains an 1-N mapping.
The best alignment which contains no 1-N map-
ping is searched in the N-Best alignments in a way
inspired by Nilsson and Goldberger (2001). For
example, if both hypothesis words e?1 and e?2 are
aligned to the same backbone span E1, then all
alignments aj={1,2} = i (where i 6= 1) will be
examined. The alignment leading to the least re-
duction of Viterbi probability when replacing the
alignment aj={1,2} = 1 will be selected.
6 Order of Hypotheses
The default order of hypotheses in Rosti et al
(2008) is to rank the hypotheses in descending of
their TER scores against the backbone. This pa-
per attempts several other orders. The first one is
system-based order, i.e. assume an arbitrary order
of the MT systems and feeds all the translations
(in their original order) from a system before the
translations from the next system. The rationale
behind the system-based order is that the transla-
tions from the same system are much more similar
to each other than to the translations from other
systems, and it might be better to build CN by
incorporating similar translations first. The sec-
ond one is N-best rank-based order, which means,
rather than keeping the translations from the same
system as a block, we feed the top-1 translations
from all systems in some order of systems, and
then the second best translations from all systems,
and so on. The presumption of the rank-based or-
der is that top-ranked hypotheses are more reliable
and it seemed beneficial to incorporate more reli-
able hypotheses as early as possible. These two
kinds of order of hypotheses involve a certain de-
gree of randomness as the order of systems is arbi-
trary. Such randomness can be removed by impos-
ing a Bayes Risk order on MT systems, i.e. arrange
the MT systems in ascending order of the Bayes
Risk of their top-1 translations. These four orders
of hypotheses are summarized in Table 1. We also
tried some intuitively bad orders of hypotheses, in-
cluding the reversal of these four orders and the
random order.
7 Evaluation
The proposed approaches of incremental IHMM
are evaluated with respect to the constrained
Chinese-to-English track of 2008 NIST Open MT
954
Order Example
System-based 1:1 . . . 1:N 2:1 . . . 2:N . . . M:1 . . . M:N
N-best Rank-based 1:1 2:1 . . . M:1 . . . 1:2 2:2 . . . M:2 . . . 1:N . . . M:N
Bayes Risk + System-based 4:1 4:2 . . . 4:N . . . 1:1 1:2 . . . 1:N . . . 5:1 5:2 . . . 5:N
Bayes Risk + Rank-based 4:1 . . . 1:1 . . . 5:1 4:2 . . . 1:2 . . . 5:2 . . . 4:N . . . 1:N . . . 5:N
Table 1: The list of order of hypothesis and examples. Note that ?m:n? refers to the n-th translation from
the m-th system.
Evaluation (NIST (2008)). In the following sec-
tions, the incremental IHMM approaches using
distortion model 1 and 2 are named as IncIHMM1
and IncIHMM2 respectively, and the consensus
decoding of multiple IHMMs as CD-IHMM. The
baselines include the TER-based method in Rosti
et al (2007), the incremental TER method in Rosti
et al (2008), and the IHMM approach in He et
al. (2008). The development (dev) set comprises
the newswire and newsgroup sections of MT06,
whereas the test set is the entire MT08. The 10-
best translations for every source sentence in the
dev and test sets are collected from eight MT sys-
tems. Case-insensitive BLEU-4, presented in per-
centage, is used as evaluation metric.
The various parameters in the IHMM model are
set as the optimal values found in He et al (2008).
The lexical translation probabilities used in the
semantic similarity model are estimated from a
small portion (FBIS + GALE) of the constrained
track training data, using standard HMM align-
ment model (Och and Ney (2003)). The back-
bone of CN is selected by MBR. The loss function
used for TER-based approaches is TER and that
for IHMM-based approaches is BLEU. As to the
incremental systems, the default order of hypothe-
ses is the ascending order of TER score against the
backbone, which is the order proposed in Rosti
et al (2008). The default order of hypotheses
for our three incremental IHMM approaches is
N-best rank order with Bayes Risk system order,
which is empirically found to be giving the high-
est BLEU score. Once the CN is built, the final
system combination output can be obtained by de-
coding it with a set of features and decoding pa-
rameters. The features we used include word con-
fidences, language model score, word penalty and
empty word penalty. The decoding parameters are
trained by maximum BLEU training on the dev
set. The training and decoding processes are the
same as described by Rosti et al (2007).
Method dev test
best single system 32.60 27.75
pair-wise TER 37.90 30.96
incremental TER 38.10 31.23
pair-wise IHMM 38.52 31.65
incremental IHMM 39.22 32.63
Table 2: Comparison between IncIHMM2 and the
three baselines
7.1 Comparison against Baselines
Table 2 lists the BLEU scores achieved by
the three baseline combination methods and
IncIHMM2. The comparison between pairwise
and incremental TER methods justifies the supe-
riority of the incremental strategy. However, the
benefit of incremental TER over pair-wise TER is
smaller than that mentioned in Rosti et al (2008),
which may be because of the difference between
test sets and other experimental conditions. The
comparison between the two pair-wise alignment
methods shows that IHMM gives a 0.7 BLEU
point gain over TER, which is a bit smaller than
the difference reported in He et al (2008). The
possible causes of such discrepancy include the
different dev set and the smaller training set for
estimating semantic similarity parameters. De-
spite that, the pair-wise IHMM method is still a
strong baseline. Table 2 also shows the perfor-
mance of IncIHMM2, our best incremental IHMM
approach. It is almost one BLEU point higher than
the pair-wise IHMM baseline and much higher
than the two TER baselines.
7.2 Comparison among the Incremental
IHMM Models
Table 3 lists the BLEU scores achieved by
the three incremental IHMM approaches. The
two distortion models for IncIHMM approach
lead to almost the same performance, whereas
CD-IHMM is much less satisfactory.
For IncIHMM, the gist of both distortion mod-
955
Method dev test
IncIHMM1 39.06 32.60
IncIHMM2 39.22 32.63
CD-IHMM 38.64 31.87
Table 3: Comparison between the three incremen-
tal IHMM approaches
els is to shift the distortion over spans to the dis-
tortion over word sequences. In distortion model 2
the word sequences are those sequences available
in one of the component translations in the CN.
Distortion model 1 is more encompassing as it also
considers the word sequences which are combined
from subsequences from various component trans-
lations. However, as mentioned in section 3.1,
the number of sequences grows exponentially and
there is therefore a limit L to the length of se-
quences. In general the limit L ? 8 would ren-
der the tuning/decoding process intolerably slow.
We tried the values 5 to 8 for L and the variation
of performance is less than 0.1 BLEU point. That
is, distortion model 1 cannot be improved by tun-
ing L. The similar BLEU scores as shown in Ta-
ble 3 implies that the incorporation of more word
sequences in distortion model 1 does not lead to
extra improvement.
Although consensus decoding is conceptually
different from both variations of IncIHMM, it
can indeed be transformed into a form similar to
IncIHMM2. IncIHMM2 calculates the parameters
of the IHMM as a weighted sum of various proba-
bilities of the component translations. In contrast,
the equations in section 4 shows that CD-IHMM
calculates the weighted sum of the logarithm of
those probabilities of the component translations.
In other words, IncIHMM2 makes use of the sum
of probabilities whereas CD-IHMM makes use
of the product of probabilities. The experiment
results indicate that the interaction between the
weights and the probabilities is more fragile in the
product case than in the summation case.
7.3 Impact of Order of Hypotheses
Table 4 lists the BLEU scores on the test set
achieved by IncIHMM1 using different orders of
hypotheses. The column ?reversal? shows the im-
pact of deliberately bad order, viz. more than one
BLEU point lower than the best order. The ran-
dom order is a baseline for not caring about or-
der of hypotheses at all, which is about 0.7 BLEU
normal reversal
System 32.36 31.46
Rank 32.53 31.56
BR+System 32.37 31.44
BR+Rank 32.6 31.47
random 31.94
Table 4: Comparison between various orders of
hypotheses. ?System? means system-based or-
der; ?Rank? means N-best rank-based order; ?BR?
means Bayes Risk order of systems. The numbers
are the BLEU scores on the test set.
point lower than the best order. Among the orders
with good performance, it is observed that N-best
rank order leads to about 0.2 to 0.3 BLEU point
improvement, and that the Bayes Risk order of
systems does not improve performance very much.
In sum, the performance of incremental alignment
is sensitive to the order of hypotheses, and the op-
timal order is defined in terms of the rank of each
hypothesis on some system?s n-best list.
8 Conclusions
This paper investigates the application of the in-
cremental strategy to IHMM, one of the state-of-
the-art alignment methods for MT output com-
bination. Such a task is subject to the prob-
lem of how to define state transitions on a grad-
ually expanding CN. We proposed three differ-
ent solutions, which share the principle that tran-
sition over CN spans must be converted to the
transition over word sequences provided by the
component translations. While the consensus de-
coding approach does not improve performance
much, the two distortion models for incremental
IHMM (IncIHMM1 and IncIHMM2) give superb
performance in comparison with pair-wise TER,
pair-wise IHMM, and incremental TER. We also
showed that the order of hypotheses is important
as a deliberately bad order would reduce transla-
tion quality by one BLEU point.
References
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Out-
puts from Machine Translation Systems. Proceed-
ings of EMNLP 2008.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer 2008. Machine Translation
956
System Combination using ITG-based Alignments.
Proceedings of ACL 2008.
Evgeny Matusov, Nicola Ueffing and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems using Enhanced
Hypothesis Alignment. Proceedings of EACL.
Nishanth Ulhas Nair and T.V. Sreenivas. 2007. Joint
Decoding of Multiple Speech Patterns for Robust
Speech Recognition. Proceedings of ASRU.
Dennis Nilsson and Jacob Goldberger 2001. Sequen-
tially Finding the N-Best List in Hidden Markov
Models. Proceedings of IJCAI 2001.
NIST 2008. The NIST Open Machine
Translation Evaluation. www.nist.gov/
speech/tests/mt/2008/doc/
Franz J. Och and Hermann Ney 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics 29(1):pp 19-51
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
ACL 2002
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz 2007. Improved Word-level System Com-
bination for Machine Translation. Proceedings of
ACL 2007.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz 2008. Incremental Hypoth-
esis Alignment for Building Confusion Networks
with Application to Machine Translation System
Combination. Proceedings of the 3rd ACL Work-
shop on SMT.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland 2007. Con-
sensus Network Decoding for Statistical Machine
Translation System Combination. Proceedings of
ICASSP vol. 4.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla and John Makhoul 2006. A Study of
Translation Edit Rate with Targeted Human Anno-
tation. Proceedings of AMTA 2006
957
Proceedings of the Second Workshop on Statistical Machine Translation, pages 72?79,
Prague, June 2007. c?2007 Association for Computational Linguistics
Training Non-Parametric Features for Statistical Machine Translation
Patrick Nguyen, Milind Mahajan and Xiaodong He
Microsoft Corporation
1 Microsoft Way,
Redmond, WA 98052
{panguyen,milindm,xiaohe}@microsoft.com
Abstract
Modern statistical machine translation sys-
tems may be seen as using two components:
feature extraction, that summarizes informa-
tion about the translation, and a log-linear
framework to combine features. In this pa-
per, we propose to relax the linearity con-
straints on the combination, and hence relax-
ing constraints of monotonicity and indepen-
dence of feature functions. We expand fea-
tures into a non-parametric, non-linear, and
high-dimensional space. We extend empir-
ical Bayes reward training of model param-
eters to meta parameters of feature genera-
tion. In effect, this allows us to trade away
some human expert feature design for data.
Preliminary results on a standard task show
an encouraging improvement.
1 Introduction
In recent years, statistical machine translation have
experienced a quantum leap in quality thanks to au-
tomatic evaluation (Papineni et al, 2002) and error-
based optimization (Och, 2003). The conditional
log-linear feature combination framework (Berger,
Della Pietra and Della Pietra, 1996) is remarkably
simple and effective in practice. Therefore, re-
cent efforts (Och et al, 2004) have concentrated on
feature design ? wherein more intelligent features
may be added. Because of their simplicity, how-
ever, log-linear models impose some constraints on
how new information may be inserted into the sys-
tem to achieve the best results. In other words,
new information needs to be parameterized care-
fully into one or more real valued feature functions.
Therefore, that requires some human knowledge and
understanding. When not readily available, this
is typically replaced with painstaking experimenta-
tion. We propose to replace that step with automatic
training of non-parametric agnostic features instead,
hopefully relieving the burden of finding the optimal
parameterization.
First, we define the model and the objective func-
tion training framework, then we describe our new
non-parametric features.
2 Model
In this section, we describe the general log-linear
model used for statistical machine translation, as
well as a training objective function and algorithm.
The goal is to translate a French (source) sentence
indexed by t, with surface string ft. Among a set of
Kt outcomes, we denote an English (target) a hy-
pothesis with surface string e(t)k indexed by k.
2.1 Log-linear Model
The prevalent translation model in modern systems
is a conditional log-linear model (Och and Ney,
2002). From a hypothesis e(t)k , we extract features
h(t)k , abbreviated hk, as a function of e
(t)
k and ft. The
conditional probability of a hypothesis e(t)k given a
source sentence ft is:
pk , p(e(t)k |ft) ,
exp[? ? hk]
Zft;?
,
72
where the partition function Zft;? is given by:
Zft;? =
?
j
exp[? ? hj ].
The vector of parameters of the model ?, gives a
relative importance to each feature function compo-
nent.
2.2 Training Criteria
In this section, we quickly review how to adjust ?
to get better translation results. First, let us define
the figure of merit used for evaluation of translation
quality.
2.2.1 BLEU Evaluation
The BLEU score (Papineni et al, 2002) was de-
fined to measure overlap between a hypothesized
translation and a set of human references. n-gram
overlap counts {cn}4n=1 are computed over the test
set sentences, and compared to the total counts of
n-grams in the hypothesis:
cn,(t)k , max. # of matching n-grams for hyp. e(t)k ,
an,(t)k , # of n-grams in hypothesis e(t)k .
Those quantities are abbreviated ck and ak to sim-
plify the notation. The precision ratio Pn for an n-
gram order n is:
Pn ,
?
t c
n,(t)
k
?
t a
n,(t)
k
.
A brevity penalty BP is also taken into account, to
avoid favoring overly short sentences:
BP , min{1; exp(1 ? ra)},
where r is the average length of the shortest sen-
tence1, and a is the average length of hypotheses.
The BLEU score the set of hypotheses {e(t)k } is:
B({e(t)k }) , BP ? exp
( 4
?
n=1
1
4 logPn
)
.
1As implemented by NIST mteval-v11b.pl.
Oracle BLEU hypothesis: There is no easy way
to pick the set hypotheses from an n-best list that
will maximize the overall BLEU score. Instead, to
compute oracle BLEU hypotheses, we chose, for
each sentence independently, the hypothesis with the
highest BLEU score computed for a sentence itself.
We believe that it is a relatively tight lower bound
and equal for practical purposes to the true oracle
BLEU.
2.2.2 Maximum Likelihood
Used in earlier models (Och and Ney, 2002), the
likelihood criterion is defined as the likelihood of an
oracle hypothesis e(t)k? , typically a single reference
translation, or alternatively the closest match which
was decoded. When the model is correct and infi-
nite amounts of data are available, this method will
converge to the Bayes error (minimum achievable
error), where we define a classification task of se-
lecting k? against all others.
2.2.3 Regularization Schemes
One can convert a maximum likelihood problem
into maximum a posteriori using Bayes? rule:
argmax
?
?
t
p(?|{e(t)k , ft}) = argmax?
?
t
pkp0(?),
where p0(?) is the prior distribution of ?. The
most frequently used prior in practice is the normal
prior (Chen and Rosenfeld, 2000):
log p0(?) , ?||?||
2
2?2 ? log |?|,
where ?2 > 0 is the variance. It can be thought of
as the inverse of a Lagrange multiplier when work-
ing with constrained optimization on the Euclidean
norm of ?. When not interpolated with the likeli-
hood, the prior can be thought of as a penalty term.
The entropy penalty may also be used:
H , ? 1T
T
?
t=1
Kt
?
k=1
pk log pk.
Unlike the Gaussian prior, the entropy is indepen-
dent of parameterization (i.e., it does not depend on
how features are expressed).
73
2.2.4 Minimum Error Rate Training
A good way of training ? is to minimize empirical
top-1 error on training data (Och, 2003). Compared
to maximum-likelihood, we now give partial credit
for sentences which are only partially correct. The
criterion is:
argmax
?
?
t
B({e(t)
k?
}) : e(t)
k?
= argmax
e(t)j
pj.
We optimize the ? so that the BLEU score of the
most likely hypotheses is improved. For that reason,
we call this criterion BLEU max. This function is
not convex and there is no known exact efficient op-
timization for it. However, there exists a linear-time
algorithm for exact line search against that objec-
tive. The method is often used in conjunction with
coordinate projection to great success.
2.2.5 Maximum Empirical Bayes Reward
The algorithm may be improved by giving partial
credit for confidence pk of the model to partially cor-
rect hypotheses outside of the most likely hypothe-
sis (Smith and Eisner, 2006):
1
T
T
?
t=1
Kt
?
k=1
pk logB({ek(t)}).
Instead of the BLEU score, we use its logrithm, be-
cause we think it is exponentially hard to improve
BLEU. This model is equivalent to the previous
model when pk give all the probability mass to the
top-1. That can be reached, for instance, when ?
has a very large norm. There is no known method
to train against this objective directly, however, ef-
ficient approximations have been developed. Again,
it is not convex.
It is hoped that this criterion is better suited for
high-dimensional feature spaces. That is our main
motivation for using this objective function through-
out this paper. With baseline features and on our
data set, this criterion also seemed to lead to results
similar to Minimum Error Rate Training.
We can normalize B to a probability measure
b({e(t)k }). The empirical Bayes reward also coin-
cides with a divergence D(p||b).
2.3 Training Algorithm
We train our model using a gradient ascent method
over an approximation of the empirical Bayes re-
ward function.
2.3.1 Approximation
Because the empirical Bayes reward is defined
over a set of sentences, it may not be decomposed
sentence by sentence. This is computationally bur-
densome. Its sufficient statistics are r, ?t ck and
?
t ak. The function may be reconstructed in a first-
order approximation with respect to each of these
statistics. In practice this has the effect of commut-
ing the expectation inside of the functional, and for
that reason we call this criterion BLEU soft. This ap-
proximation is called linearization (Smith and Eis-
ner, 2006). We used a first-order approximation for
speed, and ease of interpretation of the derivations.
The new objective function is:
J , log B?P +
4
?
n=1
1
4 log
?
t Ec
n,(t)
k
?
t Ea
n,(t)
k
,
where the average bleu penalty is:
log B?P , min{0; 1 ? r
Ek,ta1,(t)k
}.
The expectation is understood to be under the cur-
rent estimate of our log-linear model. Because B?P is
not differentiable, we replace the hard min function
with a sigmoid, yielding:
log B?P ? u(r ? Ek,ta1,(t)k )
(
1? r
Ek,ta1,(t)k
)
,
with the sigmoid function u(x) defines a soft step
function:
u(x) , 11 + e??x ,
with a parameter ? ? 1.
2.3.2 Gradients and Sufficient Statistics
We can obtain the gradients of the objective func-
tion using the chain rule by first differentiating with
respect to the probability. First, let us decompose
the log-precision of the expected counts:
log P?n = log Ecn,(t)k ? log Ea
n,(t)
k .
74
Each n-gram precision may be treated separately.
For each n-gram order, let us define sufficient statis-
tics ? for the precision:
?c? ,
?
t,k
(??pk)ck; ?a? ,
?
t,k
(??pk)ak,
where the gradient of the probabilities is given by:
??pk = pk(hk ? h?),
with:
h? ,
Kt
?
j=1
pjhj .
The derivative of the precision P?n is:
??log P?n =
1
T
[ ?c?
Eck
? ?
a
?
Eak
]
For the length, the derivative of log B?P is:
u(r?Ea)
[
(ra ? 1)[1 ? u(r ? Ea)]? +
r
(Ea)2
]
?a1? ,
where ?a1? is the 1-gram component of ?a?. Finally,
the derivative of the entropy is:
??H =
?
k,t
(1 + log pk)??pk.
2.3.3 RProp
For all our experiments, we chose RProp (Ried-
miller and Braun, 1992) as the gradient ascent al-
gorithm. Unlike other gradient algorithms, it is only
based on the sign of the gradient components at each
iteration. It is relatively robust to the objective func-
tion, requires little memory, does not require meta
parameters to be tuned, and is simple to implement.
On the other hand, it typically requires more iter-
ations than stochastic gradient (Kushner and Yin,
1997) or L-BFGS (Nocedal and Wright, 1999).
Using fairly conservative stopping criteria, we ob-
served that RProp was about 6 times faster than Min-
imum Error Rate Training.
3 Adding Features
The log-linear model is relatively simple, and is usu-
ally found to yield good performance in practice.
With these considerations in mind, feature engineer-
ing is an active area of research (Och et al, 2004).
Because the model is fairly simple, some of the in-
telligence must be shifted to feature design. After
having decided what new information should go in
the overall score, there is an extra effort involved
in expressing or parameterizing features in a way
which will be easiest for the model learn. Experi-
mentation is usually required to find the best config-
uration.
By adding non-parametric features, we propose
to mitigate the parameterization problem. We will
not add new information, but rather, propose a way
to insulate research from the parameterization. The
system should perform equivalently invariant of any
continuous invertible transformation of the original
input.
3.1 Existing Features
The baseline system is a syntax based machine
translation system as described in (Quirk, Menezes
and Cherry, 2005). Our existing feature set includes
11 features, among which the following:
? Target hypothesis word count.
? Treelet count used to construct the candidate.
? Target language models, based on the Giga-
word corpus (5-gram) and target side of parallel
training data (3-gram).
? Order models, which assign a probability to the
position of each target node relative to its head.
? Treelet translation model.
? Dependency-based bigram language models.
3.2 Re-ranking Framework
Our algorithm works in a re-ranking framework.
In particular, we are adding features which are not
causal or additive. Features for a hypothesis may
not be accumulating by looking at the English (tar-
get) surface string words from the left to the right
and adding a contribution per word. Word count,
for instance, is causal and additive. This property
is typically required for efficient first-pass decod-
ing. Instead, we look at a hypothesis sentence as a
whole. Furthermore, we assume that the Kt-best list
provided to us contains the entire probability space.
75
In particular, the computation of the partition func-
tion is performed over all Kt-best hypotheses. This
is clearly not correct, and is the subject of further
study. We use the n-best generation scheme inter-
leaved with ? optimization as described in (Och,
2003).
3.3 Issues with Parameterization
As alluded to earlier, when designing a new feature
in the log-linear model, one has to be careful to find
the best embodiment. In general, a set of features
must satisfy the following properties, ranked from
strict to lax:
? Linearity (warping)
? Monotonicity
? Independence (conjunction)
Firstly, a feature should be linearly correlated with
performance. There should be no region were it
matters less than other regions. For instance, in-
stead of a word count, one might consider adding
its logarithm instead. Secondly, the ?goodness? of a
hypothesis associated with a feature must be mono-
tonic. For instance, using the signed difference be-
tween word count in the French (source) and En-
glish (target) does not satisfy this. (In that case, one
would use the absolute value instead.) Lastly, there
should be no inter-dependence between features. As
an example, we can consider adding multiple lan-
guage model scores. Whether we should consider
ratios those of, globally linearly or log-linearly in-
terpolating them, is open to debate. When features
interact across dimensions, it becomes unclear what
the best embodiment should be.
3.4 Non-parametric Features
A generic solution may be sought in non-parametric
processing. Our method can be derived from a quan-
tized Parzen estimate of the feature density function.
3.4.1 Parzen Window
The Parzen window is an early empirical kernel
method (Duda and Hart, 1973). For an observation
hm, we extrapolate probability mass around it with
a smoothing window ?(?). The density function is:
p(h) = 1M
K
?
m=1
?(h? hm),
assuming ?(?) is a density function. Parzen win-
dows converge to the true density estimate, albeit
slowly, under weak assumptions.
3.4.2 Bin Features
One popular way of using continuous features in
log-linear models is to convert a single continuous
feature into multiple ?bin? features. Each bin feature
is defined as the indicator function of whether the
original continuous feature was in a certain range.
The bins were selected so that each bin collects an
equal share of the probability mass. This is equiva-
lent to the maximum likelihood estimate of the den-
sity function subject to a fixed number of rectangular
density kernels. Since that mapping is not differen-
tiable with respect to the original features, one may
use sigmoids to soften the boundaries.
Bin features are useful to relax the requirements
of linearity and monotonicity. However, because
they work on each feature individually, they do not
address the problem of inter-dependence between
features.
3.4.3 Gaussian Mixture Model Features
Bin features may be generalized to multi-
dimensional kernels by using a Gaussian smoothing
window instead of a rectangular window. The direct
analogy is vector quantization. The idea is to weight
specific regions of the feature space differently. As-
suming that we have M Gaussians each with mean
vector ?m and diagonal covariance matrix Cm, and
prior weight wm. We will add m new features, each
defined as the posterior in the mixture model:
hm , wmN (h;?m, Cm)?
r wrN (h;?r, Cr)
.
It is believed that any reasonable choice of kernels
will yield roughly equivalent results (Povey et al,
2004), if the amount of training data and the number
of kernels are both sufficiently large. We show two
methods for obtaining clusters. In contrast with bins,
lossless representation becomes rapidly impossible.
ML kernels: The canonical way of obtaining clus-
ter is to use the standard Gaussian mixture training.
First, a single Gaussian is trained on the whole data
set. Then, the Gaussian is split into two Gaussians,
with each mean vector perturbed, and the Gaus-
sians are retrained using maximum-likelihood in an
76
expectation-maximization framework (Rabiner and
Huang, 1993). The number of Gaussians is typically
increased exponentially.
Perceptron kernels: We also experimented with
another quicker way of obtaining kernels. We
chose an equal prior and a global covariance matrix.
Means were obtained as follows: for each sentence
in the training set, if the top-1 candidate was differ-
ent from the approximate maximum oracle BLEU
hypothesis, both were inserted. It is a quick way
to bootstrap and may reach the oracle BLEU score
quickly.
In the limit, GMMs will converge to the oracle
BLEU. In the next section, we show how to re-
estimate these kernels if needed.
3.5 Re-estimation Formul?
Features may also be trained using the same empir-
ical maximum Bayes reward. Let ? be the hyper-
parameter vector used to generate features. In the
case of language models, for instance, this could be
backoff weights. Let us further assume that the fea-
ture values are differentiable with respect to ?. Gra-
dient ascent may be applied again but this time with
respect to ?. Using the chain rule:
??J = (??h)(?hpk)(?pkJ),
with ?hpk = pk(1 ? pk)?. Let us take the example
of re-estimating the mean of a Gaussian kernel ?m:
??mhm = ?wmhm(1 ? hm)C?1m (?m ? h),
for its own feature, and for other posteriors r 6= m:
??mhr = ?wrhrhmC?1m (?m ? h),
which is typically close to zero if no two Gaussians
fire simultaneously.
4 Experimental Results
For our experiments, we used the standard NIST
MT-02 data set to evaluate our system.
4.1 NIST System
A relatively simple baseline was used for our exper-
iments. The system is syntactically-driven (Quirk,
Menezes and Cherry, 2005). The system was trained
on 175k sentences which were selected from the
NIST training data (NIST, 2006) to cover words in
source language sentences of the MT02 develop-
ment and evaluation sets. The 5-gram target lan-
guage model was trained on the Gigaword mono-
lingual data using absolute discounting smoothing.
In a single decoding, the system generated 1000 hy-
potheses per sentence whenever possible.
4.2 Leave-one-out Training
In order to have enough data for training, we gen-
erated our n-best lists using 10-fold leave-one-out
training: base feature extraction models were trained
on 9/10th of the data, then used for decoding the
held-out set. The process was repeated for all 10
parts. A single ? was then optimized on the com-
bined lists of all systems. That ? was used for an-
other round of 10 decodings. The process was re-
peated until it reached convergence after 7 iterations.
Each decoding generated about 100 hypotheses, and
there was relatively little overlap across decodings.
Therefore, there were about 1M hypotheses in total.
The combined list of all iterations was used for all
subsequent experiments of feature expansion.
4.3 BLEU Training Results
We tried training systems under the empirical Bayes
reward criterion, and appending either bin or GMM
features. We will find that bin features are es-
sentially ineffective while GMM features show a
modest improvement. We did not retrain hyper-
parameters.
4.3.1 Convexity of the Empirical Bayes Reward
The first question to ask is how many local op-
tima does the cost surface have using the standard
features. A complex cost surface indicates that some
gain may be had with non-linear features, but it also
shows that special care should be taken during op-
timization. Non-convexity is revealed by sensitivity
to initialization points. Thus, we decided to initial-
ize from all vertices of the unit hypercube, and since
we had 11 features, we ran 211 experiments. The
histogram of BLEU scores on dev data after conver-
gence is shown on Figure 1. We also plotted the his-
togram of an example dimension in Figure 2. The
range of BLEU scores and lambdas is reasonably
narrow. Even though ? seems to be bimodal, we see
77
that this does not seriously affect the BLEU score.
This is not definitive evidence but we provisionally
pretend that the cost surface is almost convex for
practical purposes.
24.8 24.9 25 25.1 25.2 25.3 25.4
0
200
400
600
800
BLEU score
n
u
m
be
r o
f t
ra
in
ed
 m
od
el
s
Figure 1: Histogram of BLEU scores after training
from 211 initializations.
?60 ?40 ?20 0
0
100
200
300
400
500
600
700
? value
n
u
m
be
r o
f t
ra
in
ed
 m
od
el
s
Figure 2: Histogram of one ? parameter after train-
ing from 211 initializations.
4.3.2 Bin Features
A log-linear model can be converted into a bin
feature model nearly exactly by setting ? values
in such a way that scores will be equal. Equiva-
lent weights (marked as ?original? in Figure 3) have
the shape of an error function (erf): this is because
the input feature is a cummulative random variable,
which quickly converges to a Gaussian (by the cen-
tral limit theorem). After training the ? weights for
the log-linear model, weights may be converted into
bins and re-trained. On Figure 3, we show that relax-
ing the monotonicity constraint leads to rough val-
ues for ?. Surprisingly, the BLEU score and ob-
jective on the training set only increases marginally.
Starting from ? = 0, we obtained nearly exactly the
same training objective value. By varying the num-
ber of bins (20-50), we observed similar behavior as
well.
0 10 20 30 40 50
?1.5
?1
?0.5
0
0.5
1
bin id
va
lu
e
 
 
original weights
trained weights
Figure 3: Values before and after training bin fea-
tures. Monotonicity constraint has been relaxed.
BLEU score is virtually unchanged.
4.3.3 GMM Features
Experiments were carried out with GMM fea-
tures. The summary is shown on Table 1. The
baseline was the log-linear model trained with the
baseline features. The baseline features are included
in all systems. We trained GMM models using the
iterative mixture splitting interleaved with EM re-
estimation, split up to 1024 and 16384 Gaussians,
which we call GMM-ML-1k and GMM-ML-16k re-
spectively. We also used the ?perceptron? selec-
tion features on the training set to bootstrap quickly
to 300k Gaussians (GMM-PCP-300k), and ran the
same algorithm on the development set (GMM-
PCP-2k). Therefore, GMM-PCP-300k had 300k
features, and was trained on 175k sentences (each
with about 700 hypotheses). For all experiments but
?unreg? (unregularized), we chose a prior Gaussian
prior with variance empirically by looking at the de-
velopment set. For all but GMM-PCP-300k, regu-
larization did not seem to have a noticeably positive
effect on development BLEU scores. All systems
were seeded with the baseline log-linear model, and
78
all additional weights set to zero, and then trained
with about 50 iterations, but convergence in BLEU
score, empirical reward, and development BLEU
score occurred after about 30 iterations. In that set-
ting, we found that regularized empirical Bayes re-
ward, BLEU score on training data, and BLEU score
on development and evaluation to be well corre-
lated. Cursory experiments revealed that using mul-
tiple initializations did not significantly alter the fi-
nal BLEU score.
System Train Dev Eval
Oracle 14.10 N/A N/A
Baseline 10.95 35.15 25.95
GMM-ML-1k 10.95 35.15 25.95
GMM-ML-16k 11.09 35.25 25.89
GMM-PCP-2k 10.95 35.15 25.95
GMM-PCP-300k-unreg 13.00 N/A N/A
GMM-PCP-300k 12.11 35.74 26.42
Table 1: BLEU scores for GMM features vs the lin-
ear baseline, using different selection methods and
number of kernels.
Perceptron kernels based on the training set im-
proved the baseline by 0.5 BLEU points. We mea-
sured significance with the Wilcoxon signed rank
test, by batching 10 sentences at a time to produce
an observation. The difference was found to be sig-
nificant at a 0.9-confidence level. The improvement
may be limited due to local optima or the fact that
original feature are well-suited for log-linear mod-
els.
5 Conclusion
In this paper, we have introduced a non-parametric
feature expansion, which guarantees invariance to
the specific embodiment of the original features.
Feature generation models, including feature ex-
pansion, may be trained using maximum regular-
ized empirical Bayes reward. This may be used as
an end-to-end framework to train all parameters of
the machine translation system. Experimentally, we
found that Gaussian mixture model (GMM) features
yielded a 0.5 BLEU improvement.
Although this is an encouraging result, further
study is required on hyper-parameter re-estimation,
presence of local optima, use of complex original
features to test the effectiveness of the parameteri-
zation invariance, and evaluation on a more compet-
itive baseline.
References
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. ACL?02.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, vol 22:1, pp.
39?71.
S. Chen and R. Rosenfeld. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. on Speech and
Audio Processing, vol 8:2, pp. 37?50.
R. O. Duda and P. E. Hart. 1973. Pattern Classification
and Scene Analysis. Wiley & Sons, 1973.
H. J. Kushner and G. G. Yin. 1997. Stochastic Approxi-
mation Algorithms and Applications. Springer-Verlag,
1997.
National Institute of Standards and Technology. 2006.
The 2006 Machine Translation Evaluation Plan.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer-Verlag, 1999.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. ACL?03.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A Smorgas-
bord of Features for Statistical Machine Translation.
HLT/NAACL?04.
F. J. Och and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Machine
Translation. ACL?02.
D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau
and G. Zweig. 2004. fMPE: Discriminatively trained
features for speech recognition. RT?04 Meeting.
C. Quirk, A. Menezes and C. Cherry. 2005. De-
pendency Tree Translation: Syntactically Informed
Phrasal SMT. ACL?05.
L. R. Rabiner and B.-H. Huang. 1993. Fundamentals of
Speech Recognition. Prentice Hall.
M. Riedmiller and H. Braun. 1992. RPROP: A Fast
Adaptive Learning Algorithm. Proc. of ISCIS VII.
D. A. Smith and J. Eisner. 2006. Minimum-Risk
Annealing for Training Log-Linear Models. ACL-
COLING?06.
79
Proceedings of the Second Workshop on Statistical Machine Translation, pages 80?87,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Word Dependent Transition Models in HMM based Word 
Alignment for Statistical Machine Translation 
Xiaodong He 
 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
xiaohe@microsoft.com 
 
 
Abstract 
In this paper, we present a Bayesian Learn-
ing based method to train word dependent 
transition models for HMM based word 
alignment. We present word alignment re-
sults on the Canadian Hansards corpus as 
compared to the conventional HMM and 
IBM model 4. We show that this method 
gives consistent and significant alignment 
error rate (AER) reduction. We also con-
ducted machine translation (MT) experi-
ments on the Europarl corpus. MT results 
show that word alignment based on this 
method can be used in a phrase-based ma-
chine translation system to yield up to 1% 
absolute improvement in BLEU score, 
compared to a conventional HMM, and 
0.8% compared to a IBM model 4 based 
word alignment. 
1 Introduction 
Word alignment is an important step of most 
modern approaches to statistical machine 
translation (Koehn et al, 2003). The classical 
approaches to word alignment are based on IBM 
models 1-5 (Brown et al, 1994) and the HMM 
based alignment model (Vogel et al, 1996) (Och 
and Ney, 2000a, 2000b), while recently 
discriminative approaches (Moore, 2006) and 
syntax based approaches (Zhang and Gildea, 2005) 
for word alignment are also studied. In this paper, 
we present improvements to the HMM based 
alignment model originally proposed by (Vogel et 
al., 1996, Och and Ney, 2000a).  
Although HMM based word alignment ap-
proaches give good performance, one weakness of 
it is the coarse transition models. In the HMM 
based alignment model (Vogel et al, 1996), it is 
assumed that the HMM transition probabilities de-
pend only on the jump width from the last state to 
the next state. Therefore, the knowledge of transi-
tion probabilities given a particular source word e 
is not sufficiently modeled. 
In order to improve transition models in the 
HMM based alignment, Och and Ney (2000a) ex-
tended the transition models to be word-class de-
pendent. In that approach, words of the source lan-
guage are first clustered into a number of word 
classes, and then a set of transition parameters is 
estimated for each word class. In (2002), Toutano-
va et al modeled self-transition (i.e., jump width is 
zero) probability separately from other transition 
probabilities. A word dependent self-transition 
model P(stay|e) is introduced to decide whether to 
stay at the current source word e at the next step, or 
jump to a different word. It was also shown that 
with the assumption that a source word with fertili-
ty greater than one generates consecutive words in 
the target language, this probability approximates 
fertility modeling. Deng and Byrne in (2005) im-
proved this idea. They proposed a word-to-phrase 
HMM in which a source word dependent phrase 
length model is used to model the approximate 
fertility, i.e., the length of consecutive target words 
generated by the source word. It provides more 
powerful modeling of approximate fertility than 
the single P(stay|e) parameter.  
However, these methods only model the proba-
bility of state occupancy rather than a full set of 
transition probabilities. Important knowledge of 
jumping from e to another position, e.g., jumping 
80
forward (monotonic alignment) or jumping back-
ward (non-monotonic alignment), is not modeled.  
In this paper, we present a method to further im-
prove the transition models for HMM alignment 
model. For each source word e, we not only model 
its self-transition probability, but also the probabil-
ity of jumping from word e to a different word. For 
this purpose, we estimate a full transition model 
for each source word.  
A key problem for detailed word-dependent 
transition modeling is data sparsity. In (Toutanova 
et al, 2002), the word dependent self-transition 
probability P(stay|e) is interpolated with the global 
HMM self-transition probability to alleviate the 
data sparsity problem, where an interpolation 
weight is used for all words and that weight is 
tuned on a hold-out set. In the proposed word de-
pendent transition model, because there are a large 
number of parameters to estimate, the data sparsity 
problem is even more severe. Moreover, since the 
sparsity of different words are very different, it is 
difficult to find a one-size-fits-all interpolation 
weight, and therefore simple linear interpolation is 
not optimal. In order to address this problem, we 
use Bayesian learning so that the transition model 
parameters are estimated by maximum a posteriori 
(MAP) training. With the help of the prior distribu-
tion of the model, the training is regularized and 
results in robust models.  
In the next section we briefly review modeling 
of transition probabilities in a conventional HMM 
alignment model (Vogel et al, 1996, Och and Ney, 
2000a). Then we describe the equations of MAP 
training for word dependent transition models. In 
section 5, we present word alignment results that 
show significant alignment error rate reductions 
compared to the baseline HMM and IBM model 4. 
We also conducted phrase-based machine transla-
tion experiments on the Europarl corpus, English ? 
French track, and shown that the proposed method 
can lead to significant BLEU score improvement 
compared to the HMM and IBM model 4.  
2 Baseline HMM alignment model 
We briefly review the HMM based word alignment 
models (Vogel, 1996, Och and Ney, 2000a). Let?s 
denote by 1 1( ,..., )J Jf f f=  as the French sentence, 
1 1( ,..., )I Ie e e=  as the English sentence, and 
1 1( ,..., )J Ja a a= as the alignment that specifies the 
position of the English word aligned to each 
French word. In the HMM based word alignment, 
a HMM is built at English side, i.e., each (position, 
word) pair, ( , )
jj aa e , is a HMM state, which emits 
the French word fj. In order to mitigate the sparse 
data problem, it is assumed that the emission prob-
ability only depends on the English word, i.e., 
( | , ) ( | )
j jj j a j ap f a e p f e= , and the transition prob-
ability only  depends on the position of the last 
state and the length of the English sentence, i.e., 
11 1
( | , , ) ( | , )
jj j a j jp a a e I p a a I?? ?= . Then, Vogel et 
al. (1996) give 
 
 
1
1 1 1
1
( | ) ( | , ) ( | )
j
J
J
J I
j j j a
ja
p f e p a a I p f e
?
=
? ?= ? ???  (1) 
 
In the HMM of (Vogel et al, 1996), it is further 
assumed these transition probabilities 
1( | , )? ?= =j jp a i a i I  depend only on the jump 
width (i - i'), i.e.,   
 
1
( )( | , )
( )
I
l
c i ip i i I
c l i
=
??
? =
???
             (2) 
 
Therefore, the transition probability 
1( | , )j jp a a I?  depends on aj-1 but only through the 
distortion set {c(i - i')}. 
In (Och and Ney, 2000a), the word null is intro-
duced to generate the French words that don't align 
to any English words. If we denote by j_ the posi-
tion of the last French word before j that aligns to a 
non-null English word, the transition probabilities 
1( | , )j jp a i a i I? ?= =  in (1) is computed as 
_
( | , ) ( | , )j jp a i a i I p i i I? ?= = = % , where 
 
0
0
                            if    0( | , ) (1 ) ( | , )  otherwise
p i
p i i I
p p i i I
=?
? = ?
?? ??
%
 
 
state i=0 denotes the state of a null word at the 
English side, and p0 is the probability of jumping 
to state 0, which is estimated from hold-out data.  
For convenience, we denote by 
{ }( | , ), ( | )j ip i i I p f e?? =  the HMM parameter set. 
81
In the training stage, ? are usually estimated 
through maximum likelihood (ML) training, i.e.,  
 
1 1arg max ( | , )J IML p f e
?
? = ?   (3) 
 
and the efficient Expectation-Maximization al-
gorithm can be used to optimize ? iteratively until 
convergence (Rabiner 1989).  
For the interest of this paper, we elaborate tran-
sition parameter estimation with more details. 
These transition probabilities { }( | , )p i i I?  is a mul-
tinomial distribution estimated according to (2), 
where at each iteration the distortion set {c(i - i')} 
is the fractional count of transitions with jump 
width d = i - i', i.e.,  
 
1
1 1 1
1 1
( ) Pr( , | , , )
J I
J I
j j
j i
c d a i a i d f e
?
+
= =
?= = = + ??? (4) 
 
where ?' is the model obtained from the immediate 
previous iteration and these terms in (4) can be 
efficiently computed by using the Forward-
Backward algorithm (Rabiner 1989). In practice, 
we can bucket the distortion parameters {c(d)} into 
a few buckets as implemented in (Liang et al, 
2006). In our implementation, 15 buckets are used 
for c(?-7), c(-6), ... c(0), ..., c(?7). The probability 
mass for transitions with jump width larger than 6 
is uniformly divided. As suggested in (Liang et al, 
2006), we also use two separate sets of distortion 
parameters for transitioning into the first state, and 
for transitioning out of the last state, respectively. 
Finally, we further smooth transition probabilities 
with a uniform distribution as described in (Och 
and Ney, 2000a),   
_ _
1( | , ) (1 ) ( | , )j j j jp a a I p a a II? ?? = ? + ? ? . 
After training, Viterbi decoding is used to find 
the best alignment sequence 1?
Ja . i.e., 
1
1 _
1
? arg max ( | , ) ( | )
jJ
J
J
j j j a
a j
a p a a I p f e
=
? ?= ? ?? . 
 
3 Word-dependent transition models in 
HMM based alignment model 
As discussed in the previous sections, conventional 
transition models that only depend on source word 
positions are not accurate enough. There are only 
limited distortion parameters to model the transi-
tion between HMM states for all English words, 
and the knowledge of transition probabilities given 
a particular source word is not represented. In or-
der to improve the transition model in HMM, we 
extend the transition probabilities to be word de-
pendent so that the probability of jumping from 
state aj_to aj not only depends on aj_, but also de-
pends on the English word at position aj_. This 
gives 
_
1
1 1 _
1
( | ) ( | , , ) ( | )
j j
J
J
J I
j j a j a
ja
p f e p a a e I p f e
=
? ?= ? ??? . 
Compared to (1), we need to estimate the transition 
parameter 
_
_
( | , , )
jj j ap a a e I  which is _jae  depen-
dent. Correspondingly, the HMM parameters we 
need to estimate are { }( | , , ), ( | )i j ip i i e I p f e??? = , 
which provides a much richer set of free parame-
ters to model transition probabilities.   
4 Bayesian Learning for word-dependent 
transition models  
4.1 Maximum a posteriori training  
Using ML training, we can obtain the estimation 
formula for word dependent transition probabilities 
{ }( | , , )p i i e I?  similar as (2), i.e., 
1
( ; )( | , , )
( ; )
ML I
l
c i i ep i i e I
c l i e
=
??
? =
???
  (5) 
where at each training iteration the word dependent 
distortion set {c(i - i';e)} is computed by 
1
1 1 1
1 1
( ; )
( ) Pr( , | , , )
j
J I
J I
a j j
j i
c d e
e e a i a i d f e?
?
+
= =
=
?= = = + ???
     (6) 
where d = i - i' is the jump width, and ( )
ja
e e? = is 
the Kronecker delta function that equals one if 
ja
e e= , and zero otherwise. 
However, for many non-frequent words, the 
data samples for c(d;e) is very limited and there-
fore may lead to a biased model that severely over-
fits to the sparse data. In order to address this issue, 
maximum a posteriori (MAP) framework is ap-
plied (Gauvain and Lee, 1994). In MAP training, 
an appropriate prior distribution is used to incorpo-
82
rate prior knowledge into the model parameter es-
timation,   
1 1 1arg max ( | , ) ( | )J I IMAP p f e g e
?
? = ? ?   (7) 
where the prior distribution 1( | )Ig e?  characterizes 
the distribution  of the model parameter set ? giv-
en the English sentence. The relation between ML 
and MAP estimation is through the Bayes' theorem 
where the posterior distribution 
1 1 1 1 1( | , ) ( | , ) ( | )J I J I Ip f e p f e g e? ? ? ? , and 
1 1( | , )J Ip f e ? is the likelihood function.  
In transition model estimation, the transition 
model { }( | , , )ip i i e I??  is a multinomial distribution. 
Its conjugate prior distribution is a Dirichlet distri-
bution taking the following form (Bishop 2006), 
( ) , 11
1
( | , , ) | ( | , , ) i i
I
vI
i i
i
g p i i e I e p i i e I ? ?
? ?
=
? ?? ?  (8) 
where{ }
,i iv ?  is the set of hyper-parameters of the 
prior distribution. Note that for mathematic tracta-
bility, 
,i iv ?  needs to be greater than 1, which is 
usually the case in practice.  
Substitute (8) into (7) and using EM algorithm, 
we can obtain the iterative MAP training formula 
for transition models (Gauvain and Lee, 1994) 
,
,
1 1
( ; ) 1( | , , )
( ; )
i i
MAP I I
i l
l l
c i i e v
p i i e I
c l i e v I
?
?
= =
?? + ?
? =
?? + ?? ?
 (9) 
4.2 Setting hyper-parameters for the prior 
distribution 
In Bayesian learning, the hyper-parameter set 
{ }
,i iv ? of the prior distribution is assumed known 
based on a subjective knowledge about the model. 
In our method, we set the prior with word-
independent transition probabilities.  
 
,
( | , ) 1i iv p i i I?? ?= ? +     (10) 
 
where ? is a positive parameter that needs to tune 
on a hold-out data set. We will investigate the ef-
fect of ? with experimental results in later sections. 
Substituting (10) into (9), the MAP based transi-
tion model training formula becomes 
 
1
( ; ) ( | , )( | , , )
( ; )
MAP I
l
c i i e p i i Ip i i e I
c l i e
?
?
=
? ?? + ?
? =
?? +?
 (11) 
 
Note that for frequent words that have a large 
amount of data samples for c(d;e), the sum of 
1,...,
( ; )
=
??? l I c l i e  is large, so that ( | , , )MAPp i i e I? is 
dominated by the data distribution. For rare words 
that have low counts of c(d;e), ( | , , )MAPp i i e I?  will 
approach to the word independent model. On the 
other hand, for the same word, when a small ? is 
used, a weak prior is applied, and the transition 
probability is more dependent on the training data 
of that word. When ? becomes larger and larger, a 
stronger prior knowledge is applied, and the word 
dependent transition model will approach to the 
word-independent transition model. Therefore, we 
can vary the parameter ? to control the contribution 
of prior distribution in model training and tune the 
word alignment performance. 
5 Experimental Results  
5.1 Word alignment on the Canadian Han-
sards English-French corpus  
We evaluated our word dependent transition mod-
els for HMM based word alignment on the Eng-
lish-French Hansards corpus. Only a subset of 
500K sentence pairs was used in our experiments 
including 447 test sentence-pairs. Tests sentence-
pairs were manually aligned and were marked with 
both sure and possible alignments (Och and Ney 
2000a). Using this annotation, we report the word 
alignment performance in terms of alignment error 
rate (AER) as defined by Och and Ney (2000a): 
 
| | | |1 | | | |
A S A PAER
A S
? + ?
= ?
+
   (12) 
 
where S denotes the set of sure gold alignments, P 
denotes the set of possible gold alignments, A de-
notes the set of alignments generated by the word 
alignment method under test.  
We first trained the IBM model 1 and then a 
baseline HMM model as described in section 2 on 
the Hansards corpus. As the common practice, we 
initialized the translation probabilities of model 1 
with uniform distribution over word pairs occur 
together in a same sentence pair. HMM was initia-
83
lized with uniform transition probabilities and 
model 1 translation probabilities. Both model 1 and 
HMM were trained with 5 iterations. For the pro-
posed word dependent transition model based 
HMM (WDHMM), we used the same settings as 
the HMM baseline except that the transition prob-
ability is computed according to (11). We also 
trained  IBM model 4 using GIZA++ provided by 
Och and Ney (2000c), where 5 iterations of  model 
4 training was performed after 5 iterations of mod-
el 1 plus 5 iterations of HMM. 
The effect of hyper-parameters in the prior dis-
tribution for WDHMM is shown in Figure 1. The 
horizontal dot line represents the AER given by the 
baseline HMM. The dash-line curve represents the 
AERs of WDHMM given different ??s. We vary 
the value of ? in the range from 0 to 1E5 and 
present that range in a log-scale in the figure. Since 
? = 0 is not a valid value in the log domain, we ac-
tually use the left-most point in the figure to 
represent the case of ? = 0. From Fig. 1 it is shown 
that when ? is zero, we actually use the ML trained 
word-dependent transition model. Due to the 
sparse data problem, the model is poorly estimated 
and lead to a high AER. When increase ? to a larg-
er value, a stronger prior is applied to give a more 
robust model. Then in a large range 
of [100,2000]? ? , WDHMM outperforms baseline 
HMM significantly. When ? gets even larger, MAP 
model training becomes being over-dominated by 
the prior distribution, and that eventually results in 
a performance approaching to that of the baseline 
HMM. Fig. 1 only presents AER results that are 
calculated after combination of word alignments of 
both E?F and F?E directions based on a set of 
heuristics proposed by Och and Ney (2000b). We 
have observed the similar trend of AER change for 
the E?F and F?E alignment directions, respec-
tively. However, due to the limit of the space, we 
didn?t include them in this paper.  
In table 1-3, we give a detailed comparison be-
tween baseline HMM, WDHMM (with ? = 1000), 
and IBM model 4. Compared to the baseline 
HMM, the proposed WDHMM can reduce AER by 
more than 13%. It even outperforms IBM model 4 
after two direction word alignment combination. 
Meanwhile we noticed  that although IBM model 4 
gives superior performance over the baseline 
HMM on both of the two alignment directions, its 
AER after combination is almost the same as that 
of the baseline HMM.  We hypothesize that it may 
due to the modeling mechanism difference be-
tween HMM and model 4.  
0 1 2 3 4 58
8.5
9
9.5
10
10.5
11
log10(tau)
AE
R
 
%
 
 
WDHMM
HMM baseline
 
Figure 1: The AER of HMM baseline and the AER 
of WDHMM as the prior parameter ? is varied from 0 to 
1E5. Note that the x axis is in log scale and we use the 
left-most point in the figure to represent the case of ? = 
0. These results are calculated after combination of 
word alignments of both E?F and F?E directions.  
 
model E ? F F ? E combined 
baseline HMM  12.7 13.7 9.8 
WDHMM  
(? = 1000) 
11.6 12.7 8.5 
IBM model 4 
(GIZA++) 
11.3 12.1 9.7 
Table 1: Comparison of test set AER between vari-
ous models trained on 500K sentence pairs. All numbers 
are in percentage. 
 
model E ? F F ? E combined 
baseline HMM  85.2 83.1 91.7 
WDHMM  
(? = 1000) 
86.1 83.8 93.3 
IBM model 4 
(GIZA++) 
87.2 86.2 91.6 
Table 2: Comparison of test set Precision between 
various models trained on 500K sentence pairs. All 
numbers are in percentage. 
 
model E ? F F ? E combined 
baseline HMM  90.6 91.4 88.3 
WDHMM  
(? = 1000) 
91.9 92.6 89.1 
IBM model 4 
(GIZA++) 
91.1 90.8 88.4 
Table 3: Comparison of test set Recall between vari-
ous models trained on 500K sentence pairs. All numbers 
are in percentage. 
84
5.2 Machine translation on Europarl corpus 
We further tested our WDHMM on a phrase-based 
machine translation system to see whether our im-
provement on word alignment can also improve 
MT accuracy measured by BLEU score (Papineni 
et al, 2002). The machine translation experiment 
was conducted on the English-to-French track of 
NAACL 2006 Europarl evaluation workshop. The 
supplied training corpus contains 688K sentence 
pairs. Text data are already tokenized. In our expe-
riment, we first lower-cased all text, then word 
clustering was performed to cluster words of Eng-
lish and French into 32 word classes respectively 
using the tool provided by (J. Goodman). Then 
word alignment was performed. Both baseline 
HMM and IBM model 4 use word-class based 
transition models, and in WDHMM the word-class 
based transition model was used for prior distribu-
tion. The IBM model 4 is trained by GIZA++ with 
a regimen of 5 iterations of Model 1, 5 iterations of 
HMM, and 5 iterations of Model 4. Alignments of 
both directions are generated and then are com-
bined by heuristic rules described in (Och and Ney 
2000b). Then phrase table was extracted from the 
word aligned bilingual texts. The maximum phrase 
length was set to 7. In the phrase-based MT system, 
there are four channel models. They are direct 
maximum likelihood estimate of the probability of 
target phrase given source phrase, and the same 
estimate of source given target; we also compute 
the lexicon weighting features for source given 
target and target given source, respectively. Other 
models include word count and phrase count, and a 
3-gram language model provided by the workshop. 
These models are combined in a log-linear frame-
work with different weights (Och and Ney, 2002). 
The model weight vector is trained on a dev set 
with 2000 English sentences, each of which has 
one French translation reference. In the experiment, 
only the first 500 sentences were used to train the 
log-linear model weight vector, where minimum 
error rate (MER) training was used (Och, 2003). 
After MER training, the weight vector that gives 
the best accuracy on the development set was se-
lected. We then applied it to tests. There are 2000 
sentences in the development-test set devtest, 2000 
sentences in a test set test, and 1064 out-of-domain 
sentences called nc-test. The Pharaoh phrase-based 
decoder (Koehn 2004b) was used for decoding. 
The maximum re-ordering limit for decoding was 
set to 7. We used default settings for all other pa-
rameters. 
We present BLEU scores of MT systems using 
different word alignments on all three test sets, 
where Fig 2 shows BLEU scores of the two in-
domain tests, and Fig 3 shows MT results on the 
out-of-domain test set. In testing, the prior parame-
ter ? of WDHMM was varied in the range of [20, 
5000].  
In Fig. 2, the horizontal dash line and the hori-
zontal dot line represent BLEU scores of the base-
line HMM on devtest set and test set, respectively. 
The dash-line curve and dot-line curve represent 
the BLEU scores of WDHMM on these two tests. 
It is shown in the figure that WDHMM can 
achieve the best BLEU scores on both devtest and 
test when the prior parameter ? is set to 100. Fur-
thermore, WDHMM also gives considerable im-
provement on BLEU score over the baseline HMM 
in a broad range of ? from 50 to 1000, which indi-
cates that WDHMM works pretty stable within a 
reasonable range of prior distributions.  
In Fig. 3, the horizontal dash line represents the 
BLEU score of baseline HMM on nc-test set and 
the dash-line curve represents BLEU scores of 
WDHMM on the out-of-domain test. The best 
BLEU is obtained at ? = 500. It is interesting to see 
that the best ? for the out-of-domain test is larger 
than that of an in-domain test. One possible expla-
nation is that for out-of-domain data, we need 
more robust modeling for outliers other than more 
accurate (in-domain) modeling. However, since the 
difference between ? = 500 and ? = 100 are very 
small, further experiments are needed before we 
can draw a conclusion.  
We gives a detailed BLEU-wise comparison be-
tween baseline HMM and WDHMM in Table 4, 
where for WDHMM, ? =100 is used since it gives 
the best performance on the development-test set 
devtest. In the same table, we also provide BLEU 
results of using IBM model 4. Compared to base-
line HMM alignment model, WDHMM can im-
prove the BLEU score nearly 1% on in-domain test 
sets, and the improvement reduces to about 0.5% 
on the out-of-domain test. When compared to IBM 
model 4, WDHMM still gives higher BLEU 
scores, and outperform model 4 by about 0.8% on 
the test set. However the gain is reduced to 0.3% 
on devtest and 0.5% on the out-of-domain nc-test. 
85
1 1.5 2 2.5 3 3.5 429
29.5
30
30.5
31
log10(tau)
BL
EU
 
%
 
 
devtest
test
 
Figure 2: Machine translation results on Europarl, 
English to French track, devtest and test sets. The 
BLEU score of HMM baseline and the BLEU score of 
WDHMM as the prior parameter ? is varied from 20 to 
5000. Note that the x axis is in log scale.  
1 1.5 2 2.5 3 3.5 420
20.5
21
21.5
22
log10(tau)
BL
EU
 
%
 
 
nc-test
 
Figure 3: Machine translation results on Europarl, 
English to French track, out-of-domain test sets. The 
BLEU score of HMM baseline and the BLEU score of 
WDHMM as the prior parameter ? is varied from 20 to 
5000. Note that the x axis is in log scale. 
 
model devtest test nc-test 
baseline HMM  29.69 29.65 20.51 
WDHMM (? = 100) 30.59 30.65 20.96 
IBM model 4 30.29 29.86 20.51 
Table 4: Comparison of BLEU scores on devtest, test, 
and nc-test set between various word alignment models. 
All numbers are in percentage. 
 
In order to verify whether these gains from 
WDHMM are statistically significant, we imple-
mented paired bootstrap resampling method pro-
posed by Koehn (2004b) to compute statistical sig-
nificance of the above test results. In table 5, it is 
shown that BLEU gains of WDHMM over HMM 
and IBM-4 on different test sets, except the gain 
over IBM model 4 on the devtest set, are statistical-
ly significant with a significance level > 95%. 
 
significance level  devtest test nc-test 
WDHMM (?=100) 
vs. HMM 
99.9% 99.9% 99.5% 
WDHMM (?=100)  
vs. IBM model 4 
93.7% 99.9% 99.3% 
Table 5: Statistical significance test of the BLEU im-
provement of WDHMM  (? = 100) vs. HMM baseline, 
and WDHMM  (? = 100) vs. IBM model 4 on devtest, 
test, and nc-test sets. 
5.3 Runtime performance of WDHMM  
WDHMM runs as fast as a normal HMM, and 
the extra memory needed for the word dependent 
transition model is proportional to the vocabulary 
size of the source language given that the distortion 
sets of {c(d;e)}  are bucketed. Runtime speed of 
WDHMM and IBM-model 4 using GIZA++ is ta-
bulated in table 6. The results are based on Euro-
parl English to French alignment and these tests 
were conducted on a fast PC with 3.0GHz CPU 
and 16GB memory. In Table 6, WDHMM includes 
5 iterations of model 1 training followed by 5 itera-
tions of WDHMM, while "IBM model 4" includes 
5 iterations for model 1, 5 iterations for HMM, and 
5 iterations for model 4. It is shown in Table 6 that 
WDHMM is more than four times faster to pro-
duce the end-to-end word alignment. 
 
model   runtime 
(min) 
WDHMM   121 
IBM model 4    537 
Table 6: comparison of runtime performance bew-
teen WDHMM training and IBM model 4 training using 
GIZA++. 
6 Discussion 
Other works have been done to improve transition 
models in HMM based word alignment. Och and 
Ney (2000a) have suggested estimating word-class 
based transition models so as to provide more de-
tailed transition probabilities. However, due to the 
sparse data problem, only a small number of word 
classes are usually used and the many words in the 
same class still have to share the same transition 
model. Toutanova et al (2002) has proposed to 
86
estimate a word-dependent self-transition model 
P(stay|e) so that each word can have its own prob-
ability to decide whether to stay or jump to a dif-
ferent word. Later Deng and Byrne (2005) pro-
posed a word dependent phrase length model to 
better model state occupancy. However, these 
model can only model the probability of self-
jumping. Important knowledge of jumping from e 
to a different position should also be word depen-
dent but is not modeled.  
Another interesting comparison is between 
WDHMM and the fertility-based models, e.g., 
IBM model 3-5. Compared to these models, a ma-
jor disadvantage of HMM is the absence of a mod-
el of source word fertility. However, as discussed 
in (Toutanova et al 2002),the word dependent self-
transition model can be viewed as an approxima-
tion of fertility model. i.e., it models the number of 
consecutive target words generated by the source 
word with a geometric distribution. Therefore, with 
a well estimated word dependent transition model, 
this weakness of HMM is alleviated. 
In this work, we proposed estimating a full 
word-dependent transition models in HMM based 
word alignment, and with Bayesian learning we 
can achieve robust model estimation under the 
sparse data condition. We have conducted a series 
of experiments to evaluate this method on word 
alignment and machine translation tests, and show 
significant improvement over baseline HMM in 
terms of AER and BLEU. It also performs better 
than the much more complicated IBM model 4 
based word alignment model on various word 
alignment and machine translation tasks.  
Acknowledgments  The author is grateful to Chris 
Quirk and Arul Menezes for assistance with the 
MT system and for the valuable suggestions and 
discussions.  
References 
C. M. Bishop, 2006. Pattern Recognition and Machine 
Learning. Springer. 
P. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer. 
1994. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19:263?311. 
Y. Deng and W. Byrne, 2005, HMM Word and Phrase 
Alignment For Statistical Machine Translation, in 
Proceedings of HLT/EMNLP. 
J. Gauvain and C.-H. Lee, 1994, Maximum a Posteriori 
Estimation For Multivariate Gaussian Mixture Ob-
servations Of Markov Chains, IEEE Trans on Speech 
and Audio Processing. 
J. Goodman, http://research.microsoft.com/~joshuago/ 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
Phrase-Based Translation. In Proceedings of HLT-
NAACL. 
P. Koehn, 2004a, Statistical Significance Tests for Ma-
chine Translation Evaluation, in Proceedings of 
EMNLP. 
P. Koehn. 2004b. Pharaoh: A Beam Search Decoder For 
Phrase Based Statistical Machine Translation Mod-
els. In Proceedings of AMTA. 
P. Liang, B. Taskar, and D. Klein, 2006, Alignment by 
Agreement, in Proceedings of NAACL. 
R. Moore, W. Yih and A. Bode, 2006, Improved Dis-
criminative Bilingual Word Alignment, In Proceed-
ings of COLING/ACL. 
F. J. Och and H. Ney. 2000a. A comparison of Align-
ment Models for Statistical Machine Translation. In 
Proceedings of COLING. 
F. J. Och and H. Ney. 2000b. Improved Statistical 
Alignment Models. In Proceedings of ACL. 
F. J. Och and H. Ney. 2000c. Giza++: Training of statis-
tical translation models. http://www-i6.informatik. 
rwthaachen.de/och/software/GIZA++.html. 
F. J. Och and H. Ney. 2002. Discriminative training and 
Maximum Entropy Models for Statistical Machine 
Translation, In Proceedings of ACL. 
F. J. Och, 2003, Minimum Error Rate Training in Statis-
tical Machine Translation. In Proceedings of ACL. 
K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 
2002. Bleu: A Method For Automatic Evaluation Of 
Machine Translation. in Proceedings of ACL.  
L. R. Rabiner, 1989 A tutorial on hidden Markov mod-
els and selected applications in speech recognition. 
Proceedings of the IEEE. 
K. Toutanova, H. T. Ilhan, and C. D. Manning. 2002. 
Extensions to HMM-based Statistical Word Align-
ment Models. In Proceedings of EMNLP. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based 
Word Alignment In Statistical Translation. In Pro-
ceedings of COLING. 
H. Zhang and D. Gildea, 2005, Stochastic Lexicalized 
Inversion Transduction Grammar for Alignment, In 
Proceedings of ACL. 
87
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355?362,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain Adaptation via Pseudo In-Domain Data Selection
Amittai Axelrod
University of Washington
Seattle, WA 98105
amittai@uw.edu
Xiaodong He
Microsoft Research
Redmond, WA 98052
xiaohe@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98052
jfgao@microsoft.com
Abstract
We explore efficient domain adaptation for the
task of statistical machine translation based
on extracting sentences from a large general-
domain parallel corpus that are most relevant
to the target domain. These sentences may
be selected with simple cross-entropy based
methods, of which we present three. As
these sentences are not themselves identical
to the in-domain data, we call them pseudo
in-domain subcorpora. These subcorpora ?
1% the size of the original ? can then used
to train small domain-adapted Statistical Ma-
chine Translation (SMT) systems which out-
perform systems trained on the entire corpus.
Performance is further improved when we use
these domain-adapted models in combination
with a true in-domain model. The results
show that more training data is not always
better, and that best results are attained via
proper domain-relevant data selection, as well
as combining in- and general-domain systems
during decoding.
1 Introduction
Statistical Machine Translation (SMT) system per-
formance is dependent on the quantity and quality
of available training data. The conventional wisdom
is that more data is better; the larger the training cor-
pus, the more accurate the model can be.
The trouble is that ? except for the few all-purpose
SMT systems ? there is never enough training data
that is directly relevant to the translation task at
hand. Even if there is no formal genre for the text
to be translated, any coherent translation task will
have its own argot, vocabulary or stylistic prefer-
ences, such that the corpus characteristics will nec-
essarily deviate from any all-encompassing model of
language. For this reason, one would prefer to use
more in-domain data for training. This would em-
pirically provide more accurate lexical probabilities,
and thus better target the task at hand. However, par-
allel in-domain data is usually hard to find1, and so
performance is assumed to be limited by the quan-
tity of domain-specific training data used to build the
model. Additional parallel data can be readily ac-
quired, but at the cost of specificity: either the data
is entirely unrelated to the task at hand, or the data is
from a broad enough pool of topics and styles, such
as the web, that any use this corpus may provide is
due to its size, and not its relevance.
The task of domain adaptation is to translate a text
in a particular (target) domain for which only a small
amount of training data is available, using an MT
system trained on a larger set of data that is not re-
stricted to the target domain. We call this larger set
of data a general-domain corpus, in lieu of the stan-
dard yet slightly misleading out-of-domain corpus,
to allow a large uncurated corpus to include some
text that may be relevant to the target domain.
Many existing domain adaptation methods fall
into two broad categories. Adaptation can be done at
the corpus level, by selecting, joining, or weighting
the datasets upon which the models (and by exten-
sion, systems) are trained. It can be also achieved at
the model level by combining multiple translation or
language models together, often in a weighted man-
ner. We explore both categories in this work.
1Unless one dreams of translating parliamentary speeches.
355
First, we present three methods for ranking the
sentences in a general-domain corpus with respect to
an in-domain corpus. A cutoff can then be applied to
produce a very small?yet useful? subcorpus, which
in turn can be used to train a domain-adapted MT
system. The first two data selection methods are ap-
plications of language-modeling techniques to MT
(one for the first time). The third method is novel
and explicitly takes into account the bilingual na-
ture of the MT training corpus. We show that it is
possible to use our data selection methods to subse-
lect less than 1% (or discard 99%) of a large general
training corpus and still increase translation perfor-
mance by nearly 2 BLEU points.
We then explore how best to use these selected
subcorpora. We test their combination with the in-
domain set, followed by examining the subcorpora
to see whether they are actually in-domain, out-of-
domain, or something in between. Based on this, we
compare translation model combination methods.
Finally, we show that these tiny translation mod-
els for model combination can improve system per-
formance even further over the current standard way
of producing a domain-adapted MT system. The re-
sulting process is lightweight, simple, and effective.
2 Related Work
2.1 Training Data Selection
An underlying assumption in domain adaptation is
that a general-domain corpus, if sufficiently broad,
likely includes some sentences that could fall within
the target domain and thus should be used for train-
ing. Equally, the general-domain corpus likely in-
cludes sentences that are so unlike the domain of the
task that using them to train the model is probably
more harmful than beneficial. One mechanism for
domain adaptation is thus to select only a portion of
the general-domain corpus, and use only that subset
to train a complete system.
The simplest instance of this problem can be
found in the realm of language modeling, using
perplexity-based selection methods. The sentences
in the general-domain corpus are scored by their per-
plexity score according to an in-domain language
model, and then sorted, with only the lowest ones
being retained. This has been done for language
modeling, including by Gao et al(2002), and more
recently by Moore and Lewis (2010). The ranking
of the sentences in a general-domain corpus accord-
ing to in-domain perplexity has also been applied to
machine translation by both Yasuda et al(2008), and
Foster et al(2010). We test this approach, with the
difference that we simply use the source side per-
plexity rather than computing the geometric mean
of the perplexities over both sides of the corpus. We
also reduce the size of the training corpus far more
aggressively than Yasuda et als 50%. Foster et al
(2010) do not mention what percentage of the cor-
pus they select for their IR-baseline, but they con-
catenate the data to their in-domain corpus and re-
port a decrease in performance. We both keep the
models separate and reduce their size.
A more general method is that of (Matsoukas et
al., 2009), who assign a (possibly-zero) weight to
each sentence in the large corpus and modify the em-
pirical phrase counts accordingly. Foster et al(2010)
further perform this on extracted phrase pairs, not
just sentences. While this soft decision is more flex-
ible than the binary decision that comes from includ-
ing or discarding a sentence from the subcorpus, it
does not reduce the size of the model and comes
at the cost of computational complexity as well as
the possibility of overfitting. Additionally, the most
effective features of (Matsoukas et al, 2009) were
found to be meta-information about the source doc-
uments, which may not be available.
Another perplexity-based approach is that taken
by Moore and Lewis (2010), where they use the
cross-entropy difference as a ranking function rather
than just cross-entropy. We apply this criterion for
the first time to the task of selecting training data
for machine translation systems. We furthermore ex-
tend this idea for MT-specific purposes.
2.2 Translation Model Combination
In addition to improving the performance of a sin-
gle general model with respect to a target domain,
there is significant interest in using two translation
models, one trained on a larger general-domain cor-
pus and the other on a smaller in-domain corpus, to
translate in-domain text. After all, if one has ac-
cess to an in-domain corpus with which to select
data from a general-domain corpus, then one might
as well use the in-domain data, too. The expectation
is that the larger general-domain model should dom-
356
inate in regions where the smaller in-domain model
lacks coverage due to sparse (or non-existent) ngram
counts. In practice, most practical systems also per-
form target-side language model adaptation (Eck et
al., 2004); we eschew this in order to isolate the ef-
fects of translation model adaptation alone.
Directly concatenating the phrase tables into one
larger one isn?t strongly motivated; identical phrase
pairs within the resulting table can lead to unpre-
dictable behavior during decoding. Nakov (2008)
handled identical phrase pairs by prioritizing the
source tables, however in our experience identical
entries in phrase tables are not very common when
comparing across domains. Foster and Kuhn (2007)
interpolated the in- and general-domain phrase ta-
bles together, assigning either linear or log-linear
weights to the entries in the tables before combining
overlapping entries; this is now standard practice.
Lastly, Koehn and Schroeder (2007) reported
improvements from using multiple decoding paths
(Birch et al, 2007) to pass both tables to the Moses
SMT decoder (Koehn et al, 2003), instead of di-
rectly combining the phrase tables to perform do-
main adaptation. In this work, we directly com-
pare the approaches of (Foster and Kuhn, 2007) and
(Koehn and Schroeder, 2007) on the systems gener-
ated from the methods mentioned in Section 2.1.
3 Experimental Framework
3.1 Corpora
We conducted our experiments on the Interna-
tional Workshop on Spoken Language Translation
(IWSLT) Chinese-to-English DIALOG task 2, con-
sisting of transcriptions of conversational speech in
a travel setting. Two corpora are needed for the
adaptation task. Our in-domain data consisted of the
IWSLT corpus of approximately 30,000 sentences
in Chinese and English. Our general-domain cor-
pus was 12 million parallel sentences comprising a
variety of publicly available datasets, web data, and
private translation texts. Both the in- and general-
domain corpora were identically segmented (in Chi-
nese) and tokenized (in English), but otherwise un-
processed. We evaluated our work on the 2008
IWSLT spontaneous speech Challenge Task3 test
2http://iwslt2010.fbk.eu/node/33
3Correct-Recognition Result (CRR) condition
set, consisting of 504 Chinese sentences with 7 En-
glish reference translations each. This is the most
recent IWSLT test set for which the reference trans-
lations are available.
3.2 System Description
In order to highlight the data selection work, we
used an out-of-the-box Moses framework using
GIZA++ (Och and Ney, 2003) and MERT (Och,
2003) to train and tune the machine translation sys-
tems. The only exception was the phrase table
for the large out-of-domain system trained on 12m
sentence pairs, which we trained on a cluster us-
ing a word-dependent HMM-based alignment (He,
2007). We used the Moses decoder to produce all
the system outputs, and scored them with the NIST
mt-eval31a 4 tool used in the IWSLT evalutation.
3.3 Language Models
Our work depends on the use of language models to
rank sentences in the training corpus, in addition to
their normal use during machine translation tuning
and decoding. We used the SRI Language Model-
ing Toolkit (Stolcke, 2002) was used for LM train-
ing in all cases: corpus selection, MT tuning, and
decoding. We constructed 4gram language mod-
els with interpolated modified Kneser-Ney discount-
ing (Chen and Goodman, 1998), and set the Good-
Turing threshold to 1 for trigrams.
3.4 Baseline System
The in-domain baseline consisted of a translation
system trained using Moses, as described above, on
the IWSLT corpus. The resulting model had a phrase
table with 515k entries. The general-domain base-
line was substantially larger, having been trained on
12 million sentence pairs, and had a phrase table
containing 1.5 billion entries. The BLEU scores of
the baseline single-corpus systems are in Table 1.
Corpus Phrases Dev Test
IWSLT 515k 45.43 37.17
General 1,478m 42.62 40.51
Table 1: Baseline translation results for in-domain and
general-domain systems.
4http://www.itl.nist.gov/iad/mig/tools/
357
4 Training Data Selection Methods
We present three techniques for ranking and select-
ing subsets of a general-domain corpus, with an eye
towards improving overall translation performance.
4.1 Data Selection using Cross-Entropy
As mentioned in Section 2.1, one established
method is to rank the sentences in the general-
domain corpus by their perplexity score accord-
ing to a language model trained on the small in-
domain corpus. This reduces the perplexity of the
general-domain corpus, with the expectation that
only sentences similar to the in-domain corpus will
remain. We apply the method to machine trans-
lation, even though perplexity reduction has been
shown to not correlate with translation performance
(Axelrod, 2006). For this work we follow the proce-
dure of Moore and Lewis (2010), which applies the
cosmetic change of using the cross-entropy rather
than perplexity.
The perplexity of some string s with empirical n-
gram distribution p given a language model q is:
2?
?
x p(x) log q(x) = 2H(p,q) (1)
where H(p, q) is the cross-entropy between p and
q. We simplify this notation to just HI(s), mean-
ing the cross-entropy of string s according to a lan-
guage model LMI which has distribution q. Se-
lecting the sentences with the lowest perplexity is
therefore equivalent to choosing the sentences with
the lowest cross-entropy according to the in-domain
language model. For this experiment, we used a lan-
guage model trained (using the parameters in Sec-
tion 3.3) on the Chinese side of the IWSLT corpus.
4.2 Data Selection using Cross-Entropy
Difference
Moore and Lewis (2010) also start with a language
model LMI over the in-domain corpus, but then fur-
ther construct a language modelLMO of similar size
over the general-domain corpus. They then rank the
general-domain corpus sentences using:
HI(s)?HO(s) (2)
and again taking the lowest-scoring sentences. This
criterion biases towards sentences that are both like
the in-domain corpus and unlike the average of the
general-domain corpus. For this experiment we re-
used the in-domain LM from the previous method,
and trained a second LM on a random subset of
35k sentences from the Chinese side of the general
corpus, except using the same vocabulary as the in-
domain LM.
4.3 Data Selection using Bilingual
Cross-Entropy Difference
In addition to using these two monolingual criteria
for MT data selection, we propose a new method
that takes in to account the bilingual nature of the
problem. To this end, we sum cross-entropy differ-
ence over each side of the corpus, both source and
target:
[HI?src(s)?HO?src(s)]+[HI?tgt(s)?HO?tgt(s)]
(3)
Again, lower scores are presumed to be better. This
approach reuses the source-side language models
from Section 4.2, but requires similarly-trained ones
over the English side. Again, the vocabulary of the
language model trained on a subset of the general-
domain corpus was restricted to only cover those
tokens found in the in-domain corpus, following
Moore and Lewis (2010).
5 Results of Training Data Selection
The baseline results show that a translation system
trained on the general-domain corpus outperforms a
system trained on the in-domain corpus by over 3
BLEU points. However, this can be improved fur-
ther. We used the three methods from Section 4 to
identify the best-scoring sentences in the general-
domain corpus.
We consider three methods for extracting domain-
targeted parallel data from a general corpus: source-
side cross-entropy (Cross-Ent), source-side cross-
entropy difference (Moore-Lewis) from (Moore and
Lewis, 2010), and bilingual cross-entropy difference
(bML), which is novel.
Regardless of method, the overall procedure is
the same. Using the scoring method, We rank the
individual sentences of the general-domain corpus,
select only the top N . We used the top N =
{35k, 70k, 150k} sentence pairs out of the 12 mil-
358
lion in the general corpus 5. The net effect is that of
domain adaptation via threshhold filtering. New MT
systems were then trained solely on these small sub-
corpora, and compared against the baseline model
trained on the entire 12m-sentence general-domain
corpus. Table 2 contains BLEU scores of the sys-
tems trained on subsets of the general corpus.
Method Sentences Dev Test
General 12m 42.62 40.51
Cross-Entropy 35k 39.77 40.66
Cross-Entropy 70k 40.61 42.19
Cross-Entropy 150k 42.73 41.65
Moore-Lewis 35k 36.86 40.08
Moore-Lewis 70k 40.33 39.07
Moore-Lewis 150k 41.40 40.17
bilingual M-L 35k 39.59 42.31
bilingual M-L 70k 40.84 42.29
bilingual M-L 150k 42.64 42.22
Table 2: Translation results using only a subset of the
general-domain corpus.
All three methods presented for selecting a sub-
set of the general-domain corpus (Cross-Entropy,
Moore-Lewis, bilingual Moore-Lewis) could be
used to train a state-of-the-art machine transla-
tion system. The simplest method, using only the
source-side cross-entropy, was able to outperform
the general-domain model when selecting 150k out
of 12 million sentences. The other monolingual
method, source-side cross-entropy difference, was
able to perform nearly as well as the general-
domain model with only 35k sentences. The bilin-
gual Moore-Lewis method proposed in this paper
works best, consistently boosting performance by
1.8 BLEU while using less than 1% of the available
training data.
5.1 Pseudo In-Domain Data
The results in Table 2 show that all three meth-
ods (Cross-Entropy, Moore-Lewis, bilingual Moore-
Lewis) can extract subsets of the general-domain
corpus that are useful for the purposes of statistical
machine translation. It is tempting to describe these
as methods for finding in-domain data hidden in a
5Roughly 1x, 2x, and 4x the size of the in-domain corpus.
general-domain corpus. Alas, this does not seem to
be the case.
We trained a baseline language model on the in-
domain data and used it to compute the perplexity
of the same (in-domain) held-out dev set used to
tune the translation models. We extracted the top
N sentences using each ranking method, varying N
from 10k to 200k, and then trained language models
on these subcorpora. These were then used to also
compute the perplexity of the same held-out dev set,
shown below in Figure 1.
020406080100120140
'0
20
25
30
35
40
50
70
100
125
150
175
Top-r
anked
 
gener
al-dom
ain se
ntenc
es (in k
)
Devset Perplexity
In-dom
ain ba
seline
Cross
-
Entrop
y
Moore
-
Lewis
bilingu
al M-L
Figure 1: Corpus Selection Results
The perplexity of the dev set according to LMs
trained on the top-ranked sentences varied from 77
to 120, depending on the size of the subset and the
method used. The Cross-Entropy method was con-
sistently worse than the others, with a best perplex-
ity of 99.4 on 20k sentences, and bilingual Moore-
Lewis was consistently the best, with a lowest per-
plexity of 76.8. And yet, none of these scores are
anywhere near the perplexity of 36.96 according to
the LM trained only on in-domain data.
From this it can be deduced that the selection
methods are not finding data that is strictly in-
domain. Rather they are extracting pseudo in-
domain data which is relevant, but with a differing
distribution than the original in-domain corpus.
As further evidence, consider the results of con-
catenating the in-domain corpus with the best ex-
tracted subcorpora (using the bilingual Moore-
Lewis method), shown in Table 3. The change in
359
both the dev and test scores appears to reflect dissim-
ilarity in the underlying data. Were the two datasets
more alike, one would expect the models to rein-
force each other rather than cancel out.
Method Sentences Dev Test
IWSLT 30k 45.43 37.17
bilingual M-L 35k 39.59 42.31
bilingual M-L 70k 40.84 42.29
bilingual M-L 150k 42.64 42.22
IWSLT + bi M-L 35k 47.71 41.78
IWSLT + bi M-L 70k 47.80 42.30
IWSLT + bi M-L 150k 48.44 42.01
Table 3: Translation results concatenating the in-domain
and pseudo in-domain data to train a single model.
6 Translation Model Combination
Because the pseudo in-domain data should be kept
separate from the in-domain data, one must train
multiple translation models in order to advanta-
geously use the general-domain corpus. We now ex-
amine how best to combine these models.
6.1 Linear Interpolation
A common approach to managing multiple transla-
tion models is to interpolate them, as in (Foster and
Kuhn, 2007) and (Lu? et al, 2007). We tested the
linear interpolation of the in- and general-domain
translation models as follows: Given one model
which assigns the probability P1(t|s) to the trans-
lation of source string s into target string t, and a
second model which assigns the probability P2(t|s)
to the same event, then the interpolated translation
probability is:
P (t|s) = ?P1(t|s) + (1? ?)P2(t|s) (4)
Here ? is a tunable weight between 0 and 1, which
we tested in increments of 0.1. Linear interpolation
of phrase tables was shown to improve performance
over the individual models, but this still may not be
the most effective use of the translation models.
6.2 Multiple Models
We next tested the approach in (Koehn and
Schroeder, 2007), passing the two phrase tables di-
rectly to the decoder and tuning a system using both
phrase tables in parallel. Each phrase table receives
a separate set of weights during tuning, thus this
combined translation model has more parameters
than a normal single-table system.
Unlike (Nakov, 2008), we explicitly did not at-
tempt to resolve any overlap between the phrase ta-
bles, as there is no need to do so with the multiple
decoding paths. Any phrase pairs appearing in both
models will be treated separately by the decoder.
However, the exact overlap between the phrase ta-
bles was tiny, minimizing this effect.
6.3 Translation Model Combination Results
Table 4 shows baseline results for the in-domain
translation system and the general-domain system,
evaluated on the in-domain data. The table also
shows that linearly interpolating the translation
models improved the overall BLEU score, as ex-
pected. However, using multiple decoding paths,
and no explicit model merging at all, produced even
better results, by 2 BLEU points over the best indi-
vidual model and 1.3 BLEU over the best interpo-
lated model, which used ? = 0.9.
System Dev Test
IWSLT 45.43 37.17
General 42.62 40.51
Interpolate IWSLT, General 48.46 41.28
Use both IWSLT, General 49.13 42.50
Table 4: Translation model combination results
We conclude that it can be more effective to not
attempt translation model adaptation directly, and
instead let the decoder do the work.
7 Combining Multi-Model and Data
Selection Approaches
We presented in Section 5 several methods to im-
prove the performance of a single general-domain
translation system by restricting its training corpus
on an information-theoretic basis to a very small
number of sentences. However, Section 6.3 shows
that using two translation models over all the avail-
able data (one in-domain, one general-domain) out-
performs any single individual translation model so
far, albeit only slightly.
360
Method Dev Test
IWSLT 45.43 37.17
General 42.62 40.51
both IWSLT, General 49.13 42.50
IWSLT, Moore-Lewis 35k 48.51 40.38
IWSLT, Moore-Lewis 70k 49.65 40.45
IWSLT, Moore-Lewis 150k 49.50 41.40
IWSLT, bi M-L 35k 48.85 39.82
IWSLT, bi M-L 70k 49.10 43.00
IWSLT, bi M-L 150k 49.80 43.23
Table 5: Translation results from using in-domain and
pseudo in-domain translation models together.
It is well and good to use the in-domain data
to select pseudo in-domain data from the general-
domain corpus, but given that this requires access
to an in-domain corpus, one might as well use it.
As such, we used the in-domain translation model
alongside translation models trained on the subcor-
pora selected using the Moore-Lewis and bilingual
Moore-Lewis methods in Section 4. The results are
in Table 5.
A translation system trained on a pseudo in-
domain subset of the general corpus, selected with
the bilingual Moore-Lewis method, can be further
improved by combining with an in-domain model.
Furthermore, this system combination works better
than the conventional multi-model approach by up
to 0.7 BLEU on both the dev and test sets.
Thus a domain-adapted system comprising two
phrase tables trained on a total of 180k sen-
tences outperformed the standard multi-model sys-
tem which was trained on 12 million sentences. This
tiny combined system was also 3+ points better than
the general-domain system by itself, and 6+ points
better than the in-domain system alone.
8 Conclusions
Sentence pairs from a general-domain corpus that
seem similar to an in-domain corpus may not actu-
ally represent the same distribution of language, as
measured by language model perplexity. Nonethe-
less, we have shown that relatively tiny amounts of
this pseudo in-domain data can prove more useful
than the entire general-domain corpus for the pur-
poses of domain-targeted translation tasks.
This paper has also explored three simple yet
effective methods for extracting these pseudo in-
domain sentences from a general-domain corpus. A
translation model trained on any of these subcorpora
can be comparable ? or substantially better ? than a
translation system trained on the entire corpus.
In particular, the new bilingual Moore-Lewis
method, which is specifically tailored to the ma-
chine translation scenario, is shown to be more ef-
ficient and stable for MT domain adaptation. Trans-
lation models trained on data selected in this way
consistently outperformed the general-domain base-
line while using as few as 35k out of 12 million sen-
tences. This fast and simple technique for discarding
over 99% of the general-domain training corpus re-
sulted in an increase of 1.8 BLEU points.
We have also shown in passing that the linear in-
terpolation of translation models may work less well
for translation model adaptation than the multiple
paths decoding technique of (Birch et al, 2007).
These approaches of data selection and model com-
bination can be stacked, resulting in a compact, two
phrase-table, translation system trained on 1% of the
available data that again outperforms a state-of-the-
art translation system trained on all the data.
Besides improving translation performance, this
work also provides a way to mine very large corpora
in a computationally-limited environment, such as
on an ordinary computer or perhaps a mobile device.
The maximum size of a useful general-domain cor-
pus is now limited only by the availability of data,
rather than by how large a translation model can be
fit into memory at once.
References
Amittai Axelrod. 2006. Factored Language Models for
Statistical Machine Translation. M.Sc. Thesis. Univer-
sity of Edinburgh, Scotland.
Alexandra Birch, Miles Osborne and Philipp Koehn.
2007. CCG Supertags in Factored Translation Models.
Workshop on Statistical Machine Translation, Associ-
ation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report 10-98, Computer Science
Group, Harvard University.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
361
Translation based on Information Retrieval. Language
Resources and Evaluation.
George Foster and Roland Kuhn. 2007. Mixture-Model
Adaptation for SMT. Workshop on Statistical Machine
Translation, Association for Computational Linguis-
tics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative Instatnce Weighting for Domain Adap-
tation in Statistical Machine Translation. Empirical
Methods in Natural Language Processing.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu
Lee. 2002. Toward a Unified Approach to Statistical
Language Modeling for Chinese. ACM Transactions
on Asian Language Information Processing.
Xiaodong He. 2007. Using Word-Dependent Transition
Models in HMM-based Word Alignment for Statisti-
cal Machine Translation. Workshop on Statistical Ma-
chine Translation, Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2003. Moses: Open Source
Toolkit for Statistical Machine Translation. Demo Ses-
sion, Association for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. Workshop on Statistical Machine Translation,
Association for Computational Linguistics.
Yajuan Lu?, Jin Huang and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning.
Spyros Matsoukas, Antti-Veikko Rosti, Bing Zhang.
2009. Discriminative Corpus Weight Estimation for
Machine Translation. Empirical Methods in Natural
Language Processing.
Robert Moore and William Lewis. 2010. Intelligent Se-
lection of Language Model Training Data. Association
for Computational Linguistics.
Preslav Nakov. 2008. Improving English-Spanish Sta-
tistical Machine Translation: Experiments in Domain
Adaptation, Sentence Paraphrasing, Tokenization, and
Recasing. Workshop on Statistical Machine Transla-
tion, Association for Computational Linguistics.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. Spoken Language Process-
ing.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, Ei-
ichiro Sumita. 2008. Method of Selecting Train-
ing Data to Build a Compact and Efficient Transla-
tion Model. International Joint Conference on Natural
Language Processing.
362
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 666?676, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Lexicon Models from Search Logs for Query Expansion 
 
Jianfeng Gao 
Microsoft Research, Redmond 
Washington 98052, USA 
jfgao@microsoft.com 
 
Shasha Xie 
Educational Testing Service, Princeton 
New Jersey 08540, USA 
sxie@ets.org 
 
Xiaodong He 
Microsoft Research, Redmond 
Washington 98052, USA 
xiaohe@microsoft.com 
 
 
Alnur Ali 
Microsoft Bing, Bellevue 
Washington 98004, USA 
alnurali@microsoft.com 
 
Abstract 
This paper explores log-based query expan-
sion (QE) models for Web search. Three 
lexicon models are proposed to bridge the 
lexical gap between Web documents and 
user queries. These models are trained on 
pairs of user queries and titles of clicked 
documents. Evaluations on a real world data 
set show that the lexicon models, integrated 
into a ranker-based QE system, not only 
significantly improve the document retriev-
al performance but also outperform two 
state-of-the-art log-based QE methods. 
1 Introduction 
Term mismatch is a fundamental problem in Web 
search, where queries and documents are com-
posed using different vocabularies and language 
styles. Query expansion (QE) is an effective strate-
gy to address the problem. It expands a query is-
sued by a user with additional related terms, called 
expansion terms, so that more relevant documents 
can be retrieved.  
In this paper we explore the use of clickthrough 
data and translation models for QE. We select ex-
pansion terms for a query according to how likely 
it is that the expansion terms occur in the title of a 
document that is relevant to the query. Assuming 
that a query is parallel to the titles of documents 
clicked for that query (Gao et al
icon models are trained on query-title pairs ex-
tracted from clickthrough data. The first is a word 
model that learns the translation probability be-
tween single words. The second model uses lexi-
calized triplets to incorporate word dependencies 
for translation. The third is a bilingual topic model, 
which represents a query as a distribution of hid-
den topics and learns the translation between a 
query and a title term at the semantic level. We 
will show that the word model provides a rich set 
of expansion candidates while the triplet and topic 
models can effectively select good expansion 
terms, and that a ranker-based QE system which 
incorporates all three of these models not only sig-
nificantly improves Web search result but outper-
forms other log-based QE methods that are state-
of-the-art. 
There is growing interest in applying user logs 
to improve QE. A recent survey is due to Baeze-
Yates and Ribeiro-Neto (2011). Below, we briefly 
discuss two log-based QE methods that are closest 
to ours and are re-implemented in this study for 
comparison. Both systems use the same type of log 
data that we used to train the lexicon models. The 
term correlation model of Cui et al
to our knowledge the first to explore query-
document relations for direct extraction of expan-
sion terms for Web search. The method outper-
forms traditional QE methods that do not use log 
data e.g. the local analysis model of Xu and Croft 
(1996). In addition, as pointed out by Cui et al
(2003) there are three important advantages that 
make log-based QE a promising technology to im-
prove the performance of commercial search en-
gines. First, unlike traditional QE methods that are 
based on relevance feedback, log-based QE derives 
expansion terms from search logs, allowing term 
correlations to be pre-computed offline. Compared 
to methods that are based on thesauri either com-
piled manually (Prager et alu-
666
tomatically from document collections (Jing and 
Croft 1994), the log-based method is superior in 
that it explicitly captures the correlation between 
query terms and document terms, and thus can 
bridge the lexical gap between them more effec-
tively. Second, since search logs retrain query-
document pairs clicked by millions of users, the 
term correlations reflect the preference of the ma-
jority of users. Third, the term correlations evolve 
along with the accumulation of user logs, thus can 
reflect updated user interests at a specific time. 
However, as pointed out by Riezler et al
(2008), Cui et ald method suf-
fers low precision of QE partly because the corre-
lation model does not explicitly capture context 
information and is susceptible to noise. Riezler et 
al. developed a QE system by retraining a standard 
phrase-based statistical machine translation (SMT) 
system using query-snippet pairs extracted from 
clickthrough data (Riezler et al
Liu 2010). The SMT-based system can produce 
cleaner, more relevant expansion terms because 
rich context information useful for filtering noisy 
expansions is captured by combining language 
model and phrase translation model in its decoder. 
Furthermore, in the SMT system all component 
models are properly smoothed using sophisticated 
techniques to avoid sparse data problems while the 
correlation model relies on pure counts of term 
frequencies. However, the SMT system is used as a 
black box in their experiments. So the relative con-
tribution of different SMT components is not veri-
fied empirically. In this study we break this black 
box in order to build a better, simpler QE system. 
We will show that the proposed lexicon models 
outperform significantly the term correlation mod-
el, and that a simpler QE system that incorporates 
the lexicon models can beat the sophisticated, 
black-box SMT system. 
2 Lexicon Models 
We view search queries and Web documents as 
two different languages, and cast QE as a means to 
bridge the language gap by translating queries to 
documents, represented by their titles. In this sec-
tion, we will describe three translation models that 
are based on terms, triplets, and topics, respective-
ly, and the way these models are learned from que-
ry-title pairs extracted from clickthrough data. 
2.1 Word Model 
The word model takes the form of IBM Model 1 
(Brown et alafferty 1999). Let 
            be a query,   be an expansion term 
candidate, the translation probability from   to   is 
defined as  
   |     ? ( |  ) (  | )
 
   
 (1) 
where    |   is the unsmoothed unigram proba-
bility of word   in query  . The word translation 
probabilities    |   are estimated on the query-
title pairs derived from the clickthrough data by 
assuming that the title terms are likely to be the 
desired expansions of the paired query. Our train-
ing method follows the standard procedure of 
training statistical word alignment models pro-
posed by Brown et al we opti-
mize the model parameters   by maximizing the 
probability of generating document titles from que-
ries over the entire training corpus: 
           ?    |     
 
   
 (2) 
where both the titles   and the paired queries   are 
viewed as bag of words. The translation probability 
    |      takes the form of IBM Model 1 as  
   |     
 
      
?? (  |    )
 
   
 
   
 (3) 
where   is a constant,   is the length of  , and   is 
the length of  . To find the optimal word transla-
tion probabilities of IBM Model 1, we used the EM 
algorithm, where the number of iterations is deter-
mined empirically on held-out data. 
2.2 Triplet Model 
The word model is context independent. The triplet 
model, which is originally proposed for SMT (Ha-
san et al to capture inter-term 
dependencies for selecting expansion terms. The 
model is based on lexicalized triplets (       ) 
which can be understood as two query terms trig-
gering one expansion term. The translation proba-
bility of   given   for the triplet model is parame-
terized as 
667
   |     
 
 
? ?  ( |     )
 
     
   
   
 (4) 
where Z is a normalization factor based on the cor-
responding query length, i.e.,   
      
 
, and 
 (  |     ) is the probability of translating    into 
   given another query word   . Since    can be 
any word in   that is not necessary to be adjacent 
to   , the triple model is able to combine local (i.e. 
word and phrase level) and global (i.e. query level) 
contextual information useful for word translation.  
Similar to the case of word model, we used the 
EM algorithm to estimate the translation probabili-
ties    |      on the query-title pairs.  Since the 
number of all possible triplets (      ) is large and 
as a consequence the model training could suffer 
the data sparseness problem, in our experiments 
count-based cutoff is applied to prune the model to 
a manageable size. 
2.3 Bilingual Topic Model (BLTM) 
The BLTM was originally proposed for Web doc-
ument ranking by Gao et aln-
derlying the model is that a search query and its 
relevant Web documents share a common distribu-
tion of (hidden) topics, but use different (probably 
overlapping) vocabularies to express these topics. 
Intuitively, BLTM-based QE works as follows. 
First, a query is represented as a vector of topics. 
Then, all the candidate expansion terms, which are 
selected from document, are ranked by how likely 
it is that these document terms are selected to best 
describe those topics. In a sense, BLTM is similar 
to the word model and the triplet model since they 
all map a query to a document word. BLTM differs 
in that the mapping is performed at the topic level 
(via a language independent semantic representa-
tion) rather than at the word level. In our experi-
ments BLTM is found to often select a different set 
of expansion terms and is complementary to the 
word model and the triplet model. 
Formally, BLTM-based QE assumes the follow-
ing story of generating   from  : 
1. First, for each topic  , a pair of different 
word distributions    
    
   are selected 
from a Dirichlet prior with concentration pa-
rameter ?, where  
 
 is a topic-specific query 
term distribution, and   
  a topic-specific 
document term distribution. Assuming there 
are   topics, we have two sets of distribu-
tions       
      
   and    
   
      
  . 
2. Given  , a topic distribution    is drawn 
from a Dirichlet prior with concentration pa-
rameter  . 
3. Then a document term (i.e., expansion term 
candidate)   is generated by first selecting a 
topic   according to the topic distribution   , 
and then drawing a word from   
 . 
By summing over all possible topics, we end up 
with the following model form 
       |   ?   |  
     |   
 
 (5) 
The BLTM training follows the method described 
in Gao et ale EM algorithm to 
estimate the parameters (         of BLTM by 
maximizing the joint log-likelihood of the query-
title pairs and the parameters. In training, we also 
constrain that the paired query and title have simi-
lar fractions of tokens assigned to each topic. The 
constraint is enforced on expectation using posteri-
or regularization (Ganchev et al
3 A Ranker-Based QE System 
This section describes a ranker-based QE system in 
which the three lexicon models described above 
are incorporated. The system expands an input 
query in two distinct stages, candidate generation 
and ranking, as illustrated by an example in Figure 
1. 
Original query jaguar locator 
Ranked expansion  jaguar finder 
candidates 
(altered words are in 
car locator 
jaguar location 
italic) jaguar directory 
 ? 
 jaguar list 
Expanded query OR(jaguar, car)  
(selected expansion 
terms are in italic) 
OR(locator, finder, location, 
directory) 
Figure 1. An example of an original query, its expan-
sion candidates and the expanded query generated by 
the ranker-based QE system. 
 
668
In candidate generation, an input query   is 
first tokenized into a sequence of terms. For each 
term   that is not a stop word, we consult a word 
model described in Section 2.1 to identify the best 
  altered words according to their word transla-
tion probabilities from  . Then, we form a list of 
expansion candidates, each of which contains all 
the original words in   except for the word that is 
substituted by one of its altered words. So, for a 
query with   terms, there are at most    candi-
dates. 
In the second stage, all the expansion candidates 
are ranked using a ranker that is based on the Mar-
kov Random Field (MRF) model in which the 
three lexicon models are incorporated as features.  
Expansion terms of a query are taken from those 
terms in the  -best (     in our experiments) 
expansion candidates of the query that have not 
been seen in the original query string. 
In the remainder of this section we will describe 
in turn the MRF-based ranker, the ranking features, 
and the way the ranker parameters are estimated. 
3.1 MRF-Based Ranker 
The ranker is based on the MRF model that models 
the joint distribution of         over a set of ex-
pansion term random variables             and 
a query random variable  . It is constructed from a 
graph   consisting of a query node and nodes for 
each expansion term. Nodes in the graph represent 
random variables and edges define the independ-
ence semantics between the variables. An MRF 
satisfies the Markov property (Bishop 2006), 
which states that a node is independent of all of its 
non-neighboring nodes given observed values of 
its neighbors, defined by the clique configurations 
of  . The joint distribution over the random varia-
bles in   is defined as  
        
 
  
?       
      
 (6) 
where      is the set of cliques in  , and each 
       is a non-negative potential function de-
fined over a clique configuration c that measures 
the compatibility of the configuration,   is a set of 
parameters that are used within the potential func-
tion, and    normalizes the distribution. For rank-
ing expansion candidates, we can drop the expen-
sive computation of    since it is independent of 
E, and simply rank each expansion candidate   by 
its unnormalized joint probability with   under the 
MRF. It is common to define MRF potential func-
tions of the exponential form as        
           , where      is a real-valued feature 
function over clique values and    is the weight of 
the feature function. Then, we can compute the 
posterior     |   as 
    |   
       
     
 (7) 
    
?   ?          
      
 ?       
      
  
which is essentially a weighted linear combination 
of a set of features. 
Therefore, to instantiate the MRF model, one 
needs to define a graph structure and a set of po-
tential functions. In this paper, the graphical model 
representation we propose for QE is a fully con-
nected graph shown in Figure 2, where all expan-
sion terms and the original query are assumed de-
pendent with each other. In what follows, we will 
define six types of cliques that we are interested in 
defining features (i.e., potential functions) over. 
3.2 Features 
The cliques and features are inspired by the com-
ponent models used in SMT systems. The cliques 
defined in   for MRF can be grouped into two cat-
egories. The first includes three types of cliques 
involving both the query node and one or more 
expansion terms. The potential functions defined 
over these cliques attempt to abstract the idea be-
hind the query to title translation models. The other 
three types, belonging to the second category, in-
volve only expansion terms. Their potential func-
 
 
Figure 2: The structure of the Markov random field for 
representing the term dependency among the query   
and the expansion terms        . 
669
tions attempt to abstract the idea behind the target 
language models.  
The first type of cliques involves a single ex-
pansion term and the query node. The potentials 
functions for these cliques are defined as 
                             (6) 
               
                   
where the three feature functions of the form 
       are defined as the log probabilities of 
translating   to   according to the word, triplet and 
topic models defined in Equations (1), (4) and (5), 
respectively. 
                   |    
                   |    
                       |    
The second type of cliques contains the query 
node and two expansion terms,    and     , which 
appear in consecutive order in the expansion. The 
potential functions over these cliques are defined 
as 
                                        (7) 
where the feature        is defined as the log prob-
ability of generating an expansion bigram given   
           |               |    
Unlike the language models used for document 
ranking (e.g., Zhai and Lafferty 2001), we cannot 
compute the bigram probability by simply counting 
the relative frequency of           in   because 
the query is usually very short and the bigram is 
unlikely to occur. Thus, we approximate the bi-
gram probability by assuming that the words in   
are independent with each other. We thus have 
         |   
            
    
 
 
           |   ?     |        
 
   
?      
 
   
  
where     |         is the translation probability 
computed using a variant of the triplet model de-
scribed  in Section 2.2. The model variation differs 
from the one of Equation (4) in two respects. First, 
it models the translation in a different direction i.e., 
from expansion to query. Second, we add a con-
straint to the triplets such that (       ) must be an 
ordered, contiguous bigram. The model variation is 
also trained using EM on query-title pairs.       
and       |    are assigned respectively by the 
unigram and bigram language models, estimated 
from the collection of document titles of the click-
through data, and        is the unigram probability 
of the query term, estimated from the collection of 
queries of the clickthrough data. 
The third type of cliques contains the query 
node and two expansion terms,    and   , which 
occur unordered within the expansion. The poten-
tial functions over these cliques are defined as 
                                    (8) 
where the feature        is defined as the log prob-
ability of generating a pair of expansion terms 
        given   
         |             |  .  
Unlike            |   defined in Equation (7), this 
class of features captures long-span term depend-
ency in the expansion candidate. Similar to the 
computation of          |   in Equation (7), we 
approximate        |   as     
       |   
          
    
 
 
         |   ?     |      
 
   
?      
 
   
  
where     |       is the translation probability 
computed using the triplet model described  in Sec-
tion 2.2, but in the expansion-to-query direction. 
      is assigned by a unigram language model 
estimated from the collection of document titles of 
the clickthrough data.     |    is assigned by a co-
occurrence model, estimated as  
    |    
        
?         
  
where         is the number of times that the two 
terms occur in the same title in clickthrough data.  
We now turn to the other three types of cliques 
that do not contain the query node. The fourth type 
of cliques contains only one expansion term. The 
potential functions are defined as 
670
                          (9) 
                  
where       is the unigram probability computed 
using a unigram language model trained on the 
collection of document titles. 
The fifth type of cliques contains a pair of terms 
appearing in consecutive order in the expansion. 
The potential functions are defined as 
                                    (10) 
                      |     
where       |    is the bigram probability com-
puted using a bigram language model trained on 
the collection of document titles. 
The sixth type of cliques contains a pair of 
terms appearing unordered within the expansion. 
The potential functions are defined as 
                                (11) 
                  |     
where     |    is the assigned by a co-occurrence 
model trained on the collection of document titles. 
3.3 Parameter Estimation 
The MRF model uses 8 classes of features defined 
on 6 types of cliques, as in Equations (6) to (11). 
Following previous work (e.g., Metzler and Croft 
2005; Bendersky et alhat all 
features within the same feature class are weighted 
by the same tied parameter   . Thus, the number of 
free parameters of the MRF model is significantly 
reduced. This not only makes the model training 
easier but also improves the robustness of the 
model. After tying the parameters and using the 
exponential potential function form, the MRF-
based ranker can be parameterized as  
    |  
    
?      ?          
 
   
  (12) 
   ?          
 
   
  
     ?            
 
   
  
   ?               
   
   
  
   ? ?             
 
     
   
   
  
   ?        
 
   
  
   ?             
   
   
  
   ? ?           
 
     
   
   
 
where there are in total 8  ?s to be estimated. 
Although the MRF is by nature a generative 
model, it is not always appropriate to train the pa-
rameters using conventional likelihood based ap-
proaches due to the metric divergence problem 
(Morgan et alaximum likelihood 
estimate is unlikely to be the one that optimizes the 
evaluation metric. In this study the effectiveness of 
a QE method is evaluated by first issuing a set of 
queries which are expanded using the method to a 
search engine and then measuring the Web search 
performance. Better QE methods are supposed to 
lead to better Web search results using the corre-
spondingly expanded query set. 
For this reason, the parameters of the MRF-
based ranker are optimized directly for Web 
search. In our experiments, the objective in train-
ing is Normalized Discounted Cumulative Gain 
(NDCG, Jarvelin and Kekalainen 2000), which is 
widely used as quality measure for Web search. 
Formally, we view parameter training as a multi-
dimensional optimization problem, with each fea-
ture class as one dimension. Since NDCG is not 
differentiable, we tried in our experiments numeri-
cal algorithms that do not require the computation 
of gradient. Among the best performers was the 
Powell Search algorithm (Press et al
constructs a set of   virtual directions that are con-
jugate (i.e., independent with each other), then it 
uses line search  times (    in our case), each 
on one virtual direction, to find the optimum. Line 
search is a one-dimensional optimization algo-
rithm. Our implementation follows the one de-
scribed in Gao et alsed to opti-
mize averaged precision. 
4 Experiments 
We evaluate the performance of a QE method by 
first issuing a set of queries which are expanded 
using the method to a search engine and then 
671
measuring the Web search performance. Better QE 
methods are supposed to lead to better Web search 
results using the correspondingly expanded query 
set.  
Due to the characteristics of our QE methods, 
we cannot conduct experiments on standard test 
collections such as the TREC data because they do 
not contain related user logs we need. Therefore, 
following previous studies of log-based QE (e.g., 
Cui et allthe 
proprietary datasets that have been developed for 
building a commercial search engine, and demon-
strate the effectiveness of our methods by compar-
ing them against previous state-of-the-art log-
based QE methods. 
The relevance judgment set consists of 4,000 
multi-term English queries. On average, each que-
ry is associated with 197 Web documents (URLs). 
Each query-URL pair has a relevance label. The 
label is human generated and is on a 5-level rele-
vance scale, 0 to 4, with 4 meaning document D is  
the  most  relevant  to  query Q  and 0 meaning  D 
is  not  relevant to Q.  
The relevance judgment set is constructed as 
follows. First, the queries are sampled from a year 
of search engine logs. Adult, spam, and bot queries 
are all removed. Queries are ?de-duped? so that 
only unique queries remain. To reflect a natural 
query distribution, we do not try to control the 
quality of these queries. For example, in our query 
sets, there are roughly 20% misspelled queries, 20% 
navigational queries, and 10% transactional que-
ries. Second, for each query, we collect Web doc-
uments to be judged by issuing the query to several 
popular search engines (e.g., Google, Bing) and 
fetching retrieval results from each. Finally, the 
query-document pairs are judged by a group of 
well-trained assessors. In this study all the queries 
are preprocessed as follows. The text is white-
space tokenized and lowercased, numbers are re-
tained, and no stemming/inflection treatment is 
performed. We split the judgment set into two non-
overlapping datasets, namely training and test sets, 
respectively. Each dataset contains 2,000 queries. 
The query-title pairs used for model training are 
extracted from one year of query log files using a 
procedure similar to Gao et al
periments we used a randomly sampled subset of 
20,692,219 pairs that do not overlap the queries 
and documents in the test set. 
Our Web document collection consists of ap-
proximately 2.5 billion Web pages. In the retrieval 
experiments we use the index based on the content 
fields (i.e., body and title text) of each Web page. 
The Web search performance is evaluated by 
mean NDCG. We report NDCG scores at trunca-
tion levels of 1, 3, and 10.  We also perform a sig-
nificance test using the paired t-test. Differences 
are considered statistically significant when p-
value is less than 0.05. 
4.1 Comparing Systems 
Table 1 shows the main document ranking results 
using different QE systems, developed and evalu-
ated using the datasets described above.  
NoQE (Row 1) is the baseline retrieval system 
that uses the raw input queries and the BM25 doc-
ument ranking model. Rows 2 to 4 are different QE 
systems. Their results are obtained by first expand-
ing a query, then using BM25 to rank the docu-
ments with respect to the expanded query.  
TC (Row 2) is our implementation of the corre-
lation-based QE system (Cui et al
takes the following steps to expand an input query 
 : 
# QE methods NDCG@1 NDCG@3 NDCG@10 
1 NoQE 34.70 36.50 41.54 
2 TC 33.78 36.57 42.33
 ? 
3 SMT 34.79
 ? 36.98 ?? 42.84 ?? 
4 MRF 36.10 ??? 38.06 ??? 43.71 ??? 
5 MRFum+bm+cm 33.31 36.12 42.26
 ? 
6 MRFtc 34.50
 ? 36.59 42.33 ? 
7 MRFwm 34.73
 ? 36.62 42.73 ?? 
8 MRFtm 35.13
 ?? 37.46 ??? 42.82 ?? 
9 MRFbltm 34.34
 ? 36.19 41.98 ? 
10 MRFwm+tm 35.21
 ??? 37.46 ??? 42.83 ?? 
11 MRFwm+tm+bltm 35.84
 ??? 37.70 ??? 43.14 ??? 
Table 1: Ranking results using BM25 with different 
query expansion systems. The superscripts      and    
indicate statistically significant improvements 
         over NoQE, TC, and SMT, respectively. 
Rows 5 to 11 are different versions of MRF in Row 5, 
They use the same candidate generator but use in the 
ranker different feature classes, as specified by the 
subscript. tc specifies the feature class defined as the 
scoring function in Equation (13). Refer to Equation 
(12) for the names of other feature classes. 
 
 
672
1. Extract all query terms   (eliminating 
stopwords) from  . 
2. Find all documents that have clicks on a 
query that contains one or more of these 
query terms. 
3. For each title term   in these documents, 
calculate its evidence of being selected as 
an expansion term according to the whole 
query via a scoring function        |   . 
4. Select n title terms with the highest score 
(where the value of n is optimized on train-
ing data) and formulate the expanded que-
ry by adding these terms into  . 
5. Use the expanded query to rank documents. 
The scoring function is based on the term correla-
tion model, and is defined as 
       |     (?   |    
   
) (13) 
   |   ?    |     |  
    
  
where    is the set of documents clicked for the 
queries containing the term   and is collected from 
search logs,    |   is a normalized tf-idf weight 
of the document term in  , and    |   is the rela-
tive occurrence of   among all the documents 
clicked for the queries containing  . Table 1 shows 
that TC leads to significant improvement over 
NoQE in NDCG@10, but not in NDCG@1 and 
NDCG@3 (Row 2 vs. Row 1). The result is not 
entirely consistent with what reported in Cui et al
(2003). A possible reason is that Cui et al
formed the evaluation using documents and search 
logs collected from the Encarta website, which is 
much cleaner and more homogenous than the data 
sets we used. The result suggests that although QE 
improves the recall of relevant documents, it is 
also likely to introduce noise that hurts the preci-
sion of document retrieval. 
SMT (Row 3) is a SMT-based QE system. Fol-
lowing Riezler et al is an im-
plementation of a phrase-based SMT system with a 
standard set of features for translation model and 
language model, combined under a log linear mod-
el framework (Koehn et alrom 
Riezler et al translation model 
is trained on query-snippet pairs and the language 
model on queries, in our implementation the trans-
lation model is trained on query-title pairs and the 
language model on titles. To apply the system to 
QE, expansion terms of a query are taken from 
those terms in the 10-best translations of the query 
that have not been seen in the original query string. 
We see that SMT significantly outperforms TC in 
NDCG at all levels. The result confirms the con-
clusion of Riezler et alt context 
information is crucial for improving retrieval pre-
cision by filtering noisy expansions.  
Both TC and SMT, considered as state-of-the-
art QE methods, have been frequently used for 
comparison in related studies. Thus, we also used 
them as baselines in our experiments. 
MRF (Row 4) is the ranker-based QE system 
described in Section 3, which uses a MRF-based 
ranker to incorporate all 8 classes of features de-
rived from a variety of lexicon translation models 
and language models as in Equation (12). Results 
show that the ranker-based QE system significantly 
outperforms both NoQE and the two state-of-the-
art QE methods. The fact that MRF beats SMT 
with a statistically significant margin although the 
former is a much simpler system indicates that text 
translation and QE are different tasks and some 
SMT components, designed for the task of regular 
text translation, are not as effective in selecting 
expansion terms. We will explore this in more de-
tail in the next section. 
4.2 Comparing Models 
The experiments presented in this section investi-
gate in detail the effectiveness of different models, 
e.g., the lexicon models and the language models 
described in Sections 2 and 3, in ranking expansion 
candidates for QE. The results are summarized in 
Rows 5 to 11 in Table 1, where a number of differ-
ent versions of the ranker-based QE system are 
compared. These versions, labeled as MRFf, use 
the same candidate generator, and differ in the fea-
ture classes (which are specified by the subscript f) 
incorporated in the MRF-based ranker. In what 
follows, we focus our discussion on the results of 
the three lexicon models. 
MRFwm (Row 7) uses the word translation 
model described in Section 2.1. Both the word 
model and term correlation model used in MRFtm 
(Row 6) are context independent. They differ 
mainly in the training methods. For the sake of 
comparison, in our experiment the word model is 
673
EM-trained with the correlation model as initial 
point. Rezler et al that statisti-
cal translation model is superior to correlation 
model because the EM training captures the hidden 
alignment information when mapping document 
terms to query terms, leading to a better smoothed 
probability distribution. Our result (Row 7 vs. Row 
6) verifies the hypothesis. Notice that MRFtc out-
performs TC in NDCG@1 (Row 6 vs. Row 2) 
mainly because in the former the expansion candi-
dates are generated by a word translation model 
and are less noisy. 
It is encouraging to observe that the rankers us-
ing the triplet model features achieve the QE per-
formance either in par with or better than that of 
SMT (Rows 8, 10 and 11 vs. Row 3), although the 
latter is a much more sophisticated system. The 
result suggests that not all SMT components are 
useful for QE. For example, language models are 
indispensable for translation but are less effective 
than word models for QE (Row 5 vs. Rows 6 and 
7). We also observe that the triplet model not only 
outperforms significantly the word model due to 
the use of contextual information (Row 8 vs. Row 
7), but also seems to subsume the latter in that 
combining the features derived from both models 
in the ranker leads to little improvement over the 
ranker that uses only the triplet model features 
(Row 10 vs. Row 8).  
The bilingual topic model underperforms the 
word model and the triplet model (Row 9 vs. Rows 
7 and 8). However, we found that the bilingual top-
ic model often selects a different set of expansion 
terms and is complementary to the other two lexi-
con models. As a result, unlike the case of combin-
ing the word model and triplet model features, in-
corporating the bilingual topic model features in 
the ranker leads to some visible improvement in 
NDCG at all positions (Row 11 vs. Row 10). 
To better understand empirically how the MRF-
based QE system achieves the improvement, we 
analyzed the expansions generated by our system 
in detail and obtained several interesting findings. 
First, as expected, in comparison with the word 
model, the triplet translation model is more effec-
tive in benefitting long queries, e.g., notably que-
ries containing questions and queries containing 
song lyrics. Second, unlike the two lexicon models, 
the bilingual topic model tends to generate expan-
sions that are more likely to relate to an entire que-
ry rather than individual query terms. Third, the 
features involving the order of the expansion terms 
benefitted queries containing named entities. 
5 Related Work 
In comparison with log-based methods studied in 
this paper, the QE methods based on automatic 
relevance feedback have been studied much more 
extensively in the information retrieval (IR) com-
munity, and have been proved useful for improving 
IR performance on benchmark datasets such as 
TREC (e.g., Rocchio 1971; Xu and Croft 1996; 
Lavrenko 2001; Zhai and Lafferty 2001). Howev-
er, these methods cannot be applied directly to a 
commercial Web search engine because the rele-
vant documents are not always available and gen-
erating pseudo-relevant documents requires multi-
phase retrieval, which is prohibitively expensive. 
Although automatic relevance feedback is not the 
focus of this study, our method shares a lot of simi-
larities with some of them. For example, similar to 
the way the parameters of our QE ranker are esti-
mated, Cao et alethod of se-
lecting expansion terms to directly optimize aver-
age precision. The MRF model has been previous-
ly used for QE, in the form of relevance feedback 
and pseudo-relevance feedback (Metzler et al
2007; Lang et al MRF models 
use the features derived from IR systems such as 
Indri, we use the SMT-inspired features.  
Using statistical translation models for IR is not 
new (e.g., Berger and Lafferty 1999; Jin et al
Xue et alveness of the statistical 
translation-based approach to Web search has been 
demonstrated empirically in recent studies where 
word-based and phrase-based translation models 
are trained on large amounts of clickthrough data 
(e.g., Gao et alwork extends 
these studies and constructs QE-oriented transla-
tion models that capture more flexible dependen-
cies. 
In addition to QE, search logs have also been 
used for other Web search tasks, such as document 
ranking (Joachims 2002; Agichtein et al
search query processing and spelling correction 
(Huang et alre-
trieval (Craswell and Szummer 2007), and user 
query clustering (Baeza-Yates and Tiberi 2007; 
Wen et al
6 Conclusions 
674
In this paper we extend the previous log-based QE 
methods in two directions. First, we formulate QE 
as the problem of translating a source language of 
queries into a target language of documents, repre-
sented as titles. This allows us to adapt the estab-
lished techniques developed for SMT to QE. Spe-
cially, we propose three lexicon models based on 
terms, lexicalized triplets, and topics, respectively. 
These models are trained on pairs of user queries 
and the titles of clicked documents using EM. Se-
cond, we present a ranker-based QE system, the 
heart of which is a MRF-based ranker in which the 
lexicon models are incorporated as features. We 
perform experiments on the Web search task using 
a real world data set. Results show that the pro-
posed system outperforms significantly other state-
of-the-art QE systems. 
This study is part of a bigger, ongoing project, 
aiming to develop a real-time QE system for Web 
search, where simplicity is the key to the success. 
Thus, what we learned from this study is particu-
larly encouraging. We demonstrate that with large 
amounts of clickthrough data for model training, 
simple lexicon models can achieve state-of-the-art 
QE performance, and that the MRF-based ranker 
provides a simple and flexible framework to incor-
porate a variety of features capturing different 
types of term dependencies in such an effective 
way that the Web search performance can be di-
rectly optimized. 
References  
Agichtein, E., Brill, E., and Dumais, S. 2006. Im-
proving web search ranking by incorporating us-
er behavior information. In SIGIR, pp. 19-26. 
Baeze-Yates, R., and Ribeiro-Neto, B. 2011. Mod-
ern Information Retrieval. Addison-Wesley. 
Baeza-Yates, R. and Tiberi, A. 2007. Extracting 
semantic relations from query logs. In SIGKDD, 
pp. 76-85. 
Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G. 
2005. Query expansion using term relationships 
in language models for information retrieval. In 
CIKM, pp. 688-695. 
Bendersky, M., Metzler, D., and Croft, B. 2010. 
Learning concept importance using a weighted 
dependence model. In WSDM, pp. 31-40.  
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229. 
Bishop, C. M. 2006. Patten recognition and ma-
chine learning. Springer.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet al Machine 
Learning Research, 3: 993-1022. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., 
and Mercer, R. L. 1993. The mathematics of sta-
tistical machine translation: parameter estimation. 
Computational Linguistics, 19(2): 263-311. 
Cao, G., Nie, J-Y., Gao, J., and Robertson, S. 2008. 
Selecting good expansion terms for pseudo-
relevance feedback. In SIGIR, pp. 289-305. 
Craswell, N. and Szummer, M. 2007. Random 
walk on the click graph. In SIGIR. pp. 239-246. 
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2002. 
Probabilistic query expansion using query logs. 
In WWW, pp. 325-332.  
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2003. 
Query expansion by mining user log. IEEE 
Trans on Knowledge and Data Engineering. Vol. 
15, No. 4. pp. 1-11. 
Dempster, A., Laird, N., and Rubin, D. 1977. Max-
imum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical 
Society, 39: 1-38. 
Ganchev, K., Graca, J., Gillenwater, J., and Taskar, 
B. 2010. Posterior regularization for structured 
latent variable models. Journal of Machine 
Learning Research, 11 (2010): 2001-2049. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684. 
Gao, J., He, X., and Nie, J-Y. 2010a. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM, pp. 
1139-1148. 
Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X. 
2010b. A large scale ranker-based system for 
query spelling correction. In COLING, pp. 358-
366. 
675
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR, pp. 355-362. 
Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear 
discriminant model for information retrieval. In 
SIGIR, pp. 290-297. 
Hasan, S., Ganitkevitch, J., Ney, H., and Andres-
Fnerre, J. 2008. Triplet lexicon models for statis-
tical machine translation. In EMNLP, pp. 372-
381. 
Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and 
Behr, F. 2010. Exploring web scale language 
models for search query processing. In WWW, pp. 
451-460. 
Jarvelin, K. and Kekalainen, J. 2000. IR evaluation 
methods for retrieving highly relevant docu-
ments. In SIGIR, pp. 41-48 
Jin, R., Hauptmann, A. G., and Zhai, C. 2002. Title 
language model for information retrieval. In 
SIGIR, pp. 42-48. 
Jing, Y., and Croft., B. 1994. An association 
thesaurus for information retrieval. In RIAO, pp. 
146-160. 
Joachims, T. 2002. Optimizing search engines us-
ing clickthrough data. In SIGKDD, pp. 133-142. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Lang, H., Metzler, D., Wang, B., and Li, J-T. 2010. 
Improving latent concept expansion using 
markov random fields. In CIKM, pp. 249-258. 
Lavrenko, V., and Croft, B. 2001. Relevance-based 
language models. In SIGIR, pp. 120-128. 
Lease, M. 2009. An improved markov random 
field model for supporting verbose queries. In 
SIGIR, pp. 476-483 
Metzler, D., and Croft, B. 2005. A markov random 
field model for term dependencies. In SIGIR, pp. 
472-479. 
Metzler, D., and Croft, B. 2007. Latent concept 
expansion using markov random fields. In 
SIGIR, pp. 311-318. 
Morgan, W., Greiff, W., and Henderson, J.  2004.  
Direct maximization of average precision by 
hill-climbing with a comparison to a maximum 
entropy approach.  Technical report.  MITRE. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Prager, J., Chu-Carroll, J., and Czuba, K. 2001. 
Use of Wordnet hypernyms for answering what 
is questions. In TREC 10. 
Press, W. H., Teukolsky, S. A., Vetterling, W. T., 
and Flannery, B. P. 1992. Numerical Recipes in 
C. Cambridge Univ. Press. 
Rocchio, J. 1971. Relevance feedback in infor-
mation retrieval. In The SMART retrieval system: 
experiments in automatic document processing, 
pp. 313-323, Prentice-Hall Inc. 
Riezler, S., Liu, Y. and Vasserman, A. 2008. 
Translating queries into snippets for improving 
query expansion. In COLING 2008. 737-744. 
Riezler, S., and Liu, Y. 2010. Query rewriting us-
ing monolingual statistical machine translation. 
Computational Linguistics, 36(3): 569-582. 
Wen, J., Nie, J-Y., and Zhang, H. 2002. Query 
clustering using user logs. ACM TOIS, 20(1): 59-
81. 
Xu, J., and Croft, B. 1996. Query expansion using 
local and global document analysis. In SIGIR. 
Xue, X., Jeon, J., Croft, W. B. 2008. Retrieval 
models for Question and answer archives.  In 
SIGIR, pp. 475-482. 
Zhai, C., and Lafferty, J. 2001a. Model-based 
feedback in the kl-divergence retrieval model. In 
CIKM, pp. 403-410. 
Zhai, C., and Lafferty, J. 2001b. A study of 
smoothing methods for language models applied 
to ad hoc information retrieval. In SIGIR, pp. 
334-342. 
676
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2?13,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Modeling Interestingness with Deep Neural Networks 
Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,ppantel,mgamon,xiaohe,deng}@microsoft.com 
 
 
Abstract 
This paper presents a deep semantic simi-
larity model (DSSM), a special type of 
deep neural networks designed for text 
analysis, for recommending target docu-
ments to be of interest to a user based on a 
source document that she is reading. We 
observe, identify, and detect naturally oc-
curring signals of interestingness in click 
transitions on the Web between source and 
target documents, which we collect from 
commercial Web browser logs. The DSSM 
is trained on millions of Web transitions, 
and maps source-target document pairs to 
feature vectors in a latent space in such a 
way that the distance between source doc-
uments and their corresponding interesting 
targets in that space is minimized. The ef-
fectiveness of the DSSM is demonstrated 
using two interestingness tasks: automatic 
highlighting and contextual entity search. 
The results on large-scale, real-world da-
tasets show that the semantics of docu-
ments are important for modeling interest-
ingness and that the DSSM leads to signif-
icant quality improvement on both tasks, 
outperforming not only the classic docu-
ment models that do not use semantics but 
also state-of-the-art topic models. 
1 Introduction 
Tasks of predicting what interests a user based on 
the document she is reading are fundamental to 
many online recommendation systems. A recent 
survey is due to Ricci et al. (2011). In this paper, 
we exploit the use of a deep semantic model for 
two such interestingness tasks in which document 
semantics play a crucial role: automatic highlight-
ing and contextual entity search. 
Automatic Highlighting. In this task we want 
a recommendation system to automatically dis-
cover the entities (e.g., a person, location, organi-
zation etc.) that interest a user when reading a doc-
ument and to highlight the corresponding text 
spans, referred to as keywords afterwards. We 
show in this study that document semantics are 
among the most important factors that influence 
what is perceived as interesting to the user. For 
example, we observe in Web browsing logs that 
when a user reads an article about a movie, she is 
more likely to browse to an article about an actor 
or character than to another movie or the director. 
Contextual entity search. After identifying 
the keywords that represent the entities of interest 
to the user, we also want the system to recommend 
new, interesting documents by searching the Web 
for supplementary information about these enti-
ties. The task is challenging because the same key-
words often refer to different entities, and interest-
ing supplementary information to the highlighted 
entity is highly sensitive to the semantic context. 
For example, ?Paul Simon? can refer to many peo-
ple, such as the singer and the senator. Consider 
an article about the music of Paul Simon and an-
other about his life. Related content about his up-
coming concert tour is much more interesting in 
the first context, while an article about his family 
is more interesting in the second. 
At the heart of these two tasks is the notion of 
interestingness. In this paper, we model and make 
use of this notion of interestingness with a deep 
semantic similarity model (DSSM). The model, 
extending from the deep neural networks shown 
recently to be highly effective for speech recogni-
tion (Hinton et al., 2012; Deng et al., 2013) and 
computer vision (Krizhevsky et al., 2012; Mar-
koff, 2014), is semantic because it maps docu-
ments to feature vectors in a latent semantic space, 
also known as semantic representations. The 
model is deep because it employs a neural net-
work with several hidden layers including a spe-
cial convolutional-pooling structure to identify 
keywords and extract hidden semantic features at 
different levels of abstractions, layer by layer. The 
semantic representation is computed through a 
deep neural network after its training by back-
propagation with respect to an objective tailored 
2
to the respective interestingness tasks. We obtain 
naturally occurring ?interest? signals by observ-
ing Web browser transitions, from a source docu-
ment to a target document, in Web usage logs of a 
commercial browser. Our training data is sampled 
from these transitions. 
The use of the DSSM to model interestingness 
is motivated by the recent success of applying re-
lated deep neural networks to computer vision 
(Krizhevshy et al. 2012; Markoff, 2014), speech 
recognition (Hinton et al. 2012), text processing 
(Collobert et al. 2011),  and Web search (Huang 
et al. 2013). Among them, (Huang et al. 2013) is 
most relevant to our work. They also use a deep 
neural network to map documents to feature vec-
tors in a latent semantic space. However, their 
model is designed to represent the relevance be-
tween queries and documents, which differs from 
the notion of interestingness between documents 
studied in this paper. It is often the case that a user 
is interested in a document because it provides 
supplementary information about the entities or 
concepts she encounters when reading another 
document although the overall contents of the sec-
ond documents is not highly relevant. For exam-
ple, a user may be interested in knowing more 
about the history of University of Washington af-
ter reading the news about President Obama?s 
visit to Seattle. To better model interestingness, 
we extend the model of Huang et al. (2013) in two 
significant aspects. First, while Huang et al. treat 
a document as a bag of words for semantic map-
ping, the DSSM treats a document as a sequence 
of words and tries to discover prominent key-
words. These keywords represent the entities or 
concepts that might interest users, via the convo-
lutional and max-pooling layers which are related 
to the deep models used for computer vision 
(Krizhevsky et al., 2013) and speech recognition 
(Deng  et al., 2013a) but are not used in Huang et 
al.?s model. The DSSM then forms the high-level 
semantic representation of the whole document 
based on these keywords. Second, instead of di-
rectly computing the document relevance score 
using cosine similarity in the learned semantic 
space, as in Huang et al. (2013), we feed the fea-
tures derived from the semantic representations of 
documents to a ranker which is trained in a super-
vised manner. As a result, a document that is not 
highly relevant to another document a user is read-
ing (i.e., the distance between their derived feature 
                                                          
1 We stress here that, although the click signal is available to 
form a dataset and a gold standard ranker (to be described in 
vectors is big) may still have a high score of inter-
estingness because the former provides useful in-
formation about an entity mentioned in the latter. 
Such information and entity are encoded, respec-
tively, by (some subsets of) the semantic features 
in their corresponding documents. In Sections 4 
and 5, we empirically demonstrate that the afore-
mentioned two extensions lead to significant qual-
ity improvements for the two interestingness tasks 
presented in this paper.  
Before giving a formal description of the 
DSSM in Section 3, we formally define the inter-
estingness function, and then introduce our data 
set of naturally occurring interest signals. 
2 The Notion of Interestingness 
Let ?  be the set of all documents. Following 
Gamon et al. (2013), we formally define the inter-
estingness modeling task as learning the mapping 
function: 
	 ?: ? ? ? ? ??	 		
where the function ???, ?? is the quantified degree 
of interest that the user has  in the target document 
? ? ? after or while reading the source document 
? ? ?. 
Our notion of a document is meant in its most 
general form as a string of raw unstructured text. 
That is, the interestingness function should not 
rely on any document structure such as title tags, 
hyperlinks, etc., or Web interaction data. In our 
tasks, documents can be formed either from the 
plain text of a webpage or as a text span in that 
plain text, as will be discussed in Sections 4 and 5. 
2.1 Data 
We can observe many naturally occurring mani-
festations of interestingness on the Web. For ex-
ample, on Twitter, users follow shared links em-
bedded in tweets. Arguably the most frequent sig-
nal, however, occurs in Web browsing events 
where users click from one webpage to another 
via hyperlinks. When a user clicks on a hyperlink, 
it is reasonable to assume that she is interested in 
learning more about the anchor, modulo cases of 
erroneous clicks. Aggregate clicks can therefore 
serve as a proxy for interestingness. That is, for a 
given source document, target documents that at-
tract the most clicks are more interesting than doc-
uments that attract fewer clicks1.  
Section 4), our task is to model interestingness between un-
structured documents, i.e., without access to any document 
structure or Web interaction data. Thus, in our experiments, 
3
We collect a large dataset of user browsing 
events from a commercial Web browser. Specifi-
cally, we sample 18 million occurrences of a user 
click from one Wikipedia page to another during 
a one year period. We restrict our browsing events 
to Wikipedia since its pages tend to contain many 
anchors (79 on average, where on average 42 have 
a unique target URL). Thus, they attract enough 
traffic for us to obtain robust browsing transition 
data2. We group together all transitions originat-
ing from the same page and randomly hold out 
20% of the transitions for our evaluation data 
(EVAL), 20% for training the DSSM described in 
Section 3.2 (TRAIN_1), and the remaining 60% 
for training our task specific rankers described in 
Section 3.3 (TRAIN_2). In our experiments, we 
used different settings for the two interestingness 
tasks. Thus, we postpone the detailed description 
of these datasets and other task-specific datasets 
to Sections 4 and 5. 
3 A Deep Semantic Similarity Model 
(DSSM) 
This section presents the architecture of the 
DSSM, describes the parameter estimation, and 
the way the DSSM is used in our tasks. 
                                                          
we remove all structural information (e.g., hyperlinks and 
XML tags) in our documents, except that in the highlighting 
experiments (Section 4) we use anchor texts to simulate the 
candidate keywords to be highlighted. We then convert each 
3.1 Network Architecture 
The heart of the DSSM is a deep neural network 
with convolutional structure, as shown in Figure 
1. In what follows, we use lower-case bold letters, 
such as ?, to denote column vectors, ???? to de-
note the ??? element of ?, and upper-case letters, 
such as ?, to denote matrices. 
Input Layer ?. It takes two steps to convert a doc-
ument ?, which is a sequence of words, into a vec-
tor representation ? for the input layer of the net-
work: (1) convert each word in ? to a word vector, 
and (2) build ? by concatenating these word vec-
tors. To convert a word ? into a word vector, we 
first represent ? by a one-hot vector using a vo-
cabulary that contains ?  high frequent words 
(? ? 150K in this study). Then, following Huang 
et al. (2013), we map ? to a separate tri-letter vec-
tor. Consider the word ?#dog#?, where # is a word 
boundary symbol. The nonzero elements in its tri-
letter vector are ?#do?, ?dog?, and ?og#?. We then 
form the word vector of ? by concatenating its 
one-hot vector and its tri-letter vector. It is worth 
noting that the tri-letter vector complements the 
one-hot vector representation in two aspects. First, 
different OOV (out of vocabulary) words can be 
represented by tri-letter vectors with few colli-
sions. Second, spelling variations of the same 
word can be mapped to the points that are close to 
each other in the tri-letter space. Although the 
number of unique English words on the Web is 
extremely large, the total number of distinct tri-
letters in English is limited (restricted to the most 
frequent 30K in this study). As a result, incorpo-
rating tri-letter vectors substantially improves the 
representation power of word vectors while keep-
ing their size small.  
To form our input layer ? using word vectors, 
we first identify a text span with a high degree of 
relevance, called focus, in ?  using task-specific 
heuristics (see Sections 4 and 5 respectively). Sec-
ond, we form ? by concatenating each word vec-
tor in the focus and a vector that is the summation 
of all other word vectors, as shown in Figure 1. 
Since the length of the focus is much smaller than 
that of its document, ? is able to capture the con-
textual information (for the words in the focus) 
Web document into plain text, which is white-space to-
kenized and lowercased. Numbers are retained and no stem-
ming is performed. 
2 We utilize the May 3, 2013 English Wikipedia dump con-
sisting of roughly 4.1 million articles from http://dumps.wiki-
media.org. 
Figure 1: Illustration of the network architec-
ture and information flow of the DSSM 
 
4
useful to the corresponding tasks, with a manage-
able vector size. 
Convolutional Layer ? . A convolutional layer 
extracts local features around each word ??	in a 
word sequence of length ?  as follows. We first 
generate a contextual vector ??  by concatenating 
the word vectors of ?? and its surrounding words defined by a window (the window size is set to 3 
in this paper). Then, we generate for each word a 
local feature vector ??  using a tanh  activation 
function and a linear projection matrix ??, which 
is the same across all windows ? in the word se-
quence, as: 
?? ? tanh??????? , where	? ? 1? 1) ?) 
Max-pooling Layer ?. The size of the output ? 
depends on the number of words in the word se-
quence. Local feature vectors have to be com-
bined to obtain a global feature vector, with a 
fixed size independent of the document length, in 
order to apply subsequent standard affine layers. 
We design ? by adopting the max operation over 
each ?time? ? of the sequence of vectors computed 
by (1), which forces the network to retain only the 
most useful, partially invariant local features pro-
duced by the convolutional layer: 
???? ? max???,?,??u????? (2) 
where the max operation is performed for each di-
mension of ? across ? ? 1,? , ? respectively.  
That convolutional and max-pooling layers are 
able to discover prominent keywords of a docu-
ment can be demonstrated using the procedure in 
Figure 2 using a toy example. First, the convolu-
tional layer of (1) generates for each word in a 5-
word document a 4-dimensional local feature vec-
tor, which represents a distribution of four topics. 
For example, the most prominent topic of ?? within its three word context window is the first 
topic, denoted by ???1?, and the most prominent 
topic of ?? is ???3?. Second, we use max-pooling of (2) to form a global feature vector, which rep-
resents the topic distribution of the whole docu-
ment. We see that ??1? and ??3? are two promi-
nent topics. Then, for each prominent topic, we 
trace back to the local feature vector that survives 
max-pooling: 
??1? ? max???,?,?????1?? ? ???1?  
??3? ? max???,?,?????3?? ? ???3?.  
Finally, we label the corresponding words of these 
local feature vectors, ?? and ??, as keywords of the document.  
Figure 3 presents a sample of document snip-
pets and their keywords detected by the DSSM ac-
cording to the procedure elaborated in Figure 2. It 
is interesting to see that many names are identified 
as keywords although the DSSM is not designed 
explicitly for named entity recognition. 
Fully-Connected Layers ?  and ? . The fixed 
sized global feature vector ? of (2) is then fed to 
several standard affine network layers, which are 
stacked and interleaved with nonlinear activation 
functions, to extract highly non-linear features ? 
at the output layer. In our model, shown in Figure 
1, we have: 
? ? tanh?????? (3) 
? ? tanh?????? (4) 
where ?? and ?? are learned linear projection matri-ces. 
3.2 Training the DSSM 
To optimize the parameters of the DSSM of Fig-
ure 1, i.e., ? ? ???,??,???, we use a pair-wise rank loss as objective (Yih et al. 2011). Consider 
a source document ?  and two candidate target 
documents ??	and ??, where ?? is more interesting 
than ??  to a user when reading ?. We construct 
two pairs of documents ??, ??? and ??, ???, where the former is preferred and should have a higher 
u1 u2 u3 u4 u5
w1 w2 w3 w4 w5
2
3
4
1
 
w1 w2 w3 w4 w5
v
2
3
4
1
Figure 2: Toy example of (upper) a 5-word 
document and its local feature vectors ex-
tracted using a convolutional layer, and (bot-
tom) the global feature vector of the document 
generated after max-pooling. 
 
 
5
interestingness score. Let ? be the difference of 
their interestingness scores: ?	? ???, ??? ?
???, ??? , where ?  is the interestingness score, computed as the cosine similarity: 
???, ?? ? sim???, ?? ?
?????
???????? 
(5) 
where ?? and ?? are the feature vectors of ? and ?, respectively, which are generated using the 
DSSM, parameterized by ?. Intuitively, we want 
to learn ? to maximize ?. That is, the DSSM is 
learned to represent documents as points in a hid-
den interestingness space, where the similarity be-
tween a document and its interesting documents is 
maximized.  
We use the following logistic loss over ? , 
which can be shown to upper bound the pairwise 
accuracy: 
???; ?? ? log?1 ? exp?????? (6) 
                                                          
3 In our experiments, we observed better results by sampling 
more negative training examples (e.g., up to 100) although 
this makes the training much slower. An alternative approach 
The loss function in (6) has a shape similar to the 
hinge loss used in SVMs. Because of the use of 
the cosine similarity function, we add a scaling 
factor ? that magnifies ? from [-2, 2] to a larger 
range. Empirically, the value of ? makes no dif-
ference as long as it is large enough. In the exper-
iments, we set ? ? 10. Because the loss function 
is differentiable, optimizing the model parameters 
can be done using gradient-based methods. Due to 
space limitations, we omit the derivation of the 
gradient of the loss function, for which readers are 
referred to related derivations (e.g., Collobert et 
al. 2011; Huang et al. 2013; Shen et al. 2014). 
In our experiments we trained DSSMs using 
mini-batch Stochastic Gradient Descent. Each 
mini-batch consists of 256 source-target docu-
ment pairs. For each source document ?, we ran-
domly select from that batch four target docu-
ments which are not paired with ?  as negative 
training samples3. The DSSM trainer is imple-
mented using a GPU-accelerated linear algebra li-
brary, which is developed on CUDA 5.5. Given 
the training set (TRAIN_1 in Section 2), it takes 
approximately 30 hours to train a DSSM as shown 
in Figure 1, on a Xeon E5-2670 2.60GHz machine 
with one Tesla K20 GPU card. 
In principle, the loss function of (6) can be fur-
ther regularized (e.g. by adding a term of 2? norm) 
to deal with overfitting. However, we did not find 
a clear empirical advantage over the simpler early 
stop approach in a pilot study, hence we adopted 
the latter in the experiments in this paper. Our ap-
proach adjusts the learning rate ?  during the 
course of model training. Starting with ? ? 1.0, 
after each epoch (a pass over the entire training 
data), the learning rate is adjusted as ? ? 0.5 ? ? 
if the loss on validation data (held-out from 
TRAIN_1) is not reduced. The training stops if 
either ?  is smaller than a preset threshold 
(0.0001) or the loss on training data can no longer 
be reduced significantly. In our experiments, the 
DSSM training typically converges within 20 
epochs. 
3.3 Using the DSSM 
We experiment with two ways of using the DSSM 
for the two interestingness tasks. First, we use the 
DSSM as a feature generator. The output layer of 
the DSSM can be seen as a set of semantic fea-
tures, which can be incorporated in a boosted tree 
is to approximate the partition function using Noise Contras-
tive Estimation (Gutmann and Hyvarinen 2010). We leave it 
to future work.  
? the comedy festival formerly known as 
the us comedy arts festival is a comedy 
festival held each year in las vegas 
nevada from its 1985 inception to 2008 
. it was held annually at the wheeler 
opera house and other venues in aspen 
colorado . the primary sponsor of the 
festival was hbo with co-sponsorship by 
caesars palace . the primary venue tbs 
geico insurance twix candy bars and 
smirnoff vodka hbo exited the festival 
business in 2007 and tbs became the pri-
mary sponsor the festival includes 
standup comedy performances appearances 
by the casts of television shows? 
 
? bad samaritans is an american comedy
series produced by walt becker kelly
hayes and ross putman . it premiered on 
netflix on march 31 2013 cast and char-
acters . the show focuses on a community 
service parole group and their parole 
officer brian kubach as jake gibson an 
aspiring professional starcraft player 
who gets sentenced to 2000 hours of com-
munity service for starting a forest 
fire during his breakup with drew prior 
to community service he had no real am-
bition in life other than to be a pro-
fessional gamer and become wealthy 
overnight like mark zuckerberg as in 
life his goal during ? 
Figure 3: A sample of document snippets and 
the keywords (in bold) detected by the DSSM. 
 
 
6
based ranker (Friedman 1999) trained discrimina-
tively on the task-specific data. Given a source-
target document pair ??, ??, the DSSM generates 
600 features (300 from the output layers ?? and ?? 
for each ? and ?, respectively). 
Second, we use the DSSM as a direct imple-
mentation of the interestingness function ?. Re-
call from Section 3.2 that in model training, we 
measure the interestingness score for a document 
pair using the cosine similarity between their cor-
responding feature vectors (?? and ??). Similarly 
at runtime, we define	? ? 	sim???, ?? as (5). 
4 Experiments on Highlighting 
Recall from Section 1 that in this task, a system 
must select ? most interesting keywords in a doc-
ument that a user is reading. To evaluate our mod-
els using the click transition data described in Sec-
tion 2, we simulate the task as follows. We use the 
set of anchors in a source document ? to simulate 
the set of candidate keywords that may be of in-
terest to the user while reading ?, and treat the text 
of a document that is linked by an anchor in ? as a 
target document ?. As shown in Figure 1, to apply 
DSSM to a specific task, we need to define the fo-
cus in source and target documents. In this task, 
the focus in s is defined as the anchor text, and the 
focus in t is defined as the first 10 tokens in t. 
We evaluate the performance of a highlighting 
system against a gold standard interestingness 
function ?? which scores the interestingness of an 
anchor as the number of user clicks on ? from the 
anchor in ? in our data. We consider the ideal se-
lection to then consist of the ?  most interesting 
anchors according to ??. A natural metric for this 
task is Normalized Discounted Cumulative Gain 
(NDCG) (Jarvelin and Kekalainen 2000). 
We evaluate our models on the EVAL dataset 
described in Section 2. We utilize the transition 
distributions in EVAL to create three other test 
sets, following the stratified sampling methodol-
ogy commonly employed in the IR community, 
for the frequently, less frequently, and rarely 
viewed source pages, referred to as HEAD, 
TORSO, and TAIL, respectively. We obtain 
these sets by first sorting the unique source docu-
ments according to their frequency of occurrence 
in EVAL. We then partition the set so that HEAD 
corresponds to all transitions from the source 
pages at the top of the list that account for 20% of 
the transitions in EVAL; TAIL corresponds to the 
transitions at the bottom also accounting for 20% 
of the transitions in EVAL; and TORSO corre-
sponds to the remaining transitions. 
4.1 Main Results 
Table 1 summarizes the results of various models 
over the three test sets using NDCG at truncation 
levels 1, 5, and 10. 
Rows 1 to 3 are simple heuristic baselines. 
RAND selects ?  random anchors, 1stK selects 
the first ? anchors and LastK the last ? anchors.  
The other models in Table 1 are boosted tree 
based rankers trained on TRAIN_2 described in 
Section 2. They vary only in their features. The 
ranker in Row 4 uses Non-Semantic Features 
(NSF) only. These features are derived from the 
 # Models HEAD TORSO TAIL 
   @1 @5 @10 @1 @5 @10 @1 @5 @10 
src
  o
nly
 
1 RAND 0.041 0.062 0.081 0.036 0.076 0.109 0.062 0.195 0.258 
2 1stK 0.010 0.177 0.243 0.072 0.171 0.240 0.091 0.274 0.348 
3 LastK 0.170 0.022 0.027 0.022 0.044 0.062 0.058 0.166 0.219
4 NSF 0.215 0.253 0.295 0.139 0.229 0.282 0.109 0.293 0.365 
5 NSF+WCAT 0.438 0.424 0.463 0.194 0.290 0.346 0.118 0.317 0.386 
6 NSF+JTT 0.220 0.302 0.343 0.141 0.241 0.295 0.111 0.300 0.369 
7 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 0.258 0.313 0.110 0.299 0.372 
8 NSF+DSSM 0.362 0.386 0.421 0.178 0.275 0.330 0.116 0.312 0.382 
src
+ta
r 9 NSF+WCAT 0.505 0.475 0.501 0.224 0.304 0.356 0.129 0.324 0.391 10 NSF+JTT 0.345 0.380 0.418 0.183 0.280 0.332 0.131 0.321 0.390 
11 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 0.274 0.325 0.123 0.311 0.380 
12 NSF+DSSM 0.554 0.524 0.547 0.241 0.317 0.367 0.135 0.329 0.398 
Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO and 
TAIL test sets. Bold indicates statistical significance over all non-shaded results using t-test (? ?
0.05). 
 
 
7
source document s and from user session infor-
mation in the browser log. The document features 
include: position of the anchor in the document, 
frequency of the anchor, and anchor density in the 
paragraph.  
The rankers in Rows 5 to 12 use the NSF and 
the semantic features computed from source and 
target documents of a browsing transition. We 
compare semantic features derived from three dif-
ferent sources. The first feature source comes 
from our DSSMs (DSSM and DSSM_BOW) us-
ing the output layers as feature generators as de-
scribed in Section 3.3. DSSM is the model de-
scribed in Section 3 and DSSM_BOW is the 
model proposed by Huang et al. (2013) where 
documents are view as bag of words (BOW) and 
the convolutional and max-pooling layers are not 
used. The two other sources of semantic features 
are used as a point of comparison to the DSSM. 
One is a generative semantic model (Joint Transi-
tion Topic model, or JTT) (Gamon et al. 2013). 
JTT is an LDA-style model (Blei et al. 2003) that 
is trained jointly on source and target documents 
linked by browsing transitions. JTT generates a 
total of 150 features from its latent variables, 50 
each for the source topic model, the target topic 
model and the transition model. The other seman-
tic model of contrast is a manually defined one, 
which we use to assess the effectiveness of auto-
matically learned models against human model-
ers. To this effect, we use the page categories that 
editors assign in Wikipedia as semantic features 
(WCAT). These features number in the multiple 
thousands. Using features such as WCAT is not a 
viable solution in general since Wikipedia catego-
ries are not available for all documents. As such, 
we use it solely as a point of comparison against 
DSSM and JTT. 
We also distinguish between two types of 
learned rankers: those which draw their features 
only from the source (src only) document and 
those that draw their features from both the source 
and target (src+tar) documents. Although our 
task setting allows access to the content of both 
source and target documents, there are practical 
scenarios where a system should predict what in-
terests the user without looking at the target doc-
ument because the extra step of identifying a suit-
able target document for each candidate concept 
or entity of interest is computationally expensive.  
4.2 Analysis of Results 
As shown in Table 1, NSF+DSSM, which incor-
porates our DSSM, is the overall best performing 
system across test sets. The task is hard as evi-
denced by the weak baseline scores. One reason is 
the large average number of candidates per page. 
On HEAD, we found an average of 170 anchors 
(of which 95 point to a unique target URL). For 
TORSO and TAIL, we found the average number 
of anchors to be 94 (52 unique targets) and 41 (19 
unique targets), respectively. 
Clearly, the semantics of the documents form 
important signals for this task: WCAT, JTT, 
DSSM_BOW, and DSSM all significantly boost 
the performance over NSF alone. There are two 
interesting comparisons to consider: (a) manual 
semantics vs. learned semantics; and (b) deep se-
mantic models vs. generative topic models. On 
(a), we observe somewhat surprisingly that the 
learned DSSM produces features that outperform 
the thousands of features coming from manually 
(editor) assigned Wikipedia category features 
(WCAT), in all but the TAIL where the two per-
form statistically the same. In contrast, features 
from the generative model (JTT) perform worse 
than WCAT across the board except on TAIL 
where JTT and WCAT are statistically tied. On 
(b), we observe that DSSM outperforms a state-
of-the-art generative model (JTT) on HEAD and 
TORSO. On TAIL, they are statistically indistin-
guishable. 
We turn now to inspecting the scenario where 
features are only drawn from the source document 
(Rows 1-8 in Table 1). Again we observe that se-
mantic features significantly boost the perfor-
mance against NSF alone, however they signifi-
cantly deteriorate when compared to using fea-
tures from both source and target documents. In 
this scenario, the manual semantics from WCAT 
outperform all other models, but with a diminish-
ing effect as we move from HEAD through 
TORSO to TAIL. DSSM is the best performing 
learned semantic model. 
Finally, we present the results to justify the two 
modifications we made to extend the model of 
Huang et al. (2013) to the DSSM, as described in 
Section 1. First, we see in Table 1 that 
DSSM_BOW, which has the same network struc-
ture of Huang et al.?s model, is much weaker than 
DSSM, demonstrating the benefits of using con-
volutional and max-pooling layers to extract se-
mantic features for the highlighting task. Second, 
we conduct several experiments by using the co-
sine scores between the output layers of DSSM 
for ? and ? as features (following the procedure in 
Section 3.3 for using the DSSM as a direct imple-
mentation of ?). We found that adding the cosine 
8
features to NSF+DSSM does not lead to any im-
provement. We also combined NSF with solely 
the cosine features from DSSM (i.e., without the 
other semantic features drawn from its output lay-
ers). But we still found no improvement over us-
ing NSF alone. Thus, we conclude that for this 
task it is much more effective to feed the features 
derived from DSSM to a supervised ranker than 
directly computing the interestingness score using 
cosine similarity in the learned semantic space, as 
in Huang et al. (2013). 
5 Experiments on Entity Search 
We construct the evaluation data set for this sec-
ond task by randomly sampling a set of documents 
from a traffic-weighted set of Web documents. In 
a second step, we identify the entity names in each 
document using an in-house named entity recog-
nizer. We issue each entity name as a query to a 
commercial search engine, and retain up to the 
top-100 retrieved documents as candidate target 
documents. We form for each entity a source doc-
ument which consists of the entity text and its sur-
rounding text defined by a 200-word window. We 
define the focus (as in Figure 1) in ? as the entity 
text, and the focus in ? as the first 10 tokens in ?. 
The final evaluation data set contains 10,000 
source documents. On average, each source docu-
ment is associated with 87 target documents. Fi-
nally, the source-target document pairs are labeled 
in terms of interestingness by paid annotators. The 
label is on a 5-level scale, 0 to 4, with 4 meaning 
the target document is the most interesting to the 
source document and 0 meaning the target is of no 
interest. 
We test our models on two scenarios. The first 
is a ranking scenario where ?  interesting docu-
ments are displayed to the user. Here, we select 
the top-? ranked documents according to their in-
terestingness scores. We measure the performance 
via NDCG at truncation levels 1 and 3. The sec-
ond scenario is to display to the user all interesting 
results. In this scenario, we select all target docu-
ments with an interestingness score exceeding a 
predefined threshold. We evaluate this scenario 
using ROC analysis and, specifically, the area un-
der the curve (AUC). 
5.1 Main Results 
The main results are summarized in Table 2. Rows 
1 to 6 are single model results, where each model 
is used as a direct implementation of the interest-
ingness function ?. Rows 7 to 9 are ranker results, 
where ? is defined as a boosted tree based ranker 
that incorporates different sets of features ex-
tracted from source and target documents, includ-
ing the features derived from single models. As in 
the highlighting experiments, all the machine-
learned single models, including the DSSM, are 
trained on TRAIN_1, and all the rankers are 
trained on TRAIN_2. 
5.2 Analysis of Results 
BM25 (Rows 1 and 2 in Table 2) is the classic 
document model (Robertson and Zaragoza 2009). 
It uses the bag-of-words document representation 
and the BM25 term weighting function. In our set-
ting, we define the interestingness score of a doc-
ument pair as the dot product of their BM25-
weighted term vectors. To verify the importance 
of using contextual information, we compare two 
different ways of forming the term vector of a 
source document. The first only uses the entity 
text (Row 1). The second (Row 2) uses both the 
entity text and and its surrounding text in a 200-
word window (i.e., the entire source document). 
Results show that the model using contextual in-
formation is significantly better. Therefore, all the 
other models in this section use both the entity 
texts and their surrounding text. 
WTM (Row 3) is our implementation of the 
word translation model for IR (Berger and Laf-
ferty 1999; Gao et al. 2010). WTM defines the in-
terestingness score as: 
???, ?? ? ? ? ????|???????|?????????? ,  
# Models @1 @3 AUC 
1 BM25 (entity)  0.133 0.195 0.583 
2 BM25 0.142 0.227 0.675 
3 WTM 0.191 0.287 0.678 
4 BLTM 0.214 0.306 0.704 
5 DSSM 0.259* 0.356* 0.711* 
6 DSSM_BOW 0.223 0.322 0.699 
7 Baseline ranker 0.283 0.360 0.723 
8 7 + DSSM(1) 0.301# 0.385# 0.758# 
9 7 + DSSM(600) 0.327## 0.402## 0.782##
Table 2: Contextual entity search task perfor-
mance (NDCG @ K and AUC). * indicates sta-
tistical significance over all non-shaded single 
model results (Rows 1 to 6) using t-test (? ?
0.05). # indicates statistical significance over re-
sults in Row 7. ## indicates statistical signifi-
cance over results in Rows 7 and 8. 
 
 
 
9
where ????|?? is the unigram probability of word 
?? in ?, and ????|??? is the probability of trans-
lating ?? into ??, trained on source-target docu-ment pairs using EM (Brown et al. 1993). The 
translation-based approach allows any pair of 
non-identical but semantically related words to 
have a nonzero matching score. As a result, it sig-
nificantly outperforms BM25. 
BTLM (Row 4) follows the best performing 
bilingual topic model described in Gao et al. 
(2011), which is an extension of PLSA (Hofmann 
1999). The model is trained on source-target doc-
ument pairs using the EM algorithm with a con-
straint enforcing a source document ? and its tar-
get document ? to not only share the same prior 
topic distribution, but to also have similar frac-
tions of words assigned to each topic. BLTM de-
fines the interestingness score between s and t as: 
???, ?? ? ? ? ????|??????|???????? .  
The model assumes the following story of gener-
ating ? from ?. First, for each topic ? a word dis-
tribution ?? is selected from a Dirichlet prior with 
concentration parameter ? . Second, given ? , a 
topic distribution ??  is drawn from a Dirichlet 
prior with parameter ? . Finally, ?  is generated 
word by word. Each word ?? is generated by first 
selecting a topic ?  according to ?? , and then 
drawing a word from ?? . We see that BLTM models interestingness by taking into account the 
semantic topic distribution of the entire docu-
ments. Our results in Table 2 show that BLTM 
outperforms WTM by a significant margin in 
both NDCG and AUC. 
DSSM (Row 5) outperforms all the competing 
single models, including the state-of-the-art topic 
model BLTM. Now, we inspect the difference be-
tween DSSM and BLTM in detail. Although both 
models strive to generate the semantic representa-
tion of a document, they use different modeling 
approaches. BLTM by nature is a generative 
model. The semantic representation in BLTM is a 
distribution of hidden semantic topics. Such a dis-
tribution is learned using Maximum Likelihood 
Estimation in an unsupervised manner, i.e., max-
imizing the log-likelihood of the source-target 
document pairs in the training data. On the other 
hand, DSSM represents documents as points in a 
hidden semantic space using a supervised learning 
method, i.e., paired documents are closer in that 
latent space than unpaired ones. We believe that 
the superior performance of DSSM is largely due 
to the fact that the model parameters are discrimi-
natively trained using an objective that is tailored 
to the interestingness task.  
In addition to the difference in training meth-
ods, DSSM and BLTM also use different model 
structures. BLTM treats a document as a bag of 
words (thus losing some important contextual in-
formation such as word order and inter-word de-
pendencies), and generates semantic representa-
tions of documents using linear projection. 
DSSM, on the other hand, treats text as a sequence 
of words and better captures local and global con-
text, and generates highly non-linear semantic 
features via a deep neural network. To further ver-
ify our analysis, we inspect the results of a variant 
of DSSM, denoted as DSSM_BOW (Row 6), 
where the convolution and max-pooling layers are 
removed. This model treats a document as a bag 
of words, just like BLTM. These results demon-
strate that the effectiveness of DSSM can also be 
attributed to the convolutional architecture in the 
neural network, in addition to being deep and be-
ing discriminative. 
We turn now to discussing the ranker results in 
Rows 7 to 9. The baseline ranker (Row 7) uses 158 
features, including many counts and single model 
scores, such as BM25 and WMT. DSSM (Row 5) 
alone is quite effective, being close in perfor-
mance to the baseline ranker with non-DSSM fea-
tures. Integrating the DSSM score computed in (5) 
as one single feature into the ranker (Row 8) leads 
to a significant improvement over the baseline. 
The best performing combination (Row 9) is ob-
tained by incorporating the DSSM feature vectors 
of source and target documents (i.e., 600 features 
in total) in the ranker. 
We thus conclude that on both tasks, automatic 
highlighting and contextual entity search, features 
drawn from the output layers of our deep semantic 
model result in significant gains after being added 
to a set of non-semantic features, and in compari-
son to other types of semantic models used in the 
past. 
6 Related Work 
In addition to the notion of relevance as described 
in Section 1, related to interestingness is also the 
notion of salience (also called aboutness) (Gamon 
et al. 2013; 2014; Parajpe 2009; Yih et al. 2006). 
Salience is the centrality of a term to the content 
of a document. Although salience and interesting-
ness interact, the two are not the same. For exam-
ple, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average 
user would probably not be interested in learning 
more about Obama while reading that article.  
10
There are many systems that identify popular 
content in the Web or recommend content (e.g., 
Bandari et al. 2012; Lerman and Hogg 2010; 
Szabo and Huberman 2010), which is closely re-
lated to the highlighting task. In contrast to these 
approaches, we strive to predict what term a user 
is likely to be interested in when reading content, 
which may or may not be the same as the most 
popular content that is related to the current docu-
ment. It has empirically been demonstrated in 
Gamon et al. (2013) that popularity is in fact a ra-
ther poor predictor for interestingness. The task of 
contextual entity search, which is formulated as an 
information retrieval problem in this paper, is also 
related to research on entity resolution (Stefanidis 
et al. 2013).  
Latent Semantic Analysis (Deerwester et al. 
1990) is arguably the earliest semantic model de-
signed for IR. Generative topic models widely 
used for IR include PLSA (Hofmann 1990) and 
LDA (Blei et al. 2003). Recently, these models 
have been extended to handle cross-lingual cases, 
where there are pairs of corresponding documents 
in different languages (e.g., Dumais et al. 1997; 
Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). 
By exploiting deep architectures, deep learning 
techniques are able to automatically discover from 
training data the hidden structures and the associ-
ated features at different levels of abstraction use-
ful for a variety of tasks (e.g., Collobert et al. 
2011; Hinton et al. 2012; Socher et al. 2012; 
Krizhevsky et al., 2012; Gao et al. 2014). Hinton 
and Salakhutdinov (2010) propose the most origi-
nal approach based on an unsupervised version of 
the deep neural network to discover the hierar-
chical semantic structure embedded in queries and 
documents. Huang et al. (2013) significantly ex-
tends the approach so that the deep neural network 
can be trained on large-scale query-document 
pairs giving much better performance. The use of 
the convolutional neural network for text pro-
cessing, central to our DSSM, was also described 
in Collobert et al. (2011) and Shen et al. (2014) 
but with very different applications. The DSSM 
described in Section 3 can be viewed as a variant 
of the deep neural network models used in these 
previous studies. 
7 Conclusions 
Modeling interestingness is fundamental to many 
online recommendation systems. We obtain natu-
rally occurring interest signals by observing Web 
browsing transitions where users click from one 
webpage to another. We propose to model this 
?interestingness? with a deep semantic similarity 
model (DSSM), based on deep neural networks 
with special convolutional-pooling structure, 
mapping source-target document pairs to feature 
vectors in a latent semantic space. We train the 
DSSM using browsing transitions between docu-
ments. Finally, we demonstrate the effectiveness 
of our model on two interestingness tasks: auto-
matic highlighting and contextual entity search. 
Our results on large-scale, real-world datasets 
show that the semantics of documents computed 
by the DSSM are important for modeling interest-
ingness and that the new model leads to signifi-
cant improvements on both tasks. DSSM is shown 
to outperform not only the classic document mod-
els that do not use (latent) semantics but also state-
of-the-art topic models that do not have the deep 
and convolutional architecture characterizing the 
DSSM. 
One area of future work is to extend our 
method to model interestingness given an entire 
user session, which consists of a sequence of 
browsing events. We believe that the prior brows-
ing and interaction history recorded in the session 
provides additional signals for predicting interest-
ingness. To capture such signals, our model needs 
to be extended to adequately represent time series 
(e.g., causal relations and consequences of ac-
tions). One potentially effective model for such a 
purpose is based on the architecture of recurrent 
neural networks (e.g., Mikolov et al. 2010; Chen 
and Deng, 2014), which can be incorporated into 
the deep semantic model proposed in this paper. 
Additional Authors 
Yelong Shen (Microsoft Research, One Microsoft 
Way, Redmond, WA 98052, USA, email: 
yeshen@microsoft.com). 
Acknowledgments 
The authors thank Johnson Apacible, Pradeep 
Chilakamarri, Edward Guo, Bernhard Kohlmeier, 
Xiaolong Li, Kevin Powell, Xinying Song and 
Ye-Yi Wang for their guidance and valuable dis-
cussions. We also thank the three anonymous re-
viewers for their comments. 
References 
Bandari, R., Asur, S., and Huberman, B. A. 2012. 
The pulse of news in social media: forecasting 
popularity. In ICWSM. 
11
Bengio, Y., 2009. Learning deep architectures for 
AI. Fundamental Trends in Machine Learning, 
2(1):1?127. 
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet allocation. Journal of Machine 
Learning Research, 3. 
Broder, A., Fontoura, M., Josifovski, V., and 
Riedel, L. 2007. A semantic approach to contex-
tual advertising. In SIGIR. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. 
J., and Mercer, R. L. 1993. The mathematics of 
statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263-
311. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML, pp. 89-96.  
Chen, J. and Deng, L. 2014. A primal-dual method 
for training recurrent neural networks con-
strained by the echo-state property. In ICLR. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P., 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 12. 
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Indexing 
by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
Deng, L., Hinton, G., and Kingsbury, B. 2013. 
New types of deep neural network learning for 
speech recognition and related applications: An 
overview. In ICASSP. 
Deng, L., Abdel-Hamid, O., and Yu, D., 2013a. A 
deep convolutional neural network using heter-
ogeneous pooling for trading acoustic invari-
ance with phonetic confusion. In ICASSP. 
Dumais, S. T., Letsche, T. A., Littman, M. L., and 
Landauer, T. K. 1997. Automatic cross-linguis-
tic information retrieval using latent semantic 
indexing. In AAAI-97 Spring Symposium Series: 
Cross-Language Text and Speech Retrieval. 
Friedman, J. H. 1999. Greedy function approxi-
mation: a gradient boosting machine. Annals of 
Statistics, 29:1189-1232. 
Gamon, M., Mukherjee, A., Pantel, P. 2014. Pre-
dicting interesting things in text. In COLING. 
Gamon, M., Yano, T., Song, X., Apacible, J. and 
Pantel, P. 2013. Identifying salient entities in 
web pages. In CIKM. 
Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM. pp. 
1139-1148. 
Gao, J., He, X., Yih, W-t., and Deng, L. 2014. 
Learning continuous phrase representations for 
translation modeling. In ACL. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR. pp. 675-684.  
Graves, A., Mohamed, A., and Hinton, G. 2013. 
Speech recognition with deep recurrent neural 
networks. In ICASSP. 
Gutmann, M. and Hyvarinen, A. 2010. Noise-con-
trastive estimation: a new estimation principle 
for unnormalized statistical models. In Proc. 
Int. Conf. on Artificial Intelligence and Statis-
tics (AISTATS2010). 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, 29:82-97. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering binary codes for documents by learning 
deep generative models. Topics in Cognitive 
Science, pp. 1-18. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR. pp. 50-57. 
Huang, P., He, X., Gao, J., Deng, L., Acero, A., 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM. 
Jarvelin, K. and Kekalainen, J. 2000. IR evalua-
tion methods for retrieving highly relevant doc-
uments. In SIGIR. pp. 41-48. 
Krizhevsky, A., Sutskever, I. and Hinton, G. 
2012. ImageNet classification with deep convo-
lutional neural networks. In NIPS. 
Lerman, K., and Hogg, T. 2010. Using a model of 
social dynamics to predict popularity of news. 
In WWW. pp. 621-630. 
Markoff, J. 2014. Computer eyesight gets a lot 
more accurate. In New York Times. 
Mikolov, T.. Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In 
INTERSPEECH. pp. 1045-1048. 
Paranjpe, D. 2009. Learning document aboutness 
from implicit user feedback and document 
structure. In CIKM. 
12
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual document representations from 
discriminative projections. In EMNLP. pp. 251-
261. 
Ricci, F., Rokach, L., Shapira, B., and Kantor, P. 
B. (eds) 2011. Recommender System Handbook, 
Springer. 
Robertson, S., and Zaragoza, H. 2009. The proba-
bilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information 
Retrieval, 3(4):333-389. 
Shen, Y., He, X., Gao. J., Deng, L., and Mesnil, G. 
2014. A latent semantic model with convolu-
tional-pooling structure for information re-
trieval. In CIKM. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic compositionality through recursive 
matrix-vector spaces. In EMNLP. 
Stefanidis, K., Efthymiou, V., Herschel, M., and 
Christophides, V. 2013. Entity resolution in the 
web of data.  CIKM?13 Tutorial. 
Szabo, G., and Huberman, B. A. 2010. Predicting 
the popularity of online content. Communica-
tions of the ACM, 53(8). 
Wu, Q., Burges, C.J.C., Svore, K., and Gao, J. 
2009. Adapting boosting for information re-
trieval measures. Journal of Information Re-
trieval, 13(3):254-270. 
Yih, W., Goodman, J., and Carvalho, V. R. 2006. 
Finding advertising keywords on web pages. In 
WWW. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
 
13
Proceedings of NAACL-HLT 2013, pages 450?459,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Training MRF-Based Phrase Translation Models using Gradient Ascent 
 
Jianfeng Gao 
Microsoft Research 
Redmond, WA, USA 
jfgao@microsoft.com 
 
Xiaodong He 
Microsoft Research 
Redmond, WA, USA 
xiaohe@microsoft.com 
 
Abstract 
This paper presents a general, statistical 
framework for modeling phrase translation 
via Markov random fields. The model al-
lows for arbituary features extracted from a 
phrase pair to be incorporated as evidence. 
The parameters of the model are estimated 
using a large-scale discriminative training 
approach that is based on stochastic gradi-
ent ascent and an N-best list based expected 
BLEU as the objective function. The model 
is easy to be incoporated into a standard 
phrase-based statistical machine translation 
system, requiring no code change in the 
runtime engine. Evaluation is performed on 
two Europarl translation tasks, German-
English and French-English. Results show 
that incoporating the Markov random field 
model significantly improves the perfor-
mance of a state-of-the-art phrase-based 
machine translation system, leading to a 
gain of  0.8-1.3 BLEU points. 
1 Introduction 
The phrase translation model, also known as the 
phrase table, is one of the core components of a 
phrase-based statistical machine translation (SMT) 
system. The most common method of constructing 
the phrase table takes a two-phase approach. First, 
the bilingual phrase pairs are extracted heuristical-
ly from an automatically word-aligned training da-
ta. The second phase is parameter estimation, 
where each phrase pair is assigned with some 
scores that are estimated based on counting of 
words or phrases on the same word-aligned train-
ing data. 
There has been a lot of research on improving 
the quality of the phrase table using more princi-
pled methods for phrase extraction (e.g., Lamber 
and Banchs 2005), parameter estimation (e.g., 
Wuebker et al 2010; He and Deng 2012), or both 
(e.g., Marcu and Wong 2002; Denero et al 2006). 
The focus of this paper is on the parameter estima-
tion phase. We revisit the problem of scoring a 
phrase translation pair by developing a new phrase 
translation model based on Markov random fields 
(MRFs) and large-scale discriminative training. 
We strive to address the following three primary 
concerns. 
First of all, instead of parameterizing a phrase 
translation pair using a set of scoring functions that 
are learned independently (e.g., phrase translation 
probabilities and lexical weights) we use a general, 
statistical framework in which arbitrary features 
extracted from a phrase pair can be incorporated to 
model the translation in a unified way. To this end, 
we propose the use of a MRF model.  
Second, because the phrase model has to work 
with other component models in an SMT system in 
order to produce good translations and the quality 
of translation is measured via BLEU score, it is de-
sirable to optimize the parameters of the phrase 
model jointly with other component models with 
respect to an objective function that is closely re-
lated to the evaluation metric under consideration, 
i.e., BLEU in this paper. To this end, we resort to a 
large-scale discriminative training approach, fol-
lowing the pioneering work of Liang et al (2006). 
Although there are established methods of tuning a 
handful of features on small training sets, such as 
the MERT method (Och 2003), the development of 
discriminative training methods for millions of fea-
tures on millions of sentence pairs is still an ongo-
ing area of research. A recent survey is due to 
Koehn (2010). In this paper we show that by using 
stochastic gradient ascent and an N-best list based 
450
expected BLEU as the objective function, large-
scale discriminative training can lead to significant 
improvements. 
The third primary concern is the ease of adop-
tion of the proposed method. To this end, we use a 
simple and well-established learning method, en-
suring that the results can be easily reproduced. 
We also develop the features for the MRF model in 
such a way that the resulting model is of the same 
format as that of a traditional phrase table. Thus, 
the model can be easily incorporated into a stand-
ard phrase-based SMT system, requiring no code 
change in the runtime engine. 
In the rest of the paper, Section 2 presents the 
MRF model for phrase translation. Section 3 de-
scribes the way the model parameters are estimated. 
Section 4 presents the experimental results on two 
Europarl translation tasks. Section 5 reviews pre-
vious work that lays the foundation of this study. 
Section 6 concludes the paper. 
2 Model 
The traditional translation models are directional 
models that are based on conditional probabilities. 
As suggested by the noisy-channel model for SMT 
(Brown et al 1993): 
? = argmax

| = argmax

()| (1) 
The Bayes rule leads us to invert the conditioning 
of translation probability from a foreign (source) 
sentence  to an English (target) translation .  
However, in practice, the implementation of 
state-of-the-art phrase-based SMT systems uses a 
weighted log-linear combination of several models 
?(,,)  including the logarithm of the phrase 
probability (and the lexical weight) in source-to-
target and target-to-source directions (Och and Ney 
2004) 
? = argmax ? 	?(,,)   (2) 
= argmax


(,) 
 
where   in ?(,,)  is a hidden structure that 
best derives  from , called the Viterbi derivation 
afterwards. In phrase-based SMT,  consists of (1) 
the segmentation of the source sentence into 
phrases, (2) the segmentation of the target sentence 
into phrases, and (3) an alignment between the 
source and target phrases. 
In this paper we use Markov random fields 
(MRFs) to model the joint distribution (, ) 
over a source-target translation phrase pair (, ), 
parameterized by . Different from the directional 
translation models, as in Equation (1), the MRF 
model is undirected, which we believe upholds the 
spirit of the use of bi-directional translation proba-
bilities under the log-linear framework. That is, the 
agreement or the compatibility of a phrase pair is 
more effective to score translation quality than a 
directional translation probability which is mod-
eled based on an imagined generative story does. 
2.1 MRF 
MRFs, also known as undirected graphical models, 
are widely used in modeling joint distributions of 
spatial or contextual dependencies of physical phe-
nomena (Bishop 2006). A Markov random field is 
constructed from a graph  . The nodes of the 
graph represent random variables, and edges define 
the independence semantics between the random 
variables. An MRF satisfies the Markov property, 
which states that a node is independent of all of its 
non-neighbors, defined by the clique configura-
tions of . In modeling a phrase translation pair, 
we define two types of nodes, (1) two phrase nodes 
and (2) a set of word nodes, each for a word in the-
se phrases, such as the graph in Figure 1. Let us 
denote a clique by  and the set of variables in that 
clique by ,  . Then, the joint distribution over 
the random variables in  is defined as 
(, ) = 	? (, ;)
() , (3) 
where  = , ? , || ,  = , ? , ||  and ()  is 
the set of cliques in , and each (, ;) is a 
non-negative potential function defined over a 
clique  that measures the compatibility of the var-
iables in ,  is a set of parameters that are used 
within the potential function.   in Equation (3), 
sometimes called the partition function, is a nor-
malization constant and is given by  
 = ? ? ? (, ;)
()   (4) 
= ? ? 
(, ) ,  
which ensures that the distribution (, ) given 
by Equation (3) is correctly normalized. The pres-
451
ence of  is one of the major limitations of MRFs 
because it is generally not feasible to compute due 
to the exponential number of terms in the summa-
tion. However, we notice that   is a global con-
stant which is independent of  and . Therefore, in 
ranking phrase translation hypotheses, as per-
formed by the decoder in SMT systems, we can 
drop   and simply rank each hypothesis by its 
unnormalized joint probability. In our implementa-
tion, we only store in the phrase table for each 
translation pair ,  its unnormalized probability, 
i.e., 
(, ) as defined in Equation (4). 
It is common to define MRF potential functions 
of the exponential form as , ; =
exp (), where  is a real-valued feature 
function over clique  and  is the weight of the 
feature function. In phrase-based SMT systems, the 
sentence-level translation probability from   to  
is decomposed as the product of a set of phrase 
translation probabilities. By dropping the phrase 
segmentation and distortion model components, we 
have  
(|) ? max

(|,) (5) 
(|,) = ? (|)(,)? ,  
where   is the Viterbi derivation. Similarly, the 
joint probability (,) can be decomposed as 
, ? max

(,,) (6) 
,, = ? (, )(,)?   
? ? log, ,?   
? ? ? ()?((,)),?   
= ?  ?Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292?301,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Maximum Expected BLEU Training of Phrase and Lexicon  Translation Models    Xiaodong He Li Deng Microsoft Research Microsoft Research One Microsoft Way, Redmond, WA, USA One Microsoft Way, Redmond, WA, USA xiaohe@microsoft.com  deng@microsoft.com   Abstract This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.  1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al, 2002, 2003, Liang et al, 2006, Blunsom et al, 2008, Chiang et al, 2009, Foster et al 2010, Xiao et al 2011). Och (2003) proposed using a log-linear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric.  While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al, 2003, Brown et al, 1993). Moreover, the 
parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al, 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al, 2006, Chiang et al, 2009). Liang et al (2006) proposed a large set of lexical and Part-of-Speech  features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by picking a pseudo reference. Many such non-desirable heuristics led to moderate gains reported in that work. Chiang et al (2009) improved a syntactic SMT system by adding as many as ten thousand syntactic features, and used Margin Infused Relaxed Algorithm (MIRA) to train the feature weights. However, the number of parameters in common phrase and lexicon translation models is much larger.  In this work, we present a new, highly effective discriminative learning method for phrase and lexicon translation models. The training objective is an expected BLEU score, which is closely linked to translation quality. Further, we apply a Kullback?Leibler (KL) divergence regularization to prevent over-fitting. For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A 
292
similar GT technique has been successfully used in speech recognition (Gopalakrishnan et al, 1991, Povey, 2004, He et al, 2008). Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-to-English dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvement over a state-of-the-art baseline, and the system using the proposed method achieved the best single system translation result in the Chinese-to-English MT track.  2. Related Work One best known approach in discriminative training for SMT is proposed by Och (2003). In that work, multiple features, most of them are derived from generative models, are incorporated into a log-linear model, and the relative weights of them are tuned discriminatively on a small tuning set. However, in practice, this approach only works with a handful of parameters.  More closely related to our work, Liang et al (2006) proposed a large set of lexical and Part-of-Speech features in addition to the phrase translation model. Weights of these features are trained using perceptron on a training set of 67K sentences. In that paper, the authors pointed out that forcing the model to update towards the reference translation could be problematic. This is because the hidden structure such as phrase segmentation and alignment could be abused if the system is forced to produce a reference translation. Therefore, instead of pushing the parameter update towards the reference translation (a.k.a. bold updating), the author proposed a local updating strategy where the model parameters are updated towards a pseudo-reference (i.e., the hypothesis in the n-best list that gives the best BLEU score). Experimental results showed that their approach outperformed a baseline by 0.8 BLEU point when using monotonic decoding, but there was no 
significant gain over a stronger baseline with a full-distortion model. In our work, we use the expectation of BLEU scores as the objective. This avoids the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective.  As another closely related study, Chiang et al (2009) incorporated about ten thousand syntactic features in addition to the baseline features. The feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al, 2008) and lattice-based MERT (Macherey et al, 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al (2010) proposed a method to train the phrase translation model using Expectation-Maximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence. However, as pointed out by Liang et al(2006), the same problem as in the bold updating existed, i.e., forced alignment between a source sentence and its reference translation was tricky, and the proposed alignment was likely to be unreliable. The method presented in this paper is free from this problem. 3. Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each 
293
source phrase to a target phrase, re-order target phrases into target sentence (Koehn et al, 2003).  In decoding, the optimal translation ? given the source sentence F is obtained according to   ? = argmax? ? ? ?  (1) where  ? ? ? = 1? ??? ??log ? ??(?, ?)?  (2)  and ? = ??? ??log ? ??(?, ?)??  is the normalization denominator to ensure that the probabilities sum to one. Note that we define the feature functions {??(?, ?)}  in log domain to simplify the notation in later sections. Feature weights ? =  ? {??} are usually tuned by MERT. Features used in a phrase-based system usually include LM, reordering model, word and phrase counts, and phrase and lexicon translation models. Given the focus of this paper, we review only the phrase and lexicon translation models below.   3.1. Phrase translation model A set of phrase pairs are extracted from word-aligned parallel corpus according to phrase extraction rules (Koehn et al, 2003). Phrase translation probabilities are then computed as relative frequencies of phrases over the training dataset. i.e., the probability of translating a source phrase ? to a target phrase ? is computed by   ? ? ? =  ??(?, ?)?(?)  (3)  where ?(?, ?) is the joint counts of ? and ?, and ?(?) is the marginal counts of ?. In translation, the input sentence is segmented into K phrases, and the source-to-target forward phrase (FP) translation feature is scored as:  ? ? ?, ? = ? ?? ???  (4)  where ?? and ?? are the k-th phrase in E and F, respectively.  The target-to-source (backward) phrase translation model is defined similarly.   
3.2. Lexicon translation model There are several variations in lexicon translation features (Ayan and Dorr 2006, Koehn et al, 2003, Quirk et al, 2005). We use the word translation table from IBM Model 1 (Brown et al, 1993) and compute the sum over all possible word alignments within a phrase pair without normalizing for length (Quirk et al, 2005). The source-to-target forward lexicon (FL) translation feature is:  ? ? ?, ? = ? ??,? ??,????  (5)  where ??,?  is the m-th word of the k-th target phrase ??,  ??,? is the r-th word in the k-th source phrase ?? , and ?(??,?|??,?)  is the probability of translating word ??,? to word ??,?. In IBM model 1, these probabilities are learned via maximizing a joint likelihood between the source and target sentences. The target-to-source (backward) lexicon translation model is defined similarly. 4. Maximum Expected-BLEU Training  4.1. Objective function We denote by ? the set of all the parameters to be optimized, including forward phrase and lexicon translation probabilities and their backward counterparts. For simplification of notation,  ??  is formed as a matrix, where its elements {?? } are probabilities subject to ??? = 1. E.g., each row is a probability distribution.  The utility function over the entire training set is defined as:  ?  ?(?)  ?=  ? ??(??,? , ??|??,? , ??) ????(??, ???)????  ???,?,??  ?(6)  where N is the number of sentences in the training set, ???  is the reference translation of the n-th source sentence ??, and ?? ? ???(??) that denotes the list of translation hypotheses of ??. Since the sentences are independent with each other, the joint posterior can be decomposed: 
?? ??,? , ?? ??,? , ?? =  ? ?? ?? ??????    (7) 
294
and ?? ?? ??  is the posterior defined in (2), the subscript ? indicates that it is computed based on the parameter set ?. ? ?  is proportional (with a factor of N) to the expected sentence BLEU score over the entire training set, i.e., after some algebra,  
?(?)  ?=  ? ??(??|??)????(??, ???)??
?
???  In a phrase-based SMT system, the total number of parameters of phrase and lexicon translation models, which we aim to learn discriminatively, is very large (see Table 1). Therefore, regularization is critical to prevent over-fitting. In this work, we regularize the parameters with KL regularization. KL divergence is commonly used to measure the distance between two probability distributions. For the whole parameter set ? , the KL regularization is defined in this work as the sum of KL divergence over the entire parameter space:  ??(??||?) = ??? log ???????  (8)  where ??  is a constant prior parameter set. In training, we want to improve the utility function while keeping the changes of the parameters from ?? at minimum. Therefore, we design the objective function to be maximized as:  ? ? = log? ? ? ? ? ??(??||?) ? (9)  where the prior model ?? in our approach is the relative-frequency-based phrase translation model and the maximum-likelihood-estimated IBM model 1 (word translation model). ? is a hyper-parameter controlling the degree of regularization.   4.2. Optimization In this section, we derived GT formulas for iteratively updating the parameters so as to optimize objective (9). GT is based on extended Baum-Welch (EBW) algorithm first proposed by Gopalakrishnan et al (1991) and commonly used in speech recognition (e.g., He et al 2008).  4.2.1. Extended Baum-Welch Algorithm Baum-Eagon inequality (Baum and Eagon, 1967) gives the GT formula to iteratively maximize positive-coefficient polynomials of random 
variables that are subject to sum-to-one constants. Baum-Welch algorithm is a model update algorithm for hidden Markov model which uses this GT. Gopalakrishnan et al (1991) extended the algorithm to handle rational function, i.e., a ratio of two polynomials, which is more commonly encountered in discriminative training.  Here we briefly review EBW. Assuming a set of random variables ? = {?? }  that subject to the constraint that ??? = 1 , and assume ?(?)and ?(?) are two positive polynomial functions of ? , a GT of ? for the rational function ? ? = ?(?)?(?)  can be obtained through the following two steps:  i) Construct the auxiliary function:  ? ? = ? ? ? ? ?? ? ?  (10)  where ?? are the values from the previous iteration. Increasing f guarantees an increase of r, i.e., ?? ?  > 0 and ? ? ? ? ?? =  ? ?? ? ? ? ? ? ?? .  ii) Derive GT formula for ? ?    
?? = ???
??(?)??? ???? + ? ? ?????? ??(?)??? ????? + ?  
    (11) 
 where D is a smoothing factor.   4.2.2. GT of Translation Models Now we derive the GTs of translation models for our objective.  Since maximizing ? ?  is equivalent to maximizing ?? ? , we have the following auxiliary function:  ? ? = ?(?)???? ? (??||?)    (12)  After substituting (2) and (7) into (6), and drop optimization irrelevant terms in KL regularization, we have ? ?  in a rational function form:  ? ? = ? ? ? ? ??(?)     (13) where     ? ? = ???? ??, ???????  ???,?,?? , ? ? = ?? ??????  , and ? ? =  
295
???? ??, ??????? ???? ??, ???????  ???,?,??  are all positive polynomials of ?. Therefore, we can follow the two steps of EBW to derive the GT formulas for ?. If we denote by  ??  the probability of translating the source phrase i to the target phrase j. Then, the updating formula is (derivation omitted):  ?? = ?? (??, ?, ?, ?)??? + ? ?? ? ? ??? + ??????? (??, ?, ?, ?)???? + ? ?? ? ? + ??  (14) where ? ? = ?/? ?  and  ?? ??, ?, ?, ? =  ??? ?? ??  ?  ? ???? ??, ??? ??? ??  ? ?(??,? = ?, ??,? = ?)? . In which ?? ??  takes a form similar to (6), but is the expected BLEU score for sentence n using models from the previous iteration. ??,? and ??,? are the k-th phrases of ?? and ??, respectively. The smoothing factor set of  ?? according to the Baum-Eagon inequality is usually far too large for practical use. In practice, one general guide of setting ??  is to make all updated value positive. Similar to (Povey 2004), we set ?? by  ?? = max ?(0,??? (??, ?, ?, ?)???? )   (15)  to ensure the denominator of (15) is positive. Further, we set a low-bound of ??  as max?{? ? ? ??,?,?,???? ??? }  to guarantee the numerator to be positive. We denote by ? ?  the probability of translating the source word i to the target word j. Then following the same derivation, we get the updating formula for forward lexicon translation model:  ? ? = ?? (??, ?, ?, ?)??? + ? ?? ? ? ? ?? + ??? ???? (??, ??, ?, ?)???? + ? ?? ? ? + ??  (16) where ? ? = ?/? ?  and ?? ??, ?, ?, ?  ?=  ??? ?? ??  ?  ? ???? ??, ??? ??? ?? ? ?(??,?,? = ?)?? ? ?, ?,?, ? , and  ?? ?, ?,?, ? = ?(??,?,???)??(??,?,?|??,?,?)? ??(??,?,?|??,?,?)? , in which ??,?,? and ??,?,? are the r-th and m-th word in the k-th phrase of the source sentence ?? and the target hypothesis ??, respectively. Value of ??  is set in a 
way similar to (15). GTs for updating backward phrase and lexicon translation models can be derived in a similar way, and is omitted here.  4.3. Implementation issues   4.3.1. Normalizing ? The posterior ??? ?? ??  in the model updating formula is computed according to (2). In decoding, only the relative values of ? matters. However, the absolute value will affect the posterior distribution, e.g., an overly large absolute value of ? would lead to a very sharp posterior distribution. In order to control the sharpness of the posterior distribution, we normalize ? by its L1 norm:  ?? = ??|??|?   (17)  4.3.2. Computing the sentence BLEU sore The commonly used BLEU-4 score is computed by  ????-?? 4 = BP ? exp 14 log??????    (18)  In the updating formula, we need to compute the sentence-level ???? ??, ??? . Since the matching count may be sparse at the sentence level, we smooth raw precisions of high-order n-grams by:  ?? = #(?-?????? ????????) + ? ? ???#(?-??????) + ?    (19)  where ??? is the prior value of ??, ? is a smoothing factor usually takes a value of 5 and  ??? can be set by ??? = ???? ? ???? ????, for n = 3, 4. ?? and ?? are estimated empirically. Brevity penalty (BP) also plays a key role. Instead of clip it at 1, we use a non-clipped BP, ?? = ?(????), for sentence-level BLEU1. We further scale the reference length, r, by a factor such that the total length of references on the training set equals that of the baseline output2.                                                              1 This is to better approximate corpus-level BLEU, i.e., as discussed in (Chiang, et al, 2008), the per-sentence BP might effectively exceed unity in corpus-level BLEU computation. 2  This is to focus the training on improving BLEU by improving n-gram match instead of by improving BP, e.g., this makes the BP of the baseline output already being perfect. 
296
4.3.3. Training procedure The parameter set ? is optimized on the training set while the feature weights ? are tuned on a small tuning set3. Since ? and ? affect the training of each other, we train them in alternation. I.e., at each iteration, we first fix ? and update ?, then we re-tune ? given the new ?. Due to mismatch between training and tuning data, the training process might not always converge. Therefore, we need a validation set to determine the stop point of training. At the end, ? and ? that give the best score on the validation set are selected and applied to the test set. Fig. 1 gives a summary of the training procedure. Note that step 2 and 4 are parallelize-able across multiple processors.   
 Figure 1. The max expected-BLEU training algorithm. 5. Evaluation In evaluating the proposed method, we use two separate datasets. We first describe the experiments with the Europarl dataset (Koehn 2002), followed by the experiments with the more recent IWSLT-2011 task (Federico et al, 2011).  5.1 Experimental setup in the Europarl task In evaluating the proposed method, we use two separate datasets. First, we conduct experiments on the Europarl German-to-English dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ?  tuning, and the rest for validation. The test set consists of 2000 sentences.                                                              3 Usually, the tuning set matches the test condition better, and therefore is preferable for ? tuning. 
To build the baseline phrase-based SMT system, we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extract the phrase table from the word aligned bilingual texts (Koehn et al, 2003). The maximum phrase length is set to four. Other models used in the baseline system include lexicalized ordering model, word count and phrase count, and a 3-gram LM trained on the English side of the parallel training corpus. Feature weights are tuned by MERT. A fast beam-search phrase-based decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. Details of the phrase and lexicon translation models are given in Table 1. This baseline achieves a BLEU score of 26.22% on the test set. This baseline system is also used to generate a 100-best list of the training corpus during maximum expected BLEU training.       Translation model  # parameters Phrase models (fore. & back.)   9.2 M Lexicon model (IBM-1 src-to-tgt) 12.9 M Lexicon model (IBM-1 tgt-to-src) 11.9 M Table 1. Summary of phrase and lexicon translation models  5.2 Experimental results on the Europarl task During training, we first tune the regularization factor ? based on the performance on the validation set. For simplicity reasons, the tuning of ? makes use of only the phrase translation models.  Table 2 reports the BLEU scores and gains over the baseline given different values of ?. The results highlight the importance of regularization. While ? =  ? ? ?5?10?? gives the best score on the validation set, the gain is shown to be substantially reduced to merely 0.2 BLEU point when ? = 0, i.e., no regularization.  We set the optimal value of ? = 5?10?? in all remaining experiments.   Test on Validation Set ????% ?????% Baseline 26.70 -- ? = 0 (no regularization) 26.91 +0.21 ? =  ? ? ?1?10?? 27.31 +0.61 ? =  ? ? ?5?10?? 27.44 +0.74 ? = 10?10?? 27.27 +0.57 Table 2. Results on degrees of regularizations. BLEU scores are reported on the validation set. ????? denotes the gain over the baseline.  Fixing the optimal regularization factor ?, we then study the relationship between the expected 
1. Build the baseline system, estimate { ?, ? }. 2. Decode N-best list for training corpus using the baseline system, compute ????(??,???). 3. set ?? = ?, ?? = ?. 4. Max expected BLEU training  a. Go through the training set. i. Compute ????(??|??) and ??(??) . ii. Accumulate statistics {?}. b. Update: ?? ?  ?? by one iteration of GT. 5. MERT on the tuning set: ??? ? ?. 6. Test on the validation set using { ?, ? }. 7. Go to step 3 unless training converges or reaches a certain number of iterations. 8. Pick the best { ?, ? } on the validation set.  
297
sentence-level BLEU (Exp. BLEU) score of N-best lists and the corpus-level BLEU score of 1-best translations. The conjectured close relationship between the two is important in justifying our use of the former as the training objective. Fig. 2 shows these two scores on the training set over training iterations. Since the expected BLEU is affected by ? strongly, we fix the value of ? in order to make the expected BLEU comparable across different iterations. From Fig. 2 it is clear that the expected BLEU score correlates strongly with the real BLEU score, justifying its use as our training objective.  
        Figure 2. Expected sentence BLEU and 1-best corpus BLEU on the 751K sentence of training data.  Next, we study the effects of training the phrase translation probabilities and the lexicon translation probabilities according to the GT formulas presented in the preceding section. The break-down results are shown in Table 3. Compared with the baseline, training phrase or lexicon models alone gives a gain of 0.7 and 0.5 BLEU points, respectively, on the test set. For a full training of both phrase and lexicon models, we adopt two learning schedules: update both models together at each iteration (simultaneously), or update them in two stages (two-stage), where the phrase models are trained first until reaching the best score on the validation set and then the lexicon models are trained. Both learning schedules give significant improvements over the baseline and also over training phrase or lexicon models alone. The two-stage training of both models gives the best result of 27.33%, outperforming the baseline by 1.1 BLEU points.  
More detail of the two-stage training is provided in Fig. 3, where BLEU scores in each stage are shown as a function of the GT training iteration. The phrase translation probabilities (PT) are trained alone in the first stage, shown in blue color. After five iterations, the BLEU score on the validation set reaches the peak value, with further iteration giving BLEU score fluctuation. Hence, we perform lexicon model (LEX) training starting from the sixth iteration with the corresponding BLEU scores shown in red color in Fig. 3. The BLEU score is further improved by 0.4 points after additional three iterations of training the lexicon models. In total, nine iterations are performed to complete the two-stage GT training of all phrase and lexicon models.   BLEU (%) validation test Baseline 26.70 26.22 Train phrase models alone 27.44 26.94* Train lexicon models alone 27.36 26.71 Both models: simultaneously  27.65 27.13* Both models: two-stage 27.82 27.33* Table 3. Results on the Europarl German-to-English dataset. The BLEU measures from various settings of maximum expected BLEU training are compared with the baseline, where * denotes that the gain over the baseline is statistically significant with a significance level > 99%, measured by paired bootstrap resampling method proposed by Koehn (2004).              
  Figure 3. BLEU scores on the validation set as a function of the GT training iteration in two-stage training of both the phrase translation models (PT) and the lexicon models (LEX). The BLEU scores on training phrase models are shown in blue, and on training lexicon models in red.  
32%	 ?
33%	 ?
34%	 ?
35%	 ?
36%	 ?
37%	 ?
25%	 ?
26%	 ?
27%	 ?
28%	 ?
29%	 ?
30%	 ?
0	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ?
Exp.	 ?BLEU	 ?
	 ?BLEU	 ?
26.0%	 ?
26.5%	 ?
27.0%	 ?
27.5%	 ?
28.0%	 ?
0	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ? 10	 ?
PT	 ?
LEX	 ?
#iteration 
Expec
ted BL
EU 
BLEU
 
BLEU
 
#iteration 
298
5.3 Experiments on the IWSLT2011 benchmark As the second evaluation task, we apply our new method described in this paper to the 2011 IWSLT Chinese-to-English machine translation benchmark (Federico et al, 2011). The main focus of the IWSLT2011 Evaluation is the translation of TED talks (www.ted.com). These talks are originally given in English. In the Chinese-to-English translation task, we are provided with human translated Chinese text with punctuations inserted. The goal is to match the human transcribed English speech with punctuations.  This is an open-domain spoken language translation task. The training data consist of 110K sentences in the transcripts of the TED talks and their translations, in English and Chinese, respectively. Each sentence consists of 20 words on average. Two development sets are provided, namely, dev2010 and tst2010. They consist of 934 sentences and 1664 sentences, respectively. We use dev2010 for ? tuning and tst2010 for validation. The test set tst2011 consists of 1450 sentences. In our system, a primary phrase table is trained from the 110K TED parallel training data, and a 3-gram LM is trained on the English side of the parallel data. We are also provided additional out-of-domain data for potential usage. From them, we train a secondary 5-gram LM on 115M sentences of supplementary English data, and a secondary phrase table from 500K sentences selected from the supplementary UN corpus by the method proposed by Axelrod et al (2011).  In carrying out the maximum expected BLEU training, we use 100-best list and tune the regularization factor to the optimal value of ? = 1?10?? . We only train the parameters of the primary phrase table. The secondary phrase table and LM are excluded from the training process since the out-of-domain phrase table is less relevant to the TED translation task, and the large LM slows down the N-best generation process significantly.  At the end, we perform one final MERT to tune the relative weights with all features including the secondary phrase table and LM.  The translation results are presented in Table 4. The baseline is a phrase-based system with all features including the secondary phrase table and LM. The new system uses the same features except that the primary phrase table is discriminatively 
trained using maximum expected-BLEU and GT optimization as described earlier in this paper.   The results are obtained using the two-stage training schedule, including six iterations for training phrase translation models and two iterations for training lexicon translation models. The results in Table 4 show that the proposed method leads to an improvement of 1.2 BLEU point over the baseline. This gives the best single system result on this task.  BLEU (%) Validation Test Baseline 11.48 14.68 Max expected  BLEU training 12.39 15.92 Table 4. The translation results on IWSLT 2011 MT_CE task.  6. Summary  The contributions of this work can be summarized as follows. First, we propose a new objective function (Eq. 9) for training of large-scale translation models, including phrase and lexicon models, with more parameters than all previous methods have attempted. The objective function consists of 1) the utility function of expected BLEU score, and 2) the regularization term taking the form of KL divergence in the parameter space. The expected BLEU score is closely linked to translation quality and the regularization is essential when many parameters are trained at scale. The importance of both is verified experimentally with the results presented in this paper.  Second, through non-trivial derivation, we show that the novel objective function of Eq. (9) is amenable to iterative GT updates, where each update is equipped with a closed-form formula.  Third, the new objective function and new optimization technique are successfully applied to two important machine translation tasks, with implementation issues resolved (e.g., training schedule and hyper-parameter tuning, etc.).  The superior results clearly demonstrate the effectiveness of the proposed algorithm. Acknowledgments  The authors are grateful to Chris Quirk, Mei-Yuh Hwang, and Bowen Zhou for the assistance with the MT system and/or for the valuable discussions.  
299
References  Amittai Axelrod, Xiaodong He, Jianfeng Gao. 2011, Domain adaptation via pseudo in-domain data selection. In Proc. of EMNLP, 2011. Necip Fazil Ayan, and Bonnie J. Dorr. Going. 2006. Beyond AER: an extensive analysis of word alignments and their impact on MT. In Proc. of COLING-ACL, 2006. Leonard Baum and J. A. Eagon. 1967. An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology, Bulletin of the American Mathematical Society, Jan. 1967. Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL 2008. Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 1993. David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng, 2008. Decomposability of translation metrics for improved evaluation and efficient algorithms. In Proc. of EMNLP, 2008. David Chiang, Kevin Knight and Weri Wang, 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL-HLT, 2009. Marcello Federico, L. Bentivogli, M. Paul, and S. Stueker. 2011. Overview of the IWSLT 2011 Evaluation Campaign. In Proc. of IWSLT, 2011. George Foster, Cyril Goutte and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation. In Proc. of EMNLP, 2010. P. S. Gopalakrishnan, Dimitri Kanevsky, Arthur Nadas, and David Nahamoo. 1991. An inequality for rational functions with applications to some statistical estimation problems. IEEE Trans. Inform. Theory, 1991. Xiaodong He. 2007. Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation. 
In Proc. of the Second ACL Workshop on Statistical Machine Translation. Xiaodong He, Li Deng, Wu Chou, 2008.  Discriminative learning in sequential pattern recognition. IEEE Signal Processing Magazine, Sept. 2008. Philipp Koehn. 2002. Europarl: A Multilingual Corpus for Evaluation of Machine Translation. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proc. of NAACL. 2003. Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004. Percy Liang, Alexandre Bouchard-Cote, Dan Klein and Ben. Taskar. 2006. An end-to-end discriminative approach to machine translation, In Proc. of COLING-ACL, 2006. Wolfgang Macherey, Franz Josef Och, gnacio Thayer, and Jakob Uskoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proc. of EMNLP 2008. Robert Moore and Chris Quirk. 2007. Faster Beam-Search Decoding for Phrasal Statistical Machine Translation. In Proc. of MT Summit XI. Franz Josef Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation, In Proc. of ACL 2002. Franz Josef Och, 2003, Minimum error rate training in statistical machine translation. In Proc. of ACL 2003. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL 2002. Daniel Povey. 2004. Discriminative Training for large Vocabulary Speech Recognition. Ph.D. dissertation, Cambridge University, Cambridge, UK, 2004. Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proc. of ACL 2005. 
300
Antti-Veikko Rosti, Bing hang, Spyros Matsoukas, and Richard Schard Schwartz. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Proc. of workshop on statistical machine translation 2011.  David A Smith, Jason Eisner. 2006. Minimum risk annealing for training log-linear models, In Proc. of COLING-ACL 2006. Joern Wuebker, Arne Mauser and Hermann Ney. 2010. Training phrase translation models with leaving-one-out, In Proc. of ACL 2010. Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proc. of EMNLP 2008. Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin. 2011. Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training. In Proc. Of  EMNLP 2011.   
301


References
 Amittai Axelrod, Xiaodong He, Jianfeng Gao.
2011, Domain adaptation via pseudo in-domain 
data selection. In Proc. of EMNLP, 2011.
Necip Fazil Ayan, and Bonnie J. Dorr. Going. 
2006. Beyond AER:  an extensive analysis of 
word alignments and their impact on MT. In 
Proc. of COLING-ACL, 2006.
Leonard Baum and J.  A.  Eagon. 1967.  An 
inequality with applications to statistical 
prediction for functions of Markov processes 
and to a model of ecology, Bulletin of the
American Mathematical Society, Jan. 1967.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 
2008. A discriminative latent variable model for 
statistical machine translation. In Proc. of ACL
2008.
Peter F. Brown, Stephen A. Della Pietra, Vincent 
Della J.Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
Parameter estimation. Computational 
Linguistics, 1993.
David Chiang, Steve DeNeefe, Yee Seng Chan, 
and Hwee Tou Ng, 2008. Decomposability of 
translation metrics for improved evaluation and 
efficient algorithms. In Proc. of EMNLP, 2008.
David Chiang, Kevin Knight and Weri Wang, 
2009.  11,001 new features for  statistical 
machine translation. In  Proc. of NAACL-HLT, 
2009.
Marcello Federico, L. Bentivogli, M. Paul, and S. 
Stueker. 2011. Overview of the IWSLT 2011 
Evaluation Campaign. In Proc. of IWSLT, 2011.
George Foster,  Cyril Goutte and Roland Kuhn. 
2010. Discriminative Instance Weighting for 
Domain Adaptation in  Statistical Machine 
Translation. In Proc. of EMNLP, 2010.
P.  S.  Gopalakrishnan, Dimitri Kanevsky, Arthur 
Nadas, and David Nahamoo. 1991. An 
inequality for rational functions with 
applications to some statistical estimation 
problems. IEEE Trans. Inform. Theory, 1991.
Xiaodong He. 2007. Using Word-Dependent 
Transition Models in HMM based Word 
Alignment for Statistical Machine Translation. 
In  Proc. of the Second ACL Workshop on 
Statistical Machine Translation.
Xiaodong He, Li Deng, Wu Chou,  2008.  
Discriminative learning in sequential pattern 
recognition. IEEE Signal Processing Magazine, 
Sept. 2008.
Philipp Koehn. 2002. Europarl: A Multilingual 
Corpus for Evaluation of Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase based 
translation. In Proc. of NAACL. 2003.
Philipp Koehn. 2004. Statistical significance tests 
for machine translation evaluation. In  Proc. of 
EMNLP 2004.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein 
and Ben. Taskar. 2006. An end-to-end 
discriminative approach to machine translation,
In Proc. of COLING-ACL, 2006.
Wolfgang Macherey, Franz Josef Och, gnacio 
Thayer, and Jakob Uskoreit. 2008. Lattice-based 
minimum error rate training for statistical 
machine translation. In Proc. of EMNLP 2008.
Robert Moore and Chris Quirk. 2007. Faster 
Beam-Search Decoding for Phrasal Statistical 
Machine Translation. In Proc. of MT Summit 
XI.
Franz Josef Och and H. Ney. 2002. Discriminative 
training and maximum entropy models for 
statistical machine translation, In Proc. of ACL 
2002.
Franz Josef Och, 2003, Minimum  error rate 
training in statistical machine translation. In 
Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proc. of ACL 2002.
Daniel Povey. 2004. Discriminative Training for 
large Vocabulary Speech Recognition. Ph.D. 
dissertation, Cambridge University, Cambridge, 
UK, 2004.
Chris Quirk, Arul Menezes, and Colin Cherry. 
2005. Dependency treelet translation: 
Syntactically informed phrasal SMT. In Proc. of 
ACL 2005.
Antti-Veikko Rosti, Bing hang, Spyros Matsoukas, 
and Richard Schard Schwartz. 2011. Expected 
BLEU  training for graphs: bbn system 
description for WMT system combination task. 
In  Proc. of  workshop on statistical machine 
translation 2011. 
David A Smith, Jason Eisner. 2006. Minimum risk 
annealing for training log-linear models,  In 
Proc. of COLING-ACL 2006.
Joern Wuebker, Arne Mauser and Hermann Ney.
2010. Training phrase translation models with 
leaving-one-out, In Proc. of ACL 2010.
Roy Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice Minimum 
Bayes-Risk decoding for statistical machine 
translation. In Proc. of EMNLP 2008.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun 
Lin. 2011. Fast Generation of Translation Forest 
for Large-Scale SMT Discriminative
Training. In Proc. Of EMNLP 2011.Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699?709,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Learning Continuous Phrase Representations for                                     
Translation Modeling 
 
Jianfeng Gao    Xiaodong He    Wen-tau Yih    Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,xiaohe,scottyih,deng}@microsoft.com 
 
 
 
Abstract 
This paper tackles the sparsity problem in 
estimating phrase translation probabilities 
by learning continuous phrase representa-
tions, whose distributed nature enables the 
sharing of related phrases in their represen-
tations. A pair of source and target phrases 
are projected into continuous-valued vec-
tor representations in a low-dimensional 
latent space, where their translation score 
is computed by the distance between the 
pair in this new space. The projection is 
performed by a neural network whose 
weights are learned on parallel training 
data. Experimental evaluation has been 
performed on two WMT translation tasks. 
Our best result improves the performance 
of a state-of-the-art phrase-based statistical 
machine translation system trained on 
WMT 2012 French-English data by up to 
1.3 BLEU points. 
1 Introduction 
The phrase translation model, also known as the 
phrase table, is one of the core components of 
phrase-based statistical machine translation (SMT) 
systems. The most common method of construct-
ing the phrase table takes a two-phase approach 
(Koehn et al 2003). First, the bilingual phrase 
pairs are extracted heuristically from an automat-
ically word-aligned training data. The second 
phase, which is the focus of this paper, is parame-
ter estimation where each phrase pair is assigned 
with some scores that are estimated based on 
counting these phrases or their words using the 
same word-aligned training data. 
Phrase-based SMT systems have achieved 
state-of-the-art performance largely due to the fact 
that long phrases, rather than single words, are 
used as translation units so that useful context in-
formation can be captured in selecting translations. 
However, longer phrases occur less often in train-
ing data, leading to a severe data sparseness prob-
lem in parameter estimation. There has been a 
plethora of research reported in the literature on 
improving parameter estimation for the phrase 
translation model (e.g., DeNero et al 2006; 
Wuebker et al 2010; He and Deng 2012; Gao and 
He 2013).  
This paper revisits the problem of scoring a 
phrase translation pair by developing a Continu-
ous-space Phrase Translation Model (CPTM). 
The translation score of a phrase pair in this model 
is computed as follows. First, we represent each 
phrase as a bag-of-words vector, called word vec-
tor henceforth. We then project the word vector, 
in either the source language or the target lan-
guage, into a respective continuous feature vector 
in a common low-dimensional space that is lan-
guage independent. The projection is performed 
by a multi-layer neural network. The projected 
feature vector forms the continuous representa-
tion of a phrase. Finally, the translation score of a 
source-target phrase pair is computed by the dis-
tance between their feature vectors.  
The main motivation behind the CPTM is to 
alleviate the data sparseness problem associated 
with the traditional counting-based methods by 
grouping phrases with a similar meaning across 
different languages. This style of grouping is 
made possible because of the distributed nature of 
the continuous-space representations for phrases. 
No such sharing was possible in the original sym-
bolic space for representing words or phrases.  In 
this model, semantically or grammatically related 
phrases, in both the source and the target lan-
guages, would tend to have similar (close) feature 
vectors in the continuous space, guided by the 
training objective. Since the translation score is a 
smooth function of these feature vectors, a small 
699
change in the features should only lead to a small 
change in the translation score. 
The primary research task in developing the 
CPTM is learning the continuous representation 
of a phrase that is effective for SMT. Motivated 
by recent studies on continuous-space language 
models (e.g., Bengio et al 2003; Mikolov et al 
2011; Schwenk et al, 2012), we use a neural net-
work to project a word vector to a feature vector. 
Ideally, the projection would discover those latent 
features that are useful to differentiate good trans-
lations from bad ones, for a given source phrase. 
However, there is no training data with explicit 
annotation on the quality of phrase translations. 
The phrase translation pairs are hidden in the par-
allel source-target sentence pairs, which are used 
to train the traditional translation models. The 
quality of a phrase translation can only be judged 
implicitly through the translation quality of the 
sentences, as measured by BLEU, which contain 
the phrase pair. In order to overcome this chal-
lenge and let the BLEU metric guide the projec-
tion learning, we propose a new method to learn 
the parameters of a neural network. This new 
method, via the choice of an appropriate objective 
function in training, automatically forces the fea-
ture vector of a source phrase to be closer to the 
feature vectors of its candidate translations. As a 
result, the BLEU score is improved when these 
translations are selected by an SMT decoder to 
produce final, sentence-level translations. The 
new learning method makes use of the L-BFGS 
algorithm and the expected BLEU as the objective 
function defined on N-best lists. 
To the best of our knowledge, the CPTM pro-
posed in this paper is the first continuous-space 
phrase translation model that makes use of joint 
representations of a phrase in the source language 
and its translation in the target language (to be de-
tailed in Section 4) and that is shown to lead to 
significant improvement over a standard phrase-
based SMT system (to be detailed in Section 6).  
Like the traditional phrase translation model, 
the translation score of each bilingual phrase pair 
is modeled explicitly in our model. However, in-
stead of estimating the phrase translation score on 
aligned parallel data, our model intends to capture 
the grammatical and semantic similarity between 
a source phrase and its paired target phrase by pro-
jecting them into a common, continuous space 
that is language independent. 
                                                          
1 Niehues et al (2011) use different translation units in order 
to integrate the n-gram translation model into the phrase-
based approach. However, it is not clear how a continuous 
The rest of the paper is organized as follows. 
Section 2 reviews previous work. Section 3 re-
views the log-linear model for phrase-based SMT 
and Sections 4 presents the CPTM. Section 5 de-
scribes the way the model parameters are esti-
mated, followed by the experimental results in 
Section 6. Finally, Section 7 concludes the paper. 
2 Related Work 
Representations of words or documents as contin-
uous vectors have a long history. Most of the ear-
lier latent semantic models for learning such vec-
tors are designed for information retrieval 
(Deerwester et al 1990; Hofmann 1999; Blei et al 
2003). In contrast, recent work on continuous 
space language models, which estimate the prob-
ability of a word sequence in a continuous space 
(Bengio et al 2003; Mikolov et al 2010), have ad-
vanced the state of the art in language modeling, 
outperforming the traditional n-gram model on 
speech recognition (Mikolov et al 2012; Sunder-
meyer et al 2013) and machine translation 
(Mikolov 2012; Auli et al 2013). 
Because these models are developed for mono-
lingual settings, word embedding from these mod-
els is not directly applicable to translation. As a 
result, variants of such models for cross-lingual 
scenarios have been proposed so that words in dif-
ferent languages are projected into the shared la-
tent vector space (Dumais et al 1997; Platt et al 
2010; Vinokourov et al 2002; Yih et al 2011; 
Gao et al 2011; Huang et al 2013; Zou et al 
2013). In principle, a phrase table can be derived 
using any of these cross-lingual models, although 
decoupling the derivation from the SMT training 
often results in suboptimal performance (e.g., 
measured in BLEU), as we will show in Section 6. 
Recently, there is growing interest in applying 
continuous-space models for translation. The 
most related to this study is the work of continu-
ous space n-gram translation models (Schwenk et 
al. 2007; Schwenk 2012; Son et al 2012), where 
the feed-forward neural network language model 
is extended to represent translation probabilities. 
However, these earlier studies focused on the n-
gram translation models, where the translation 
probability of a phrase or a sentence is decom-
posed as a product of n-gram probabilities as in a 
standard n-gram language model. Therefore, it is 
not clear how their approaches can be applied to 
the phrase translation model1, which is much more 
version of such a model can be trained efficiently because the 
factor models used by Son et al cannot be applied directly. 
700
widely used in modern SMT systems. In contrast, 
our model learns jointly the representations of a 
phrase in the source language as well as its trans-
lation in the target language. The recurrent contin-
uous translation models proposed by Kalchbren-
ner and Blunsom (2013) also adopt the recurrent 
language model (Mikolov et al 2010). But unlike 
the n-gram translation models above, they make 
no Markov assumptions about the dependency of 
the words in the target sentence. Continuous space 
models have also been used for generating trans-
lations for new words (Mikolov et al 2013a) and 
ITG reordering (Li et al 2013). 
There has been a lot of research on improving 
the phrase table in phrase-based SMT (Marcu and 
Wong 2002; Lamber and Banchs 2005; Denero et 
al. 2006; Wuebker et al 2010; Zhang et al, 2011; 
He and Deng 2012; Gao and He 2013). Among 
them, (Gao and He 2013) is most relevant to the 
work described in this paper. They estimate 
phrase translation probabilities using a discrimi-
native training method under the N-best reranking 
framework of SMT. In this study we use the same 
objective function to learn the continuous repre-
sentations of phrases, integrating the strengths as-
sociated with these earlier studies. 
3 The Log-Linear Model for SMT 
Phrase-based SMT is based on a log-linear model 
which requires learning a mapping between input 
? ? ? to output ? ? ?. We are given 
? Training samples (?? , ??)  for ? = 1??,  
where each source sentence ?? is paired with 
a reference translation in target language ??; 
? A procedure GEN to generate a list of N-best 
candidates GEN(??) for an input ?? , where 
GEN  in this study is the baseline phrase-
based SMT system, i.e., an in-house 
implementation of the Moses system (Koehn 
et al 2007) that does not use the CPTM, and 
each ? ? GEN(??)  is labeled by the 
sentence-level BLEU score (He and Deng 
2012), denoted by sBleu(??, ?) , which 
measures the quality of ? with respect to its 
reference translation ??; 
? A vector of features ? ? ?? that maps each 
(??, ?) to a vector of feature values
2; and 
? A parameter vector ? ? ??, which assigns a 
real-valued weight to each feature. 
                                                          
2 Our baseline system uses a set of standard features sug-
gested in Koehn et al (2007), which is also detailed in Sec-
tion 6. 
The components GEN(. ), ? and ?  define a log-
linear model that maps ?? to an output sentence as 
follows: 
?? = argmax
(?,?)?GEN(??)
?T?(??, ?, ?) (1) 
which states that given ? and ?, argmax returns 
the highest scoring translation ??,  maximizing 
over  correspondences ?. In phrase-based SMT, ? 
consists of a segmentation of the source and target 
sentences into phrases and an alignment between 
source and target phrases. Since computing the 
argmax  exactly is intractable, it is commonly 
performed approximatedly by beam search (Och 
and Ney 2004). Following Liang et al (2006), we 
assume that every translation candidate is always 
coupled with a corresponding ?, called the Viterbi 
derivation, generated by (1). 
4 A Continuous-Space Phrase Transla-
tion Model (CPTM) 
The architecture of the CPTM is shown in Figures 
1 and 2, where for each pair of source and target 
phrases (??, ??)  in a source-target sentence pair, 
we first project them into feature vectors ??? and 
??? in a latent, continuous space via a neural net-
work with one hidden layer (as shown in Figure 
2), and then compute the translation score, 
score(??, ??), by the distance of their feature vec-
tors in that space. 
We start with a bag-of-words representation of 
a phrase ? ? ??, where ? is a word vector and ? 
is the size of the vocabulary consisting of words 
in both source and target languages, which is set 
to 200K in our experiments. We then learn to pro-
ject ? to a low-dimensional continuous space ??: 
?(?): ?? ? ??  
The projection is performed using a fully con-
nected neural network with one hidden layer and 
tanh activation functions. Let ?1 be the projec-
tion matrix from the input layer to the hidden layer 
and ?2  the projection matrix from the hidden 
layer to the output layer, we have 
? ? ?(?) = tanh (?2
T(tanh(?1
T?))) (2) 
 
 
701
 
Figure 2. A neural network model for phrases 
giving rise to their continuous representations. 
The model with the same form is used for both 
source and target languages. 
 
  
The translation score of a source phrase f and a 
target phrase e can be measured as the similarity   
(or distance) between their feature vectors. We 
choose the dot product as the similarity function3: 
score(?, ?) ? sim?(?? , ??) = ??
T?? (3) 
According to (2), we see that the value of the scor-
ing function is determined by the projection ma-
trices ? = {?1,?2}. 
The CPTM of (2) and (3) can be incorporated 
into the log-linear model for SMT (1) by 
                                                          
3 In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for 
nonlinear multi-layer neural networks, and the latter works 
better for linear neural networks. For the sake of clarity, we 
choose dot product when we describe the CPTM and its train-
ing in Sections 4 and 5, respectively. 
4 The baseline SMT needs to be reasonably good in the 
sense that the oracle BLEU score on the generated n-best 
introducing a new feature ??+1  and a new feature 
weight ??+1. The new feature is defined as 
??+1(??, ?, ?) = ? sim?(?? , ??)(?,? )??   (4) 
Thus, the phrase-based SMT system, into which 
the CPTM is incorporated, is parameterized by 
(?, ?), where ? is a vector of a handful of param-
eters used in the log-linear model of (1), with one 
weight for each feature; and ? is the projection 
matrices used in the CPTM defined by (2) and (3). 
In our experiments we take three steps to learn 
(?, ?): 
1. We use a baseline phrase-based SMT sys-
tem to generate for each source sentence in 
training data an N-best list of translation hy-
potheses4. 
2. We set ? to that of the baseline system and 
let ??+1 = 1, and optimize ? w.r.t. a loss 
function on training data5. 
3. We fix ? , and optimize ?  using MERT 
(Och 2003) to maximize BLEU on dev data. 
In the next section, we will describe Step 2 in de-
tail as it is directly related to the CPTM training. 
 
lists needs to be significantly higher than that of the top-1 
translations so that the CPTM can be effectively trained. 
5 The initial value of ??+1 can also be tuned using the dev 
set. However, we find in a pilot study that it is good enough 
to set it to 1 when the values of all the baseline feature 
weights, used in the log-linear model of (1), are properly nor-
malized, such as by setting ?? = ??/?  for ? = 1?? , 
where ? is the unnormalized weight value of the target lan-
guage model. 
 
Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous repre-
sentation is shown in Figure 2. 
 
 
200K (d) 
100 
100 (?) 
(?1???) 
Word vector 
Neural network 
Feature vector 
?1 
?2 
? 
? 
Raw phrase  ? or ? 
  ?(the process of)         (machine translation)     
(consists of). . . 
 ?(le processus de)    
(traduction automatique)  (consiste en). . .  
???1 ?? ??+1 
???1 ?? ??+1 
??? 
 
 
??? 
 
??(?)
score(??, ??) = ???
T ???  
 
Target phrases  
Continuous representations of  
target phrases  
Source phrases  
 
Continuous representations of  
source phrases  
Translation score as dot product of 
feature vectors in the continuous space 
702
5 Training CPTM 
This section describes the loss function we em-
ploy with the CPTM and the algorithm to train the 
neural network weights. 
We define the loss function ?(?) as the nega-
tive of the N-best list based expected BLEU, de-
noted by xBleu(?). In the reranking framework of 
SMT outlined in Section 3, xBleu(?) over one 
training sample (??, ??) is defined as 
xBleu(?) = ? ?(?|??)sBleu(??, ?)??GEN(??)  (5) 
where sBleu(??, ?)  is the sentence-level BLEU 
score, and  ?(?|??) is the translation probability 
from ?? to ? computed using softmax as  
?(?|??) =
exp(??T?(??,?,?))
? exp(??T?(??,??,?))???GEN(??)
  (6) 
where ?T? is the log-linear model of (1), which 
also includes the feature derived from the CPTM 
as defined by (4), and ? is a tuned smoothing fac-
tor. 
Let ?(?) be a loss function which is differen-
tiable w.r.t. the parameters of the CPTM, ?. We 
can compute the gradient of the loss and learn ? 
using gradient-based numerical optimization al-
gorithms, such as L-BFGS or stochastic gradient 
descent (SGD).  
5.1 Computing the Gradient 
Since the loss does not explicitly depend on ?, we 
use the chain rule for differentiation: 
??(?)
??
= ?
??(?)
?sim?(?? , ??)
?sim?(?? , ??)
??
(?,? )
 
= ? ??(?,?)
?sim?(?? , ??)
??
(?,? )
 (7) 
which takes the form of summation over all phrase 
pairs occurring either in a training sample (sto-
chastic mode) or in the entire training data (batch 
mode). ?(?,?) in (7) is known as the error term of 
the phrase pair (?, ?), and is defined as   
?(?,?) = ?
??(?)
?sim?(??,??)
  (8) 
It describes how the overall loss changes with the 
translation score of the phrase pair (?, ?). We will 
leave the derivation of ?(?,?) to Section 5.1.2, and 
will first describe how the gradient of 
sim?(?? , ??) w.r.t. ? is computed. 
5.1.1 Computing ?????(??, ??)/?? 
Without loss of generality, we use the following 
notations to describe a neural network: 
? ?? is the projection matrix for the l-th layer 
of the neural network; 
? ? is the input word vector of a phrase; 
? ?? is the sum vector of the l-th layer; and  
? ?? = ?(??) is the output vector of the l-th 
layer, where ? is an activation function; 
Thus, the CPTM defined by (2) and (3) can be rep-
resented as  
?1 = ?1
T?  
?1 = ?(?1)  
?2 = ?2
T?1  
?2 = ?(?2)  
sim?(?? , ??) = (??
2)
T
??
2  
The gradient of the matrix ?2 which projects the 
hidden vector to the output vector is computed as: 
?sim?(?? , ??)
??2
=
?(??
2)
T
??2
??
2 + (??
2)
T ???
2
??2
 
= ??
1 (??
2 ? ??(??
2))
T
+ ??
1 (??
2 ? ??(??
2))
T
 (9) 
where ? is the element-wise multiplication (Hada-
mard product). Applying the back propagation 
principle, the gradient of the projection matrix 
mapping the input vector to the hidden vector ?1 
is computed as 
?sim?(?? , ??)
??1
 
= ?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  
+?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  (10) 
The derivation can be easily extended to a neural 
network with multiple hidden layers.  
5.1.2 Computing ?(?,?) 
To simplify the notation, we rewrite our loss func-
tion of (5) and (6) over one training sample as  
703
?(?) = ?xBleu(?) = ?
G(?)
Z(?)
 (11) 
where 
G(?) = ? sBleu(?, ??) exp(?
T?(??, ?, ?))?   
Z(?) = ? exp(?T?(??, ?, ?))?   
Combining (8) and (11), we have 
?(?,?) =
?xBleu(?)
?sim?(?? , ??)
 (12) 
=
1
Z(?)
(
?G(?)
?sim?(?? , ??)
?
?Z(?)
?sim?(?? , ??)
xBleu(?)) 
Because ? is only relevant to ??+1 which is de-
fined in (4), we have 
??T?(??, ?, ?)
?sim?(?? , ??)
= ??+1
???+1(??, ?, ?)
?sim?(?? , ??)
  
= ??+1?(?, ?; ?) (13) 
where ?(?, ?; ?)  is the number of times the 
phrase pair (?, ?)  occurs in ? . Combining (12) 
and (13), we end up with the following equation 
?(?,?)
= ? U(?,?)?(?|??)??+1?(?, ?; ?)
(?,?)????(??)
 
where  (14) 
U(?, ?) = sBleu(??, ?) ? xBleu(?).  
5.2 The Training Algorithm 
In our experiments we train the parameters of the 
CPTM, ?, using the L-BFGS optimizer described 
in Andrew and Gao (2007), together with the loss 
function described in (5). The gradient is com-
puted as described in Sections 5.1. Although SGD 
has been advocated for neural network training 
due to its simplicity and its robustness to local 
minima (Bengio 2009), we find that in our task 
that the L-BFGS minimizes the loss in a desirable 
fashion empirically when iterating over the com-
plete training data (batch mode). For example, the 
convergence of the algorithm was found to be 
smooth, despite the non-convexity in our loss. An-
other merit of batch training is that the gradient 
over all training data can be computed efficiently. 
As shown in Section 5.1, computing 
?sim?(x? , x?)/??  requires large-scale matrix 
multiplications, and is expensive for multi-layer 
neural networks. Eq. (7) suggests that 
?sim?(x? , x?)/??  and ?(?,?)  can be computed 
separately, thus making the computation cost of 
the former term only depends on the number of 
phrase pairs in the phrase table, but not the size of 
training data. Therefore, the training method de-
scribed here can be used on larger amounts of 
training data with little difficulty.  
As described in Section 4, we take three steps 
to learn the parameters for both the log-linear 
model of SMT and the CPTM. While steps 1 and 
3 can be easily parallelized on a computer cluster, 
the CPTM training is performed on a single ma-
chine. For example, given a phrase table contain-
ing 16M pairs and a 1M-sentence training set, it 
takes a couple of hours to generate the N-best lists 
on a cluster, and about 10 hours to train the CPTM 
on a Xeon E5-2670 2.60GHz machine.   
For a non-convex problem, model initialization 
is important. In our experiments we always initial-
ize ?1 using a bilingual topic model trained on 
parallel data (see detail in Section 6.2), and ?2 as 
an identity matrix. In principle, the loss function 
of (5) can be further regularized (e.g. by adding a 
term of ?2 norm) to deal with overfitting. How-
ever, we did not find clear empirical advantage 
over the simpler early stop approach in a pilot 
study, which is adopted in the experiments in this 
paper.   
6 Experiments 
This section evaluates the CPTM presented on 
two translation tasks using WMT data sets. We 
first describe the data sets and baseline setup. 
Then we present experiments where we compare 
different versions of the CPTM and previous 
models. 
6.1 Experimental Setup 
Baseline. We experiment with an in-house 
phrase-based system similar to Moses (Koehn et 
al. 2007), where the translation candidates are 
scored by a set of common features including 
maximum likelihood estimates of source given 
target phrase mappings ????(?|?) and vice versa 
????(?|?), as well as lexical weighting estimates 
???(?|?) and ???(?|?), word and phrase penal-
ties, a linear distortion feature, and a lexicalized 
reordering feature. The baseline includes a stand-
ard 5-gram modified Kneser-Ney language model 
trained on the target side of the parallel corpora 
described below. Log-linear weights are estimated 
with the MERT algorithm (Och 2003). 
704
Evaluation. We test our models on two different 
data sets. First, we train an English to French sys-
tem based on the data of WMT 2006 shared task 
(Koehn and Monz 2006). The parallel corpus in-
cludes 688K sentence pairs of parliamentary pro-
ceedings for training. The development set con-
tains 2000 sentences, and the test set contains 
other 2000 sentences, all from the official WMT 
2006 shared task. 
Second, we experiment with a French to Eng-
lish system developed using 2.1M sentence pairs 
of training data, which amounts to 102M words, 
from the WMT 2012 campaign. The majority of 
the training data set is parliamentary proceedings 
except for 5M words which are newswire. We use 
the 2009 newswire data set, comprising 2525 sen-
tences, as the development set. We evaluate on 
four newswire domain test sets from 2008, 2010 
and 2011 as well as the 2010 system combination 
test set, containing 2034 to 3003 sentences. 
In this study we perform a detailed empirical 
comparison using the WMT 2006 data set, and 
verify our best models and results using the larger 
WMT 2012 data set. 
The metric used for evaluation is case insensi-
tive BLEU score (Papineni et al 2002). We also 
perform a significance test using the Wilcoxon 
signed rank test. Differences are considered statis-
tically significant when the p-value is less than 
0.05. 
6.2 Results of the CPTM 
Table 1 shows the results measured in BLEU eval-
uated on the WMT 2006 data set, where Row 1 is 
the baseline system. Rows 2 to 4 are the systems 
enhanced by integrating different versions of the 
CPTM. Rows 5 to 7 present the results of previous 
models. Row 8 is our best system. Table 2 shows 
the main results on the WMT 2012 data set. 
CPTM is the model described in Sections 4. 
As illustrated in Figure 2, the number of the nodes 
in the input layer is the vocabulary size ?. Both 
the hidden layer and the output layer have 100 
nodes6. That is, ?1 is a ? ? 100 matrix and ?2 
a 100 ? 100  matrix. The result shows that 
CPTM leads to a substantial improvement over 
the baseline system with a statistically significant 
margin of 1.0 BLEU points as in Table 1.  
We have developed a set of variants of CPTM 
to investigate two design choices we made in de-
veloping the CPTM: (1) whether to use a linear 
                                                          
6 We can achieve slightly better results using more nodes in 
the hidden and output layers, say 500 nodes. But the model 
projection or a multi-layer nonlinear projection; 
and (2) whether to compute the phrase similarity 
using word-word similarities as suggested by e.g., 
the lexical weighting model (Koehn et al 2003). 
We compare these variants on the WMT 2006 
data set, as shown in Table 1. 
CPTML (Row 3 in Table 1) uses a linear neural 
network to project a word vector of a phrase ? to 
a feature vector ?: ? ? ?(?) = ?T?, where ? is 
a ? ? 100  projection matrix. The translation 
score of a source phrase f and a target phrase e is 
measured as the similarity of their feature vectors. 
We choose cosine similarity because it works bet-
ter than dot product for linear projection. 
CPTMW (Row 4 in Table 1) computes the phrase 
similarity using word-word similarity scores. This 
follows the common smoothing strategy of ad-
dressing the data sparseness problem in modeling 
phrase translations, such as the lexical weighting 
model (Koehn et al 2003) and the word factored 
n-gram translation model (Son et al 2012). Let ? 
denote a word, and ? and ? the source and target 
phrases, respectively. We define 
sim(?, ?) =
1
|?|
? sim?(?, ?) +???
1
|?|
? sim?(?, ?)???   
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
sim?(?, ?)
=
? sim(?,??) exp(?sim(?,??))????
? exp(?sim(?,??))????
 
 
training is too slow to perform a detailed study within a rea-
sonable time. Therefore, all the models reported in this paper 
use 100 nodes.   
# Systems WMT test2006 
1 Baseline 33.06 
2 CPTM 34.10? 
3 CPTML 33.60
?? 
4 CPTMW 33.25
? 
5 BLTMPR 33.15
? 
6 DPM 33.29? 
7 MRFP 33.91
? 
8 Comb (2 + 7) 34.39?? 
Table 1: BLEU results for the English to French 
task using translation models and systems built 
on the WMT 2006 data set. The superscripts ? 
and ? indicate statistically significant difference 
(p < 0.05) from Baseline and CPTM, respec-
tively. 
 
705
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
where ? is the tuned smoothing parameter.  
Similar to CPTM, CPTMW also uses a nonlin-
ear projection to map each word (not a phrase vec-
tor as in CPTM) to a feature vector. 
Two observations can be made by comparing 
CPTM in Row 2 to its variants in Table 1. First of 
all, it is more effective to model the phrase trans-
lation directly than decomposing it into word-
word translations in the CPTMs. Second, we see 
that the nonlinear projection is able to generate 
more effective features, leading to better results 
than the linear projection. 
We  also compare the best version of the CPTM 
i.e., CPTM, with three related models proposed 
previously. We start the discussion with the re-
sults on the WMT 2006 data set in Table 1. 
Rows 5 and 6 in Table 1 are two state-of-the-
art latent semantic models that are originally 
trained on clicked query-document pairs (i.e., 
clickthrough data extracted from search logs) for 
query-document matching (Gao et al 2011). To 
adopt these models for SMT, we view source-tar-
get sentence pairs as clicked query-document 
pairs, and trained both models using the same 
methods as in Gao et al (2011) on the parallel bi-
lingual training data described earlier. Specifi-
cally, BTLMPR is an extension to PLSA, and is 
the best performer among different versions of the 
Bi-Lingual Topic Model (BLTM) described in 
Gao et al (2011). BLTM with Posterior Regular-
ization (BLTMPR) is trained on parallel training 
data using the EM algorithm with a constraint en-
forcing a source sentence and its paralleled target 
sentence to not only share the same prior topic dis-
tribution, but to also have similar fractions of 
words assigned to each topic. We incorporated the 
model into the log-linear model for SMT (1) as 
                                                          
7 Gao and He (2013) reported results of MRF models with 
different feature sets. We picked the MRF using phrase fea-
tures only (MRFP) for comparison since we are mainly inter-
ested in phrase representation. 
follows. First of all, the topic distribution of a 
source sentence ?? , denoted by ?(?|??) , is in-
duced from the learned topic-word distributions 
using EM. Then, each translation candidate ? in 
the N-best list GEN(??) is scored as 
?(?|??) = ? ? ?(?|?)?(?|??)????    
?(??|?) can be similarly computed. Finally, the 
logarithms of the two probabilities are incorpo-
rated into the log-linear model of (1) as two addi-
tional features. DPM is the Discriminative Projec-
tion Model described in Gao et al (2011), which 
is an extension of LSA. DPM uses a matrix to pro-
ject a word vector of a sentence to a feature vector. 
The projection matrix is learned on parallel train-
ing data using the S2Net alorithm (Yih et al 
2011). DPM can be incorporated into the log-lin-
ear model for SMT (1) by introducing a new fea-
ture ??+1 for each phrase pair, which is defined 
as the cosine similarity of the phrases in the pro-
ject space.  
As we see from Table 1, both latent semantic 
models, although leading to some slight improve-
ment over Baseline, are much less effective than 
CPTM. 
Finally, we compare the CPTM with the Mar-
kov Random Field model using phrase features 
(MRFP in Tables 1 and 2), proposed by Gao and 
He (2013)7, on both the WMT 2006 and WMT 
2012 datasets. MRFp is a state-of-the-art large 
scale discriminative training model that uses the 
same expected BLEU training criterion, which 
has proven to give superior performance across a 
range of MT tasks recently (He and Deng 2012, 
Setiawan and Zhou 2013, Gao and He 2013).  
Unlike CPTM, MRFp is a linear model that 
simply treats each phrase pair as a single feature. 
Therefore, although both are trained using the 
# Systems dev news2011 news2010 news2008 newssyscomb2010 
1 Baseline 23.58 25.24 24.35 20.36 24.14 
2 MRFP 24.07
? 26.00? 24.90 20.84? 25.05? 
3 CPTM 24.12? 26.25? 25.05? 21.15?? 24.91? 
4 Comb (2 + 3) 24.46?? 26.56?? 25.52?? 21.64?? 25.22? 
Table 2:   BLEU results for the French to English task using translation models and systems built on 
the WMT 2012 data set. The superscripts ? and ? indicate statistically significant difference (p < 
0.05) from Baseline and MRFp, respectively. 
 
 
706
same expected BLEU based objective function, 
CPTM and MRFp model the translation relation-
ship between two phrases from different angles. 
MRFp estimates one translation score for each 
phrase pair explicitly without parameter sharing, 
while in CPTM, all phrases share the same neural 
network that projects raw phrases to the continu-
ous space, providing a more smoothed estimation 
of the translation score for each phrase pair.  
The results in Tables 1 and 2 show that CPTM 
outperforms MRFP on most of the test sets across 
the two WMT data sets, but the difference be-
tween them is often not significant. Our interpre-
tation is that although CPTM provides a better 
smoothed estimation for low-frequent phrase 
pairs, which otherwise suffer the data sparsity is-
sue, MRFp provides a more precise estimation for 
those high-frequent phrase pairs. That is, CPTM 
and MRFp capture complementary information 
for translation. We thus combine CPTM and 
MRFP (Comb in Tables 1 and 2) by incorporating 
two features, each for one model, into the log-lin-
ear model of SMT (1). We observe that for both 
translation tasks, accuracy improves by up to 0.8 
BLEU over MRFP alone (e.g., on the news2008 
test set in Table 2). The results confirm that 
CPTM captures complementary translation infor-
mation to MRFp. Overall, we improve accuracy 
by up to 1.3 BLEU over the baseline on both 
WMT data sets. 
7 Conclusions 
The work presented in this paper makes two major 
contributions. First, we develop a novel phrase 
translation model for SMT, where joint represen-
tations are exploited of a phrase in the source lan-
guage and of its translation in the target language, 
and where the translation score of the pair of 
source-target phrases are represented as the dis-
tance between their feature vectors in a low-di-
mensional, continuous space. The space is derived 
from the representations generated using a multi-
layer neural network. Second, we present a new 
learning method to train the weights in the multi-
layer neural network for the end-to-end BLEU 
metric directly. The training method is based on 
L-BFGS. We describe in detail how the gradient 
in closed form, as required for efficient optimiza-
tion, is derived. The objective function, which 
takes the form of the expected BLEU computed 
from N-best lists, is very different from the usual 
objective functions used in most existing architec-
tures of neural networks, e.g., cross entropy (Hin-
ton et al 2012) or mean square error (Deng et al 
2012). We hence have provided details in the der-
ivation of the gradient, which can serve as an ex-
ample to guide the derivation of neural network 
learning with other non-standard objective func-
tions in the future. 
Our evaluation on two WMT data sets show 
that incorporating the continuous-space phrase 
translation model into the log-linear framework 
significantly improves the accuracy of a state-of-
the-art phrase-based SMT system, leading to a 
gain up to 1.3 BLEU. Careful implementation of 
the L-BFGS optimization based on the BLEU-
centric objective function, together with the asso-
ciated closed-form gradient, is a key to the suc-
cess.  
A natural extension of this work is to expand 
the model and learning algorithm from shallow to 
deep neural networks. The deep models are ex-
pected to produce more powerful and flexible se-
mantic representations (e.g., Tur et al, 2012), and 
thus greater performance gain than what is pre-
sented in this paper. 
8 Acknowledgements 
We thank Michael Auli for providing a dataset 
and for helpful discussions. We also thank the four 
anonymous reviewers for their comments.  
References 
Andrew, G. and Gao, J. 2007. Scalable training 
of L1-regularized log-linear models. In 
ICML.  
Auli, M., Galley, M., Quirk, C. and Zweig, G. 
2013 Joint language and translation modeling 
with recurrent neural networks. In EMNLP. 
Bengio, Y. 2009. Learning deep architectures for 
AI. Fundamental Trends Machine Learning, 
vol. 2, no. 1, pp. 1?127. 
Bengio, Y., Duharme, R., Vincent, P., and Janvin, 
C. 2003. A neural probabilistic language 
model. JMLR, 3:1137-1155. 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3: 993-1022. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P. 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 
12. 
707
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Index-
ing by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
DeNero, J., Gillick, D., Zhang, J., and Klein, D. 
2006. Why generative phrase models underper-
form surface heuristics. In Workshop on Statis-
tical Machine Translation, pp. 31-38. 
Deng, L., Yu, D., and Platt, J. 2012. Scalable 
stacking and learning for building deep archi-
tectures. In ICASSP. 
Diamantaras, K. I., and Kung, S. Y. 1996. Princi-
ple Component Neural Networks: Theory and 
Applications. Wiley-Interscience. 
Dumais S., Letsche T., Littman M. and Landauer 
T. 1997. Automatic cross-language retrieval us-
ing latent semantic indexing. In AAAI-97 
Spring Symposium Series: Cross-Language 
Text and Speech Retrieval. 
Ganchev, K., Graca, J., Gillenwater, J., and 
Taskar, B. 2010. Posterior regularization for 
structured latent variable models. Journal of 
Machine Learning Research, 11 (2010): 2001-
2049. 
Gao, J., and He, X. 2013. Training MRF-based 
translation models using gradient ascent. In 
NAACL-HLT, pp. 450-459. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684.  
He, X., and Deng, L. 2012. Maximum expected 
bleu training of phrase and lexicon translation 
models. In ACL, pp. 292-301. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering Binary Codes for Documents by Learn-
ing Deep Generative Models. Topics in Cogni-
tive Science, pp. 1-18. 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, vol. 29, no. 6, pp. 82-97. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR, pp. 50-57. 
Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM.  
Kalchbrenner, N. and Blunsom, P. 2013. Recur-
rent continuous translation models. In EMNLP. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, 
C., Federico, M., Bertoldi, N., Cowan, B., Shen, 
W., Moran, C., Zens, R., Dyer, C., Bojar, O., 
Constantin, A., and Herbst, E. 2007. Moses: 
open source toolkit for statistical machine trans-
lation. In ACL 2007, demonstration session. 
Koehn, P. and Monz, C. 2006. Manual and auto-
matic evaluation of machine translation be-
tween European languages. In Workshop on 
Statistical Machine Translation, pp. 102-121. 
Koehn, P., Och, F., and Marcu, D. 2003. Statisti-
cal phrase-based translation. In HLT-NAACL, 
pp. 127-133. 
Lambert, P. and Banchs, R. E. 2005. Data inferred 
multi-word expressions for statistical machine 
translation. In MT Summit X, Phuket, Thailand. 
Li, P., Liu, Y., and Sun, M. 2013. Recursive auto-
encoders for ITG-based translation. In EMNLP. 
Liang,P., Bouchard-Cote,A., Klein, D. and 
Taskar, B. 2006. An end-to-end discriminative 
approach to machine translation. In COLING-
ACL. 
Marcu, D., and Wong, W. 2002. A phrase-based, 
joint probability model for statistical machine 
translation. In EMNLP. 
Mikolov, T., Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In INTER-
SPEECH, pp. 1045-1048. 
Mikolov, T., Kombrink, S., Burget, L., Cernocky, 
J., and Khudanpur, S. 2011. Extensions of re-
current neural network language model. In 
ICASSP, pp. 5528-5531. 
Mikolov, T. 2012. Statistical Language Model 
based on Neural Networks. Ph.D. thesis, Brno 
University of Technology. 
Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. 
Exploiting similarities among languages for 
machine translation. CoRR. 2013; 
abs/1309.4148. 
Mikolov, T., Yih, W. and Zweig, G. 2013b. Lin-
guistic Regularities in Continuous Space Word 
Representations. In NAACL-HLT. 
Mimno, D., Wallach, H., Naradowsky, J., Smith, 
D. and McCallum, A. 2009. Polylingual topic 
models. In EMNLP. 
708
Niehues J., Herrmann, T., Vogel, S., and Waibel, 
A. 2011. Wider context by using bilingual lan-
guage models in machine translation. 
Och, F. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL, pp. 160-
167.  
Och, F., and Ney, H. 2004. The alignment tem-
plate approach to statistical machine translation. 
Computational Linguistics, 29(1): 19-51. 
Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 
2002. BLEU: a method for automatic evaluation 
of machine translation. In ACL. 
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual Document Representations from 
Discriminative Projections. In EMNLP. 
Rosti, A-V., Hang, B., Matsoukas, S., and 
Schwartz, R. S. 2011. Expected BLEU training 
for graphs: bbn system description for WMT 
system combination task. In Workshop on Sta-
tistical Machine Translation. 
Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. 
A. R. 2007. Smooth bilingual n-gram transla-
tion. In EMNLP-CoNLL, pp. 430-438. 
Schwenk, H. 2012. Continuous space translation 
models for phrase-based statistical machine 
translation. In COLING. 
Schwenk, H., Rousseau, A., and Mohammed A. 
2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine 
translation. In NAACL-HLT Workshop on the 
future of language modeling for HLT, pp. 11-
19. 
Setiawan, H. and Zhou, B., 2013. Discriminative 
training of 150 million translation parameters 
and its application to pruning. In NAACL. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic Compositionality through Recursive 
Matrix-Vector Spaces. In EMNLP. 
Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 
2011. Parsing natural scenes and natural lan-
guage with recursive neural networks. In ICML. 
Son, L. H., Allauzen, A., and Yvon, F. 2012. Con-
tinuous space translation models with neural 
networks. In NAACL-HLT, pp. 29-48. 
Sundermeyer, M., Oparin, I., Gauvain, J-L. 
Freiberg, B., Schluter, R. and Ney, H. 2013. 
Comparison of feed forward and recurrent neu-
ral network language models. In ICASSP, pp. 
8430?8434. 
Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 
2012. Towards deeper understanding: deep con-
vex networks for semantic utterance classifica-
tion. In ICASSP. 
Vinokourov,A., Shawe-Taylor,J. and Cristia-
nini,N. 2002. Inferring a semantic representa-
tion of text via cross-language correlation anal-
ysis. In NIPS. 
Weston, J., Bengio, S., and Usunier, N. 2011. 
Large scale image annotation: learning to rank 
with joint word-image embeddings. In IJCAI. 
Wuebker, J., Mauser, A., and Ney, H. 2010. Train-
ing phrase translation models with leaving-one-
out. In ACL, pp. 475-484. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
Zhang, Y., Deng, L., He, X., and Acero, A. 2011. 
A novel decision function and the associated de-
cision-feedback learning for speech translation. 
In ICASSP. 
Zhila, A., Yih, W., Meek, C., Zweig, G. and 
Mikolov, T. 2013. Combining heterogeneous 
models for measuring relational similarity. In 
NAACL-HLT. 
Zou, W. Y., Socher, R., Cer, D., and Manning, C. 
D. 2013. Bilingual word embeddings for 
phrase-based machine translation. In EMNLP. 
709
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643?648,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Semantic Parsing for Single-Relation Question Answering
Wen-tau Yih Xiaodong He Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,xiaohe,meek}@microsoft.com
Abstract
We develop a semantic parsing framework
based on semantic similarity for open do-
main question answering (QA). We focus
on single-relation questions and decom-
pose each question into an entity men-
tion and a relation pattern. Using convo-
lutional neural network models, we mea-
sure the similarity of entity mentions with
entities in the knowledge base (KB) and
the similarity of relation patterns and re-
lations in the KB. We score relational
triples in the KB using these measures
and select the top scoring relational triple
to answer the question. When evaluated
on an open-domain QA task, our method
achieves higher precision across different
recall points compared to the previous ap-
proach, and can improve F
1
by 7 points.
1 Introduction
Open-domain question answering (QA) is an im-
portant and yet challenging problem that remains
largely unsolved. In this paper, we focus on an-
swering single-relation factual questions, which
are the most common type of question observed in
various community QA sites (Fader et al, 2013),
as well as in search query logs. We assumed
such questions are answerable by issuing a single-
relation query that consists of the relation and an
argument entity, against a knowledge base (KB).
Example questions of this type include: ?Who is
the CEO of Tesla?? and ?Who founded Paypal??
While single-relation questions are easier to
handle than questions with more complex and
multiple relations, such as ?When was the child of
the former Secretary of State in Obama?s admin-
istration born??, single-relation questions are still
far from completely solved. Even in this restricted
domain there are a large number of paraphrases of
the same question. That is to say that the problem
of mapping from a question to a particular relation
and entity in the KB is non-trivial.
In this paper, we propose a semantic parsing
framework tailored to single-relation questions.
At the core of our approach is a novel semantic
similarity model using convolutional neural net-
works. Leveraging the question paraphrase data
mined from the WikiAnswers corpus by Fader et
al. (2013), we train two semantic similarity mod-
els: one links a mention from the question to an
entity in the KB and the other maps a relation pat-
tern to a relation. The answer to the question can
thus be derived by finding the relation?entity triple
r(e
1
, e
2
) in the KB and returning the entity not
mentioned in the question. By using a general se-
mantic similarity model to match patterns and re-
lations, as well as mentions and entities, our sys-
tem outperforms the existing rule learning system,
PARALEX (Fader et al, 2013), with higher pre-
cision at all the recall points when answering the
questions in the same test set. The highest achiev-
able F
1
score of our system is 0.61, versus 0.54 of
PARALEX.
The rest of the paper is structured as follows.
We first survey related work in Sec. 2, followed by
the problem definition and the high-level descrip-
tion of our approach in Sec. 3. Sec. 4 details our
semantic models and Sec. 5 shows the experimen-
tal results. Finally, Sec. 6 concludes the paper.
2 Related Work
Semantic parsing of questions, which maps nat-
ural language questions to database queries, is
a critical component for KB-supported QA. An
early example of this research is the semantic
parser for answering geography-related questions,
learned using inductive logic programming (Zelle
and Mooney, 1996). Research in this line origi-
nally used small, domain-specific databases, such
as GeoQuery (Tang and Mooney, 2001; Liang et
643
al., 2013). Very recently, researchers have started
developing semantic parsers for large, general-
domain knowledge bases like Freebase and DB-
pedia (Cai and Yates, 2013; Berant et al, 2013;
Kwiatkowski et al, 2013). Despite significant
progress, the problem remains challenging. Most
methods have not yet been scaled to large KBs
that can support general open-domain QA. In con-
trast, Fader et al (2013) proposed the PARALEX
system, which targets answering single-relation
questions using an automatically created knowl-
edge base, ReVerb (Fader et al, 2011). By
applying simple seed templates to the KB and
by leveraging community-authored paraphrases of
questions from WikiAnswers, they successfully
demonstrated a high-quality lexicon of pattern-
matching rules can be learned for this restricted
form of semantic parsing.
The other line of work related to our approach
is continuous representations for semantic simi-
larity, which has a long history and is still an
active research topic. In information retrieval,
TF-IDF vectors (Salton and McGill, 1983), latent
semantic analysis (Deerwester et al, 1990) and
topic models (Blei et al, 2003) take the bag-of-
words approach, which captures well the contex-
tual information for documents, but is often too
coarse-grained to be effective for sentences. In
a separate line of research, deep learning based
techniques have been proposed for semantic un-
derstanding (Mesnil et al, 2013; Huang et al,
2013; Shen et al, 2014b; Salakhutdinov and Hin-
ton, 2009; Tur et al, 2012). We adapt the work
of (Huang et al, 2013; Shen et al, 2014b) for mea-
suring the semantic distance between a question
and relational triples in the KB as the core compo-
nent of our semantic parsing approach.
3 Problem Definition & Approach
In this paper, we focus on using a knowledge
base to answer single-relation questions. A single-
relation question is defined as a question com-
posed of an entity mention and a binary rela-
tion description, where the answer to this ques-
tion would be an entity that has the relation with
the given entity. An example of a single-relation
question is ?When were DVD players invented??
The entity is dvd-player and the relation is
be-invent-in. The answer can thus be de-
scribed as the following lambda expression:
?x. be-invent-in(dvd-player, x)
Q? RP ?M (1)
RP ? when were X invented (2)
M ? dvd players (3)
when were X invented
? be-invent-in (4)
dvd players
? dvd-player (5)
Figure 1: A potential semantic parse of the ques-
tion ?When were DVD players invented??
A knowledge base in this work can be simply
viewed as a collection of binary relation instances
in the form of r(e
1
, e
2
), where r is the relation and
e
1
and e
2
are the first and second entity arguments.
Single-relation questions are perhaps the easiest
form of questions that can directly be answered
by a knowledge base. If the mapping of the re-
lation and entity in the question can be correctly
resolved, then the answer can be derived by a sim-
ple table lookup, assuming that the fact exists in
the KB. However, due to the large number of para-
phrases of the same question, identifying the map-
ping accurately remains a difficult problem.
Our approach in this work can be viewed as a
simple semantic parser tailored to single-relation
questions, powered by advanced semantic similar-
ity models to handle the paraphrase issue. Given a
question, we first separate it into two disjoint parts:
the entity mention and the relation pattern. The
entity mention is a subsequence of consecutive
words in the question, where the relation pattern
is the question where the mention is substituted
by a special symbol. The mapping between the
pattern and the relation in the KB, as well as the
mapping between the mention and the entity are
determined by corresponding semantic similarity
models. The high-level approach can be viewed
as a very simple context-free grammar, which is
shown in Figure 1.
The probability of the rule in (1) is 1 since
we assume the input is a single-relation ques-
tion. For the exact decomposition of the ques-
tion (e.g., (2), (3)), we simply enumerate all com-
binations and assign equal probabilities to them.
The performance of this approach depends mainly
on whether the relation pattern and entity mention
can be resolved correctly (e.g., (4), (5)). To deter-
644
15K 15K 15K 15K 15K
500 500 500
max max
...
...
... max
500
...
...
Word hashing layer: ft
Convolutional layer: ht
Max pooling layer: v
Semantic layer: y
     <s>             w1              w2           wT             <s>Word sequence: xt
Word hashing matrix: Wf
Convolution matrix: Wc
Max pooling operation
Semantic projection matrix: Ws
... ...
500
Figure 2: The CNNSM maps a variable-length
word sequence to a low-dimensional vector in a
latent semantic space. A word contextual window
size (i.e., the receptive field) of three is used in the
illustration. Convolution over word sequence via
learned matrix W
c
is performed implicitly via the
earlier word hashing layer?s mapping with a local
receptive field. The max operation across the se-
quence is applied for each of 500 feature dimen-
sions separately.
mine the probabilities of such mappings, we pro-
pose using a semantic similarity model based on
convolutional neural networks, which is the tech-
nical focus in this paper.
4 Convolutional Neural Network based
Semantic Model
Following (Collobert et al, 2011; Shen et al,
2014b), we develop a new convolutional neural
network (CNN) based semantic model (CNNSM)
for semantic parsing. The CNNSM first uses a
convolutional layer to project each word within a
context window to a local contextual feature vec-
tor, so that semantically similar word-n-grams are
projected to vectors that are close to each other
in the contextual feature space. Further, since the
overall meaning of a sentence is often determined
by a few key words in the sentence, CNNSM uses
a max pooling layer to extract the most salient lo-
cal features to form a fixed-length global feature
vector. The global feature vector can be then fed
to feed-forward neural network layers to extract
non-linear semantic features. The architecture of
the CNNSM is illustrated in Figure 2. In what fol-
lows, we describe each layer of the CNNSM in
detail, using the annotation illustrated in Figure 2.
In our model, we leverage the word hash-
ing technique proposed in (Huang et al, 2013)
where we first represent a word by a letter-
trigram count vector. For example, given a
word (e.g., cat), after adding word boundary sym-
bols (e.g., #cat#), the word is segmented into a se-
quence of letter-n-grams (e.g., letter-trigrams: #-
c-a, c-a-t, a-t-#). Then, the word is represented
as a count vector of letter-trigrams. For exam-
ple, the letter-trigram representation of ?cat? is:
In Figure 2, the word hashing matrix W
f
de-
notes the transformation from a word to its letter-
trigram count vector, which requires no learning.
Word hashing not only makes the learning more
scalable by controlling the size of the vocabulary,
but also can effectively handle the OOV issues,
sometimes due to spelling mistakes. Given the
letter-trigram based word representation, we rep-
resent a word-n-gram by concatenating the letter-
trigram vectors of each word, e.g., for the t-th
word-n-gram at the word-n-gram layer, we have:
l
t
=
[
f
T
t?d
, ? ? ? , f
T
t
, ? ? ? , f
T
t+d
]
T
, t = 1, ? ? ? , T
where f
t
is the letter-trigram representation of the
t-th word, and n = 2d + 1 is the size of the con-
textual window. The convolution operation can
be viewed as sliding window based feature extrac-
tion. It captures the word-n-gram contextual fea-
tures. Consider the t-th word-n-gram, the convo-
lution matrix projects its letter-trigram representa-
tion vector l
t
to a contextual feature vector h
t
. As
shown in Figure 2, h
t
is computed by
h
t
= tanh(W
c
? l
t
), t = 1, ? ? ? , T
where W
c
is the feature transformation matrix, as
known as the convolution matrix, which are shared
among all word n-grams. The output of the con-
volutional layer is a sequence of local contextual
feature vectors, one for each word (within a con-
textual window). Since many words do not have
significant influence on the semantics of the sen-
tence, we want to retain in the global feature vector
only the salient features from a few key words. For
this purpose, we use a max operation, also known
as max pooling, to force the network to retain only
645
the most useful local features produced by the con-
volutional layers. Referring to the max-pooling
layer of Figure 2, we have
v(i) = max
t=1,??? ,T
{f
t
(i)}, i = 1, ? ? ? ,K
where v(i) is the i-th element of the max pool-
ing layer v, h
t
(i) is the i-th element of the t-th
local feature vector h
t
. K is the dimensionality
of the max pooling layer, which is the same as
the dimensionality of the local contextual feature
vectors {h
t
}. One more non-linear transformation
layer is further applied on top of the global feature
vector v to extract the high-level semantic repre-
sentation, denoted by y. As shown in Figure 2, we
have y = tanh(W
s
? v), where v is the global fea-
ture vector after max pooling, W
s
is the semantic
projection matrix, and y is the vector representa-
tion of the input query (or document) in latent se-
mantic space. Given a pattern and a relation, we
compute their relevance score by measuring the
cosine similarity between their semantic vectors.
The semantic relevance score between a pattern Q
and a relation R is defined as the cosine score of
their semantic vectors y
Q
and y
R
.
We train two CNN semantic models from sets of
pattern?relation and mention?entity pairs, respec-
tively. Following (Huang et al, 2013), for every
pattern, the corresponding relation is treated as a
positive example and 100 randomly selected other
relations are used as negative examples. The set-
ting for the mention?entity model is similar.
The posterior probability of the positive relation
given the pattern is computed based on the cosine
scores using softmax:
P (R
+
|Q) =
exp(? ? cos(y
R
+ , y
Q
))
?
R
?
exp(? ? cos(y
R
?
, y
Q
))
where ? is a scaling factor set to 5. Model train-
ing is done by maximizing the log-posteriori us-
ing stochastic gradient descent. More detail can
be found in (Shen et al, 2014a).
5 Experiments
In order to provide a fair comparison to previ-
ous work, we experimented with our approach
using the PARALAX dataset (Fader et al, 2013),
which consists of paraphrases of questions mined
from WikiAnswers and answer triples from Re-
Verb. In this section, we briefly introduce the
dataset, describe the system training and evalua-
tion processes and, finally, present our experimen-
tal results.
5.1 Data & Model Training
The PARALEX training data consists of ap-
proximately 1.8 million pairs of questions and
single-relation database queries, such as ?When
were DVD players invented??, paired with
be-invent-in(dvd-player,?). For eval-
uation, the authors further sampled 698 questions
that belong to 37 clusters and hand labeled the an-
swer triples returned by their systems.
To train our two CNN semantic models, we
derived two parallel corpora based on the PAR-
ALEX training data. For relation patterns, we first
scanned the original training corpus to see if there
was an exact surface form match of the entity (e.g.,
dvd-player would map to ?DVD player? in the
question). If an exact match was found, then the
pattern would be derived by replacing the mention
in the question with the special symbol. The corre-
sponding relation of this pattern was thus the rela-
tion used in the original database query, along with
the variable argument position (i.e., 1 or 2, indicat-
ing whether the answer entity was the first or sec-
ond argument of the relation). In the end, we de-
rived about 1.2 million pairs of patterns and rela-
tions. We then applied these patterns to all the 1.8
million training questions, which helped discover
160 thousand new mentions that did not have the
exact surface form matches to the entities.
When training the CNNSM for the pattern?
relation similarity measure, we randomly split the
1.2 million pairs of patterns and relations into two
sets: the training set of 1.19 million pairs, and
the validation set of 12 thousand pairs for hyper-
parameter tuning. Data were tokenized by re-
placing hyphens with blank spaces. In the ex-
periment, we used a context window (i.e., the re-
ceptive field) of three words in the convolutional
neural networks. There were 15 thousand unique
letter-trigrams observed in the training set (used
for word hashing). Five hundred neurons were
used in the convolutional layer, the max-pooling
layer and the final semantic layer, respectively.
We used a learning rate of 0.002 and the train-
ing converged after 150 iterations. A similar set-
ting was used for the CNNSM for the mention?
entity model, which was trained on 160 thousand
mention-entity pairs.
5.2 Results
We used the same test questions in the PARALEX
dataset to evaluate whether our system could find
646
F1
Precision Recall MAP
CNNSM
pm
0.57 0.58 0.57 0.28
CNNSM
p
0.54 0.61 0.49 0.20
PARALEX 0.54 0.77 0.42 0.22
Table 1: Performance of two variations of our sys-
tems, compared with the PARALEX system.
the answers from the ReVerb database. Because
our systems might find triples that were not re-
turned by the PARALEX systems, we labeled these
new question?triple pairs ourselves.
Given a question, the system first enumerated
all possible decompositions of the mentions and
patterns, as described earlier. We then computed
the similarity scores between the pattern and all
relations in the KB and retained 150 top-scoring
relation candidates. For each selected relation, the
system then checked all triples in the KB that had
this relation and computed the similarity score be-
tween the mention and corresponding argument
entity. The product of the probabilities of these
two models, which are derived from the cosine
similarity scores using softmax as described in
Sec. 4, was used as the final score of the triple for
ranking the answers. The top answer triple was
used to compute the precision and recall of the sys-
tem when reporting the system performance. By
limiting the systems to output only answer triples
with scores higher than a predefined threshold, we
could control the trade-off between recall and pre-
cision and thus plot the precision?recall curve.
Table 1 shows the performance in F
1
, preci-
sion, recall and mean average precision of our sys-
tems and PARALEX. We provide two variations
here. CNNSM
pm
is the full system and consists
of two semantic similarity models for pattern?
relation and mention?entity. The other model,
CNNSM
p
, only measures the similarity between
the patterns and relations, and maps a mention to
an entity when they have the same surface form.
Since the trade-off between precision and re-
call can be adjusted by varying the threshold, it
is more informative to compare systems on the
precision?recall curves, which are shown in Fig-
ure 3. As we can observe from the figure, the
precision of our CNNSM
pm
system is consistently
higher than PARALEX across all recall regions.
The CNNSM
m
system also performs similarly to
CNNSM
pm
in the high precision regime, but is in-
ferior when recall is higher. This is understandable
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6
Pre
cis
ion
Recall
  CNNSMpm  CNNSMp  Paralex
Figure 3: The precision?recall curves of the two
variations of our systems and PARALEX.
since the system does not match mentions with
entities of different surface forms (e.g., ?Robert
Hooke? to ?Hooke?). Notice that the highest F
1
values of them are 0.61 and 0.56, compared to
0.54 of PARALEX. Tuning the thresholds using a
validation set would be needed if there is a metric
(e.g., F
1
) that specifically needs to be optimized.
6 Conclusions
In this work, we propose a semantic parsing
framework for single-relation questions. Com-
pared to the existing work, our key insight is to
match relation patterns and entity mentions using
a semantic similarity function rather than lexical
rules. Our similarity model is trained using convo-
lutional neural networks with letter-trigrams vec-
tors. This design helps the model go beyond bag-
of-words representations and handles the OOV is-
sue. Our method achieves higher precision on the
QA task than the previous work, PARALEX, con-
sistently at different recall points.
Despite the strong empirical performance, our
system has room for improvement. For in-
stance, due to the variety of entity mentions in
the real world, the parallel corpus derived from
the WikiAnswers data and ReVerb KB may not
contain enough data to train a robust entity link-
ing model. Replacing this component with a
dedicated entity linking system could improve
the performance and also reduce the number of
pattern/mention candidates when processing each
question. In the future, we would like to extend
our method to other more structured KBs, such as
Freebase, and to explore approaches to extend our
system to handle multi-relation questions.
647
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Qingqing Cai and Alexander Yates. 2013. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 423?433,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference of Em-
pirical Methods in Natural Language Processing
(EMNLP ?11), Edinburgh, Scotland, UK, July 27-
31.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1608?1618,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion & knowledge management, pages 2333?2338.
ACM.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969?978.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw
Hill.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014a. A convolutional latent
semantic model for web search. Technical Report
MSR-TR-2014-55, Microsoft Research.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014b. Learning semantic
representations using convolutional neural networks
for web search. In Proceedings of the Companion
Publication of the 23rd International Conference on
World Wide Web Companion, pages 373?374.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Machine Learn-
ing: ECML 2001, pages 466?477. Springer.
Gokhan Tur, Li Deng, Dilek Hakkani-Tur, and Xi-
aodong He. 2012. Towards deeper understanding:
deep convex networks for semantic utterance classi-
fication. In Acoustics, Speech and Signal Processing
(ICASSP), 2012 IEEE International Conference on,
pages 5045?5048. IEEE.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Confer-
ence on Artificial Intelligence, pages 1050?1055.
648
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
