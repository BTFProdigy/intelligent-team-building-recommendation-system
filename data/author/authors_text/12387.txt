Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1475?1483,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Chinese Semantic Role Labeling with Shallow Parsing
Weiwei Sun and Zhifang Sui and Meng Wang and Xin Wang
Institute of Computational Linguistics
Peking University
Key Laboratory of Computational Linguistics
Ministry of Education, China
weiwsun@gmail.com;{szf,wm}@pku.edu.cn;xinwang.cpku@gmail.com;
Abstract
Most existing systems for Chinese Seman-
tic Role Labeling (SRL) make use of full
syntactic parses. In this paper, we evalu-
ate SRL methods that take partial parses as
inputs. We first extend the study on Chi-
nese shallow parsing presented in (Chen
et al, 2006) by raising a set of addi-
tional features. On the basis of our shal-
low parser, we implement SRL systems
which cast SRL as the classification of
syntactic chunks with IOB2 representation
for semantic roles (i.e. semantic chunks).
Two labeling strategies are presented: 1)
directly tagging semantic chunks in one-
stage, and 2) identifying argument bound-
aries as a chunking task and labeling their
semantic types as a classification task. For
both methods, we present encouraging re-
sults, achieving significant improvements
over the best reported SRL performance
in the literature. Additionally, we put
forward a rule-based algorithm to auto-
matically acquire Chinese verb formation,
which is empirically shown to enhance
SRL.
1 Introduction
In the last few years, there has been an increas-
ing interest in Semantic Role Labeling (SRL) on
several languages, which consists of recognizing
arguments involved by predicates of a given sen-
tence and labeling their semantic types. Nearly
all previous Chinese SRL research took full syn-
tactic parsing as a necessary pre-processing step,
such as (Sun and Jurafsky, 2004; Xue, 2008; Ding
and Chang, 2008). Many features are extracted to
encode the complex syntactic information. In En-
glish SRL research, there have been some attempts
at relaxing the necessity of using full syntactic
parses; better understanding of SRL with shallow
parsing is achieved by CoNLL-2004 shared task
(Carreras and M`arquez, 2004). However, it is still
unknown how these methods perform on other lan-
guages, such as Chinese.
To date, the best SRL performance reported on
the Chinese Proposition Bank (CPB) corresponds
to a F-measure is 92.0, when using the hand-
crafted parse trees from Chinese Penn Treebank
(CTB). This performance drops to 71.9 when a
real parser is used instead
1
(Xue, 2008). Com-
paratively, the best English SRL results reported
drops from 91.2 (Pradhan et al, 2008) to 80.56
(Surdeanu et al, 2007). These results suggest that
as still in its infancy stage, Chinese full parsing
acts as a central bottleneck that severely limits our
ability to solve Chinese SRL. On the contrary, Chi-
nese shallow parsing has gained a promising re-
sult (Chen et al, 2006); hence it is an alternative
choice for Chinese SRL.
This paper addresses the Chinese SRL problem
on the basis of shallow syntactic information at
the level of phrase chunks. We first extend the
study on Chinese chunking presented in (Chen et
al., 2006) by raising a set of additional features.
The new set of features yield improvement over
the strong chunking system described in (Chen et
al., 2006). On the basis of our shallow parser, we
implement lightweight systems which solve SRL
as a sequence labeling problem. This is accom-
plished by casting SRL as the classification of syn-
tactic chunks (e.g. NP-chunk) into one of semantic
labels with IOB2 representation (?). With respect
to the labeling strategy, we distinguish two differ-
ent approaches. The first one directly recognizes
semantic roles by an IOB-type sequence tagging.
The second approach divides the problem into two
independent subtasks: 1) Argument Identification
(AI) and 2) Semantic Role Classification (SRC).
1
This F-measure is evaluated on the basis of hand-crafted
word segmentation and POS tagging.
1475
A Chinese word consists of one or more char-
acters, and each character, in most cases, is a mor-
pheme. The problem of how the words are con-
structed from morphemes, known as word for-
mation, is very important for a majority of Chi-
nese language processing tasks. To capture Chi-
nese verb formation information, we introduce a
rule-based algorithm with a number of heuristics.
Experimental results indicate that word formation
features can help both shallow parsing and SRL.
We present encouraging SRL results on CPB
2
.
The best F-measure performance (74.12) with
gold segmentation and POS tagging can be
achieved by the first method. This result yield
significant improvement over the best reported
SRL performance (71.9) in the literature (Xue,
2008). The best recall performance (71.50) can be
achieved by the second method. This result is also
much higher than the best reported recall (65.6) in
(Xue, 2008).
2 Related Work
Previous work on Chinese SRL mainly focused on
how to implement SRL methods which are suc-
cessful on English, such as (Sun and Jurafsky,
2004; Xue and Palmer, 2005; Xue, 2008; Ding
and Chang, 2008). Sun and Jurafsky (2004) did
the preliminary work on Chinese SRL without
any large semantically annotated corpus of Chi-
nese. Their experiments were evaluated only on
ten specified verbs with a small collection of Chi-
nese sentences. This work made the first attempt
on Chinese SRL and produced promising results.
After the CPB was built, (Xue and Palmer, 2005)
and (Xue, 2008) have produced more complete
and systematic research on Chinese SRL. Ding
and Chang (2008) divided SRC into two sub-tasks
in sequence. Under the hierarchical architecture,
each argument should first be determined whether
it is a core argument or an adjunct, and then be
classified into fine-grained categories. Chen et
al. (2008) introduced an application of transduc-
tive SVM in Chinese SRL. Because their experi-
ments took hand-crafted syntactic trees as input,
how transductive SVMs perform in Chinese SRL
in realistic situations is still unknown.
Most existing systems for automatic Chinese
SRL make use of a full syntactic parse of the sen-
tence in order to define argument boundaries and
2
Our system is available at
http://code.google.com/p/csrler/
to extract relevant information for training clas-
sifiers to disambiguate between role labels. On
the contrary, in English SRL research, there have
been some attempts at relaxing the necessity of us-
ing syntactic information derived from full parse
trees. For example, Hacioglu and Ward (2003)
considered SRL as a chunking task; Pradhan et
al. (2005) introduced a new procedure to incor-
porate SRL results predicted respectively on full
and shallow syntactic parses. Previous work on
English suggests that even good labeling perfor-
mance has been achieved by full parse based SRL
systems, partial parse based SRL systems can still
enhance their performance. Though better under-
standing of SRL with shallow parsing on English
is achieved by CoNLL-2004 shared task (Carreras
and M`arquez, 2004), little is known about how
these SRL methods perform on Chinese.
3 Chinese Shallow Parsing
There have been some research on Chinese shal-
low parsing, and a variety of chunk defini-
tions have been proposed. However, most of
these studies did not provide sufficient detail.
In our system, we use chunk definition pre-
sented in (Chen et al, 2006), which provided
a chunk extraction tool. The tool to extract
chunks from CTB was developed by modify-
ing the English tool used in CoNLL-2000 shared
task, Chunklink
3
, and is publicly available at
http://www.nlplab.cn/chenwl/chunking.html. The
definition of syntactic chunks is illustrated in Line
CH in Figure 1. For example, ?????/the in-
surance company?, consisting of two nouns, is a
noun phrase.
With IOB2 representation (Ramshaw and Mar-
cus, 1995), the problem of Chinese chunking can
be regarded as a sequence labeling task. In this
paper, we first implement the chunking method
described in (Chen et al, 2006) as a strong base-
line. To conveniently illustrate, we denote a word
in focus with a fixed window w
?2
w
?1
ww
+1
w
+2
,
where w is current token. The baseline features
includes:
? Uni-gram word/POS tag feature: w
?2
, w
?1
,
w, w
+1
, w
+2
;
? Bi-gram word/POS tag feature: w
?2
w
?1
,
w
?1
w, w w
+1
, w
+1
w
+2
;
3
http://ilk.uvt.nl/team/sabine/chunklink/chunklink 2-2-
2000 for conll.pl
1476
WORD: ?? ?? ?? ?? ? ? ?? ?? ?? ?? ??
POS: [P] [NT] [NN NN] [AD] [P] [NR] [NN] [VP] [NN NN]
CH: [PP NP] [NP] [ADVP] [PP NP NP ] [VP] [NP]
M1: B-A* I-A*
4
B-A0 B-AM-ADV B-A2 I-A2 I-A2 B-V B-A1
M2-AI: B-A I-A B-A B-A B-A I-A I-A B-V B-A
M2-SRC: AM-TMP A0 AM-ADV A2 Rel A1
Until now, the insurance company has provided insurance services for the Sanxia Project.
Figure 1: An example from Chinese PropBank.
That means 18 features are used to represent a
given token. For instance, the bi-gram Word fea-
tures at 5th word position (???/company?) in
Figure 1 are ?? ???, ??? ???, ??? ??,
?? ??.
To improve shallow parsing, we raised an addi-
tional set of features. We will discuss these fea-
tures in section 5.
4 SRL with Shallow Parsing
The CPB is a project to add predicate-argument
relations to the syntactic trees of the CTB. Similar
to English PropBank, the semantic arguments of a
predicate are labeled with a contiguous sequence
of integers, in the form of AN (i.e. ArgN ); the ad-
juncts are annotated as such with the label AM (i.e.
ArgM) followed by a secondary tag that represents
the semantic classification of the adjunct. The as-
signment of argument labels is illustrated in Figure
1, where the predicate is the verb ???/provide?.
For example, the noun phrase ?????/the in-
surance company? is labeled as A0, meaning that it
is the proto-Agent of ??; the preposition phrase
?????/until now? is labeled as AM-TMP, in-
dicating a temporal component.
4.1 System Architecture
SRL is a complex task which has to be decom-
posed into a number of simpler decisions and tag-
ging schemes in order to be addressed by learn-
ing techniques. Regarding the labeling strategy,
we can distinguish at least two different strategies.
The first one consists of performing role identifi-
cation directly as IOB-type sequence tagging. The
second approach consists of dividing the problem
into two independent subtasks.
4
The semantic chunk labels here are B-AM-TMP and I-
AM-TMP. Limited to the document length, we cannot put all
detailed chunk labels in one line in Figure 1.
4.1.1 One-stage Strategy
In the one-stage strategy, on the basis of syntac-
tic chunks, we define semantic chunks which do
not overlap nor embed using IOB2 representation.
Syntactic chunks outside a chunk receive the tag
O. For syntactic chunks forming a chunk of type
A*, the first chunk receives the B-A* tag (Begin),
and the remaining ones receive the tag I-A* (In-
side). Then a SRL system can work directly by
using sequence tagging techinique. Since the se-
mantic annotation in the PropBank corpus does
not have any embedded structure, there is no loss
of information in this representation. The line M1
in Figure 1 illustrates this semantic chunk defini-
tion.
4.1.2 Two-stage Strategy
In the two-stage architecture, we divide Chinese
SRL into two subtasks: 1) semantic chunking for
AI, in which the argument boundaries are pre-
dicted, and 2) classification for SRC, in which the
already recognized arguments are assigned role la-
bels. In the first stage, we define semantic chunks
B-A which means begin of an argument and I-A
which means inside of an argument. In the second
stage, we solve SRC problem as a multi-class clas-
sification. The lines M2-AI and M2-SRC in Fig-
ure 1 illustrate this two-stage architecture. For ex-
ample, the noun phrase ?????/the insurance
company? is proto-Agent, and thus should be la-
beled as B-A in the AI chunking phase, and then
be tagged as A0. The phrase ??????/for the
Sanxia Project? consists of three chunks, which
should be labeled as B-A, I-A, and I-A respectively
in the AI chunking phase, then these three chunks
as a whole argument should be recognized as A2.
4.1.3 Chunk-by-Chunk
There is also another semantic chunk definition,
where the basic components of a semantic chunk
are words rather than syntactic chunks. A good
election for this problem is chunk-by-chunk pro-
1477
cessing instead of word-by-word. The motivation
is twofold: 1) phrase boundaries are almost always
consistent with argument boundaries; 2) chunk-
by-chunk processing is computationally less ex-
pensive and allows systems to explore a relatively
larger context. This paper performs a chunk-by-
chunk processing, but admitting a processing by
words within the target verb chunks.
4.2 Features
Most of the feature templates are ?standard?,
which have been used in previous SRL research.
We give a brief description of ?standard? features,
but explain our new features in detail.
5
4.2.1 Features for Semantic Chunking
In the semantic chunking tasks, i.e. the one-stage
method and the first step in the two-stage method,
we use the same set of features. The features
are extracted from three types of elements: syn-
tactic chunks, target verbs, links between chunks
and target verbs. They are formed making use
of words, POS tags and chunks of the sentence.
Xue (2008) put forward a rough verb classifica-
tion where verb classes are automatically derived
from the frame files, which are verb lexicon for
the CPB annotation. This kind of verb class in-
formation has been shown very useful for Chinese
SRL. Our system also includes this feature. In our
experiments, we represent a verb in two dimen-
sions: 1) number of arguments, and 2) number of
framesets. For example, a verb may belong to the
class ?C1C2,? which means that this verb has two
framesets, with the first frameset having one argu-
ment and the second having two arguments.
To conveniently illustrate, we de-
note a token chunk with a fixed context
w
i?1
[
c
k
w
i
...w
h
...w
j
]w
j+1
, where w
h
is the
head word of this chunk c
k
. The complete list of
features is listed here.
Extraction on Syntactic Chunks
Chunk type: c
k
.
Length: the number of words in a chunk.
Head word/POS tag. The rules described in
(Sun and Jurafsky, 2004) are used to extract head
word.
IOB chunk tag of head word: chunk tag of head
word with IOB2 representation (e.g. B-NP, I-NP).
5
The source code of our system also provides lots of com-
ments for implementation of all features.
Chunk words/POS tags context. Chunk con-
text includes one word before and one word after:
w
i?1
and w
j+1
.
POS tag chain: sequential containers of each
word?s POS tag: w
i
... w
j
. For example, this fea-
ture for ?????? is ?NN NN?.
Position: the position of the phrase with respect
to the predicate. It has three values as before, after
and here.
Extraction on Target Verbs Given a target verb
w
v
and its context, we extract the following fea-
tures.
Predicate, its POS tag, and its verb class.
Predicate IOB chunk tag context: the chain of
IOB2 chunk tags centered at the predicate within
a window of size -2/+2.
Predicate POS tag context: the POS tags of
the words that immediately precede and follow the
predicate.
Number of predicates: the number of predicates
in the sentence.
Extraction on Links To capture syntactic prop-
erties of links between the chunks and the verbs,
we use the following features.
Path: a flat path is defined as a chain of base
phrases between the token and the predicate. At
both ends, the chain is terminated with the POS
tags of the predicate and the headword of the to-
ken.
Distance: we have two notions of distance. The
first is the distance of the token from the predicate
as a number of base phrases, and the second is the
same distance as the number of VP chunks.
Combining Features We also combine above
features as some new features.
Conjunctions of position and head word, tar-
get verb, and verb class, including: position w
h
,
position w
v
, position w
h
w
v
, position class,
and position w
h
class.
Conjunctions of position and POS tag of
head word, target verb, and verb class, in-
cluding: position w
h
w
v
, position w
h
, and
position w
h
class.
4.2.2 Features for SRC
In the SRC stage of the two-stage method, dif-
ferent from previous work, our system only uses
word-based features, i.e. features extracted from
words and POS tags, to represent a given argu-
ment. Experiments show that a good semantic
1478
role classifier can be trained by using only word-
based features. To gather all argument position
information predicted in AI stage, we design a
coarse frame feature, which is a sequential collec-
tion of arguments. So far, we do not know the
detailed semantic type of each argument, and we
use XP as each item in the frame. To distinguish
the argument in focus, we use a special symbol
to indicate the corresponding frame item. For in-
stance, the Frame feature for argument ???
? is XP+XP+XP+XP+V+!XP, where !XP means
that it is the argument in focus.
Denote 1) a given argument
w
i?2
w
i?1
[w
i
w
i+1
...w
j?1
w
j
]w
j+1
w
j+2
, and
2) a given predicate w
v
. The features for SRC are
listed as follows.
Words/POS tags context of arguments: the con-
tents and POS tags of the following words: w
i
,
w
i?1
, w
i?2
, w
i+1
, w
i+2
, w
j
, w
j+1
, w
j?1
, w
j?2
,
w
j+1
, w
j+2
; the POS tags of the following words:
w
i+1
, w
i+2
, w
j+1
, w
j+2
.
Token Position.
Predicate, its POS, and its verb class.
Coarse Frame.
Combining features: conjunctions of bound-
ary words, including w
i?1
w
j+1
and w
i?2
w
j+2
;
conjunction of POS tags of boundary words, in-
cluding w
i?1
w
j+1
and w
i?2
w
j+2
; conjunction
of token position, boundary words, and predi-
cate word, including position w
i
w
j
, w
i
w
j
w
v
;
position w
i
w
j
w
v
; conjunction of token posi-
tion, boundary words? POS tags, and predicate
word, also including position w
i
w
j
, w
i
w
j
w
v
;
position w
i
w
j
w
v
; conjunction of predicate and
frame; conjunction of target verb class and frame;
conjunction of boundary words? POS tags, and
predicate word.
5 Automatic Chinese Verb Formation
Analyzing
5.1 Introduction to Chinese Word Formation
Chinese words consist of one or more charac-
ters, and each character, in most cases, is a mor-
pheme which is the smallest meaningful unit of
the language. According to the number of mor-
phemes, the words can be grouped into two sets,
simple words (consisting of one morpheme) and
compound words (consisting of two morphemes
or more). There are 9 kinds of word formation in
Chinese compound words, and table 1 shows the
detail with examples. Note that, attributive-head
and complementarity are not for Chinese verbs.
Types Examples
reduplication ??(look)??(think)
affixation ??(intensify)??(feel)
subject-verb ??(hear)??(dictate)
verb-object ??(quit smoking)
??(haircut)
verb-complement ??(inform)??(plant)
verb-result ??(exceed)??(boil)
adverbial-head ??(retreat)??(misuse)
coordinate ??(cherish)??(chase)
attributive-head* ??(rumor)??(hospital)
complementarity* ??(paper)??(horse)
Table 1: Example Words with Formation
The internal structure of a word constraints its
external grammatical behavior, and the formation
of a verb can provide very important information
for Chinese SRL. Take ???/exceed? as an ex-
ample, the two characters are both verbal mor-
phemes, and the character ??? means ?pass? and
the character ??? with the meaning of ?over?
shows the complement of the action of ???. In
this word, ??? is usually collocated with an ob-
ject, and hence a Patient role should comes af-
ter the verb ????. Note that, the verb ???,
however, is unlikely to have an object. Take ??
?/haircut? as another example, the first charac-
ter ??? is a verbal morpheme with the meaning
of ?cut? and the second character ??? is a nomi-
nal morpheme with the meaning of ?hair?. In this
word, ??? acts as the object of ???, and the word
???? is unlikely to have an Patient any more in
the sentential context.
5.2 Verb Formation Analyzing Method
To automatically analyze verb formation, we in-
troduce a rule-based algorithm. Pseudo code in
Algorithm 1 illustrates our algorithm. This algo-
rithm takes three string (one or more Chinese char-
acters) sets as lexicon knowledge:
? adverbial suffix set A: strings in A are usu-
ally realized as the modifier in a adverbial-
head type word, e.g. ?/not, ?/not,
?/always,?/both,?/all.
? object head setO: strings inO are usually re-
alized as the head in a verb-object type word,
e.g. ?/change,?/get,?/talk,?/send.
1479
Algorithm 1: Verb Formation Analyzing.
Data: adverbial suffix set A, object head set
O, complement suffix set C
input : word W = c
1
...c
n
and its POS P
output: head character h, adverbial character
a, complement character c, object
character o
begin
h = c = a = o = null;
if n = 4 and c
1
= c
3
and c
2
= c
4
then
return Verb formation of W
?
= c
1
c
3
;
else if n = 3 and c
2
= c
3
then
h = c
1
, c = c
2
;
else if n = 2 and c
1
= c
2
then
h = c
1
;
else if n = 1 then
h = c
1
;
else if c
n
? C and c
n?1
c
n
? C and
P=?VV? then
h = c
1
, c = c
n
/c
n?1
c
n
;
else if c
1
? A then
a = c
1
, h = c
2
...c
n
;
else if c
1
? O and P=?VV? then
h = c
1
, o = c
2
...c
n
;
end
? complement suffix set C: strings in C are
usually realized as complement in a verb-
complement type word: e.g. ?/out, ?/in,
?/finish,?/come,??/not.
Note that, to date there is no word formation
annotation corpus, so direct evaluation of our rule-
based algorithm is impossible. This paper makes
task-oriented evaluation which measures improve-
ments in SRL.
5.3 Using Word Formation Information to
improve Shallow Parsing
The majority of Chinese nouns are of type
attributive-head. This means that for most nouns
the last character provides very important infor-
mation indicating the head of the noun. For ex-
ample, the word formations of ???/peach?, ??
?/willow? and ????/boxtree? (three different
kinds of trees), are attributive-head and they have
the same head word ??/tree?. While for verbs, the
majority are of three types: verb-object, coordi-
nate and adverbial-head. For example, words ??
?/enlarge?, ???/make more drastic? and ??
?/accelerate? have the same head ??/add?. The
head morpheme is very useful in alleviating the
data sparseness in word level. However, for any
given word, it is very hard to accurately find the
head. In the shallow paring experiments, we use
a very simple rule to get a pseudo head character:
1) extracting the last word for a noun, and 2) ex-
tracting the first word for a verb. The new features
include:
Pattern 1: conjunction of pseudo head of w
i?1
and POS tags of w
i?1
and w
i
.
Pattern 2: conjunction of pseudo head of w
i
and
POS tags of w
i?1
and w
i
.
Pattern 3: conjunction of length/POS tags of
w
i?1
, w
i
, w
i+1
.
5.4 Using Verb Formation Information to
improve SRL
We use some new verb formation features to im-
prove our SRL system. The new features are listed
as follows. The first four are used in semantic
chunking task, and all are used in SRC task.
First/last characters.
Word length.
Conjunction of word length and first/last char-
acter.
Conjunction of token position and first/last
character.
The head string of a verb (e.g. ??? in ????).
The adverbial string of a verb (e.g. ??? in ??
??).
The complement string of a verb (e.g. ??? in
????).
The object string of a verb (e.g. ??? in ??
??).
6 Results and Discussion
6.1 Experimental Setting
6.1.1 Data
Experiments in previous work are mainly based on
CPB and CTB, but the experimental data prepar-
ing procedure does not seem consistent. For ex-
ample, the sum of each semantic role reported in
(Ding and Chang, 2008) is extremely smaller than
the corresponding occurrence statistics in origi-
nal data files in CPB. In this paper, we mod-
ify CoNLL-2005 shared task software
6
to pro-
cess CPB and CTB. In our experiments, we use
the CPB 1.0 and CTB 5.0. The data is divided
into three parts: files from chtb 081 to chtb 899
are used as training set; files from chtb 041 to
6
http://www.lsi.upc.edu/?srlconll/soft.html
1480
chtb 080 as development set; files from chtb 001
to chtb 040, and chtb 900 to chtb 931 as test set.
The data setting is the same as (Xue, 2008). The
results were evaluated for precision, recall and F-
measure numbers using the srl-eval.pl script pro-
vided by CoNLL-2005 shared task.
6.1.2 Classifier
For both syntactic and semantic chunking, we
used TinySVM along with YamCha
7
(Kudo and
Matsumoto, 2000; Kudo and Matsumoto, 2001).
In the chunking experiments, all SVM classifiers
were realized with a polynomial kernel of de-
gree 2. Pair-wise strategy is used to solve multi-
class classification problem. For the SRC ex-
periments, we use a linear SVM classifier, along
with One-Vs-All approach for multi-class classifi-
cation. SVM
lin
8
, a fast linear SVM solvers, is used
for supervised learning. l
2
-SVM-MFN (modified
finite newton) method is used to solve the opti-
mization problem (Keerthi and DeCoste, 2005).
6.2 Shallow Parsing Performance
P(%) R(%) F
?=1
Baseline 93.54 93.00 93.27
Ours 93.83 93.39 93.61
Table 2: Shallow parsing performance
Table 2 summarizes the overall shallow pars-
ing performance on test set. The first line shows
the performance of baseline. Comparing the best
system performance 94.13 F-measure of CoNLL
2000 shared task (Syntactic Chunking on English),
we can see Chinese shallow parsing has reached
a comparable result, tough the comparison of nu-
meric performance is not very fair, because of dif-
ferent languages, different chunk definition, dif-
ferent training data sizes, etc.. The second line
Ours shows the performance when new features
are added, from which we can see the word for-
mation based features can help shallow parsing.
Table 3 shows the detailed performance of noun
phrase (NP) and verb phrase (VP), which make up
most of phrase chunks in Chinese. Our new fea-
tures help NP more, whereas the effect of new fea-
tures for VP is not significant. That is in part be-
cause most VP chunk recognition error is caused
by long dependency, where word formation fea-
7
http://chasen.org/?taku/index.html.en
8
http://people.cs.uchicago.edu/?vikass/svmlin.html
P(%) R(%) F
?=1
NP(Baseline) 90.84 90.05 90.44
NP(Ours) 91.42 90.78 91.10
VP(Baseline) 94.44 94.55 94.50
VP(Ours) 94.65 94.74 94.69
Table 3: Performance of NP-chunk and VP-chunk
tures do not work. Take the sentences below for
example:
1. [
V P
??????]? (Therefore (we)
achieve victory.)
2. [
ADV P
??] [
V P
????] ?????
????? (Therefore the major changes
have not been met before.)
The contexts of the word ???/therefore? in the
two sentences are similar, where ???? is fol-
lowed by verbal components. In the second sen-
tence, the word ???/therefore? will be correctly
recognized as an adverbial phrase unless classifier
knows the following component is a clause. Un-
fortunately, word formation features cannot sup-
ply this kind of information.
6.3 SRL Performance
P(%) R(%) A(%) F
?=1
(Xue, 2008) 79.5 65.6 ? 71.9
M1? 79.02 69.12 ? 73.74
M1+ 79.25 69.61 ? 74.12
M2?/AI 80.34 75.11 ? 77.63
M2+/AI 80.01 75.15 ? 77.51
M2?/SRC ? ? 92.57 ?
M2+wf/SRC ? ? 93.25 ?
M2+/SRC ? ? 93.42 ?
M2?AI+SRC 76.48 71.50 ? 73.90
Table 4: Overall SRL performance of different
methods
Table 4 lists the overall SRL performance num-
bers on test set using different methods mentioned
earlier; these results are based on features com-
puted from gold standard segmentation and POS
tagging, but automatic recognized chunks, which
is parsed by our improved shallow parsing sys-
tem. For the AI and the whole SRL tasks, we
report the precision (P), recall (R) and the F
?=1
-
measure scores, and for the SRC task we report
the classification accuracy (A). The first line (Xue,
1481
2008) shows the SRL performance reported in
(Xue, 2008). To the authors? knowledge, this re-
sult is best SRL performance in the literature. Line
2 and 3 shows the performance of the one-stage
systems: 1) Line M1? is the performance without
word formation features; 2) Line M1+ is the per-
formance when verb formation features are added.
Line 4 to 8 shows the performance of the two-stage
systems: 1) Line M2?/AI and M2+/AI shows the
performance of AI phase without and within word
formation features respectively; 2) Line M2?/SRC
shows the SRC performance with trivial word-
based features (i.e. frame features and verb forma-
tion features are not used); 3) Line M2+wf/SRC is
the improved SRC performance when coarse verb
formation features are added; 4) Line M2+/SRC
is the SRC performance with all features; 5) Line
M2?AI+SRC shows the performance of SRL sys-
tem, which uses baseline features to identify argu-
ments, and use all features to classify arguments.
6.4 Discussion
The results summarized in Table 4 indicate that
according to the-state-of-the-art in Chinese pars-
ing, SRL systems based on shallow parsing out-
performs the ones based on full parsing. Com-
parison between one-stage strategy and two-stage
strategy indicates 1) that there is no significant dif-
ference in the F-measure; and 2) that two-stage
strategy method can achieve higher recall while
one-stage strategy method can achieve higher pre-
cision. Both the one-stage strategy and two-stage
strategy methods yield significant improvements
over the best reported SRL performance in the lit-
erature, especially in terms of recall performance.
Comparison SRL performance with full parses
and partial parses indicates that both models have
strong and weak points. The full parse based
method can implement high precision SRL sys-
tems, while the partial parse based methods can
implement high recall SRL systems. This is fur-
ther justification for combination strategies that
combine these independent SRL models.
Generally, Table 4 shows that verb formation
features can enhance Chinese SRL, especially for
fine-grained role classification. The effect of word
formation in formation in both shallow parsing
and SRL suggests that automatic word formation
analyzing is very important for Chinese language
processing. The rule-based algorithm is just a pre-
liminary study on this new topic, which requires
Num of words P (%) R (%) F
?=1
Length = 1 84.69% 75.48% 79.82
Length = 2 82.14% 74.21% 77.97
Length = 3 75.43% 63.98% 69.24
Length = 4 75.71% 65.63% 70.32
Length = 5 72.46% 64.38% 68.18
Length = 6 72.97% 66.21% 69.43
Length = 7 77.03% 67.65% 72.04
Length = 8 74.39% 57.28% 64.72
Length = 9 66.67% 51.16% 57.89
Length = 10 68.08% 58.28% 62.80
Length = 11+ 67.40% 57.71% 62.18
Table 5: SRL performance with arguments of dif-
ferent length
more research effort.
Though our SRC module does not use any pars-
ing information, our system can achieve 93.42%
accuracy, comparing the best gold parse based re-
sult 94.68% in the literature. This result suggests
that Chinese SRC system, even without parsing,
can reach a considerable good performance. The
main reason is that in Chinese, arguments with dif-
ferent semantic types have discriminative bound-
ary words, which can be extracted without pars-
ing. It is very clear that the main bottleneck for
Chinese SRL is to accurately identify arguments
rather than to disambiguate their detailed seman-
tic types.
Table 5 summarizes the labeling performance
for argument of different length. It is not surpris-
ing that arguments are more and more difficult to
rightly recognize as the increase of their length.
But the performance decline slows up when the
length of arguments is larger than 10. In other
words, some of the arguments that are composed
of many words can still be rightly identified. The
main reason for this point is that these arguments
usually have clear collocation words locating at ar-
gument boundaries. Take the sentences below for
example,
3. ??[A1 . . . . . .?] (including ... etc.)
the object of the verb ???/include? has a defi-
nite collocation word ??/etc.?, and therefore this
object is easy to be recognized as a A1.
7 Conclusion
In this paper, we discuss Chinese SRL on the ba-
sis of partial syntactic structure. Our systems ad-
vance the state-of-the-art in Chinese SRL. We first
1482
extend the study on Chinese shallow parsing and
implement a good shallow parser. On the ba-
sis of partial parses, SRL are formulated as a se-
quence labeling problem, performing IOB2 deci-
sions on the syntactic chunks of the sentence. We
exploit a wide variety of features based on words,
POS tags, and partial syntax. Additionally, we
discuss a language special problem, i.e. Chinese
word formation. Experimental results show that
coarse word formation information can help shal-
low parsing, especially for NP-chunk recognition.
A rule-based algorithm is put forward to automat-
ically acquire Chinese verb formation, which is
empirically shown to enhance SRL.
Acknowledgments
This work is supported by NSFC Project
60873156, 863 High Technology Project of
China 2006AA01Z144 and the Project of Toshiba
(China) R&D Center.
We would like to thank Weiwei Ding for his
good advice on this research.
We would also like to thank the anonymous re-
viewers for their helpful comments.
References
Xavier Carreras and Llu??s M`arquez. 2004. Introduc-
tion to the conll-2004 shared task: Semantic role
labeling. In Hwee Tou Ng and Ellen Riloff, edi-
tors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learn-
ing (CoNLL-2004), pages 89?97, Boston, Mas-
sachusetts, USA, May 6 - May 7. Association for
Computational Linguistics.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 97?104, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Yaodong Chen, Ting Wang, Huowang Chen, and Xis-
han Xu. 2008. Semantic role labeling of Chinese
using transductive svm and semantic heuristics. In
Proceedings of the Third International Joint Confer-
ence on Natural Language Processing: Volume-II.
Weiwei Ding and Baobao Chang. 2008. Improv-
ing Chinese semantic role classification with hier-
archical feature selection strategy. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 324?333, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support
vector machines. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 25?27, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
S. Sathiya Keerthi and Dennis DeCoste. 2005. A mod-
ified finite newton method for fast solution of large
scale linear svms. J. Mach. Learn. Res., 6:341?361.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of the 2nd workshop on Learning language in
logic and the 4th conference on Computational natu-
ral language learning, pages 142?144, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL ?01: Sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1?8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 217?220, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Sameer S. Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards robust semantic role labeling.
Comput. Linguist., 34(2):289?310.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Pro-
ceedings of the 3rd ACL/SIGDAT Workshop on Very
Large Corpora, Cambridge, Massachusetts, USA,
pages 82?94.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantc parsing of Chinese. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings.
Mihai Surdeanu, Llu??s M`arquez, Xavier Carreras, and
Pere Comas. 2007. Combination strategies for se-
mantic role labeling. J. Artif. Intell. Res. (JAIR),
29:105?151.
Nianwen Xue and Martha Palmer. 2005. Automatic
semantic role labeling for Chinese verbs. In in Pro-
ceedings of the 19th International Joint Conference
on Artificial Intelligence, page 2005.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
1483
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 253?256,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Prediction of Thematic Rank for Structured Semantic Role Labeling
Weiwei Sun and Zhifang Sui and Meng Wang
Institute of Computational Linguistics
Peking University
Key Laboratory of Computational Linguistics
Ministry of Education, China
weiwsun@gmail.com;{wm,szf}@pku.edu.cn
Abstract
In Semantic Role Labeling (SRL), it is rea-
sonable to globally assign semantic roles
due to strong dependencies among argu-
ments. Some relations between arguments
significantly characterize the structural in-
formation of argument structure. In this
paper, we concentrate on thematic hierar-
chy that is a rank relation restricting syn-
tactic realization of arguments. A log-
linear model is proposed to accurately
identify thematic rank between two argu-
ments. To import structural information,
we employ re-ranking technique to incor-
porate thematic rank relations into local
semantic role classification results. Exper-
imental results show that automatic pre-
diction of thematic hierarchy can help se-
mantic role classification.
1 Introduction
In Semantic Role Labeling (SRL), it is evident that
the arguments in one sentence are highly corre-
lated. For example, a predicate will have no more
than one Agent in most cases. It is reasonable to
label one argument while taking into account other
arguments. More structural information of all ar-
guments should be encoded in SRL approaches.
This paper explores structural information of
predicate-argument structure from the perspec-
tive of rank relations between arguments. The-
matic hierarchy theory argues that there exists a
language independent rank of possible semantic
roles, which establishes priority among arguments
with respect to their syntactic realization (Levin
and Hovav, 2005). This construct has been widely
implicated in linguistic phenomena, such as in the
subject selection rule of Fillmore?s Case Grammar
(1968): ?If there is an A [=Agent], it becomes the
subject; otherwise, if there is an I [=Instrument],
it becomes the subject; otherwise, the subject is
the O [=Object, i.e., Patient/Theme]?. This rule
implicitly establishes precedence relations among
semantic roles mentioned and can be simplified to:
Agent  Instrument  Patient/Theme
Emerging from a range of more basic semantic
properties of the ranked semantic roles, thematic
hierarchies can help to construct mapping from se-
mantics to syntax. It is therefore an appealing op-
tion for argument structure analysis. For example,
if the the rank of argument a
i
is shown higher than
a
j
, then the assignment [a
i
=Patient, a
j
=Agent] is
illegal, since the role Agent is the highest role.
We test the hypothesis that thematic rank be-
tween arguments can be accurately detected by
using syntax clues. In this paper, the concept
?thematic rank? between two arguments a
i
and a
j
means the relationship that a
i
is prior to a
j
or a
j
is
prior to a
i
. Assigning different labels to different
relations between a
i
and a
j
, we formulate predic-
tion of thematic rank between two arguments as a
multi-class classification task. A log-linear model
is put forward for classification. Experiments on
CoNLL-2005 data show that this approach can
get an good performance, achieving 96.42% ac-
curacy on gold parsing data and 95.14% accuracy
on Charniak automatic parsing data.
Most existing SRL systems divide this task into
two subtasks: Argument Identification (AI) and
Semantic Role Classification (SRC). To add struc-
tural information to a local SRL approach, we in-
corporate thematic hierarchy relations into local
classification results using re-ranking technique
in the SRC stage. Two re-ranking approaches,
1) hard constraint re-ranking and 2) soft con-
straint re-ranking, are proposed to filter out un-
like global semantic role assignment. Experiments
on CoNLL-2005 data indicate that our method
can yield significant improvement over a state-of-
the-art SRC baseline, achieving 0.93% and 1.32%
253
absolute accuracy improvements on hand-crafted
and automatic parsing data.
2 Prediction of Thematic Rank
2.1 Ranking Arguments in PropBank
There are two main problems in modeling the-
matic hierarchy for SRL on PropBank. On the one
hand, there is no consistent meaning of the core
roles (i.e. Arg0-5/ArgA). On the other hand, there
is no consensus over hierarchies of the roles in the
thematic hierarchy. For example, the Patient occu-
pies the second highest hierarchy in some linguis-
tic theories but the lowest in some other theories
(Levin and Hovav, 2005).
In this paper, the proto-role theory (Dowty,
1991) is taken into account to rank PropBank argu-
ments, partially resolving the two problems above.
There are three key points in our solution. First,
the rank of Arg0 is the highest. The Agent is al-
most without exception the highest role in pro-
posed hierarchies. Though PropBank defines se-
mantic roles on a verb by verb basis, for a particu-
lar verb, Arg0 is generally the argument exhibit-
ing features of a prototypical Agent while Arg1
is a prototypical Patient or Theme (Palmer et al,
2005). As being the proto-Agent, the rank of Arg0
is higher than other numbered arguments. Second,
the rank of the Arg1 is second highest or lowest.
Both hierarchy of Arg1 are tested and discussed in
section 4. Third, we do not rank other arguments.
Two sets of roles closely correspond to num-
bered arguments: 1) referenced arguments and 2)
continuation arguments. To adapt the relation to
help these two kinds of arguments, the equivalence
relation is divided into several sub-categories. In
summary, relations of two arguments a
i
and a
j
in
this paper include: 1) a
i
 a
j
: a
i
is higher than
a
j
, 2) a
i
? a
j
: a
i
is lower than a
j
, 3) a
i
ARa
j
: a
j
is the referenced argument of a
i
, 4) a
i
RAa
j
: a
i
is
the referenced argument of a
j
, 5) a
i
ACa
j
: a
j
is
the continuation argument of a
i
, 6) a
i
CAa
j
: a
i
is
the continuation argument of a
j
, 7) a
i
= a
j
: a
i
and a
j
are labeled as the same role label, and 8)
a
i
? a
j
: a
i
and a
j
are labeled as the Arg2-5, but
not in the same type.
2.2 Prediction Method
Assigning different labels to possible rank be-
tween two arguments a
i
and a
j
, such as labeling
a
i
 a
j
as ??, identification of thematic rank
can be formulated as a classification problem. De-
lemma, POS Tag, voice, and SCF of predicate
categories, position of two arguments; rewrite
rules expanding subroots of two arguments
content and POS tags of the boundary words
and head words
category path from the predicate to candidate
arguments
single character category path from the
predicate to candidate arguments
conjunction of categories, position, head
words, POS of head words
category and single character category path
from the first argument to the second argument
Table 1: Features for thematic rank identification.
note the set of relationsR. Formally, given a score
function S
TH
: A?A?R 7? R, the relation r is
recognized in argmax flavor:
r? = r
?
(a
i
, a
j
) = argmax
r?R
S
TH
(a
i
, a
j
, r)
A probability function is chosen as the score func-
tion and the log-linear model is used to estimate
the probability:
S
TH
(a
i
, a
j
, r) =
exp{?(a
i
, a
j
, r) ?w}
?
r?R
exp{?(a
i
, a
j
, r) ?w}
where ? is the feature map and w is the param-
eter vector to learn. Note that the model pre-
dicts the rank of a
i
and a
j
through calculating
S
TH
(a
i
, a
j
, r) rather than S
TH
(a
j
, a
i
, r), where
a
i
precedes a
j
. In other words, the position infor-
mation is implicitly encoded in the model rather
than explicitly as a feature.
The system extracts a number of features to rep-
resent various aspects of the syntactic structure of
a pair of arguments. All features are listed in Table
1. The Path features are designed as a sequential
collection of phrase tags by (Gildea and Jurafsky,
2002). We also use Single Character Category
Path, in which each phrase tag is clustered to a cat-
egory defined by its first character (Pradhan et al,
2005). To characterize the relation between two
constituents, we combine features of the two indi-
vidual arguments as new features (i.e. conjunction
features). For example, if the category of the first
argument is NP and the category of the second is S,
then the conjunction of category feature is NP-S.
3 Re-ranking Models for SRC
Toutanova et al (2008) empirically showed that
global information is important for SRL and that
254
structured solutions outperform local semantic
role classifiers. Punyakanok et al (2008) raised an
inference procedure with integer linear program-
ming model, which also showed promising results.
Identifying relations among arguments can pro-
vide structural information for SRL. Take the sen-
tence ?[
Arg0
She] [
V
addressed] [
Arg1
her hus-
band] [
ArgM?MNR
with her favorite nickname].?
for example, if the thematic rank of she and her
husband is predicted as that she is higher than her
husband, then her husband should not be assigned
the highest role.
To incorporate the relation information to lo-
cal classification results, we employ re-ranking ap-
proach. Assuming that the local semantic classi-
fier can produce a list of labeling results, our sys-
tem then attempts to pick one from this list accord-
ing to the predicted ranks. Two different polices
are implemented: 1) hard constraint re-ranking,
and 2) soft constraint re-ranking.
Hard Constraint Re-ranking The one picked
up must be strictly in accordance with the ranks.
If the rank prediction result shows the rank of ar-
gument a
i
is higher than a
j
, then role assignments
such as [a
i
=Patient and a
j
=Agent] will be elim-
inated. Formally, the score function of a global
semantic role assignment is:
S(a, s) =
?
i
S
l
(a
i
, s
i
)
?
i,j,i<j
I(r
?
(a
i
, a
j
), r(s
i
, s
j
))
where the function S
l
locally scores an argument;
r
?
: A ? A 7? R is to predict hierarchy of two
arguments; r : S ? S 7? R is to point out the the-
matic hierarchy of two semantic roles. For exam-
ple, r(Agent, Patient) = ?  ?. I : R ?R 7?
{0, 1} is identity function.
In some cases, there is no role assignment sat-
isfies all predicted relations because of prediction
mistakes. For example, if the hierarchy detec-
tion result of a = (a
1
, a
2
, a
3
) is (r
?
(a
1
, a
2
) =
, r
?
(a
2
, a
3
) =, r
?
(a
1
, a
3
) =?), there will be no
legal role assignment. In these cases, our system
returns local SRL results.
Soft Constraint Re-ranking In this approach,
the predicted confidence score of relations is
added as factor items to the score function of the
semantic role assignment. Formally, the score
function in soft constraint re-ranking is:
S(a, s) =
?
i
S
l
(a
i
, s
i
)
?
i,j,i<j
S
TH
(a
i
, a
j
, r(s
i
, s
j
))
4 Experiments
4.1 Experimental Settings
We evaluated our system using the CoNLL-2005
shared task data. Hierarchy labels for experimen-
tal corpora are automatically set according to the
definition of relation labels described in section
2.1. Charniak parser (Charniak, 2000) is used for
POS tagging and full parsing. UIUC Semantic
Role Labeler
1
is a state-of-the-art SRL system. Its
argument classification module is used as a strong
local semantic role classifier. This module is re-
trained in our SRC experiments, using parameters
described in (Koomen et al, 2005). Experiments
of SRC in this paper are all based on good ar-
gument boundaries which can filter out the noise
raised by argument identification stage.
4.2 Which Hierarchy Is Better?
Detection SRL (S) SRL (G)
Baseline ? 94.77% ?
A 94.65% 95.44% 96.89%
A & P? 95.62% 95.07% 96.39%
A & P? 94.09% 95.13% 97.22%
Table 2: Accuracy on different hierarchies
Table 2 summarizes the performance of the-
matic rank prediction and SRC on different the-
matic hierarchies. All experiments are tested on
development corpus. The first row shows the per-
formance of the local sematic role classifier. The
second to the forth rows show the performance
based on three ranking approach. A means that
the rank of Agent is the highest; P? means that the
rank of Patient is the second highest; P? means
that the rank of the Patient is the lowest. Col-
umn SRL(S) shows SRC performance based on
soft constraint re-ranking approach, and column
SRL(G) shows SRC performance based on gold
hierarchies. The data shows that the third the-
matic hierarchy fits SRL best, but is harder to
learn. Compared with P?, P? is more suitable for
SRL. In the following SRC experiments, we use
the first hierarchy because it is most helpful when
predicted relations are used.
4.3 Results And Improvement Analysis
Table 3 summarizes the precision, recall, and F-
measure of this task. The second column is fre-
quency of relations in the test data, which can be
1
http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php
255
seen as a simple baseline. Moreover, another natu-
ral baseline system can predict hierarchies accord-
ing to the roles classified by local classifier. For
example, if the a
i
is labeled as Arg0 and a
j
is la-
beled as Arg2, then the relation is predicted as .
The third column BL shows the F-measure of this
baseline. It is clear that our approach significantly
outperforms the two baselines.
Rel Freq. BL P(%) R(%) F
 57.40 94.79 97.13 98.33 97.73
? 9.70 51.23 98.52 97.24 97.88
? 23.05 13.41 94.49 93.59 94.04
= 0.33 19.57 93.75 71.43 81.08
AR 5.55 95.43 99.15 99.72 99.44
AC 3.85 78.40 87.77 82.04 84.81
CA 0.16 30.77 83.33 50.00 62.50
All ? 75.75 96.42
Table 3: Thematic rank prediction performance
Table 4 summarizes overall accuracy of SRC.
Baseline performance is the overall accuracy of
the local classifier. We can see that our re-ranking
methods can yield significant improvemnts over
the baseline.
Gold Charniak
Baseline 95.14% 94.12%
Hard 95.71% 94.74%
Soft 96.07% 95.44%
Table 4: Overall SRC accuracy.
Hierarchy prediction and re-ranking can be
viewed as modification for local classification re-
sults with structural information. Take the sen-
tence ?[Some ?circuit breakers? installed after the
October 1987] crash failed [their first test].? for
example, where phrases ?Some ... 1987? and
?their ... test? are two arguments. The table be-
low shows the local classification result (column
Score(L)) and the rank prediction result (column
Score(H)). The baseline system falsely assigns
roles as Arg0+Arg1, the rank relation of which is
. Taking into account rank prediction result that
relation ? gets a extremely high probability, our
system returns Arg1+Arg2 as SRL result.
Assignment Score(L) Score(H)
Arg0+Arg1 78.97%? 82.30% :0.02%
Arg1+Arg2 14.25%? 11.93% ?:99.98%
5 Conclusion and Future Work
Inspired by thematic hierarchy theory, this paper
concentrates on thematic hierarchy relation which
characterize the structural information for SRL.
The prediction of thematic rank is formulated as
a classification problem and a log-linear model
is proposed to solve this problem. To improve
SRC, we employ re-ranking technique to incorpo-
rate thematic rank information into the local se-
mantic role classifier. Experimental results show
that our methods can construct high-performance
thematic rank detector and that identification of ar-
guments? relations can significantly improve SRC.
Acknowledgments
This work is supported by NSFC Project
60873156, 863 High Technology Project of
China 2006AA01Z144 and the project of Toshiba
(China) Co., Ltd. R&D Center.
References
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL-00.
David R. Dowty. 1991. Thematic proto-roles and ar-
gument selection. Language, 67:547?619.
Charles Fillmore. 1968. The case for case. In Em-
mon Bach and Richard Harms, editors, Universals
in Linguistic Theory, pages 1?90. Holt, Rinehart and
Winston, New York, New York.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245?288.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Pro-
ceedings of the CoNLL-2005, pages 181?184, June.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Research Surveys in Linguistics.
Cambridge University Press, New York.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argu-
ment classification. In Machine Learning.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Comput. Linguist.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist.
256
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 140?150,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain-Assisted Product Aspect Hierarchy Generation: Towards
Hierarchical Organization of Unstructured Consumer Reviews
Jianxing Yu1, Zheng-Jun Zha1, Meng Wang1, Kai Wang2, Tat-Seng Chua1
1School of Computing, National University of Singapore
2Institute for Infocomm Research, Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg kwang@i2r.a-star.edu.sg
Abstract
This paper presents a domain-assisted ap-
proach to organize various aspects of a prod-
uct into a hierarchy by integrating domain
knowledge (e.g., the product specifications),
as well as consumer reviews. Based on the
derived hierarchy, we generate a hierarchical
organization of consumer reviews on various
product aspects and aggregate consumer opin-
ions on these aspects. With such organiza-
tion, user can easily grasp the overview of
consumer reviews. Furthermore, we apply the
hierarchy to the task of implicit aspect identi-
fication which aims to infer implicit aspects of
the reviews that do not explicitly express those
aspects but actually comment on them. The
experimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach.
1 Introduction
With the rapidly expanding e-commerce, most retail
Web sites encourage consumers to write reviews to
express their opinions on various aspects of prod-
ucts. Huge collections of consumer reviews are
now available on the Web. These reviews have be-
come an important resource for both consumers and
firms. Consumers commonly seek quality informa-
tion from online consumer reviews prior to purchas-
ing a product, while many firms use online reviews
as an important resource in their product develop-
ment, marketing, and consumer relationship man-
agement. However, the reviews are disorganized,
leading to the difficulty in information navigation
and knowledge acquisition. It is impractical for user
to grasp the overview of consumer reviews and opin-
ions on various aspects of a product from such enor-
mous reviews. Among hundreds of product aspects,
it is also inefficient for user to browse consumer re-
views and opinions on a specific aspect. Thus, there
is a compelling need to organize consumer reviews,
so as to transform the reviews into a useful knowl-
edge structure. Since the hierarchy can improve in-
formation representation and accessibility (Cimiano,
2006), we propose to organize the aspects of a prod-
uct into a hierarchy and generate a hierarchical or-
ganization of consumer reviews accordingly.
Towards automatically deriving an aspect hierar-
chy from the reviews, we could refer to traditional
hierarchy generation methods in ontology learning,
which first identify concepts from the text, then
determine the parent-child relations between these
concepts using either pattern-based or clustering-
based methods (Murthy et al, 2010). However,
pattern-based methods usually suffer from inconsis-
tency of parent-child relationships among the con-
cepts, while clustering-based methods often result
in low accuracy. Thus, by directly utilizing these
methods to generate an aspect hierarchy from con-
sumer reviews, the resulting hierarchy is usually in-
accurate, leading to unsatisfactory review organiza-
tion. On the other hand, domain knowledge of prod-
ucts is now available on the Web. For example,
there are more than 248,474 product specifications
in the product sellingWeb site CNet.com (Beckham,
2005). These product specifications cover some
product aspects and provide coarse-grained parent-
child relations among these aspects. Such domain
knowledge is useful to help organize the product as-
140
Figure 1: Sample hierarchical organization for iPhone 3G
pects into a hierarchy. However, the initial hierarchy
obtained from domain knowledge usually cannot fit
the review data well. For example, the initial hierar-
chy is usually too coarse and may not cover the spe-
cific aspects commented in the reviews, while some
aspects in the hierarchy may not be of interests to
users in the reviews.
Motivated by the above observations, we propose
in this paper to organize the product aspects into a
hierarchy by simultaneously exploiting the domain
knowledge (e.g., the product specification) and con-
sumer reviews. With derived aspect hierarchy, we
generate a hierarchical organization of consumer re-
views on various aspects and aggregate consumer
opinions on these aspects. Figure 1 illustrates a sam-
ple of hierarchical review organization for the prod-
uct ?iPhone 3G?. With such organization, users can
easily grasp the overview of product aspects as well
as conveniently navigate the consumer reviews and
opinions on any aspect. For example, users can find
that 623 reviews, out of 9,245 reviews, are about the
aspect ?price?, with 241 positive and 382 negative
reviews.
Given a collection of consumer reviews on a spe-
cific product, we first automatically acquire an ini-
tial aspect hierarchy from domain knowledge and
identify the aspects from the reviews. Based on the
initial hierarchy, we develop a multi-criteria opti-
mization approach to construct an aspect hierarchy
to contain all the identified aspects. Our approach
incrementally inserts the aspects into the initial hi-
erarchy based on inter-aspect semantic distance, a
metric used to measure the semantic relation among
aspects. In order to derive reliable semantic dis-
tance, we propose to leverage external hierarchies,
sampled from WordNet and Open Directory Project,
to assist semantic distance learning. With resultant
aspect hierarchy, the consumer reviews are then or-
ganized to their corresponding aspect nodes in the
hierarchy. We then perform sentiment classification
to determine consumer opinions on these aspects.
Furthermore, we apply the hierarchy to the task of
implicit aspect identification. This task aims to infer
implicit aspects of the reviews that do not explic-
itly express those aspects but actually comment on
them. For example, the implicit aspect of the review
?It is so expensive? is ?price.? Most existing aspect
identification approaches rely on the appearance of
aspect terms, and thus are not able to handle implicit
aspect problem. Based on our aspect hierarchy, we
can infer the implicit aspects by clustering the re-
views into their corresponding aspect nodes in the
hierarchy. We conduct experiments on 11 popular
products in four domains. More details of the corpus
are discussed in Section 4. The experimental results
demonstrate the effectiveness of our approach.
The main contributions of this work can be sum-
marized as follows:
1) We propose to hierarchically organize con-
sumer reviews according to an aspect hierarchy, so
as to transfer the reviews into a useful knowledge
structure.
2) We develop a domain-assisted approach to
generate an aspect hierarchy by integrating domain
knowledge and consumer reviews. In order to de-
rive reliable semantic distance between aspects, we
propose to leverage external hierarchies to assist se-
mantic distance learning.
3) We apply the aspect hierarchy to the task of im-
plicit aspect identification, and achieve satisfactory
performance.
The rest of this paper is organized as follows. Our
approach is elaborated in Section 2 and applied to
implicit aspect identification in Section 3. Section
4 presents the evaluations, while Section 5 reviews
141
related work. Finally, Section 6 concludes this paper
with future works.
2 Approach
Our approach consists of four components, includ-
ing initial hierarchy acquisition, aspect identifica-
tion, semantic distance learning, and aspect hierar-
chy generation. Next, we first define some prelimi-
nary and notations and then elaborate these compo-
nents.
2.1 Preliminary and Notations
Preliminary 1. An aspect hierarchy is defined as a
tree that consists of a set of unique aspects A and
a set of parent-child relations R between these as-
pects.
Given the consumer reviews of a product, let
A = {a1, ? ? ? , ak} denotes the product aspects com-
mented in the reviews. H0(A0,R0) denotes the ini-
tial hierarchy derived from domain knowledge. It
contains a set of aspects A0 and relations R0. Our
task is to construct an aspect hierarchy H(A,R), to
cover all the aspects in A and their parent-child re-
lations R, so that the consumer reviews are hierar-
chically organized. Note that H0 can be empty.
2.2 Initial Hierarchy Acquisition
As aforementioned, product specifications on prod-
uct selling websites cover some product aspects and
coarse-grained parent-child relations among these
aspects. Such domain knowledge is useful to help
organize aspects into a hierarchy. We here employ
the approach proposed by Ye and Chua (2006) to au-
tomatically acquire an initial aspect hierarchy from
the product specifications. The method first identi-
fies the Web page region covering product descrip-
tions and removes the irrelevant contents from the
Web page. It then parses the region containing the
product information to identify the aspects as well as
their structure. Based on the aspects and their struc-
ture, it generates an aspect hierarchy.
2.3 Aspect Identification
To identify aspects in consumer reviews, we first
parse each review using the Stanford parser 1. Since
the aspects in consumer reviews are usually noun
1http://nlp.stanford.edu/software/lex-parser.shtml
Figure 2: Sample Pros and Cons reviews
or noun phrases (Liu, 2009), we extract the noun
phrases (NP) from the parse tree as aspect candi-
dates. While these candidates may contain much
noise, we leverage Pros and Cons reviews (see Fig-
ure 2), which are prevalent in forum Web sites,
to assist identify aspects from the candidates. It
has been shown that simply extracting the frequent
noun terms from the Pros and Cons reviews can get
high accurate aspect terms (Liu el al., 2005). Thus,
we extract the frequent noun terms from Pros and
Cons reviews as features, then train a one-class SVM
(Manevitz et al, 2002) to identify aspects from the
candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone?, we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym termsWeb site 2, then clus-
ter them to obtain unique aspects based on unigram
feature.
2.4 Semantic Distance Learning
Our aspect hierarchy generation approach is essen-
tially based on the semantic relations among as-
pects. We here define a metric, Semantic Distance,
d(ax, ay), to quantitatively measure the semantic re-
lation between aspects ax and ay. We formulate
d(ax, ay) as the weighted sum of some underlying
features,
d(ax, ay) =
?
j
wjfj(ax, ay), (1)
where wj is the weight for j-th feature function
fj(?).
Next, we first introduce the linguistic features
used in our work and then present the semantic dis-
tance learning algorithm that aims to find the opti-
mal weights in Eq.(1).
2http://thesaurus.com
142
2.4.1 Linguistic Features
Given two aspects ax and ay, a feature is defined
as a function generating a numeric score f(ax, ay)
or a vector of scores. The features include Contex-
tual, Co-occurrence, Syntactic, Pattern and Lexical
features (Yang and Callan, 2009). These features are
generated based on auxiliary documents collected
from Web.
Specifically, we issue each aspect term and aspect
term pair as queries to Google and Wikipedia, re-
spectively, and collect the top 100 returned docu-
ments of each query. We then split the documents
into sentences. Based on these documents and sen-
tences, the features are generated as follows.
Contextual features. For each aspect, we collect
the documents containing the aspect as context to
build a unigram language model without smoothing.
Given two aspects, the KL-divergence between their
language models is computed as the Global-Context
feature between them. Similarly, we collect the left
two and right two words surrounding each aspect as
context and build a unigram language model. The
KL-divergence between the language models of two
aspects is defined as the Local-Context feature.
Co-occurrence features. We measure the co-
occurrence of two aspects by Pointwise Mutual
Information (PMI): PMI(ax,ay)=log(Count(ax,ay)/
Count(ax) Count(ay)), where Count(?) stands for the
number of documents or sentences containing the
aspect(s), or the number of Google document hits
for the aspect(s). Based on different definitions of
Count(?), we define the features of Document PMI,
Sentence PMI, and Google PMI, respectively.
Syntactic features. We parse the sentences that
contain each aspect pair into syntactic trees via the
Stanford Parser. The Syntactic-path feature is de-
fined as the average length of the shortest syntactic
path between the aspect pair in the tree. In addi-
tion, for each aspect, we collect a set of sentences
containing it, and label the semantic role of the sen-
tences via ASSERT parser 3. Given two aspects,
the number of the Subject terms overlaps between
their sentence sets is computed as the Subject Over-
lap feature. Similarly, for other semantic roles, such
as objects, modifiers, and verbs, we define the fea-
tures of Object Overlap, Modifier Overlap, and Verb
3http://cemantix.org/assert.html
Overlap, respectively.
Pattern features. 46 patterns are used in our
work, including 6 patterns indicating the hypernym
relations of two aspects (Hearst, 1992), and 40 pat-
terns measuring the part-of relations of two aspects
(Girju et al, 2006). These pattern features are
asymmetric, and they take the parent-child relations
among the aspects into consideration. All the pat-
terns are listed in Appendix A (submitted as supple-
mentary material). Based on these patterns, a 46-
dimensional score vector is obtained for each aspect
pair. A score is 1 if two aspects match a pattern, and
0 otherwise.
Lexical features. We take the word length differ-
ence between two aspects, as Length Difference fea-
ture. In addition, we issue the query ?define:aspect?
to Google, and collect the definition of each aspect.
We then count the word overlaps between the defini-
tions of two aspects, as Definition Overlap feature.
2.4.2 Semantic Distance Learning
This section elaborates the learning algorithm
that optimizes the semantic distance metric, i.e.,
the weighting parameters in Eq.(1). Typically, we
can utilize the initial hierarchy as training data.
The ground-truth distance between two aspects
dG(ax, ay) is generated by summing up all the edge
distances along the shortest path between ax and ay,
where every edge weight is assumed as 1. The dis-
tance metric is then obtained by solving the follow-
ing optimization problem,
argmin
wj |mj=1
?
ax,ay?A0
x<y
(dG(ax, ay) ?
m?
j=1
wjfj(ax, ay))2+??
m?
j=1
w2j ,
(2)
where m is the dimension of linguistic feature, ? is
a tradeoff parameter. Eq.(2) can be rewrote to its
matrix form as,
argmin
w
??d? fTw??2 + ? ? ?w?2 , (3)
where vector d contains the ground-truth distance of
all the aspect pairs. Each element corresponds to
the distance of certain aspect pair, and f is the corre-
sponding feature vector. The optimal solution of w
is given as
w? = (fT f + ? ? I)?1(fTd) (4)
143
where I is the identity metric.
The above learning algorithm can perform well
when sufficient training data (i.e., aspect (term)
pairs) is available. However, the initial hierarchy is
usually too coarse and thus cannot provide sufficient
information. On the other hand, abundant hand-
crafted hierarchies are available on the Web, such
as WordNet and Open Directory Project (ODP). We
here propose to leverage these external hierarchies
to assist semantic distance learning. A distance met-
ric w0 is learned from the external hierarchies us-
ing the above algorithm. Since w0 might be biased
to the characteristics of the external hierarchies, di-
rectly using w0 in our task may not perform well.
Alternatively, we use w0 as prior knowledge to as-
sist learning the optimal distance metric w from the
initial hierarchy. The learning problem is formulated
as follows,
argmin
w
??d? fTw??2 + ? ? ?w?2 + ? ? ?w? w0?2 ,
(5)
where ? and ? are tradeoff parameters.
The optimal solution of w can be obtained as
w? = (fT f + (? + ?) ? I)?1(fTd + ? ? w0). (6)
As a result, we can compute the semantic distance
between each two aspects according to Eq.(1).
2.5 Aspect Hierarchy Generation
Given the aspectsA = {a1, ? ? ? , ak} identified from
reviews and the initial hierarchy H0(A0,R0) ob-
tained from domain knowledge, our task is to con-
struct an aspect hierarchy to contain all the aspects
in A. Inspired by Yang and Callan (2009), we adopt
a multi-criteria optimization approach to incremen-
tally insert the aspects into appropriate positions in
the hierarchy based on multiple criteria.
Before going to the details, we first introduce an
information function to measure the amount of in-
formation carried in a hierarchy. An information
function Info(H) is defined as the sum of the se-
mantic distances of all the aspect pairs in the hierar-
chy (Yang and Callan, 2009).
Info(H(A,R)) =
?
x<y;ax,ay?A
d(ax, ay). (7)
Based on this information function, we then intro-
duce the following three criteria for aspect insertion:
minimum Hierarchy Evolution, minimum Hierarchy
Discrepancy and minimum Semantic Inconsistency.
Hierarchy Evolution is designed to monitor the
structure evolution of a hierarchy. The hierarchy is
incrementally hosting more aspects until all the as-
pects are allocated. The insertion of a new aspect a
into different positions in the current hierarchy H(i)
leads to different new hierarchies. Among these new
hierarchies, we here assume that the optimal one
H(i+1) should introduce the least changes of infor-
mation to H(i).
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(i)). (8)
By plugging in Eq.(7) and using least square to
measure the information changes, we have,
obj1 = argmin
H(i+1)
(?x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?Ai d(ax, ay))2, (9)
Hierarchy Discrepancy is used to measure the
global changes of the structure. We assume a good
hierarchy should bring the least changes to the initial
hierarchy,
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(0))
i + 1 . (10)
We then get,
obj2 = argmin
H(i+1)
1
i+1(
?
x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?A0 d(ax, ay))2. (11)
Semantic Inconsistency is introduced to quantify
the inconsistency between the semantic distance es-
timated via the hierarchy and that computed from
the feature functions. We assume that a good hier-
archy should precisely reflect the semantic distance
between aspects. For two aspects, their semantic
distance reflected by the hierarchy is computed as
the sum of adjacent distances along the shortest path
between them,
dH(ax, ay) =
?
p<q;(ap,aq)?SP (ax,ay)
d(ap, aq),
(12)
where SP (ax, ay) is the shortest path between the
aspects (ax, ay), (ap, aq) are the adjacent nodes
along the path.
144
We then define the following criteria to find the
hierarchy with minimum semantic inconsistency,
obj3 = argmin
H(i+1)
?
x<y;ax,ay?Ai?{a};
(dH(ax, ay)?d(ax, ay))2,
(13)
where d(ax, ay) is the distance computed based on
the feature functions in Section 2.4.
Through integrating the above criteria, the multi-
criteria optimization framework is formulated as,
obj = argmin
H(i+1)
(?1 ? obj1 + ?2 ? obj2 + ?3 ? obj3)
?1 + ?2 + ?3 = 1; 0 ? ?1, ?2, ?3 ? 1.
(14)
where ?1, ?2, ?3 are the tradeoff parameters.
To summarize, our aspect hierarchy generation
process starts from an initial hierarchy and inserts
the aspects into it one-by-one until all the aspects
are allocated. Each aspect is inserted to the op-
timal position found by Eq.(14). It is worth not-
ing that the insertion order may influence the result.
To avoid such influence, we select the aspect with
the least objective function value in Eq.(14) to in-
sert. Based on resultant hierarchy, the consumer re-
views are then organized to their corresponding as-
pect nodes in the hierarchy. We further prune out the
nodes without reviews from the hierarchy.
Moreover, we perform sentiment classification to
determine consumer opinions on various aspects. In
particular, we train a SVM sentiment classifier based
on the Pros and Cons reviews described in Section
2.3. We collect sentiment terms in the reviews as
features and represent reviews as feature vectors us-
ing Boolean weighting. Note that we define senti-
ment terms as those appear in the sentiment lexicon
provided by MPQA project (Wilson et al, 2005).
3 Implicit Aspect Identification
In this section, we apply the aspect hierarchy to the
task of implicit aspect identification. This task aims
to infer the aspects of reviews that do not explic-
itly express those aspects but actually comment on
them (Liu et al 2005). Take the review ?The phone
is too large? as an example, the task is to infer its
implicit aspect ?size.? It has been observed that the
reviews commenting on a same aspect usually use
some same sentiment terms (Su et al, 2008). There-
fore, sentiment term is an effective feature for identi-
fying implicit aspects. We here collect the sentiment
terms as features to represent each review into a fea-
ture vector. For each aspect node in the hierarchy,
we define its centroid as the average of its feature
vectors, i.e., the feature vectors of all the reviews
that are allocated at this node. We then calculate
the cosine similarity of each implicit-aspect review
to the centriods of all the aspect nodes, and allo-
cate the review into the node with maximum sim-
ilarity. As a result, the implicit aspect reviews are
grouped to their related aspect nodes. In other word,
their aspects are identified as the corresponding as-
pect nodes.
4 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, aspect hierarchy
generation, and implicit aspect identification.
4.1 Data and Experimental Setting
The details of our product review corpus are given
in Table 1. This corpus contains consumer reviews
on 11 popular products in four domains. These
reviews were crawled from several prevalent fo-
rum Web sites, including cnet.com, viewpoints.com,
reevoo.com and gsmarena.com. All of the reviews
were posted between June, 2009 and Sep 2010. The
aspects of the reviews, as well as the opinions on
the aspects were manually annotated. We also in-
vited five annotators to construct the gold-standard
hierarchies for the products by providing them the
initial hierarchies and the aspects in reviews. The
conflicts between annotators were resolved through
their discussions. For semantic distance learning, we
collected 50 hierarchies from WordNet and ODP, re-
spectively. The details are shown in Table 2. We
listed the topics of the hierarchies in Appendix B
(submitted as supplementary material).
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the reviews corpus, # denotes the
size of the reviews/sentences
145
Statistic WordNet ODP
Total # hierarchies 50 50
Total # terms 1,964 2,210
Average # depth 5.5 5.9
Total # related topics 12 16
Table 2: Statistics of the External Hierarchies
Figure 3: Evaluations on Aspect Identification. t-test, p-
values<0.05
We employed F1-measure, which is the combina-
tion of precision and recall, as the evaluation metric
for all the evaluations. For the evaluation on aspect
hierarchy, we defined precision as the percentage of
correctly returned parent-child pairs out of the to-
tal returned pairs, and recall as the percentage of
correctly returned parent-child pairs out of the to-
tal pairs in the gold standard. Throughout the ex-
periments, we empirically set ?1 = 0.4, ?2 = 0.3,
?3 = 0.3, ? = 0.4 and ? = 0.6.
4.2 Evaluations on Aspect Identification
We compared our approach against two state-of-the-
art methods: a) the method proposed by Hu and Liu
(2004), which is based on the association rule min-
ing, and b) the method proposed byWu et al (2009),
which is based on the dependency parser. The re-
sults are presented in Figure 3. On average, our
approach significantly outperforms Hu?s and Wu?s
method in terms of F1-measure by over 5.87% and
3.27%, respectively.
4.3 Evaluations on Aspect Hierarchy
4.3.1 Comparisons with the State-of-the-Arts
We compared our approach against four tra-
ditional hierarchy generation methods in the re-
searches on ontology learning, including a) pattern-
based method (Hearst, 1992) and b) clustering-based
method by Shi et al (2008), c) the method proposed
Figure 4: Evaluations on Aspect Hierarchy Generation. t-
test, p-values<0.05. w/ H denotes the methods with ini-
tial hierarchy, accordingly, w/o H refers to the methods
without initial hierarchy.
by Snow et al (2006) which was based on a proba-
bilistic model, and d) the method proposed by Yang
and Callan (2009). Since our approach and Yang?s
method can utilize initial hierarchy to assist hier-
archy generation, we evaluated their performance
with or without initial hierarchy, respectively. For
the sake of fair comparison, Snow?s, Yang?s and our
methods used the same linguistic features in Section
2.4.1.
Figure 4 shows the performance comparisons
of these five methods. We can see that our ap-
proach without using initial hierarchy outperforms
the pattern-based, clustering-based, Snow?s, and
Yang?s methods by over 17.9%, 19.8%, 2.9% and
6.1% respectively in terms of average F1-measure.
By exploiting initial hierarchy, our approach im-
proves the performance significantly. As compared
to the pattern-based, clustering-based and Snow?s
methods, it improves the average performance by
over 49.4%, 51.2% and 34.3% respectively. Com-
pared to Yang?s method with initial hierarchy, it
achieves 4.7% improvements on the average perfor-
mance.
The results show that pattern-based and
clustering-based methods perform poor. Pattern-
based method may suffer from the problem of low
coverage of patterns, especially when the patterns
are manually pre-defined, while the clustering-
based method (Shi et al, 2008) may sustain to the
bisection clustering mechanism which can only
generate a binary-tree. The results also illustrate
that our approach outperforms Snow?s and Yang?s
methods. By exploiting external hierarchies, our
146
Figure 5: Evaluations on the Impact of Initial Hierarchy.
t-test, p-values<0.05.
approach is able to derive reliable semantic distance
between aspects and thus improve the performance.
4.3.2 Evaluations on Effectiveness of Initial
Hierarchy
In this section, we show that even based on a small
part of the initial hierarchy, our approach can still
generate a satisfactory hierarchy. We explored dif-
ferent proportion of initial hierarchy, including 0%,
20%, 40%, 60% and 80% of the aspect pairs which
are collected top-down from the initial hierarchy. As
shown in Figure 5, the performance increases when
larger proportion of the initial hierarchy is used.
Thus, we can speculate that domain knowledge is
valuable in aspect hierarchy generation.
4.3.3 Evaluations on Effectiveness of
Optimization Criteria
We conducted a leave-one-out study to evaluate
the effectiveness of each optimization criterion. In
particular, we set one of the tradeoff parameters (?1,
?2, ?3) in Eq.(14) to zero, and distributed its weight
to the rest parameters averagely. From Figure 6, we
find that removing any optimization criterion would
degrade the performance on most products. It is in-
teresting to note that removing the third optimiza-
tion criterion, i.e., minimum semantic inconsistency,
slightly increases the performance on two products
(ipad touch and sony MP3). The reason might be
that the values of the three tradeoff parameters (em-
pirically set in Section 4.1) are not suitable for these
two products.
Figure 6: Evaluations of the Optimization Criteria. % of
change in F1-measure when a single criterion is removed.
t-test, p-values<0.05.
Figure 7: Evaluations on the Impact of Linguistic Fea-
tures. t-test, p-values<0.05.
4.3.4 Evaluations on Semantic Distance
Learning
In this section, we evaluated the impact of the fea-
tures and external hierarchies in semantic distance
learning. We investigated five sets of features as de-
scribed in Section 2.4.1, including contextual, co-
occurrence, syntactic, pattern and lexical features.
From Figure 7, we observe that the co-occurrence
and pattern features perform much better than con-
textual and syntactic features. A possible reason
is that co-occurrence and pattern features are more
likely to indicate parent-child aspect relationships,
while contextual and syntactic features are proba-
ble to measure sibling aspect relationships. Among
these features, the lexical features perform the worst.
The combination of all the features achieves the best
performance.
Next, we evaluated the effectiveness of external
hierarchies in semantic distance learning. We com-
pared the performance of our approach with or with-
out the external hierarchies. From Figure 8, we find
that by exploiting the external hierarchies, our ap-
147
Figure 8: Evaluations on the Impact of External Hierar-
chy. t-test, p-values<0.05.
proach improves the performance significantly. The
improvement is over 2.81% in terms of average F1-
measure. This implies that by using external hier-
archies, our approach can obtain effective semantic
distance, and thus improve the performance of as-
pect hierarchy generation.
Additionally, for sentiment classification, our
SVM classifier achieves an average F1-measure of
0.787 in the 11 products.
4.4 Evaluations on Implicit Aspect
Identification
To evaluate the performance of our approach on im-
plicit aspect identification, we collected 29,657 im-
plicit aspect review sentences on the 11 products
from the four forum Web sites introduced in Section
4.1. While most existing approaches for implicit as-
pect identification rely on hand-crafted rules (Liu,
2009), the method proposed in Su et al (2008) can
identify implicit aspects without hand-crafted rules
based on mutual clustering. Therefore, we adopt
Su?s method as the baseline here. Figure 9 illustrates
the performance comparison between Su?s and our
approach. We can see that our approach outperforms
Su?s method by over 9.18% in terms of average F1-
measure. This shows that our approach can iden-
tify the implicit aspects accurately by exploiting the
underlying associations among the sentiment terms
and each aspect in the hierarchy.
5 Related Work
Some researches treated review organization as a
multi-document summarization problem, and gen-
erated a summary by selecting and ordering sen-
tences taken from multiple reviews (Nishikawa et
Figure 9: Evaluations on Implicit Aspects Identification.
t-test, p-values<0.05
al., 2010). These works did not drill down to the
fine-grained level to explore the opinions on the
product aspects. Other researchers proposed to pro-
duce a summary covering consumer opinions on
each aspect. For example, Hu and Liu (2004) fo-
cused on extracting the aspects and determining
opinions on the aspects. However, their gener-
ated summary was unstructured, where the possible
relationships between aspects were not recognized
(Cadilhac et al, 2010). Subsequently, Carenini et
al. (2006) proposed to map the aspect to a user-
defined taxonomy, but the taxonomy was hand-
crafted which was not scalable.
Different from the previous works, we focus on
automatically generating an aspect hierarchy to hi-
erarchically organize consumer reviews. There are
some related works on ontology learning, which
first identify concepts from text, and then determine
parent-child relations between these concepts us-
ing either pattern-based or clustering-based methods
(Murthy et al, 2010). Pattern-based methods usu-
ally defined some lexical syntactic patterns to extract
the relations, while clustering-based methods mostly
utilized the hierarchical clustering methods to build
a hierarchy (Roy et al, 2006). Some works proposed
to integrate the pattern-based and clustering-based
methods in a general model, such as the probabilistic
model (Snow et al, 2006) and metric-based model
(Yang and Callan, 2009).
The researches on aspect identification are also
related to our work. Various aspect identification
methods have been proposed (Popescu et al, 2005),
including supervised methods (Liu el al., 2005), and
unsupervised methods (Mei et al, 2007). Different
148
features have been investigated for this task. For
example, Wu et al (2009) identified aspects based
on the features explored by dependency parser.
For implicit aspect identification, some works pro-
posed to define rules for identification (Liu el al.,
2005), while others suggested to automatically gen-
erate rules via mutual clustering (Su et al, 2008).
On the other hand, there are some related works
on sentiment classification (Pang and Lee, 2008).
These works can be categorized into four granu-
larities: document-level, sentence-level, aspect-level
and word-level sentiment classification (Liu, 2009).
Existing researches have been studied unsupervised
(Kim et al, 2004), supervised (Pang et al, 2002;
Pang et al, 2005) and semi-supervised classification
approaches (Goldberg et al, 2006; Li et al, 2009)
on these four levels.
6 Conclusions and Future Works
In this paper, we have developed a domain-assisted
approach to generate product aspect hierarchy by in-
tegrating domain knowledge and consumer reviews.
Based on the derived hierarchy, we can generate
a hierarchical organization of consumer reviews as
well as consumer opinions on the aspects. With such
organization, user can easily grasp the overview of
consumer reviews, as well as seek consumer reviews
and opinions on any specific aspect by navigating
through the hierarchy. We have further applied the
hierarchy to the task of implicit aspect identification.
We have conducted evaluations on 11 different prod-
ucts in four domains. The experimental results have
demonstrated the effectiveness of our approach. In
the future, we will explore other linguistic features
to learn the semantic distance between aspects, as
well as apply our approach to other applications.
Acknowledgments
This work is supported by NUS-Tsinghua Extreme
Search (NExT) project under the grant number: R-
252-300-001-490. We give warm thanks to the
project and anonymous reviewers for their valuable
comments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
J. Beckham. The Cnet E-commerce Data set. Technical
Reports, 2005.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
A. Cadilhac, F. Benamara, and N. Aussenac-Gilles. On-
tolexical Resources for Feature based OpinionMining:
a Case-study. Ontolex, 2010.
P. Cimiano, A. Madche, S. Staab, and J. Volker. Ontology
Learning. Handbook on Ontologies, Springer, 2004.
P. Cimiano, A. Hotho, and S. Staab. Learning Concept
Hierarchies from Text Corpora using Formal Concept
Analysis. Artificial Intelligence, 2005.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
R. Girju and A. Badulescu. Automatic Discovery of Part-
whole Relations Computational Linguistics, 2006.
A. Goldberg and X. Zhu. Seeing Stars When There
Aren?t Many Stars: Graph-based Semi-supervised
Learning for Sentiment Categorization. ACL, 2006.
M.A. Hearst. Automatic Acquisition of Hyponyms from
Large Text Corpora. Coling, 1992.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
X. Hu, N. Sun, C. Zhang, and T.-S. Chua Exploiting
Internal and External Semantics for the Clustering of
Short Texts Using World Knowledge. CIKM, 2009.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
A. C. Konig and E. Brill. Reducing the Human Overhead
in Text Categorization. KDD, 2006.
Z. Kozareva, E. Riloff, and E. Hovy. Semantic Class
Learning from the Web with Hyponym Pattern Link-
age Graphs. ACL, 2008.
T. Li, Y. Zhang, and V. Sindhwani. A Non-negative Ma-
trix Tri-factorization Approach to Sentiment Classifi-
cation with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
149
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
L.M.Manevitz andM. Yousef. One-class SVMs for Doc-
ument Classification. Machine Learning, 2002.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
X. Meng and H. Wang. Mining User Reviews: from
Specification to Summarization. ACL-IJCNLP, 2009.
K. Murthy, T.A. Faruquie, L.V. Subramaniam,
K.H. Prasad, and M. Mohania. Automatically
Generating Term-frequency-induced Taxonomies.
ACL, 2010.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang and L. Lee. Seeing Stars: Exploiting Class Rela-
tionships for Sentiment Categorization with respect to
Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2008.
HH. Pang, J. Shen, and R. Krishnan Privacy-Preserving,
Similarity-Based Text Retrieval. ACM Transactions
on Internet Technology, 2010.
A.M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
H. Poon and P. Domingos. Unsupervised Ontology In-
duction from Text. ACL, 2010.
G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
S. Roy and L.V. Subramaniam. Automatic Generation
of Domain Models for Call Centers from Noisy Tran-
scriptions. ACL, 2009.
B. Shi and K. Chang. Generating a Concept Hierarchy
for Sentiment Analysis. SMC, 2008.
R. Snow and D. Jurafsky. Semantic Taxonomy Induction
from Heterogenous Evidence. ACL, 2006.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
P. Turney. Thumbs up or thumbs down? Semantic Orien-
tation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
H. Yang and J. Callan A Metric-based Framework for
Automatic Taxonomy Induction. ACL, 2009.
S. Ye and T.-S. Chua. Learning Object Models from
Semi-structured Web Documents. IEEE Transactions
on Knowledge and Data Engineering, 2006.
J. Yi, T. Nasukawa, W. Niblack, and R. Bunescu. Senti-
ment Analyzer: Extracting Sentiments about a Given
Topic using Natural Language Processing Techniques.
ICDM, 2003.
L. Zhuang,F. Jing, and X.Y. Zhu Movie Review Mining
and Summarization CIKM, 2006.
150
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1496?1505,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Aspect Ranking: Identifying Important Product Aspects from Online
Consumer Reviews
Jianxing Yu, Zheng-Jun Zha, Meng Wang, Tat-Seng Chua
School of Computing
National University of Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg
Abstract
In this paper, we dedicate to the topic of aspect
ranking, which aims to automatically identify
important product aspects from online con-
sumer reviews. The important aspects are
identified according to two observations: (a)
the important aspects of a product are usually
commented by a large number of consumers;
and (b) consumers? opinions on the important
aspects greatly influence their overall opin-
ions on the product. In particular, given con-
sumer reviews of a product, we first identify
the product aspects by a shallow dependency
parser and determine consumers? opinions on
these aspects via a sentiment classifier. We
then develop an aspect ranking algorithm to
identify the important aspects by simultane-
ously considering the aspect frequency and
the influence of consumers? opinions given to
each aspect on their overall opinions. The ex-
perimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach. We further apply the aspect
ranking results to the application of document-
level sentiment classification, and improve the
performance significantly.
1 Introduction
The rapidly expanding e-commerce has facilitated
consumers to purchase products online. More than
$156 million online product retail sales have been
done in the US market during 2009 (Forrester Re-
search, 2009). Most retail Web sites encourage con-
sumers to write reviews to express their opinions on
various aspects of the products. This gives rise to
Figure 1: Sample reviews on iPhone 3GS product
huge collections of consumer reviews on the Web.
These reviews have become an important resource
for both consumers and firms. Consumers com-
monly seek quality information from online con-
sumer reviews prior to purchasing a product, while
many firms use online consumer reviews as an im-
portant resource in their product development, mar-
keting, and consumer relationship management. As
illustrated in Figure 1, most online reviews express
consumers? overall opinion ratings on the product,
and their opinions on multiple aspects of the prod-
uct. While a product may have hundreds of aspects,
we argue that some aspects are more important than
the others and have greater influence on consumers?
purchase decisions as well as firms? product devel-
opment strategies. Take iPhone 3GS as an exam-
ple, some aspects like ?battery? and ?speed,? are
more important than the others like ?moisture sen-
sor.? Generally, identifying the important product
aspects will benefit both consumers and firms. Con-
sumers can conveniently make wise purchase deci-
sion by paying attentions on the important aspects,
while firms can focus on improving the quality of
1496
these aspects and thus enhance the product reputa-
tion effectively. However, it is impractical for people
to identify the important aspects from the numerous
reviews manually. Thus, it becomes a compelling
need to automatically identify the important aspects
from consumer reviews.
A straightforward solution for important aspect
identification is to select the aspects that are fre-
quently commented in consumer reviews as the im-
portant ones. However, consumers? opinions on
the frequent aspects may not influence their over-
all opinions on the product, and thus not influence
consumers? purchase decisions. For example, most
consumers frequently criticize the bad ?signal con-
nection? of iPhone 4, but they may still give high
overall ratings to iPhone 4. On the other hand,
some aspects, such as ?design? and ?speed,? may not
be frequently commented, but usually more impor-
tant than ?signal connection.? Hence, the frequency-
based solution is not able to identify the truly impor-
tant aspects.
Motivated by the above observations, in this pa-
per, we propose an effective approach to automat-
ically identify the important product aspects from
consumer reviews. Our assumption is that the
important aspects of a product should be the as-
pects that are frequently commented by consumers,
and consumers? opinions on the important aspects
greatly influence their overall opinions on the prod-
uct. Given the online consumer reviews of a spe-
cific product, we first identify the aspects in the re-
views using a shallow dependency parser (Wu et al,
2009), and determine consumers? opinions on these
aspects via a sentiment classifier. We then design an
aspect ranking algorithm to identify the important
aspects by simultaneously taking into account the
aspect frequency and the influence of consumers?
opinions given to each aspect on their overall opin-
ions. Specifically, we assume that consumer?s over-
all opinion rating on a product is generated based
on a weighted sum of his/her specific opinions on
multiple aspects of the product, where the weights
essentially measure the degree of importance of the
aspects. A probabilistic regression algorithm is then
developed to derive these importance weights by
leveraging the aspect frequency and the consistency
between the overall opinions and the weighted sum
of opinions on various aspects. We conduct ex-
periments on 11 popular products in four domains.
The consumer reviews on these products are crawled
from the prevalent forum Web sites (e.g., cnet.com
and viewpoint.com etc.) More details of our review
corpus are discussed in Section 3. The experimen-
tal results demonstrate the effectiveness of our ap-
proach on important aspects identification. Further-
more, we apply the aspect ranking results to the ap-
plication of document-level sentiment classification
by carrying out the term-weighting based on the as-
pect importance. The results show that our approach
can improve the performance significantly.
The main contributions of this paper include,
1) We dedicate to the topic of aspect ranking,
which aims to automatically identify important as-
pects of a product from consumer reviews.
2) We develop an aspect ranking algorithm to
identify the important aspects by simultaneously
considering the aspect frequency and the influence
of consumers? opinions given to each aspect on their
overall opinions.
3) We apply aspect ranking results to the applica-
tion of document-level sentiment classification, and
improve the performance significantly.
There is another work named aspect ranking
(Snyder et al, 2007). The task in this work is differ-
ent from ours. This work mainly focuses on predict-
ing opinionated ratings on aspects rather than iden-
tifying important aspects.
The rest of this paper is organized as follows. Sec-
tion 2 elaborates our aspect ranking approach. Sec-
tion 3 presents the experimental results, while Sec-
tion 4 introduces the application of document-level
sentiment classification. Section 5 reviews related
work and Section 6 concludes this paper with future
works.
2 Aspect Ranking Framework
In this section, we first present some notations and
then elaborate the key components of our approach,
including the aspect identification, sentiment classi-
fication, and aspect ranking algorithm.
2.1 Notations and Problem Formulation
Let R = {r1, ? ? ? , r|R|} denotes a set of online con-
sumer reviews of a specific product. Each review
r ? R is associated with an overall opinion rating
1497
Or, and covers several aspects with consumer com-
ments on these aspects. Suppose there arem aspects
A = {a1, ? ? ? , am} involved in the review corpus
R, where ak is the k-th aspect. We define ork as the
opinion on aspect ak in review r. We assume that
the overall opinion rating Or is generated based on
a weighted sum of the opinions on specific aspects
ork (Wang et al, 2010). The weights are denoted as
{?rk}mk=1, each of which essentially measures the
degree of importance of the aspect ak in review r.
Our task is to derive the important weights of as-
pects, and identify the important aspects.
Next, we will introduce the key components of
our approach, including aspect identification that
identifies the aspects ak in each review r, aspect sen-
timent classification which determines consumers?
opinions ork on various aspects, and aspect ranking
algorithm that identifies the important aspects.
2.2 Aspect Identification
As illustrated in Figure 1, there are usually two types
of reviews, Pros and Cons review and free text re-
views on the Web. For Pros and Cons reviews, the
aspects are identified as the frequent noun terms in
the reviews, since the aspects are usually noun or
noun phrases (Liu, 2009), and it has been shown
that simply extracting the frequent noun terms from
the Pros and Cons reviews can get high accurate
aspect terms (Liu el al., 2005). To identify the as-
pects in free text reviews, we first parse each review
using the Stanford parser 1, and extract the noun
phrases (NP) from the parsing tree as aspect can-
didates. While these candidates may contain much
noise, we leverage the Pros and Cons reviews to
assist identify aspects from the candidates. In par-
ticular, we explore the frequent noun terms in Pros
and Cons reviews as features, and train a one-class
SVM (Manevitz et al, 2002) to identify aspects in
the candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone,? we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym terms Web site 2, and then
cluster the terms to obtain unique aspects based on
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://thesaurus.com
unigram feature.
2.3 Aspect Sentiment Classification
Since the Pros and Cons reviews explicitly express
positive and negative opinions on the aspects, re-
spectively, our task is to determine the opinions in
free text reviews. To this end, we here utilize Pros
andCons reviews to train a SVM sentiment classifier.
Specifically, we collect sentiment terms in the Pros
and Cons reviews as features and represent each re-
view into feature vector using Boolean weighting.
Note that we select sentiment terms as those appear
in the sentiment lexicon provided by MPQA project
(Wilson et al, 2005). With these features, we then
train a SVM classifier based on Pros and Cons re-
views. Given a free text review, since it may cover
various opinions on multiple aspects, we first locate
the opinionated expression modifying each aspect,
and determine the opinion on the aspect using the
learned SVM classifier. In particular, since the opin-
ionated expression on each aspect tends to contain
sentiment terms and appear closely to the aspect (Hu
and Liu, 2004), we select the expressions which con-
tain sentiment terms and are at the distance of less
than 5 from the aspect NP in the parsing tree.
2.4 Aspect Ranking
Generally, consumer?s opinion on each specific as-
pect in the review influences his/her overall opin-
ion on the product. Thus, we assume that the con-
sumer gives the overall opinion rating Or based on
the weighted sum of his/her opinion ork on each as-
pect ak:
?m
k=1 ?rkork, which can be rewritten as
?rTor, where?r and or are the weight and opinion
vectors. Inspired by the work of Wang et al (2010),
we viewOr as a sample drawn from aGaussian Dis-
tribution, with mean ?rTor and variance ?2,
p(Or) =
1?
2??2
exp[?(Or ? ?r
Tor)2
2?2
]. (1)
To model the uncertainty of the importance
weights ?r in each review, we assume ?r as a sam-
ple drawn from a Multivariate Gaussian Distribu-
tion, with ? as the mean vector and? as the covari-
ance matrix,
p(?r) =
1
(2pi)n/2|?|1/2 exp[?
1
2(?r ? ?)
T??1(?r ? ?)].
(2)
1498
We further incorporate aspect frequency as a prior
knowledge to define the distribution of ? and ?.
Specifically, the distribution of ? and ? is defined
based on its Kullback-Leibler (KL) divergence to a
prior distribution with a mean vector?0 and an iden-
tity covariance matrix I in Eq.3. Each element in?0
is defined as the frequency of the corresponding as-
pect: frequency(ak)/
?m
i=1 frequency(ai).
p(?,?) = exp[?? ?KL(Q(?,?)||Q(?0, I))],
(3)
where KL(?, ?) is the KL divergence, Q(?,?) de-
notes a Multivariate Gaussian Distribution, and ? is
a tradeoff parameter.
Base on the above definition, the probability of
generating the overall opinion rating Or on review r
is given as,
p(Or|?, r) =
?
p(Or|?rTor, ?2)
? p(?r|?,?) ? p(?,?)d?r,
(4)
where? = {?,?,?, ?2} are the model parameters.
Next, we utilize Maximum Log-likelihood (ML)
to estimate the model parameters given the con-
sumer reviews corpus. In particular, we aim to find
an optimal ?? to maximize the probability of observ-
ing the overall opinion ratings in the reviews corpus.
?? = argmax
?
?
r?R
log(p(Or|?, r))
= argmin
?
(|R| ? 1) log det(?) +
?
r?R
[log ?2+
(Or??rTor)2
?2 + (?r ? ?)
T??1(?r ? ?)]+
(tr(?) + (?0 ? ?)TI(?0 ? ?)).
(5)
For the sake of simplicity, we denote the objective
function
?
r?R log(p(Or|?, r)) as ?(?).
The derivative of the objective function with re-
spect to each model parameter vanishes at the mini-
mizer:
??(?)
??r = ?
(?rTor?Or)or
?2 ??
?1(?r ? ?)
= 0;
(6)
??(?)
?? =
?
r?R
[???1(?r ? ?)]? ? ? I(?0 ? ?)
= 0;
(7)
??(?)
?? =
?
r?R
{?(??1)T ? [?(??1)T (?r ? ?)
(?r ? ?)T (??1)T ]}+ ? ?
[
(??1)T ? I
]
= 0;
(8)
??(?)
??2 =
?
r?R
(? 1?2 +
(Or??rTor)2
?4 ) = 0,
(9)
which lead to the following solutions:
??r = (oror
T
?2 +?
?1)?1(Oror?2 +?
?1?);
(10)
?? = (|R|??1 + ? ? I)?1(??1
?
r?R
?r + ? ? I?0);
(11)
?? = {[ 1?
?
r?R
[
(?r ? ?)(?r ? ?)T
]
+
( |R|??2? )
2I]1/2 ? (|R|??)2? I}
T ;
(12)
??2 = 1|R|
?
r?R
(Or ? ?rTor)2.
(13)
We can see that the above parameters are involved
in each other?s solution. We here utilize Alternating
Optimization technique to derive the optimal param-
eters in an iterative manner. We first hold the param-
eters ?, ? and ?2 fixed and update the parameters
?r for each review r ? R. Then, we update the
parameters ?, ? and ?2 with fixed ?r (r ? R).
These two steps are alternatively iterated until the
Eq.5 converges. As a result, we obtain the optimal
importance weights ?r which measure the impor-
tance of aspects in review r ? R. We then compute
the final importance score ?k for each aspect ak by
integrating its importance score in all the reviews as,
?k =
1
|R|
?
r?R
?rk, k = 1, ? ? ? ,m (14)
It is worth noting that the aspect frequency is con-
sidered again in this integration process. According
to the importance score ?k, we can identify impor-
tant aspects.
3 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, sentiment classi-
fication, and aspect ranking.
3.1 Data and Experimental Setting
The details of our product review data set is given
in Table 1. This data set contains consumer reviews
on 11 popular products in 4 domains. These reviews
were crawled from the prevalent forum Web sites,
including cnet.com, viewpoints.com, reevoo.com
and gsmarena.com. All of the reviews were posted
1499
between June, 2009 and Sep 2010. The aspects of
the reviews, as well as the opinions on the aspects
were manually annotated as the gold standard for
evaluations.
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the Data Sets, # denotes the size of
the reviews/sentences.
To examine the performance on aspect identifi-
cation and sentiment classification, we employed
F1-measure, which was the combination of preci-
sion and recall, as the evaluation metric. To evalu-
ate the performance on aspect ranking, we adopted
Normalized Discounted Cumulative Gain at top k
(NDCG@k) (Jarvelin and Kekalainen, 2002) as the
performance metric. Given an aspect ranking list
a1, ? ? ? , ak, NDCG@k is calculated by
NDCG@k = 1
Z
k
?
i=1
2t(i) ? 1
log(1 + i)
, (15)
where t(i) is the function that represents the reward
given to the aspect at position i, Z is a normaliza-
tion term derived from the top k aspects of a perfect
ranking, so as to normalize NDCG@k to be within
[0, 1]. This evaluation metric will favor the ranking
which ranks the most important aspects at the top.
For the reward t(i), we labeled each aspect as one of
the three scores: Un-important (score 1), Ordinary
(score 2) and Important (score 3). Three volunteers
were invited in the annotation process as follows.
We first collected the top k aspects in all the rank-
ings produced by various evaluated methods (maxi-
mum k is 15 in our experiment). We then sampled
some reviews covering these aspects, and provided
the reviews to each annotator to read. Each review
contains the overall opinion rating, the highlighted
aspects, and opinion terms. Afterward, the annota-
tors were required to assign an importance score to
each aspect. Finally, we took the average of their
scorings as the corresponding importance scores of
the aspects. In addition, there is only one parameter
? that needs to be tuned in our approach. Through-
out the experiments, we empirically set ? as 0.001.
3.2 Evaluations on Aspect Identification
We compared our aspect identification approach
against two baselines: a) the method proposed by
Hu and Liu (2004), which was based on the asso-
ciation rule mining, and b) the method proposed by
Wu et al (2009), which was based on a dependency
parser.
The results are presented in Table 2. On average,
our approach significantly outperforms Hu?s method
and Wu? method in terms of F1-measure by over
5.87% and 3.27%, respectively. In particular, our
approach obtains high precision. Such results imply
that our approach can accurately identify the aspects
from consumer reviews by leveraging the Pros and
Cons reviews.
Data set Hu?s Method Wu?s Method Our Method
Canon EOS 0.681 0.686 0.728
Fujifilm 0.685 0.666 0.710
Panasonic 0.636 0.661 0.706
MacBook 0.680 0.733 0.747
Samsung 0.594 0.631 0.712
iPod Touch 0.650 0.660 0.718
Sony NWZ 0.631 0.692 0.760
BlackBerry 0.721 0.730 0.734
iPhone 3GS 0.697 0.736 0.740
Nokia 5800 0.715 0.745 0.747
Nokia N95 0.700 0.737 0.741
Table 2: Evaluations on Aspect Identification. * signifi-
cant t-test, p-values<0.05.
3.3 Evaluations on Sentiment Classification
In this experiment, we implemented the follow-
ing sentiment classification methods (Pang and Lee,
2008):
1) Unsupervised method. We employed one un-
supervised method which was based on opinion-
ated term counting via SentiWordNet (Ohana et al,
2009).
2) Supervised method. We employed three su-
pervised methods proposed in Pang et al (2002),
including Na??ve Bayes (NB), Maximum Entropy
(ME), SVM. These classifiers were trained based on
the Pros and Cons reviews as described in Section
2.3.
1500
The comparison results are showed in Table 3. We
can see that supervised methods significantly outper-
form unsupervised method. For example, the SVM
classifier outperforms the unsupervised method in
terms of average F1-measure by over 10.37%. Thus,
we can deduce from such results that the Pros and
Cons reviews are useful for sentiment classification.
In addition, among the supervised classifiers, SVM
classifier performs the best in most products, which
is consistent with the previous research (Pang et al,
2002).
Data set Senti NB SVM ME
Canon EOS 0.628 0.720 0.739 0.726
Fujifilm 0.690 0.781 0.791 0.778
Panasonic 0.625 0.694 0.719 0.697
MacBook 0.708 0.820 0.828 0.797
Samsung 0.675 0.723 0.717 0.714
iPod Touch 0.711 0.792 0.805 0.791
Sony NWZ 0.621 0.722 0.737 0.725
BlackBerry 0.699 0.819 0.794 0.788
iPhone 3GS 0.717 0.811 0.829 0.822
Nokia 5800 0.736 0.840 0.851 0.817
Nokia N95 0.706 0.829 0.849 0.826
Table 3: Evaluations on Sentiment Classification. Senti
denotes the method based on SentiWordNet. * significant
t-test, p-values<0.05.
3.4 Evaluations on Aspect Ranking
In this section, we compared our aspect ranking al-
gorithm against the following three methods.
1) Frequency-based method. The method ranks
the aspects based on aspect frequency.
2) Correlation-based method. This method mea-
sures the correlation between the opinions on spe-
cific aspects and the overall opinion. It counts the
number of the cases when such two kinds of opin-
ions are consistent, and ranks the aspects based on
the number of the consistent cases.
3) Hybrid method. This method captures both the
aspect frequency and correlation by a linear combi-
nation, as ?? Frequency-based Ranking + (1 ? ?)?
Correlation-based Ranking, where ? is set to 0.5.
The comparison results are showed in Table 4. On
average, our approach outperforms the frequency-
based method, correlation-based method, and hy-
brid method in terms of NDCG@5 by over 6.24%,
5.79% and 5.56%, respectively. It improves the
performance over such three methods in terms of
NDCG@10 by over 3.47%, 2.94% and 2.58%, re-
spectively, while in terms of NDCG@15 by over
4.08%, 3.04% and 3.49%, respectively. We can de-
duce from the results that our aspect ranking algo-
rithm can effectively identify the important aspects
from consumer reviews by leveraging the aspect fre-
quency and the influence of consumers? opinions
given to each aspect on their overall opinions. Ta-
ble 5 shows the aspect ranking results of these four
methods. Due to the space limitation, we here only
show top 10 aspects of the product iphone 3GS. We
can see that our approach performs better than the
others. For example, the aspect ?phone? is ranked at
the top by the other methods. However, ?phone? is
a general but not important aspect.
# Frequency Correlated Hybrid Our Method
1 Phone Phone Phone Usability
2 Usability Usability Usability Apps
3 3G Apps Apps 3G
4 Apps 3G 3G Battery
5 Camera Camera Camera Looking
6 Feature Looking Looking Storage
7 Looking Feature Feature Price
8 Battery Screen Battery Software
9 Screen Battery Screen Camera
10 Flash Bluetooth Flash Call quality
Table 5: iPhone 3GS Aspect Ranking Results.
To further investigate the reasonability of our
ranking results, we refer to one of the public user
feedback reports, the ?china unicom 100 customers
iPhone user feedback report? (Chinaunicom Report,
2009). The report demonstrates that the top four as-
pects of iPhone product, which users most concern
with, are ?3G Network? (30%), ?usability? (30%),
?out-looking design? (26%), ?application? (15%).
All of these aspects are in the top 10 of our rank-
ing results.
Therefore, we can conclude that our approach is
able to automatically identify the important aspects
from numerous consumer reviews.
4 Applications
The identification of important aspects can support
a wide range of applications. For example, we can
1501
Frequency Correlation Hybrid Our Method
Data set @5 @10 @15 @5 @10 @15 @5 @10 @15 @5 @10 @15
Canon EOS 0.735 0.771 0.740 0.735 0.762 0.779 0.735 0.798 0.742 0.862 0.824 0.794
Fujifilm 0.816 0.705 0.693 0.760 0.756 0.680 0.816 0.759 0.682 0.863 0.801 0.760
Panasonic 0.744 0.807 0.783 0.763 0.815 0.792 0.744 0.804 0.786 0.796 0.834 0.815
MacBook 0.744 0.771 0.762 0.763 0.746 0.769 0.763 0.785 0.772 0.874 0.776 0.760
Samsung 0.964 0.765 0.794 0.964 0.820 0.840 0.964 0.820 0.838 0.968 0.826 0.854
iPod Touch 0.836 0.830 0.727 0.959 0.851 0.744 0.948 0.785 0.733 0.959 0.817 0.801
Sony NWZ 0.937 0.743 0.742 0.937 0.781 0.797 0.937 0.740 0.794 0.944 0.775 0.815
BlackBerry 0.837 0.824 0.766 0.847 0.825 0.771 0.847 0.829 0.768 0.874 0.797 0.779
iPhone 3GS 0.897 0.836 0.832 0.886 0.814 0.825 0.886 0.829 0.826 0.948 0.902 0.860
Nokia 5800 0.834 0.779 0.796 0.834 0.781 0.779 0.834 0.781 0.779 0.903 0.811 0.814
Nokia N95 0.675 0.680 0.717 0.619 0.619 0.691 0.619 0.678 0.696 0.716 0.731 0.748
Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,
and NDCG@15, respectively. * significant t-test, p-values<0.05.
provide product comparison on the important as-
pects to users, so that users can make wise purchase
decisions conveniently.
In the following, we apply the aspect ranking re-
sults to assist document-level review sentiment clas-
sification. Generally, a review document contains
consumer?s positive/negative opinions on various as-
pects of the product. It is difficult to get the ac-
curate overall opinion of the whole review without
knowing the importance of these aspects. In ad-
dition, when we learn a document-level sentiment
classifier, the features generated from unimportant
aspects lack of discriminability and thus may dete-
riorate the performance of the classifier (Fang et al,
2010). While the important aspects and the senti-
ment terms on these aspects can greatly influence the
overall opinions of the review, they are highly likely
to be discriminative features for sentiment classifica-
tion. These observations motivate us to utilize aspect
ranking results to assist classifying the sentiment of
review documents.
Specifically, we randomly sampled 100 reviews of
each product as the testing data and used the remain-
ing reviews as the training data. We first utilized our
approach to identify the importance aspects from the
training data. We then explored the aspect terms and
sentiment terms as features, based on which each re-
view is represented as a feature vector. Here, we
give more emphasis on the important aspects and
the sentiment terms that modify these aspects. In
particular, we set the term-weighting as 1 + ? ??k,
where ?k is the importance score of the aspect ak,
? is set to 100. Based on the weighted features, we
then trained a SVM classifier using the training re-
views to determine the overall opinions on the test-
ing reviews. For the performance comparison, we
compared our approach against two baselines, in-
cluding Boolean weighting method and frequency
weighting (tf ) method (Paltoglou et al, 2010) that
do not utilize the importance of aspects. The com-
parison results are shown in Table 6. We can see
that our approach (IA) significantly outperforms the
other methods in terms of average F1-measure by
over 2.79% and 4.07%, respectively. The results
also show that the Boolean weighting method out-
performs the frequency weighting method in terms
of average F1-measure by over 1.25%, which are
consistent with the previous research by Pang et al
(2002). On the other hand, from the IA weight-
ing formula, we observe that without using the im-
portant aspects, our term-weighting function will be
equal to Boolean weighting. Thus, we can speculate
that the identification of important aspects is ben-
eficial to improving the performance of document-
level sentiment classification.
5 Related Work
Existing researches mainly focused on determining
opinions on the reviews, or identifying aspects from
these reviews. They viewed each aspect equally
without distinguishing the important ones. In this
section, we review existing researches related to our
work.
Analysis of the opinion on whole review text had
1502
SVM + Boolean SVM + tf SVM + IA
Data set P R F1 P R F1 P R F1
Canon EOS 0.689 0.663 0.676 0.679 0.654 0.666 0.704 0.721 0.713
Fujifilm 0.700 0.687 0.693 0.690 0.670 0.680 0.731 0.724 0.727
Panasonic 0.659 0.717 0.687 0.650 0.693 0.671 0.696 0.713 0.705
MacBook 0.744 0.700 0.721 0.768 0.675 0.718 0.790 0.717 0.752
Samsung 0.755 0.690 0.721 0.716 0.725 0.720 0.732 0.765 0.748
iPod Touch 0.686 0.746 0.714 0.718 0.667 0.691 0.749 0.726 0.737
Sony NWZ 0.719 0.652 0.684 0.665 0.646 0.655 0.732 0.684 0.707
BlackBerry 0.763 0.719 0.740 0.752 0.709 0.730 0.782 0.758 0.770
iPhone 3GS 0.777 0.775 0.776 0.772 0.762 0.767 0.820 0.788 0.804
Nokia 5800 0.755 0.836 0.793 0.744 0.815 0.778 0.805 0.821 0.813
Nokia N95 0.722 0.699 0.710 0.695 0.708 0.701 0.768 0.732 0.750
Table 6: Evaluations on Term Weighting methods for Document-level Review Sentiment Classification. IA denotes
the term weighing based on the important aspects. * significant t-test, p-values<0.05.
been extensively studied (Pang and Lee, 2008). Ear-
lier research had been studied unsupervised (Kim et
al., 2004), supervised (Pang et al, 2002; Pang et al,
2005) and semi-supervised approaches (Goldberg et
al., 2006) for the classification. For example, Mullen
et al (2004) proposed an unsupervised classifica-
tion method which exploited pointwise mutual in-
formation (PMI) with syntactic relations and other
attributes. Pang et al (2002) explored several ma-
chine learning classifiers, including Na??ve Bayes,
Maximum Entropy, SVM, for sentiment classifica-
tion. Goldberg et al (2006) classified the sentiment
of the review using the graph-based semi-supervised
learning techniques, while Li el al. (2009) tackled
the problem using matrix factorization techniques
with lexical prior knowledge.
Since the consumer reviews usually expressed
opinions on multiple aspects, some works had
drilled down to the aspect-level sentiment analysis,
which aimed to identify the aspects from the reviews
and to determine the opinions on the specific aspects
instead of the overall opinion. For the topic of aspect
identification, Hu and Liu (2004) presented the asso-
ciation mining method to extract the frequent terms
as the aspects. Subsequently, Popescu et al (2005)
proposed their system OPINE, which extracted the
aspects based on the KnowItAll Web information
extraction system (Etzioni et al, 2005). Liu el al.
(2005) proposed a supervised method based on lan-
guage pattern mining to identify the aspects in the
reviews. Later, Mei et al (2007) proposed a prob-
abilistic topic model to capture the mixture of as-
pects and sentiments simultaneously. Afterwards,
Wu et al (2009) utilized the dependency parser to
extract the noun phrases and verb phrases from the
reviews as the aspect candidates. They then trained
a language model to refine the candidate set, and
to obtain the aspects. On the other hand, for the
topic of sentiment classification on the specific as-
pect, Snyder et al (2007) considered the situation
when the consumers? opinions on one aspect could
influence their opinions on others. They thus built
a graph to analyze the meta-relations between opin-
ions, such as agreement and contrast. And they pro-
posed a Good Grief algorithm to leveraging such
meta-relations to improve the prediction accuracy
of aspect opinion ratings. In addition, Wang et al
(2010) proposed the topic of latent aspect rating
which aimed to infer the opinion rating on the as-
pect. They first employed a bootstrapping-based al-
gorithm to identify the major aspects via a few seed
word aspects. They then proposed a generative La-
tent Rating Regression model (LRR) to infer aspect
opinion ratings based on the review content and the
associated overall rating.
While there were usually huge collection of re-
views, some works had concerned the topic of
aspect-based sentiment summarization to combat
the information overload. They aimed to summa-
rize all the reviews and integrate major opinions on
various aspects for a given product. For example,
Titov et al (2008) explored a topic modeling method
to generate a summary based on multiple aspects.
They utilized topics to describe aspects and incor-
1503
porated a regression model fed by the ground-truth
opinion ratings. Additionally, Lu el al. (2009) pro-
posed a structured PLSA method, which modeled
the dependency structure of terms, to extract the as-
pects in the reviews. They then aggregated opinions
on each specific aspects and selected representative
text segment to generate a summary.
In addition, some works proposed the topic of
product ranking which aimed to identify the best
products for each specific aspect (Zhang et al,
2010). They used a PageRank style algorithm to
mine the aspect-opinion graph, and to rank the prod-
ucts for each aspect.
Different from previous researches, we dedicate
our work to identifying the important aspects from
the consumer reviews of a specific product.
6 Conclusions and Future Works
In this paper, we have proposed to identify the im-
portant aspects of a product from online consumer
reviews. Our assumption is that the important as-
pects of a product should be the aspects that are fre-
quently commented by consumers and consumers?
opinions on the important aspects greatly influence
their overall opinions on the product. Based on this
assumption, we have developed an aspect ranking al-
gorithm to identify the important aspects by simulta-
neously considering the aspect frequency and the in-
fluence of consumers? opinions given to each aspect
on their overall opinions. We have conducted exper-
iments on 11 popular products in four domains. Ex-
perimental results have demonstrated the effective-
ness of our approach on important aspects identifi-
cation. We have further applied the aspect ranking
results to the application of document-level senti-
ment classification, and have significantly improved
the classification performance. In the future, we will
apply our approach to support other applications.
Acknowledgments
This work is supported in part by NUS-Tsinghua Ex-
treme Search (NExT) project under the grant num-
ber: R-252-300-001-490. We give warm thanks to
the project and anonymous reviewers for their com-
ments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
G. Carenini, R.T. Ng, and E. Zwart. Extracting Knowl-
edge from Evaluative Text. K-CAP, 2005.
G. Carenini, R.T. Ng, and E. Zwart. Multi-document
Summarization of Evaluative Text. ACL, 2006.
China Unicom 100 Customers iPhone User Feedback
Report, 2009.
Y. Choi and C. Cardie. Hierarchical Sequential Learning
for Extracting Opinions and Their Attributes. ACL,
2010.
H. Cui, V. Mittal, and M. Datar. Comparative Experi-
ments on Sentiment Classification for Online Product
Reviews. AAAI, 2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
K. Dave, S. Lawrence, and D.M. Pennock. Opinion Ex-
traction and Semantic Classification of Product Re-
views. WWW, 2003.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
J. Fang, B. Price, and L. Price. Pruning Non-Informative
Text Through Non-Expert Annotations to Improve
Aspect-Level Sentiment Classification. COLING,
2010.
O. Feiguina and G. Lapalme. Query-based Summariza-
tion of Customer Reviews. AI, 2007.
Forrester Research. State of Retailing Online 2009: Mar-
keting Report. http://www.shop.org/soro, 2009.
A. Goldberg and X. Zhu. Seeing Stars when There aren?t
Many Stars: Graph-based Semi-supervised Learning
for Sentiment Categorization. ACL, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
K. Jarvelin and J. Kekalainen. Cumulated Gain-based
Evaluation of IR Techniques. TOIS, 2002.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
J. Kim, J.J. Li, and J.H. Lee. Discovering the Discrimi-
native Views: Measuring Term Weights for Sentiment
Analysis. ACL, 2009.
1504
Kelsey Research and comscore. Online Consumer-
Generated Reviews Have Significant Impact on Offline
Purchase Behavior.
K. Lerman, S. Blair-Goldensohn, and R. McDonald.
Sentiment Summarization: Evaluating and Learning
User Preferences. EACL, 2009.
B. Li, L. Zhou, S. Feng, and K.F. Wong. A Unified Graph
Model for Sentence-Based Opinion Retrieval. ACL,
2010.
T. Li and Y. Zhang, and V. Sindhwani. A Non-negative
Matrix Tri-factorization Approach to Sentiment Clas-
sification with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
Y. Lu, C. Zhai, and N. Sundaresan. Rated Aspect Sum-
marization of Short Comments. WWW, 2009.
L.M. Manevitz and M. Yousef. One-class svms for Doc-
ument Classification. The Journal of Machine Learn-
ing, 2002.
R. McDonal, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. Structured Models for Fine-to-coarse Sen-
timent Analysis. ACL, 2007.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
H.J. Min and J.C. Park. Toward Finer-grained Sentiment
Identification in Product Reviews Through Linguistic
and Ontological Analyses. ACL, 2009.
T. Mullen and N. Collier. Sentiment Analysis using
Support Vector Machines with Diverse Information
Sources. EMNLP, 2004.
N. Nanas, V. Uren, and A.D. Roeck. Building and Ap-
plying a Concept Hierarchy Representation of a User
Profile. SIGIR, 2003.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Ohana and B. Tierney. Sentiment Classification of Re-
views Using SentiWordNet. IT&T Conference, 2009.
G. Paltoglou and M. Thelwall. A study of Information
Retrieval Weighting Schemes for Sentiment Analysis.
ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang, L. Lee, and S. Vaithyanathan. A Sentimen-
tal Education: Sentiment Analysis using Subjectivity
Summarization based on Minimum cuts Techniques.
ACL, 2004.
B. Pang and L. Lee. Seeing stars: Exploiting Class Re-
lationships for Sentiment Categorization with Respect
to Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment
analysis. Foundations and Trends in Information Re-
trieval, 2008.
A.-M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
R. Prabowo and M. Thelwall. Sentiment analysis: A
Combined Approach. Journal of Informetrics, 2009.
G. Qiu, B. Liu, J. Bu, and C. Chen.. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
M. Sanderson and B. Croft. Document-word Co-
regularization for Semi-supervised Sentiment Analy-
sis. ICDM, 2008.
B. Snyder and R. Barzilay. Multiple Aspect Ranking us-
ing the Good Grief Algorithm. NAACL HLT, 2007.
S. Somasundaran, G. Namata, L. Getoor, and J. Wiebe.
Opinion Graphs for Polarity and Discourse Classifica-
tion. ACL, 2009.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
C. Toprak, N. Jakob, and I. Gurevych. Sentence and
Expression Level Annotation of Opinions in User-
Generated Discourse. ACL, 2010.
P. Turney. Thumbs up or Thumbs down? Semantic Ori-
entation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
H. Wang, Y. Lu, and C.X. Zhai. Latent Aspect Rating
Analysis on Review Text Data: A Rating Regression
Approach. KDD, 2010.
B. Wei and C. Pal. Cross Lingual Adaptation: An Exper-
iment on Sentiment Classifications. ACL, 2010.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
T. Wilson and J. Wiebe. Annotating Attributions and Pri-
vate States. ACL, 2005.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
K. Zhang, R. Narayanan, and A. Choudhary. Voice of
the Customers: Mining Online Customer Reviews for
Product Feature-based Ranking. WOSN, 2010.
J. Zhu, H. Wang, and B.K. Tsou. Aspect-based Sentence
Segmentation for Sentiment Summarization. TSA,
2009.
L. Zhuang, F. Jing, and X.Y. Zhu. Movie Review Mining
and Summarization. CIKM, 2006.
1505
