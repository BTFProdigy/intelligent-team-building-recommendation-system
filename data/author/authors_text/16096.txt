Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 825?834,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Big Data versus the Crowd:
Looking for Relationships in All the Right Places
Ce Zhang Feng Niu Christopher Re? Jude Shavlik
Department of Computer Sciences
University of Wisconsin-Madison, USA
{czhang,leonn,chrisre,shavlik}@cs.wisc.edu
Abstract
Classically, training relation extractors relies
on high-quality, manually annotated training
data, which can be expensive to obtain. To
mitigate this cost, NLU researchers have con-
sidered two newly available sources of less
expensive (but potentially lower quality) la-
beled data from distant supervision and crowd
sourcing. There is, however, no study com-
paring the relative impact of these two sources
on the precision and recall of post-learning an-
swers. To fill this gap, we empirically study
how state-of-the-art techniques are affected by
scaling these two sources. We use corpus sizes
of up to 100 million documents and tens of
thousands of crowd-source labeled examples.
Our experiments show that increasing the cor-
pus size for distant supervision has a statis-
tically significant, positive impact on quality
(F1 score). In contrast, human feedback has a
positive and statistically significant, but lower,
impact on precision and recall.
1 Introduction
Relation extraction is the problem of populating a
target relation (representing an entity-level relation-
ship or attribute) with facts extracted from natural-
language text. Sample relations include people?s ti-
tles, birth places, and marriage relationships.
Traditional relation-extraction systems rely on
manual annotations or domain-specific rules pro-
vided by experts, both of which are scarce re-
sources that are not portable across domains. To
remedy these problems, recent years have seen in-
terest in the distant supervision approach for rela-
tion extraction (Wu and Weld, 2007; Mintz et al,
2009). The input to distant supervision is a set of
seed facts for the target relation together with an
(unlabeled) text corpus, and the output is a set of
(noisy) annotations that can be used by any ma-
chine learning technique to train a statistical model
for the target relation. For example, given the tar-
get relation birthPlace(person, place) and a seed
fact birthPlace(John, Springfield), the sentence
?John and his wife were born in Springfield in 1946?
(S1) would qualify as a positive training example.
Distant supervision replaces the expensive pro-
cess of manually acquiring annotations that is re-
quired by direct supervision with resources that al-
ready exist in many scenarios (seed facts and a
text corpus). On the other hand, distantly labeled
data may not be as accurate as manual annotations.
For example, ?John left Springfield when he was
16? (S2) would also be considered a positive ex-
ample about place of birth by distant supervision
as it contains both John and Springfield. The hy-
pothesis is that the broad coverage and high redun-
dancy in a large corpus would compensate for this
noise. For example, with a large enough corpus, a
distant supervision system may find that patterns in
the sentence S1 strongly correlate with seed facts of
birthPlace whereas patterns in S2 do not qualify
as a strong indicator. Thus, intuitively the quality of
distant supervision should improve as we use larger
corpora. However, there has been no study on the
impact of corpus size on distant supervision for re-
lation extraction. Our goal is to fill this gap.
Besides ?big data,? another resource that may
be valuable to distant supervision is crowdsourc-
825
ing. For example, one could employ crowd work-
ers to provide feedback on whether distant super-
vision examples are correct or not (Gormley et al,
2010). Intuitively the crowd workforce is a perfect
fit for such tasks since many erroneous distant la-
bels could be easily identified and corrected by hu-
mans. For example, distant supervision may mistak-
enly consider ?Obama took a vacation in Hawaii? a
positive example for birthPlace simply because
a database says that Obama was born in Hawaii;
a crowd worker would correctly point out that this
sentence is not actually indicative of this relation.
It is unclear however which strategy one should
use: scaling the text corpus or the amount of human
feedback. Our primary contribution is to empirically
assess how scaling these inputs to distant supervi-
sion impacts its result quality. We study this ques-
tion with input data sets that are orders of magnitude
larger than those in prior work. While the largest
corpus (Wikipedia and New York Times) employed
by recent work on distant supervision (Mintz et al,
2009; Yao et al, 2010; Hoffmann et al, 2011) con-
tain about 2M documents, we run experiments on
a 100M-document (50X more) corpus drawn from
ClueWeb.1 While prior work (Gormley et al, 2010)
on crowdsourcing for distant supervision used thou-
sands of human feedback units, we acquire tens of
thousands of human-provided labels. Despite the
large scale, we follow state-of-the-art distant super-
vision approaches and use deep linguistic features,
e.g., part-of-speech tags and dependency parsing.2
Our experiments shed insight on the following
two questions:
1. How does increasing the corpus size impact the
quality of distant supervision?
2. For a given corpus size, how does increasing
the amount of human feedback impact the qual-
ity of distant supervision?
We found that increasing corpus size consistently
and significantly improves recall and F1, despite re-
ducing precision on small corpora; in contrast, hu-
man feedback has relatively small impact on preci-
sion and recall. For example, on a TAC corpus with
1.8M documents, we found that increasing the cor-
pus size ten-fold consistently results in statistically
1http://lemurproject.org/clueweb09.php/
2We used 100K CPU hours to run such tools on ClueWeb.
significant improvement in F1 on two standardized
relation extraction metrics (t-test with p=0.05). On
the other hand, increasing human feedback amount
ten-fold results in statistically significant improve-
ment on F1 only when the corpus contains at least
1M documents; and the magnitude of such improve-
ment was only one fifth compared to the impact of
corpus-size increment.
We find that the quality of distant supervision
tends to be recall gated, that is, for any given rela-
tion, distant supervision fails to find all possible lin-
guistic signals that indicate a relation. By expanding
the corpus one can expand the number of patterns
that occur with a known set of entities. Thus, as a
rule of thumb for developing distant supervision sys-
tems, one should first attempt to expand the training
corpus and then worry about precision of labels only
after having obtained a broad-coverage corpus.
Throughout this paper, it is important to under-
stand the difference between mentions and entities.
Entities are conceptual objects that exist in the world
(e.g., Barack Obama), whereas authors use a variety
of wordings to refer to (which we call ?mention?)
entities in text (Ji et al, 2010).
2 Related Work
The idea of using entity-level structured data (e.g.,
facts in a database) to generate mention-level train-
ing data (e.g., in English text) is a classic one: re-
searchers have used variants of this idea to extract
entities of a certain type from webpages (Hearst,
1992; Brin, 1999). More closely related to relation
extraction is the work of Lin and Patel (2001) that
uses dependency paths to find answers that express
the same relation as in a question.
Since Mintz et al (2009) coined the name ?dis-
tant supervision,? there has been growing interest in
this technique. For example, distant supervision has
been used for the TAC-KBP slot-filling tasks (Sur-
deanu et al, 2010) and other relation-extraction
tasks (Hoffmann et al, 2010; Carlson et al, 2010;
Nguyen and Moschitti, 2011a; Nguyen and Mos-
chitti, 2011b). In contrast, we study how increas-
ing input size (and incorporating human feedback)
improves the result quality of distant supervision.
We focus on logistic regression, but it is interest-
ing future work to study more sophisticated prob-
826
Training ?Corpus?
Testing ?Corpus?
1. Parsing, Entity Linking?
Training?
Testing?
Raw Text? Structured Text?w/ Entity Mentions?
2. Distant Supervision? Statistical ?Models?
Refined ?Statistical ?Models?
Relation Extractors?
3. Human ?Feedback???????4. Apply & Evaluate?
Knowledge-base ?Entities?
Knowledge-base ?Relations?
Figure 1: The workflow of our distant supervision system. Step 1 is preprocessing; step 4 is final evaluation. The key
steps are distant supervision (step 2), where we train a logistic regression (LR) classifier for each relation using (noisy)
examples obtained from sentences that match Freebase facts, and human feedback (step 3) where a crowd workforce
refines the LR classifiers by providing feedback to the training data.
abilistic models; such models have recently been
used to relax various assumptions of distant supervi-
sion (Riedel et al, 2010; Yao et al, 2010; Hoffmann
et al, 2011). Specifically, they address the noisy as-
sumption that, if two entities participate in a rela-
tion in a knowledge base, then all co-occurrences of
these entities express this relation. In contrast, we
explore the effectiveness of increasing the training
data sizes to improve distant-supervision quality.
Sheng et al (2008) and Gormley et al (2010)
study the quality-control issue for collecting train-
ing labels via crowdsourcing. Their focus is the col-
lection process; in contrast, our goal is to quantify
the impact of this additional data source on distant-
supervision quality. Moreover, we experiment with
one order of magnitude more human labels. Hoff-
mann et al (2009) study how to acquire end-user
feedback on relation-extraction results posted on an
augmented Wikipedia site; it is interesting future
work to integrate this source in our experiments.
One technique for obtaining human input is active
learning. We tried several active-learning techniques
as described by Settles (2010), but did not observe
any notable advantage over uniform sampling-based
example selection.3
3 Distant Supervision Methodology
Relation extraction is the task of identifying re-
lationships between mentions, in natural-language
text, of entities. An example relation is that two per-
sons are married, which for mentions of entities x
and y is denoted R(x, y). Given a corpus C con-
3More details in our technical report (Zhang et al, 2012).
taining mentions of named entities, our goal is to
learn a classifier for R(x, y) using linguistic features
of x and y, e.g., dependency-path information. The
problem is that we lack the large amount of labeled
examples that are typically required to apply super-
vised learning techniques. We describe an overview
of these techniques and the methodological choices
we made to implement our study. Figure 1 illus-
trates the overall workflow of a distant supervision
system. At each step of the distant supervision pro-
cess, we closely follow the recent literature (Mintz
et al, 2009; Yao et al, 2010).
3.1 Distant Supervision
Distant supervision compensates for a lack of train-
ing examples by generating what are known as
silver-standard examples (Wu and Weld, 2007). The
observation is that we are often able to obtain a
structured, but incomplete, database D that instanti-
ates relations of interest and a text corpus C that con-
tains mentions of the entities in our database. For-
mally, a database is a tuple D = (E, R?) where E is
a set of entities and R? = (R1 . . . , RN ) is a tuple of
instantiated predicates. For example, Ri may con-
tain pairs of married people.4 We use the facts in Ri
combined with C to generate examples.
Following recent work (Mintz et al, 2009; Yao et
al., 2010; Hoffmann et al, 2011), we use Freebase5
as the knowledge base for seed facts. We use two
text corpora: (1) the TAC-KBP6 2010 corpus that
4We only consider binary predicates in this work.
5http://freebase.com
6KBP stands for ?Knowledge-Base Population.?
827
consists of 1.8M newswire and blog articles7, and
(2) the ClueWeb09 corpus that is a 2009 snapshot
of 500M webpages. We use the TAC-KBP slot fill-
ing task and select those TAC-KBP relations that are
present in the Freebase schema as targets (20 rela-
tions on people and organization).
One problem is that relations in D are defined at
the entity level. Thus, the pairs in such relations are
not embedded in text, and so these pairs lack the
linguistic context that we need to extract features,
i.e., the features used to describe examples. In turn,
this implies that these pairs cannot be used directly
as training examples for our classifier. To generate
training examples, we need to map the entities back
to mentions in the corpus. We denote the relation
that describes this mapping as the relation EL(e,m)
where e ? E is an entity in the database D and m is
a mention in the corpus C. For each relation Ri, we
generate a set of (noisy) positive examples denoted
R+i defined as R
+
i =
{(m1,m2) | R(e1, e2) ? EL(e1,m1) ? EL(e2,m2)}
As in previous work, we impose the constraint that
both mentions (m1,m2) ? R
+
i are contained in the
same sentence (Mintz et al, 2009; Yao et al, 2010;
Hoffmann et al, 2011). To generate negative ex-
amples for each relation, we follow the assumption
in Mintz et al (2009) that relations are disjoint and
sample from other relations, i.e., R?i = ?j 6=iR
+
j .
3.2 Feature Extraction
Once we have constructed the set of possible men-
tion pairs, the state-of-the-art technique to generate
feature vectors uses linguistic tools such as part-
of-speech taggers, named-entity recognizers, de-
pendency parsers, and string features. Following
recent work on distant supervision (Mintz et al,
2009; Yao et al, 2010; Hoffmann et al, 2011),
we use both lexical and syntactic features. After
this stage, we have a well-defined machine learn-
ing problem that is solvable using standard super-
vised techniques. We use sparse logistic regression
(`1 regularized) (Tibshirani, 1996), which is used in
previous studies. Our feature extraction process con-
sists of three steps:
7http://nlp.cs.qc.cuny.edu/kbp/2010/
1. Run Stanford CoreNLP with POS tagging and
named entity recognition (Finkel et al, 2005);
2. Run dependency parsing on TAC with the En-
semble parser (Surdeanu and Manning, 2010)
and on ClueWeb with MaltParser (Nivre et al,
2007)8; and
3. Run a simple entity-linking system that utilizes
NER results and string matching to identify
mentions of Freebase entities (with types).9
The output of this processing is a repository of struc-
tured objects (with POS tags, dependency parse, and
entity types and mentions) for sentences from the
training corpus. Specifically, for each pair of entity
mentions (m1,m2) in a sentence, we extract the fol-
lowing features F (m1,m2): (1) the word sequence
(including POS tags) between these mentions after
normalizing entity mentions (e.g., replacing ?John
Nolen? with a place holder PER); if the sequence
is longer than 6, we take the 3-word prefix and the
3-word suffix; (2) the dependency path between the
mention pair. To normalize, in both features we use
lemmas instead of surface forms. We discard fea-
tures that occur in fewer than three mention pairs.
3.3 Crowd-Sourced Data
Crowd sourcing provides a cheap source of human
labeling to improve the quality of our classifier. In
this work, we specifically examine feedback on the
result of distant supervision. Precisely, we construct
the union of R+1 ? . . . R
+
N from Section 3.1. We
then solicit human labeling from Mechanical Turk
(MTurk) while applying state-of-the-art quality con-
trol protocols following Gormley et al (2010) and
those in the MTurk manual.10
These quality-control protocols are critical to en-
sure high quality: spamming is common on MTurk
and some turkers may not be as proficient or care-
ful as expected. To combat this, we replicate
each question three times and, following Gormley
8We did not run Ensemble on ClueWeb because we had very
few machines satisfying Ensemble?s memory requirement. In
contrast, MaltParser requires less memory and we could lever-
age Condor (Thain et al, 2005) to parse ClueWeb with Malt-
Parser within several days (using about 50K CPU hours).
9We experiment with a slightly more sophisticated entity-
linking system as well, which resulted in higher overall quality.
The results below are from the simple entity-linking system.
10http://mturkpublic.s3.amazonaws.com/docs/
MTURK_BP.pdf
828
et al (2010), plant gold-standard questions: each
task consists of five yes/no questions, one of which
comes from our gold-standard pool.11 By retaining
only those answers that are consistent with this pro-
tocol, we are able to filter responses that were not
answered with care or competency. We only use an-
swers from workers who display overall high consis-
tency with the gold standard (i.e., correctly answer-
ing at least 80% of the gold-standard questions).
3.4 Statistical Modeling Issues
Following Mintz et al (2009), we use logistic re-
gression classifiers to represent relation extractors.
However, while Mintz et al use a single multi-class
classifier for all relations, Hoffman et al (2011) and
use an independent binary classifier for each individ-
ual relation; the intuition is that a pair of mentions
(or entities) might participate in multiple target rela-
tions. We experimented with both protocols; since
relation overlapping is rare for TAC-KBP and there
was little difference in result quality, we focus on the
binary-classification approach using training exam-
ples constructed as described in Section 3.1.
We compensate for the different sizes of distant
and human labeled examples by training an objec-
tive function that allows to tune the weight of human
versus distant labeling. We separately tune this pa-
rameter for each training set (with cross validation),
but found that the result quality was robust with re-
spect to a broad range of parameter values.12
4 Experiments
We describe our experiments to test the hypothe-
ses that the following two factors improve distant-
supervision quality: increasing the
(1) corpus size, and
(2) the amount of crowd-sourced feedback.
We confirm hypothesis (1), but, surprisingly, are un-
able to confirm (2). Specifically, when using logis-
tic regression to train relation extractors, increasing
corpus size improves, consistently and significantly,
the precision and recall produced by distant supervi-
sion, regardless of human feedback levels. Using the
11We obtain the gold standard from a separate MTurk sub-
mission by taking examples that at least 10 out of 11 turkers
answered yes, and then negate half of these examples by alter-
ing the relation names (e.g., spouse to sibling).
12More details in our technical report (Zhang et al, 2012).
methodology described in Section 3, human feed-
back has limited impact on the precision and recall
produced from distant supervision by itself.
4.1 Evaluation Metrics
Just as direct training data are scarce, ground truth
for relation extraction is scarce as well. As a result,
prior work mainly considers two types of evaluation
methods: (1) randomly sample a small portion of
predictions (e.g., top-k) and manually evaluate pre-
cision/recall; and (2) use a held-out portion of seed
facts (usually Freebase) as a kind of ?distant? ground
truth. We replace manual evaluation with a stan-
dardized relation-extraction benchmark: TAC-KBP
2010. TAC-KBP asks for extractions of 46 relations
on a given set of 100 entities. Interestingly, the Free-
base held-out metric (Mintz et al, 2009; Yao et al,
2010; Hoffmann et al, 2011) turns out to be heavily
biased toward distantly labeled data (e.g., increasing
human feedback hurts precision; see Section 4.6).
4.2 Experimental Setup
Our first group of experiments use the 1.8M-doc
TAC-KBP corpus for training. We exclude from it
the 33K documents that contain query entities in
the TAC-KBP metrics. There are two key param-
eters: the corpus size (#docs) M and human feed-
back budget (#examples) N . We perform different
levels of down-sampling on the training corpus. On
TAC, we use subsets with M = 103, 104, 105, and
106 documents respectively. For each value of M ,
we perform 30 independent trials of uniform sam-
pling, with each trial resulting in a training corpus
DMi , 1 ? i ? 30. For each training corpus D
M
i , we
perform distant supervision to train a set of logistic
regression classifiers. From the full corpus, distant
supervision creates around 72K training examples.
To evaluate the impact of human feedback, we
randomly sample 20K examples from the input cor-
pus (we remove any portion of the corpus that is
used in an evaluation). Then, we ask three differ-
ent crowd workers to label each example as either
positive or negative using the procedure described in
Section 3.3. We retain only credible answers using
the gold-standard method (see Section 3.3), and use
them as the pool of human feedback that we run ex-
periments with. About 46% of our human labels are
negative. Denote by N the number of examples that
829
Figure 2: Impact of input sizes under the TAC-KBP metric, which uses documents mentioning 100 predefined entities
as testing corpus with entity-level ground truth. We vary the sizes of the training corpus and human feedback while
measuring the scores (F1, recall, and precision) on the TAC-KBP benchmark.
we want to incorporate human feedback for; we vary
N in the range of 0, 10, 102, 103, 104, and 2 ? 104.
For each selected corpus and value of N , we per-
form without-replacement sampling from examples
of this corpus to select feedback for up to N exam-
ples. In our experiments, we found that on aver-
age an M -doc corpus contains about 0.04M distant
labels, out of which 0.01M have human feedback.
After incorporating human feedback, we evaluate
the relation extractors on the TAC-KBP benchmark.
We then compute the average F1, recall, and preci-
sion scores among all trials for each metric and each
(M,N) pair. Besides the KBP metrics, we also eval-
uate each (M,N) pair using Freebase held-out data.
Furthermore, we experiment with a much larger cor-
pus: ClueWeb09. On ClueWeb09, we vary M over
103, . . . , 108. Using the same metrics, we show at
a larger scale that increasing corpus size can signifi-
cantly improve both precision and recall.
4.3 Overall Impact of Input Sizes
We first present our experiment results on the TAC
corpus. As shown in Figure 2, the F1 graph closely
tracks the recall graph, which supports our earlier
claim that quality is recall gated (Section 1). While
increasing the corpus size improves F1 at a roughly
log-linear rate, human feedback has little impact un-
til both corpus size and human feedback size ap-
proch maximum M,N values. Table 1 shows the
quality comparisons with minimum/maximum val-
ues of M and N .13 We observe that increasing the
corpus size significant improves per-relation recall
13When the corpus size is small, the total number of exam-
ples with feedback can be smaller than the budget size N ? for
example, when M = 103 there are on average 10 examples
with feedback even if N = 104.
M = 103 M = 1.8? 106
N = 0 0.124 0.201
N = 2? 104 0.118 0.214
Table 1: TAC F1 scores with max/min values of M /N .
and F1 on 17 out of TAC-KBP?s 20 relations; in con-
trast, human feedback has little impact on recall, and
only significantly improves the precision and F1 of
9 relations ? while hurting F1 of 2 relations (i.e.,
MemberOf and LivesInCountry).14
(a) Impact of corpus size changes.
M\N 0 10 102 103 104 2e4
103 ? 104 + + + + + +
104 ? 105 + + + + + +
105 ? 106 + + + + + +
106 ? 1.8e6 0 0 0 + + +
(b) Impact of feedback size changes.
N\M 103 104 105 106 1.8e6
0? 10 0 0 0 0 0
10? 102 0 0 0 + +
102 ? 103 0 0 0 + +
103 ? 104 0 0 0 0 +
104 ? 2e4 0 0 0 0 -
0? 2e4 0 0 0 + +
Table 2: Two-tail t-test with d.f.=29 and p=0.05 on the
impact of corpus size and feedback size changes respec-
tively. (We also tried p=0.01, which resulted in change
of only a single cell in the two tables.) In (a), each col-
umn corresponds to a fixed human-feedback budget size
N . Each row corresponds to a jump from one corpus size
(M ) to the immediate larger size. Each cell value indi-
cates whether the TAC F1 metric changed significantly:
+ (resp. -) indicates that the quality increased (resp. de-
creased) significantly; 0 indicates that the quality did not
change significantly. Table (b) is similar.
14We report more details on per-relation quality in our tech-
nical report (Zhang et al, 2012).
830
(a) Impact of corpus size changes.
(b) Impact of human feedback size.
Figure 3: Projections of Figure 2 to show the impact of corpus size and human feedback amount on TAC-KBP F1,
recall, and precision.
4.4 Impact of Corpus Size
In Figure 3(a) we plot a projection of the graphs
in Figure 2 to show the impact of corpus size on
distant-supervision quality. The two curves corre-
spond to when there is no human feedback and when
we use all applicable human feedback. The fact
that the two curves almost overlap indicates that hu-
man feedback had little impact on precision or re-
call. On the other hand, the quality improvement
rate is roughly log-linear against the corpus size.
Recall that each data point in Figure 2 is the aver-
age from 30 trials. To measure the statistical signif-
icance of changes in F1, we calculate t-test results
to compare adjacent corpus size levels given each
fixed human feedback level. As shown in Table 2(a),
increasing the corpus size by a factor of 10 consis-
tently and significantly improves F1. Although pre-
cision decreases as we use larger corpora, the de-
creasing trend is sub-log-linear and stops at around
100K docs. On the other hand, recall and F1 keep
increasing at a log-linear rate.
4.5 Impact of Human Feedback
Figure 3(b) provides another perspective on the re-
sults under the TAC metric: We fix a corpus size
and plot the F1, recall, and precision as functions
of human-feedback amount. Confirming the trend
in Figure 2, we see that human feedback has little
Figure 4: TAC-KBP quality of relation extractors trained
using different amounts of human labels. The horizontal
lines are comparison points.
impact on precision or recall with both corpus sizes.
We calculate t-tests to compare adjacent human
feedback levels given each fixed corpus size level.
Table 2(b)?s last row reports the comparison, for var-
ious corpus sizes (and, hence, number of distant la-
bels), of (i) using no human feedback and (ii) using
all of the human feedback we collected. When the
corpus size is small (fewer than 105 docs), human
feedback has no statistically significant impact on
F1. The locations of +?s suggest that the influence
of human feedback becomes notable only when the
corpus is very large (say with 106 docs). However,
comparing the slopes of the curves in Figure 3(b)
against Figure 3(a), the impact of human feedback
is substantially smaller. The precision graph in Fig-
ure 3(b) suggests that human feedback does not no-
831
Figure 5: Impact of input sizes under the Freebase held-
out metric. Note that the human feedback axis is in the
reverse order compared to Figure 2.
tably improve precision on either the full corpus or
on a small 1K-doc corpus. To assess the quality of
human labels, we train extraction models with hu-
man labels only (on examples obtained from distant
supervision). We vary the amount of human labels
and plot the F1 changes in Figure 4. Although the
F1 improves as we use more human labels, the best
model has roughly the same performance as those
trained from distant labels (with or without human
labels). This suggests that the accuracy of human
labels is not substantially better than distant labels.
4.6 Freebase Held-out Metric
In addition to the TAC-KBP benchmark, we also fol-
low prior work (Mintz et al, 2009; Yao et al, 2010;
Hoffmann et al, 2011) and measure the quality us-
ing held-out data from Freebase. We randomly par-
tition both Freebase and the corpus into two halves.
One database-corpus pair is used for training and the
other pair for testing. We evaluate the precision over
the 103 highest-probability predictions on the test
set. In Figure 5, we vary the size of the corpus in the
train pair and the number of human labels; the pre-
cision reaches a dramatic peak when we the corpus
size is above 105 and uses little human feedback.
This suggests that this Freebase held-out metric is
biased toward solely relying on distant labels alone.
4.7 Web-scale Corpora
To study how a Web corpus impacts distant-
supervision quality, we select the first 100M English
webpages from the ClueWeb09 dataset and measure
how distant-supervision quality changes as we vary
the number of webpages used. As shown in Fig-
ure 6, increasing the corpus size improves F1 up to
Figure 6: Impact of corpus size on the TAC-KBP quality
with the ClueWeb dataset.
107 docs (p = 0.05), while at 108 the two-tailed
significance test reports no significant impact on F1
(p = 0.05). The dip in precision in Figure 6 from
106 to either 107 or 108 is significant (p = 0.05),
and it is interesting future work to perform a de-
tailed error analysis. Recall from Section 3 that to
preprocess ClueWeb we use MaltParser instead of
Ensemble. Thus, the F1 scores in Figure 6 are not
comparable to those from the TAC training corpus.
5 Discussion and Conclusion
We study how the size of two types of cheaply avail-
able resources impact the precision and recall of dis-
tant supervision: (1) an unlabeled text corpus from
which distantly labeled training examples can be ex-
tracted, and (2) crowd-sourced labels on training
examples. We found that text corpus size has a
stronger impact on precision and recall than human
feedback. We observed that distant-supervision sys-
tems are often recall gated; thus, to improve distant-
supervision quality, one should first try to enlarge
the input training corpus and then increase precision.
It was initially counter-intuitive to us that human
labels did not have a large impact on precision. One
reason is that human labels acquired from crowd-
sourcing have comparable noise level as distant la-
bels ? as shown by Figure 4. Thus, techniques that
improve the accuracy of crowd-sourced answers are
an interesting direction for future work. We used a
particular form of human input (yes/no votes on dis-
tant labels) and a particular statistical model to in-
corporate this information (logistic regression). It
is interesting future work to study other types of
human input (e.g., new examples or features) and
more sophisticated techniques for incorporating hu-
man input, as well as machine learning methods that
explicitly model feature interactions.
832
Acknowledgements
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or
the US government. We are thankful for the gen-
erous support from the Center for High Through-
put Computing, the Open Science Grid, and Miron
Livny?s Condor research group at UW-Madison. We
are also grateful to Dan Weld for his insightful com-
ments on the manuscript.
References
S. Brin. 1999. Extracting patterns and relations from the
world wide web. In Proceedings of The World Wide
Web and Databases, pages 172?183.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hr-
uschka Jr, and T. Mitchell. 2010. Toward an architec-
ture for never-ending language learning. In Proceed-
ings of the Conference on Artificial Intelligence, pages
1306?1313.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorpo-
rating non-local information into information extrac-
tion systems by Gibbs sampling. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 363?370.
M. Gormley, A. Gerber, M. Harper, and M. Dredze.
2010. Non-expert correction of automatically gen-
erated relation annotations. In Proceedings of the
NAACL HLT Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, pages
204?207.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
Conference on Computational Linguistics-Volume 2,
pages 539?545.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Fogarty,
and D.S. Weld. 2009. Amplifying community con-
tent creation with mixed initiative information extrac-
tion. In Proceedings of the 27th international confer-
ence on Human factors in computing systems, pages
1849?1858. ACM.
R. Hoffmann, C. Zhang, and D. Weld. 2010. Learn-
ing 5000 relational extractors. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 286?295.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D. Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 541?550.
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis.
2010. Overview of the TAC 2010 knowledge base
population track. In Text Analysis Conference.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics, pages 1003?
1011.
T.V.T. Nguyen and A. Moschitti. 2011a. End-to-end re-
lation extraction using distant supervision from exter-
nal semantic repositories. In Proceeding of the Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 277?282.
T.V.T. Nguyen and A. Moschitti. 2011b. Joint distant and
direct supervision for relation extraction. In Proceed-
ing of the International Joint Conference on Natural
Language Processing, pages 732?740.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02):95?135.
S. Riedel, L. Yao, and A. McCallum. 2010. Modeling
relations and their mentions without labeled text. In
Proceedings of the European Conference on Machine
Learning and Knowledge Discovery in Databases:
Part III, pages 148?163.
B. Settles. 2010. Active learning literature survey. Tech-
nical report, Computer Sciences Department, Univer-
sity of Wisconsin-Madison, USA.
V.S. Sheng, F. Provost, and P.G. Ipeirotis. 2008. Get
another label? Improving data quality and data min-
ing using multiple, noisy labelers. In Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 614?
622.
M. Surdeanu and C. Manning. 2010. Ensemble models
for dependency parsing: Cheap and good? In Hu-
man Language Technologies: The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 649?652.
M. Surdeanu, D. McClosky, J. Tibshirani, J. Bauer, A.X.
Chang, V.I. Spitkovsky, and C. Manning. 2010. A
simple distant supervision approach for the TAC-KBP
slot filling task. In Proceedings of Text Analysis Con-
ference 2010 Workshop.
833
D. Thain, T. Tannenbaum, and M. Livny. 2005. Dis-
tributed computing in practice: The Condor experi-
ence. Concurrency and Computation: Practice and
Experience, 17(2-4):323?356.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), pages 267?288.
F. Wu and D. Weld. 2007. Autonomously semantifying
wikipedia. In ACM Conference on Information and
Knowledge Management, pages 41?50.
L. Yao, S. Riedel, and A. McCallum. 2010. Collective
cross-document relation extraction without labelled
data. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1013?1023.
C. Zhang, F. Niu, C. Re?, and J. Shavlik. 2012. Big
data versus the crowd: Looking for relationships in
all the right places (extended version). Technical re-
port, Computer Sciences Department, University of
Wisconsin-Madison, USA.
834
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 658?664,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Understanding Tables in Context Using Standard NLP Toolkits
Vidhya Govindaraju Ce Zhang Christopher Re?
University of Wisconsin-Madison
{vidhya, czhang, chrisre}@cs.wisc.edu
Abstract
Tabular information in text documents
contains a wealth of information, and
so tables are a natural candidate for in-
formation extraction. There are many
cues buried in both a table and its sur-
rounding text that allow us to under-
stand the meaning of the data in a ta-
ble. We study how natural-language
tools, such as part-of-speech tagging,
dependency paths, and named-entity
recognition, can be used to improve the
quality of relation extraction from ta-
bles. In three domains we show that (1)
a model that performs joint probabilis-
tic inference across tabular and natural
language features achieves an F1 score
that is twice as high as either a pure-
table or pure-text system, and (2) us-
ing only shallower features or non-joint
inference results in lower quality.
1 Introduction
Tabular data is ubiquitous and often contains
high-quality, structured relational data. Re-
cent studies found billions of high-quality re-
lations on the web in HTML (Cafarella et
al., 2008). In financial applications, a huge
amount of data is buried in the tables of cor-
porate filings and earnings reports; in science,
millions of journal articles contain billions of
scientific facts in tables. Although tables de-
scribe precise, structured relations, tables are
rarely written in a way that is self-describing,
e.g., tables may contain abbreviations or only
informal schema information; in turn, the con-
tents of tables are often ambiguously specified,
which makes extracting the relations implicit
in tabular data difficult.
Tables are, however, not written in isola-
tion. The text surrounding a table in a jour-
nal article explains its contents to its intended
audience, a human reader. For example, in
a simple study, we demonstrate that humans
can achieve more than 60% higher recall by
jointly reading the text and tables in a journal
article than by only looking at the tables. The
conclusion of this experiment is not surprising,
but it raises a question: How should a system
combine tabular and natural-language features
to understand tables in text?
The literature provides a broad spectrum of
answers to this question. Most previous ap-
proaches use textual or tabular features sepa-
rately, e.g., tabular approaches that do not use
text features (Dalvi et al, 2012; Wu and Lee,
2006; Pinto et al, 2003) or textual approaches
that do not use tabular features (Mintz et al,
2009; Wu and Weld, 2010; Poon and Domin-
gos, 2007). In a prescient study, Liu et al
(2007) proposed to learn the target relation in-
dependently from both table and surface tex-
tual features, and then combine the result us-
ing a linear combination of the predictions.
In a similar spirit, we propose to use both
types of features in our approach of relation
extraction. Our proposed approach differs
from prior approaches in two ways: (1) We
use deeper?but standard?NLP features than
prior approaches for table extraction. In con-
trast to the shallow, lexical features that prior
approaches have used, we use standard NLP
features, such as dependency paths, parts of
speech, etc. Our hypothesis is that a deeper
understanding of the text in which a table is
embedded will lead to higher quality table ex-
traction. (2) Our probabilistic model jointly
uses both tabular and textual features. One
advantage of a joint approach is that one can
predict portions of the complicated predicate
that is buried in a table. For example, in a ge-
ology journal article, we may read a measure-
658
Table	 ?
although some fractional crystallization must have occurred during
the formation of these rocks, crystal fractionation alone cannot
account for the adakitic signature of the Gangdese rocks. Considering
the presence of a ~1500 km long belt of adakitic rocks, a fractionation
model would require the existence of an extremely large parent
magma body, the evidence for which is lacking. In fact, there is a
complete absence of coeval andesitic and basaltic magmatisms in this
adakite belt. Recent investigation on the crystallization history of a
hydrous primitive andesite composition shows that garnet is stable in
andesitic and basaltic bulk compositions only after large degrees of
crystallization lead to a decrease of the Mg-number to less than 0.5,
and that high Mg-number primitive melts are not garnet saturated at
high pressures (M?entener et al, 2001). However, all of the adakitic
porphyries from southern Tibet including those with lower Mg-
number show high Dy/Yb ratios and La/Yb ratios (Gao et al, 2007a).
The high Sr/Y, Dy/Yb and La/Yb ratios, low heavy REE and Y
concentrations of the Gangdese adakites require an adakitic signature
in the primary melt source.
Overall, the adakitic rocks of different ages in the Gangdese belt
display same differentiation trends (Figs. 2 and 3). In both the pre-
collision and post-collision groups, the abundances of MgO (Fig. 3a),
TiO2 (Fig. 3c) and CaO (Fig. 3d) decrease with increasing SiO2,
whereas with few exceptions, most samples have nearly constant
Al2O3 contents (Fig. 3b). Whereas the MgO and SiO2 contents of the
pre-collision adakite show a wide range, most of the post-collision
adakites have high SiO2 and low MgO contents, and plot in the high
SiO2 adakite field (Fig. 2a). In the two types of adakites, the total
alkaline contents (K2O+Na2O wt.%), K2O abundances and K2O/Na2O
ratios show a positive correlation with SiO2, displaying the typical
differentiation trend of calc-alkaline arc magmas (Fig. 2). However,
some of the post-collision adakitic rocks have unusually high K2O
contents, yielding abnormally high total alkaline contents and K2O/
Na2O ratios. Consequently, these samples significantly depart from
the overall trends (Fig. 2). This suggests that the unusual K2O
enrichment was not simply a result of magmatic differentiation.
The two generations of adakitic rock in the Gangdese belt show
many similarities in terms of distribution of trace elements with
typical incompatible trace element fractionation patterns of subduc-
tion-related magmas (Fig. 4). Overall, the adakites of different ages
display significant positive Pb and Sr anomalies, and negative Nb, Ta
and Ti anomalies (Fig. 4), correlating with typical features of adakitic
magmas (Martin et al, 2005). Despite their similar trace element
patterns, the geochemical signatures of the rocks in different regions
show some distinction. Some of the post-collision adakitic rocks from
Table 1
Major and trace elements of the post-collision adakitic rocks from the Gangdese belt, southern Tibet.
Location Zhunuo Puridazong
Sample ZM-1 ZM-2 ZM-3 ZM-4 ZM-5 ZM-6 ZM-7 ZM-8 ZM-9 ZM-10 ZM-12 ZM-13 PRDZ1 PRDZ2 PRDZ3 PRDZ4 PRDZ5
SiO2 66.69 65.96 65.37 67.00 69.09 65.10 63.86 71.56 69.73 69.37 67.27 65.99 65.61 65.97 65.73 65.02 65.29
TiO2 0.64 0.5 0.60 0.57 0.46 0.60 0.59 0.31 0.40 0.45 0.47 0.5 0.6 0.64 0.65 0.63 0.64
Al2O3 15.28 15.28 15.35 14.56 14.67 15.44 16.01 14.2 14.35 14.14 17.12 15.27 15.45 15.4 15.39 15.04 15.18
Fe2O3 4.17 3.07 4.27 4.06 3.18 4.31 3.83 2.39 3.04 3.32 2.25 3.05 3.46 3.69 3.68 3.54 3.62
MnO 0.02 0.06 0.08 0.08 0.04 0.09 0.08 0.03 0.04 0.04 0.03 0.06 0.06 0.06 0.06 0.06 0.06
MgO 1.75 1.54 1.84 1.60 1.20 1.94 2.35 0.83 1.08 1.34 0.97 1.54 1.63 1.75 1.76 1.72 1.7
CaO 2.20 2.61 3.70 2.88 2.06 3.69 3 1.81 2.38 2.52 2.43 2.59 3.27 3.22 3.33 3.16 3.24
Na2O 4.16 4.19 4.02 3.92 3.81 4.00 4.27 4.32 4.34 3.98 5.03 4.13 3.93 3.83 3.78 3.82 3.77
K2O 3.57 3.71 3.25 3.49 4.20 3.19 3.41 4.00 3.85 3.65 3.75 3.71 3.82 3.84 3.79 3.85 3.8
P2O5 0.21 0.18 0.21 0.19 0.17 0.22 0.27 0.11 0.15 0.16 0.24 0.19 0.26 0.27 0.27 0.26 0.26
LOI 0.90 2.66 0.94 1.36 0.86 1.08 1.98 0.22 0.36 0.80 0.12 2.76 1.46 1.62 1.5 1.56 1.4
Sum 99.6 99.8 99.6 99.7 99.7 99.7 99.7 99.8 99.7 99.8 99.7 99.8 99.6 100.3 99.9 98.7 99.0
Mg# 49.4 53.9 50.1 47.9 46.8 51.2 58.8 44.7 45.3 48.5 50.1 54.0 52.3 52.5 52.7 53.1 52.2
Sc 8.31 5.2 8.55 7.36 5.72 8.65 5.79 3.6 4.52 5.62 2.54 8.96 7.27 7.57 7.28 7.43 8.03
V 78.1 66 79.9 71.5 56.3 81.5 74.3 36.2 44.9 52.1 48.8 107 84.8 87.1 83.0 75.6 91.6
Cr 23.1 118 22.6 22.6 21.8 24.5 82.6 13.3 18.2 22.2 112 33.4 24.1 25.5 23.1 377.8 28.7
Co 11 14.4 12.2 10.9 7.27 13 11.5 5.21 6.7 5.24 5.99 13.1 10.6 10.8 10.6 13.4 11.4
Ni 15 86.8 13.9 12.7 10.5 15.9 17.3 6.41 8.65 14.2 6.85 17.1 11.4 11.6 11.5 74.2 12.9
Rb 232 202 141 158 218 142 153 227 207 191 193 40.3 149 143 134 149 155
Sr 681 633 884 752 550 878 807 567 664 623 824 825 1025 987.5 994 950 995
Y 12.2 9.21 17.5 11.0 11.0 11.2 9.57 6.02 7.64 8.98 5.06 8.69 12.0 12.1 11.2 11.6 12.9
Zr 47.2 96.6 114 78.6 26.8 64.7 67.5 26.5 46 77.7 18.3 166 127 136 127 139 149
Nb 9 9.3 9.72 9.14 9.83 9.23 8.98 8.6 8.41 9.77 7.3 6.45 8.83 8.77 8.07 8.65 9.28
Cs 19.1 8.47 6.42 6.97 11.1 8.28 6.26 10.8 8.35 14.0 8.42 2.07 3.66 3.79 3.49 3.60 3.92
Ba 880 669 985 912 848 964 741 861 857 838 652 1043 1287 1234 1129 1205 1225
La 37.4 31.5 33.36 37.8 38.4 38.7 33.6 28 37.2 34.8 25.4 20.6 42.2 50.8 41.5 46.7 49.7
Ce 65.7 63.8 63.6 68.3 65.9 69.6 66.9 47.4 62.7 61.5 49.7 41 83.4 94.0 81.0 88.7 95.5
Pr 8.35 7.81 8.32 8.52 8.2 8.65 8.15 5.41 7.45 7.55 6.65 5.09 9.83 10.42 9.21 9.93 10.70
Nd 31.3 28.3 31.68 30.8 28.4 31.9 29.2 19.2 26.5 27.8 25.1 19.8 37.5 38.4 34.7 37.2 39.8
Sm 5.57 4.68 6.32 5.22 4.79 5.48 4.96 3.02 4.27 4.69 4.19 3.45 6.19 6.25 5.53 6.05 6.55
Eu 1.36 1.17 1.1 1.23 1.08 1.36 1.30 0.76 1.00 1.07 1.04 1.29 1.60 1.57 1.44 1.48 1.60
Gd 4.46 3.34 4.02 3.98 3.58 4.02 3.54 2.17 3.09 3.36 2.69 2.92 4.64 4.50 4.17 4.38 4.63
Tb 0.54 0.43 0.520 0.47 0.47 0.5 0.45 0.25 0.35 0.38 0.32 0.41 0.390 0.397 0.359 0.374 0.414
Dy 2.7 1.99 2.72 2.46 2.3 2.5 2.03 1.22 1.64 1.98 1.22 1.89 2.84 2.79 2.63 2.74 2.91
Ho 0.49 0.34 0.487 0.43 0.41 0.44 0.38 0.23 0.29 0.35 0.19 0.33 0.504 0.511 0.471 0.491 0.530
Er 1.29 0.92 1.26 1.13 1.13 1.19 0.97 0.61 0.77 0.92 0.48 0.96 1.40 1.40 1.30 1.34 1.50
Tm 0.17 0.13 0.16 0.15 0.14 0.15 0.13 0.09 0.11 0.12 0.06 0.13 0.186 0.185 0.167 0.179 0.188
Yb 0.99 0.82 1 0.96 0.94 0.91 0.83 0.6 0.67 0.74 0.38 0.87 1.032 1.004 0.957 1.004 1.096
Lu 0.13 0.12 0.13 0.14 0.13 0.12 0.11 0.09 0.1 0.1 0.04 0.12 0.166 0.166 0.157 0.172 0.173
Hf 1.44 4.05 3.57 2.76 0.94 2.35 2.74 1.11 1.82 2.7 1.34 4.88 3.58 3.84 3.65 3.89 4.15
Ta 0.69 0.79 0.77 0.73 0.92 0.69 0.69 0.82 0.76 0.82 0.62 0.4 0.582 0.567 0.534 0.573 0.592
Pb 21 60.1 34.4 43.9 42.2 34.9 42.4 28 29.7 32.1 23 10.6 32.1 30.8 29.9 31.8 31.9
Th 18 27.2 24.6 24.5 30.1 21.7 24.6 22.3 26 28.4 15 2.95 16.5 17.5 16.0 17.6 17.9
U 2.97 6.64 5.66 5.91 4.48 5.16 5.39 5.11 4.84 8.33 2.83 0.92 2.58 2.43 2.35 2.46 2.57
Mg#=Mg/(Mg+0.85?TFe2+).
654 Y. Gao et al / Lithos 119 (2010) 651?663
SampleID	 ? Loca?on	 ?
ZM-??1	 ? Zhunuo	 ?
?	 ? ?	 ?
Loca?on	 ? RockType	 ?
Zhunuo	 ? Granodi-??	 ?orite	 ?
?	 ? ?	 ?
Loca?on_RockType	 ?
SampleID	 ? RockType	 ?
ZM-??1	 ? Granodi-??orite	 ?
?	 ? ?	 ?
Sample_RockType	 ?
Zhunuo	 ?(ZN),	 ?which	 ?lithologically	 ?corresp-??ond	 ?to	 ?granodiori?c	 ?and	 ?grani?c?	 ?
Context	 ?Text	 ?
Sample_Loca? n	 ?Input	 ?Data	 ? Joint	 ?Inference	 ?Results	 ?
Fig re 1: An example of joint inference be-
tween a table and its context.
1	 ? 7	 ?? 	 ? 14	 ?? 15	 ? 21	 ?? 
1	 ? 7	 ?? 8	 ? 14	 ?? 15	 ? 21	 ?? 
1	 ? 7	 ?? 8	 ? 14	 ?? 15	 ? 21	 ?? 
Whole doc. 
Table-only 
Text-only 
Geoscien?st	 ?1	 ? Geoscien?st	 ?2	 ? Geoscien?st	 ?3	 ?
Figure 2: Job assignments for the human
study.
ment in a table that tells us the type of rock
and its weight?but data such as the location
where this rock was unearthed and in what ge-
ological time interval this rock appeared may
no be specifi d in the table.
We consider tasks in three domains:
Petrology, Finan e, and Geology. For
each domain, we build a system to extract re-
lations from text, tables, or b t . We found
that a joint inference system that uses non-
shallow, but standard NLP features can sig-
nificantly improv the quality of he extracted
relations, and that this result holds consistently
across all three domains. For example, in our
Petrology application to extract a knowledge
base, call PetDB1, by using information
extracted from both text and tables, we can
achieve twice as high F1 compared to either a
pure-table or pure-text system.
2 Motivating Human Study
We describe a simple human study that mo-
tivated our approach to jointly combine both
tabular features and natural language features
to extract relations from tables. The hypoth-
1http://www.earthchem.org/petdb
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
Precision	 ? Recall	 ? F-??1	 ?
Text-??only	 ? Table-??only	 ? Whole	 ?document	 ?
Figure 3: Human quality to extract Sample-
Rocktype relations in PetDB.
Task	 ? Text Table Joint NER	 ? POS	 ?tags	 ?Stanford	 ?NER	 ?Regular	 ?Expression	 ?Dic?onary	 ?
pd?otable	 ?NER	 ?of	 ?neighbor	 ?cells	 ?Regular	 ?expression	 ?Dic?onary	 ?#	 ?columns	 ?
Whether	 ?a	 ?men?on	 ?	 ?in	 ?table	 ?also	 ?appears	 ?	 ?in	 ?the	 ?text.	 ?
EL	 ? POS	 ?tags	 ?Bing	 ?query	 ?results	 ?Freebase	 ?Stanford	 ?Parser	 ?
Pd?otable	 ?Bing	 ?query	 ?results	 ?Freebase	 ?
Subjec?e	 ?men?ns	 ?	 ?in	 ?the	 ?sentence	 ?near	 ?	 ?a	 ?table	 ?
RE	 ? Dependency	 ?path	 ?Term	 ?proximity	 ?Word	 ?sequence	 ?
Table	 ?headers	 ?Table	 ?subheaders	 ?RE	 ?of	 ?neighbor	 ?rows	 ?
Join	 ?between	 ?rela?ons	 ?(See	 ?Figure	 ?1	 ?for	 ?an	 ?example)	 ?
Figure 4: List of features we used in Text,
Table, and Joint approaches. NER, EL,
and RE refer to named-entity recognition, en-
tity linking, and relation extraction, respec-
tively.
esis that we want to validate is that the text
surrounding a table could provide valuable in-
formation even for a human reader, and there-
fore, n id l achine reading system should
also try to capture similar information.
We asked three geoscientists to manually
read journal articles nd extract relations
for the Petrology domain. We report
our r sults for he target relation, Sample-
RockType, which associates a rock type with
a rock sample (see Figure 1 for an example).
We randomly sampled 21 journal articles. For
each journal article, we pr duced three vari-
ants: (1) the original document; (2) table-
only, which is the set of tables in the docu-
ment (without the text); (3) text-only, which
is the text of the document with the tables
removed from the document. Each geoscien-
tist was asked to read and extract the relations
from one of the three variants. We then judged
the precision and recall of their extraction, as
shown in Figure 2.
659
As shown in Figure 3, human readers not
surprisingly achieve perfect precision on each
of the variants, but lower recall on both
the table-only and text-only variants. How-
ever, summing the recall of table-only (60%)
and text-only (20%) variants together would
achieve only 80% recall; this implies that in
the best case more than 20% of the extrac-
tions require that the human reader read the
table and its surrounding text jointly. Figure 1
shows one representative example.
This motivates our approach, which uses a
joint inference system to model features from
a table and its surrounding text. We also pro-
pose to use deep linguistic features instead of
shallower features to get as close as possible to
the ability of human readers in understanding
the surrounding text of a table.
3 Empirical Study & Experiments
We describe our experiments to test the hy-
pothesis that (1) deeper linguistic features can
help to extract higher quality relations from
tables, and (2) joint inference across tables and
text improves extraction quality compared to
approaches that use pure-table, pure-text, and
non-joint ways of combining these two. We
briefly describe some experiments for a dataset
that we call Geology (Zhang et al, 2013).
The detailed experimental results in all three
domains are in the technical report version of
this paper.
3.1 Experimental Setup
We consider the task of constructing a geol-
ogy knowledge base. Specifically, our goal is
to extract a Rock-TotalOrganicCarbon
relation that maps rock formations (e.g., ?Bar-
nett Formation?) to their total organic carbon
(e.g., ?6%?). Such data is important for es-
timating stored energy and for global climate
research.
Dataset. We selected 100 geology journal
articles.2 We asked three geoscientists to an-
notate these journal articles manually to ex-
tract the Rock-TotalOrganicCarbon re-
lation (1.5K tuples). We processed each doc-
ument using Stanford CoreNLP (de Marneffe
et al, 2006; Toutanova and Manning, 2000),
2We choose a set of documents that (1) are in En-
glish, and (2) contain at least one table.
PDFtoHTML3, and pdf2table (Yildiz, 2004).
We then extracted features following state-of-
the-art practices (see Figure 4).
Approaches. To validate our hypothesis,
we implement four systems, each of which has
access to different types of data:
(1) Table. This approach follows Pinto et
al. (2003) and Dalvi et al (2012) and only uses
the tables in a document.
(2) Text. This approach only has access to
the text in a document and contains all the fea-
tures mentioned in Wu and Weld (2010) and
Mintz et al (2009).
The features used in (1) and (2) are shown in
Figure 4. In both Table and Text, we use a
conditional random field (Lafferty et al, 2001)
model for the Rock-TotalOrganicCarbon
relation.
(3) Merge. Using Table and Text, we
extract all facts and their associated probabil-
ity. Following Duin (2002), we combine these
two probabilities using a linear combination.
Merge is a baseline approach that uses infor-
mation from both tables and text.
(4) Joint. We build a joint approach that
uses information from both tables and text.
This approach is a large factor graph in which
we embed the CRFs developed in Table and
Text. Additionally, we allow Joint to pre-
dict projections of each relation, as shown in
Figure 4. Recall that a key advantage of a joint
approach is that we do not need to predict all
arguments of the relation (if such a prediction
is unwarranted from the data). The inference
is done by Gibbs sampling using our inference
engine Elementary (Zhang and Re?, 2013).
We describe the Joint system in more detail
in the technical report version of this paper.
3.2 End-to-End Quality
We were able to validate that Joint achieves
higher quality than the other three approaches
we considered. Figure 5 shows the P/R curve
of different approaches on three domains. We
analyzed the domain Geology.
Joint dominates all other approaches. At
a recall of 10%, Joint achieves 3x higher pre-
cision than all other approaches. In our error
analysis, we saw that tables in geology articles
often contain ambiguous words; for example,
3http://pdftohtml.sourceforge.net/
660
0	 ?0.2	 ?
0.4	 ?0.6	 ?
0.8	 ?1	 ?
0	 ? 0.2	 ? 0.4	 ?
Recall	 ?
Table	 ?Text	 ? Merge	 ?
Joint	 ?
(b)	 ?Petrology	 ?Domain	 ?	 ?
0	 ?0.2	 ?
0.4	 ?0.6	 ?
0.8	 ?1	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ?
Pre
cisi
on	 ?
Recall	 ?
Joint	 ?
Merge	 ?Text	 ?
Table	 ?
(a)	 ?Geology	 ?Domain	 ?	 ?
0	 ?0.2	 ?
0.4	 ?0.6	 ?
0.8	 ?1	 ?
0	 ? 0.1	 ? 0.2	 ?
Recall	 ?
Joint	 ?
Table	 ?
Merge	 ?
Text	 ?
(c)	 ?Finance	 ?Domain	 ?	 ?
Figure 5: End-to-end extraction quality on Petrology, Finance, and GeoDeepDive. The
recall is limited by the quality of state-of-the-art table recognition software on PDFs.
the word ?Barnett? in a table may refer to ei-
ther a location or a rock formation. By using
features extracted from text, Joint achieves
higher precision. For recall in the range of 0?
10%, Merge outperforms both Text and Ta-
ble, with 3%?90% improvement in precision.
In Geology, Merge has precision that is
similar to Text and Table for the higher re-
call range (>10%). In this domain, we found
that relations that appeared in the text often
repeated relations described in the table. In
other domains, such as Petrology, where
the relations in text and tables have lower de-
grees of overlap, Merge significantly improves
over Text and Table (Figure 5(b)).
We conducted a statistical significance test
to check whether the improvement of Joint
over the three other approaches is statistically
significant. For each of the three probability
thresholds, t ? {.99, .90, .50}, we created the
set of predictions that Joint assigns probabil-
ity greater than t. Figure 6 shows the results
of the statistical significance test in which the
null hypothesis is that the F1 scores of two ap-
proaches are the same. With p = 0.01, Joint
has statistically significant improvement of F1
score over all three other approaches with each
probability threshold.
3.3 Shallow vs. Linguistic Features
We validate the hypothesis that using
linguistic features, e.g., part-of-speech
tags (Toutanova and Manning, 2000),
named-entity tags (Finkel et al, 2005), and
dependency trees (de Marneffe et al, 2006),
helps improve the quality of our approach,
called Joint. There are different ways to
use shallow and linguistic features; we select
Approaches \ Prob. .99 .90 .50
Text + + +
Table + + +
Merge + + +
Figure 6: Approximate randomization test
from Chinchor (1992) of F1 score with p =
0.01 on the impact of joint inference compared
with pure-table or pure-text approaches for
different probability thresholds. A + sign in-
dicates that the F1 score of joint approach in-
creased significantly.
Type	 ? Features	 ?
Shallow	 ? Regular	 ?Expressions	 ?(Dalvi	 ?et	 ?al.,	 ?2012)	 ?	 ?Term	 ?proximity	 ?(Matsuo	 ?et	 ?al.,	 ?2003)	 ?Dic?onary	 ?and	 ?Freebase	 ?(Mintz	 ?et	 ?al.,	 ?2009)	 ?
Linguis?	 ? POS	 ?tags	 ?(Wu	 ?et	 ?al.,	 ?2010)	 ?Stanford	 ?NER	 ?tags	 ?(Mintz	 ?et	 ?al.,	 ?2009)	 ?Dependence	 ?trees	 ?(Mintz	 ?et	 ?al.,	 ?2009)	 ?
Figure 7: Types of Features.
state-of-the-art approaches from the literature
(see Figure 7).
We created the following variants of Joint.
Joint(-parse) removes features generated by
the dependency parser and syntax parser.
Similarly, Joint(-ner) (Joint(-pos)) removes
all features related to NER (resp. POS).
Joint(-pos) also removes NER and parser fea-
tures because the latter two are dependent on
POS features.
Figure 8 shows the P/R curve for all
these variants on Geology, and Figure 9
shows the results of statistical significance
test. For probability threshold .90, Joint
outperforms Joint(-pos) significantly. The
difference between Joint, Joint(-parse),
661
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ?
Prec
isio
n	 ?
Recall	 ?
Joint(-??pos)	 ?
Joint	 ?Joint(-??parse)	 ?Joint(-??ner)	 ?
Figure 8: Lesion study of different features for
Geology.
Features \ Prob. .90 .50
Joint(-parse) ? Joint 0 +
Joint(-ner) ? Joint 0 +
Joint(-pos) ? Joint + +
Figure 9: Approximate randomization test of
F1 score with p = 0.01 on the impact of lin-
guistic features. For x ? y, a + indicates that
the F1 score of y is significantly higher than x.
0 indicates that the F1 score does not change
significantly.
and Joint(-ner) is not significant because
there are ?easy-to-extract? facts in the high-
probability range. For probability threshold
.50, Joint outperforms all three other vari-
ants significantly.
4 Related Work
The intuition that context features might help
table-related tasks has existed for decades. For
example, Hurst and Nasukawa (2000) men-
tioned (as future work) that context features
could be used to further improve their relation
extraction approaches from tables. Lin et al
(2010) use bag-of-words features and hyper-
links to recommend new columns for web ta-
bles. Liu et al (2007) extract features, includ-
ing font size and title, from PDF documents in
which a table appears to help the table rank-
ing task. They find that these features only
contribute less than 2% to precision. In con-
trast, in our approach linguistic features are
quite useful. The above approaches use con-
text features that can be extracted without
POS tagging or linguistic parsing. One aspect
of our work is to demonstrate that traditional
NLP tools can enhance the quality of table ex-
traction.
Extracting information from tables has been
discussed by different communities in the last
decade, including NLP (Wu and Lee, 2006;
Tengli et al, 2004; Chen et al, 2000), artifi-
cial intelligence (Fang et al, 2012; Pivk, 2006),
information retrieval (Wei et al, 2006; Pinto
et al, 2003), database (Cafarella et al, 2008),
and the web (Dalvi et al, 2012). This body of
work considers only features derived from ta-
bles and does not examine richer NLP features
as we do.
While joint inference is popular, it is not
clear when a joint inference system outper-
forms a more traditional NLP pipeline. Re-
cent studies have reached a variety of conclu-
sions: in some, joint inference helps extraction
quality (McCallum, 2009; Poon and Domin-
gos, 2007; Singh et al, 2009); and in some,
joint inference hurts extraction quality (Poon
and Domingos, 2007; Eisner, 2009). Our intu-
ition is that joint inference is helpful in this ap-
plication because our joint inference approach
combines non-redundant signals (textual ver-
sus tabular).
5 Conclusion
To improve the quality of extractions of tabu-
lar data, we use standard NLP techniques to
more deeply understand the text in which a
table is embedded. We validate that deeper
NLP features combined with a joint proba-
bilistic model has a statistically significant im-
pact on quality, i.e., recall and precision. Our
ongoing work is to apply these ideas to a much
larger corpus from each of the three domains.
6 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) DEFT Program under Air Force
Research Laboratory (AFRL) prime contract
No. FA8750-13-2-0039, the National Science
Foundation EAGER Award under No. EAR-
1242902 and CAREER Award under No. IIS-
1054009, and the Sloan Research Fellowship.
Any opinions, findings, and conclusion or rec-
ommendations expressed in this material are
those of the authors and do not necessarily re-
flect the view of DARPA, AFRL, NSF, or the
US government. We are also grateful to Jude
W. Shavlik for his insightful comments.
662
References
Michael J. Cafarella, Alon Halevy, Daisy Zhe
Wang, Eugene Wu, and Yang Zhang. 2008.
WebTables: Exploring the power of tables on the
web. Proceedings of VLDB Endowment, 1(1).
Hsin-Hsi Chen, Shih-Chung Tsai, and Jin-He Tsai.
2000. Mining tables from large scale HTML
texts. In Proceedings of the 18th Conference on
Computational Linguistics, COLING ?00.
Nancy Chinchor. 1992. The statistical significance
of the MUC-4 results. In Proceedings of the 4th
Conference on Message Understanding, MUC4
?92.
Bhavana Bharat Dalvi, William Cohen, and Jamie
Callan. 2012. WebSets: Extracting sets of en-
tities from the web using unsupervised infor-
mation extraction. In Proceedings of the 5th
ACM International Conference on Web Search
and Data Mining, WSDM ?12.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalua-
tion.
Robert Duin. 2002. The combining classifier: to
train or not to train? In 16th International
Conference on Pattern Recognition.
Jason Eisner. 2009. Joint models with missing
data for semi-supervised learning. In NAACL
HLT Workshop on Semi-supervised Learning for
Natural Language Processing.
Jing Fang, Prasenjit Mitra, Zhi Tang, and C. Lee
Giles. 2012. Table header detection and classi-
fication. In Proceedings of the 26th AAAI Con-
ference on Artificial Intelligence, AAAI ?12.
Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by
Gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ?05.
Matthew Hurst and Tetsuya Nasukawa. 2000.
Layout and language: Integrating spatial and
linguistic knowledge for layout understanding
tasks. In Proceedings of the 18th Conference on
Computational Linguistics, COLING ?00.
John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Pub-
lishers Inc.
Cindy Xide Lin, Bo Zhao, Tim Weninger, Jiawei
Han, and Bing Liu. 2010. Entity relation dis-
covery from web tables and links. In Proceedings
of the 19th International Conference on World
Wide Web, WWW ?10.
Ying Liu, Kun Bai, Prasenjit Mitra, and C. Lee
Giles. 2007. TableSeer: Automatic table meta-
data extraction and searching in digital libraries.
In Proceedings of the 7th ACM/IEEE-CS Joint
Conference on Digital Libraries, JCDL ?07.
Andrew McCallum. 2009. Joint inference for nat-
ural language processing. In Proceedings of the
13th Conference on Computational Natural Lan-
guage Learning, CoNLL ?09.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings
of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, ACL ?09.
David Pinto, Andrew McCallum, Xing Wei, and
W. Bruce Croft. 2003. Table extraction us-
ing conditional random fields. In Proceedings
of the 26th Annual International ACM SIGIR
Conference on Research and Development in In-
formaion Retrieval, SIGIR ?03.
Aleksander Pivk. 2006. Automatic ontology gen-
eration from web tabular structures. AI Com-
munication, 19(1).
Hoifung Poon and Pedro Domingos. 2007. Joint
inference in information extraction. In Proceed-
ings of the 22nd National Conference on Artifi-
cial intelligence, AAAI?07.
Sameer Singh, Karl Schultz, and Andrew Mc-
Callum. 2009. Bi-directional joint inference
for entity resolution and segmentation using
imperatively-defined factor graphs. In Pro-
ceedings of the European Conference on Ma-
chine Learning and Knowledge Discovery in
Databases, ECML PKDD ?09.
Ashwin Tengli, Yiming Yang, and Nian Li Ma.
2004. Learning table extraction from examples.
In Proceedings of the 20th International Con-
ference on Computational Linguistics, COLING
?04.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In
Proceedings of the 2000 Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?00.
Xing Wei, Bruce Croft, and Andrew McCallum.
2006. Table extraction for answer retrieval. In-
formation Retrieval, 9(5).
663
Dekai Wu and Ken Wing Kuen Lee. 2006. A gram-
matical approach to understanding textual ta-
bles using two-dimensional scfgs. In Proceedings
of the COLING/ACL, COLING-ACL ?06.
Fei Wu and Daniel S. Weld. 2010. Open informa-
tion extraction using Wikipedia. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10.
Burcu Yildiz. 2004. Information extraction ? uti-
lizing table patterns. Master?s thesis, Institutfu?r
Softwaretechnik und Interaktive Systeme.
Ce Zhang and Christopher Re?. 2013. Towards
high-throughput Gibbs sampling at scale: A
study across storage managers. SIGMOD ?13.
Ce Zhang, Vidhya Govindaraju, Jackson Bor-
chardt, Tim Foltz, Christopher Re?, and Shanan
Peters. 2013. GeoDeepDive: Statistical infer-
ence using familiar data-processing languages.
SIGMOD ?13.
664
