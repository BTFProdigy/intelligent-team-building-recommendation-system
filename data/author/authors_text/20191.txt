Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213?217,
Dublin, Ireland, August 23-24, 2014.
Copenhagen-Malm
?
o: Tree Approximations of Semantic Parsing Problems
Natalie Schluter
?
, Jakob Elming, Sigrid Klerke, H
?
ector Mart??nez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders S?gaard
?
Dpt. of Computer Science Center for Language Technology
Malm?o University University of Copenhagen
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
Abstract
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
1 Introduction
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.
1
At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1
For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B?ohmov?a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
2 Related work
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.
2
2
We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
213
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-2014
3
also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
3 Tree approximations
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of ?good?
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the ?top? fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
?root?. (Note that the ?root? of each non-singleton
connected component is marked as a ?top? node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3
http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
3.1 Graph pruning
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge?s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
3.2 Graph packing
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
214
a)
b)
c)
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
be represented if the head or arguments are ?fac-
tored out?. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges? labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p
1
and
p
2
in the graph, where p
1
= (v
1
, l, v
n
) and p
2
=
(v
1
, l
1
, v
2
), (v
2
, l
2
, v
3
), . . . , (v
n?1
, l
n?1
, v
n
), we
remove the last edge of p
2
and augment p
1
?s label
with the representation l
1
: l
2
: ? ? ? : l
n?1
of p
2
. p
1
becomes (v
1
, l and l
1
: l
2
: ? ? ? : l
n?1
, v
n
), indi-
cating that v
n
is also the child (with dependency
label l
n?1
) of the node found by travelling (from
v
1
) down an l
1
labelled edge, followed by an l
2
labelled edge, and so on until the child of the l
n?2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove ?worst? re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
4 Experiments
4.1 Data
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
215
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
4.2 Model
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)
4
with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
4.3 Baselines
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj?orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
Approach Cl DM PAS PCEDT Av
Systems
PRUNING
NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING
NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
4.4 Results
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
4
https://code.google.com/p/mate-tools/
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
F1 84.4 88.0 69.9 80.8
?
PREC 85.4 87.9 70.8 81.4
(W/O TOP) REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
F1 83.6 86.0 67.4 79.0
?
PREC 87.2 91.3 72.8 83.8
(W/O TOP) REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
(W/O TOP) REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
(W/O TOP) REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
4.5 Error Analysis
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
216
5 Conclusions
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
References
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43?48, Boulder, CO,
USA.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill?e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103?127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89?97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281?332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79?
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044?1050,
Prague, Czech Republic.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753?760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166?173, Odense, Denmark.
217
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 61?65,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
On maximum spanning DAG algorithms for semantic DAG parsing
Natalie Schluter
Department of Computer Science
School of Technology, Malm?o University
Malm?o, Sweden
natalie.schluter@mah.se
Abstract
Consideration of the decoding problem
in semantic parsing as finding a maxi-
mum spanning DAG of a weighted di-
rected graph carries many complexities
that haven?t been fully addressed in the lit-
erature to date, among which are its ac-
tual appropriateness for the decoding task
in semantic parsing, not to mention an ex-
plicit proof of its complexity (and its ap-
proximability). In this paper, we con-
sider the objective function for the maxi-
mum spanning DAG problem, and what it
means in terms of decoding for semantic
parsing. In doing so, we give anecdotal
evidence against its use in this task. In ad-
dition, we consider the only graph-based
maximum spanning DAG approximation
algorithm presented in the literature (with-
out any approximation guarantee) to date
and finally provide an approximation guar-
antee for it, showing that it is an O(
1
n
) fac-
tor approximation algorithm, where n is
the size of the digraph?s vertex set.
1 Introduction
Recent research in semantic parsing has moved at-
tention towards recovering labeled digraph repre-
sentations of the semantic relations corresponding
to the linguistic agendas across a number of theo-
ries where simple tree representations are claimed
not to be expressive enough to capture senten-
tial meaning. As digraph structures presented in
predominant semantic graph databases are mainly
acyclic, the semantic parsing problem has some-
times become associated with a maximum span-
ning directed acyclic graph (MSDAG) decoding
problem (McDonald and Pereira, 2006; Sagae and
Tsujii, 2008; Titov et al., 2009), in analogy and
perhaps as a generalisation of the maximum span-
ning tree decoding problem for syntactic depen-
dency parsing.
The appropriateness of finding the MSDAG in
decoding for semantic parsing has, however, never
been fully motivated, and in fact carries more
complexities than that of maximum spanning tree
(MST) decoding for syntactic parsing. In this pa-
per, we discuss the appropriateness of MSDAG
decoding in semantic parsing, considering the pos-
sible objective functions and whether they match
our linguistic goals for the decoding process. Our
view is that they probably do not, in general.
In addition to the problem of not being suffi-
ciently synchronised with our linguistic intuitions
for the semantic parsing decoding problem, the
MSDAG problem itself carries with it its own
complexities, which are still in the process of
becoming more understood in the algorithms re-
search community. McDonald and Pereira (2006)
claim that the MSDAG problem is NP-hard, cit-
ing (Heckerman et al., 1995); however, there is
no MSDAG problem in this latter work, and no
explicit reduction to any problem presented in
(Heckerman et al., 1995) has been published to
date. We point out that Schluter (submitted) ex-
plicitly provides a linear reduction to MSDAG
from the problem of finding a minimum weighted
directed multicut (IDMC), showing MSDAG?s
NP-hardness; this reduction also yields a result
on the approximability of MSDAG, namely that
it is APX-hard. We show in this paper that the ap-
proximation algorithm presented without any ap-
proximation guarantee in (McDonald and Pereira,
2006) is, in fact, a O(
1
n
) factor approximation al-
gorithm, where n is the size of the graphs vertex
set. This is not particularly surprising given the
problem?s APX-hardness.
Following some preliminaries on weighted di-
graphs (Section 2), we make the MSDAG prob-
lem precise through a discussion of the objective
function in question and briefly question this ob-
61
jective function with respect to decoding in se-
mantic parsing (Section 3). Finally, we discuss the
only other graph-based (approximation) algorithm
in the literature and prove its approximation guar-
antee (Section 4), followed by some brief conclu-
sions (Section 5).
2 Preliminaries
A directed graph (or digraph) G is a pair (V,E)
where V is the set of nodes and E is the set of
(directed) edges. E ? V ? V is a set of ordered
pairs of vertices. For u, v ? V , if (u, v) ? E, then
we say there is an ?edge from u to v?. If there is
any confusion over the digraph we are referring to,
then we disambiguate by using the notation G :=
(E(G), V (G)).
If all edges e ? E(G) are associated with a real
number, a weight w : E(G)? R, then we call the
digraph weighted. In this paper, all digraphs are
weighted and weights are positive.
For a subset of edges U of a weighted di-
graph, we set w(U) :=
?
e?U
w(e). Simi-
larly, for a weighted digraph G, we set w(G) :=
?
e?E(G)
w(e).
We denote the size of a set S, by |S|.
For G = (V,E), let u
1
, . . . , u
k
? V and
(u
i
, u
i+1
) ? E, for each i ? [k ? 1], then we
say that there is path (also directed path, or di-
path) of length (k? 1) from u
1
to u
k
in G. If also
(u
k
, u
1
) ? E, then we say that u
1
, u
2
, . . . , u
k
, u
1
is a cycle of length k in G.
A directed acyclic graph (DAG) is a directed
graph with no cycles. There is a special kind of
DAG, which has a special node called a root with
no incoming edges and in which there is a unique
path from the root to all nodes; this is called a tree.
Finally, a tournament is a digraph in which
all pairs of vertices are connected by exactly one
edge. If, in addition, the edges are weighted, then
we call the digraph a weighted tournament.
3 Objective functions for finding an
MSDAG of a digraph
We first make precise the objective function for
the MSDAG problem, by considering two sepa-
rate objective functions, one additive and one mul-
tiplicative, over the weights of edges in the optimal
solution:
D
?
:= argmax
D a spanning DAG of G
?
e?E(D)
w(e), and (1)
D
?
:= argmax
D a spanning DAG of G
?
e?E(D)
w(e). (2)
Maximising Equation (2) amounts to concur-
rently minimising the number of edges of weight
less than 1 in the optimal solution and maximis-
ing the number of edges of weight at least 1. In
fact, if all edge weights are less than 1, then this
problem reduces to finding the MST. However, the
objective in semantic parsing in adopting the MS-
DAG problem for decoding is to increase power
from finding only MSTs. Therefore, this version
of the problem is not the subject of this paper. If
a graph has even one edge of weight greater than
1, then all edges of lesser weights should be dis-
carded, and for the remaining subgraph, maximis-
ing Equations (1) or (2) is equivalent.
Maximising Equation (1) is complex and under
certain restrictions on edge weights may optimise
for simply the number of edges (subject to being a
DAG). For example, if the difference between any
two edge weights is less than
1
|E(G)|
? w(e) for
the smallest weighted e in E(G), then the prob-
lem reduces to finding the spanning DAG with the
greatest number of edges, as shown by Proposition
1.
Proposition 1. Let G be a weighted digraph, with
minimum edge weight M . Suppose the difference
in weight between any two edges of G is at most
1
|E(G)|
?M . Then an MSDAG forGmaximises the
number of edges of any spanning DAG for G.
Proof. Suppose D
1
, D
2
are spanning DAGs for G,
such that (without loss of generality) |E(D
1
)| =
|E(D
2
)| + 1, but that D
2
is an MSDAG and that
D
1
is not. We derive the following contradiction.
w(D
2
) =
?
e?E(D
2
)
w(e)
? |E(D
2
)| ?M + |E(D
2
)| ?
(
1
|E(G)|
?M
)
< |E(D
2
)| ?M + M
= |E(D
1
)| ?M
?
?
e?E(D
2
)
w(e)
= w(D
1
)
Admittedly, for arbitrary edge weights, the rela-
tion between the sum of edge weights and number
of edges is more intricate, and it is this problem
that we refer to as the MSDAG problem in this
paper. However, the maximisation of the number
of edges in the MSDAG does play a role when
62
using Equation (1) as the objective function, and
this may be inappropriate for decoding in seman-
tic parsing.
3.1 Two linguistic issues of in MSDAG
decoding for semantic parsing
We can identify two important related issues
against linguistic motivation for the use of MS-
DAG algorithms in decoding for semantic parsing.
The first problem is inherited from that of the arc-
factored model in syntactic parsing, and the sec-
ond problem is due to MSDAGs constrained max-
imisation of edges discussed above.
In the arc-factored syntactic parsing paradigm,
it was shown that the MST algorithm could be
used for exact inference (McDonald et al., 2005).
However, one problem with this paradigm was that
edges of the inferred solution did not linguisti-
cally constrain each other. So, for example, a verb
can be assigned two separate subject dependents,
which is linguistically absurd. Use of the MS-
DAG algorithm in semantic parsing corresponds,
in fact, to a generalisation of the arc-factored syn-
tactic parsing paradigm to semantic parsing. As
such, the problem of a lack of linguistic constraints
among edges is inherited by the arc-factored se-
mantic parsing paradigm.
However, MSDAG decoding for semantic pars-
ing suffers from a further problem. In MST de-
coding, the only constraint is really that the output
should have a tree structure; this places a precise
restriction on the number of edges in the output
(i.e., n ? 1), unlike for MSDAGs. From our dis-
cussion above, we know that the MSDAG prob-
lem is closely related to a constrained maximisa-
tion of edges. In particular, a candidate solution s
to the problem that is not optimal in terms of to-
tal weight may, however, be linguistically optimal;
adding further edges to s would increase weight,
but may be linguistically undesirable.
Consider the tree at the top of Figure 1, for the
example John loves Mary. In decoding, this tree
could be our linguistic optimal, however according
to our additive objective function, it is more likely
for us to obtain either of the bottom DAGs, which
is clearly not what is wanted in semantic parsing.
4 Related Research and an
Approximation Guarantee
The only algorithm presented to date for MSDAG
is an approximation algorithm proposed by Mc-
John loves Mary
John loves Mary John loves Mary
Figure 1: Possible spanning DAGs for John loves
Mary.
Donald and Pereira (2006), given without any ap-
proximation guarantee. The algorithm first con-
structs an MST of the weighted digraph, and
then greedily attempts to add remaining edges
to the MST in order of descending weight, so
long as no cycle is introduced. Only part of
this algorithm is greedy, so we will refer to it as
semi-greedy-MSDAG. Given the fact that MS-
DAG is APX-hard (Schluter, submitted), the fol-
lowing approximation guarantee is not surprising.
Theorem 2. semi-greedy-MSDAG is anO(
1
n
)
factor approximation algorithm for MSDAG.
Proof. We separate the proof into two parts. In
Part 1, we first consider an easy worst case
scenario for an upper bound on the error for
semi-greedy-MSDAG, without any considera-
tion for whether such a graph actually exists. Fol-
lowing this in Part 2, we construct a family of
graphs to show that this bound is tight (i.e., that
the algorithm exhibits worst imaginable behaviour
for this family of graphs).
Part 1. For G a digraph, let D be the output of
semi-greedy-MSDAG on G, and D
?
be an MS-
DAG for G. The worst case is (bounded by the
case) where the algorithm finds an MST T
?
for
G but then cannot introduce any extra edges to ob-
tain a spanning DAG of higher weight, because the
addition of any extra edges would induce a cycle.
For G?s nodes, we suppose that |V (G)| > 3. For
edges, we suppose that all the edges in T
?
have
equally the largest weight, say w
max
, of any edge
in E(G), and that all other edges in E(G) have
weight O(w
max
). We can do this, because it gives
an advantage to T
?
.
We suppose also that the underlying undirected
graph of G is complete and that the true MSDAG
for G is D
?
:= (V (G), E(G)? E(T
?
)).
This clearly must be the worst imaginable case:
that T
?
shares no edges with D
?
, but that D
?
con-
63
tains every other possible edge in the graph, with
the weight of every edge in D
?
being at most the
weight of the maximum weighted edge of those
of T
?
(remember we are trying to favour T
?
). No
other imaginable case could introduce more edges
to D
?
without inducing a cycle. So, for all G,
w(D
?
) = O
(
(n? 1)
2
? w
max
2
)
= O(n
2
?w
max
),
and we had that w(T
?
) = w(D) = O(n ? w
max
).
So at very worst, semi-greedy-MSDAG finds
a spanning DAG D of weight within O(
1
n
) of the
optimal.
Part 2. We now show that this bound is tight.
We construct a family of graphs G
n
= (V
n
, E
n
) as
follows. V
n
:= {v
0
, v
1
, v
2
, . . . , v
n
}, with n > 3,
and we suppose that n is even. Let c ? R be some
constant such that c > 3. We place the following
edges in E
n
:
(E1) (v
i
, v
i+1
) of weight c for all i ? {0, . . . , n ?
1} into E
n
, creating a path from v
0
to v
n
of
length n where every edge has weight c, and
(E2) (v
i
, v
j
) of weight c? 1 for all j ? {2, i? 1},
i ? {2, . . . , n}.
So, in addition to the path defined by the edges in
(E1), G
n
? {v
0
} contains a weighted tournament
on n? 1 nodes, such that if j < i, then there is an
edge from i to j.
Let us denote the MST of G
n
by T
?
n
and the maximal spanning tree obtainable by
semi-greedy-MSDAG, by D
n
. We will show
that the (unique) MSDAG of G
n
is the graph D
?
n
that we construct below.
It is easy to see that the MST T
?
n
of G
n
consists
of the edges in (E1), and that no further edges can
be added to T
?
n
without creating a cycle. So, D
n
=
T
?
n
.
On the other hand, we claim that there is a
unique D
?
n
consisting of:
1. the edge (v
0
, v
1
),
2. the edges (v
2i?1
, v
2i
) for all i ?
{1, . . . , n/2} into E(D
?
n
), that is every
second edge in the path described in (E1),
and
3. all the edges from (E2) except for (v
2i
, v
2i?1
)
for all i ? {1, . . . , n/2}.
We can easily see that D
?
n
is at least maximal. The
only edges not in D
?
n
are ones that are parallel
to other edges. So, introducing any other edge
from (E2) would mean removing an edge from
(E1), which would decrease D
?
n
?s overall weight.
Moreover, notice that introducing any other edge
from (E1), say (v
k?1
, v
k
) would require remov-
ing two edges (from (E2)), either (v
k
, v
k?1
) and
(v
k+1
, v
k?1
) or (v
k
, v
k?1
) and (v
k
, v
k+1
), to avoid
cycles in D
?
n
, but this also decreases overall
weight. We extend these two simple facts in the
remainder of the proof, showing that D
?
, in addi-
tion to being maximal, is also a global maximum.
We prove the result by induction on n (with n
even), that D
?
n
is the MSDAG for G
n
. We take
n = 4 as our base case.
For G
4
(see Figure 2), E(G
4
)?E(D
?
4
) contains
only three edges: the edge (v
2
, v
3
) of weight c and
the edges (v
4
, v
3
) and (v
2
, v
1
) of weight (c ? 1).
Following the same principles above, adding the
edge (v
2
, v
1
) would entail removing an edge of
higher weight; the same is true of adding the edge
(v
4
, v
3
). No further edges in either case could
be added to make up this difference and achieve
greater weight than D
?
4
. So the only option is to
add the edge (v
2
, v
3
). However, this would entail
removing either the two edges (v
3
, v
2
) and (v
4
, v
2
)
or (v
3
, v
2
) and (v
3
, v
4
) from D
?
4
, both of which ac-
tions results in lower overall weight.
Now suppose D
?
n?2
is optimal (for n ? 6,
n even). We show that this implies D
?
n
is op-
timal (with n even). Consider the two sub-
graphs G
n?2
and H of G
n
induced by the sub-
sets of V (G
n
), V (G
n?2
) = {v
0
, . . . , v
n?2
} and
V (H) := {v
n?1
, v
n
} respectively (so H =
(V (H), {(v
n?1
, v
n
), (v
n?1
, v
n
)})). We are as-
suming that the MSDAG of G
n?2
is D
?
n?2
. More-
over, the MSDAG of H is a single edge, D
H
:=
(V (H), {(v
n?1
, v
n
)}).
D
?
n
includes the MSDAGs (D
?
n?2
and D
H
) of
these two digraphs, so for these parts of D
?
n
, we
have reached an upper bound for optimality. Now
we consider the optimal way to connect D
?
n?2
and
D
H
to create D
?
n
.
Let C denote the set of edges in G
n
which
connect D
H
to G
n?2
and vice versa. C =
{(v
n?2
, v
n?1
)} ? {(u
n
, u
i
) | 1 ? i < n} ?
{(u
n?1
, u
i
) | 1 ? i < n ? 1}. Note that the
only edge from C not included in D
?
n
is e
C
:=
(v
n?2
, v
n?1
). By the discussion above, we know
that including e
C
would mean excluding two other
edges from C of weight at least (c?1), which can-
not be optimal. Therefore D
?
n
must be optimal.
64
0 1 2 3 4
(c-1) (c-1)
(c-1)
(c-1)
c
c
c c
(c-1) (c-1)
Figure 2: G
4
, with D
?
4
in blue and green and T
?
4
in red and green.
So we have constructed a family of graphs G
n
where w(D
n
) = w(T
?
n
) = nc and
w(D
?
n
) =
(
n?1
?
i=0
i(c? 1) + (n ? c)
)
? (
n
2
? c)
=
n(n? 1)
2
(c? 1)?
n
2
? c.
This completes the proof that
semi-greedy-MSDAG is an
O
(
nc
n(n?1)
2
(c?1)?
n
2
?c
)
= O(
1
n
) factor ap-
proximation algorithm for MSDAG.
Now let us consider the version of the state-
ment of the MSDAG problem that, rather than
maximising the weight of a spanning DAG D
?
of a weighted digraph G, looks to minimise the
weight of the set C
?
of edges that must be re-
moved from G in order for G ? C
?
to be an MS-
DAG for G. Clearly these problems are identical.
We refer to the minimisation version of the state-
ment as MSDAG
C
, and to C
?
as the complement
(in G) of the MSDAG D
?
:= G ? C
?
. Also, let
semi-greedy-MSDAG
C
be the same algorithm
as semi-greedy-MSDAG except that it outputs
C
?
rather than D
?
.
Using the same graphs and proof structure as in
the proof of Theorem 2, the following theorem can
be shown.
Theorem 3. semi-greedy-MSDAG
C
is
an O(n) factor approximation algorithm for
MSDAG
C
.
5 Conclusions and Open Questions
This paper provides some philosophical and math-
ematical foundations for the MSDAG problem as
decoding in semantic parsing. We have put for-
ward the view that the objective in semantic pars-
ing is not in fact to find the MSDAG, however it re-
mains open as to whether this mismatch can be tol-
erated, given empirical evidence of MSDAG de-
coding?s utility in semantic parsing. We have also
pointed to an explicit proof of the APX-hardness
(that of (Schluter, submitted)) of MSDAG and
given an approximation guarantee of the only pub-
lished approximation algorithm for this problem.
In particular, Schluter (submitted) provides
an approximation preserving reduction from
MSDAG
C
to IDMC. Moreover, the best known
approximation ratio for IDMC is O(n
11
23
) (Agar-
wal et al., 2007), which yields a better (in terms
of worst case error) approximation algorithm for
MSDAG
C
. An interesting open problem would
compare these two decoding approximation algo-
rithms empirically for semantic parsing decoding
and in terms of expected performance (or error)
both in general as well as specifically for semantic
parsing decoding.
References
Amit Agarwal, Noga Alon, and Moses Charikar. 2007.
Improved approximation for directed cut problems.
In Proceedings of STOC, San Diego, CA.
D. Heckerman, D. Geiger, and D. M. Chickering.
1995. Learning bayesian networks: The combina-
tion of knowledge and statistical data. Technical re-
port, Microsoft Research. MSR-TR-94-09.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL, pages 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Haji. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of HLT-EMNLP, pages
523?530, Vancouver, BC, Canada.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency dag parsing. In 22nd International
Conference on Computational Linguistics (Coling
2008), Manchester, UK.
Natalie Schluter. submitted. On the complexity of
finding a maximum spanning dag and other dag
parsing related restrictions.
Ivan Titov, James Henderson, Paola Merlo, and
Gabrielle Musillo. 2009. Online graph planariza-
tion for synchronous parsing of semantic and syn-
actic dependencies. In Proceedings of IJCAI 2009,
pages 1562?1567.
65
