Large linguistically-processed Web corpora for multiple languages
Marco Baroni
SSLMIT
University of Bologna
Italy
baroni@sslmit.unibo.it
Adam Kilgarriff
Lexical Computing Ltd. and
University of Sussex
Brighton, UK
adam@lexmasterclass.com
Abstract
The Web contains vast amounts of linguis-
tic data. One key issue for linguists and
language technologists is how to access
it. Commercial search engines give highly
compromised access. An alternative is to
crawl the Web ourselves, which also al-
lows us to remove duplicates and near-
duplicates, navigational material, and a
range of other kinds of non-linguistic mat-
ter. We can also tokenize, lemmatise and
part-of-speech tag the corpus, and load the
data into a corpus query tool which sup-
ports sophisticated linguistic queries. We
have now done this for German and Ital-
ian, with corpus sizes of over 1 billion
words in each case. We provide Web ac-
cess to the corpora in our query tool, the
Sketch Engine.
1 Introduction
The Web contains vast amounts of linguistic data
for many languages (Kilgarriff and Grefenstette,
2003). One key issue for linguists and language
technologists is how to access it. The drawbacks
of using commercial search engines are presented
in Kilgarriff (2003). An alternative is to crawl the
Web ourselves.1 We have done this for two lan-
guages, German and Italian, and here we report on
the pipeline of processes which give us reasonably
well-behaved, ?clean? corpora for each language.
1Another Web access option is Alexa (http://pages.
alexa.com/company/index.html), who allow the
user (for a modest fee) to access their cached Web directly.
Using Alexa would mean one did not need to crawl; however
in our experience, crawling, given free software like Heritrix,
is not the bottleneck. The point at which input is required is
the filtering out of non-linguistic material.
We use the German corpus (which was developed
first) as our example throughout. The procedure
was carried on a server running RH Fedora Core 3
with 4 GB RAM, Dual Xeon 4.3 GHz CPUs and
about 2.5 TB hard disk space. We are making the
tools we develop as part of the project freely avail-
able,2 in the hope of stimulating public sharing of
resources and know-how.
2 Crawl seeding and crawling
We would like a ?balanced? resource, containing
a range of types of text corresponding, to some
degree, to the mix of texts we find in designed lin-
guistic corpora (Atkins et al, 1992), though also
including text types found on the Web which were
not anticipated in linguists? corpus design discus-
sions. We do not want a ?blind? sample dominated
by product listings, catalogues and computer sci-
entists? bulletin boards. Our pragmatic solution is
to query Google through its API service for ran-
dom pairs of randomly selected content words in
the target language. In preliminary experimenta-
tion, we found that single word queries yielded
many inappropriate pages (dictionary definitions
of the word, top pages of companies with the word
in their name), whereas combining more than two
words retrieved pages with lists of words, rather
than collected text.
Ueyama (2006) showed how queries for words
sampled from traditional written sources such as
newspaper text and published essays tend to yield
?public sphere? pages (online newspaper, govern-
ment and academic sites), whereas basic vocabu-
lary/everyday life words tend to yield ?personal?
pages (blogs, bulletin boards). Since we wanted
both types, we obtained seed URLs with queries
2http://sslmitdev-online.sslmit.unibo.
it/wac/wac.php
87
for words from both kinds of sources. For Ger-
man, we sampled 2000 mid-frequency words from
a corpus of the Su?ddeutsche Zeitung newspaper
and paired them randomly. Then, we found a ba-
sic vocabulary list for German learners,3 removed
function words and particles and built 653 random
pairs. We queried Google via its API retrieving
maximally 10 pages for each pair. We then col-
lapsed the URL list, insuring maximal sparseness
by keeping only one (randomly selected) URL for
each domain, leaving a list of 8626 seed URLs.
They were fed to the crawler.
The crawls are performed using the Her-
itrix crawler,4 with a multi-threaded breadth-first
crawling strategy. The crawl is limited to pages
whose URL does not end in one of several suffixes
that cue non-html data (.pdf, .jpeg, etc.)5 For
German, the crawl is limited to sites from the .de
and .at domains. Heritrix default crawling op-
tions are not modified in any other respect. We
let the German crawl run for ten days, retrieving
gzipped archives (the Heritrix output format) of
about 85GB.
3 Filtering
We undertake some post-processing on the ba-
sis of the Heritrix logs. We identify documents
of mime type text/html and size between 5
and 200KB. As observed by Fletcher (2004) very
small documents tend to contain little genuine text
(5KB counts as ?very small? because of the html
code overhead) and very large documents tend to
be lists of various sorts, such as library indices,
store catalogues, etc. The logs also contain sha-
1 fingerprints, allowing us to identify perfect du-
plicates. After inspecting some of the duplicated
documents (about 50 pairs), we decided for a dras-
tic policy: if a document has at least one dupli-
cate, we discard not only the duplicate(s) but also
the document itself. We observed that, typically,
such documents came from the same site and were
warning messages, copyright statements and sim-
ilar, of limited or no linguistic interest. While the
strategy may lose some content, one of our gen-
eral principles is that, given how vast the Web is,
we can afford to privilege precision over recall.
All the documents that passed the pre-filtering
3http://mypage.bluewin.ch/a-z/
cusipage/
4http://crawler.archive.org
5Further work should evaluate pros and cons of retrieving
documents in other formats, e.g., Adobe pdf.
stage are run through a perl program that performs
1) boilerplate stripping 2) function word filtering
3) porn filtering.
Boilerplate stripping
By ?boilerplate? we mean all those components
of Web pages which are the same across many
pages. We include stripping out HTML markup,
javascript and other non-linguistic material in this
phase. We aimed to identify and remove sections
of a document that contain link lists, navigational
information, fixed notices, and other sections poor
in human-produced connected text. For purposes
of corpus construction, boilerplate removal is crit-
ical as it will distort statistics collected from the
corpus.6 We adopted the heuristic used in the Hyp-
pia project BTE tool,7: content-rich sections of a
page will have a low html tag density, whereas
boilerplate is accompanied by a wealth of html
(because of special formatting, newlines, links,
etc.) The method is based on general properties
of Web documents, so is relatively independent of
language and crawling strategy.
Function word and pornography filtering
Connected text in sentences reliably contains a
high proportion of function words (Baroni, to ap-
pear), so, if a page does not meet this criterion
we reject it. The German function word list con-
tains 124 terms. We require that a minimum of 10
types and 30 tokens appear in a page, with a ra-
tio of function words to total words of at least one
quarter. The filter also works as a simple language
identifier.8
Finally, we use a stop list of words likely to oc-
cur in pornographic Web pages, not out of prudery,
but because they tend to contain randomly gener-
ated text, long keyword lists and other linguisti-
cally problematic elements. We filter out docu-
ments that have at least three types or ten tokens
from a list of words highly used in pornography.
The list was derived from the analysis of porno-
graphic pages harvested in a previous crawl. This
is not entirely satisfactory, since some of the words
6We note that this phase currently removes the links from
the text, so we can no longer explore the graph structure of
the dataset. In future we may retain link structure, to support
research into the relation between it and linguistic character-
istics.
7http://www.smi.ucd.ie/hyppia/
8Of course, these simple methods will not filter out all
machine-generated text (typically produced as part of search
engine ranking scams or for other shady purposes); some-
times this appears to have been generated with a bigram lan-
guage model, and thus identifying it with automated tech-
niques is far from trivial.
88
in the list, taken in isolation, are wholly innocent
(fat, girls, tongue, etc.) We shall revisit the strat-
egy in due course.
This filtering took 5 days and resulted in a ver-
sion of the corpus containing 4.86M documents
for a total of 20GB of uncompressed data.
4 Near-duplicate detection
We use a simplified version of the ?shingling? al-
gorithm (Broder et al, 1997). For each document,
after removing all function words, we take finger-
prints of a fixed number s of randomly selected n-
grams; then, for each pair of documents, we count
the number of shared n-grams, which can be seen
as an unbiased estimate of the overlap between the
two documents (Broder et al, 1997; Chakrabarti,
2002). We look for pairs of documents sharing
more than t n-grams, and we discard one of the
two.
After preliminary experimentation, we chose to
extract 25 5-grams from each document, and to
treat as near-duplicates documents that shared at
least two of these 5-grams. Near-duplicate spot-
ting on the German corpus took about 4 days.
2,466,271 near-duplicates were removed. The cor-
pus size decreased to 13GB. Most of the process-
ing time was spent in extracting the n-grams and
adding the corresponding fingerprints to the data-
base (which could be parallelized).
5 Part-of-speech tagging/lemmatization
and post-annotation cleaning
We performed German part-of-speech tagging and
lemmatization with TreeTagger.9 Annotation took
5 days. The resulting corpus contains 2.13B
words, or 34GB of data including annotation.
After inspecting various documents from the
annotated corpus, we decided to perform a further
round of cleaning. There are two reasons for this:
first, we can exploit the annotation to find other
anomalous documents, through observing where
the distribution of parts-of-speech tags is very un-
usual and thus not likely to contain connected text.
Second, the TreeTagger was not trained on Web
data, and thus its performance on texts that are
heavy on Web-like usage (e.g., texts all in lower-
case, colloquial forms of inflected verbs, etc.) is
dismal. While a better solution to this second
problem would be to re-train the tagger on Web
9http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger
data (ultimately, the documents displaying the sec-
ond problem might be among the most interest-
ing ones to have in the corpus!), for now we try to
identify the most problematic documents through
automated criteria and discard them. The cues we
used included the number of words not recognised
by the lemmatizer; the proportion of words with
upper-case initial letters; proportion of nouns, and
proportion of sentence markers.
After this further processing step, the corpus
contains 1,870,259 documents from 10818 differ-
ent domains, and its final size is 1.71 billion to-
kens (26GB of data, with annotation). The final
size of the Italian corpus is 1,875,337 documents
and about 1.9 billion tokens.
6 Indexing and Web user interface
We believe that matters of efficient indexing and
user friendly interfacing will be crucial to the suc-
cess of our initiative, both because many linguists
will lack the relevant technical skills to write their
own corpus-access routines, and because we shall
not publicly distribute the corpora for copyright
reasons; an advanced interface that allows lin-
guists to do actual research on the corpus (includ-
ing the possibility of saving settings and results
across sessions) will allow us to make the corpus
widely available while keeping it on our servers.10
We are using the Sketch Engine,11 a corpus query
tool which has been widely used in lexicography
and which supports queries combining regular ex-
pressions and boolean operators over words, lem-
mas and part-of-speech tags.
7 Comparison with other corpora
We would like to compare the German Web cor-
pus to an existing ?balanced? corpus of German
attempting to represent a broad range of genres
and topics. Unfortunately, as far as we know no
resource of this sort is publicly available (which
is one of the reasons why we are interested in de-
veloping the German Web corpus in the first in-
stance.) Instead, we use a corpus of newswire
articles from the Austria Presse Agentur (APA,
kindly provided to us by ?OFAI) as our reference
10The legal situation is of course complex. We consider
that our case is equivalent to that of other search engines,
and that offering linguistically-encoded snippets of pages to
researchers does not go beyond the ?fair use? terms routinely
invoked by search engine companies in relation to Web page
caching.
11http://www.sketchengine.co.uk/
89
WEB APA
ich hier APA NATO
dass wir Schlu? EU
und man Prozent Forts
sie nicht Mill AFP
ist das MRD Dollar
oder sind Wien Reuters
kann so Kosovo Dienstag
du mir DPA Mittwoch
wenn ein US Donnerstag
was da am sei
Table 1: Typical Web and APA words
point. This corpus contains 28M tokens, and,
despite its uniformity in terms of genre and re-
stricted thematic range, it has been successfully
employed as a general-purpose German corpus in
many projects. After basic regular-expression-
based normalization and filtering, the APA con-
tains about 500K word types, the Web corpus
about 7.4M. There is a large overlap among the 30
most frequent words in both corpora: 24 out of 30
words are shared. The non-overlapping words oc-
curring in the Web top 30 only are function words:
sie ?she?, ich ?I?, werden ?become/be?, oder ?or?,
sind ?are?, er ?he?. The words only in the APA
list show a bias towards newswire-specific vocab-
ulary (APA, Prozent ?percent?, Schlu? ?closure?)
and temporal expressions that are also typical of
newswires (am ?at?, um ?on the?, nach ?after?).
Of the 232,322 hapaxes (words occurring only
once) in the APA corpus, 170,328 (73%) occur in
the Web corpus as well.12 89% of these APA ha-
paxes occur more than once in the Web corpus,
suggesting how the Web data will help address
data sparseness issues.
Adopting the methodology of Sharoff (2006),
we then extracted the 20 words most characteris-
tics of the Web corpus vs. APA and vice versa,
based on the log-likelihood ratio association mea-
sure. Results are presented in Table 1. The APA
corpus has a strong bias towards newswire par-
lance (acronyms and named entities, temporal ex-
pressions, financial terms, toponyms), whereas the
terms that come out as most typical of the Web
corpus are function words that are not strongly
connected with any particular topic or genre. Sev-
eral of these top-ranked function words mark first
and second person forms (ich, du, wir, mir).
This preliminary comparison both functioned as
a ?sanity check?, showing that there is consider-
12Less than 1% of the Web corpus hapaxes are attested in
the APA corpus.
able overlap between our corpus and a smaller cor-
pus used in previous research, and suggested that
the Web corpus has more a higher proportion of
interpersonal material.
8 Conclusion
We have developed very large corpora from the
Web for German and Italian (with other languages
to follow). We have filtered and cleaned the text so
that the obvious problems with using the Web as a
corpus for linguistic research do not hold. Prelim-
inary evidence suggests the ?balance? of our Ger-
man corpus compares favourably with that of a
newswire corpus (though of course any such claim
begs a number of open research questions about
corpus comparability). We have lemmatised and
part-of-speech-tagged the data and loaded it into
a corpus query tool supporting sophisticated lin-
guistic queries, and made it available to all.
References
B. Atkins, J. Clear, and N. Ostler. 1992. Corpus design
criteria. Literary and Linguistic Computing, 7:1?16.
M. Baroni. to appear. Distributions in text. In
A. Lu?deling and M. Kyto?, editors, Corpus lin-
guistics: An international handbook. Mouton de
Gruyter, Berlin.
A. Broder, S. Glassman, M. Manasse, and G. Zweig.
1997. Syntactic clustering of the Web. In Proc.
Sixth International World-Wide Web Conference.
S. Chakrabarti. 2002. Mining the Web: Discovering
knowledge from hypertext data. Morgan Kaufmann,
San Francisco.
W. Fletcher. 2004. Making the web more useful as
a source for linguistic corpora. In U. Connor and
T. Upton, editors, Corpus Linguistics in North Amer-
ica 2002.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the Web as corpus. Compu-
tational Linguistics, 29(3):333?347.
A. Kilgarriff. 2003. Linguistic search engine. In
K. Simov, editor, Proc. SPROLAC Workshop, Lan-
caster.
S. Sharoff. 2006. Creating general-purpose corpora
using automated search engine queries. In M. Ba-
roni and S. Bernardini, editors, WaCky! Working pa-
pers on the Web as Corpus. Gedit, Bologna.
M. Ueyama. 2006. Creation of general-purpose
Japanese Web corpora with different search engine
query strategies. In M. Baroni and S. Bernardini,
editors, WaCky! Working papers on the Web as Cor-
pus. Gedit, Bologna.
90
211
212
213
214
c? 2003 Association for Computational Linguistics
Introduction to the Special Issue on the
Web as Corpus
Adam Kilgarriff? Gregory Grefenstette?
Lexicography MasterClass Ltd. and ITRI Clairvoyance Corporation
University of Brighton
The Web, teeming as it is with language data, of all manner of varieties and languages, in
vast quantity and freely available, is a fabulous linguists? playground. This special issue of
Computational Linguistics explores ways in which this dream is being explored.
1. Introduction
The Web is immense, free, and available by mouse click. It contains hundreds of
billions of words of text and can be used for all manner of language research.
The simplest language use is spell checking. Is it speculater or speculator? Google
gives 67 for the former (usefully suggesting the latter might have been intended) and
82,000 for the latter. Question answered.
Language scientists and technologists are increasingly turning to the Web as a
source of language data, because it is so big, because it is the only available source
for the type of language in which they are interested, or simply because it is free
and instantly available. The mode of work has increased dramatically from a standing
start seven years ago with the Web being used as a data source in a wide range of
research activities: The papers in this special issue form a sample of the best of it. This
introduction to the issue aims to survey the activities and explore recurring themes.
We first consider whether the Web is indeed a corpus, then present a history of
the theme in which we view the Web as a development of the empiricist turn that has
brought corpora center stage in the course of the 1990s. We briefly survey the range
of Web-based NLP research, then present estimates of the size of the Web, for English
and for other languages, and a simple method for translating phrases. Next we open
the Pandora?s box of representativeness (concluding that the Web is not representative
of anything other than itself, but then neither are other corpora, and that more work
needs to be done on text types). We then introduce the articles in the special issue and
conclude with some thoughts on how the Web could be put at the linguist?s disposal
rather more usefully than current search engines allow.
1.1 Is the Web a Corpus?
To establish whether the Web is a corpus we need to find out, discover, or decide what
a corpus is. McEnery and Wilson (1996, page 21) say
In principle, any collection of more than one text can be called a
corpus. . . . But the term ?corpus? when used in the context of modern
linguistics tends most frequently to have more specific connotations
than this simple definition provides for. These may be considered un-
? Lewes Rd, Brighton, BN2 4JG, UK. E-mail: Adam.Kilgarriff@itri.brighton.ac.uk
? Suite 700, 5001 Baum Blvd, Pittsburgh, PA 15213-1854. E-mail: grefen@clairvoyancecorp.com
334
Computational Linguistics Volume 29, Number 3
der four main headings: sampling and representativeness, finite size,
machine-readable form, a standard reference.
We would like to reclaim the term from the connotations. Many of the collections
of texts that people use and refer to as their corpus, in a given linguistic, literary, or
language-technology study, do not fit. A corpus comprising the complete published
works of Jane Austen is not a sample, nor is it representative of anything else. Closer
to home, Manning and Schu?tze (1999, page 120) observe:
In Statistical NLP, one commonly receives as a corpus a certain amount
of data from a certain domain of interest, without having any say in
how it is constructed. In such cases, having more training data is
normally more useful than any concerns of balance, and one should
simply use all the text that is available.
We wish to avoid a smuggling of values into the criterion for corpus-hood. McEnery
and Wilson (following others before them) mix the question ?What is a corpus?? with
?What is a good corpus (for certain kinds of linguistic study)?? muddying the simple
question ?Is corpus x good for task y?? with the semantic question ?Is x a corpus at
all?? The semantic question then becomes a distraction, all too likely to absorb energies
that would otherwise be addressed to the practical one. So that the semantic question
may be set aside, the definition of corpus should be broad. We define a corpus simply
as ?a collection of texts.? If that seems too broad, the one qualification we allow relates
to the domains and contexts in which the word is used rather than its denotation: A
corpus is a collection of texts when considered as an object of language or literary study.
The answer to the question ?Is the web a corpus?? is yes.
2. History
For chemistry or biology, the computer is merely a place to store and process infor-
mation gleaned about the object of study. For linguistics, the object of study itself (in
one of its two primary forms, the other being acoustic) is found on computers. Text
is an information object, and a computer?s hard disk is as valid a place to go for its
realization as the printed page or anywhere else.
The one-million-word Brown corpus opened the chapter on computer-based lan-
guage study in the early 1960s. Noting the singular needs of lexicography for big data,
in the 1970s Sinclair and Atkins inaugurated the COBUILD project, which raised the
threshold of viable corpus size from one million to, by the early 1980s, eight million
words (Sinclair 1987). Ten years on, Atkins again took the lead with the develop-
ment (from 1988) of the British National Corpus (BNC) (Burnard 1995), which raised
horizons tenfold once again, with its 100 million words and was in addition widely
available at low cost and covered a wide spectrum of varieties of contemporary British
English.1 As in all matters Zipfian, logarithmic graph paper is required. Where corpus
size is concerned, the steps of interest are 1, 10, 100, . . . , not 1, 2, 3, . . .
Corpora crashed into computational linguistics at the 1989 ACL meeting in Van-
couver, but they were large, messy, ugly objects clearly lacking in theoretical integrity
in all sorts of ways, and many people were skeptical regarding their role in the disci-
pline. Arguments raged, and it was not clear whether corpus work was an acceptable
1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in
speech recognition (see Church and Mercer [1993] for references).
335
Kilgarriff and Grefenstette Web as Corpus: Introduction
part of the field. It was only with the highly successful 1993 special issue of this
journal, ?Using Large Corpora? (Church and Mercer 1993), that the relation between
computational linguistics and corpora was consummated.
There are parallels with Web corpus work. The Web is anarchic, and its use is
not in the familiar territory of computational linguistics. However, as students with
no budget or contacts realize, it is the obvious place to obtain a corpus meeting their
specifications, as companies want the research they sanction to be directly related
to the language types they need to handle (almost always available on the Web), as
copyright continues to constrain ?traditional? corpus development,2 as people want
to explore using more data and different text types, so Web-based work will grow.
The Web walked in on ACL meetings starting in 1999. Rada Mihalcea and Dan
Moldovan (1999) used hit counts for carefully constructed search engine queries to
identify rank orders for word sense frequencies, as an input to a word sense dis-
ambiguation engine. Philip Resnik (1999) showed that parallel corpora?until then a
promising research avenue but largely constrained to the English-French Canadian
Hansard?could be found on the Web: We can grow our own parallel corpus using
the many Web pages that exist in parallel in local and in major languages. We are
glad to have the further development of this work (co-authored by Noah Smith) pre-
sented in this special issue. In the student session of ACL 2000, Rosie Jones and Rayid
Ghani (2001) showed how, using the Web, one can build a language-specific corpus
from a single document in that language. In the main session Atsushi Fujii and Tet-
suya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be
acquired from the Web.
2.1 Some Current Themes
Since then there have been many papers, at ACL and elsewhere, and we can mention
only a few. The EU MEANING project (Rigau et al 2002) takes forward the exploration
of the Web as a data source for word sense disambiguation, working from the premise
that within a domain, words often have just one meaning, and that domains can be
identified on the Web. Mihalcea and Tchklovski complement this use of Web as corpus
with Web technology to gather manual word sense annotations on the Word Expert
Web site.3 Santamari?a et al, in this issue, discuss how to link word senses to Web
directory nodes, and thence to Web pages.
The Web is being used to address data sparseness for language modeling. In
addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers
lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda
et al (2003) ?balance? their corpus using Web documents.
The information retrieval community now has a Web track as a component of its
TREC evaluation initiative. The corpus for this exercise is a substantial (around 100GB)
sample of the Web, largely using documents in the .gov top level domain, as frozen
at a given date (Hawking et al 1999).
The Web has recently been used by groups at Sheffield and Microsoft, among
others, as a source of answers for question-answering applications, in a merge of search
engine and language-processing technologies (Greenwood, Roberts, and Gaizauskas
2 Lawyers may argue that the legal issues for Web corpora are no different from those around non-Web
corpora. However, first, language researchers can develop Web corpora just by saving Web pages on
their own computer without any copying, thereby avoiding copyright issues, and second, a Web
corpus is a very minor subspecies of the caches and indexes held by search engines and assorted other
components of the infrastructure of the Web: If a Web corpus is infringing copyright, then it is merely
doing on a small scale what search engines such as Google are doing on a colossal scale.
3 ?http://teach-computers.org/word-expert.html?.
336
Computational Linguistics Volume 29, Number 3
2002; Dumais et al 2002). AnswerBus (Zheng 2002) will answer questions posed in
English, German, French, Spanish, Italian, and Portuguese.
Naturally, the Web is also coming into play in other areas of linguistics. Agirre
et al 2000) are exploring the automatic population of existing ontologies using the
Web as a source for new instances. Varantola (2000) shows how translators can use
?just-in-time? sublanguage corpora to choose correct target language terms for areas
in which they are not expert. Fletcher (2002) demonstrates methods for gathering and
using Web corpora in a language-teaching context.
2.2 The 100M Words of the BNC
One hundred million words is a large enough corpus for many empirical strategies
for learning about language, either for linguists and lexicographers (Baker, Fillmore,
and Lowe 1998; Kilgarriff and Rundell 2002) or for technologies that need quantitative
information about the behavior of words as input (most notably parsers [Briscoe and
Carroll 1997; Korhonen 2000]). However, for some purposes, it is not large enough.
This is an outcome of the Zipfian nature of word frequencies. Although 100 million is
a huge number, and the BNC contains ample information on the dominant meanings
and usage patterns for the 10,000 words that make up the core of English, the bulk
of the lexical stock occurs less than 50 times in the BNC, which is not enough to
draw statistically stable conclusions about the word. For rarer words, rare meanings
of common words, and combinations of words, we frequently find no evidence at all.
Researchers are obliged to look to larger data sources (Keller and Lapata, this issue;
also Section 3.3). They find that probabilistic models of language based on very large
quantities of data, even if those data are noisy, are better than ones based on estimates
(using sophisticated smoothing techniques) from smaller, cleaner data sets.
Another argument is made vividly by Banko and Brill (2001). They explore the
performance of a number of machine learning algorithms (on a representative dis-
ambiguation task) as the size of the training corpus grows from a million to a bil-
lion words. All the algorithms steadily improve in performance, though the question
?Which is best?? gets different answers for different data sizes. The moral: Perfor-
mance improves with data size, and getting more data will make more difference than
fine-tuning algorithms.
2.3 Giving and Taking
Dragomir Radev has made a useful distinction between NLP ?giving? and ?taking.?4
NLP can give to the Web technologies such as summarization (for Web pages or
Web search results); machine translation; multilingual document retrieval; question-
answering and other strategies for finding not only the right document, but the right
part of a document; and tagging, parsing, and other core technologies (to improve
indexing for search engines, the viability of this being a central information retrieval
research question for the last 20 years). ?Taking? is, simply, using the Web as a source
of data for any CL or NLP goal and is the theme of this special issue. If we focus too
closely on the giving side of the equation, we look only at short to medium-term goals.
For the longer term, for ?giving? as well as for other purposes, a deeper understanding
of the linguistic nature of the Web and its potential for CL/NLP is required. For that,
we must take the Web itself, in whatever limited way, as an object of study.
Much Web search engine technology has been developed with reference to lan-
guage technology. The prototype for AltaVista was developed in a joint project be-
4 In remarks made in a panel discussion at the Empirical NLP Conference, Hong Kong, October 2002.
337
Kilgarriff and Grefenstette Web as Corpus: Introduction
tween Oxford University Press (exploring methods for corpus lexicography [Atkins
1993]) and DEC (interested in fast access to very large databases). Language identifi-
cation algorithms (Beesley 1988; Grefenstette 1995), now widely used in Web search
engines, were developed as NLP technology. The special issue explores a ?homecom-
ing? of Web technologies, with the Web now feeding one of the hands that fostered
it.
3. Web Size and the Multilingual Web
There were 56 million registered network addresses in July 1999, 125 million in January
2001, and 172 million in January 2003. A plot of this growth of the Web in terms of
computer hosts can easily be generated. Linguistic aspects take a little more work
and can be estimated only by sampling and extrapolation. Lawrence and Giles (1999)
compared the overlap between page lists returned by different Web browsers over the
same set of queries and estimated that, in 1999, there were 800 million indexable Web
pages available. By sampling pages, and estimating an average page length of seven
to eight kilobytes of nonmarkup text, they concluded that there might be six terabytes
of text available then. In 2003, Google claims to search four times this number of Web
pages, which raises the number of bytes of text available just through this one Web
server to over 20 terabytes from directly accessible Web pages. At an average of 10
bytes per word, a generous estimate for Latin-alphabet languages, that suggests two
thousand billion words.
The Web is clearly a multilingual corpus. How much of it is English? Xu (2000) es-
timated that 71% of the pages (453 million out of 634 million Web pages indexed by the
Excite engine at that time) were written in English, followed by Japanese (6.8%), Ger-
man (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish
(0.7%).
We have measured the counts of some English phrases according to various search
engines over time and compared them with counts in the BNC, which we know has
100 million words. Table 1 shows these counts in the BNC, on AltaVista in 1998 and
in 2001, and then on Alltheweb in 2003. For example, the phrase deep breath appears
732 times in the BNC. It was indexed 54,550 times by AltaVista in 1998. This rose
Table 1
Frequencies of English phrases in the BNC and on AltaVista in 1998 and 2001, and on
AlltheWeb in 2003. The counts for the BNC and AltaVista are for individual occurrences of the
phrase. The counts for AlltheWeb are page counts (the phrase may appear more than once on
any page).
Sample Phrase BNC WWW WWW WWW
(100 M) Fall 1998 Fall 2001 Spring 2003
medical treatment 414 46,064 627,522 1,539,367
prostate cancer 39 40,772 518,393 1,478,366
deep breath 732 54,550 170,921 868,631
acrylic paint 30 7,208 43,181 151,525
perfect balance 38 9,735 35,494 355,538
electromagnetic radiation 39 17,297 69,286 258,186
powerful force 71 17,391 52,710 249,940
concrete pipe 10 3,360 21,477 43,267
upholstery fabric 6 3,157 8,019 82,633
vital organ 46 7,371 28,829 35,819
338
Computational Linguistics Volume 29, Number 3
to 170,921 in 2001. And in 2003, we could find 868,631 Web pages containing the
contiguous words deep breath according to AlltheWeb. The numbers found through the
search engines are more than three orders of magnitude higher than the BNC counts,
giving a first indication of the size of the English corpus available on the Web.
We can derive a more precise estimate of the number of words available through
a search engine by using the counts of function words as predictors of corpus size.
Function words, such as the, with, and in, occur with a frequency that is relatively
stable over many different types of texts. From a corpus of known size, we can cal-
culate the frequency of the function words and extrapolate. In the 90-million-word
written-English component of the BNC, the appears 5,776,487 times, around seven
times for every 100 words. In the U.S. Declaration of Independence, the occurs 84
times. We predict that the Declaration is about 84 ? 100/7 = 1,200 words long. In fact,
the text contains about 1,500 words. Using the frequency of one word gives a first
approximation. A better result can be obtained by using more data points.
From the first megabyte of the German text found in the European Corpus Ini-
tiative Multilingual Corpus,5 we extracted frequencies for function words and other
short, common words. We removed from the list words that were also common words
in other languages.6 AltaVista provided, on its results pages, along with a page count
for a query, the number of times that each query word was found on the Web.7 Ta-
ble 2 shows the relative frequency of the words from our known corpus, the index
frequencies that AltaVista gave (February 2000), and the consequent estimates of the
size of the German-language Web indexed by AltaVista.
We set aside words which give discrepant predictions (too high or too low) as (1)
AltaVista does not record in its index the language a word comes from, so the count
for the string die includes both the German and English occurrences, and (2) a word
might be under- or overrepresented in the training corpus or on the Web (consider
here, which occurs very often in ?click here?). Averaging the remaining predictions
gives an estimate of three billion words of German that could be accessed through
AltaVista on the day in February 2000 that we conducted our test.
Table 2
Short German words in the ECI corpus and via AltaVista, giving German Web estimates.
Word Known-Size-Corpus AltaVista Prediction for
Relative Frequency Frequency German-Language Web
oder 0.00561180 13,566,463 2,417,488,684
sind 0.00477555 11,944,284 2,501,132,644
auch 0.00581108 15,504,327 2,668,062,907
wird 0.00400690 11,286,438 2,816,750,605
nicht 0.00646585 18,294,174 2,829,353,294
eine 0.00691066 19,739,540 2,856,389,983
sich 0.00604594 17,547,518 2,902,363,900
ist 0.00886430 26,429,327 2,981,546,991
auf 0.00744444 24,852,802 3,338,438,082
und 0.02892370 101,250,806 3,500,617,348
Average 3,068,760,356
5 ?http://www.elsnet.org/resources/eciCorpus.html?.
6 These lists of short words and frequencies were initially used to create a language identifier.
7 AltaVista has recently stopped providing information about how often individual words in a query
have been indexed and now returns only a page count for the entire query.
339
Kilgarriff and Grefenstette Web as Corpus: Introduction
Table 3
Estimates of Web size in words, as indexed by AltaVista, for various languages.
Language Web Size
Albanian 10,332,000
Breton 12,705,000
Welsh 14,993,000
Lithuanian 35,426,000
Latvian 39,679,000
Icelandic 53,941,000
Basque 55,340,000
Latin 55,943,000
Esperanto 57,154,000
Roumanian 86,392,000
Irish 88,283,000
Estonian 98,066,000
Slovenian 119,153,000
Croatian 136,073,000
Malay 157,241,000
Turkish 187,356,000
Language Web Size
Catalan 203,592,000
Slovakian 216,595,000
Polish 322,283,000
Finnish 326,379,000
Danish 346,945,000
Hungarian 457,522,000
Czech 520,181,000
Norwegian 609,934,000
Swedish 1,003,075,000
Dutch 1,063,012,000
Portuguese 1,333,664,000
Italian 1,845,026,000
Spanish 2,658,631,000
French 3,836,874,000
German 7,035,850,000
English 76,598,718,000
This technique has been tested on controlled data (Grefenstette and Nioche 2000)
in which corpora of different languages were mixed in various proportions and found
to give reliable results. Table 3 provides estimates for the number of words that
were available in 30 different Latin-script languages through AltaVista in March 2001.
English led the pack with 76 billion words, and seven additional languages already
had over a billion.
From the table, we see that even ?smaller? languages such as Slovenian, Croatian,
Malay, and Turkish have more than one hundred million words on the Web. Much of
the research that has been undertaken on the BNC simply exploits its scale and could
be transferred directly to these languages.
The numbers presented in Table 3 are lower bounds, for a number of reasons:
? AltaVista covers only a fraction of the indexable Web pages available
(the fraction was estimated at just 15% by Lawrence and Giles [1999]).
? AltaVista may be biased toward North American (mainly
English-language) pages by the strategy it uses to crawl the Web.
? AltaVista indexes only pages that can be directly called by a URL and
does not index text found in databases that are accessible through dialog
windows on Web pages (the ?hidden Web?). This hidden Web is vast
(consider MedLine,8 just one such database, with more than five billion
words; see also Ipeirotis, Gravano, and Sahami [2001]), and it is not
considered at all in the AltaVista estimates.
Repeating the procedure after an interval, the second author and Nioche showed
that the proportion of non-English text to English is growing. In October 1996 there
8 ?http://www4.ncbi.nlm.nih.gov/PubMed/?.
340
Computational Linguistics Volume 29, Number 3
Table 4
AltaVista frequencies for candidate translations of groupe de travail.
labor cluster 21
labor grouping 28
labour concern 45
labor concern 77
work grouping 124
work cluster 279
labor collective 423
labour collective 428
work collective 759
work concern 772
labor group 3,977
labour group 10,389
work group 148,331
were 38 German words for every 1,000 words of English indexed by AltaVista. In
August 1999, there were 71, and in March 2001, 92.
3.1 Finding the Right Translation
How can these large numbers be used for other language-processing tasks? Consider
the compositional French noun phrase groupe de travail. In the MEMODATA bilingual
dictionary,9 the French word groupe is translated by the English words cluster, group,
grouping, concern, and collective. The French word travail translates as work, labor, or
labour. Many Web search engines allow the user to search for adjacent phrases. Com-
bining the possible translations of groupe de travail and submitting them to AltaVista
in early 2003 yielded the counts presented in Table 4. The phrase work group is 15
times more frequent than any other and is also the best translation among the tested
possibilities. A set of controlled experiments of this form is described in Grefenstette
(1999). In Grefenstette?s study, a good translation was found in 87% of ambiguous
cases from German to English and 86% of ambiguous cases from Spanish to English.
4. Representativeness
We know the Web is big, but a common response to a plan to use the Web as a
corpus is ?but it?s not representative.? There are a great many things to be said about
this. It opens up a pressing yet alost untouched practical and theoretical issue for
computational linguistics and language technology.
4.1 Theory
First, ?representativeness? begs the question ?representative of what?? Outside very
narrow, specialized domains, we do not know with any precision what existing corpora
might be representative of. If we wish to develop a corpus of general English, we
may think it should be representative of general English, so we then need to define
the population of ?general English-language events? of which the corpus will be a
sample. Consider the following issues:
? Production and reception: Is a language event an event of speaking or
writing, or one of reading or hearing? Standard conversations have, for
each utterance, one speaker and one hearer. A Times newspaper article
has (roughly) one writer and several hundred thousand readers.
9 See ?http://www.elda.fr/cata/text/M0001.html?. The basic multilingual lexicon produced by
MEMODATA contains 30,000 entries for five languages: French, English, Italian, German, Spanish.
341
Kilgarriff and Grefenstette Web as Corpus: Introduction
? Speech and text: Do speech events and written events have the same
status? It seems likely that there are orders of magnitude more speech
events than writing events, yet most corpus research to date has tended
to focus on the more tractable task of gathering and working with text.
? Background language: Does muttering under one?s breath or talking in
one?s sleep constitute a speech event, and does doodling with words
constitute a writing event? Or, on the reception side, does passing (and
possibly subliminally reading) a roadside advertisement constitute a
reading event? And what of having the radio on but not attending to it,
or the conversational murmur in a restaurant?
? Copying: if I?d like to teach the world to sing, and, like Michael Jackson or
the Spice Girls, am fairly successful in this goal and everyone sings my
song, then does each individual singing constitute a distinct language
production event?
In the text domain, organizations such as Reuters produce news feeds
that are typically adapted to the style of a particular newspaper and then
republished: Is each republication a new writing event? (These issues,
and related themes of cut-and-paste authorship, ownership, and
plagiarism, are explored in Wilks [2003].)
4.2 Technology
Application developers urgently need to know what to do about sublanguages. It
has often been argued that, within a sublanguage, few words are ambiguous, and a
limited repertoire of grammatical structures is used (Kittredge and Lehrberger 1982).
This points to sublanguage-specific application development?s being substantially sim-
pler than general-language application development. However, many of the resources
that developers may wish to use are general-language resources, such as, for English,
WordNet, ANLT, XTag, COMLEX, and the BNC. Are they relevant for building ap-
plications for sublanguages? Can they be used? Is it better to use a language model
based on a large general-language corpus or a relatively tiny corpus of the right kind
of text? Nobody knows. There is currently no theory, no mathematical models, and
almost no discussion.
A related issue is that of porting an application from the sublanguage for which
it was developed to another. It should be possible to use corpora for the two sublan-
guages to estimate how large a task this will be, but again, our understanding is in
its infancy.
4.3 Language Modeling
Much work in recent years has gone into developing language models. Clearly, the
statistics for different types of text will be different (Biber 1993). This imposes a lim-
itation on the applicability of any language model: We can be confident only that
it predicts the behavior of language samples of the same text type as the training-
data text type (and we can be entirely confident only if training and test samples are
random samples from the same source).
When a language technology application is put to use, it will be applied to new
text for which we cannot guarantee the text type characteristics. There is little work
on assessing how well one language model fares when applied to a text type that is
different from that of the training corpus. Two studies in this area are Sekine (1997)
and Gildea (2001), both of which show substantial variation in model performance
342
Computational Linguistics Volume 29, Number 3
Table 5
Hits for Spanish pensar que with and without possible ?dequeismos errors? (spurious de
between the verb and the relative), from Alltheweb.com (March 2003). Not all items are errors
(e.g., ?. . .pienso de que manera. . .? . . . think how. . .). The correct form is always at least 500
times more common than any potentially incorrect form.
pienso de que 388
pienso que 356,874
piensas de que 173
piensas que 84,896
piense de que 92
piense que 67,243
pensar de que 1,640
pensar que 661,883
when the training corpus changes. The lack of theory of text types leaves us without
a way of assessing the usefulness of language-modeling work.
4.4 Language Errors
Web texts are produced by a wide variety of authors. In contrast to paper-based, copy-
edited published texts, Web-based texts may be produced cheaply and rapidly with
little concern for correctness. On Google a search for ?I beleave? has 3,910 hits, and
?I beleive,? 70,900. The correct ?I believe? appears on over four million pages. Table 5
presents what is regarded as a common grammatical error in Spanish, comparing the
frequency of such forms to the accepted forms on the Web. All the ?erroneous? forms
exist, but much less often than the ?correct? forms. The Web is a dirty corpus, but
expected usage is much more frequent than what might be considered noise.
4.5 Sublanguages and General-Language-Corpus Composition
A language can be seen as a modest core of lexis, grammar, and constructions, plus
a wide array of different sublanguages, as used in each of a myriad of human ac-
tivities. This presents a challenge to general-language resource developers: Should
sublanguages be included? The three possible positions are
? No, none should.
? Some, but not all, should.
? Yes, all should.
The problem with the first position is that, with all sublanguages removed, the
residual core gives an impoverished view of language (quite apart from demarcation
issues and the problem of determining what is left). The problem with the second is
that it is arbitrary. The BNC happens to include cake recipes and research papers on
gastro-uterine diseases, but not car manuals or astronomy texts. The third has not,
until recently, been a viable option.
4.6 Literature
To date, corpus developers have been obliged to make pragmatic decisions about the
sorts of text to go into a corpus. Atkins, Clear, and Ostler (1992) describe the desiderata
and criteria used for the BNC, and this stands as a good model for a general-purpose,
general-language corpus. The word representative has tended to fall out of discussions,
to be replaced by the meeker balanced.
343
Kilgarriff and Grefenstette Web as Corpus: Introduction
The recent history of mathematically sophisticated modeling of language variation
begins with Biber (1988), who identifies and quantifies the linguistic features associated
with different spoken and written text types. Habert and colleagues (Folch et al 2000;
Beaudouin et al 2001) have been developing a workstation for specifying subcorpora
according to text type, using Biber-style analyses, among others. In Kilgarriff (2001)
we present a first pass at quantifying similarity between corpora, and Cavaglia (2002)
continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001)
directly address the relation between NLP systems and text type; one further such item
is Roland et al (2000). Buitelaar and Sacaleanu (2001) explores the relation between
domain and sense disambiguation. A practical discussion of a central technical concern
is Vossen (2001), which tailors a general-language resource for a domain.
Baayen (2001) presents sophisticated mathematical models for word frequency
distributions, and it is likely that his mixture models have potential for modeling
sublanguage mixtures. His models have been developed with a specific, descriptive
goal in mind and using a small number of short texts: It is unclear whether they can
be usefully applied in NLP.
Although the extensive literature on text classification (Manning and Schu?tze 1999,
pages 575?608) is certainly relevant, it most often starts from a given set of categories
and cannot readily be applied to the situation in which the categories are not known in
advance. Also, the focus is usually on content words and topics or domains, with other
differences of genre or sublanguage remaining unexamined. Exceptions focusing on
genre include Kessler, Nunberg, and Schu?tze (1997) and Karlgren and Cutting (1994).
4.7 Representativeness: Conclusion
The Web is not representative of anything else. But neither are other corpora, in any
well-understood sense. Picking away at the question merely exposes how primitive
our understanding of the topic is and leads inexorably to larger and altogether more
interesting questions about the nature of language, and how it might be modeled.
?Text type? is an area in which our understanding is, as yet, very limited. Although
further work is required irrespective of the Web, the use of the Web forces the issue.
Where researchers use established corpora, such as Brown, the BNC, or the Penn
Treebank, researchers and readers are willing to accept the corpus name as a label for
the type of text occurring in it without asking critical questions. Once we move to the
Web as a source of data, and our corpora have names like ?April03-sample77,? the
issue of how the text type(s) can be characterized demands attention.
5. Introduction to Articles in This Special Issue
One use of a corpus is to extract a language model: a list of weighted words, or
combinations of words, that describe (1) how words are related, (2) how they are
used with each other, and (3) how common they are in a given domain. Language
models are used in speech processing to predict which word combinations are likely
interpretations of a sound stream, in information retrieval to decide which words are
useful indicators of a topic, and in machine translation to identify good translation
candidates.
In this volume, Celina Santamari?a, Julio Gonzalo, and Felisa Verdejo describe how
to build sense-tagged corpora from the Web by associating word meanings with Web
page directory nodes. The Open Directory Project (at ?dmoz.org?) is a collaborative,
volunteer project for classifying Web pages into a taxonomic hierarchy. Santamari?a et
al. present an algorithm for attaching WordNet word senses to nodes in this same
taxonomy, thus providing automatically created links between word senses and Web
344
Computational Linguistics Volume 29, Number 3
pages. They also show how this method can be used for automatic acquisition of
sense-tagged corpora, from which one could, among other things, produce language
models tied to certain senses of words, or for a certain domain.
Unseen words, or word sequences?that is, words or sequences not occurring in
training data?are a problem for language models. If the corpus from which a particu-
lar model is extracted is too small, there are many such sequences. Taking the second
author?s work, as described above, as a starting point, Frank Keller and Mirella Lapata
examine how useful the Web is as a source of frequency information for rare items:
specifically, for dependency relations involving two English words such as <fulfill OB-
JECT obligation>. They generate pairs of common words, constructing combinations
that are and are not attested in the BNC. They then compare the frequency of these
combinations in a larger 325-million-word corpus and on the Web. They find that Web
frequency counts are consistent with those for other large corpora. They also report
on a series of human-subject experiments in which they establish that Web statistics
are good at predicting the intuitive plausibility of predicate-argument pairs. Other
experiments discussed in their article show that Web counts correlate reliably with
counts re-created using class-based smoothing and overcome some problems of data
sparseness in the BNC.
Other very large corpora are available for English (English is an exception), and
the other three papers in the special issue all exploit the multilinguality of the Web.
Andy Way and Nano Gough show how the Web can provide data for an example-
based machine translation (Nagao 1984) system. First, they extract 200,000 phrases
from a parsed corpus. These phrases are sent to three online translation systems. Both
original phrases and translations are chunked. From these pairings a set of chunk
translations is extracted to be applied in a piecewise fashion to new input text. The
authors use the Web again at a final stage to rerank possible translations by verifying
which subsequences among the possible translations are most attested.
The two remaining articles present methods for building aligned bilingual corpora
from the Web. It seems plausible that such automatic construction of translation dic-
tionaries can palliate the lack of translation resources for many language pairs. Philip
Resnik was the first to recognize that it is possible to build large parallel bilingual
corpora from the Web. He found that one can exploit the appearance of language
flags and other clues that often lead to a version of the same page in a different
language.10 In this issue, Resnik and Noah Smith present their STRAND system for
building bilingual corpora from the Web.
An alternative method is presented by Wessel Kraaij, Jian-Yun Nie, and Michel
Simard. They use the resulting parallel corpora to induce a probabilistic translation
dictionary that is then embedded into a cross-language information retrieval system.
Various alternative embeddings are evaluated using the CLEF (Peters 2001) multilin-
gual information retrieval test beds.
6. Prospects
The default means of access to the Web is through a search engine such as Google.
Although the Web search engines are dazzlingly efficient pieces of technology and
excellent at the task they set for themselves, for the linguist they are frustrating:
10 For example, one can find Azerbaijan news feeds online at ?http://www.525ci.com? in Azeri (written
with a Turkish code set), and on the same page are pointers to versions of the same stories in English
and in Russian.
345
Kilgarriff and Grefenstette Web as Corpus: Introduction
? The search engine results do not present enough instances (1,000 or 5,000
maximum).
? They do not present enough context for each instance (Google provides a
fragment of around ten words).
? They are selected according to criteria that are, from a linguistic
perspective, distorting (with uses of the search term in titles and
headings going to the top of the list and often occupying all the top
slots).
? They do not allow searches to be specified according to linguistic criteria
such as the citation form for a word, or word class.
? The statistics are unreliable, with frequencies given for ?pages containing
x? varying according to search engine load and many other factors.
If only these constraints were removed, a search engine would be a wonderful
tool for language researchers. Each of the constraints could straightforwardly be re-
solved by search engine designers, but linguists are not a powerful lobby, and search
engine company priorities will never perfectly match our community?s. This suggests
a better solution: Do it ourselves. Then the kinds of processing and querying would
be designed explicitly to meet linguists? desiderata, without any conflict of interest or
?poor relation? role. Large numbers of possibilities open up. All those processes of
linguistic enrichment that have been applied with impressive effect to smaller corpora
could be applied to the Web. We could parse the Web. Web searches could be specified
in terms of lemmas, constituents (e.g., noun phrase), and grammatical relations rather
than strings. The way would be open for further anatomizing of Web text types and
domains. Thesauruses and lexicons could be developed directly from the Web. And
all for a multiplicity of languages.11
The Web contains enormous quantities of text, in numerous languages and lan-
guage types, on a vast array of topics. Our take on the Web is that it is a fabulous
linguists? playground. We hope the special issue will encourage you to come on out
and play!
References
Agirre, Eneko, Olatz Ansa, Eduard Hovy
and David Martinez. 2000. Enriching very
large ontologies using the WWW. In
Proceedings of the Ontology Learning
Workshop of the European Conference of AI
(ECAI), Berlin.
Atkins, Sue. 1993. Tools for computer-aided
corpus lexicography: The Hector project.
Acta Linguistica Hungarica, 41:5?72.
Atkins, Sue, Jeremy Clear, and Nicholas
Ostler. 1992. Corpus design criteria.
Literary and Linguistic Computing, 7(1):1?16.
Baayen, Harald. 2001. Word Frequency
Distributions. Kluwer, Dordrecht.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of
COLING-ACL, pages 86?90, Montreal,
August.
Banko, Michele and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics and the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
Toulouse.
Beaudouin, Vale?rie, Serge Fleury, Beno??t
Habert, Gabriel Illouz, Christian Licoppe,
and Marie Pasquier. 2001. Typweb:
de?crire la toile pour mieux comprendre
les parcours. In Colloque International sur
les Usages et les Services des
Te?le?communications (CIUST?01), Paris,
June. Available at ?http://www.cavi.univ-
11 The idea is developed further in Grefenstette (2001) and in Kilgarriff (2003).
346
Computational Linguistics Volume 29, Number 3
paris3.fr/ilpga/ilpga/sfleury/typweb.htm?.
Beesley, Kenneth R. 1988. Language
identifier: A computer program for
automatic natural-language identification
of on-line text. In Language at Crossroads:
Proceedings of the 29th Annual Conference of
the American Translators Association, pages
47?54, October 12?16.
Biber, Douglas. 1988. Variation across speech
and writing. Cambridge University Press,
Cambridge.
Biber, Douglas. 1993. Using
register-diversified corpora for general
language studies. Computational Linguistics,
19(2):219?242.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC, April.
Buitelaar, Paul and Bogdan Sacaleanu. 2001.
Ranking and selecting synsets by domain
relevance. In Proceedings of the Workshop on
WordNet and Other Lexical Resources:
Applications, Extensions and Customizations,
NAACL, Pittsburgh, June.
Burnard, Lou. 1995. The BNC Reference
Manual. Oxford University Computing
Service, Oxford.
Cavaglia, Gabriela. 2002. Measuring corpus
homogeneity using a range of measures
for inter-document distance. In Proceedings
of the Third International Conference on
Language Resources and Evaluation, pages
426?431, Las Palmas de Gran Canaria,
Spain, May.
Church, Kenneth W. and Robert L. Mercer.
1993. Introduction to the special issue on
computational linguistics using large
corpora. Computational Linguistics,
19(1):1?24.
Dumais, Susan, Michele Banko, Eric Brill,
Jimmy Lin, and Andrew Ng. 2002. Web
question answering: Is more always
better? In Proceedings of the 25th ACM
SIGIR, pages 291?298, Tampere, Finland.
Fletcher, William. 2002. Facilitating
compilation and dissemination of ad-hoc
web corpora. In Teaching and Language
Corpora 2002. Available at ?http://
miniappolis.com/KWiCFinder/
KWiCFinder.html?.
Folch, Helka, Serge Heiden, Beno??t Habert,
Serge Fleury, Gabriel Illouz, Pierre Lafon,
Julien Nioche, and Sophie Pre?vost. 2000.
Typtex: Inductive typological text
classification by multivariate statistical
analysis for NLP systems
tuning/evaluation. In Proceedings of the
Second Language Resources and Evaluation
Conference, pages 141?148, Athens,
May?June.
Fujii, Atsushi and Tetsuya Ishikawa. 2000.
Utilizing the World Wide Web as an
encyclopedia: Extracting term
descriptions from semi-structured text. In
Proceedings of the 38th Meeting of the ACL,
pages 488?495, Hong Kong, October.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
Conference on Empirical Methods in NLP,
Pittsburgh, PA.
Greenwood, Mark, Ian Roberts, and Robert
Gaizauskas. 2002. University of Sheffield
TREC 2002 Q & A system. In E. M.
Voorhees and Lori P. Buckland, editors,
The Eleventh Text Retrieval Conference
(TREC-11), Washington. U.S. Government
Printing Office.
Grefenstette, Gregory. 1995. Comparing two
language identification schemes. In
Proceedings of the Third International
Conference on the Statistical Analysis of
Textual Data (JADT?95), pages 263?268,
Rome, December 11?13. Available at
?www.xrce.xerox.com/competencies/content-
analysis/publications/Documents/P49030/
content/gg aslib.pdf?.
Grefenstette, Gregory. 1999. The WWW as a
resource for example-based MT tasks.
Paper presented at ASLIB ?Translating
and the Computer? conference, London,
October.
Grefenstette, Gregory. 2001. Very large
lexicons. In Walter Daelemans, Khalil
Simaan, Jakub Zavrel, and Jorn Veenstra,
editors, Computational Linguistics in the
Netherlands 2000: Selected Papers from the
Eleventh CLIN Meeting, Language and
Computers 37. Rodopi, Amsterdam.
Grefenstette, Gregory and Julien Nioche.
2000. Estimation of english and
non-english language use on the WWW.
In Proceedings of the RIAO (Recherche
d?Informations Assiste?e par Ordinateur),
pages 237?246, Paris.
Hawking, D., E. Voorhees, N. Craswell, and
P. Bailey. 1999. Overview of the TREC8
Web track. In Proceedings of the Eighth Text
Retrieval Conference, Gaithersburg,
Maryland, November.
Ipeirotis, Panagiotis G., Luis Gravano, and
Mehran Sahami. 2001. Probe, count, and
classify: Categorizing hidden Web
databases. In Proceedings of the SIGMOD
Conference, Santa Barbara, CA.
Jones, Rosie and Rayid Ghani. 2000.
Automatically building a corpus for a
minority language from the Web. In
Proceedings of the Student Workshop of the
38th Annual Meeting of the Association for
347
Kilgarriff and Grefenstette Web as Corpus: Introduction
Computational Linguistics, Hong Kong,
pages 29?36.
Karlgren, Jussi and Douglass Cutting. 1994.
Recognizing text genres with simple
metrics using discriminant analysis. In
Proceedings of COLING-94, pages
1071?1075, Kyoto, Japan.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Schu?tze. 1997. Automatic
detection of text genre. In Proceedings of
ACL and EACL, pages 39?47, Madrid.
Kilgarriff, Adam. 2001. Comparing corpora.
International Journal of Corpus Linguistics,
6(1):1?37.
Kilgarriff, Adam. 2003. Linguistic search
engine. In Kiril Simov, editor, Shallow
Processing of Large Corpora: Workshop Held in
Association with Corpus Linguistics 2003,
Lancaster, England, March.
Kilgarriff, Adam and Michael Rundell. 2002.
Lexical profiling software and its
lexicographical applications?A case
study. In Proceedings of EURALEX ?02,
Copenhagen, August.
Kittredge, Richard and John Lehrberger.
1982. Sublanguage: Studies of Language in
Restricted Semantic Domains. De Gruyter,
Berlin.
Korhonen, Anna. 2000. Using semantically
motivated estimates to help
subcategorization acquisition. In
Proceedings of the Joint Conference on
Empirical Methods in NLP and Very Large
Corpora, pages 216?223, Hong Kong,
October.
Lawrence, Steve and C. Lee Giles. 1999.
Accessibility of information on the Web.
Nature, 400:107?109.
Manning, Christopher and Hinrich Schu?tze.
1999. Foundations of Statistical Natural
Language Processing. MIT Press, Cambridge.
McEnery, Tony and Andrew Wilson. 1996.
Corpus Linguistics. Edinburgh University
Press, Edinburgh.
Mihalcea, Rada and Dan Moldovan. 1999. A
method for word sense disambiguation of
unrestricted text. In Proceedings of the 37th
Meeting of ACL, pages 152?158, College
Park, MD, June.
Nagao, Makoto. 1984. A framework of a
mechanical translation between Japanese
and English by analogy principle. In Alick
Elithorn and Ranan Banerji, editors,
Artificial and Human Intelligence.
North-Holland, Edinburgh, pages 173?180.
Peters, Carol, editor. 2001. Cross-Language
Information Retrieval and Evaluation,
Workshop of Cross-Language Evaluation Forum
(CLEF 2000) Lisbon, Portugal, September
21?22, 2000, Revised Papers. Lecture Notes
in Computer Science. Springer-Verlag.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Meeting of ACL, pages 527?534, College
Park, MD, June.
Rigau, German, Bernardo Magnini, Eneko
Agirre, and John Carroll. 2002. Meaning:
A roadmap to knowledge technologies. In
Proceedings of COLING Workshop on A
Roadmap for Computational Linguistics,
Taipei, Taiwan.
Roland, Douglas, Daniel Jurafsky, Lise
Menn, Susanne Gahl, Elizabeth Elder, and
Chris Riddoch. 2000. Verb
subcategorization frequency differences
between business-news and balanced
corpora: The role of verb sense. In
Proceedings of the Workshop on Comparing
Corpora, 38th ACL, Hong Kong, October.
Sekine, Satshi. 1997. The domain
dependence of parsing. In Proceedings of
the Fifth Conference on Applied Natural
Language Processing, pages 96?102,
Washington, DC, April.
Sinclair, John M., editor. 1987. Looking Up:
An Account of the COBUILD Project in
Lexical Computing. Collins, London.
Varantola, Krista. 2000. Translators and
disposable corpora. In Proceedings of CULT
(Corpus Use and Learning to Translate),
Bertinoro, Italy, November.
Villasenor-Pineda, L., M. Montes y Go?mez,
M. Pe?rez-Coutino, and D. Vaufreydaz.
2003. A corpus balancing method for
language model construction. In Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing-2003), pages 393?401, Mexico
City, February.
Volk, Martin. 2001. Exploiting the WWW as
a corpus to resolve PP attachment
ambiguities. In Proceedings of Corpus
Linguistics 2001, Lancaster, England.
Vossen, Piek. 2001. Extending, trimming
and fusing WordNet for technical
documents. In Proceedings of the NAACL
2001 Workshop on WordNet and Other Lexical
Resources, Pittsburgh, June. Available at
?http://engr.smu.edu/?rada/mwnw/
papers/WNW-NACL-205.pdf.gz?.
Wilks, Yorick. 2003. On the ownership of
text. Computers and the Humanities.
Forthcoming.
Xu, J. L. 2000. Multilingual search on the
World Wide Web. In Proceedings of the
Hawaii International Conference on System
Science (HICSS-33), Maui, Hawaii, January.
Zheng, Zhiping. 2002. AnswerBus question
answering system. In E. M. Voorhees and
Lori P. Buckland, editors, Proceedings of
HLT Human Language Technology Conference
(HLT 2002), San Diego, CA, March 24?27.
The SENSEVAL?3 English Lexical Sample Task
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Adam Kilgarriff
Information Technology Research Institute
University of Brighton
Brighton, UK
Adam.Kilgarriff@itri.brighton.ac.uk
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for
the English lexical sample task, which was orga-
nized as part of the SENSEVAL-3 evaluation exer-
cise. The task drew the participation of 27 teams
from around the world, with a total of 47 systems.
1 Introduction
We describe in this paper the task definition, re-
sources, participating systems, and comparative re-
sults for the English lexical sample task, which was
organized as part of the SENSEVAL-3 evaluation ex-
ercise. The goal of this task was to create a frame-
work for evaluation of systems that perform targeted
Word Sense Disambiguation.
This task is a follow-up to similar tasks organized
during the SENSEVAL-1 (Kilgarriff and Palmer,
2000) and SENSEVAL-2 (Preiss and Yarowsky,
2001) evaluations. The main changes in this
year?s evaluation consist of a new methodology for
collecting annotated data (with contributions from
Web users, as opposed to trained lexicographers),
and a new sense inventory used for verb entries
(Wordsmyth).
2 Building a Sense Tagged Corpus with
Volunteer Contributions over the Web
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002) 1. To overcome the
current lack of sense tagged data and the limitations
imposed by the creation of such data using trained
lexicographers, the OMWE system enables the col-
lection of semantically annotated corpora over the
Web. Sense tagged examples are collected using
1Open Mind Word Expert can be accessed at http://teach-
computers.org/
a Web-based application that allows contributors to
annotate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one meaning per word, the se-
lection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
Similar to the annotation scheme used for the En-
glish lexical sample at SENSEVAL-2, we use a ?tag
until two agree? scheme, with an upper bound on the
number of annotations collected for each item set to
four.
2.1 Source Corpora
The data set used for the SENSEVAL-3 English
lexical sample task consists of examples extracted
from the British National Corpus (BNC). Ear-
lier versions of OMWE also included data from
the Penn Treebank corpus, the Los Angeles Times
collection as provided during TREC conferences
(http://trec.nist.gov), and Open Mind Common Sense
(http://commonsense.media.mit.edu).
2.2 Sense Inventory
The sense inventory used for nouns and adjec-
tives is WordNet 1.7.1 (Miller, 1995), which
is consistent with the annotations done for the
same task during SENSEVAL-2. Verbs are in-
stead annotated with senses from Wordsmyth
(http://www.wordsmyth.net/). The main reason mo-
tivating selection of a different sense inventory is the
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Class Nr of Avg senses Avg senses
words (fine) (coarse)
Nouns 20 5.8 4.35
Verbs 32 6.31 4.59
Adjectives 5 10.2 9.8
Total 57 6.47 4.96
Table 1: Summary of the sense inventory
weak verb performance of systems participating in
the English lexical sample in SENSEVAL-2, which
may be due to the high number of senses defined for
verbs in the WordNet sense inventory. By choos-
ing a different set of senses, we hope to gain insight
into the dependence of difficulty of the sense disam-
biguation task on sense inventories.
Table 1 presents the number of words under each
part of speech, and the average number of senses for
each class.
2.3 Multi-Word Expressions
For this evaluation exercise, we decided to isolate the
task of semantic tagging from the task of identifying
multi-word expressions; we applied a filter that re-
moved all examples pertaining to multi-word expres-
sions prior to the tagging phase. Consequently, the
training and test data sets made available for this task
do not contain collocations as possible target words,
but only single word units. This is a somewhat dif-
ferent definition of the task as compared to previous
similar evaluations; the difference may have an im-
pact on the overall performance achieved by systems
participating in the task.
2.4 Sense Tagged Data
The inter-tagger agreement obtained so far is closely
comparable to the agreement figures previously re-
ported in the literature. Kilgarriff (2002) mentions
that for the SENSEVAL-2 nouns and adjectives there
was a 66.5% agreement between the first two tag-
gings (taken in order of submission) entered for
each item. About 12% of that tagging consisted of
multi-word expressions and proper nouns, which are
usually not ambiguous, and which are not consid-
ered during our data collection process. So far we
measured a 62.8% inter-tagger agreement between
the first two taggings for single word tagging, plus
close-to-100% precision in tagging multi-word ex-
pressions and proper nouns (as mentioned earlier,
this represents about 12% of the annotated data).
This results in an overall agreement of about 67.3%
which is reasonable and closely comparable with
previous figures. Note that these figures are col-
lected for the entire OMWE data set build so far,
which consists of annotated data for more than 350
words.
In addition to raw inter-tagger agreement, the
kappa statistic, which removes from the agreement
rate the amount of agreement that is expected by
chance(Carletta, 1996), was also determined. We
measure two figures: micro-average   , where num-
ber of senses, agreement by chance, and   are de-
termined as an average for all words in the set,
and macro-average   , where inter-tagger agreement,
agreement by chance, and   are individually deter-
mined for each of the words in the set, and then
combined in an overall average. With an average of
five senses per word, the average value for the agree-
ment by chance is measured at 0.20, resulting in a
micro-   statistic of 0.58. For macro-   estimations,
we assume that word senses follow the distribution
observed in the OMWE annotated data, and under
this assumption, the macro-   is 0.35.
3 Participating Systems
27 teams participated in this word sense disambigua-
tion task. Tables 2 and 3 list the names of the partic-
ipating systems, the corresponding institutions, and
the name of the first author ? which can be used
as reference to a paper in this volume, with more
detailed descriptions of the systems and additional
analysis of the results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of 47 submissions were received for this task.
Tables 2 and 3 show all the submissions for each
team, gives a brief description of their approaches,
and lists the precision and recall obtained by each
system under fine and coarse grained evaluations.
The precision/recall baseline obtained for this task
under the ?most frequent sense? heuristic is 55.2%
(fine grained) and 64.5% (coarse grained). The per-
formance of most systems (including several unsu-
pervised systems, as listed in Table 3) is significantly
higher than the baseline, with the best system per-
forming at 72.9% (79.3%) for fine grained (coarse
grained) scoring.
Not surprisingly, several of the top performing
systems are based on combinations of multiple clas-
sifiers, which shows once again that voting schemes
that combine several learning algorithms outperform
the accuracy of individual classifiers.
4 Conclusion
The English lexical sample task in SENSEVAL-
3 featured English ambiguous words that were to
be tagged with their most appropriate WordNet or
Wordsmyth sense. The objective of this task was
to: (1) Determine feasibility of reliably finding the
Fine Coarse
System/Team Description P R P R
htsa3 A Naive Bayes system, with correction of the a-priori frequencies, by
U.Bucharest (Grozea) dividing the output confidence of the senses by  	
	 (   ) 72.9 72.9 79.3 79.3
IRST-Kernels Kernel methods for pattern abstraction, paradigmatic and syntagmatic info.
ITC-IRST (Strapparava) and unsupervised term proximity (LSA) on BNC, in an SVM classifier. 72.6 72.6 79.5 79.5
nusels A combination of knowledge sources (part-of-speech of neighbouring words,
Nat.U. Singapore (Lee) words in context, local collocations, syntactic relations), in an SVM classifier. 72.4 72.4 78.8 78.8
htsa4 Similar to htsa3, with different correction function of a-priori frequencies. 72.4 72.4 78.8 78.8
BCU comb An ensemble of decision lists, SVM, and vectorial similarity, improved
Basque Country U. with a variety of smoothing techniques. The features consist 72.3 72.3 78.9 78.9
(Agirre & Martinez) of local collocations, syntactic dependencies, bag-of-words, domain features.
htsa1 Similar to htsa3, but with smaller number of features. 72.2 72.2 78.7 78.7
rlsc-comb A regularized least-square classification (RLSC), using local and topical
U.Bucharest (Popescu) features, with a term weighting scheme. 72.2 72.2 78.4 78.4
htsa2 Similar to htsa4, but with smaller number of features. 72.1 72.1 78.6 78.6
BCU english Similar to BCU comb, but with a vectorial space model learning. 72.0 72.0 79.1 79.1
rlsc-lin Similar to rlsc-comb, with a linear kernel, and a binary weighting scheme. 71.8 71.8 78.4 78.4
HLTC HKUST all A voted classifier combining a new kernel PCA method, a Maximum Entropy
HKUST (Carpuat) model, and a boosting-based model, using syntactic and collocational features 71.4 71.4 78.6 78.6
TALP A system with per-word feature selection, using a rich feature set. For
U.P.Catalunya learning, it uses SVM, and combines two binarization procedures: 71.3 71.3 78.2 78.2
(Escudero et al) one vs. all, and constraint learning.
MC-WSD A multiclass averaged perceptron classifier with two components: one
Brown U. trained on the data provided, the other trained on this data, and on 71.1 71.1 78.1 78.1
(Ciaramita & Johnson) WordNet glosses. Features consist of local and syntactic features.
HLTC HKUST all2 Similar to HLTC HKUST all, also adds a Naive Bayes classifier. 70.9 70.9 78.1 78.1
NRC-Fine Syntactic and semantic features, using POS tags and pointwise mutual infor-
NRC (Turney) mation on a terabyte corpus. Five basic classifiers are combined with voting. 69.4 69.4 75.9 75.9
HLTC HKUST me Similar to HLTC HKUST all, only with a maximum entropy classifier. 69.3 69.3 76.4 76.4
NRC-Fine2 Similar to NRC-Fine, with a different threshold for dropping features 69.1 69.1 75.6 75.6
GAMBL A cascaded memory-based classifier, using two classifiers based on global
U. Antwerp (Decadt) and local features, with a genetic algorithm for parameter optimization. 67.4 67.4 74.0 74.0
SinequaLex Semantic classification trees, built on short contexts and document se-
Sinequa Labs (Crestan) mantics, plus a decision system based on information retrieval techniques. 67.2 67.2 74.2 74.2
CLaC1 A Naive Bayes approach using a context window around the target word, 67.2 67.2 75.1 75.1
Concordia U. (Lamjiri) which is dynamically adjusted
SinequaLex2 A cumulative method based on scores of surrounding words. 66.8 66.8 73.6 73.6
UMD SST4 Supervised learning using Support Vector Machines, using local and
U. Maryland (Cabezas) wide context features, and also grammatical and expanded contexts. 66.0 66.0 73.7 73.7
Prob1 A probabilistic modular WSD system, with individual modules based on
Cambridge U. (Preiss) separate known approaches to WSD (26 different modules) 65.1 65.1 71.6 71.6
SyntaLex-3 A supervised system that uses local part of speech features and bigrams,
U.Toronto (Mohammad) in an ensemble classifier using bagged decision trees. 64.6 64.6 72.0 72.0
UNED A similarity-based system, relying on the co-occurrence of nouns and
UNED (Artiles) adjectives in the test and training examples. 64.1 64.1 72.0 72.0
SyntaLex-4 Similar to SyntaLex-3, but with unified decision trees. 63.3 63.3 71.1 71.1
CLaC2 Syntactic and semantic (WordNet hypernyms) information of neighboring
words, fed to a Maximum Entropy learner. See also CLaC1 63.1 63.1 70.3 70.3
SyntaLex-1 Bagged decision trees using local POS features. See also SyntaLex-3. 62.4 62.4 69.1 69.1
SyntaLex-2 Similar to SyntaLex-1, but using broad context part of speech features. 61.8 61.8 68.4 68.4
Prob2 Similar to Prob1, but invokes only 12 modules. 61.9 61.9 69.3 69.3
Duluth-ELSS An ensemble approach, based on three bagged decision trees, using
U.Minnesota (Pedersen) unigrams, bigrams, and co-occurrence features 61.8 61.8 70.1 70.1
UJAEN A Neural Network supervised system, using features based on semantic
U.Jae?n (Garc??a-Vega) relations from WordNet extracted from the training data 61.3 61.3 69.5 69.5
R2D2 A combination of supervised (Maximum Entropy, HMM Models, Vector
U. Alicante (Vazquez) Quantization, and unsupervised (domains and conceptual density) systems. 63.4 52.1 69.7 57.3
IRST-Ties A generalized pattern abstraction system, based on boosted wrapper
ITC-IRST (Strapparava) induction, using only few syntagmatic features. 70.6 50.5 76.7 54.8
NRC-Coarse Similar to NRC-Fine; maximizes the coarse score, by training on coarse senses. 48.5 48.5 75.8 75.8
NRC-Coarse2 Similar to NRC-Coarse, with a different threshold for dropping features. 48.4 48.4 75.7 75.7
DLSI-UA-LS-SU A maximum entropy method and a bootstrapping algorithm (?re-training?) with,
U.Alicante (Vazquez) iterative feeding of training cycles with new high-confidence examples. 78.2 31.0 82.8 32.9
Table 2: Performance and short description of the supervised systems participating in the SENSEVAL-3
English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both
fine grained and coarse grained scoring. Corresponding team and reference to system description (in this
volume) are indicated for the first system for each team.
Fine Coarse
System/Team Description P R P R
wsdiit An unsupervised system using a Lesk-like similarity between context
IIT Bombay of ambiguous words, and dictionary definitions. Experiments are 66.1 65.7 73.9 74.1
(Ramakrishnan et al) performed for various window sizes, various similarity measures
Cymfony A Maximum Entropy model for unsupervised clustering, using neighboring
(Niu) words and syntactic structures as features. A few annotated instances 56.3 56.3 66.4 66.4
are used to map context clusters to WordNet/Worsmyth senses.
Prob0 A combination of two unsupervised modules, using basic part of speech
Cambridge U. (Preiss) and frequency information. 54.7 54.7 63.6 63.6
clr04-ls An unsupervised system relying on definition properties (syntactic, semantic,
CL Research subcategorization patterns, other lexical information), as given in a dictionary. 45.0 45.0 55.5 55.5
(Litkowski) Performance is generally a function of how well senses are distinguished.
CIAOSENSO An unsupervised system that combines the conceptual density idea with the
U. Genova (Buscaldi) frequency of words to disambiguate; information about domains is also 50.1 41.7 59.1 49.3
taken into account.
KUNLP An algorithm that disambiguates the senses of a word by selecting a substituent
Korea U. (Seo) among WordNet relatives (antonyms, hypernyms, etc.). The selection 40.4 40.4 52.8 52.8
is done based on co-occurrence frequencies, measured on a large corpus.
Duluth-SenseRelate An algorithm that assigns the sense to a word that is most related to the
U.Minnesota (Pedersen) possible senses of its neighbors, using WordNet glosses to measure 40.3 38.5 51.0 48.7
relatedness between senses.
DFA-LS-Unsup A combination of three heuristics: similarity between synonyms and the context,
UNED (Fernandez) according to a mutual information measure; lexico-syntactic patterns extracted 23.4 23.4 27.4 27.4
from WordNet glosses; the first sense heuristic.
DLSI-UA-LS-NOSU An unsupervised method based on (Magnini & Strapparava 2000) WordNet
U.Alicante (Vazquez) domains; it exploits information contained in glosses of WordNet domains, and 19.7 11.7 32.2 19.0
uses ?Relevant Domains?, obtained from association ratio over domains and words.
Table 3: Performance and short description for the Unsupervised systems participating in the SENSEVAL-3
English lexical sample task.
appropriate sense for words with various degrees of
polysemy, using different sense inventories; and (2)
Determine the usefulness of sense annotated data
collected over the Web (as opposed to other tradi-
tional approaches for building semantically anno-
tated corpora).
The results of 47 systems that participated in this
event tentatively suggest that supervised machine
learning techniques can significantly improve over
the most frequent sense baseline, and also that it is
possible to design unsupervised techniques for reli-
able word sense disambiguation. Additionally, this
task has highlighted creation of testing and training
data by leveraging the knowledge of Web volunteers.
The training and test data sets used in this exercise
are available online from http://www.senseval.org
and http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Open Mind Word Expert project, making this task
possible. In particular, we are grateful to Gwen
Lenker ? our most productive contributor. We are
also grateful to all the participants in this task, for
their hard work and involvement in this evaluation
exercise. Without them, all these comparative anal-
yses would not be possible.
We are indebted to the Princeton WordNet team,
for making WordNet available free of charge, and to
Robert Parks from Wordsmyth, for making available
the verb entries used in this evaluation.
We are particularly grateful to the National Sci-
ence Foundation for their support under research
grant IIS-0336793, and to the University of North
Texas for a research grant that provided funding for
contributor prizes.
References
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249?
254.
T. Chklovski and R. Mihalcea. 2002. Building a sense tagged
corpus with Open Mind Word Expert. In Proceedings of the
ACL 2002 Workshop on ?Word Sense Disambiguation: Re-
cent Successes and Future Directions?, Philadelphia, July.
A. Kilgarriff and M. Palmer, editors. 2000. Computer and
the Humanities. Special issue: SENSEVAL. Evaluating Word
Sense Disambiguation programs, volume 34, April.
G. Miller. 1995. Wordnet: A lexical database. Communication
of the ACM, 38(11):39?41.
J. Preiss and D. Yarowsky, editors. 2001. Proceedings of
SENSEVAL-2, Association for Computational Linguistics
Workshop, Toulouse, France.
123
124
125
126
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 41?44,
Prague, June 2007. c?2007 Association for Computational Linguistics
An efficient algorithm for building a distributional thesaurus (and other
Sketch Engine developments)
Pavel Rychly?
Masaryk University
Brno, Czech Republic
pary@fi.muni.cz
Adam Kilgarriff
Lexical Computing Ltd
Brighton, UK
adam@lexmasterclass.com
Abstract
Gorman and Curran (2006) argue that the-
saurus generation for billion+-word corpora
is problematic as the full computation takes
many days. We present an algorithm with
which the computation takes under two
hours. We have created, and made pub-
licly available, thesauruses based on large
corpora for (at time of writing) seven major
world languages. The development is imple-
mented in the Sketch Engine (Kilgarriff et
al., 2004).
Another innovative development in the same
tool is the presentation of the grammatical
behaviour of a word against the background
of how all other words of the same word
class behave. Thus, the English noun con-
straint occurs 75% in the plural. Is this
a salient lexical fact? To form a judge-
ment, we need to know the distribution for
all nouns. We use histograms to present the
distribution in a way that is easy to grasp.
1 Thesaurus creation
Over the last ten years, interest has been growing
in distributional thesauruses (hereafter simply ?the-
sauruses?). Following initial work by (Spa?rck Jones,
1964) and (Grefenstette, 1994), an early, online dis-
tributional thesaurus presented in (Lin, 1998) has
been widely used and cited, and numerous authors
since have explored thesaurus properties and param-
eters: see survey component of (Weeds and Weir,
2005).
A thesaurus is created by
? taking a corpus
? identifying contexts for each word
? identifying which words share contexts.
For each word, the words that share most contexts
(according to some statistic which also takes account
of their frequency) are its nearest neighbours.
Thesauruses generally improve in accuracy with
corpus size. The larger the corpus, the more clearly
the signal (of similar words) will be distinguished
from the noise (of words that just happen to share
a few contexts). Lin?s was based on around 300M
words and (Curran, 2004) used 2B (billion).
A direct approach to thesaurus computation looks
at each word and compares it with each other word,
checking all contexts to see if they are shared. Thus,
complexity is O(n2m) where n in the number of
types and m is the size of the context vector. The
number of types increases with the corpus size, and
(Ravichandran et al, 2005) propose heuristics for
thesaurus building without undertaking the complete
calculation. The line of reasoning is explored further
by (Gorman and Curran, 2006), who argue that the
complete calculation is not realistic given large cor-
pora. They estimate that, given a 2B corpus and its
184,494-word vocabulary comprising all words oc-
curring over five times, the full calculation will take
nearly 300 days. With the vocabulary limited to the
75,800 words occuring over 100 times, the calcula-
tion took 18 days.
The naive algorithm has complexity O(n2m) but
this is not the complexity of the problem. Most of
41
the n2 word pairs have nothing in common so there
is no reason to check them. We proceed by working
only with those word pairs that do have something in
common. This allows us to create thesauruses from
1B corpora in under 2 hours.
1.1 Algorithm
We prepare the corpus by lemmatizing and then
shallow parsing to identify grammatical relation in-
stances with the form ?w1, r, w??, where r is a
grammatical relation, w1 and w? are words. We
count the frequency of each triple and sort all
?w1, r, w?, score? 4-tuples by ?contexts? where a
context is a ?r, w?? pair. Only 4-tuples with positive
score are included.
The algorithm then loops over each context
(CONTEXTS is the set of all contexts):
for ?r, w?? in CONTEXTS:
WLIST = set of all w where ?w, r,w?? exists
for w1 in WLIST:
for w2 in WLIST:
sim(w1, w2)+ = f(frequencies)1
The outer loop is linear in the number of contexts.
The inner loop is quadratic in the number of words
in WLIST, that is, the number of words sharing a
particular context ?r, w??. This list is usually small
(less than 1000), so the quadratic complexity is man-
ageable.
We use a heuristic at this point. If WLIST has
more than 10,000 members, the context is skipped.
Any such general context is very unlikely to make
a substantial difference to the similarity score, since
similarity scores are weighted according to how spe-
cific they are. The computational work avoided can
be substantial.
The next issue is how to store the whole
sim(w1, w2) matrix. Most of the values are very
small or zero. These values are not stored in the
final thesaurus but they are needed during the com-
putation. A strategy for this problem is to gener-
ate, sort and sum in sequential scan. That means
that instead of incrementing the sim(w1, w2) score
as we go along, we produce ?w1, w2, x? triples in
a very long list, running, for a billion-word corpus,
1In this paper we do not discuss the nature of this function
as it is does not impact on the complexity. It is explored exten-
sively in (Curran, 2004; Weeds and Weir, 2005).
into hundreds of GB. For such huge data, a variant
of TPMMS (Two Phase Multi-way Merge Sort) is
used. First we fill the whole available memory with
a part of the data, sort in memory (summing where
we have multiple instances of the same ?w1, w2? as
we proceed) and output the sorted stream. Then we
merge sorted streams, again summing as we pro-
ceed.
Another technique we use is partitioning. The
outer loop of the algorithm is fast and can be run
several times with a limit on which words to process
and output. For example, the first run processes only
word pairs ?w1, w2? where the ID of w1 is between
0 and 99, the next, where it is between 100 and 199,
etc. In such limited runs there is a high probability
that most of the summing is done in memory. We es-
tablish a good partitioning with a dry run in which a
plan is computed such that all runs produce approxi-
mately the number of items which can be sorted and
summed in memory.
1.2 Experiments
We experimented with the 100M-word BNC2, 1B-
word Oxford English Corpus3 (OEC), and 1.9B-
word Itwac (Baroni and Kilgarriff, 2006).
All experiments were carried out on a machine
with AMD Opteron quad-processor. The machine
has 32 GB of RAM but each process used only
1GB (and changing this limit produced no signifi-
cant speedup). Data files were on a Promise disk
array running Disk RAID5.
Parameters for the computation include:
? hits threshold MIN: only words entering into a
number of triples greater than MIN will have
thesaurus entries, or will be candidates for be-
ing in other words? thesaurus entries. (Note
that words not passing this threshold can still
be in contexts, so may contribute to the simi-
larity of two other words: cf Daelemans et al?s
title (1999).)
? the number of words (WDS) above the thresh-
old
2http://www.natcorp.ox.ac.uk
3http://www.askoxford.com/oec/ We are grateful to Oxford
University Press for permission to use the OEC.
42
Corp MIN WDS TYP CTX TIME
BNC 1 152k 5.7m 608k 13m 9s
BNC 20 68k 5.6m 588k 9m 30s
OEC 2 269k 27.5m 994k 1hr 40m
OEC 20 128k 27.3m 981k 1hr 27m
OEC 200 48k 26.7m 965k 1hr 10m
Itwac 20 137k 24.8m 1.1m 1hr 16m
Table 1: Thesaurus creation jobs and timings
? the number of triples (types) that these words
occur in (TYP)
? the number of contexts (types) that these words
occur in (CTX)
We have made a number of runs with different
values of MIN for BNC, OEC and Itwac and present
details for some representative ones in Table 1.
For the BNC, the number of partitions that the TP-
MMS process was divided into was usually between
ten and twenty; for the OEC and ITwac it was around
200.
For the OEC, the heuristic came into play and, in
a typical run, 25 high-frequency, low-salience con-
texts did not play a role in the theasurus compu-
tation. They included: modifier?more; modifier?
not; object-of?have; subject-of?have. In Gorman
and Curran, increases in speed were made at sub-
stantial cost to accuracy. Here, data from these high-
frequency contexts makes negligible impact on the-
saurus entries.
1.3 Available thesauruses
Thesauruses of the kind described are pub-
licly available on the Sketch Engine server
(http://www.sketchengine.co.uk) based on corpora
of between 50M and 2B words for, at time of writ-
ing, Chinese, English, French, Italian, Japanese,
Portuguese, Slovene and Spanish.
2 Histograms for presenting statistical
facts about a word?s grammar
75% of the occurrences of the English noun con-
straint in the BNC are in the plural. Many dictio-
naries note that some nouns are usually plural: the
question here is, how salient is the fact about con-
Figure 1: Distribution of nouns with respect to pro-
portion of instances in plural, from 0 to 1 in 10 steps,
with the class that constraint is in, in white.
straint?45
To address it we need to know not only the propor-
tion for constraint but also the proportion for nouns
in general. If the average, across nouns, is 50% then
it is probably not noteworthy. But if the average is
2%, it is. If it is 30%, we may want to ask a more
specific question: for what proportion of nouns is the
percentage higher than 75%. We need to view ?75%
plural? in the context of the whole distribution.
All the information is available. We can deter-
mine, in a large corpus such as the BNC, for each
noun lemma with more than (say) fifty occurrences,
what percentage is plural. We present the data in a
histogram: we count the nouns for which the propor-
tion is between 0 and 0.1, 0.1 and 0.2, . . . , 0.9 and
1. The histogram is shown in Fig 1, based on the
14,576 nouns with fifty or more occurrences in the
BNC. (The first column corresponds to 6113 items.)
We mark the category containing the item of inter-
est, in red (white in this paper). We believe this is
an intuitive and easy-to-interpret way of presenting
a word?s relative frequency in a particular grammat-
ical context, against the background of how other
words of the same word class behave.
We have implemented histograms like these in the
Sketch Engine for a range of word classes and gram-
matical contexts. The histograms are integrated into
4Other 75% plural nouns which might have served as the
example include: activist bean convulsion ember feminist intri-
cacy joist mechanic relative sandbag shutter siding teabag tes-
ticle trinket tusk. The list immediately suggests a typology of
usually-plural nouns, indicating how this kind of analysis pro-
vokes new questions.
5Of course plurals may be salient for one sense but not oth-
ers.
43
the word sketch6 for each word. (Up until now the
information has been available but hard to interpret.)
In accordance with the word sketch principle of not
wasting screen space, or user time, on uninteresting
facts, histograms are only presented where a word is
in the top (or bottom) percentile for a grammatical
pattern or construction.
Similar diagrams have been used for similar pur-
poses by (Lieber and Baayen, 1997). This is, we
believe, the first time that they have been offered as
part of a corpus query tool.
3 Text type, subcorpora and keywords
Where a corpus has components of different text
types, users often ask: ?what words are distinctive of
a particular text type?, ?what are the keywords??.7
Computations of this kind often give unhelpful re-
sults because of the ?lumpiness? of word distribu-
tions: a word will often appear many times in an
individual text, so statistics designed to find words
which are distinctively different between text types
will give high values for words which happen to be
the topic of just one particular text (Church, 2000).
(Hlava?c?ova? and Rychly?, 1999) address the prob-
lem through defining ?average reduced frequency?
(ARF), a modified frequency count in which the
count is reduced according to the extent to which
occurrences of a word are bunched together.
The Sketch Engine now allows the user to prepare
keyword lists for any subcorpus, either in relation to
the full corpus or in relation to another subcorpus,
using a statistic of the user?s choosing and basing
the result either on raw frequency or on ARF.
Acknowledgements
This work has been partly supported by the
Academy of Sciences of Czech Republic under the
project T100300419, by the Ministry of Education
of Czech Republic within the Center of basic re-
search LC536 and in the National Research Pro-
gramme II project 2C06009.
6A word sketch is a one-page corpus-derived account of a
word?s grammatical and collocation behaviour.
7The well-established WordSmith corpus tool
(http://www.lexically.net/wordsmith) has a keywords function
which has been very widely used, see e.g., (Berber Sardinha,
2000).
References
Marco Baroni and Adam Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple lan-
guages. In EACL.
Tony Berber Sardinha. 2000. Comparing corpora with
wordsmith tools: how large must the reference corpus
be? In Proceedings of the ACL Workshop on Compar-
ing Corpora, pages 7?13.
Kenneth Ward Church. 2000. Empirical estimates of
adaptation: The chance of two noriegas is closer to
p/2 than p2. In COLING, pages 180?186.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, Edinburgh Univesity.
Walter Daelemans, Antal van den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34(1-3).
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In ACL.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer.
Jaroslava Hlava?c?ova? and Pavel Rychly?. 1999. Dispersion
of words in a language corpus. In Proc. TSD (Text
Speech Dialogue), pages 321?324.
Adam Kilgarriff, Pavel Rychly?, Pavel Smrz?, and David
Tugwell. 2004. The sketch engine. In Proc. EU-
RALEX, pages 105?116.
Rochelle Lieber and Harald Baayen. 1997. Word fre-
quency distributions and lexical semantics. Computers
in the Humanities, 30:281?291.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768?774.
Deepak Ravichandran, Patrick Pantel, and Eduard H.
Hovy. 2005. Randomized algorithms and nlp: Using
locality sensitive hash functions for high speed noun
clustering. In ACL.
Karen Spa?rck Jones. 1964. Synonymy and Semantic
Classificiation. Ph.D. thesis, Edinburgh University.
Julie Weeds and David J. Weir. 2005. Co-occurrence re-
trieval: A flexible framework for lexical distributional
similarity. Computational Linguistics, 31(4):439?475.
44
Proceedings of the Fourth International Natural Language Generation Conference, pages 133?135,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Shared-task Evaluations in HLT: Lessons for NLG
Anja Belz
University of Brighton, UK
A.S.Belz@brighton.ac.uk
Adam Kilgarriff
Lexical Computing Ltd., UK
adam@lexmasterclass.com
1 Introduction
While natural language generation (NLG) has a
strong evaluation tradition, in particular in user-
based and task-oriented evaluation, it has never
evaluated different approaches and techniques by
comparing their performance on the same tasks
(shared-task evaluation, STE). NLG is charac-
terised by a lack of consolidation of results, and
by isolation from the rest of NLP where STE is
now standard. It is, moreover, a shrinking field
(state-of-the-art MT and summarisation no longer
perform generation as a subtask) which lacks the
kind of funding and participation that natural lan-
guage understanding (NLU) has attracted.
Evidence from other NLP fields shows that STE
campaigns (STECs) can lead to rapid technolog-
ical progress and substantially increased partici-
pation. The past year has seen a groundswell of
interest in comparative evaluation among NLG re-
searchers, the first comparative results are being
reported (Belz and Reiter, 2006), and the move to-
wards some form of comparative evaluation seems
inevitable. In this paper we look at how two
decades of NLP STECs might help us decide how
best to make this move.
2 Shared-task evaluation in HLT
Over the past twenty years, virtually every field
of research in human language technology (HLT)
has introduced STECs. A small selection is pre-
sented in the table below1. NLG researchers have
tended to be somewhat unconvinced of the benefits
of comparative evaluation in general, and the kind
of competitive, numbers-driven STECs that have
been typical of NLU in particular. Yet STECs do
not have to be hugely competitive events fixated
on one task with associated input/output data and
single evaluation metric, static over time.
Tasks: There is a distinction between (i) evalu-
ations designed to help potential users to decide
1Apologies for omissions, and for bias towards English.
whether the technology will be valuable to them,
and (ii) evaluations designed to help system devel-
opers improve the core technology (Spa?rck Jones
and Galliers, 1996). In the former, the applica-
tion context is a critical variable in the task defi-
nition; in the latter it is fixed. Developer-oriented
evaluation promotes focus on the task in isolation,
but if the context is fixed badly, or if the outside
world changes but the evaluation does not, then
it becomes irrelevant. NLP STECs have so far fo-
cused on developer-oriented evaluation, but there
are increasing calls for more ?embedded?, more
task-based types of evaluations2 .
Existing NLP STECs show that tasks need to be
broadly based and continuously evolving. To be-
gin with, the task needs to be simple, easy to un-
derstand and easy for people to recognise as their
task. Over time, as the limitations of the sim-
ple task are noted and a more substantial com-
munity is ?on board?, tasks can multiply, diversify
and become more sophisticated. This is something
that TREC has been good at (still going strong 14
years on), and the parsing community has failed to
achieve (see notes in table).
Evaluation: NLP STECs have tended to use au-
tomatic evaluations because of their speed and re-
producibility, but some have used human evalua-
tors, in particular in fields where language is gen-
erated (MT, summarisation, speech synthesis).
Evaluation scores are not independent of the
task and context for which they are calculated.
This is clearly true of human-based evaluation, but
even scores by a simple metric like word error
rate in speech recognition are not comparable un-
less certain parameters are the same: background-
noise, language, whether or not speech is con-
trolled. Development of evaluation methods and
benchmark tasks therefore must go hand in hand.
Evaluation methods have to be accepted by the
research community as providing a true approxi-
2A prominent theme at the 2005 ELRA/ELDA Workshop
on HLT Evaluation.
133
Name Start Domain Sponsors Notes
MUC 1987 Information extraction US Govt Rapidly came to define IE; ended 1998.
PARSEVAL 1991 Parsing ? Only ever defined a metric, no STEC1.
TREC 1992 Information retrieval US Govt Large and long-running, multiple tracks.
SEMEVAL 1994 Semantic interpretation US Govt No STEC emerged2.
NIST-MT 1994 Machine translation US Govt Revitalised since 2001 by BLEU3.
Morpholympics 1994 Morphological analysis GLDV German morphological analysis; one-off.
SENSEVAL 1998 Word sense disambiguation ACL-SIGLEX Validity of WSD task problematic.
SUMMAC 1998 Text summarization US Govt One-off.
CoNLL 1999 Various ACL-SIGNLL Focus on learning algorithms.
CLEF 2000 IR across languages EU Project
DUC 2001 Document summarization US Govt Defines field.
Morfolimpiadas 2003 Morphological analysis Portuguese Govt Portuguese language only.
SIGHAN 2003 Chinese tokenization ACL-SIGHAN Key benchmark.
Blizzard 2003 Speech synthesis Festvox project Building synthetic voice from given data.
HAREM 2005 Named-entity recognition Portuguese Govt Portuguese language only.
RTE 2005 Textual entailment EU Project
TC-STAR 2005 Speech-to-speech translation EU integrated project Black-box and glass-box evaluation4.
Notes
1. PARSEVAL is an evaluation measure, not a full STEC. This has proved problematic: the parsing community no longer
accepts the PARSEVAL measure, but there has been no organisational framework for establishing an alternative.
2. SEMEVAL did not proceed largely because it was too ambitious and agreement between people with different interests
and theoretical positions was not achieved. It was eventually reduced in scope and aspects became incorporated in MUC,
SUMMAC and SENSEVAL.
3. MT has been transformed by corpus methods, which have shifted MT from a backwater to perhaps the most vibrant area
of NLP in the last five years.
4. In TC-STAR, the SST task is broken down into numerous subtasks. The modules and systems that meet the given criteria
are exchanged among the participants, lowering the barrier to entry.
mation of quality. E.g. BLEU is strongly disliked
in the non-statistical part of the MT community be-
cause it is biased in favour of statistical MT sys-
tems. PARSEVAL stopped being used when the
parsing community moved towards dependency
parsing and related approaches.
Sharing: As PARSEVAL shows, measures and
resources alone are not enough. Also required are
(i) an event (or better, cycle of events) so people
can attend and feel part of a community; (ii) a fo-
rum for reviewing task definitions and evaluation
methods; (iii) a committee which ?owns? the STEC,
and organises the next campaign.
Funding is usually needed for gold-standard
corpus creation but rarely for anything else (Kil-
garriff, 2003). Participants can be expected to
cover the cost of system development and work-
shop attendance. A funded project is best seen as
supporting and enabling the STEC (especially dur-
ing the early stages) rather than being it.
In sum, STECs are good for community build-
ing. They produce energy (as we saw when the
possibility was raised for NLG at UCNLG?05 and
ENLG?05) which can lead to rapid scientific and
technological progress. They make the field look
like a game and draw people in.
3 Towards an NLG STEC
In 1981, Spa?rck Jones wrote that IR lacked con-
solidation and the ability to build new work on
old, and that this was substantially because there
was no commonly agreed framework for describ-
ing and evaluating systems (Spa?rck Jones, 1981,
p. 245). Since 1981, various NLP sub-disciplines
have consolidated results and progressed collec-
tively through STECs, and have seen successful
commercial deployment of NLP technology (e.g.
speech recognition software, document retrieval
and dialogue systems).
However, Spa?rck Jones?s 1981 analysis could
be said to still hold of NLG today. There has
been little consolidation of results or collective
progress, and there still is virtually no commercial
deployment of NLG systems or components.
We believe that comparative evaluation is key
if NLG is to consolidate and progress collectively.
Conforming to the evaluation paradigm now com-
mon to the rest of NLP will also help re-integration,
and open up the field to new researchers.
Tasks: In defining sharable tasks with associ-
ated data resources for NLG, the core problem is
deciding what inputs should look like. There is
a real risk that agreement cannot be achieved on
134
this, so not many groups participate, or the plan
never reaches fruition (as happened in SEMEVAL).
There are, however, ways in which this problem
can be circumvented. One is to use a more abstract
task specification describing system functionality,
so that participants can use their own inputs, and
systems are compared in task-based evaluations
similar to the traditions and standards of software
evaluation (as in Morpholympics). An alternative
is to approach the issue through tasks with inputs
and outputs that ?occur naturally?, so that partic-
ipants can use their own NLG-specific represen-
tations. Examples include data-to-text mappings
where e.g. time-series data or a data repository are
mapped to fault reports, forecasts, etc.
Both data-independent task definitions and
tasks with naturally occurring data have promise,
but we propose the second as the simpler, easier
to organise solution, at least initially. A specific
proposal of a set of tasks can be found elsewhere
in this volume (Reiter and Belz, 2006). An inter-
esting idea (recommended by ELRA/ELDA) is to
break down the input-output mapping into stages
(as in the TC-STAR workshops, see table) and then,
in a second round of evaluations, to make available
intermediate representations from the most suc-
cessful systems from the first round. In this way,
standardised representations might develop almost
as a side-effect of STECs.
Evaluation: As in MT there are at least two cri-
teria of quality for NLG systems: language quality
(fluency in MT) and correctness of content (ade-
quacy in MT). In NLG, these have mostly been
evaluated directly using human scores or prefer-
ence judgments, although recently automatic met-
rics such as BLEU have been used. They have also
been evaluated indirectly, e.g. by measuring read-
ing speeds and manual post-processing3 . A more
user-oriented type of evaluation has been to assess
real-world usefulness, in other words, whether the
generated texts achieve their purpose (e.g. whether
users learn more with NLG techniques than with
cheaper alternatives4).
The majority of NLP STECs have used automatic
evaluation methods, and the ability to produce re-
sults ?at the push of a button?, quickly and repro-
ducibly, is ideal in the context of STECs. However,
existing metrics are unlikely to be suitable for NLG
3E.g. in the SkillSum and SumTime projects at Aberdeen.
4E.g. evaluation of the NL interface of the DIAG intelligent
tutoring system, di Eugenio et al
(Belz and Reiter, 2006), and there is a lot of scepti-
cism among NLG researchers regarding automatic
evaluation. We believe that NLG should develop
its own automatic metrics (development of such
metrics is part of the proposal by Reiter and Belz,
this volume), but for the time being an NLG STEC
needs to involve human-based evaluations of the
intrinsic as well as extrinsic type.
Sharing: A recent survey conducted on the main
NLG and corpus-based NLP mailing lists5 revealed
that there are virtually no data resources that could
be directly used in shared tasks. Considerable in-
vestment has to go into developing such resources,
and direct funding is necessary. This points to a
funded project, but we recommend direct involve-
ment of the NLG community and SIGGEN. Other
aspects of organisation are not NLG-specific, so
the general recommendations in the preceding sec-
tion apply.
4 Conclusion
STECs have been remarkable stimulants to
progress in other areas of HLT, through their
community-building role, and through ?hot-
housing? solutions to specific problems. There are
also lessons to be learnt about STECs not being
overly ambitious, remaining responsive to devel-
opments in the broader field and wider world, and
having appropriate institutional standing. We be-
lieve that NLG can benefit greatly from the intro-
duction of shared tasks, provided that an inclusive
and flexible approach is taken which is informed
by the specific requirements of NLG.
References
A. Belz and E. Reiter. 2006. Comparing automatic
and human evaluation of NLG systems. In Proc.
EACL?06, pages 313?320.
A. Kilgarriff. 2003. No-bureaucracy evaluation. In
Proc. Workshop on Evaluation Initiatives in NLP,
EACL?03.
E. Reiter and A. Belz. 2006. GENEVAL: A pro-
posal for shared-task evaluation in NLG. In Proc.
INLG?06.
K. Spa?rck Jones and J. R. Galliers. 1996. Evaluating
Natural Language Processing Systems: An Analysis
and Review. Springer Verlag.
K. Spa?rck Jones, 1981. Information Retrieval Experi-
ment, chapter 12. Butterworth & Co.
5Belz, March 2006.
135
Annotated web as corpus 
 
Paul Rayson 
Computing Department, 
Lancaster University, UK 
p.rayson@lancs.ac.uk
James Walkerdine 
Computing Department, 
Lancaster University, UK 
j.walkerdine@lancs.ac.uk 
William H. Fletcher 
United States Naval  
Academy, USA 
fletcher@usna.edu 
Adam Kilgarriff 
Lexical Computing Ltd., UK 
adam@lexmasterclass.com 
 
Abstract 
This paper presents a proposal to facili-
tate the use of the annotated web as cor-
pus by alleviating the annotation bottle-
neck for corpus data drawn from the web. 
We describe a framework for large-scale 
distributed corpus annotation using peer-
to-peer (P2P) technology to meet this 
need. We also propose to annotate a large 
reference corpus in order to evaluate this 
framework. This will allow us to investi-
gate the affordances offered by distrib-
uted techniques to ensure replicability of 
linguistic research based on web-derived 
corpora. 
1 Introduction 
Linguistic annotation of corpora contributes cru-
cially to the study of language at several levels: 
morphology, syntax, semantics, and discourse. 
Its significance is reflected both in the growing 
interest in annotation software for word sense 
tagging (Edmonds and Kilgarriff, 2002) and in 
the long-standing use of part-of-speech taggers, 
parsers and morphological analysers for data 
from English and many other languages. 
Linguists, lexicographers, social scientists and 
other researchers are using ever larger amounts 
of corpus data in their studies. In corpus linguis-
tics the progression has been from the 1 million-
word Brown and LOB corpora of the 1960s, to 
the 100 million-word British National Corpus of 
the 1990s. In lexicography this progression is 
paralleled, for example, by Collins Dictionaries? 
initial 10 million word corpus growing to their 
current corpus of around 600 million words. In 
addition, the requirement for mega- and even 
giga-corpora1 extends to other applications, such 
as lexical frequency studies, neologism research, 
and statistical natural language processing where 
models of sparse data are built. The motivation 
for increasingly large data sets remains the same. 
Due to the Zipfian nature of word frequencies, 
around half the word types in a corpus occur 
only once, so tremendous increases in corpus 
size are required both to ensure inclusion of es-
sential word and phrase types and to increase the 
chances of multiple occurrences of a given type.  
In corpus linguistics building such mega-
corpora is beyond the scope of individual re-
searchers, and they are not easily accessible 
(Kennedy, 1998: 56) unless the web is used as a 
corpus (Kilgarriff and Grefenstette, 2003). In-
creasingly, corpus researchers are tapping the 
Web to overcome the sparse data problem (Kel-
ler et al, 2002). This topic generated intense in-
terest at workshops held at the University of Hei-
delberg (October 2004), University of Bologna 
(January 2005), University of Birmingham (July 
2005) and now in Trento in April 2006. In addi-
tion, the advantages of using linguistically anno-
tated data over raw data are well documented 
(Mair, 2005; Granger and Rayson, 1998). As the 
size of a corpus increases, a near linear increase 
in computing power is required to annotate the 
text. Although processing power is steadily 
growing, it has already become impractical for a 
single computer to annotate a mega-corpus.  
Creating a large-scale annotated corpus from 
the web requires a way to overcome the limita-
tions on processing power. We propose distrib-
uted techniques to alleviate the limitations on the 
                                                 
1 See, for example, those distributed by the Linguistic 
Data Consortium: http://www.ldc.upenn.edu/ 
27
volume of data that can be tagged by a single 
processor. The task of annotating the data will be 
shared by computers at collaborating institutions 
around the world, taking advantage of processing 
power and bandwidth that would otherwise go 
unused. Such large-scale parallel processing re-
moves the workload bottleneck imposed by a 
server based structure. This allows for tagging a 
greater amount of textual data in a given amount 
of time while permitting other users to use the 
system simultaneously. Vast amounts of data can 
be analysed with distributed techniques. The fea-
sibility of this approach has been demonstrated 
by the SETI@home project2. 
The framework we propose can incorporate 
other annotation or analysis systems, for exam-
ple, lemmatisation, frequency profiling, or shal-
low parsing. To realise and evaluate the frame-
work, it will be developed for a peer-to-peer 
(P2P) network and deployed along with an exist-
ing lexicographic toolset, the Sketch Engine. A 
P2P approach allows for a low cost implementa-
tion that draws upon available resources (existing 
user PCs). As a case study for evaluation, we 
plan to collect a large reference corpus from the 
web to be hosted on servers from Lexical Com-
puting Ltd. We can evaluate annotation speed 
gains of our approach comparatively against the 
single server version by utilising processing 
power in computer labs at Lancaster University 
and the United States Naval Academy (USNA) 
and we will call for volunteers from the corpus 
community to be involved in the evaluation as 
well.  
A key aspect of our case study research will be 
to investigate extending corpus collection to new 
document types. Most web-derived corpora have 
exploited raw text or HTML pages, so efforts 
have focussed on boilerplate removal and clean-
up of these formats with tools like Hyppia-BTE, 
Tidy and Parcels 3  (Baroni and Sharoff, 2005). 
Other document formats such as Adobe PDF and 
MS-Word have been neglected due to the extra 
conversion and clean-up problems they entail. 
By excluding PDF documents, web-derived cor-
pora are less representative of certain genres 
such as academic writing. 
                                                 
                                                
2 http://setiathome.ssl.berkeley.edu/ 
3 http://www.smi.ucd.ie/hyppia/, 
http://parcels.sourceforge.net and 
http://tidy.sourceforge.net. 
2 Related Work  
The vast majority of previous work on corpus 
annotation has utilised either manual coding or 
automated software tagging systems, or else a 
semi-automatic combination of the two ap-
proaches e.g. automated tagging followed by 
manual correction. In most cases a stand-alone 
system or client-server approach has been taken 
by annotation software using batch processing 
techniques to tag corpora. Only a handful of 
web-based or email services (CLAWS4, Amal-
gam5, Connexor6) are available, for example, in 
the application of part-of-speech tags to corpora. 
Existing tagging systems are ?small scale? and 
typically impose some limitation to prevent over-
load (e.g. restricted access or document size). 
Larger systems to support multiple document 
tagging processes would require resources that 
cannot be realistically provided by existing sin-
gle-server systems. This corpus annotation bot-
tleneck becomes even more problematic for vo-
luminous data sets drawn from the web. The use 
of the web as a corpus for teaching and research 
on language has been proposed a number of 
times (Kilgarriff, 2001; Robb, 2003; Rundell, 
2000; Fletcher, 2001, 2004b) and received a spe-
cial issue of the journal Computational Linguis-
tics (Kilgarriff and Grefenstette, 2003). Studies 
have used several different methods to mine web 
data. Turney (2001) extracts word co-occurrence 
probabilities from unlabelled text collected from 
a web crawler. Baroni and Bernardini (2004) 
built a corpus by iteratively searching Google for 
a small set of seed terms. Prototypes of Internet 
search engines for linguists, corpus linguists and 
lexicographers have been proposed: WebCorp 
(Kehoe and Renouf, 2002), KWiCFinder 
(Fletcher, 2004a) and the Linguist?s Search En-
gine (Kilgarriff, 2003; Resnik and Elkiss, 2003). 
A key concern in corpus linguistics and related 
disciplines is verifiability and replicability of the 
results of studies. Word frequency counts in 
internet search engines are inconsistent and unre-
liable (Veronis, 2005). Tools based on static cor-
pora do not suffer from this problem, e.g. 
BNCweb7, developed at the University of Zurich, 
and View 8  (Variation in English Words and 
Phrases, developed at Brigham Young University) 
 
4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 
5 http://www.comp.leeds.ac.uk/amalgam/amalgam/ 
amalghome.htm 
6 http://www.connexor.com 
7 http://homepage.mac.com/bncweb/home.html 
8 http://view.byu.edu/ 
28
are both based on the British National Corpus. 
Both BNCweb and View enable access to anno-
tated corpora and facilitate searching on part-of-
speech tags. In addition, PIE9 (Phrases in Eng-
lish), developed at USNA, which performs 
searches on n-grams (based on words, parts-of-
speech and characters), is currently restricted to 
the British National Corpus as well, although 
other static corpora are being added to its data-
base. In contrast, little progress has been made 
toward annotating sizable sample corpora from 
the web. 
?Real-time? linguistic analysis of web data at 
the syntactic level has been piloted by the Lin-
guist?s Search Engine (LSE). Using this tool, 
linguists can either perform syntactic searches 
via parse trees on a pre-analysed web collection 
of around three million sentences from the Inter-
net Archive (www.archive.org) or build their 
own collections from AltaVista search engine 
results. The second method pushes the new col-
lection onto a queue for the LSE annotator to 
analyse. A new collection does not become 
available for analysis until the LSE completes 
the annotation process, which may entail signifi-
cant delay with multiple users of the LSE server. 
The Gsearch system (Corley et al, 2001) also 
selects sentences by syntactic criteria from large 
on-line text collections. Gsearch annotates cor-
pora with a fast chart parser to obviate the need 
for corpora with pre-existing syntactic mark-up. 
In contrast, the Sketch Engine system to assist 
lexicographers to construct dictionary entries 
requires large pre-annotated corpora. A word 
sketch is an automatic one-page corpus-derived 
summary of a word's grammatical and colloca-
tional behaviour. Word Sketches were first used 
to prepare the Macmillan English Dictionary for 
Advanced Learners (2002, edited by Michael 
Rundell). They have also served as the starting 
point for high-accuracy Word Sense Disam-
biguation. More recently, the Sketch Engine was 
used to develop the new edition of the Oxford 
Thesaurus of English (2004, edited by Maurice 
Waite). 
Parallelising or distributing processing has 
been suggested before. Clark and Curran?s (2004) 
work is in parallelising an implementation of 
log-linear parsing on the Wall Street Journal 
Corpus, whereas we focus on part-of-speech tag-
ging of a far larger and more varied web corpus, 
a technique more widely considered a prerequi-
site for corpus linguistics research. Curran (2003) 
                                                 
9 http://pie.usna.edu/ 
suggested distributed processing in terms of web 
services but only to ?allow components devel-
oped by different researchers in different loca-
tions to be composed to build larger systems? 
and not for parallel processing. Most signifi-
cantly, previous investigations have not exam-
ined three essential questions: how to apply dis-
tributed techniques to vast quantities of corpus 
data derived from the web, how to ensure that 
web-derived corpora are representative, and how 
to provide verifiability and replicability. These 
core foci of our work represent crucial innova-
tions lacking in prior research. In particular, rep-
resentativeness and replicability are key research 
concerns to enhance the reliability of web data 
for corpora. 
In the areas of Natural Language Processing 
(NLP) and computational linguistics, proposals 
have been made for using the computational Grid 
for data-intensive NLP and text-mining for e-
Science (Carroll et al, 2005; Hughes et al 2004). 
While such an approach promises much in terms 
of emerging infrastructure, we wish to exploit 
existing computing infrastructure that is more 
accessible to linguists via a P2P approach. In 
simple terms, P2P is a technology that takes ad-
vantage of the resources and services available at 
the edge of the Internet (Shirky, 2001). Better 
known for file-sharing and Instant Messenger 
applications, P2P has increasingly been applied 
in distributed computational systems. Examples 
include SETI@home (looking for radio evidence 
of extraterrestrial life), ClimatePrediction.net 
(studying climate change), Predictor@home (in-
vestigating protein-related diseases) and Ein-
stein@home (searching for gravitational signals).  
A key advantage of P2P systems is that they 
are lightweight and geared to personal computing 
where informal groups provide unused process-
ing power to solve a common problem. Typically, 
P2P systems draw upon the resources that al-
ready exist on a network (e.g. home or work 
PCs), thus keeping the cost to resource ratio low. 
For example the fastest supercomputer cost over 
$110 million to develop and has a peak perform-
ance of 12.3 TFLOPS (trillions of floating-point 
operations per second). In contrast, a typical day 
for the SETI@home project involved a perform-
ance of over 20 TFLOPS, yet cost only $700,000 
to develop; processing power is donated by user 
PCs. This high yield for low start-up cost makes 
it ideal for cheaply developing effective compu-
tational systems to realise, deploy and evaluate 
our framework. The deployment of computa-
tional based P2P systems is supported by archi-
29
tectures such as BOINC10, which provide a plat-
form on which volunteer based distributed com-
puting systems can be built. Lancaster's own P2P 
Application Framework (Walkerdine et al, sub-
mitted) also supports higher-level P2P applica-
tion development and can be adapted to make 
use of the BOINC architecture.  
3 Research hypothesis and aims 
Our research hypothesis is that distributed com-
putational techniques can alleviate the annotation 
bottleneck for processing corpus data from the 
web. This leads us to a number of research ques-
tions: 
? How can corpus data from the web be di-
vided into units for processing via distrib-
uted techniques? 
? Which corpus annotation techniques are 
suitable for distributed processing? 
? Can distributed techniques assist in corpus 
clean-up and conversion to allow inclu-
sion of a wider variety of genres and to 
support more representative corpora? 
In the early stages of our proposed research, 
we are focussing on grammatical word-class 
analysis (part-of-speech tagging) of web-derived 
corpora of English and aspects of corpus clean-
up and conversion. Clarifying copyright issues 
and exploring models for legal dissemination of 
corpora compiled from web data are key objec-
tives of this stage of the investigation as well. 
4 Methodology 
The initial focus of the work will be to develop 
the framework for distributed corpus annotation. 
Since existing solutions have been centralised in 
nature, we first must examine the consequences 
that a distributed approach has for corpus annota-
tion and identify issues to address. 
A key concern will be handling web pages 
within the framework, as it is essential to mini-
mise the amount of data communicated between 
peers. Unlike the other distributed analytical sys-
tems mentioned above, the size of text document 
and analysis time is largely proportional for cor-
pora annotation. This places limitations on work 
unit size and distribution strategies. In particular, 
three areas will be investigated: 
? Mechanisms for crawling/discovery of a 
web corpus domain - how to identify 
pages to include in a web corpus. Also 
                                                 
10 BOINC, Berkeley Open Infrastructure for Network 
Computing. http://boinc.berkeley.edu. 
investigate appropriate criteria for han-
dling pages which are created or modi-
fied dynamically.  
? Mechanisms to generate work units for 
distributed computation - how to split 
the corpus into work units and reduce the 
communication / computation time ratio 
that is crucial for such systems to be ef-
fective. 
? Mechanisms to support the distribution 
of work units and collection of results - 
how to handle load balancing. What data 
should be sent to peers and how is the 
processed information handled and ma-
nipulated? What mechanisms should be 
in place to ensure correctness of results?  
How can abuse be prevented and secu-
rity concerns of collaborating institutions 
be addressed?  BOINC already provides 
a good platform for this, and these as-
pects will be investigated within the pro-
ject. 
Analysis of existing distributed computation 
systems will help to inform the design of the 
framework and tackle some of these issues. Fi-
nally, the framework will also cater for three 
common strategies for corpus annotation: 
? Site based corpus annotation - in which 
the user can specify a web site to anno-
tate 
? Domain based corpus annotation - in 
which the user specifies a content do-
main (with the use of keywords) to  an-
notate 
? Crawler based corpus annotation - more 
general web based corpus annotation in 
which crawlers are used to locate web 
pages 
From a computational linguistic view, the 
framework will also need to take into account the 
granularity of the unit (for example, POS tagging 
requires sentence-units, but anaphoric annotation 
needs paragraphs or larger). Secondly, we need 
to investigate techniques for identifying identical 
documents, virtually identical documents and 
highly repetitive documents, such as those pio-
neered by Fletcher (2004b) and shingling tech-
niques described by Chakrabarti (2002).  
The second stage of our work will involve im-
plementing the framework within a P2P envi-
ronment. We have already developed a prototype 
of an object-oriented application environment to 
support P2P system development using JXTA 
(Sun's P2P API). We have designed this envi-
ronment so that specific application functionality 
30
can be captured within plug-ins that can then in-
tegrate with the environment and utilise its func-
tionality. This system has been successfully 
tested with the development of plug-ins support-
ing instant messaging, distributed video encoding 
(Hughes and Walkerdine, 2005), distributed vir-
tual worlds (Hughes et al, 2005) and digital li-
brary management (Walkerdine and Rayson, 
2004). It is our intention to implement our dis-
tributed corpus annotation framework as a plug-
in. This will involve implementing new func-
tionality and integrating this with our existing 
annotation tools (such as CLAWS11). The devel-
opment environment is also flexible enough to 
utilise the BOINC platform, and such support 
will be built into it. 
Using the P2P Application Framework as a 
basis for the development secures several advan-
tages. First, it reduces development time by al-
lowing the developer to reuse existing function-
ality; secondly, it already supports essential as-
pects such as system security; and thirdly, it has 
already been used successfully to deploy compa-
rable P2P applications. A lightweight version of 
the application framework will be bundled with 
the corpus annotation plug-in, and this will then 
be made publicly available for download in 
open-source and executable formats. We envis-
age our end-users will come from a variety of 
disciplines such as language engineering and lin-
guistics. For the less-technical users, the proto-
type will be packaged as a screensaver or instant 
messaging client to facilitate deployment. 
5 Evaluation 
We will evaluate the framework and prototype 
developed by applying it as a pre-processor step 
for the Sketch Engine system. The Sketch Engine 
requires a large well-balanced corpus which has 
been part-of-speech tagged and shallow parsed to 
find subjects, objects, heads, and modifiers. We 
will use the existing non-distributed processing 
tools on the Sketch Engine as a baseline for a 
comparative evaluation of the AWAC frame-
work instantiation by utilising processing power 
and bandwidth in learning labs at Lancaster Uni-
versity and USNA during off hours. 
We will explore techniques to make the result-
ing annotated web corpus data available in static 
form to enable replication and verification of 
corpus studies based on such data. The initial 
solution will be to store the resulting reference 
                                                 
                                                
11 http://www.comp.lancs.ac.uk/ucrel/claws/ 
corpus in the Sketch Engine. We will also inves-
tigate whether the distributed environment un-
derlying our approach offers a solution to the 
problem of reproducibility in web-based corpus 
studies based in general. Current practise else-
where includes the distribution of URL lists, but 
given the dynamic nature of the web, this is not 
sufficiently robust. Other solutions such as com-
plete caching of the corpora are not typically 
adopted due to legal concerns over copyright and 
redistribution of web data, issues considered at 
length by Fletcher (2004a). Other requirements 
for reference corpora such as retrieval and stor-
age of metadata for web pages are beyond the 
scope of what we propose here. 
To improve the representative nature of web-
derived corpora, we will research techniques to 
enable the importing of additional document 
types such as PDF. We will reuse and extend 
techniques implemented in the collection, encod-
ing and annotation of the PERC Corpus of Pro-
fessional English12. A majority of this corpus has 
been collected by conversion of on-line academic 
journal articles from PDF to XML with a combi-
nation of semi-automatic tools and techniques 
(including Adobe Acrobat version 6). Basic is-
sues such as character encoding, table/figure ex-
traction and maintaining text flow around em-
bedded images need to be dealt with before an-
notation processing can begin. We will compara-
tively evaluate our techniques against others such 
as pdf2txt, and Multivalent PDF ExtractText13. 
Part of the evaluation will be to collect and anno-
tate a sample corpus. We aim to collect a corpus 
from the web that is comparable to the BNC in 
content and annotation. This corpus will be 
tagged using the P2P framework. It will form a 
test-bed for the framework and we will utilise the 
non-distributed annotation system on the Sketch 
Engine as a baseline for comparison and evalua-
tion. To evaluate text conversion and clean-up 
routines for PDF documents, we will use a 5-
million-word gold-standard sub-corpus extracted 
 
12 The Corpus of Professional English (CPE) is a ma-
jor research project of PERC (the Professional Eng-
lish Research Consortium) currently underway that, 
when finished, will consist of a 100-million-word 
computerised database of English used by profession-
als in science, engineering, technology and other 
fields. Lancaster University and Shogakukan Inc. are 
PERC Member Institutions. For more details, see 
http://www.perc21.org/ 
13 http://multivalent.sourceforge.net/ 
31
from the PERC Corpus of Professional 
English14.  
                                                
6 Conclusion 
Future work includes an analysis of the balance 
between computational and bandwidth require-
ments. It is essential in distributing the corpus 
annotation to achieve small amounts of data 
transmission in return for large computational 
gains for each work-unit.  
In this paper, we have discussed the require-
ment for annotation of web-derived corpus data. 
Currently, a bottleneck exists in the tagging of 
web-derived corpus data due to the voluminous 
amount of corpus processing involved. Our pro-
posal is to construct a framework for large-scale 
distributed corpus annotation using existing peer-
to-peer technology. We have presented the chal-
lenges that lie ahead for such an approach. Work 
is now underway to address the clean-up of PDF 
data for inclusion into corpora downloaded from 
the web. 
Acknowledgements 
We wish to thank the anonymous reviewers who 
commented our paper. We are grateful to Shoga-
kukan Inc. (Tokyo, Japan) for supporting re-
search at Lancaster University into the process of 
conversion and clean-up of PDF to text, and to 
the Professional English Research Consortium 
for the provision of the gold-standard corpus for 
our evaluation. 
References  
Baroni, M. and Bernardini, S. (2004). BootCaT: 
Bootstrapping Corpora and Terms from the Web. 
In Proceedings of LREC2004, Lisbon, pp. 1313-
1316. 
Baroni, M. and Sharoff, S. (2005). Creating special-
ized and general corpora using automated search 
engine queries. Web as Corpus Workshop, Bir-
mingham University, UK, 14th July 2005. 
Carroll, J., R. Evans and E. Klein (2005) Supporting 
text mining for e-Science: the challenges for Grid-
enabled natural language processing. In Workshop 
on Text Mining, e-Research And Grid-enabled 
Language Technology at the Fourth UK e-Science 
Programme All Hands Meeting (AHM2005), Not-
tingham, UK. 
 
14 This corpus has already been manually re-typed at 
Shogakukan Inc. from PDF originals downloaded 
from the web. 
Chakrabarti, S. (2002) Mining the Web: Discovering 
Knowledge from Hypertext Data. Morgan Kauf-
mann. 
Clark, S. and Curran, J. R.. (2004). Parsing the wsj 
using ccg and log-linear models. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL ?04). 
Corley, S., Corley, M., Keller, F., Crocker, M., & 
Trewin, S. (2001). Finding Syntactic Structure in 
Unparsed Corpora: The Gsearch Corpus Query 
System. Computers and the Humanities, 35, 81-94. 
Curran, J.R. (2003). Blueprint for a High Performance 
NLP Infrastructure. In Proc. of Workshop on Soft-
ware Engineering and Architecture of Language 
Technology Systems (SEALTS) Edmonton, Canada, 
2003, pp. 40 ? 45. 
Edmonds, P and Kilgarriff, A. (2002). Introduction to 
the special issue on evaluating word sense disam-
biguation systems. Journal of Natural Language 
Engineering, 8 (2), pp. 279-291. 
Fletcher, W. H. (2001).  Concordancing the Web with 
KWiCFinder.  Third North American Symposium 
on Corpus Linguistics and Language Teaching, 
Boston, MA, 23-25 March 2001.  
Fletcher, W. H. (2004a). Facilitating the compilation 
and dissemination of ad-hoc Web corpora. In G. 
Aston, S. Bernardini and D. Stewart (eds.), Cor-
pora and Language Learners, pp. 271 ? 300, John 
Benjamins, Amsterdam. 
Fletcher, W. H. (2004b). Making the Web More Use-
ful as a Source for Linguistic Corpora. In Ulla 
Connor and Thomas A. Upton (eds.) Applied Cor-
pus Linguistics. A Multidimensional Perspective. 
Rodopi, Amsterdam, pp. 191 ? 205. 
Granger, S., and Rayson, P. (1998). Automatic profil-
ing of learner texts. In S. Granger (ed.) Learner 
English on Computer. Longman, London and New 
York, pp. 119-131. 
Hughes, B, Bird, S., Haejoong, K., and Klein, E. 
(2004). Experiments with data-intensive NLP on a 
computational grid. Proceedings of the Interna-
tional Workshop on Human Language Technology. 
http://eprints.unimelb.edu.au/archive/00000503/. 
Hughes, D., Gilleade, K., Walkerdine, J. and Mariani, 
J., Exploiting P2P in the Creation of Game Worlds. 
In the proceedings of ACM GDTW 2005, Liver-
pool, UK, 8th-9th November, 2005. 
Hughes, D. and Walkerdine, J. (2005), Distributed 
Video Encoding Over A Peer-to-Peer Network. In 
the proceedings of PREP 2005, Lancaster, UK, 
30th March - 1st April, 2005 
Kehoe, A. and Renouf, A. (2002) WebCorp: Applying 
the Web to Linguistics and Linguistics to the Web. 
32
World Wide Web 2002 Conference, Honolulu, Ha-
waii. 
Keller, F., Lapata, M. and Ourioupina, O. (2002). 
Using the Web to Overcome Data Sparseness. Pro-
ceedings of the Conference on Empirical Methods 
in Natural Language Processing, Philadelphia, 
July 2002, pp. 230-237. 
Kennedy, G. (1998). An introduction to corpus lin-
guistics. Longman, London. 
Kilgarriff, A. (2001). Web as corpus. In Proceedings 
of Corpus Linguistics 2001, Lancaster University, 
29 March - 2 April 2001, pp. 342 ? 344. 
Kilgarriff, A. (2003). Linguistic Search Engine. In 
proceedings of Workshop on Shallow Processing of 
Large Corpora (SProLaC 2003), Lancaster Uni-
versity, 28 - 31 March 2003, pp. 53 ? 58. 
Kilgarriff, A. and Grefenstette, G (2003). Introduction 
to the Special Issue on the Web as Corpus. Compu-
tational Linguistics, 29: 3, pp. 333-347. 
Mair, C. (2005). The corpus-based study of language 
change in progress: The extra value of tagged cor-
pora. Presentation at the AAACL/ICAME Confer-
ence, Ann Arbor, May 2005. 
Resnik, P. and Elkiss, A. (2003) The Linguist's Search 
Engine: Getting Started Guide. Technical Report: 
LAMP-TR-108/CS-TR-4541/UMIACS-TR-2003-
109, University of Maryland, College Park, No-
vember 2003. 
Robb, T. (2003) Google as a Corpus Tool? In ETJ 
Journal, Volume 4, number 1, Spring 2003. 
Rundell, M. (2000). "The biggest corpus of all", Hu-
manising Language Teaching. 2:3; May 2000. 
Shirky, C. (2001) Listening to Napster, in Peer-to-
Peer: Harnessing the power of Disruptive Tech-
nologies, O'Reilly. 
Turney, P. (2001). Word Sense Disambiguation by 
Web Mining for Word Co-occurrence Probabili-
ties. In proceedings of SENSEVAL-3, Barcelona, 
Spain, July 2004 pp. 239-242. 
Veronis, J. (2005). Web: Google's missing pages: 
mystery solved? 
http://aixtal.blogspot.com/2005/02/web-googles-
missing-pages-mystery.html (accessed April 28, 
2005). 
Walkerdine, J., Gilleade, K., Hughes, D., Rayson, P., 
Simms, J., Mariani, J., and Sommerville, I. A 
Framework for P2P Application Development. Pa-
per submitted to Software Practice and Experience. 
Walkerdine, J. and Rayson, P. (2004) P2P-4-DL: 
Digital Library over Peer-to-Peer. In Caronni G., 
Weiler N., Shahmehri N. (eds.) Proceedings of 
Fourth IEEE International Conference on Peer-to-
Peer Computing (PSP2004) 25-27 August 2004, 
Zurich, Switzerland. IEEE Computer Society 
Press, pp. 264-265. 
 
 
33
 34
Chinese Sketch Engine and 
the Extraction of Grammatical Collocations
Chu-Ren Huang  
Inst. of Linguistics  
Academia Sinica  
churen@sinica.edu.tw
Adam Kilgarriff 
Lexicography MasterClass 
Information Technology  
adam@lexmasterclass.com 
Yiching Wu 
Inst. of Linguistics 
Tsing Hua University 
d898702@oz.nthu.edu.tw 
Chih-Ming Chiu 
Inst. of Information Science 
Academia Sinica 
henning@hp.iis.sinica.edu.tw 
Simon Smith 
Dept. of Applied English 
Ming Chuan University 
ssmith@mcu.edu.tw 
Pavel Rychly 
Faculty of Informatics 
Masaryk University. 
pary@textforge.cz 
Ming-Hong Bai 
Inst. of Information Science 
Academia Sinica 
mhbai@sinica.edu.tw 
Keh-Jiann Chen 
Inst. of Information Science 
Academia Sinica 
kchen@iis.sinica.edu.tw
Abstract. This paper introduces a new 
technology for collocation extraction in Chinese. 
Sketch Engine (Kilgarriff et al, 2004) has 
proven to be a very effective tool for automatic 
description of lexical information, including 
collocation extraction, based on large-scale 
corpus. The original work of Sketch Engine was 
based on BNC. We extend Sketch Engine to 
Chinese based on Gigaword corpus from LDC. 
We discuss the available functions of the 
prototype Chinese Sketch Engine (CSE) as well 
as the robustness of language-independent 
adaptation of Sketch Engine. We conclude by 
discussing how Chinese-specific linguistic 
information can be incorporated to improve the 
CSE prototype.  
1. Introduction 
The accessibility to large scale corpora, at 
one billion words or above, has become both a 
blessing and a challenge for NLP research. How 
to efficiently use a gargantuan corpus is an 
urgent issue concerned by both users and corpora 
designers. Adam Kilgarriff et al (2004) 
developed the Sketch Engine to facilitate 
efficient use of corpora. Their claims are two 
folded: that genuine linguistic generalizations 
can be automatically extracted from a corpus 
with simple collocation information provided 
that the corpus is large enough; and that such a 
methodology is easily adaptable for a new 
language. The first claim was fully substantiated 
with their work on BNC. The current paper deals 
with the second claim by adapting the Sketch 
Engine to Chinese.  
2. Online Chinese Corpora: The State of 
the Arts 
2.1 Chinese Corpora 
The first online tagged Chinese corpus is 
Academia Sinica Balanced Corpus of Modern 
Chinese (Sinica Corpus), which has been 
web-accessible since November, 1996. The 
current version contains 5.2028 million words 
(7.8927 million characters). The corpus data was 
collected between 1990 and 1996 (CKIP, 
1995/1998). Two additional Chinese corpora 
were made available on line in 2003. The first is 
the Sinorama Chinese-English Parallel Text 
Corpus (Sinorama Corpus). The Sinorama 
Corpus is composed of 2,373 parallel texts in 
both Chinese and English that were published 
between 1976 and 2000. There are 103,252 pairs 
of sentences, composed of roughly 3.2 million 
48
English words and 5.3 million Chinese 
characters 1 . The second one is the modern 
Chinese corpus developed by the Center for 
Chinese Linguistics (CCL Corpus) at Peking 
University. It contains eighty-five million 
(85,398,433) simplified Chinese characters 
which were published after 1919 A.D. 
2.2 Extracting Linguistic Information from 
Online Chinese Corpora: Tools and Interfaces 
The Chinese corpora discussed above are 
all equipped with an online interface to allow 
users to extract linguistic generalizations. Both 
Sinica Corpus and CCL Corpus offer 
KWIC-based functions, while Sinorama Corpus 
gives sentence and paragraph aligned output. 
2.2.1 String Matching or Word Matching 
The basic unit of query that a corpus allows 
defines the set of information that can be 
extracted from that corpus. While there is no 
doubt that segmented corpus allows more precise 
linguistic generalizations, string-based 
collocation still afford a corpus of the robustness 
that is not restricted by an arbitrary word-list or 
segmentation algorithm. This robustness is of 
greatest value when extracting neologism or 
sub-lexical collocations. Since CCL Corpus is 
not segmented and tagged, string-based KWIC is 
its main tool for extracting generalizations. This 
comes with the familiar pitfall of word boundary 
ambiguity. For instance, a query of ci.yao ??
?secondary? may yield the intended result (la), as 
well as noise (1b). 
1a. ??????
dan zhe shi ci.yao de 
but this is secondary DE
1http://cio.nist.gov/esd/emaildir/lists/mt_list/msg0003
3.html 
?But this is secondary? 
 b. ????????!
ta ji ci yao.qiu ta da.fu 
he several time ask her answer 
?He had asked her to answer for several times? 
 Sinica Corpus, on the other hand, is fully 
segmented and allows word-based 
generalizations. In addition, Sinica Corpus also 
allows wildcards in its search. Users specify a 
wildcard of arbitrary length (*), or fixed length 
(?). This allows search of a class of words 
sharing some character strings.
2.2.2 Display of Extracted Data 
Formal restriction on the display of 
extracted data also constraints the type of 
information that can be obtained from that 
corpus. Sinica Corpus allows users to change 
window size from about 25 to 57 Chinese 
characters. However, since a Chinese sentence 
may be longer than 57 characters, Sinica Corpus 
cannot guarantee that a full sentence is displayed. 
CCL Corpus, on the other hand, is able to show a 
full output sentence, which may be up to 200 
Chinese characters. However, it does not display 
more than a full sentence. Thus it cannot show 
discourse information. Sinorama Corpus with 
TOTALrecall interface is most versatile in this 
respect. Aligned bilingual full sentences are 
shown with an easy link to the full text. 
In terms of size and completeness of 
extracted data, Sinica Corpus returns all matched 
examples. However, cut and paste must be 
performed for the user to build his/her dataset. 
CCL Corpus, on the other hand, limits data to 
500 lines per page, but allows easy download of 
output data. Lastly, Sinorama/TOTALrecall 
provides choices of 5 to 100 sentences per page. 
49
2.2.3 Refining Extracted Information: Filter 
and Sorter 
Both Sinica Corpus and CCL corpus allows 
users to process extracted information, using 
linguistic and contextual filter or sorter. The CCL 
corpus requires users to remember the rules, 
while Sinica Corpus allows users to fill in blanks 
and/or choose from pull-down menu. In 
particular, Sinica Corpus allows users to refine 
their generalization by quantitatively 
characterizing the left and right contexts. The 
quantitative sorting functions allowed include 
both word and POS frequency, as well as word 
mutual information.  
2.2.4 Extracting Grammatical Information 
Availability of grammatical information 
depends on corpus annotation. CCL and 
Sinorama Corpus do not have POS tags. Sinica 
Corpus is the only Chinese corpus allowing users 
to access an overview of a keyword?s syntactic 
behavior. Users can obtain a list of types and 
distribution of the keyword?s syntactic category. 
In addition, users can find possible collocations 
of the keyword from the output of Mutual 
Information (MI).  
The most salient grammatical information, 
such as grammatical functions (subject, object, 
adjunct etc.) is beyond the scope of the 
traditional corpus interface tools. Traditional 
corpora rely on the human users to arrive at these 
kinds of generalizations.
3. Sketch Engine: A New Corpus-based 
approach to Grammatical Information  
Several existing linguistically annotated 
corpus of Chinese, e.g. Penn Chinese Tree Bank 
(Xia et al, 2000), Sinica Treebank (Chen et al, 
2003), Proposition Bank (Xue and Palmer, 2003, 
2005) and Mandarin VerbNet (Wu and Liu, 
2003), suffer from the same problem. They are 
all extremely labor-intensive to build and 
typically have a narrow coverage. In addition, 
since structural assignment is theory-dependent 
and abstract, inter-annotator consistency is 
difficult to achieve. Since there is also no general 
consensus on the annotation scheme in Chinese 
NLP and linguistics, building an effective 
interface for public use is almost impossible. 
The Sketch Engine offers an answer to the 
above issues.
3.1 Initial Implementation and Design of the 
Sketch Engine 
The Sketch Engine is a corpus processing 
system developed in 2002 (Kilgarriff and 
Tugwell, 2002; Kilgarriff et al, 2004). The main 
components of the Sketch Engine are KWIC 
concordances, word sketches, grammatical 
relations, and a distributional thesaurus. In its 
first implementation, it takes as input basic BNC 
(British National Corpus, (Leech, 1992)) data: 
the annotated corpus, as well as list of lemmas 
with frequencies. In other words, the Sketch 
Engine has a relatively low threshold for the 
complexity of input corpus. 
The Sketch Engine has a versatile query 
system. Users can restrict their query in any 
sub-corpus of BNC. A query string may be a 
word (with or without POS specification), or a 
phrasal segment. A query can also be performed 
using Corpus Query Language (CQL). The 
output display format can be adjusted, and the 
displayed window of a specific item can be 
freely expanded left and right. Most of all, the 
Sketch Engine produces a Word Sketch 
(Kilgarriff and Tugwell, 2002) that is an 
automatically generated grammatical description 
of a lemma in terms of corpus collocations. All 
items in each collocation are linked back to the 
original corpus data. Hence it is similar to a 
50
Linguistic Knowledge Net anchored by a lexicon 
(Huang et al, 2001). 
 A Word Sketch is a one-page list of a 
keyword?s functional distribution and collocation 
in the corpus. The functional distribution 
includes: subject, object, prepositional object, 
and modifier. Its collocations are described by a 
list of linguistically significant patterns in the 
language. Word Sketch uses regular expressions 
over POS-tags to formalize rules of collocation 
patterns, e.g. (2) is used to retrieve the 
verb-object relation in English:
2. 1:?V? ?(DET|NUM|ADJ|ADV|N)?* 2:?N? 
The expression in (2) says: extract the data 
containing a verb followed by a noun regardless 
of how many determiners, numerals, adjectives, 
adverbs and nouns preceding the noun. It can 
extract data containing cook meals and cooking a 
five-course gala dinner, and cooked the/his/two 
surprisingly good meals etc.
The Sketch Engine also produces thesaurus 
lists, for an adjective, a noun or a verb, the other 
words most similar to it in their use in the 
language (Kilgarriff et al 2004). For instance, 
the top five synonym candidates for the verb kill
are shoot (0.249), murder (0.23), injure (0.229), 
attack (0.223), and die (0.212).2 It also provides 
direct links to the Sketch Difference which lists 
the similar and different patterns between a 
keyword and its similar word. For example, both 
kill and murder can occur with objects such as 
people and wife, but murder usually occurs with 
personal proper names and seldom selects animal 
nouns as complement whereas kill can take fox,
whale, dolphin, and guerrilla, etc. as its object. 
 The Sketch Engine adopts Mutual 
2 The similarity is measured and ranked adopting 
Lin?s (1998) mathematics. 
Information (MI) to measure the salience of a 
collocation. Salience data are shown against each 
collocation in Word Sketches and other Sketch 
Engine output. MI provides a measure of the 
degree of association of a given segment with 
others. Pointwise MI, calculated by Equation 3, 
is what is used in lexical processing to return the 
degree of association of two words x and y (a 
collocation).
3. 
)(
)|(log);(
xP
yxPyxI  
3.2 Application to Chinese Corpus 
In order to show the cross-lingual 
robustness of the Sketch Engine as well as to 
propose a powerful tool for collocation 
extraction based on a large scale corpus with 
minimal pre-processing; we constructed Chinese 
Sketch Engine (CSE) by loading the Chinese 
Gigaword to the Sketch Engine (Kilgarriff et al, 
2005). The Chinese Gigaword contains about 
1.12 billion Chinese characters, including 735 
million characters from Taiwan?s Central News 
Agency, and 380 million characters from China?s 
Xinhua News Agency3. Before loading Chinese 
Gigaword into Sketch Engine, all the simplified 
characters were converted into traditional 
characters, and the texts were segmented and 
POS tagged using the Academia Sinica 
segmentation and tagging system (Huang et al, 
1997). An array of machine was used to process 
the 1.12 million characters, which took over 3 
days to perform. All components of the Sketch 
Engine were implemented, including 
Concordance, Word Sketch, Thesaurus and 
Sketch Difference.  
 In our initial in-house testing of this 
prototype of the Chinese Sketch Engine, it does 
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T09 
51
produce the expected results with an easy to use 
interface. For instance, the Chinese Word Sketch 
correctly shows that the most common and 
salient object of dai.bu ??  ?to arrest? is
xian.fan ??  ?suspect?; the most common 
subject jing.fang ??!?police?; and the most 
common modifier dang.chang??.
 The output data of Thesaurus correctly 
verify the following set of synonyms from the 
Chinese VerbNet Project: that ren.wei ???to
think? behaves most like biao.shi ??  ?to 
express, to state? (salience 0.451), while yi.wei?
? ?to take somebody/something as? is more like 
jue.de?? ?to feel, think? (salience 0.488). The 
synonymous relation can be illustrated by (4) and 
(5).
4a. ????????????????????
?????????????????
ta ren.wei dao hai.wai tou.zi you yi ge guan.nian 
hen zhong.yao, jiu shi yao zhi.dao dang.di de 
you.xi gui.ze 
?He believes that for those investing overseas, 
there is a very important principle-one must know 
the local rules of the game, and accept them.? 
 b. ????????????????????
?????!
zhi.zheng.dang ye biao.shi, you.yu gong.shi 
zheng.yi tai da, kong.pa wu.fa quan.li zhi.chi 
?The KMT also commented that due to the many 
controversies surrounding PTV, it could not 
wholeheartedly support it either.? 
5a. ?????????????????????
????????
he.jia.ju jiu ren.wei??dian.shi you ji.ben yu.yan 
he wen.fa, yao jiang.jiu mai.dian he shi.chang??
?Ho Chia-chu says, "Television has its own 
fundamental language and grammar. You must 
consider selling points and the market."? 
b. ?????????????????????
???????????
ta biao.shi??wo xi.wang fuo.jiao.tu neng liao.jie, 
fu.quan she.hui yu jue.wu de she.hui shi bu 
xiang.he de??
?She says "I hope that followers of Buddhism can 
realize that a patriarchal society is incompatible 
with an enlightened society."? 
The above examples show that ren.wei and 
biao.shi can take both direct and indirect 
quotation. Yi.wei and jue.de, on the other hand, 
can only be used in reportage and cannot 
introduce direct quotation. 
Distinction between near synonymous pairs 
can be obtained from Sketch Difference. This 
function is verified with results from Tsai et al?s 
study on gao.xing?? ?glad? and kuai.le??!
?happy? (Tsai et al, 1998). Gao.xing ?glad? 
specific patterns include the negative imperative 
bie? ?don?t?. It also has a dominant collocation 
with the potentiality complement marker de?
(e.g. ta gao.xing de you jiao you tiao ????
???? ?she was so happy that she cried and 
danced?). In contrast, kuai.le ?happy? has the 
specific collocation with holiday nouns such as 
qiu.jie ??  ?Autumn Festival?. The Sketch 
Difference result is consistent with the account 
that gao.xing/kuai.le contrast is that inchoative 
state vs. homogeneous state. 
4. Evaluation and Future Developments 
An important feature of the prototype of the 
Chinese Sketch Engine is that, in order to test the 
robustness of the Sketch Engine design, the 
original regular expression patterns were adopted 
with minimal modification for Chinese. Even 
though both are SVO languages with similar 
surface word order, it is obvious that they differ 
substantially in terms of assignment of 
grammatical functions. In addition, the Sinica 
tagset is different from the BNC tagset and 
52
actually has much richer functional information. 
These are the two main directions that we will 
pursue in modification and improvement of the 
Chinese Sketch Engine. 
4.1 Word Boundary Representation 
Word breaks are not conventionalized in 
Chinese texts. This poses a challenge in Chinese 
language processing. The Chinese Sketch Engine 
inserted space after segmentation, which helps to 
visualize words. In the future, it will be trivial to 
allow the conventional alternative of no word 
boundary markups. However, it will not be trivial 
to implement fuzzy function to allow searches 
for non-canonical lemmas (i.e. lemmas that are 
segmented differently from the standard corpus). 
4.2 Sub-Corpora Comparison  
The Chinese Gigaword corpus is marked 
with two different genres, story and non-story. A 
still more salient sub-corpus demarcation is the 
one between Mainland China corpus and Taiwan 
corpus. Sketch Difference between lemmas form 
two sub-corpora is being planned. This would 
allow future comparative studies and would have 
wide applications in the localization adaptations 
of language related applications.  
4.3 Collating Frequency Information with 
POS
One of the convenient features of Sketch 
Engine that a frequency ranked word list is 
linked to all major components. This allows a 
very easy and informative reference. Since 
cross-categorical derivation with zero 
morphology is dominant in Chinese, it would 
help the processing greatly if POS information is 
added to the word list. Adding such information 
would also open the possibility of accessing the 
POS ranked frequency information. 
4.5 Fine-tuning Collocation Patterns  
The Sketch Engine relies on collocation 
patterns, such as (2) above, to extract 
collocations. The regular expression format 
allows fast processing of large scale corpora with 
good results. However, these patterns can be 
fine-tuned for better results. We give VN 
collocates with object function as example here. 
In (6), verbs are underlined with a single line, 
and the collocated nouns identified by English 
Word Sketch are underlined with double lines. 
Other nominal objects that the Sketch Engine 
misses are marked with a dotted line. 
6.a. In addition to encouraging kids to ask, think and 
do, parents need to be tolerant and appreciative to 
avoid killing a child's creative sense.
b. Children are taught to love their parents,
classmates, animals, nature . . . . in fact they are 
taught to love just about everything except to 
love China, their mother country. 
c. For example, the government deliberately chose 
not to teach Chinese history and culture, nor 
civics, in the schools. 
d. At the game there will be a lottery drawing for a 
motorcycle! And perhaps you'll catch a foul ball
or a home run.
The sentences in (6) show that the current Sketch 
Engine tend to only identify the first object when 
there are multiple objects. The resultant 
distributional information thus obtained will be 
valid given a sufficiently large corpus. However, 
if the collocation patterns are fine-tuned to allow 
treatment of coordination, richer and more 
precise information can be extracted. 
 A regular expression collocation pattern 
also runs the risk of mis-classification. For 
instance, speech act verbs often allow subject to 
occur in post-verbal positions, and intransitive 
53
verbs can often take temporal nouns in 
post-verbal positions too.  
7. a. ?you can say goodbye to your competitive 
career. 
b. `No,' said Scarlet, `but then I don't notice much.'
8. a. Where did you sleep last night?
  b. ?it arrived Thursday morning.
  c. From Arty's room came the sound of an 
accordion.
9. `I'll look forward to that.' `So will I.' 
Such non-canonical word orders are even more 
prevalent in Chinese. Chinese objects often occur 
in pre-verbal positions in various pre-posing 
constructions, such as topicalization. 
10. ???????????
quan.gu mian.bao, chi le hen jian.kang 
whole-grain bread, eat LE very healthy 
?Eating whole-grain bread is very healthy.? 
11a. ??????????????????
you ren chang.shi yao jiang zhe he.hua fen.lei,
que yue fen yue lei 
someone try to JIANG the lotus classify, but more 
classify more tired 
?People have tried to decide what category the 
lotus belongs in, but have found the effort 
taxing.? 
b. ??????????
wo yi.ding yao ba lao.da chu.diao
I must want BA the oldest (son) get rid of
?I really want to get rid of the older son.?
When objects are pre-posed, they tend to stay 
closer to the verb than the subject. Adding object 
marking information, such as ba?, jiang?, lian
?  would help correctly identify collocating 
pre-posed objects. However, for those unmarked 
pre-posed structures, closeness to the verb may 
not provide sufficient information. Several rules 
will need to be implemented jointly.  
 The above example underlines a critical 
issue. That is, whether relative position alone is 
enough to identify positional information. The 
Sketch Engine is in essence a powerful tool 
extracting generalizations from annotated corpus 
data. We have shown that it can extract useful 
grammatical information with POS tag alone. If 
the corpus is tagged with richer annotation, the 
Sketch Engine should be able to extract even 
richer information. 
 The Sinica Corpus tagset adapts to the fact 
that Chinese has a freer word order than English 
by incorporating semantic information with the 
grammatical category. For instance, locational 
and temporal nouns, proper nouns, and common 
nouns each are assigned a different tag. Verbs are 
sub-categorized according to activity and 
transitivity. Such information is not available in 
the BNC tagset and hence not used in the 
original Sketch Engine design. We will enrich the 
collocation patterns with the annotated linguistic 
information from the Sinica Corpus tagset. In 
particular, we are converting ICG lexical 
subcategorization frames (Chen and Huang 1990) 
to Sketch Engine collocation patters. These ICG 
frames, called Basic Patterns and Adjunct 
Patterns, have already been fully annotated 
lexically and tested on the Sinica Corpus. We 
expect their incorporation to improve Chinese 
Sketch Engine results markedly. 
6. Conclusion 
In this paper, we introduce a powerful tool 
for extraction of collocation information from 
large scale corpora. Our adaptation proved the 
cross-lingual robustness of the Sketch Engine. In 
particular, we show the robustness of the Sketch 
Engine by achieving better results through 
fine-tuning of the collocation patterns via 
integrating available grammatical knowledge. 
54
References 
Chen, Keh-Jiann and Huang, Chu-Ren. 1990.  
Information-based Case Grammar.  
Proceedings of the 13th COLING. Helsinki, 
Finland. 2:54-59. 
Chen, Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, 
Chi-Ching Luo, Ming-Chung Chang, and 
Chao-Jan Chen. 2003. Sinica Treebank: 
Design Criteria, Representational Issues and 
Implementation. In Anne Abeill?e, (ed.): 
Building and Using Parsed Corpora. Text, 
Speech and Language Technology,
20:231-248. Dordrecht: Kluwer.  
CKIP (Chinese Knowledge Information Processing 
Group). 1995/1998. The Content and 
Illustration of Academica Sinica Corpus.
(Technical Report no 95-02/98-04). Taipei: 
Academia Sinica  
Huang, Chu-Ren, Feng-Ju Lo, Hui-Jun Hsiao, 
Chiu-Jung Lu, and Ching-chun Hsieh. 2001. 
From Language Archives to Digital 
Museums: Synergizing Linguistic Databases. 
Presented at the IRCS workshop on linguistic 
Databases. University of Pennsylvania. 
Huang, Chu-Ren, Keh-Jiann Chen, and Lili Chang. 
1997. Segmentation Standard for Chinese 
Natural Language Processing. 
Computational Linguistics and Chinese 
Language Processing. 2(2):47-62.  
Kilgarriff, Adam and Tugwell, David. Sketching 
Words. 2002. In Marie-H?l?ne Corr?ard (ed.): 
Lexicography and Natural Language 
Processing. A Festschrift in Honour of B.T.S. 
Atkins. 125-137. Euralex.  
Kilgarriff, Adam, Chu-Ren Huang, Pavel Rychl?, 
Simon Smith, and David Tugwell. 2005. 
Chinese Word Sketches. ASIALEX 2005: 
Words in Asian Cultural Context. Singapore.  
Kilgarriff, Adam, Pavel Rychl?, Pavel Smrz and 
David Tugwell. 2004. The Sketch Engine. 
Proceedings of EURALEX, Lorient, France. 
(http://www.sketchengine.co.uk/) 
Leech, Geoffrey. 1992. 100 million words of 
English: the British National Corpus (BNC). 
Language Research 28(1):1-13 
Lin, Dekang. 1998. An Information-Theoretic 
Definition of Similarity. Proceedings of 
International Conference on Machine 
Learning. Madison, Wisconsin. 
(http://www.cs.umanitoba.ca/~lindek/publica
tion.htm) 
Tsai, Mei-Chih, Chu-Ren Huang, Keh-Jiann Chen, 
and Kathleen Ahrens. 1998. Towards a 
Representation of Verbal Semantics--An 
Approach Based on Near Synonyms. 
Computational Linguistics and Chinese 
Language Processing. 3(1): 61-74. 
Wu, Yiching and Liu, Mei-Chun. 2003. The 
Construction and Application of Mandarin 
Verbnet. Proceedings of the Third 
International Conference of Internet Chinese 
Education. 39-48. Taipei, Taiwan. 
Xia, Fei, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Fu-Dong Chiou, 
Shizhe Huang, Tony Kroch, and Mitch 
Marcus. 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text 
Annotation. Proceedings of the second 
International Conference on Language 
Resources and Evaluation (LREC 2000), 
Athens, Greece.  
    (http://www.cis.upenn.edu/~chinese/ctb.html)
Xue, Nianwen and Palmer, Martha. 2003. 
Annotating Propositions in the Penn Chinese 
Treebank. Proceedings of the Second Sighan 
Workshop. Sapporo, Japan. 
     (http://www.cis.upenn.edu/~xueniwen/) 
Xue, Nianwen and Palmer, Martha. 2005. 
Automatic Semantic Role Labeling for 
Chinese Verbs. Proceedings of the 19th 
International Joint Conference on Artificial 
Intelligence. Edinburgh, Scotland. 
     (http://www.cis.upenn.edu/~xueniwen/) 
Websites
Sinica Corpus.  
http://www.sinica.edu.tw/SinicaCorpus/  
British National Corpus (BNC). 
http://www.natcorp.ox.ac.uk/  
Center for Chinese Linguistics, PKU.  
http://ccl.pku.edu.cn/#  
Corpora And NLP (Natural Language Processing) 
for Digital Learning of English (CANDLE). 
http://candle.cs.nthu.edu.tw/candle/     
FrameNet.  
http://www.icsi.berkeley.edu/~framenet/  
Penn Chinese Treebank. 
http://www.cis.upenn.edu/~chinese/ctb.html  
Proposition Bank.  
http://www.cis.upenn.edu/~ace/  
Sinica Treebank.   
http://treebank.sinica.edu.tw/  
Sketch Engine (English).  
http://www.sketchengine.co.uk/     
Sketch Engine (Chinese).  
http://corpora.fi.muni.cz/chinese/  
Sou Wen Jie Zi-A Linguistic KnowledgeNet. 
http://words.sinica.edu.tw/ 
55
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 53?56,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Finding Terms in Corpora for Many Languages with the Sketch Engine
Adam Kilgarriff
Lexical Computing Ltd., United Kingdom
adam.kilgarriff@sketchengine.co.uk
Milo
?
s Jakub??
?
cek and Vojt
?
ech Kov
?
a
?
r and Pavel Rychl?y and V??t Suchomel
Masaryk University, Czech Republic
Lexical Computing Ltd., United Kingdom
{xjakub, xkovar3, pary, xsuchom2}@fi.muni.cz
1 Overview
Term candidates for a domain, in a language,
can be found by
? taking a corpus for the domain, and a refer-
ence corpus for the language
? identifying the grammatical shape of a term
in the language
? tokenising, lemmatising and POS-tagging
both corpora
? identifying (and counting) the items in each
corpus which match the grammatical shape
? for each item in the domain corpus, compar-
ing its frequency with its frequency in the
refence corpus.
Then, the items with the highest frequency in the
domain corpus in comparison to the reference cor-
pus will be the top term candidates.
None of the steps above are unusual or innova-
tive for NLP (see, e. g., (Aker et al., 2013), (Go-
jun et al., 2012)). However it is far from trivial
to implement them all, for numerous languages,
in an environment that makes it easy for non-
programmers to find the terms in a domain. This
is what we have done in the Sketch Engine (Kil-
garriff et al., 2004), and will demonstrate. In this
abstract we describe how we addressed each of the
stages above.
2 The reference corpus
Lexical Computing Ltd. (LCL) has been build-
ing reference corpora for over a decade. Corpora
are available for, currently, sixty languages. They
were collected by LCL from the web. For the
world?s major languages (and some others), these
are in the billions of words, gathered using Spider-
Ling (Suchomel and Pomik?alek, 2012) and form-
ing the TenTen corpus family (Jakub???cek et al.,
2013).
3 The domain corpus
There are two situations: either the user already
has a corpus for the domain they are interested in,
or they do not. In the first case, there is a web in-
terface for uploading and indexing the corpus in
the Sketch Engine. In the second, we offer Web-
BootCaT (Baroni et al., 2006), a procedure for
sending queries of ?seed terms? to a commercial
search engine; gathering the pages that the search
engine identifies; and cleaning, deduplicating and
indexing them as a corpus (Baroni and Bernardini,
2004). (The question ?how well does it work??
is not easy to answer, but anecdotal evidence over
ten years suggests: remarkably well.)
4 Grammatical shape
We make the simplifying assumption that terms
are noun phrases (in their canonical form, without
leading articles: the term is base station, not the
base stations.) Then the task is to write a noun
phrase grammar for the language.
5 Tokenising, lemmatising, POS-tagging
For each language, we need processing tools.
While many in the NLP world make the case for
language-independent tools, and claim that their
tools are usable for any, or at least many, lan-
guages, we are firm believers in the maxim ?never
trust NLP tools from people who don?t speak the
language?. While we use language-independent
components in some cases (in particular TreeTag-
ger,
1
RFTagger
2
and FreeLing
3
), we collaborate
with NLP experts in the language to ascertain what
the best available tools are, sometimes to assist
1
http://www.cis.uni-muenchen.de/
?
schmid/tools/TreeTagger/
2
http://www.cis.uni-muenchen.de/
?
schmid/tools/RFTagger/
3
http://nlp.lsi.upc.edu/freeling/
53
in obtaining and customising them, and to verify
that they are producing good quality output. In
most cases these collaborators are also the people
who have written the sketch grammar and the term
grammar for the language.
4
6 Identifying and counting candidates
Within the Sketch Engine we already have ma-
chinery for shallow parsing, based on a ?Sketch
Grammar? of regular expressions over part-of-
speech tags, written in CQL (Corpus Query Lan-
guage, an extended version of the formalism de-
veloped in Stuttgart in the 1990s (Schulze and
Christ, 1996)). Our implementation is mature, sta-
ble and fast, processing million-word corpora in
seconds and billion-word corpora in a few hours.
The machinery has most often been used to find
<grammatical-relation, word1, word2> triples for
lexicography and related research. It was straight-
forward to modify it to find, and count, the items
having the appropriate shape for a term.
7 Comparing frequencies
The challenge of identifying the best candidate
terms for the domain, given their frequency in
the domain corpus and the reference corpus, is a
variant on the challenge of finding the keywords
in a corpus. As argued in (Kilgarriff, 2009), a
good method is simply to take the ratio of the nor-
malised frequency of the term in the domain cor-
pus to its normalised frequency in a reference cor-
pus. Before taking the ratio, we add a constant,
the ?simple maths parameter?, firstly, to address
the case where the candidate is absent in the refer-
ence corpus (and we cannot divide by zero), and
secondly, because there is no one right answer:
depending on the user needs and on the nature
of the corpora, the constant can be raised to give
a list with more higher-frequency candidates, or
lowered to give more emphasis to lower-frequency
items.
Candidate terms are then presented to the user
in a sorted list, with the best candidates ? those
with the highest domain:reference ratio ? at the
top. Each item in the list is clickable: the user can
click to see a concordance for the term, in either
the domain or the reference corpus.
4
Collaborators are typically credited on the ?info? page
for a reference corpus on the Sketch Engine website. The
collaborations are also often agreeable and fruitful in research
terms, resulting in many joint publications.
Figure 2: Term finding results for Japanese, WIPO format.
8 Current status
Languages currently covered by the terminolo-
gy finding system are sumarized in Table 1.
Language POS tagger Ref. corpus
Chinese simp. Stanford NLP zhTenTen11
Chinese trad. Stanford NLP zhTenTen11
English TreeTagger enTenTen08
French TreeTagger frTenTen12
German RFTagger deTenTen10
Japanese MeCab+Comainu jpTenTen11
Korean HanNanum koTenTen12
Portuguese Freeling ptTenTen11
Russian RFTagger ruTenTen11
Spanish Freeling esTenTen11
Table 1: Terminology support for languages in Sketch En-
gine in January 2014. POS tagger is mentioned as an im-
portant part of the corpus processing chain. The last column
shows the corresponding default reference corpus.
The display of term finding results is shown
in Figure 1 for English, for a bootcatted climate-
change corpus. Figure 2 shows a result set for
Japanese in the mobile telecommunications do-
main, prepared for the first users of the sys-
temm, the World Intellectual Property Organisa-
tion (WIPO), using their patents data, with their
preferred display format.
The user can modify various extraction related
options: Keyword reference corpus, term refer-
ence corpus, simple maths parameter, word length
and other word properties, number of top results
to display. The form is shown in Figure 3.
9 Current challenges
9.1 Canonical form: lemmas and word forms
In English one (almost) always wants to present
each word in the term candidate in its canonical,
54
Figure 1: Term finding result in the Sketch Engine ? keywords on the left, multiword terms on the right. The values in paren-
theses represent keyness score and frequency in the focus corpus. The green coloured candidates were used in a WebBootCaT
run to build the corpus. The tickboxes are for specifying seed terms for iterating the corpus-building process.
Figure 3: Term finding settings form
dictionary form. But in French one does not. The
top term candidate in one of our first experiments,
using a French volcanoes corpus, was nu?ee ar-
dente. The problem here is that ardente is the
feminine form of the adjective, as required by the
fact that nu?ee is a feminine noun. Simply tak-
ing the canonical form of each word (masculine
singular, for adjectives) would flout the rule of
adjective-noun gender agreement. A gender re-
specting lemma turns out necessary in such cases.
Noun lemmas beginning with a capital letter
and gender respecting ending of adjectives had to
be dealt with to correctly extract German phrases.
In most of the languages we have been work-
ing on, there are also some terms which should be
given in the plural: an English example is current
affairs. This is a familiar lexicographic puzzle: for
some words, there are distinct meanings limited to
some part or parts of the paradigm, and this needs
noting. We are currently exploring options for this.
9.2 Versions of processing chains
If the version of the tools used for the reference
corpus is not identical to the version used on the
55
domain corpus, it is likely that the candidate list
will be dominated by cases where the two versions
treated the expression differently. Thus the two
analyses of the expression will not match and (in
simple cases), one of the analyses will have fre-
quency zero in each corpus, giving one very high
and one very low ratio. This makes the tool unus-
able if processing chains are not the same.
The reference corpus is processed in batch
mode, and we hope not to upgrade it more than
once a year. The domain corpus is processed
at runtime. Until the development of the term-
finding function, it did not greatly matter if dif-
ferent versions were used. For term-finding, we
have had to look carefully at the tools, separating
each out into an independent module, so that we
can be sure of applying the same versions through-
out. It has been a large task. (It also means that
solutions based on POS-tagging by web services,
where we do not control the web service, are not
viable, since then, an unexpected upgrade to the
web service will break our system.)
10 Evaluation
We have undertaken a first evaluation using the
GENIA corpus (Kim et al., 2003), in which all
terms have been manually identified.
5
First, a plain-text version of GENIA was ex-
tracted and loaded into the system. Keyword and
term extraction was performed to obtain the top
2000 keywords and top 1000 multi-word terms.
Terms manually annotated in GENIA as well as
terms extracted by our tool were normalized be-
fore comparison (lower case, spaces and hyphens
removed) and then GENIA terms were looked up
in the extraction results. 61 of the top 100 GE-
NIA terms were found by the system. The terms
not found were not English words: most were
acronyms, e.g. EGR1, STAT-6.
Concerning the domain corpus size: Although
the extraction method works well even with very
small corpora (e.g. the sample environmental cor-
pus in 1 consists of 100,000 words), larger cor-
pora should be employed to cover more terms. An
early version of this extraction tool was used to
help lexicographers compile environment protec-
tion related terminology. A 50 million words cor-
pus was sufficient in that case. (Avinesh et al.,
2012) report 30 million words is enough.
5
GENIA has also been used for evaluating term-finding
systems by (Zhang et al., 2008).
11 Conclusion
We have built a system for finding terms in a
domain corpus. It is currently set up for nine lan-
guages. In 2014 we shall extend the coverage of
languages and improve the system according to
further feedback from users.
Acknowledgement
This work has been partly supported by the
Ministry of Education of CR within the LINDAT-
Clarin project LM2010013.
References
[Aker et al.2013] A. Aker, M. Paramita, and
R. Gaizauskas. 2013. Extracting bilingual ter-
minologies from comparable corpora. In Proc.
ACL, pages 402?411.
[Avinesh et al.2012] PVS Avinesh, D. McCarthy,
D. Glennon, and J. Pomik?alek. 2012. Domain
specific corpora from the web. In Proc. EURALEX.
[Baroni and Bernardini2004] M. Baroni and S. Bernar-
dini. 2004. Bootcat: Bootstrapping corpora and
terms from the web. In Proc. LREC.
[Baroni et al.2006] M. Baroni, A. Kilgarriff,
J. Pomik?alek, and P. Rychl?y. 2006. Webboot-
cat: instant domain-specific corpora to support
human translators. In Proc. EAMT, pages 247?252.
[Gojun et al.2012] A. Gojun, U. Heid, B. Weissbach,
C. Loth, and I. Mingers. 2012. Adapting and evalu-
ating a generic term extraction tool. In Proc. LREC,
pages 651?656.
[Jakub???cek et al.2013] M. Jakub???cek, A. Kilgarriff,
V. Kov?a?r, P. Rychl?y, and V. Suchomel. 2013. The
tenten corpus family. In Proc. Corpus Linguistics.
[Kilgarriff et al.2004] A. Kilgarriff, P. Rychl?y, P. Smr?z,
and D. Tugwell. 2004. The sketch engine. Proc.
EURALEX, pages 105?116.
[Kilgarriff2009] A. Kilgarriff. 2009. Simple maths for
keywords. In Proc. Corpus Linguistics.
[Kim et al.2003] J-D. Kim, T. Ohta, Y. Tateisi, and
J. Tsujii. 2003. Genia corpusa semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
[Schulze and Christ1996] B. M. Schulze and O. Christ.
1996. The CQP user?s manual. Univ. Stuttgart.
[Suchomel and Pomik?alek2012] V. Suchomel and
J. Pomik?alek. 2012. Efficient web crawling for
large text corpora. In Proc. WAC7, pages 39?43.
[Zhang et al.2008] Z. Zhang, J. Iria, C. A. Brewster, and
F. Ciravegna. 2008. A comparative evaluation of
term recognition algorithms. In Proc. LREC, pages
2108?2113.
56
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 21?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Detailed, Accurate, Extensive, Available English Lexical Database
Adam Kilgarriff
Lexical Computing Ltd
Brighton, UK
adam@lexmasterclass.com
Abstract
We present an English lexical database which
is fuller, more accurate and more consistent
than any other. We believe this to be so be-
cause the project has been well-planned, with
a 12-month intensive planning phase prior to
the lexicography beginning; well-resourced,
employing a team of fifteen highly experi-
enced lexicographers for a thirty-month main
phase; it has had access to the latest corpus
and dictionary-editing technology; it has not
been constrained to meet any goals other than
an accurate description of the language; and
it has been led by a team with singular expe-
rience in delivering high-quality and innova-
tive resources. The lexicon will be complete
in Summer 2010 and will be available for NLP
groups, on terms designed to encourage its re-
search use.
1 Introduction
Most NLP applications need lexicons. NLP re-
searchers have used databases from dictionary pub-
lishers (Boguraev and Briscoe, 1989; Wilks et al,
1996), or developed NLP resources (COMLEX
(Macleod et al, 1994), XTAG (Doran et al, 1994))
or used WordNet,(Fellbaum, 1998) or have switched
to fully corpus-based strategies which need no lex-
icons. However the publishers? dictionaries were
pre-corpus, often inconsistent, and licencing con-
straints were in the end fatal. COMLEX and XTAG
address only syntax; WordNet, only semantics. Also
these resources were not produced by experienced
lexicographers, nor according to a detailed, stringent
?style guide? specifying how to handle all the phe-
nomena (in orthography, morphology, syntax, se-
mantics and pragmatics, from spelling variation to
register to collocation to sense distinction) that make
lexicography complex. Unsupervised corpus meth-
ods are intellectually exciting but do not provide the
lexical facts that many applications need.
We present DANTE (Database of Analysed Texts
of English), an English lexical database. For the
commonest 50,000 words of English, it gives a de-
tailed account of the word?s meaning(s), grammar,
phraseology and collocation and any noteworthy
facts about its pragmatics or distribution.
In outline this is what dictionaries have been do-
ing for many years. This database is of more interest
to NLP than others (for English) because of its:
? quality and consistency
? level of detail
? number of examples
? accountability to the corpus
? purity: it has been created only as an anal-
ysis of English, and has not been compro-
mised by publishing constraints or other non-
lexicographic goals
? availability, on licencing terms that promote its
research use and also the re-use of enhanced
versions created by NLP groups.
2 The Project
The overall project is the preparation of a New En-
glish Irish Dictionary, and is funded by Foras na
Gaeilge, the official body for the (Gaelic) Irish lan-
guage.1 The project was designed according to a
1FnG was set up following the Good Friday Agreement of
1998 on Northern Ireland, between the Governments of the Re-
21
model where the first stage of the production of
a blingual dictionary is a target-language-neutral
monolingual analysis of the source language listing
all the phenomena that might possibly have an unex-
pected translation. (The next stages are then trans-
lation and ?finishing?.) The 2.3 MEuro contract for
the analysis of English was won by Lexicography
MasterClass Ltd in 2007.2 The lexicographers are
working on the letter ?s? at time of writing and the
database will be complete in Summer 2010.
3 Lexicography
Writing a dictionary is a large and complex under-
taking. Planning is paramount.
In the planning phase, we identified all the as-
pects of the behaviour of English words which
a full account of the lexicon should cover. We
then found words exemplifying all aspects, and pre-
pared a sample of one hundred model entries, where
the hundred words chosen covered all the prin-
cipal phenomena (Atkins and Grundy, 2006). A
detailed style guide and corresponding DTD were
written. We created the New Corpus for Ire-
land (NCI) (Kilgarriff, 2006), and set up a corpus
query system (Lexical Computing?s Sketch Engine;
http://www.sketchengine.co.uk) and dictionary edit-
ing system (IDM?s DPS: http://www.idm.fr) for the
project to use. 50,000 headwords were identified
and each was classified into one of eighteen cate-
gories according to type and complexity. This sup-
ported detailed planning of lexicographers? work-
loads and hence, scheduling, as well as adding to the
richness of the data. Template entries (Atkins and
Rundell, 2008, pp123-128) were developed for 68
lexical sets and for words belonging to these sets, the
template was automatically inserted into the draft
dictionary, saving lexicographer time and encourag-
ing consistency.
We identified forty syntactic patterns for verbs,
eighteen for nouns and eighteen for adjectives. Lexi-
cographers were required to note all the patterns that
applied for each word sense.
The lexicographers were all known to the man-
agement team beforehand for their high-quality
public of Ireland and the UK. FnaG is an institution of the two
countries.
2Lexicography MasterClass had also previously undertaken
the planning of the project.
work. They were trained in the dictionary style
at two workshops, and their work was thoroughly
checked throughout the project, with failings re-
ported back and progress monitored.
A typical short entry is honeymoon (shown here
in full but for truncated examples). Note the level
of detail including senses, subsenses, grammatical
structures and collocations. All points are exem-
plified by one or usually more corpus example sen-
tences. (The style guide, available online, states the
conditions for giving one, two or three examples for
a phenomenon.)
honeymoon
? n holiday after wedding
Following the wedding day, Jane and . . .
Upon your return from honeymoon . . .
Lee and Zoe left for a honeymoon in . . .
SUPPORT VERB spend
They now live in Cumbernauld after spending . . .
Their honeymoon was spent at Sandals . . .
SUPPORT VERB have
I hope that you have an absolutely fantastic . . .
The reception was held at the local pub and . . .
SUPPORT PREP on
I have a ring on my left hand which Martha . . .
The groom whisked the bride off on honeymoon . . .
This particular portrait was a festive affair, . . .
STRUCTURE N premod
destination hotel suite holiday night couple
Classic honeymoon destinations like the . . .
We can help and recommend all types of . . .
We were staying in the honeymoon suite . . .
A magical honeymoon holiday in the beautiful . . .
Our honeymoon packages offer a wide range of . . .
It is the favourite of our many honeymoon couples.
? v spend one?s honeymoon
STRUCTURE Particle (locative)
They?ll be honeymooning in Paris (ooh, la la).
Mr and Mrs Maunder will honeymoon in . . .
The couple spent the early part of their . . .
A Dave Lister from five years in the future is . . .
? n period of grace
VARIANT FORM honeymoon period
Since his May 1997 landslide election, Blair has . . .
The UN and Europe were pan national organisations
CHUNK the honeymoon is over
VARIANT the honey moon period is over
The shortest post-election honeymoon is over.
Could the honeymoon period be over that quickly?
22
4 Corpus strategy and innovation
The project team combined expertise in corpora,
computational linguistics and lexicography, and
from the outset the project was to be solidly corpus-
based. In the planning phase we had built the NCI:
by the time the compilation phase started, in 2007, it
was evident not only that the NCI would no longer
capture current English, but also that the field had
moved on and at 250m words, it was too small.
We appended the Irish English data from the NCI
to the much larger and newer UKWaC (Ferraresi et
al., 2008) and added some contemporary American
newspaper text to create the project corpus, which
was then pos-tagged with TreeTagger 3 and loaded
into the Sketch Engine.
The distinctive feature of the Sketch Engine is
?word sketches?: one-page, corpus-driven sum-
maries of a word?s grammatical and collocational
behaviour. The corpus is parsed and a table of col-
locations is given for each grammatical relation. For
DANTE, the set of grammatical relations was de-
fined to give an exact match to the grammatical pat-
terns that the lexicographers were to record. The
same names were used. The word sketch for the
word would, in so far as the POS-tagging, parsing,
and statistics worked correctly, identify precisely the
grammatical patterns and collocations that the lexi-
cographer needed to note in the dictionary.
As is evident, a very large number of corpus sen-
tences needed taking from the corpus into the dic-
tionary. This was streamlined with two processes:
GDEX, for sorting the examples so that the ?best?
(according to a set of heuristics) are shown to the
lexicographer first (Kilgarriff et al, 2008), and ?one-
click-copying? of sentences onto the clipboard (in-
cluding highlighting the nodeword). (In contrast to
a finished dictionary, examples were not edited.)
5 XML-based dictionary preparation
The document type definition uses seventy-two el-
ements. It is as restrictive as possible, given that
accuracy and then clarity take priority. Lexicogra-
phers were not permitted to submit work which did
not validate. Wherever there was a fixed range of
possible values for an information field, the list was
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
included in the DTD as possible values for an at-
tribute and the lexicographer used menu-selection
rather than text-entry.
The database was also used for checking potential
problems in a number of ways. For example, there
are some word senses where examples are not re-
quired, but it is unusual for both senses of a two-or-
more-sense word not to need examples, so we rou-
tinely used XML searching to check lexicographers?
work for any such cases and scrutinised them prior
to approval.
6 None of the usual constraints
Most dictionary projects are managed by publishers
who are focused on the final (usually print) product,
so constraints such as fitting in limited page-space,
or using simplified codes to help naive users, or re-
sponding to the marketing department, or tailoring
the analysis according to the specialist interests of
some likely users, or features of the target language
(for a bilingual dictionary) usually play a large role
in the instructions given to lexicographers. In this
project, with the separation of the project team from
the publisher, we were unusually free of such com-
promising factors.
7 Leadership
Many lexicographic projects take years or decades
longer than scheduled, and suffer changes of intel-
lectual leadership, or are buffeted by political and
economic constraints, all of which produce grave in-
consistencies of style, scale and quality between dif-
ferent sections of the data. A consistent lexicon is
impossible without consistent and rigorous manage-
ment. The credentials of the managers are an indi-
cator of the likely quality of the data.
Sue Atkins, the project manager, has been
the driving force behind the Collins-Robert En-
glish/French Dictionaries (first two editions), the
COBUILD project (with John Sinclair), The Euro-
pean Association for Lexicography (with Reinhart
Hartmann), the British National Corpus, the Ox-
ford Hachette English/French dictionaries (assisted
by Valerie Grundy, DANTE Chief Editor) and with
Charles Fillmore, FrameNet. She has co-published
the Oxford Guide to Practical Lexicography with
Michael Rundell, another of the project management
23
team, who has been Managing Editor of a large num-
ber of dictionaries at Longman and Macmillan.
8 Licencing
In the late 1980s it seemed likely that Longman Dic-
tionary of Contemporary English (LDOCE) would
have a great impact on NLP. But its star rose, but
then promptly fell. As a Longman employee with
the task of developing LDOCE use within NLP, the
first author investigated the reasons long and hard.
The problem was that NLP groups could not
do anything with their LDOCE-based work. They
could describe the work in papers, but the work it-
self was embedded in enhanced versions of LDOCE,
or LDOCE-derived resources, and the licence that
allowed them to use LDOCE did not alow them to
publish or licence or give away any such resource.
So LDOCE research, for academics, was a dead end.
A high-quality dictionary represents an invest-
ment of millions so one cannot expect its owners to
give it away. The challenge then is to arrive at a
model for a dictionary?s use in which its exploration
and enhancement is encouraged, and is not a dead
end, and also in which the owner?s interest in a re-
turn on investment is respected.
DANTE will be made available in a way designed
to meet these goals. It will be licenced for NLP re-
search for no fee. The licence will not allow the
licencee to pass on the resource, but will include an
undertaking from the owner to pass on the licencee?s
enhanced version to other groups on the same terms
(provided it passes quality tests). The owner, or its
agent, will also, where possible, integrate and cross-
validate enhancements from different users. The
owner will retain the right to licence the enhanced
data, for a fee, for commercial use. The model is
presented fully in (Kilgarriff, 1998).
9 DANTE Disambiguation
?DANTE disambiguation? is a program currently in
preparation which takes arbitrary text and, for each
content word in the text, identifies the DANTE pat-
terns it matches and thereby assigns it to one of the
word?s senses in the DANTE database. It is designed
to demonstrate the potential that DANTE has for
NLP, and to undertake in a systematic way a piece
of work that many DANTE users would otherwise
need to do themselves: converting as many DANTE
data fields as possible into methods which either do
or do not match a particular instance of the word.
The program will be freely available alongside the
database.
Acknowledgments
Thanks to colleagues on the project, particularly the
management team of Sue Atkins, Michael Rundell,
Valerie Grundy, Diana Rawlinson and Cathal Con-
very.
References
Sue Atkins and Valerie Grundy. 2006. Lexicographic
profiling: an aid to consistency in dictionary entry de-
sign. In Proc. Euralex, Torino.
Sue Atkins and Michael Rundell. 2008. Oxford Guide to
Practical Lexicography. OUP, Oxford.
Bran Boguraev and Ted Briscoe, editors. 1989. Compu-
tational lexicography for natural language processing.
Longman, London.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: a wide
coverage grammar for english. In Proc. COLING,
pages 922?928.
Christiane Fellbaum, editor. 1998. WordNet, an elec-
tronic lexical database. MIT Press.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
UKWaC, a very large web-derived corpus of English.
In Proc ?WAC4, LREC, Marrakesh.
Adam Kilgarriff, Milos Husak, Katy McAdam, Michael
Rundell, and Pavel Rychly. 2008. Gdex: Automati-
cally finding good dictionary examples in a corpus. In
Proc. Euralex, Barcelona.
Adam Kilgarriff. 1998. Business models for dictioanries
and NLP. Int Jnl Lexicography, 13(2):107?118.
Adam Kilgarriff. 2006. Efficient corpus development
for lexicography: building the new corpus for ireland.
Language Resources and Evaluation Journal.
Catherine Macleod, Ralph Grishman, and Adam Mey-
ers. 1994. The comlex syntax project: the first year.
In Proc ?Human Language Technology workshop, pages
8?12.
Yorick Wilks, Brian Slator, and Louise Guthrie. 1996.
Electric words: dictionaries, computers, and mean-
ings. MIT Press, Cambridge, MA, USA.
24
No-bureaucracy evaluation
Adam Kilgarri
ITRI, University of Brighton
adam@itri.brighton.ac.uk
Senseval is a series of evaluation exer-
cises for Word Sense Disambiguation. The
core design is in accordance with the MUC
and TREC model of quantitative, developer-
oriented (rather than user-oriented) evalua-
tion. The rst was in 1998, with tasks for
three languages and 25 participating research
teams, the second in 2001, with tasks for
twelve languages, thirty-ve participating re-
search teams and over 90 participating sys-
tems. The third is currently in planning. The
scale of the resources developed is indicated in
Table 1 (reproduced from (Edmonds and Kil-
garri, 2002)).
1
In this paper we address ve of the workshop
themes from a Senseval perspective:
1. organisational structure
2. re-use of corpus resources: pro and con
3. the web and evaluation
4. Senseval and Machine Translation eval-
uation
5. re-use of metrics: a cautionary tale.
1 Organisation
One aspect of Senseval of interest here is its
organizational structure. It has no centralised
sponsor to fund or supply infrastructure. Al-
most all work was done by volunteer eort
with just modest local grant funding for par-
ticular subtasks, with organisers answerable
to no-one beyond the community of WSD re-
searchers. This was possible because of the
1
Senseval data sets and results are available at
http://www.senseval.org
level of commitment. People wanted the eval-
uation framework, so they were willing to nd
the time, from whatever slack they were able
to concoct.
At the Senseval-1 workshop, the possibil-
ity of nding an o?cial sponsor {most likely
the EU or a branch of the US administration{
was discussed at length and vigorously. The
prevailing view was that, while it was nice
to have more money around, it was not nec-
essary and came at a cost. Various experi-
ences were cited where researchers felt their
energies had been diverted from the research
itself to the processes of grant applications,
cost statements, and the strange business of
writing reports which in all likelihood no-one
will ever read. My experience, as co-ordinator
of Senseval-1 and chair of Senseval-2, was
that, without external funding but with great
goodwill and energy for the task at various lo-
cations round the globe, it was possible to get
a vast amount done in a short time, at some
cost to family life but with a minimum of mis-
directed eort.
At several points, potential funders have
said \All you need to do is ll in our form..." It
is always worth asking whether this is a poi-
soned chalice. How much eort will it take
to ll in, and how much more to follow it
through? What is the cost to my engagement
and enthusiasm of doing things their way (as
I shall have to, if I take the king's shilling, as
good governance demands that procedures are
followed, forms are lled, any changes to the
original plan are justied and documented ...).
I should note that, possibly, my perspective
here is atypical. As the co-ordinator, without
Table 1: Senseval-2, resources, participation, results.
Language Task
a
Systems Lemmas Instances
b
IAA
c
Baseline
d
Best score
Czech AW 1 {
e
277,986 { { 94
Basque LS 3 40 5,284 75 65 76
Dutch AW 1 1,168 16,686 { 75 84
English AW 21 1,082 2,473 75 57 69
English LS 26 73 12,939 86 48/16
f
64/40
Estonian AW 2 4,608 11,504 72 85 67
Italian LS 2 83 3,900 21 { 39
Japanese LS 7 100 10,000 86 72 78
Japanese TM 9 40 1,200 81 37 79
Korean LS 2 11 1,733 { 71 74
Spanish LS 12 39 6,705 64 48 65
Swedish LS 8 40 10,241 95 { 70
a
AW: all-words task, LS: lexical sample, TM: translation memory.
b
Total instances annotated in both training and test corpora. In the default case, they were split 2:1
between training and test sets.
c
Inter-annotator agreement is generally the average percentage of cases where two (or more) anno-
tators agree, before adjudication. However there are various ways in which it can be calculated, so the
gures in the table are not all directly comparable.
d
Generally, choosing the corpus-attested most frequent sense, although this was not always possible
or straightforward.
e
A dash `{' indicates the data was unavailable.
f
Supervised and unsupervised scores are separated by a slash.
a funder as taskmaster, I had a particularly
free hand to ordain as I saw t. This was
most agreeable, but it is quite possible that
others involved saw me as their (more or less
reasonable, more or less benevolent) dictator
and bureaucracy, and did not share the plea-
sures of autonomy that I experienced.
I am not sure that I advocate the no-
bureaucracy approach: clearly, it depends on
there being some slack somewhere which can
be redirected. It is however a model well worth
considering, if only because it is such fun work-
ing with other committed volunteers for no
better reason than that you all want to reach
the same goal.
2 The re-use trap
Consider the following position (Redux, 2001):
As followers of the literature will
have noted, great strides have been
made in statistical parsing. In two
decades, system performance gures
have soared to over 90%. This
is a magnicent tale. Parsing is
cracked. An enormous debt is owed
to the producers of the Penn Tree-
bank. As anticipated by Don Walker,
marked-up resources were what we
needed. Once we had them, the al-
gorithm boys could set to work, and
whoomph!
The benets of concentrating on the
one corpus have been enormous. The
eld has focused. It has been the mi-
croscope under which the true nature
of language has become apparent.
Like Mendel unpacking the secrets
of all species' genetics through as-
siduous attention to sweet peas, and
sweet peas alone, Charniak, Collins,
and others have unpacked the secrets
of grammatical structure through
rigorous attention to the Wall Street
Journal.
We would now like to point out the
unhelpfulness of comments appear-
ing on the CORPORA mailing list,
reporting low performance of various
statistical POS-taggers when applied
to text of dierent types to the train-
ing material, and also of a footnote
to a recent ACL paper, according
to which a leading Penn-Treebank-
trained parser was applied to literary
texts but then its performance "sig-
nicantly degraded". These results
have not, I am glad to say, entered
beyond that footnote into the scien-
tic literature. The authors should
realise that it is prima facie invalid
to apply a resource trained on one
type of data, to another. Anyone
wishing to use a statistical parser on
a text type for which a manually-
parsed training corpus does not ex-
ist, must rst create the training cor-
pus. If they are not willing to do
that, they may as well accept that
ten years of dazzling progress is of
no use to them.
. . .
So now, our proposal. We are encour-
aged to see the amount of work based
on the Wall Street Journal which ap-
pears in ACL proceedings. However
we remain concerned about the quan-
tity of papers appearing there which
fail to use a rigorous methodology,
and fail to build on the progress out-
lined above. These papers tend to
fall outside the domain which has
become the testing ground for our
understanding of the phenomenon of
language, viz, the Wall Street Jour-
nal. Outside the Wall Street Journal,
we are benighted. May I suggest that
ACL adopt a policy of accepting only
papers investigating the language of
the Wall Street Journal.
A similar position was discussed in relation
to Senseval. There was a move to use, in part
or in whole, the same sample of words (ca 40
items) for Senseval-2 (English lexical sample
task) as had been used in Senseval-1. This
would have promoted comparability of results
across the two exercises. However, we were
anxious about continuing to focus our eorts
on just 40 of the 10,000 ambiguous words of
the language, as it seemed plausible that some
issues had simply not arisen in the rst sample,
and if we did not switch sample, there was no
chance that they would ever be encountered.
All Senseval resources are in the public
domain and can be (and have been) used by
researchers wanting to compare their system
performance with performance gures as in
Senseval proceedings. Of course such com-
parison will never be fair, as systems compet-
ing under the examination conditions of the
evaluation exercise were operating under time
pressure, and did not always have time to cor-
rect even the most egregious of bugs. However
it is hard to see how the evaluation series can
keep the sheer range and variety of language
use on the agenda if samples are reused.
3 Language ow and the web
You cannot step twice into the same
river, for other waters are constantly
owing on.
Heraclitus (c. 535-c. 475 BC)
We are currently planning a Senseval-3 task
where the test data will be instances of words
in web pages, as located by a search engine.
Test data will be dened by URL, line num-
ber and byte oset. The goal is to explore
what happens when laboratory conditions are
changed for web conditions. It will support ex-
ploration of how supervised-training systems
perform when test set and training set are no
longer subsets of the same whole. Partipants
will be expected to rst retrieve the web page
and then apply WSD to it. This will allow
systems to use a wider context than is possi-
ble in the usual paradigm of short-context test
instances. They could, for example, gather a
corpus of the reference URL, plus any pages it
links to, plus other pages close to it in its di-
rectory tree, in order to identify the domain
of the instance. In general, it makes space
for a range of techniques which the Senseval
paradigm to date has ruled out.
Clearly, web pages may change or die be-
tween selecting URLs for manual tagging at
set-up time, and the evaluation period, re-
sulting in wasted manual-tagging eort. We
shall minimize the waste by, rst, drawing
up a candidate list of URL's, then, checking
them to see whether they are still available
and unchanged a month or so later. The fact
that some web pages have died will not in-
validate the exercise. It just means there will
be fewer usable test instances than test-URLs
distributed.
One hypothesis to be explored is that
supervised-training systems are less resilient
than other system-types, in the real world situ-
ation where the data to be disambiguated \in
anger" may not match the text type of the
training corpus. The relation between the per-
formance of supervised-training systems in the
laboratory and in the wild is to my mind one
of the critical issues at the current point in
time, given the ascendancy that the paradigm
has achieved in CL.
It may also shed light on the relation be-
tween a linguistic/collocational view of word
senses and one dominated by domain. In-
evitably, for some words, there will be a poor
match between the domains of training-corpus
instances and the domains of web instances.
While this might seem `unfair' and a problem
following from the biases of the web, it is a fact
of linguistic life. The concept of an unbiased
corpus has no theoretical credentials. The task
will explore the implications of working with a
corpus whose biases are unknown, and in any
case forever changing.
The web also happens to be the corpus that
many potential customers for WSD need to
operate on, so the task will provide a picture
of whether WSD technology is yet ready for
these potential clients.
4 Senseval and Machine
Translation evaluation
As noted above, overall Senseval design is
taken from MUC. We have also followed MUC
and TREC discussions of the hub-and-spokes
model and the need to forever look towards
updating the task, to guard against partici-
pants becoming expert at the task as dened
but not at anything else.
WSD is not a task of interest in itself. One
does WSD in order to improve performance on
some other task. The critical end-to-end task,
for WSD, is Machine Translation (Kilgarri,
1997).
In Senseval-2, for Japanese there was a
translation memory task, which took the form
of an MT evaluation (Kurohashi, 2001). In
that experimental design, each system re-
sponse potentially requires individual atten-
tion from a human assessor. As in assess-
ing human or computer translation, one can-
not specify a complete set of correct answers
ahead of time, so one must be open to the
possibility that the system response is cor-
rect but dierent from all the responses seen
to date. Thus the exercise is potentially far
more expensive than the MUC model. In the
MUC model, human attention is required for
each data instance. In this model, human at-
tention is potentially required for each data-
instance/system combination.
Another consequence is that there is no free-
standing, system-independent gold standard
corpus of correct answers. New or revised sys-
tems cannot simply test against a gold stan-
dard (unless they limit their range of possible
answers to ones already encountered, which
would introduce further biases).
So it is a more complex and costly form
of evaluation. However it is also far more
closely related to a real task. It is a direction
that Senseval needs to take.
2
The MUC-
style xed-sense-inventory should be seen as
what was necessary to open the chapter on
WSD evaluation: a graspable, manageable
task when we had no experience of the dif-
culties we might encounter, which also pro-
vided researchers with some objective datasets
for their development work. For the future the
2
It is also the route we have taken in the WASPS
project, which is geared towards WSD for MT (Koeling
et al, 2003).
emphasis needs to be on assessments such as
the Japanese one, related to real tasks.
5 Metric re-use: kappa
Consider the (ctional) game show \Couples".
The idea is to establish which couples share the
same world view to the greatest extent. Each
member of the couple is put in a space where
they cannot hear what the other is saying, and
is then asked twenty multiple-choice questions
like
What is the greatest UK pop group of the
1960s?
The Beatles/The Rolling Stones
or
Which month is your oldest nephew/niece's
birthday?
Jan/Feb/Mar/Apr/May/Jun/Jul
/Aug/Sep/Oct/Nov/Dec /No-
nephew-or-niece
The couple that gives the same answer most
often wins.
Dierent couples get dierent questions,
sometimes with dierent numbers of multiple-
choice options, and this introduces a risk of
unfairness. If one couple gets all two-way
choices, while another gets all 13-way choices,
and both agree half the time, the 13-way cou-
ple have really done much better. Random
guessing would have got (on average) a 50%
score for the couple who got the two-way ques-
tions, whereas it would only have got a 1/13
or 7.7% score for the others.
One way to x the problem is to give, for
each question, not a full point but a score mod-
ied to allow for what random guessing would
have given. This can be dened as
 =
P (A)  P (E)
1  P (E)
where P (A) is the proportion of times they
actually agree, and P (E) is the proportion of
times they would agree by chance.
This is called the Kappa statistic. It was de-
veloped within the discipline of Content Anal-
ysis, and introduced into the HLT world by
Jean Carletta (Carletta, 1996).
Inter-Annotator Agreement
For HLT, the issue arises in manual tagging
tasks, such as manually identifying the word
class or word sense of a word in the text, or
the discourse function of a clause. In each of
these cases, there will be a xed set of possible
answers. Consider two exercises, one where a
team of two human taggers tag a set of clauses
for discourse function using a set of four pos-
sible functions, the other where another team
of two uses a set of fteen possible functions.
If the rst team gave the same answers 77%
of the time, and the second gave the same an-
swers 71% of the time, then, at a rst pass,
the rst team had a higher agreement level.
However they were using a smaller tagset, and
we can use kappa to compensate for that. The
kappa gure for the rst team is
0:77  1=4
1  1=4
=
0:52
0:75
= 0:69
and that for the second team is
0:71   1=15
1  1=15
=
0:64
0:93
= 0:69
The inter-annotator agreement (IAA) can be
presented as simple agreement gures of 77%
and 71%, or as kappa values of 0.69 in both
cases.
IAA matters to HLT evaluation because hu-
man tagging is what is needed to produced
`gold standard' datasets against which system
performance can be judged. The simplest ap-
proach is for a person to mark up a text,
and to evaluate the system against those tag-
gings. But the person might make mistakes,
and there may be problems of interpretation
and judgement calls where a dierent human
may well have given a dierent answer. So, for
gold standard dataset development, each item
to be tagged should be tagged by at least two
people.
How condent can we be in the integrity of
the gold standard? Do we really know that it
is correct? A central consideration is IAA: if
taggers agreed with each other nearly all the
time, we can be condent that, rstly, the gold
standard corpus is not full of errors, and sec-
ondly, that the system of categories, or tags,
according to which the markup took place is
adequate to the task. If the tags are not well-
suited to the task and adequately dened, it
will frequently be arbitrary which tag a tagger
selects, and this will show up in low IAA.
Reservations
Carletta presented kappa as a better mea-
sure of IAA than uncorrected agreement. In
the specic cases she describes, this is certainly
valid.
Those cases are very specic. Kappa is rel-
evant where the concern is that an IAA gure
based on a small tagset is being compared with
one based on a large tagset. Where that is the
focus of the investigation, kappa is an appro-
priate statistic.
Where it is not, there are arguments for and
against the use of kappa. In its favour is that
it builds in compensation for distortions that
might otherwise go unnoticed resulting from
dierent tagset sizes.
Against is, principally, the argument that
kappa gures are hard to interpret. A simple
agreement gure is just that: it is clear what
it means, and the critical question of whether,
say, 90% agreement is `good enough' is one
for the reader to form their own judgment on.
With a kappa gure of .85, the reader needs to,
rstly, understand the mathematics of kappa,
and secondly, bear in mind the various com-
plexities of how kappa might have been calcu-
lated (see also below), before forming a judg-
ment. To \help" the reader with this task,
there are various discussions in the literature
as to how dierent kappa gures are to be in-
terpreted. Sadly, these are contradictory (and
even if they weren't, it is the duty of any criti-
cal reader to form their own judgment on what
is good enough.)
Complexities in the calculation
Above we present kappa in its simplest form.
Naturally, when used in earnest additional is-
sues arise. The observations below arose prin-
cipally from the consideration of how we might
use kappa in Senseval. The task was to pro-
duce a gold standard corpus in which words
were associated with their appropriate mean-
ings, with the inventory of meanings taken
from a dictionary.
Firstly, tagset size is assumed to be xed.
In the Senseval context, there were three is-
sues here.
1. There were two variants of the task: `lex-
ical sample' and `all-words'. In the all-
words variant, all content words in a text
are tagged. Some will be highly polyse-
mous, others not polysemous at all. It
is not clear how to present kappa gures
that are averages across datasets where
the tagset size varies.
In the lexical sample task, rst, a sample
of sentences containing a particular word
is identied, and then, only the instances
of that word are tagged, so the issue does
not arise immediately. It does still arise
if a kappa gure is to be computed which
draws together data from more than one
lexical-sample word.
2. In addition to the dictionary senses for
the word, there were two tags, U for
`unassignable' and P for `proper name',
which were always available as options for
the human taggers. If included, for pur-
poses of calculating kappa, a word that
only has two dictionary senses is classi-
ed as a four-way choice, which seems in-
appropriate, particularly as U and P tags
were quite rare and absent entirely for
some words.
3. There were a number of other `marginal'
senses which, if included in the tag count,
extend it greatly (for some words). In
the Senseval-1, taggers largely worked
within a given word class, so noun in-
stances of oat were treated separately
from verb instances, but, in e.g., noun
cases where none of the noun instances
tted, they were instructed to consider
whether any of the verb senses were a
good semantic match (even though they
evidently could not be a syntactic match).
Also some words formed part of numer-
ous multi-word units that were listed in
the dictionary. Where a tagger found
the lexical-sample word occurring within
a listed multi-word unit, the instruction
was to assign that as a sense.
One response to issues 2 and 3 is to use a
more sophisticated model of random guessing,
in which, rather than assuming all tags are
equally likely for the random guesser, we use
the relative frequencies of the dierent tags as
the basis for a probability model . The method
succeeds in giving less weight to marginal tags,
at the cost of making the maths of the calu-
clation more complex and the output kappa
gures correspondingly harder to interpret.
Secondly, the Senseval tagging scheme
allowed human taggers to give multiple an-
swers, and also allowed multiple answers in the
tagging scheme.
Thirdly, in Senseval the number of hu-
mans tagging an instance varied (according to
whether or not the instance was problematic).
Fourthly, there is a distinction between two
kinds of occasion on which two taggers give
dierent tags. It may be a problematic case
to tag, or it may be simple human error (such
as a typo). Arguably, simple typos and sim-
ilar are of no theoretical interest and should
be corrected before considering IAA. A related
point is the distinction between agreement lev-
els (between individual taggers) and replica-
bility (between teams of taggers). Where the
concern is the integrity of a gold standard re-
source, replicability is the real matter of in-
terest: would another team of taggers, using
the same data, guidelines and methods, arrive
at the same taggings? A tagging methodology
which guards against simple errors, wayward
individuals, and wayward interpretations will
tend to produce replicable datasets.
All of these considerations can be addressed
using a variant of kappa. My point is that
kappa becomes harder and harder to interpret,
as more and more assumptions and intricacies
are built into its calculation.
Kappa has been widely embraced as an ex-
ample of an aspect of evaluation technology
that carries across dierent HLT evaluation
tasks, giving a shimmer of statistical sophis-
tication wherever it alights. My sense is that
it is a bandwagon, which HLT researchers have
felt they ought to jump on in order to display
their scientic credentials and ability to use
statistics, which, in many places where it has
been used, has led to little but gratuitous ob-
fuscation.
6 Conclusion
Clearly, we would like new HLT evaluation ex-
ercises to benet from evaluation work already
done. This paper explores several issues that
have arisen from the Senseval experience.
References
Jean Carletta. 1996. Assessing agreement on clas-
sication tasks: The kappa statistic. Computa-
tional Linguistics, 22(2):249{254.
Philip Edmonds and Adam Kilgarri. 2002. Intro-
duction to the special issue on evaluating word
sense disambiguation systems. Journal of Nat-
ural Language Engineering, 8(4).
Adam Kilgarri. 1997. What is word sense dis-
ambiguation good for? In Proc. Natural Lan-
guage Processing in the Pacic Rim (NLPRS
'97), pages 209{214, Phuket, Thailand, Decem-
ber.
Rob Koeling, Roger Evans, Adam Kilgarri, and
David Tugwell. 2003. An evaluation pf a lex-
icographer's workbench: building lexicons for
machine translation. In EACL workshop on re-
sources for Machine Translation, Budapest.
Sadao Kurohashi. 2001. senseval-2 japanese
translation task. In Proc. senseval-2: Second
International Workshop on Evaluating WSD
Systems, pages 37{40, Toulouse, July. ACL.
Swift Redux. 2001. A modest proposal. ELSnews,
10(2):7.
EACL-2006   11th Conference  of the European Chapter of the  Association for Computational Linguistics   Proceedings of the 2nd International Workshop on   Web as Corpus    Chairs: Adam Kilgarriff Marco Baroni        April 2006 Trento, Italy 
The conference, the workshop and the tutorials are sponsored by: 
 
 
 
 
 
 
Celct 
c/o BIC, Via dei Solteri, 38 
38100 Trento, Italy 
http://www.celct.it 
 
 
 
   
 
Xerox Research Centre Europe 
6 Chemin de Maupertuis 
38240 Meylan, France 
http://www.xrce.xerox.com 
 
 
 
 
  
Thales 
45 rue de Villiers 
92526 Neuilly-sur-Seine Cedex, France 
http://www.thalesgroup.com 
 
 
EACL-2006  is supported by 
Trentino S.p.a.   and Metalsistem Group  
 
 
 
 
 
? April 2006, Association for Computational Linguistics 
 
Order copies of ACL proceedings from:  
Priscilla Rasmussen,  
Association for Computational Linguistics (ACL),  
3 Landmark Center, 
East Stroudsburg, PA 18301  USA 
 
Phone  +1-570-476-8006 
Fax  +1-570-476-0860 
E-mail:  acl@aclweb.org  
On-line order form:  http://www.aclweb.org/ 
 
 
 
 
 
 
 
  
 
  
CELI s.r.l.  
Corso Moncalieri, 21 
10131 Torino, Italy 
http://www.celi.it 
WAC2: Programme 
 
9.00-9.30 Marco Baroni and Adam Kilgarriff 
Introduction 
 
9.30-10.00 Andr?s Kornai, P?ter Hal?csy, Viktor Nagy, Csaba Oravecz, Viktor Tr?n and 
D?niel Varga 
   Web-based frequency dictionaries for medium density languages 
 
10.00-10.30 Mike Cafarella and Oren Etzioni 
  BE: a search engine for NLP research 
 
 Break 
 
11.00-11.30 Masatsugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro Sasaki, 
Takehito Utsuro and Satoshi Sato 
A comparative study on compositional translation estimation using a 
domain/topic-specific corpus collected from the web 
 
11.30-12.00 Gemma Boleda, Stefan Bott, Rodrigo Meza, Carlos Castillo, Toni Badia and 
Vicente L?pez 
   CUCWeb: a Catalan corpus built from the web 
 
12.00-12.30 Paul Rayson, James Walkerdine, William H. Fletcher and Adam Kilgarriff 
  Annotated web as corpus 
 
 Lunch 
 
2.30-3.00 Arno Scharl and Albert Weichselbraun 
  Web coverage of the 2004 US presidential election 
 
3.00-3.30 C?drick Fairon 
  Corporator: A tool for creating RSS-based specialized corpora 
 
3.30-4.00 Demos, part 1 
 
 Break 
 
4.30-4.50 Demos, part 2 
 
4.50-5.20 Davide Fossati, Gabriele Ghidoni, Barbara Di Eugenio, Isabel Cruz, Huiyong 
Xiao and Rajen Subba 
   The problem of ontology alignment on the web: a first report 
 
5.20-5.50 Kie Zuraw 
  Using the web as a phonological corpus: a case study from Tagalog 
 
5.50-6.00 Organization, next meeting, closing 
 
Reserve paper 
 
R?diger Gleim, Alexander Mehler and Matthias Dehmer  
  Web corpus mining by instance of Wikipedia 
 
  
 
 
 
iii
Programme Committee 
 
Toni Badia 
Marco Baroni (co-chair) 
Silvia Bernardini 
Massimiliano Ciaramita 
Barbara Di Eugenio 
Roger Evans 
Stefan Evert 
William Fletcher 
R?diger Gleim 
Gregory Grefenstette 
P?ter Hal?csy 
Frank Keller 
Adam Kilgarriff (co-chair) 
Rob Koeling 
Mirella Lapata 
Anke L?deling 
Alexander Mehler 
Drago Radev 
Philip Resnik 
German Rigau 
Serge Sharoff 
David Weir 
 
 
iv
Preface 
 
What is the role of a workshop series on web as corpus? 
 
We argue, first, that attention to the web is critical to the health of non-corporate NLP, since 
the academic community runs the risk of being sidelined by corporate NLP if it does not 
address the issues involved in using very-large-scale web resources; second, that text type 
comes to the fore when we study the web, and the workshops provide a venue for nurturing 
this under-explored dimension of language; and thirdly that the WWW community is an 
important academic neighbour for CL, and the workshops will contribute to contact between 
CL and WWW. 
 
High-performance NLP needs web-scale resources 
 
The most talked-about presentation of the ACL 2005 was Franz-Josef Och?s, in which he 
presented statistical MT results based on a 200 billion word English corpus.  His results led 
the field.  He was in a privileged position to have access to a corpus of that size.  He works at 
Google.   
 
With enormous data, you get better results. (See e.g. Banko and Brill 2001.)  It seems to us 
there are two possible responses for the academic NLP community.  The first is to accept 
defeat: ?we will never have resources on the scale Google has, so we should accept that our 
systems will not really compete, that they will be proofs-of-concept or deal with niche 
problems, but will be out of the mainstream of high-performance HLT system development.? 
The second is to say: we too need to make resources on this scale available, and they should 
be available to researchers in universities as well as behind corporate firewalls: and we can do 
it, because resources of the right scale are available, for free, on the web.  We shall of course 
have to acquire new expertise along the way ? at, inter alia, WAC workshops. 
 
Text type 
 
The most interesting question that the use of web corpora raises is text type.  (We use ?text 
type? as a cover-all term to include domain, genre, style etc.)  The first question about web 
corpora from an outsider is usually ?how do you know that your web corpus is 
representative?? to which the fitting response is ?how do you know whether any corpus is 
representative (of what?)?.   These questions will only receive satisfactory answers when we 
have a fuller account of how to identify and distinguish different kinds of text. 
 
While text type is not centre-stage in this volume, we suspect it will be prominent in 
discussions at the workshop and will be the focus of papers in future workshops. 
 
The WWW community: links, web-as-graph, and linguistics 
 
One of CL?s academic neighbours is the WWW community (as represented by, eg, the 
WWW conference series).  Many of their key questions concern the nature of the web, 
viewing it as a large set of domains, or as a graph, or as a bag of bags of words.  The web is 
substantially a linguistic object, and there is potential for these views of the web contributing 
to our linguistic understanding. For example, the graph structure of the web has been used to 
identify highly connected areas which are ?web communities?.  How does that graph-
theoretical connectedness relate to the linguistic properties one would associate with a 
discourse community?  To date the links between the communities have been not been strong.  
(Few WWW papers are referenced in CL papers, and vice versa.)  The workshops will 
provide a venue where WWW and CL interests intersect. 
 
v
Recent work by co-chairs and colleagues 
 
At risk of abusing chairs? privilege, we briefly mention two pieces of our own work.  In the 
first we have created web corpora of over 1 billion words for German and Italian.  The text 
has been de-duplicated, passed through a range of filters, part-of-speech tagged, lemmatized, 
and loaded into a web-accessible corpus query tool supporting a wide range of linguists? 
queries.  It offers one model of how to use the web as a corpus. The corpora will be 
demonstrated in the main EACL conference (Baroni and Kilgarriff 2006).   
 
In the second, WebBootCaT (work with Jan Pomikalek and Pavel Rychl? of Masaryk 
University, Brno), we have prepared a version of the BootCaT tools (Baroni and Bernardini 
2004) as a web service. Users fill in a web form with the target language and some ?seed 
terms? to specify the domain of the target corpus, and press the ?Build Corpus? button.  A 
corpus is built.  Thus, people without any programming or software-installation skills can 
create corpora to their own specification.  The system will be demonstrated in the ?demos? 
session of the workshop.   
 
The workshop series to date 
 
This is the second international workshop, the first being held in July 2005 in Birmingham, 
UK (in association with Corpus Linguistics 2005).  There was an earlier Italian event in Forl?, 
in January 2005.   All three have attracted high levels of interest. The papers in this volume 
were selected following a highly competitive review process, and we would like to thank all 
those who submitted, all those on the programme committee who contributed to the review 
process, and the additional reviewers who helped us to get through the large number of 
submissions. Special thanks to Stefan Evert for help with assembling the proceedings. 
(Cafarella and Etzioni have an abstract rather than a full paper to avoid duplicate publication: 
we felt their presentation would make an important contribution to the workshop, which was a 
distinct issue to them not having a new text available.)  
 
We are confident that there will be much of interest for anyone engaged with NLP and the 
web. 
 
References 
 
Banko, M. and E. Brill. 2001. ?Mitigating the Paucity-of-Data Problem: Exploring the Effect of 
Training Corpus Size on Classifier Performance for Natural Language Processing.?  In Proc. 
Human Language Technology Conference (HLT 2001) 
Baroni, M and S. Bernardini 2004. BootCaT: Bootstrapping corpora and terms from the web. Proc. 
LREC 2004, Lisbon: ELDA. 1313-1316.  
Baroni, M. and A. Kilgarriff 2006.  ?Large linguistically-processed web corpora for multiple 
languages.? Proc EACL, Trento, Italy. 
M?rquez, L. and D. Klein 2006. Announcement and Call for Papers for the Tenth Conference on 
Computational Natural Language Learning. http://www.cnts.ua.ac.be/conll/cfp.html  
Och, F-J. 2005. ?Statistical Machine Translation: The Fabulous Present and Future? Invited talk at 
ACL Workshop on Building and Using Parallel Texts, Ann Arbor. 
 
 
 
Adam Kilgarriff and Marco Baroni, February 2006 
vi
Table of Contents
Web-based frequency dictionaries for medium density languages
Andra?s Kornai, Pe?ter Hala?csy, Viktor Nagy, Csaba Oravecz, Viktor Tro?n and Da?niel Varga . . . . . . . . . . . . . 1
BE: A search engine for NLP research
Mike Cafarella and Oren Etzioni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
A comparative study on compositional translation estimation using a domain/topic-specific corpus collected from
the Web
Masatsugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro Sasaki, Takehito Utsuro and S. Sato . . .11
CUCWeb: A Catalan corpus built from the Web
Gemma Boleda, Stefan Bott, Rodrigo Meza, Carlos Castillo, Toni Badia and Vicente Lo?pez . . . . . . . . . . . 19
Annotated Web as corpus
Paul Rayson, James Walkerdine, William H. Fletcher and Adam Kilgarriff . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Web coverage of the 2004 US Presidential election
Arno Scharl and Albert Weichselbraun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Corporator: A tool for creating RSS-based specialized corpora
Ce?drick Fairon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
The problem of ontology alignment on the Web: A first report
Davide Fossati, Gabriele Ghidoni, Barbara Di Eugenio, Isabel Cruz, Huiyong Xiao and Rajen Subba . . . 51
Using the Web as a phonological corpus: A case study from Tagalog
Kie Zuraw. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59
Web corpus mining by instance of Wikipedia
Ru?diger Gleim, Alexander Mehler and Matthias Dehmer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
vii
 viii
Helping Our Own:
Text Massaging for Computational Linguistics as a New Shared Task
Robert Dale
Centre for Language Technology
Macquarie University
Sydney, Australia
Robert.Dale@mq.edu.au
Adam Kilgarriff
Lexical Computing Ltd
Brighton
United Kingdom
adam@lexmasterclass.com
Abstract
In this paper, we propose a new shared task
called HOO: Helping Our Own. The aim is
to use tools and techniques developed in com-
putational linguistics to help people writing
about computational linguistics. We describe
a text-to-text generation scenario that poses
challenging research questions, and delivers
practical outcomes that are useful in the first
case to our own community and potentially
much more widely. Two specific factors make
us optimistic that this task will generate useful
outcomes: one is the availability of the ACL
Anthology, a large corpus of the target text
type; the other is that CL researchers who are
non-native speakers of English will be moti-
vated to use prototype systems, providing in-
formed and precise feedback in large quantity.
We lay out our plans in detail and invite com-
ment and critique with the aim of improving
the nature of the planned exercise.
1 Introduction
A forbidding challenge for many scientists whose
first language is not English is the writing of ac-
ceptable English prose. There is a concern?
perhaps sometimes imagined, but real enough to be
a worry?that papers submitted to conferences and
journals may be rejected because the use of language
is jarring and makes it harder for the reader to follow
what the author intended. While this can be a prob-
lem for native speakers as well, non-native speakers
typically face a greater obstacle.
The Association for Computational Linguistics?
mentoring service is one part of a response.1 A men-
toring service can address a wider range of problems
than those related purely to writing; but a key moti-
vation behind such services is that an author?s mate-
rial should be judged on its research content, not on
the author?s skills in English.
This problem will surface in any discipline where
authors are required to provide material in a lan-
guage other than their mother tongue. However, as
a discipline, computational linguistics holds a priv-
ileged position: as scientists, language (of different
varieties) is our object of study, and as technologists,
language tasks form our agenda. Many of the re-
search problems we focus on could assist with writ-
ing problems. There is already existing work that
addresses specific problems in this area (see, for ex-
ample, (Tetreault and Chodorow, 2008)), but to be
genuinely useful, we require a solution to the writing
problem as a whole, integrating existing solutions to
sub-problems with new solutions for problems as yet
unexplored.
Our proposal, then, is to initiate a shared task that
attempts to tackle the problem head-on; we want to
?help our own? by developing tools which can help
non-native speakers of English (NNSs) (and maybe
some native ones) write academic English prose of
the kind that helps a paper get accepted.
The kinds of assistance we are concerned
with here go beyond that which is provided by
commonly-available spelling checkers and grammar
checkers such as those found in Microsoft Word
(Heidorn, 2000). The task can be simply expressed
as a text-to-text generation exercise:
1See http://acl2010.org/mentoring.htm.
Given a text, make edits to the text to im-
prove the quality of the English it con-
tains.
This simple characterisation masks a number of
questions that must be answered in order to fully
specify a task. We turn to these questions in Sec-
tion 3, after first elaborating on why we think this
task is likely to deliver useful results.
2 Why This Will Work
2.1 Potential Users
We believe this initiative has a strong chance of suc-
ceeding simply because there will be an abundance
of committed, serious and well-informed users to
give feedback on proposed solutions. A famil-
iar problem for technological developments in aca-
demic research is that of capturing the time and in-
terest of potential users of the technology, to obtain
feedback about what works in a real world task set-
ting, with an appropriate level of engagement.
It is very important to NNS researchers that their
papers are not rejected because the English is not
good or clear enough. They expect to invest large
amounts of time in honing the linguistic aspects of
their papers. One of us vividly recalls an explana-
tion by a researcher that, prior to submitting a pa-
per, he took his draft and submitted each sentence
in turn, in quotation marks (to force exact matches
only), to Google. If there were no Google hits, it
was unlikely that the sentence was satisfactory En-
glish and it needed reworking; if there were hits, the
hits needed checking to ascertain whether they ap-
peared to be written by another non-native speaker.2
To give that researcher a tool that improves on this
situation should not be too great a challenge.
For HOO, we envisage that the researchers them-
selves, as well as their colleagues, will want to use
the prototype systems when preparing their confer-
ence and journal submissions. They will have the
skills and motivation to integrate the use of proto-
types into their paper-writing.
2See the Microsoft ESL Assistant at
http://www.eslassistant.com as an embodiment of
a similar idea.
2.2 The ACL Anthology
Over a number of years, the ACL has sponsored
the ongoing development of the ACL Anthology, a
large collection of papers in the domain of computa-
tional linguistics. This provides an excellent source
for the construction of language models for the task
described here. The more recently-prepared ACL
Anthology Reference Corpus (Bird et al, 2008), in
which 10,921 of the Anthology texts (around 40 mil-
lion words) have been made available in plain text
form, has also been made accessible via the Sketch
Engine, a leading corpus query tool.3
The corpus is not perfect, of course: not every-
thing in the ACL Anthology is written in flawless
English; the ARC was prepared in 2007, so new top-
ics, vocabulary and ideas in CL will not be repre-
sented; and the fact that the texts have been auto-
matically extracted from PDF files means that there
are errors from the conversion process.
3 The Task in More Detail
3.1 How Do We Measure Quality?
To be able to evaluate the performance of systems
which attempt to improve the quality of a text,
we require some means of measuring text quality.
One approach would be to develop measures, or
make use of existing measures, of characteristics
of text quality such as well-formedness and read-
ability (see, for example, (Dale and Chall, 1948;
Flesch, 1948; McLaughlin, 1969; Coleman and
Liau, 1975)). Given a text and a version of that text
that had been subjected to rewriting, we could then
compare both texts using these metrics. However,
there is always a concern that the metrics may not re-
ally measure what they are intended to measure (see,
for example, (Le Vie Jr, 2000)); readability metrics
have often been criticised for not being good mea-
sures of actual readability. The measures also tend
to be aggregate measures (for example, providing an
average readability level across an entire text), when
the kinds of changes that we are interested in evalu-
ating are often very local in nature.
Given these concerns, we opt for a different route:
for the initial pilot run of the proposed task, we in-
tend to provide a set of development data consisting
3See http://sketchengine.co.uk/open.
of 10 conference papers in two versions: an original
version of the paper, and an improved version where
errors in expression and language use have been cor-
rected. We envisage that participants will focus on
developing techniques that attempt to replicate the
kinds of corrections found in the improved versions
of the papers. For evaluation, we will provide a fur-
ther ten papers in their original versions, and each
participant?s results will then be compared against a
held-back set of corrected versions for these papers.
We would expect the evaluation to assess the follow-
ing:
? Has the existence of each error annotated in the
manually revised versions been correctly iden-
tified?
? Have the spans or extents of the errors been ac-
curately identified?
? Has the type of error, as marked in the annota-
tions, been correctly identified?
? How close is the automatically-produced cor-
rection to the manually-produced correction?
? What corrections are proposed that do not cor-
respond to errors identified in the manually-
corrected text?
With respect to this last point: we anticipate looking
closely at all such machine-proposed-errors, since
some may indeed be legitimate. Either the human
annotators may have missed them, or may not have
considered them significant enough to be marked. If
there are many such cases, we will need to review
how we handle ?prima facie false positives? in the
evaluation metrics.
Evaluation of the aspects described above can
be achieved automatically; there is also scope, of
course, for human evaluation of the overall relative
quality of the system-generated texts, although this
is of course labour intensive.
3.2 Where Does the Source Data Come From?
We have two candidates which we aim to explore
as sources of data for the exercise. It is almost cer-
tain the first of these two options will yield mate-
rial which is denser in errors, and closer to the kinds
of source material that any practical application will
have to work with; however, the pragmatics of the
situation mean that we may have to fall back on our
second option.
First, we intend to approach the Mentoring Chairs
for the ACL conferences over the last few years with
our proposal; then, with their permission, we ap-
proach the authors of papers that were submitted for
mentoring. If these authors are willing, we use their
initial submissions to the mentoring process as the
original document set.
If this approach yields an insufficient number of
papers (it may be that some authors are not willing
to have their drafts made available in this way, and
it would not be possible to make them anonymous)
then we will source candidate papers from the ACL
Anthology. The process we have in mind is this:
? Identify a paper whose authors are non-native
English speakers.
? If a quick reading of the paper reveals a mod-
erately high density of correctable errors with
in the first page, that paper becomes a candi-
date for the data set; if it contains very few cor-
rectable errors, the paper is ruled as inappropri-
ate.
? Repeat this process until we have a sufficiently
large data set.
We then contact the authors to determine whether
they are happy for their papers to be used in this ex-
ercise. If they are not, the paper is dropped and the
next paper?s author is asked.
3.3 Where do the Corrections Come From?
For the initial pilot, two copy-editors (who may or
may not be the authors of this paper) hand-correct
the papers in both the development and evaluation
data sets. For a full-size exercise there should be
more than two such annotators, just as there should
be more than ten papers in each of the development
and evaluation sets, but our priority here is to test the
model before investing further in it.
The copy-editors will then compare corrections,
and discuss differences. The possible cases are:
1. One annotator identifies a correction that the
other does not.
2. Both annotators identify different corrections
for the same input text fragment.
We propose to deal with instances of the first type as
follows:
? The two annotators will confer to determine
whether one has simply made a mistake?as
many authors can testify, no proofreader will
find all the errors in a text.
? If agreement on the presence or absence of an
error cannot be reached, the instance will be
dealt with as described below for cases of the
second type, with absence of an error being
considered a ?null correction?.
Instances of the second type will be handled as fol-
lows:
? If both annotators agree that both alternatives
are acceptable, then both alternatives will be
provided in the gold standard.
? If no agreement can be reached, then neither
alternative will be provided in the gold standard
(which effectively means that a null correction
is recorded).
Other strategies, such as using a third annotator as
a tie-breaker, can be utilised if the task generates a
critical mass of interest and volunteer labour.
3.4 What Kinds of Corrections?
Papers can go through very significant changes and
revisions during the course of their production: large
portions of the material can be added or removed,
the macro-structure can be re-organised substan-
tially, arguments can be refined or recast. Ideally, a
writing advisor might help with large-scale concerns
such as these; however, we aim to start at a much
simpler level, focussing on what is sometimes re-
ferred to as a ?light copy-edit?. This involves a range
of phenomena which can be considered sentence-
internal:
? domain- and genre-specific spelling errors, in-
cluding casing errors;
? dispreferred or suboptimal lexical choices;
? basic grammatical errors, including common
ESL problems like incorrect preposition and
determiner usage;
? reduction of syntactic complexity;
? stylistic infelicities which, while not grammati-
cally incorrect, are unwieldy and impact on flu-
ency and ease of reading.
The above are all identifiable and correctable within
the context of a single sentence; however, we also in-
tend to correct inconsistencies across the document
as whole:
? consistency of appropriate tense usage;
? spelling and hyphenation instances where there
is no obvious correct answer, but a uniformity
is required.
We envisage that the process of marking up the gold-
standard texts will allow us to develop more formal
guidelines and taxonomic descriptions for use sub-
sequent to the pilot exercise. There are, of course,
existing approaches to error markup that can pro-
vide a starting point here, in particular the schemes
used in the large-scale exercises in learner error
annotation undertaken at CECL, Louvain-la-Neuve
(Dagneaux et al, 1996) and at Cambridge ESOL
(Nicholls, 2003).
3.5 How Should the Task be Approached?
There are many ways in which the task could be ad-
dressed; it is open to both rule-based and statistical
solutions. An obvious way to view the task is as a
machine translation problem from poor English to
better English; however, supervised machine learn-
ing approaches may be ruled out by the absence of
an appropriately large training corpus, something we
may not see until the task has generated significant
momentum (or more volunteer annotators at an early
stage!).
There is clearly a wealth of existing research on
grammar and style checking that can be brought
to bear. Although grammar and style checking
has been in the commercial domain now for three
decades, the task may provide a framework for the
first comparative test of many of these applications.
Because the nature of errors is so diverse, this
task offers the opportunity to exercise a broad range
of approaches to the problem, and also allows for
narrowly-focussed solutions that attempt to address
specific problems with high accuracy.
4 Some Potential Problems
Our proposal is not without possible problems and
detrimental side effects.
Clearly there are ethical issues that need to be
considered carefully; even if an author is happy for
their data to be used in this way, one might find ret-
rospective embarrassment at eponynmous error de-
scriptions entering the common vocabulary in the
field?it?s one thing to be acknowledged for Kneser-
Ney smoothing, but perhaps less appealing to be fa-
mous for the Dale-Kilgarriff adjunct error.
Our suggestion that the ACL Anthology might be
used as a source for language modelling brings its
own downsides: in particular, if anything is likely
to increase the oft-complained-about sameness of
CL papers, this will! There is also an ethical is-
sue around the fine line between what such systems
will do and plagiarism; one might foresee the advent
of a new scholastic crime labelled ?machine-assisted
style plagiarism?.
There are no doubt other issues we have not yet
considered; again, feedback on potential pitfalls is
eagerly sought.
5 Next Steps
Our aim is to obtain feedback on this proposal from
conference participants and others, with the aim of
refining our plan in the coming months. If we sense
that there is a reasonable degree of interest in the
task, we would aim to publish the initial data set well
before the end of the year, with a first evaluation tak-
ing place in 2011.
In the name of better writing, CLers of the world
unite?you have nothing to lose but your worst sen-
tences!
Acknowledgements
We thank the two anonymous reviewers for useful
feedback on this proposal, and Anja Belz for encour-
aging us to develop the idea.
References
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008. The
acl anthology reference corpus: A reference dataset for
bibliographic research in computational linguistics. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC 2008), location = Marrakesh,
Morocco.
Meri Coleman and T. L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Journal
of Applied Psychology, 60:283?284.
E Dagneaux, S Denness, S Granger, and F Meunier.
1996. Error tagging manual version 1.1. Technical
report, Centre for English Corpus Linguistics, Univer-
site? Catholique de Louvain.
Edgar Dale and Jeanne S. Chall. 1948. A formula for
predicting readability. Educational research bulletin,
27:11?20.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of Applied Psychology, 32:221?233.
George Heidorn. 2000. Intelligent writing assistance. In
R Dale, H Moisl, and H Somers, editors, Handbook of
Natural Language Processing, pages 181?207. Marcel
Dekker Inc.
Donald S. Le Vie Jr. 2000. Documentation metrics:
What do you really want to measure? Intercom.
G. Harry McLaughlin. 1969. SMOG grading ? a new
readability formula. Journal of Reading, pages 639?
646.
D Nicholls. 2003. The cambridge learner corpus: error
coding and analysis for lexicography and ELT. In Pro-
ceedings of the Corpus Linguistics 2003 Conference
(CL 2003), page 572.
J R Tetreault and M S Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics.
