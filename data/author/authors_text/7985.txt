Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 600?609, Prague, June 2007. c?2007 Association for Computational Linguistics
Japanese Dependency Analysis Using the Ancestor-Descendant Relation
Akihiro Tamura?? Hiroya Takamura?? Manabu Okumura??
? Common Platform Software Research Laboratories NEC Corporation
a-tamura@ah.jp.nec.com
?? Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan
{takamura,oku}@pi.titech.ac.jp
Abstract
We propose a novel method for Japanese de-
pendency analysis, which is usually reduced
to the construction of a dependency tree. In
deterministic approaches to this task, depen-
dency trees are constructed by series of ac-
tions of attaching a bunsetsu chunk to one of
the nodes in the tree being constructed. Con-
ventional techniques select the node based
on whether the new bunsetsu chunk and each
node in the trees are in a parent-child rela-
tion or not. However, tree structures include
relations between two nodes other than the
parent-child relation. Therefore, we use
ancestor-descendant relations in addition to
parent-child relations, so that the added re-
dundancy helps errors be corrected. Ex-
perimental results show that the proposed
method achieves higher accuracy.
1 Introduction
Japanese dependency analysis has been recognized
as one of the basic techniques in Japanese process-
ing. A number of techniques have been proposed
for years. Japanese dependency is usually repre-
sented by the relation between phrasal units called
?bunsetsu? chunks, which are the smallest meaning-
ful sequences consisting of an independent word and
accompanying words (e.g., a noun and a particle).
Hereafter, a ?chunk? means a bunsetsu chunk in this
paper. The relation between two chunks has a di-
?Akihiro Tamura belonged to Tokyo Institute of Technology
when this work was done.
Figure 1: Example of a dependency tree
rection from the modifier to the modifiee. All de-
pendencies in a sentence are represented by a de-
pendency tree, where a node indicates a chunk, and
node B is the parent of node A when chunk B is the
modifiee of chunk A. Figure 1 shows an example of
a dependency tree. The task of Japanese dependency
analysis is to find the modifiee for each chunk in a
sentence. The task is usually regarded as construc-
tion of a dependency tree.
In primitive approaches, the probabilities of de-
pendencies are given by manually constructed rules
and the modifiee of each chunk is determined. How-
ever, those rule-based approaches have problems in
coverage and consistency. Therefore, a number of
statistical techniques using machine learning algo-
rithms have recently been proposed. In most con-
ventional statistical techniques, the probabilities of
dependencies between two chunks are learned in the
learning phase, and then the modifiee of each chunk
is determined using the learned models in the anal-
ysis phase. In terms of dependency trees, the parent
node of each node is determined based on the likeli-
ness of parent-child relations between two nodes.
We here take notice of the characteristics of de-
pendencies which cannot be captured well only by
600
the parent-child relation. Consider, for example,
Figure 1. In Figure 1, ID 3(pizza-and) and ID
4(salad-accusative) are in a parallel structure. In the
structure, node 4 is a child of node 5(ate), but node
3 is not a child of 5, although 3 and 4 are both foods
and should share a tendency of being subcategorized
by the verb ?eat?. A number of conventional models
use the pair of 3(pizza-and) and 5(ate) as a nega-
tive instance because 3 does not modify 5. Conse-
quently, those models cannot learn and use the sub-
categorization preference of verbs well in the paral-
lel structures.
We focus on ancestor-descendant relations to
compensate for the weakness. Two nodes are in the
ancestor-descendant relation when one of the two
nodes is included in the path from the root node to
the other node. The upper node of the two nodes
is called an ?ancestor node? and the lower node a
?descendant node?. When the ancestor-descendant
relation is used, both of the above two instances
for nodes 3 and 4 can be considered as positive in-
stances. Therefore, it is expected that the ancestor-
descendant relation helps the algorithm capture the
characteristics that cannot be captured well by the
parent-child relation.
We aim to improve the performance of Japanese
dependency analysis by taking the ancestor-
descendant relation into account. In exploiting
ancestor-descendant information, it came to us that
redundant information is effectively utilized in a
coding problem in communications (Mackay, 2003).
Therefore, we propose a method in which the prob-
lem of determining the modifiee of a chunk is re-
garded as a kind of a coding problem: dependency is
expressed as a sequence of values, each of which de-
notes whether a parent-child relation or an ancestor-
descendant relation holds between two chunks.
In Section 2, we present the related work. In Sec-
tion 3, we explain our method. In Section 4, we de-
scribe our experiments and their results, where we
show the effectiveness of the proposed method. In
Section 5, we discuss the results of the experiments.
Finally, we describe the summary of this paper and
the future work in Section 6.
2 Conventional Statistical Methods for
Japanese Dependency Analysis
First, we describe general formulation of the
probability model for dependency analysis. We
denote a sequence of chunks, ?b1, b2, ..., bm?,
by B, and a sequence of dependency pat-
terns, ?Dep(1), Dep(2), ..., Dep(m)?, by D, where
Dep(i) = j means that bi modifies bj . Given the se-
quence B of chunks as an input, dependency analy-
sis is defined as the problem of finding the sequence
D of the dependency patterns that maximizes the
conditional probability P (D | B). A number of
the conventional methods assume that dependency
probabilities are independent of each other and ap-
proximate P (D | B) with
?m?1
i=1 P (Dep(i) | B).
P (Dep(i) | B) is estimated using machine learn-
ing algorithms. For example, Haruno et al (1999)
used Decision Trees, Sekine (2000) used Maximum
Entropy Models, Kudo and Matsumoto (2000) used
Support Vector Machines.
Another notable method is Cascaded Chunking
Model by Kudo and Matsumoto (2002). In their
model, a sentence is parsed by series of the fol-
lowing processes: whether or not the current chunk
modifies the following chunk is estimated, and if it
is so, the two chunks are merged together. Sassano
(2004) parsed a sentence efficiently using a stack.
The stack controls the modifier being analyzed.
These conventional methods determine the mod-
ifiee of each chunk based on the likeliness of de-
pendencies between two chunks (in terms of depen-
dency tree, the likeliness of parent-child relations
between two nodes). The difference between the
conventional methods and the proposed method is
that the proposed method determines the modifiees
based on the likeliness of ancestor-descendant re-
lations in addition to parent-child relations, while
the conventional methods tried to capture charac-
teristics that cannot be captured by parent-child re-
lations, by adding ad-hoc features such as features
of ?the chunk modified by the candidate modifiee?
to features of the candidate modifiee and the mod-
ifier. However, these methods do not deal with
ancestor-descendant relations between two chunks
directly, while our method uses that information di-
rectly. In Section 5, we empirically show that our
method uses the ancestor-descendant relation more
601
effectively than the conventional ones and explain
that our method is justifiable in terms of a coding
problem.
3 Proposed Method
The methods explained in this section construct a
dependency tree by series of actions of attaching
a node to one of the nodes in the trees being con-
structed. Hence, when the parent node of a certain
node is being determined, it is required that the par-
ent node should already be included in the tree being
constructed. To satisfy the requirement, we note the
characteristic of Japanese dependencies: dependen-
cies are directed from left to right. (i.e., the par-
ent node is closer to the end of a sentence than its
child node). Therefore, our methods analyze a sen-
tence backwards as in Sekine (2000) and Kudo and
Matsumoto (2000). Consider, for example, Figure
1. First, our methods determine the parent node of
ID 4(salad-accusative), and then that of ID 3(pizza-
and) is determined. Next, the parent node of ID 2(at
lunchtime), and finally, that of ID 1(he-nominative)
is determined and dependencies in a sentence are
identified. Please note that our methods are applica-
ble only to dependency structures of languages that
have a consistent head-direction like Japanese.
We explain three methods that are different in
the information used in determining the modifiee of
each chunk. In Section 3.1, we explain PARENT
METHOD and ANCESTOR METHOD, which de-
termine the modifiee of each chunk based on the
likeliness of only one type of the relation. PARENT
METHOD uses the parent-child relation, which is
used in conventional Japanese dependency analy-
sis. ANCESTOR METHOD is novel in that it
uses the ancestor-descendant relation which has not
been used in the existing methods. In Section
3.2, we explain our method, PARENT-ANCESTOR
METHOD, which determines the modifiees based
on the likeliness of both ancestor-descendant and
parent-child relations.
When the modifiee is determined using the
ancestor-descendant relation, it is necessary to take
into account the relations with every node in the tree.
Consider, for example, the case that the modifiee
of ID 1(he-nominative) is determined in Figure 1.
When using the parent-child relation, the modifiee
can be determined based only on the relation be-
tween ID 1 and 5. On the other hand, when using the
ancestor-descendant relation, the modifiee cannot be
determined based only on the relation between ID
1 and 5. This is because if one of ID 2, 3 and 4
is the modifiee of ID 1, the relation between ID 1
and 5 is ancestor-descendant. ID 5 is determined
as the modifiee of ID 1 only after the relations with
each node of ID 2, 3 and 4 are recognized not to
be ancestor-descendant. An elegant way to use the
ancestor-descendant relation, which we propose in
this paper, is to represent a dependency as a code-
word where each bit indicates the relation with a
node in the tree, and determine the modifiee based
on the relations with every node in the tree (for de-
tails to the next section).
3.1 Methods with a single relation: PARENT
METHOD and ANCESTOR METHOD
Figure 2 shows the pseudo code of the algo-
rithm to construct a dependency tree using PAR-
ENT METHOD or ANCESTOR METHOD. As
mentioned above, the two methods analyze a sen-
tence backwards. We should note that node1 to
noden in the algorithm respectively correspond to
the last chunk to the first chunk of a sentence.
MODEL PARENT(nodei,nodej) indicates the pre-
diction whether nodej is the parent of nodei or
not, which is the output of the learned model.
MODEL ANCESTOR(nodei,nodej) indicates the
prediction whether nodej is the ancestor of nodei or
not. String output indicates the sequence of the i?
1 predictions stored in step 3. The codeword denoted
by string[k] is the binary sequence given to the ac-
tion that nodei is attached to nodek. Parent[nodei]
indicates the node to which nodei is attached, and
Dis indicates a distance function. Thus, our method
predicts the correct actions by measuring the dis-
tance between the codeword string[k] and the pre-
dicted binary (later extended to real-valued) se-
quences string output. In other words, our method
selects the action that is the closest to the outputs of
the learned model.
Both models are learned from dependency trees
given as training data as shown in Figure 3. Each
relation is learned from ordered pairs of two nodes
in the trees. However, our algorithm in Figure 2
targets at dependencies directed from left to right.
602
1:for i = 1, 2, ..., n do
2: for j = 1, 2, ..., i ? 1 do
3: result parent[j]=MODEL PARENT(nodei,nodej)(in case of PARENT and PARENT-ANCESTOR METHOD)
3: result ancestor[j]=MODEL ANCESTOR(nodei,nodej)(in case of ANCESTOR and PARENT-ANCESTOR METHOD)
4: end
5: Parent[nodei]=argmink Dis(string[k], string output)
6:end
Figure 2: Pseudo code of PARENT, ANCESTOR,
and PARENT-ANCESTOR METHODS
Figure 3: Example of training instances
Therefore, the instances with a right-to-left depen-
dency are excluded from the training data. For ex-
ample, the instance with node4 being the candi-
date parent (or ancestor) of node1 is excluded in
Figure 3. MODEL PARENT uses ordered pairs
of a parent node and a child node as positive in-
stances and the other ordered pairs as negative in-
stances. MODEL ANCESTOR uses ordered pairs
of an ancestor node and a descendant node as
positive instances and the other ordered pairs as
negative instances. From the above description
and Figure 3, the number of training instances
used in learning MODEL PARENT is the same
as the number of training instances used in learn-
ing MODEL ANCESTOR. However, the number of
positive instances in learning MODEL ANCESTOR
is larger than in learning MODEL PARENT be-
cause the set of parent-child relations is a subset of
ancestor-descendant relations.
As mentioned above, the two methods analyze a
sentence backwards. We should note that node1 to
noden in the algorithm respectively correspond to
the last chunk to the first chunk of a sentence.
Next, we illustrate the process of determining the
parent node of a certain node nodem(with Figures 4
and 5). Hereafter, nodem is called a target node.
The parent node is determined based on the like-
liness of a relation; the parent-child and ancestor-
descendant relation are used in PARENT METHOD
and ANCESTOR METHOD respectively.
Our methods regard a dependency between the
target node and its parent node as a set of relations
between the target node and each node in the tree.
Each relation corresponds to one bit, which becomes
1 if the relation holds, ?1 otherwise. For example,
a sequence (?1,?1,?1, 1) represents that the par-
ent of node5 is node4 in PARENT METHOD (Fig-
ure 4), since the relation holds only between nodes
4 and 5.
First, the learned model judges whether the tar-
get node and each node in the current tree are in
a certain relation or not; PARENT METHOD uses
MODEL PARENT as the learned model and AN-
CESTOR METHOD uses MODEL ANCESTOR.
The sequence of the m?1 predictions by the learned
model is stored in string output.
The codeword string[k] is the binary (?1 or 1)
sequence that is to be output when the target node
is attached to the nodek. In Figures 4 and 5, the
set of string[k] (for node5) is in the dashed square.
For example, string[2] in ANCESTOR METHOD
(Figure 5) is (1, 1,?1,?1) since nodes 1 and 2 are
the ancestor of node5 if node5 is attached to node2.
Next, among the set of string[k], the codeword
that is the closest to the string output is selected.
The target node is then attached to the node cor-
responding to the selected codeword. In Figure 4,
the string[4], (?1,?1,?1, 1), is selected and then
node5 is attached to node4.
Japanese dependencies have the non-crossing
constraint: dependencies do not cross one another.
To satisfy the constraint, we remove the nodes that
will break the non-crossing constraint from the can-
didates of a parent node in step 5 of the algorithm.
PARENT METHOD differs from conventional
methods such as Sekine (2000) or Kudo and Mat-
sumoto (2000), in the process of determining the
parent node. These conventional methods select the
node given by argmaxjP (nodej | nodei) as the
parent node of nodei, setting the beam width to 1.
However, their processes are essentially the same as
the process in PARENT METHOD.
603
Figure 4: Analysis example using PARENT
METHOD
Figure 5: Analysis example using ANCESTOR
METHOD
3.2 Proposed method: PARENT-ANCESTOR
METHOD
The proposed method determines the parent node of
a target node based on the likeliness of ancestor-
descendant relations in addition to parent-child
relations. The use of ancestor-descendant rela-
tions makes it possible to capture the character-
istics which cannot be captured by parent-child
relations alone. The pseudo code of the pro-
posed method, PARENT-ANCESTOR METHOD,
is shown in Figure 2. MODEL PARENT and
MODEL ANCESTOR are learned as described in
Section 3.1. String output is the concatenation
of the predictions by both MODEL PARENT and
MODEL ANCESTOR. In addition, string[k] is
provided based not only on parent-child relations but
also on ancestor-descendant relations. An analysis
example using PARENT-ANCESTOR METHOD is
shown in Figure 6.
Figure 6: Analysis example using PARENT-
ANCESTOR METHOD
4 Experiment
4.1 Experimental settings
We used Kyoto University text corpus (Version
2.0) (Kurohashi and Nagao, 1997) for training and
test data. The articles on January 1st through 8th
(7,958 sentences) were used as training data, and the
articles on January 9th (1,246 sentences) as test data.
The dataset is the same as in leading works (Sekine,
2000; Kudo and Matsumoto, 2000; Kudo and Mat-
sumoto, 2002; Sassano, 2004).
We used SVMs as the algorithm of learning and
analyzing the relations between nodes. We used the
third degree polynomial kernel function and set the
soft margin parameter C to 1, which is exactly the
same setting as in Kudo and Matsumoto (2002). We
can obtain the real-valued score in step 3 of the al-
gorithm, which is the output of the separating func-
tion. The score can be regarded as likeliness of the
two nodes being in the parent-child (or the ancestor-
descendant). Therefore, we used the sequence of
the outputs of SVMs as string output, instead of
converting the scores into binary values indicating
whether a certain relation holds or not.
Two feature sets are used: static features and dy-
namic features. The static features used in the ex-
periments are shown in Table 1. The features are the
same as those used in Kudo and Matsumoto (2002).
In Table 1, HeadWord means the rightmost con-
tent word in the chunk whose part-of-speech is not
a functional category. FunctionalWord means the
604
Table 1: Static features used in experiments
Head Word (surface-form, POS, POS-subcategory,
inflection-type, inflection-form), Functional Word (
Modifier / surface-form, POS, POS-subcategory, inflection-type,
Modifiee inflection-form), brackets, quotation-marks,
punctuation-marks, position in sentence (beginning, end)
Between two distance (1,2-5,6-), case-particles, brackets,
chunks quotation-marks, punctuation-parks
Figure 7: Dynamic features
rightmost functional word or the inflectional form of
the rightmost predicate if there is no functional word
in the chunk.
Next, we explain the dynamic features used in
the experiments. Three types of dynamic features
were used in Kudo and Matsumoto (2002): (A)
the chunks modifying the current candidate modi-
fiee, (B) the chunk modified by the current candidate
modifiee, and (C) the chunks modifying the current
candidate modifier. The type C is not available in the
proposed method because the proposed method an-
alyzes a sentence backwards unlike Kudo and Mat-
sumoto (2002). Therefore, we did not use the type
C. We used the type A? and B? which are recursive
expansion of type A and B as the dynamic features
(Figure 7). The form of functional words or inflec-
tion was used as a type A? feature and POS and POS-
subcategory of HeadWord as a type B? feature.
4.2 Experimental results
In this section, we show the effectiveness of the pro-
posed method. First, we compare the three methods
described in Section 3: PARENT METHOD, AN-
CESTOR METHOD, and PARENT-ANCESTOR
METHOD. The results are shown in Table 2. Here,
dependency accuracy is the percentage of correct
dependencies (correct parent-child relations in trees
in test data), and sentence accuracy is the percent-
age of the sentences in which all the modifiees are
determined correctly (correctly constructed trees in
test data).
Table 2 shows that PARENT-ANCESTOR
METHOD is more accurate than the other two
Table 2: Result of dependency analysis using meth-
ods described in Section 3
Method Dependency SentenceAccuracy Accuracy
PARENT 88.95% 44.87%
ANCESTOR 87.64% 43.74%
PARENT-ANCESTOR 89.54% 47.38%
Table 3: Comparison to conventional methods
Feature Method Dependency SentenceAccuracy Accuracy
Only Proposed method 88.88% 46.33%
static Kudo and Matsumoto (2002) 88.71% 45.19%
Static + Proposed method 89.43% 47.94%
Dynamic A,B Kudo and Matsumoto (2002) 89.19% 46.64%
Original
Proposed method 89.54% 47.38%
Sekine (2000) 87.20% 40.76%
Kudo and Matsumoto (2000) 89.09% 46.17%
Kudo and Matsumoto (2002) 89.29% 47.53%
Sassano (2004) 89.56% 48.35%
w/o Rich Sassano (2004) 89.19% 47.05%w/o Conj 89.41% 47.86%
methods. In other words, the accuracy of depen-
dency analysis improves by utilizing the redundant
information. The improvement is statistically sig-
nificant in the sign-test with 1% significance-level.
Next, we compare the proposed method with
conventional methods. We compare the proposed
method particularly with Kudo and Matsumoto
(2002) with the same feature set. The reasons are
that Cascaded Chunking Model proposed in Kudo
and Matsumoto (2002) is used in a popular Japanese
dependency analyzer, CaboCha 1, and the compari-
son can highlight the effectiveness of our approach
because we can experiment under the same condi-
tions (e.g., dataset, feature set, learning algorithm).
A summary of the comparison is shown in Table 3.
Table 3 shows that the proposed method
outperforms conventional methods except Sas-
sano (2004)2, while Sassano (2004) used richer fea-
tures which are not used in the proposed method,
such as features for conjunctive structures based on
Kurohashi and Nagao (1994), features concerning
the leftmost content word in the candidate modi-
fiee. The comparison of the proposed method with
Sassano (2004)?s method without the features of
1http://chasen.org/?taku/software/
cabocha/
2We have not tested the improvement statistically because
we do not have access to the conventional methods.
605
Table 4: Accuracy of dependency analysis on paral-
lel structures
Parallel structures Other thanparallel structures
PARENT 74.18% 91.21%
ANCESTOR 73.24% 90.01%
PARENT-ANCESTOR 76.29% 91.63%
conjunctive structures (w/o Conj) and without the
richer features derived from the words in chunks
(w/o Rich) suggests that the proposed method is bet-
ter than or comparable to Sassano (2004)?s method.
5 Discussion
5.1 Performance on parallel structures
As mentioned in Section 1, the ancestor-descendant
relation is supposed to help to capture parallel struc-
tures. In this section, we discuss the performance of
dependency analysis on parallel structures. Parallel
structures such as those of nouns (e.g., Tom and Ken
eat hamburgers.) and those of verbs (e.g., Tom eats
hamburgers and drinks water.), are marked in Kyoto
University text corpus. We investigate the accuracy
of dependency analysis on parallel structures using
the information.
Table 4 shows that the accuracy on parallel struc-
tures improves by adding the ancestor-descendant
relation. The improvement is statistically significant
in the sign-test with 1% significance-level. Table 4
also shows that error reduction rate on parallel struc-
tures by adding the ancestor-descendant relation is
8.3% and the rate on the others is 4.7%. These show
that the ancestor-descendant relation work well es-
pecially for parallel structures.
In Table 4, the accuracy on parallel structures
using PARENT METHOD is slightly better than
that using ANCESTOR METHOD, while the dif-
ference is not statistically significant in the sign-
test. It shows that the parent-child relation is also
necessary for capturing the characteristics of paral-
lel structures. Consider the following two instances
in Figure 1 as an example: the ordered pair of ID
3(pizza-and) and ID 5(ate), and the ordered pair of
ID 4(salad-accusative) and ID 5. In ANCESTOR
METHOD, both instances are positive instances. On
the other hand, only the ordered pair of ID 4 and
ID 5 is a positive instance in PARENT METHOD.
Table 5: Comparison between usages of the
ancestor-descendant relation
Dependency Sentence
Accuracy Accuracy
Feature 88.57% 44.71%
Model 88.88% 46.33%
Hence, PARENT METHOD can learn appropriate
case-particles in a modifier of a verb. For exam-
ple, the particle which means ?and? does not mod-
ify verbs. However, it is difficult for ANCESTOR
METHOD to learn the characteristic. Therefore,
both parent-child and ancestor-descendant relations
are necessary for capturing parallel structures.
5.2 Discussion on usages of the
ancestor-descendant relation
In the proposed method, MODEL ANCESTOR,
which judges whether the relation between two
nodes is ancestor-descendant or not, is prepared,
and the information on the ancestor-descendant re-
lation is directly utilized. On the other hand,
conventional methods add the features regarding
the ancestor or descendant chunk to capture the
ancestor-descendant relation. In this section, we
empirically show that the proposed method utilizes
the information on the ancestor-descendant rela-
tion more effectively than conventional methods.
The results in the previous sections could not show
the effectiveness because MODEL PARENT and
MODEL ANCESTOR in the proposed method use
the features regarding the ancestor-descendant rela-
tion.
Table 5 shows the result of dependency analy-
sis using two types of usages of the information
on the ancestor-descendant relation. ?Feature? indi-
cates the conventional usage and ?Model? indicates
our usage. Please note that MODEL PARENT and
MODEL ANCESTOR used in ?Model? do not use
the features regarding the ancestor-descendant rela-
tion. Table 5 shows that our usage is more effec-
tive than the conventional usage. This is because
our usage takes advantage of redundancy in terms
of a coding problem as described in the next sec-
tion. Moreover, the learned features through the pro-
posed method would include more information than
606
ad-hoc features that were manually added.
5.3 Proposed method in terms of a coding
problem
In a coding problem, redundancy is effectively uti-
lized so that information can be transmitted more
properly (Mackay, 2003). This idea is the same as
the main point of the proposed method. In this sec-
tion, we discuss the proposed method in terms of a
coding problem.
In a coding problem, when encoding information,
the redundant bits are attached so that the added re-
dundancy helps errors be corrected. Moreover, the
following fact is known (Mackay, 2003):
the error-correcting ability is higher when the dis-
tances between the codewords are longer. (1)
For example, consider the following three types
of encodings: (A) two events are encoded respec-
tively into the codewords ?1 and 1 (the simplest
encoding), (B) into the codewords (?1,?1, 1) and
(1, 1, 1) (hamming distance:2), and (C) into the
codewords (?1,?1,?1) and (1, 1, 1) (hamming
distance:3). Please note that the hamming distance is
defined as the number of bits that differ between two
codewords. In (A), the correct information is not
transmitted if a one-bit error occurs. In (B), if an er-
ror occurs in the third bit, the error can be corrected
by assuming that the original codeword is closest
to the received codeword. In (C), any one-bit error
can be corrected. Thus, (B) has the higher error-
correcting ability than (A), and (C) has the higher
error-correcting ability than (B).
We explain the problem of determining the par-
ent node of a target node in the proposed method in
terms of the coding theory. A sequence of numbers
corresponds to a codeword. It is assumed that the
codeword which expresses the correct parent node
of the target node is transmitted. The codeword is
transmitted through the learned model through chan-
nels to the receiver. The receiver infers the parent
node from the received sequence (string output) in
consideration of the codewords that can be transmit-
ted (string[k]). Therefore, error-correcting ability,
the ability of correcting the errors in predictions in
step 3, is dependent on the distances between the
codewords (string[k]).
The codewords in PARENT-ANCESTOR
METHOD are the concatenation of the bits based on
both parent-child relations and ancestor-descendant
relations. Consequently, the distances between
codewords in PARENT-ANCESTOR METHOD are
longer than those in PARENT METHOD or AN-
CESTOR METHOD. From (1), the error-correcting
ability is expected to be higher. In terms of a coding
problem, the proposed method exploits the essence
of (1), and utilizes ancestor-descendant relations
effectively.
We assume that every bit added as redundancy is
correctly transmitted for the above-mentioned dis-
cussion. However, some of these added bits may be
transmitted wrongly in the proposed method. In that
case, the added redundancy may not help errors be
corrected than cause an error. In the experiments of
dependency analysis, the advantage prevails against
the disadvantage because accuracy of each bit of the
codeword is 94.5%, which is high value.
Discussion on applicability of existing codes
A number of approaches use Error Correcting
Output Coding (ECOC) (Dietterich and Bakiri,
1995; Ghani, 2000) for solving multiclass classifica-
tion problems as a coding problem. The approaches
assign a unique n-bit codeword to each class, and
then n classifiers are trained to predict each bit. The
predicted class is the one whose codeword is clos-
est to the codeword produced by the classifiers. The
codewords in these approaches are designed to be
well-separated from one another and have sufficient
error-correcting ability (e.g., BCH code).
However, these existing codewords are not ap-
plicable to the proposed method. In the proposed
method, we have two models respectively derived
from the parent-child and ancestor-descendant rela-
tion, which can be interpreted in terms of both lin-
guistic aspects and tree structures. If we use ECOC,
however, pairs of nodes are divided into positive and
negative instances arbitrarily. Since this division
lacks linguistic or structural meaning, training in-
stances will lose consistency and any proper model
will not be obtained. Moreover, we have to prepare
different models for each stage in tree construction,
because the length of the codewords vary according
to the number of nodes in the current tree.
607
Table 6: Result of dependency analysis using vari-
ous distance functions
Distance Method Dependency SentenceFunction Accuracy Accuracy
Hamming
PARENT(n) 85.05% 35.35%
PARENT(f) 85.48% 39.87%
ANCESTOR(n) 87.54% 43.42%
ANCESTOR(f) 86.97% 43.18%
Proposed method(n) 88.36% 43.74%
Proposed method(f) 88.45% 44.79%
PARENT 88.95% 44.87%
Cosine / ANCESTOR 87.64% 43.74%
Euclidean Proposed method 89.54% 47.38%
Manhattan
PARENT(n) 88.74% 44.63%
PARENT(f) 88.90% 44.79%
ANCESTOR 87.64% 43.74%
Proposed method 89.24% 46.89%
5.4 Influence of distance functions
In this section, we compare the performance of de-
pendency analysis with various distance functions:
hamming distance, euclidean distance, cosine dis-
tance, and manhattan distance. These distance func-
tions between sequences X=?x1 x2 ... xn? and
Y =?y1 y2 ... yn? are defined as follows:
? Ham(X,Y ) =
?n
i=1(1 ? ?(xi, yi)),
? Euc(X,Y ) =
??n
i=1(xi ? yi)2,
? Cos(X,Y ) = 1 ?
?n
i=1 xi?yi??n
i=1 x
2
i
??n
i=1 y
2
i
,
? Man(X,Y ) =
?n
i=1 | xi ? yi |.
In the hamming distance, string output is con-
verted to a binary sequence with their elements be-
ing of ?1 or 1. The cosine distance is equivalent to
the Euclidean distance under the condition that the
absolute value of every component of string[k] is
1.
The results of dependency analysis using these
distance functions are shown in Table 6. In Table
6, ?(n)? means that the nearest chunk in a sentence
is selected as the modifiee in order to break a tie,
which happens when the number of sequences satis-
fying the condition in step 5 is two or more, while
?(f)? means that the furthest chunk is selected. If the
results in case of (n) and (f) are the same, (n) and (f)
are omitted and only one result is shown.
Table 6 shows that the proposed method out-
performs PARENT METHOD and ANCESTOR
METHOD in any distance functions. It means that
the effectiveness of the proposed method does not
depend on distance functions. The result using the
hamming distance is much worse than using the
other distance functions. It means that using the
scores output by SVMs as the likeliness of a certain
relation improves the accuracy. The results of (n)
and (f) in the hamming distance are different. It is
because the hamming distances are always positive
integers and ties are more likely to happen. Table
6 also shows that the result of the cosine or the eu-
clidean distance is better than that of the manhattan
distance.
6 Conclusions
We proposed a novel method for Japanese depen-
dency analysis, which determines the modifiee of
each chunk based on the likeliness not only of
the parent-child relation but also of the ancestor-
descendant relation in a dependency tree. The
ancestor-descendant relation makes it possible to
capture the parallel structures in more depth. In
terms of a coding theory, the proposed method
boosts error-correcting ability by adding the redun-
dant bits based on ancestor-descendant relations and
increasing the distance between two codewords. Ex-
perimental results showed the effectiveness of the
proposed method. In addition, the results showed
that the proposed method outperforms conventional
methods.
Future work includes the following. In this pa-
per, we use the features proposed in Kudo and Mat-
sumoto (2002). By extracting new features that are
more suitable for the ancestor-descendant relation,
we can further improve our method. The features
used by Sassano (2004) are promising as well. We
are also planning to apply the proposed method to
other tasks which need to construct tree structures.
For example, (zero-) anaphora resolution is consid-
ered as a good candidate task for application.
References
Thomas G. Dietterich and Ghulum Bakiri. 1995. Solving
Multiclass Learning Problems via Error-Correcting
Output Codes. Journal of Artificial Intelligence Re-
search, 2:263?286.
Rayid Ghani. 2000. Using Error-Correcting Codes For
608
Text Classification. In Proc. of ICML-2000, pages
303?310.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1999. Using Decision Trees to Construct
a Practical Parser. Machine Learning, 34:131?149.
Taku Kudo and Yuji Matsumoto. 2000. Japanese Depen-
dency Analysis Based on Support Vector Machines. In
Proc. of EMNLP/VLC 2000, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc. of
CoNLL 2002, pages 63?69.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Uni-
versity text corpus project. In Proc. of ANLP, pages
115?118, Japan.
David J. C. Mackay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Manabu Sassano. 2004. Linear-Time Dependency Anal-
ysis for Japanese. In Proc. of COLING 2004, pages
8?14.
Satoshi Sekine. 2000. Japanese dependency analysis us-
ing a deterministic finite state transducer. In Proc. of
COLING 2000, pages 761?767.
609
Latent Variable Models for Semantic Orientations of Phrases
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Japan Society of the Promotion of Science
tinui@lr.pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose models for semantic orienta-
tions of phrases as well as classification
methods based on the models. Although
each phrase consists of multiple words, the
semantic orientation of the phrase is not a
mere sum of the orientations of the com-
ponent words. Some words can invert the
orientation. In order to capture the prop-
erty of such phrases, we introduce latent
variables into the models. Through exper-
iments, we show that the proposed latent
variable models work well in the classifi-
cation of semantic orientations of phrases
and achieved nearly 82% classification ac-
curacy.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial
areas. It can be applied to, for example, a survey
of new products or a questionnaire analysis. Au-
tomatic sentiment analysis enables a fast and com-
prehensive investigation.
The most fundamental step for sentiment anal-
ysis is to acquire the semantic orientations of
words: desirable or undesirable (positive or neg-
ative). For example, the word ?beautiful? is pos-
itive, while the word ?dirty? is negative. Many
researchers have developed several methods for
this purpose and obtained good results (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al, 2004; Takamura
et al, 2005; Kobayashi et al, 2001). One of
the next problems to be solved is to acquire se-
mantic orientations of phrases, or multi-term ex-
pressions. No computational model for semanti-
cally oriented phrases has been proposed so far al-
though some researchers have used techniques de-
veloped for single words. The purpose of this pa-
per is to propose computational models for phrases
with semantic orientations as well as classification
methods based on the models. Indeed the seman-
tic orientations of phrases depend on context just
as the semantic orientations of words do, but we
would like to obtain the most basic orientations of
phrases. We believe that we can use the obtained
basic orientations of phrases for affect analysis of
higher linguistic units such as sentences and doc-
uments.
The semantic orientation of a phrase is not a
mere sum of its component words. Semantic
orientations can emerge out of combinations of
non-oriented words. For example, ?light laptop-
computer? is positively oriented although neither
?light? nor ?laptop-computer? has a positive ori-
entation. Besides, some words can invert the ori-
entation of a neighboring word, such as ?low?
in ?low risk?, where the negative orientation of
?risk? is inverted to a ?positive? by the adjective
?low?. This kind of non-compositional operation
has to be incorporated into the model. We focus
on ?noun+adjective? in this paper, since this type
of phrase contains most of interesting properties
of phrases, such as emergence or inversion of se-
mantic orientations.
In order to capture the properties of semantic
orientations of phrases, we introduce latent vari-
ables into the models, where one random variable
corresponds to nouns and another random vari-
able corresponds to adjectives. The words that
are similar in terms of semantic orientations, such
as ?risk? and ?mortality? (i.e., the positive ori-
entation emerges when they are ?low?), make a
cluster in these models. Our method is language-
201
independent in the sense that it uses only cooccur-
rence data of words and semantic orientations.
2 Related Work
We briefly explain related work from two view-
points: the classification of word pairs and the
identification of semantic orientation.
2.1 Classification of Word Pairs
Torisawa (2001) used a probabilistic model to
identify the appropriate case for a pair of words
constituting a noun and a verb with the case of
the noun-verb pair unknown. Their model is the
same as Probabilistic Latent Semantic Indexing
(PLSI) (Hofmann, 2001), which is a generative
probability model of two random variables. Tori-
sawa?s method is similar to ours in that a latent
variable model is used for word pairs. How-
ever, Torisawa?s objective is different from ours.
In addition, we used not the original PLSI, but
its expanded version, which is more suitable for
this task of semantic orientation classification of
phrases.
Fujita et al (2004) addressed the task of the de-
tection of incorrect case assignment in automat-
ically paraphrased sentences. They reduced the
task to a problem of classifying pairs of a verb
and a noun with a case into correct or incorrect.
They first obtained a latent semantic space with
PLSI and adopted the nearest-neighbors method,
in which they used latent variables as features. Fu-
jita et al?s method is different from ours, and also
from Torisawa?s, in that a probabilistic model is
used for feature extraction.
2.2 Identification of Semantic Orientations
The semantic orientation classification of words
has been pursued by several researchers (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al, 2004; Takamura et
al., 2005). However, no computational model for
semantically oriented phrases has been proposed
to date although research for a similar purpose has
been proposed.
Some researchers used sequences of words as
features in document classification according to
semantic orientation. Pang et al (2002) used bi-
grams. Matsumoto et al (2005) used sequential
patterns and tree patterns. Although such patterns
were proved to be effective in document classi-
fication, the semantic orientations of the patterns
themselves are not considered.
Suzuki et al (2006) used the Expectation-
Maximization algorithm and the naive bayes clas-
sifier to incorporate the unlabeled data in the clas-
sification of 3-term evaluative expressions. They
focused on the utilization of context information
such as neighboring words and emoticons. Tur-
ney (2002) applied an internet-based technique to
the semantic orientation classification of phrases,
which had originally been developed for word sen-
timent classification. In their method, the num-
ber of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
?phrase NEAR good?) is used to determine the
orientation. Baron and Hirst (2004) extracted col-
locations with Xtract (Smadja, 1993) and classi-
fied the collocations using the orientations of the
words in the neighboring sentences. Their method
is similar to Turney?s in the sense that cooccur-
rence with seed words is used. The three methods
above are based on context information. In con-
trast, our method exploits the internal structure of
the semantic orientations of phrases.
Inui (2004) introduced an attribute plus/minus
for each word and proposed several rules that
determine the semantic orientations of phrases
on the basis of the plus/minus attribute val-
ues and the positive/negative attribute values of
the component words. For example, a rule
[negative+minus=positive] determines ?low (mi-
nus) risk (negative)? to be positive. Wilson et
al. (2005) worked on phrase-level semantic orien-
tations. They introduced a polarity shifter, which
is almost equivalent to the plus/minus attribute
above. They manually created the list of polarity
shifters. The method that we propose in this paper
is an automatic version of Inui?s or Wilson et al?s
idea, in the sense that the method automatically
creates word clusters and their polarity shifters.
3 Latent Variable Models for Semantic
Orientations of Phrases
As mentioned in the Introduction, the semantic
orientation of a phrase is not a mere sum of its
component words. If we know that ?low risk? is
positive, and that ?risk? and ?mortality?, in some
sense, belong to the same semantic cluster, we can
infer that ?low mortality? is also positive. There-
fore, we propose to use latent variable models to
extract such latent semantic clusters and to real-
ize an accurate classification of phrases (we focus
202
N Z
A
N
A C
N Z
A C
N Z
A C
N Z
A C
(a) (b) (c) (d) (e)
Figure 1: Graphical representations:(a) PLSI, (b) naive bayes, (c) 3-PLSI, (d) triangle, (e) U-shaped;
Each node indicates a random variable. Arrows indicate statistical dependency between variables. N , A,
Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations.
on two-term phrases in this paper). The models
adopted in this paper are also used for collabora-
tive filtering by Hofmann (2004).
With these models, the nouns (e.g., ?risk? and
?mortality?) that become positive by reducing
their degree or amount would make a cluster. On
the other hand, the adjectives or verbs (e.g., ?re-
duce? and ?decrease?) that are related to reduction
would also make a cluster.
Figure 1 shows graphical representations of sta-
tistical dependencies of models with a latent vari-
able. N , A, Z and C respectively correspond to
nouns, adjectives, latent clusters and semantic ori-
entations. Figure 1-(a) is the PLSI model, which
cannot be used in this task due to the absence of
a variable for semantic orientations. Figure 1-(b)
is the naive bayes model, in which nouns and ad-
jectives are statistically independent of each other
given the semantic orientation. Figure 1-(c) is,
what we call, the 3-PLSI model, which is the 3-
observable variable version of the PLSI. We call
Figure 1-(d) the triangle model, since three of its
four variables make a triangle. We call Figure 1-
(e) the U-shaped model. In the triangle model and
the U-shaped model, adjectives directly influence
semantic orientations (rating categories) through
the probability P (c|az). While nouns and adjec-
tives are associated with the same set of clusters Z
in the 3-PLSI and the triangle models, only nouns
are clustered in the U-shaped model.
In the following, we construct a probability
model for the semantic orientations of phrases us-
ing each model of (b) to (e) in Figure 1. We ex-
plain in detail the triangle model and the U-shaped
model, which we will propose to use for this task.
3.1 Triangle Model
Suppose that a set D of tuples of noun n, adjective
a (predicate, generally) and the rating c is given :
D = {(n1, a1, c1), ? ? ? , (n|D|, a|D|, c|D|)}, (1)
where c ? {?1, 0, 1}, for example. This can be
easily expanded to the case of c ? {1, ? ? ? , 5}. Our
purpose is to predict the rating c for unknown pairs
of n and a.
According to Figure 1-(d), the generative prob-
ability of n, a, c, z is the following :
P (nacz) = P (z|n)P (a|z)P (c|az)P (n). (2)
Remember that for the original PLSI model,
P (naz) = P (z|n)P (a|z)P (n).
We use the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977) to estimate the pa-
rameters of the model. According to the theory of
the EM algorithm, we can increase the likelihood
of the model with latent variables by iteratively in-
creasing the Q-function. The Q-function (i.e., the
expected log-likelihood of the joint probability of
complete data with respect to the conditional pos-
terior of the latent variable) is expressed as :
Q(?) =
?
nac
fnac
?
z
P? (z|nac) log P (nazc|?), (3)
where ? denotes the set of the new parameters.
fnac denotes the frequency of a tuple n, a, c in the
data. P? represents the posterior computed using
the current parameters.
The E-step (expectation step) corresponds to
simple posterior computation :
P? (z|nac) = P (z|n)P (a|z)P (c|az)?
z P (z|n)P (a|z)P (c|az)
. (4)
For derivation of update rules in the M-step (max-
imization step), we use a simple Lagrange method
for this optimization problem with constraints :
?z, ?n P (n|z) = 1, ?z,
?
a P (a|z) = 1, and
?a, z, ?c P (c|az) = 1. We obtain the following
update rules :
P (z|n) =
?
ac fnacP? (z|nac)
?
ac fnac
, (5)
203
P (y|z) =
?
nc fnacP? (z|nac)
?
nac fnacP? (z|nac)
, (6)
P (c|az) =
?
n fnacP? (z|nac)
?
nc fnacP? (z|nac)
. (7)
These steps are iteratively computed until conver-
gence. If the difference of the values of Q-function
before and after an iteration becomes smaller than
a threshold, we regard it as converged.
For classification of an unknown pair n, a, we
compare the values of
P (c|na) =
?
z P (z|n)P (a|z)P (c|az)
?
cz P (z|n)P (a|z)P (c|az)
. (8)
Then the rating category c that maximize P (c|na)
is selected.
3.2 U-shaped Model
We suppose that the conditional probability of c
and z given n and a is expressed as :
P (cz|na) = P (c|az)P (z|n). (9)
We compute parameters above using the EM al-
gorithm with the Q-function :
Q(?) =
?
nac
fnac
?
z
P? (z|nac) log P (cz|na, ?).(10)
We obtain the following update rules :
E step
P? (z|nac) = P (c|az)P (z|n)?
z P (c|az)P (z|n)
, (11)
M step
P (c|az) =
?
n fnacP? (z|nac)
?
nc fnacP? (z|nac)
, (12)
P (z|n) =
?
ac fnacP? (z|nac)
?
ac fnac
. (13)
For classification, we use the formula :
P (c|na) =
?
z
P (c|az)P (z|n). (14)
3.3 Other Models for Comparison
We will also test the 3-PLSI model corresponding
to Figure 1-(c).
In addition to the latent models, we test a base-
line classifier, which uses the posterior probabil-
ity :
P (c|na) ? P (n|c)P (a|c)P (c). (15)
This baseline model is equivalent to the 2-term
naive bayes classifier (Mitchell, 1997). The graph-
ical representation of the naive bayes model is (b)
in Figure 1. The parameters are estimated as :
P (n|c) = 1 + fnc|N | + fc
, (16)
P (a|c) = 1 + fac|A| + fc
, (17)
where |N | and |A| are the numbers of the words
for n and a, respectively.
Thus, we have four different models : naive
bayes (baseline), 3-PLSI, triangle, and U-shaped.
3.4 Discussions on the EM computation, the
Models and the Task
In the actual EM computation, we use the tem-
pered EM (Hofmann, 2001) instead of the stan-
dard EM explained above, because the tempered
EM can avoid an inaccurate estimation of the
model caused by ?over-confidence? in computing
the posterior probabilities. The tempered EM can
be realized by a slight modification to the E-step,
which results in a new E-step :
P? (z|nac) =
(
P (c|az)P (z|n)
)?
?
z
(
P (c|az)P (z|n)
)? , (18)
for the U-shaped model, where ? is a positive
hyper-parameter, called the inverse temperature.
The new E-steps for the other models are similarly
expressed.
Now we have two hyper-parameters : inverse
temperature ?, and the number of possible val-
ues M of latent variables. We determine the
values of these hyper-parameters by splitting the
given training dataset into two datasets (the tempo-
rary training dataset 90% and the held-out dataset
10%), and by obtaining the classification accuracy
for the held-out dataset, which is yielded by the
classifier with the temporary training dataset.
We should also note that Z (or any variable)
should not have incoming arrows simultaneously
from N and A, because the model with such ar-
rows has P (z|na), which usually requires an ex-
cessively large memory.
To work with numerical scales of the rating
variable (i.e., the difference between c = ?1 and
c = 1 should be larger than that of c = ?1
and c = 0), Hofmann (2004) used also a Gaus-
sian distribution for P (c|az) in collaborative filter-
ing. However, we do not employ a Gaussian, be-
cause in our dataset, the number of rating classes is
204
only 3, which is so small that a Gaussian distribu-
tion cannot be a good approximation of the actual
probability density function. We conducted pre-
liminary experiments with the model with Gaus-
sians, but failed to obtain good results. For other
datasets with more classes, Gaussians might be a
good model for P (c|az).
The task we address in this paper is somewhat
similar to the trigram prediction task, in the sense
that both are classification tasks given two words.
However, we should note the difference between
these two tasks. In our task, the actual answer
given two specific words are fixed as illustrated
by the fact ?high+salary? is always positive, while
the answer for the trigram prediction task is ran-
domly distributed. We are therefore interested in
the semantic orientations of unseen pairs of words,
while the main purpose of the trigram prediction
is accurately estimate the probability of (possibly
seen) word sequences.
In the proposed models, only the words that ap-
peared in the training dataset can be classified. An
attempt to deal with the unseen words is an in-
teresting task. For example, we could extend our
models to semi-supervised models by regarding C
as a partially observable variable. We could also
use distributional similarity of words (e.g., based
on window-size cooccurrence) to find an observed
word that is most similar to the given unseen word.
However, such methods would not work for the
semantic orientation classification, because those
methods are designed for simple cooccurrence and
cannot distinguish ?survival-rate? from ?infection-
rate?. In fact, the similarity-based method men-
tioned above failed to work efficiently in our pre-
liminary experiments. To solve the problem of un-
seen words, we would have to use other linguistic
resources such as a thesaurus or a dictionary.
4 Experiments
4.1 Experimental Settings
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper ar-
ticles (1995) written in Japanese, and annotated
the pairs with semantic orientation tags : positive,
neutral or negative. We thus obtained the labeled
dataset consisting of 12066 pair instances (7416
different pairs). The dataset contains 4459 neg-
ative instances, 4252 neutral instances, and 3355
positive instances. The number of distinct nouns is
4770 and the number of distinct adjectives is 384.
To check the inter-annotator agreement between
two annotators, we calculated ? statistics, which
was 0.640. This value is allowable, but not quite
high. However, positive-negative disagreement is
observed for only 0.7% of the data. In other words,
this statistics means that the task of extracting neu-
tral examples, which has hardly been explored, is
intrinsically difficult.
We employ 10-fold cross-validation to obtain
the average value of the classification accuracy.
We split the dataset such that there is no overlap-
ping pair (i.e., any pair in the training dataset does
not appear in the test dataset).
If either of the two words in a pair in the test
dataset does not appear in the training dataset, we
excluded the pair from the test dataset since the
problem of unknown words is not in the scope of
this research. Therefore, we evaluate the pairs that
are not in the training dataset, but whose compo-
nent words appear in the training dataset.
In addition to the original dataset, which we call
the standard dataset, we prepared another dataset
in order to examine the power of the latent variable
model. The new dataset, which we call the hard
dataset, consists only of examples with 17 difficult
adjectives such as ?high?, ?low?, ?large?, ?small?,
?heavy?, and ?light?. 1 The semantic orientations
of pairs including these difficult words often shift
depending on the noun they modify. Thus, the
hard dataset is a subset of the standard dataset. The
size of the hard dataset is 4787. Please note that
the hard dataset is used only as a test dataset. For
training, we always use the standard dataset in our
experiments.
We performed experiments with all the values
of ? in {0.1, 0.2, ? ? ? , 1.0} and with all the values
of M in {10, 30, 50, 70, 100, 200, 300, 500}, and
predicted the best values of the hyper-parameters
with the held-out method in Section 3.4.
4.2 Results
The classification accuracies of the four methods
with ? and M predicted by the held-out method
are shown in Table 1. Please note that the naive
bayes method is irrelevant of ? and M . The table
shows that the triangle model and the U-shaped
1The complete list of the 17 Japanese adjectives with their
English counterparts are : takai (high), hikui (low), ookii
(large), chiisai (small), omoi (heavy), karui (light), tsuyoi
(strong), yowai (weak), ooi (many), sukunai (few/little), nai
(no), sugoi (terrific), hageshii (terrific), hukai (deep), asai
(shallow), nagai (long), mizikai (short).
205
Table 1: Accuracies with predicted ? and M
standard hard
accuracy ? M accuracy ? M
Naive Bayes 73.40 ? ? 65.93 ? ?
3-PLSI 67.02 0.73 91.7 60.51 0.80 87.4
Triangle model 81.39 0.60 174.0 77.95 0.60 191.0
U-shaped model 81.94 0.64 60.0 75.86 0.65 48.3
model achieved high accuracies and outperformed
the naive bayes method. This result suggests that
we succeeded in capturing the internal structure
of semantically oriented phrases by way of latent
variables. The more complex structure of the tri-
angle model resulted in the accuracy that is higher
than that of the U-shaped model.
The performance of the 3-PLSI method is even
worse than the baseline method. This result shows
that we should use a model in which adjectives can
directly influence the rating category.
Figures 2, 3, 4 show cross-validated accuracy
values for various values of ?, respectively yielded
by the 3-PLSI model, the triangle model and the
U-shaped model with different numbers M of pos-
sible states for the latent variable. As the figures
show, the classification performance is sensitive to
the value of ?. M = 100 and M = 300 are mostly
better than M = 10. However, this is a tradeoff
between classification performance and training
time, since large values of M demand heavy com-
putation. In that sense, the U-shaped model is use-
ful in many practical situations, since it achieved a
good accuracy even with a relatively small M .
To observe the overall tendency of errors, we
show the contingency table of classification by the
U-shaped model with the predicted values of hy-
perparameters, in Table 2. As this table shows,
most of the errors are caused by the difficulty of
classifying neutral examples. Only 2.26% of the
errors are mix-ups of the positive orientation and
the negative orientation.
We next investigate the causes of errors by ob-
serving those mix-ups of the positive orientation
and the negative orientation.
One type of frequent errors is illustrated by the
pair ?food (?s price) is high?, in which the word
?price? is omitted in the actual example 2. As in
this expression, the attribute (price, in this case) of
an example is sometimes omitted or not correctly
2This kind of ellipsis often occurs in Japanese.
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 2: 3-PLSI model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 3: Triangle model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 4: U-shaped model with standard dataset
206
Table 2: Contingency table of classification result by the U-shaped model
U-shaped model
positive neutral negative sum
positive 1856 281 69 2206
Gold standard neutral 202 2021 394 2617
negative 102 321 2335 2758
sum 2160 2623 2798 7581
identified. To tackle these examples, we will need
methods for correctly identifying attributes and
objects. Some researchers are starting to work on
this problem (e.g., Popescu and Etzioni (2005)).
We succeeded in addressing the data-sparseness
problem by introducing a latent variable. How-
ever, this problem still causes some errors. Pre-
cise statistics cannot be obtained for infrequent
words. This problem will be solved by incorporat-
ing other resources such as thesaurus or a dictio-
nary, or combining our method with other methods
using external wider contexts (Suzuki et al, 2006;
Turney, 2002; Baron and Hirst, 2004).
4.3 Examples of Obtained Clusters
Next, we qualitatively evaluate the proposed meth-
ods. For several clusters z, we extract the words
that occur more than twice in the whole dataset
and are in top 50 according to P (z|n). The model
used here as an example is the U-shaped model.
The experimental settings are ? = 0.6 and M =
60. Although some elements of clusters are com-
posed of multiple words in English, the original
Japanese counterparts are single words.
Cluster 1 trouble, objection, disease, complaint, anx-
iety, anamnesis, relapse
Cluster 2 risk, mortality, infection rate, onset rate
Cluster 3 bond, opinion, love, meaning, longing, will
Cluster 4 vote, application, topic, supporter
Cluster 5 abuse, deterioration, shock, impact, burden
Cluster 6 deterioration, discrimination, load, abuse
Cluster 7 relative importance, degree of influence,
number, weight, sense of belonging, wave,
reputation
These obtained clusters match our intuition. For
example, in cluster 2 are the nouns that are neg-
ative when combined with ?high?, and positive
when combined with ?low?. In fact, the posterior
probabilities of semantic orientations for cluster 2
are as follows :
P (negative|high, cluster 2) = 0.995,
P (positive|low, cluster 2) = 0.973.
With conventional clustering methods based on
the cooccurrence of two words, cluster 2 would
include the words resulting in the opposite orien-
tation, such as ?success rate?. We succeeded in
obtaining the clusters that are suitable for our task,
by incorporating the new variable c for semantic
orientation in the EM computation.
5 Conclusion
We proposed models for phrases with semantic
orientations as well as a classification method
based on the models. We introduced a latent vari-
able into the models to capture the properties of
phrases. Through experiments, we showed that
the proposed latent variable models work well
in the classification of semantic orientations of
phrases and achieved nearly 82% classification ac-
curacy. We should also note that our method is
language-independent although evaluation was on
a Japanese dataset.
We plan next to adopt a semi-supervised learn-
ing method in order to correctly classify phrases
with infrequent words, as mentioned in Sec-
tion 4.2. We would also like to extend our method
to 3- or more term phrases. We can also use the
obtained latent variables as features for another
classifier, as Fujita et al (2004) used latent vari-
ables of PLSI for the k-nearest neighbors method.
One important and promising task would be the
use of semantic orientations of words for phrase
level classification.
References
Faye Baron and Graeme Hirst. 2004. Collocations
as cues to semantic orientation. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society Series B, 39(1):1?38.
207
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2004. Detection of incorrect case assignments in au-
tomatically generated paraphrases of Japanese sen-
tences. In Proceedings of the 1st International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 14?21.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of ad-
jectives. In Proceedings of the Thirty-Fifth Annual
Meeting of the Association for Computational Lin-
guistics and the Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 174?181.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42:177?196.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22:89?115.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute
of Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115?1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelli-
gence, SLUD-33, pages 45?50.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Proceedings of the 9th Pacific-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD-
05), pages 301?310.
Tom M. Mitchell. 1997. Machine Learning. McGraw
Hill.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?02), pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of joint conference on Hu-
man Language Technology / Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP?05), pages 339?346.
Frank Z. Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,,
19(1):143?177.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Pro-
ceedings of the 7th International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-06), pages 502?513.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 133?140.
Kentaro Torisawa. 2001. An unsuperveised method
for canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS 2001), pages
211?218.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?02), pages 417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of joint
conference on Human Language Technology / Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP?05), pages 347?354.
208
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 781?789,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Text Summarization Model
based on Maximum Coverage Problem and its Variant
Hiroya Takamura and Manabu Okumura
Precision and Intelligence Laboratory, Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, 226-8503
takamura@pi.titech.ac.jp oku@pi.titech.ac.jp
Abstract
We discuss text summarization in terms of
maximum coverage problem and its vari-
ant. We explore some decoding algorithms
including the ones never used in this sum-
marization formulation, such as a greedy
algorithm with performance guarantee, a
randomized algorithm, and a branch-and-
bound method. On the basis of the results
of comparative experiments, we also aug-
ment the summarization model so that it
takes into account the relevance to the doc-
ument cluster. Through experiments, we
showed that the augmented model is su-
perior to the best-performing method of
DUC?04 on ROUGE-1 without stopwords.
1 Introduction
Automatic text summarization is one of the tasks
that have long been studied in natural language
processing. This task is to create a summary, or
a short and concise document that describes the
content of a given set of documents (Mani, 2001).
One well-known approach to text summariza-
tion is the extractive method, which selects some
linguistic units (e.g., sentences) from given doc-
uments in order to generate a summary. The ex-
tractive method has an advantage that the gram-
maticality is guaranteed at least at the level of the
linguistic units. Since the actual generation of
linguistic expressions has not achieved the level
of the practical use, we focus on the extractive
method in this paper, especially the method based
on the sentence extraction. Most of the extractive
summarization methods rely on sequentially solv-
ing binary classification problems of determining
whether each sentence should be selected or not.
In such sequential methods, however, the view-
point regarding whether the summary is good as
a whole, is not taken into consideration, although
a summary conveys information as a whole.
We represent text summarization as an opti-
mization problem and attempt to globally solve
the problem. In particular, we represent text sum-
marization as a maximum coverage problem with
knapsack constraint (MCKP). One of the advan-
tages of this representation is that MCKP can di-
rectly model whether each concept in the given
documents is covered by the summary or not,
and can dispense with rather counter-intuitive ap-
proaches such as giving penalty to each pair of two
similar sentences. By formally apprehending the
target problem, we can use a lot of knowledge and
techniques developed in the combinatorial mathe-
matics, and also analyse results more precisely. In
fact, on the basis of the results of the experiments,
we augmented the summarization model.
The contributions of this paper are as follows.
We are not the first to represent text summarization
as MCKP. However, no researchers have exploited
the decoding algorithms for solving MCKP in
the summarization task. We conduct compre-
hensive comparative experiments of those algo-
rithms. Specifically, we test the greedy algorithm,
the greedy algorithm with performance guarantee,
the stack decoding, the linear relaxation problem
with randomized decoding, and the branch-and-
bound method. On the basis of the experimental
results, we then propose an augmented model that
takes into account the relevance to the document
cluster. We empirically show that the augmented
model is superior to the best-performing method
of DUC?04 on ROUGE-1 without stopwords.
2 Related Work
Carbonell and Goldstein (2000) used sequential
sentence selection in combination with maximal
marginal relevance (MMR), which gives penalty
to sentences that are similar to the already se-
lected sentences. Schiffman et al?s method (2002)
is also based on sequential sentence selection.
Radev et al (2004), in their method MEAD, used
a clustering technique to find the centroid, that
781
is, the words with high relevance to the topic
of the document cluster. They used the centroid
to rank sentences, together with the MMR-like
redundancy score. Both relevance and redun-
dancy are taken into consideration, but no global
viewpoint is given. In CLASSY, which is the
best-performing method in DUC?04, Conroy et
al. (2004) scored sentences with the sum of tf-idf
scores of words. They also incorporated sentence
compression based on syntactic or heuristic rules.
McDonald (2007) formulated text summariza-
tion as a knapsack problem and obtained the
global solution and its approximate solutions. Its
relation to our method will be discussed in Sec-
tion 6.1. Filatova and Hatzivassiloglou (2004) first
formulated text summarization as MCKP. Their
decoding method is a greedy one and will be em-
pirically compared with other decoding methods
in this paper. Yih et al (2007) used a slightly-
modified stack decoding. The optimization prob-
lem they solved was the MCKP with the last sen-
tence truncation. Their stack decoding is one of
the decoding methods discussed in this paper. Ye
et al (2007) is another example of coverage-based
methods. Shen et al (2007) regarded summariza-
tion as a sequential labelling task and solved it
with Conditional Random Fields. Although the
model is globally optimized in terms of likelihood,
the coverage of concepts is not taken into account.
3 Modeling text summarization
In this paper, we focus on the extractive summa-
rization, which generates a summary by select-
ing linguistic units (e.g., sentences) in given doc-
uments. There are two types of summarization
tasks: single-document summarization and multi-
document summarization. While single-document
summarization is to generate a summary from a
single document, multi-document summarization
is to generate a summary frommultiple documents
regarding one topic. Such a set of multiple docu-
ments is called a document cluster. The method
proposed in this paper is applicable to both tasks.
In both tasks, documents are split into several lin-
guistic units D = {s1, ? ? ? , s|D|} in preprocess-
ing. We will select some linguistic units from D to
generate a summary. Among other linguistic units
that can be used in the method, we use sentences
so that the grammaticality at the sentence level is
going to be guaranteed.
We introduce conceptual units (Filatova and
Hatzivassiloglou, 2004), which compose the
meaning of a sentence. Sentence si is represented
by a set of conceptual units {ei1, ? ? ? , ei|si|}. For
example, the sentence ?The man bought a book
and read it? could be regarded as consisting of two
conceptual units ?the man bought a book? and ?the
man read the book?. It is not easy, however, to
determine the appropriate granularity of concep-
tual units. A simple way would be to regard the
above sentence as consisting of four conceptual
units ?man?, ?book?, ?buy?, and ?read?. There
is some work on the definition of conceptual units.
Hovy et al (2006) proposed to use basic elements,
which are dependency subtrees obtained by trim-
ming dependency trees. Although basic elements
were proposed for evaluation of summaries, they
can probably be used also for summary genera-
tion. However, such novel units have not proved
to be useful for summary generation. Since we fo-
cus more on algorithms and models in this paper,
we simply use words as conceptual units.
The goal of text summarization is to cover as
many conceptual units as possible using only a
small number of sentences. In other words, the
goal is to find a subset S(? D) that covers as
many conceptual units as possible. In the follow-
ing, we introduce models for that purpose. We
think of the situation that the summary length must
be at most K (cardinality constraint) and the sum-
mary length is measured by the number of words
or bytes in the summary.
Let xi denote a variable which is 1 if sentence
si is selected, otherwise 0, aij denote a constant
which is 1 if sentence si contains word ej , oth-
erwise 0. We regard word ej as covered when at
least one sentence containing ej is selected as part
of the summary. That is, word ej is covered if and
only if
?
i aijxi ? 1. Now our objective is to find
the binary assignment on xi with the best coverage
such that the summary length is at most K:
max. |{j|
?
i aijxi ? 1}|
s.t.
?
i cixi ? K; ?i, xi ? {0, 1},
where ci is the cost of selecting si, i.e., the number
of words or bytes in si.
For convenience, we rewrite the problem above:
max.
?
j zj
s.t.
?
i cixi ? K; ?j,
?
i aijxi ? zj ;
?i, xi ? {0, 1}; ?j, zj ? {0, 1},
782
where zj is 1 when ej is covered, 0 otherwise. No-
tice that this new problem is equivalent to the pre-
vious one.
Since not all the words are equally important,
we introduce weights wj on words ej . Then the
objective is restated as maximizing the weighted
sum
?
j wjzj such that the summary length is at
most K. This problem is called maximum cov-
erage problem with knapsack constraint (MCKP),
which is an NP-hard problem (Khuller et al,
1999). We should note that MCKP is different
from a knapsack problem. MCKP merely has a
constraint of knapsack form. Filatova and Hatzi-
vassiloglou (2004) pointed out that text summa-
rization can be formalized by MCKP.
The performance of the method depends on how
to represent words and which words to use. We
represent words with their stems. We use only
the words that are content words (nouns, verbs,
or adjectives) and not in the stopword list used in
ROUGE (Lin, 2004).
The weights wj of words are also an impor-
tant factor of good performance. We tested two
weighting schemes proposed by Yih et al (2007).
The first one is interpolated weights, which are in-
terpolated values of the generative word probabil-
ity in the entire document and that in the beginning
part of the document (namely, the first 100 words).
Each probability is estimated with the maximum
likelihood principle. The second one is trained
weights. These values are estimated by the logis-
tic regression trained on data instances, which are
labeled 1 if the word appears in a summary in the
training dataset, 0 otherwise. The feature set for
the logistic regression includes the frequency of
the word in the document cluster and the position
of the word instance and others.
4 Algorithms for solving MCKP
We explain how to solve MCKP. We first explain
the greedy algorithm applied to text summariza-
tion by Filatova and Hatzivassiloglou (2004). We
then introduce a greedy algorithm with perfor-
mance guarantee. This algorithm has never been
applied to text summarization. We next explain the
stack decoding used by Yih et al (2007). We then
introduce an approximate method based on linear
relaxation and a randomized algorithm, followed
by the branch-and-bound method, which provides
the exact solution.
Although the algorithms used in this paper
themselves are not novel, this work is the first
to apply the greedy algorithm with performance
guarantee, the randomized algorithm, and the
branch-and-bound to solve the MCKP and auto-
matically create a summary. In addition, we con-
duct a comparative study on summarization algo-
rithms including the above.
There are some other well-known methods for
similar problems (e.g., the method of conditional
probability (Hromkovic?, 2003)). A pipage ap-
proach (Ageev and Sviridenko, 2004) has been
proposed for MCKP, but we do not use this algo-
rithm, since it requires costly partial enumeration
and solutions to many linear relaxation problems.
As in the previous section, D denotes the set of
sentences {s1, ? ? ? , s|D|}, and S denotes a subset
of D and thus represents a summary.
4.1 Greedy algorithm
Filatova and Hatzivassiloglou (2004) used a
greedy algorithm. In this section, Wl denotes the
sum of the weights of the words covered by sen-
tence sl. W ?l denotes the sum of the weights of the
words covered by sl, but not by current summary
S. This algorithm sequentially selects sentence sl
with the largest W ?l .
Greedy Algorithm
U ? D, S ? ?
while U 6= ?
si ? argmaxsl?U W ?l
if ci +
?
sl?S cl ? K then insert si into S
delete si in U
end while
output S.
This algorithm has performance guarantee
when the problem has a unit cost (i.e., when each
sentence has the same length), but no performance
guarantee for the general case where costs can
have different values.
4.2 Greedy algorithm with performance
guarantee
We describe a greedy algorithm with performance
guarantee proposed by Khuller et al (1999), which
proves to achieve an approximation factor of (1 ?
1/e)/2 for MCKP. This algorithm sequentially se-
lects sentence sl with the largest ratio W ?l /cl. Af-
ter the sequential selection, the set of the selected
sentences is compared with the single-sentence
summary that has the largest value of the objec-
tive function. The larger of the two is going to
783
be the output of this new greedy algorithm. Here
score(S) is
?
j wjzj , the value of the objective
function for summary S.
Greedy Algorithm with Performance Guarantee
U ? D, S ? ?
while U 6= ?
si ? argmaxsl?U W ?l /cl
if ci +
?
sl?S cl ? K then insert si into S
delete si in U
end while
st ? argmaxsl Wl
if score(S) ? Wt, output S,
otherwise, output {st}.
They also proposed an algorithm with a better per-
formance guarantee, which is not used in this pa-
per because it is costly due to its partial enumera-
tion.
4.3 Stack decoding
Stack decoding is a decoding method proposed by
Jelinek (1969). This algorithm requires K priority
queues, k-th of which is the queue for summaries
of length k. The objective function value is used
for the priority measure. A new solution (sum-
mary) is generated by adding a sentence to a cur-
rent solution in k-th queue and inserted into a suc-
ceeding queue.1 The ?pop? operation in stack de-
coding pops the candidate summary with the least
priority in the queue. By restricting the size of
each queue to a certain constant stacksize, we can
obtain an approximate solution within a practical
computational time.
Stack Decoding
for k = 0 to K ? 1
for each S ? queues[k]
for each sl ? D
insert sl into S
insert S into queues[k + cl]
pop if queue-size exceeds the stacksize
end for
end for
end for
return the best solution in queues[K]
4.4 Randomized algorithm
Khuller et al (2006) proposed a randomized al-
gorithm (Hromkovic?, 2003) for MCKP. In this al-
gorithm, a relaxation linear problem is generated
by replacing the integer constraints xi ? {0, 1}
1We should be aware that stack in a strict data-structure
sense is not used in the algorithm.
and zj ? {0, 1} with linear constraints xi ? [0, 1]
and zj ? [0, 1]. The optimal solution x?i to the re-
laxation problem is regarded as the probability of
sentence si being selected as a part of summary:
x?i = P (xi = 1). The algorithm randomly se-
lects sentence si with probability x?i , in order to
generate a summary. It has been proved that the
expected length of each randomly-generated sum-
mary is upper-bounded by K, and the expected
value of the objective function is at least the op-
timal value multiplied by (1?1/e) (Khuller et al,
2006). This random generation of a summary is it-
erated many times, and the summaries that are not
longer than K are stored as candidate summaries.
Among those many candidate summaries, the one
with the highest value of the objective function is
going to be the output by this algorithm.
4.5 Branch-and-bound method
The branch-and-bound method (Hromkovic?,
2003) is an efficient method for finding the exact
solutions to integer problems. Since MCKP is an
NP-hard problem, it cannot generally be solved in
polynomial time under a reasonable assumption
that NP 6=P. However, if the size of the problem
is limited, sometimes we can obtain the exact
solution within a practical time by means of the
branch-and-bound method.
4.6 Weakly-constrained algorithms
In evaluation with ROUGE (Lin, 2004), sum-
maries are truncated to a target length K. Yih et
al. (2007) used a stack decoding with a slight mod-
ification, which allows the last sentence in a sum-
mary to be truncated to a target length. Let us call
this modified algorithm the weakly-constrained
stack decoding. The weakly-constrained stack de-
coding can be implemented simply by replacing
queues[k + cl] with queues[min(k + cl,K)]. We
can also think of weakly-constrained versions of
the greedy and randomized algorithms introduced
before.
In this paper, we do not adopt weakly-
constrained algorithms, because although an ad-
vantage of the extractive summarization is the
guaranteed grammaticality at the sentence level,
the summaries with a truncated sentence will relin-
quish this advantage. We mentioned the weakly-
constrained algorithms in order to explain the re-
lation between the proposed model and the model
proposed by Yih et al (2007).
784
5 Experiments and Discussion
5.1 Experimental Setting
We conducted experiments on the dataset of
DUC?04 (2004) with settings of task 2, which is
a multi-document summarization task. 50 docu-
ment clusters, each of which consists of 10 doc-
uments, are given. One summary is to be gen-
erated for each cluster. Following the most rel-
evant previous method (Yih et al, 2007), we set
the target length to 100 words. DUC?03 (2003)
dataset was used as the training dataset for trained
weights. All the documents were segmented
into sentences using a script distributed by DUC.
Words are stemmed by Porter?s stemmer (Porter,
1980). ROUGE version 1.5.5 (Lin, 2004) was
used for evaluation.2 Among others, we focus
on ROUGE-1 in the discussion of the result, be-
cause ROUGE-1 has proved to have strong corre-
lation with human annotation (Lin, 2004; Lin and
Hovy, 2003). Wilcoxon signed rank test for paired
samples with significance level 0.05 was used for
the significance test of the difference in ROUGE-
1. The simplex method and the branch-and-bound
method implemented in GLPK (Makhorin, 2006)
were used to solve respectively linear and integer
programming problems.
The methods that are compared here are the
greedy algorithm (greedy), the greedy algorithm
with performance guarantee (g-greedy), the ran-
domized algorithm (rand), the stack decoding
(stack), and the branch-and-bound method (exact).
5.2 Results
The experimental results are shown in Tables 1
and 2. The columns 1, 2, and SU4 in the ta-
bles respectively refer to ROUGE-1, ROUGE-2,
and ROUGE-SU4. In addition, rand100k refers to
the randomized algorithmwith 100,000 randomly-
generated solution candidates, and stack30 refers
to stack with the stacksize being 30. The right-
most column (?time?) shows the average computa-
tional time required for generating a summary for
a document cluster.
Both with interpolated (Table 1) and trained
weights (Table 2), g-greedy significantly outper-
formed greedy. With interpolated weights, there
was no significant difference between exact and
g-greedy, and between exact and stack30. With
trained weights, there was no significant differ-
2With options -n 4 -m -2 4 -u -f A -p 0.5 -l 100 -t 0 -d -s.
Table 1: ROUGE of MCKP with interpolated
weights. Underlined ROUGE-1 scores are signif-
icantly different from the score of exact. Compu-
tational time was measured in seconds.
ROUGE time
1 2 SU4 (sec)
greedy 0.283 0.083 0.123 <0.01
g-greedy 0.294 0.080 0.121 0.01
rand100k 0.300 0.079 0.119 1.88
stack30 0.304 0.078 0.120 4.53
exact 0.305 0.081 0.121 4.04
Table 2: ROUGE of MCKP with trained weights.
Underlined ROUGE-1 scores are significantly dif-
ferent from the score of exact. Computational time
was measured in seconds.
ROUGE time
1 2 SU4 (sec)
greedy 0.283 0.080 0.121 < 0.01
g-greedy 0.310 0.077 0.118 0.01
rand100k 0.299 0.077 0.117 1.93
stack30 0.309 0.080 0.120 4.23
exact 0.307 0.078 0.119 4.56
ence between exact and the other algorithms ex-
cept for greedy and rand100k. The result sug-
gests that approximate fast algorithms can yield
results comparable to the exact method in terms of
ROUGE-1 score. We will later discuss the results
in terms of objective function values and search
errors in Table 4.
We should notice that stack outperformed ex-
act with interpolated weights. To examine this
counter-intuitive point, we changed the stack-
size of stack with interpolated weights (inter) and
trained weights (train) from 10 to 100 and ob-
tained Table 3. This table shows that the ROUGE-
1 value does not increase as the stacksize does;
ROUGE-1 for stack with interpolated weights
does not change much with the stacksize, and the
peak of ROUGE-1 for trained weights is at the
stacksize of 20. Since stack with a larger stack-
size selects a solution from a larger number of so-
lution candidates, this result is counter-intuitive in
the sense that non-global decoding by stack has a
favorable effect.
We also counted the number of the document
clusters for which an approximate algorithm with
interpolated weights yielded the same solution as
785
Table 3: ROUGE of stack with various stacksizes
size 10 20 30 50 100
inter 0.304 0.304 0.304 0.304 0.303
train 0.308 0.310 0.309 0.308 0.307
Table 4: Search errors of MCKP with interpolated
weights
solution same search error
ROUGE (=) = ? ?
greedy 0 1 35 14
g-greedy 0 5 26 19
rand100k 6 5 25 14
stack30 16 11 8 11
exact (?same solution? column in Table 4). If
the approximate algorithm failed to yield the ex-
act solution (?search error? column), we checked
whether the search error made ROUGE score
unchanged (?=? column), decreased (??? col-
umn), or increased (??? column) compared with
ROUGE score of exact. Table 4 shows that (i)
stack30 is a better optimizer than other approx-
imate algorithms, (ii) when the search error oc-
curs, stack30 increases ROUGE-1 more often than
it decreases ROUGE-1 compared with exact in
spite of stack30?s inaccurate solution, (iii) ap-
proximate algorithms sometimes achieved better
ROUGE scores. We observed similar phenomena
for trained weights, though we skip the details due
to space limitation.
These observations on stacksize and search er-
rors suggest that there exists another maximization
problem that is more suitable to summarization.
We should attempt to find the more suitable maxi-
mization problem and solve it using some existing
optimization and approximation techniques.
6 Augmentation of the model
On the basis of the experimental results in the pre-
vious section, we augment our text summarization
model. We first examine the current model more
carefully. As mentioned before, we used words
as conceptual units because defining those units
is hard and still under development by many re-
searchers. Suppose here that a more suitable unit
has more detailed information, such as ?A did B
to C?. Then the event ?A did D to E? is a com-
pletely different unit from ?A did B to C?. How-
ever, when words are used as conceptual units, the
two events have a redundant part ?A?. It can hap-
pen that a document is concise as a summary, but
redundant on word level. By being to some extent
redundant on the word level, a summary can have
sentences that are more relevant to the document
cluster, as both of the sentences above are relevant
to the document cluster if the document cluster is
about ?A?. A summary with high cohesion and co-
herence would have redundancy to some extent. In
this section, we will use this conjecture to augment
our model.
6.1 Augmented summarization model
The objective function of MCKP consists of only
one term that corresponds to coverage. We add
another term
?
i(
?
j wjaij)xi that corresponds
to relevance to the topic of the document clus-
ter. We represent the relevance of sentence si by
the sum of the weights of words in the sentence
(
?
j wjaij). We take the summation of the rele-
vance values of the selected sentences:
max. (1? ?)
?
j wjzj + ?
?
i(
?
j wjaij)xi
s.t.
?
i cixi ? K; ?j,
?
i aijxi ? zj ;
?i, xi ? {0, 1}; ?j, zj ? {0, 1},
where ? is a constant. We call this model MCKP-
Rel, because the relevance to the document cluster
is taken into account.
We discuss the relation to the model proposed
by McDonald (2007), whose objective function
consists of a relevance term and a negative re-
dundancy term. We believe that MCKP-Rel is
more intuitive and suitable for summarization, be-
cause coverage in McDonald (2007) is measured
by subtracting the redundancy represented with
the sum of similarities between two sentences,
while MCKP-Rel focuses directly on coverage.
Suppose sentence s1 contains conceptual units A
and B, s2 contains A, and s3 contains B. The
proposed coverage-based methods can capture the
fact that s1 has the same information as {s2, s3},
while similarity-based methods only learn that s1
is somewhat similar to each of s2 and s3. We
also empirically showed that our method outper-
forms McDonald (2007)?s method in experiments
on DUC?02, where our method achieved 0.354
ROUGE-1 score with interpolated weights and
0.359 with trained weights when the optimal ? is
given, while McDonald (2007)?s method yielded
at most 0.348. However, this very point can also
786
Table 5: ROUGE-1 of MCKP-Rel with inter-
polated weights. The values in the parentheses
are the corresponding values of ? predicted using
DUC?03 as development data. Underlined are the
values that are significantly different from the cor-
responding values of MCKP.
interpolated trained
greedy 0.287 (0.1) 0.288 (0.8)
g-greedy 0.307 (0.3) 0.320 (0.4)
rand100k 0.310 (0.1) 0.316 (0.5)
stack30 0.324 (0.1) 0.327 (0.3)
exact 0.320 (0.3) 0.329 (0.5)
exactopt 0.327 (0.2) 0.329 (0.5)
be a drawback of our method, since our method
premises that a sentence is represented as a set
of conceptual units. Similarity-based methods are
free from such a premise. Taking advantages of
both models is left for future work.
The decoding algorithms introduced before are
also applicable toMCKP-Rel, becauseMCKP-Rel
can be reduced to MCKP by adding, for each sen-
tence si, a dummy conceptual unit which exists
only in si and has the weight
?
j wjaij .
6.2 Experiments of the augmented model
We ran greedy, g-greedy, rand100k, stack30
and exact to solve MCKP-Rel. We experimented
on DUC?04 with the same experimental setting as
the previous ones.
6.2.1 Experiments with the predicted ?
We determined the value of ? for each method us-
ing DUC?03 as development data. Specifically, we
conducted experiments on DUC?03 with different
? (? {0.0, 0.1, ? ? ? , 1.0}) and simply selected the
one with the highest ROUGE-1 value.
The results with these predicted ? are shown
in Table 5. Only ROUGE-1 values are shown.
Method exactopt is exact with the optimal ?, and
can be regarded as the upperbound of MCKP-Rel.
To evaluate the appropriateness of models without
regard to search quality, we first focused on exact
and found that MCKP-Rel outperformed MCKP
with exact. This means that MCKP-Rel model
is superior to MCKP model. Among the algo-
rithms, stack30 and exact performed well. All
methods except for greedy yielded significantly
better ROUGE values compared with the corre-
sponding results in Tables 1 and 2.
Figures 1 and 2 show ROUGE-1 for different
values of ?. The leftmost part (? = 0.0) cor-
responds to MCKP. We can see from the figures,
that MCKP-Rel at the best ? always outperforms
MCKP, and that MCKP-Rel tends to degrade for
very large ?. This means that excessive weight on
relevance has an adversative effect on performance
and therefore the coverage is important.
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  0.2  0.4  0.6  0.8  1
RO
UG
E-1
lambda
exact
stack30
rand100kg-greedygreedy
Figure 1: MCKP-Rel with interpolated weights
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  0.2  0.4  0.6  0.8  1
RO
UG
E-1
lambda
exact
stack30
rand100kg-greedygreedy
Figure 2: MCKP-Rel with trained weights
6.2.2 Experiments with the optimal ?
In the experiments above, we found that ? =
0.2 is the optimal value for exact with interpo-
lated weights. We suppose that this ? gives the
best model, and examined search errors as we
did in Section 5.2. We obtained Table 6, which
shows that search errors in MCKP-Rel counter-
intuitively increase (?) ROUGE-1 score less of-
ten than MCKP did in Table 4. This was the case
also for trained weights. This result suggests that
MCKP-Rel is more suitable to text summariza-
tion than MCKP is. However, exact with trained
weights at the optimal ?(= 0.4) in Figure 2 was
outperformed by stack30. It suggests that there is
still room for future improvement in the model.
787
Table 6: Search errors of MCKP-Rel with interpo-
lated weights (? = 0.2).
solution same search error
ROUGE (=) = ? ?
greedy 0 2 42 6
g-greedy 1 0 34 15
rand100k 3 6 33 8
stack30 14 13 14 10
6.2.3 Comparison with DUC results
In Section 6.2.1, we empirically showed that
the augmented model MCKP-Rel is better than
MCKP, whose optimization problem is used also
in one of the state-of-the-art methods by Yih et
al. (2007). It would also be beneficial to read-
ers to directly compare our method with DUC
results. For that purpose, we conducted experi-
ments with the cardinality constraint of DUC?04,
i.e., each summary should be 665 bytes long or
shorter. Other settings remained unchanged. We
compared the MCKP-Rel with peer65 (Conroy et
al., 2004) of DUC?04, which performed best in
terms of ROUGE-1 in the competition. Tables 7
and 8 are the ROUGE-1 scores, respectively eval-
uated without and with stopwords. The latter is the
official evaluation measure of DUC?04.
Table 7: ROUGE-1 of MCKP-Rel with byte con-
straints, evaluated without stopwords. Underlined
are the values significantly different from peer65.
interpolated train
greedy 0.289 (0.1) 0.284 (0.8)
g-greedy 0.297 (0.4) 0.323 (0.3)
rand100k 0.315 (0.2) 0.308 (0.4)
stack30 0.324 (0.2) 0.323 (0.3)
exact 0.325 (0.3) 0.326 (0.5)
exactopt 0.325 (0.3) 0.329 (0.4)
peer65 0.309
In Table 7, MCKP-Rel with stack30 and exact
yielded significantly better ROUGE-1 scores than
peer65. Although stack30 and exact yielded
greater ROUGE-1 scores than peer65 also in Ta-
ble 8, the difference was not significant. Only
greedy was significantly worse than peer65.3 One
3We actually succeeded in greatly improving the
ROUGE-1 value of MCKP-Rel evaluated with stopwords by
using all the words including stopwords as conceptual units.
However, we ignore those results in this paper, because it
Table 8: ROUGE-1 of MCKP-Rel with byte con-
straints, evaluated with stopwords. Underlined are
the values significantly different from peer65.
interpolated train
greedy 0.374 (0.1) 0.377 (0.4)
g-greedy 0.371 (0.0) 0.385 (0.2)
rand100k 0.373 (0.2) 0.366 (0.3)
stack30 0.384 (0.1) 0.386 (0.3)
exact 0.383 (0.3) 0.384 (0.4)
exactopt 0.385 (0.1) 0.384 (0.4)
peer65 0.382
possible explanation on the difference between Ta-
ble 7 and Table 8 is that peer65 would probably be
tuned to the evaluation with stopwords, since it is
the official setting of DUC?04.
From these results, we can conclude that the
MCKP-Rel is at least comparable to the best-
performing method, if we choose a powerful de-
coding method, such as stack and exact.
7 Conclusion
We regarded text summarization as MCKP. We
applied some algorithms to solve the MCKP and
conducted comparative experiments. We con-
ducted comparative experiments. We also aug-
mented our model to MCKP-Rel, which takes into
consideration the relevance to the document clus-
ter and performs well.
For future work, we will try other conceptual
units such as basic elements (Hovy et al, 2006)
proposed for summary evaluation. We also plan to
include compressed sentences into the set of can-
didate sentences to be selected as done by Yih et
al. (2007). We also plan to design other decod-
ing algorithms for text summarization (e.g., pipage
approach (Ageev and Sviridenko, 2004)). As dis-
cussed in Section 6.2, integration with similarity-
based models is worth consideration. We will in-
corporate techniques for arranging sentences into
an appropriate order, while the current work con-
cerns only selection. Deshpande et al (2007) pro-
posed a selection and ordering technique, which is
applicable only to the unit cost case such as selec-
tion and ordering of words for title generation. We
plan to refine their model so that it can be applied
to general text summarization.
just trickily uses non-content words to increase the evalua-
tion measure, disregarding the actual quality of summaries.
788
References
Alexander A. Ageev and Maxim Sviridenko. 2004. Pi-
page rounding: A new method of constructing algo-
rithms with proven performance guarantee. Journal
of Combinatorial Optimization, 8(3):307?328.
JohnM. Conroy, Judith D. Schlesinger, John Goldstein,
and Dianne P. O?Leary. 2004. Left-brain/right-brain
multi-document summarization. In Proceedings of
the Document Understanding Conference (DUC).
Pawan Deshpande, Regina Barzilay, and David Karger.
2007. Randomized decoding for selection-and-
ordering problems. In Proceedings of the Human
Language Technologies Conference and the North
American Chapter of the Association for Compu-
tational Linguistics Annual Meeting (HLT/NAACL),
pages 444?451.
DUC. 2003. Document Understanding Conference. In
HLT/NAACL Workshop on Text Summarization.
DUC. 2004. Document Understanding Conference. In
HLT/NAACL Workshop on Text Summarization.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the
20th International Conference on Computational
Linguistics (COLING), pages 397?403.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In Proceedings of
ANLP/NAACL Workshop on Automatic Summariza-
tion, pages 40?48.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-
nichi Fukumoto. 2006. Automated summarization
evaluation with basic elements. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC).
Juraj Hromkovic?. 2003. Algorithmics for Hard Prob-
lems. Springer.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, 13:675?685.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Samir Khuller, Louiqa Raschid, and Yao Wu. 2006.
LP randomized rounding for maximum coverage
problem and minimum set cover with threshold
problem. Technical Report CS-TR-4805, The Uni-
versity of Maryland.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology (HLT-NAACL?03), pages
71?78.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pages 74?81.
Andrew Makhorin, 2006. Reference Manual of GNU
Linear Programming Kit, version 4.9.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Publisher.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the 29th European Conference on Infor-
mation Retrieval (ECIR), pages 557?564.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
Dragomir R. Radev, Hongyan Jing, Ma?gorzata Stys?,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
Management, 40(6):919?938.
Barry Schiffman, Ani Nenkova, and Kathleen McKe-
own. 2002. Experiments in multidocument sum-
marization. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, pages 52?58.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence (IJCAI), pages 2862?2867.
Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long
Qiu. 2007. Document concept lattice for text un-
derstanding and summarization. Information Pro-
cessing and Management, 43(6):1643?1662.
Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence (IJCAI), pages
1776?1782.
789
Classification of Multiple-Sentence Questions
Akihiro Tamura, Hiroya Takamura, and Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
aki@lr.pi.titech.ac.jp
{takamura, oku}@pi.titech.ac.jp
Abstract. Conventional QA systems cannot answer to the questions
composed of two or more sentences. Therefore, we aim to construct a
QA system that can answer such multiple-sentence questions. As the first
stage, we propose a method for classifying multiple-sentence questions
into question types. Specifically, we first extract the core sentence from
a given question text. We use the core sentence and its question focus in
question classification. The result of experiments shows that the proposed
method improves F-measure by 8.8% and accuracy by 4.4%.
1 Introduction
Question-Answering (QA) systems are useful in that QA systems return the
answer itself, while most information retrieval systems return documents that
may contain the answer.
QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC
(Question & Answering Challenge)2 in Japan. In these workshops, the inputs
to systems are only single-sentence questions, which are defined as the ques-
tions composed of one sentence. On the other hand, on the web there are a
lot of multiple-sentence questions (e.g., answer bank3, AskAnOwner4), which
are defined as the questions composed of two or more sentences: For example,
?My computer reboots as soon as it gets started. OS is Windows XP. Is there
any homepage that tells why it happens??. For conventional QA systems, these
questions are not expected and existing techniques are not applicable or work
poorly to these questions. Therefore, constructing QA systems that can handle
multiple-sentence questions is desirable.
An usual QA system is composed of three components: question process-
ing, document retrieval, and answer extraction. In question processing, a given
question is analyzed, and its question type is determined. This process is called
?question classification?. Depending on the question type, the process in the an-
swer extraction component usually changes. Consequently, the accuracy and the
efficiency of answer extraction depend on the accuracy of question classification.
1 http://trec.nist.gov/tracks.htm
2 http://www.nlp.is.ritsumei.ac.jp/qac/
3 http://www.theanswerbank.co.uk/
4 http://www.askanowner.com/
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 426?437, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Classification of Multiple-Sentence Questions 427
Therefore, as a first step towards developing a QA system that can han-
dle multiple-sentence questions, we propose a method for classifying multiple-
sentence questions. Specifically, in this work, we treat only questions which re-
quire one answer. For example, if the question ?The icon to return to desktop has
been deleted. Please tell me how to recover it.? is given, we would like ?WAY?
to be selected as the question type. We thus introduce core sentence extraction
component, which extracts the most important sentence for question classifica-
tion. This is because there are unnecessary sentences for question classification
in a multiple-sentence question, and we hope noisy features should be eliminated
before question classification with the component. If a multiple-sentence question
is given, we first extract the most important sentence for question classification
and then classify the question using the only information in the sentence.
In Section 2, we present the related work. In Section 3, we explain our pro-
posed method. In Section 4, we describe our experiments and results, where we
can confirm the effectiveness of the proposed method. Finally, in Section 5, we
describe the summary of this paper and the future work.
2 Related Work
This section presents some existing methods for question classification. The
methods are roughly divided into two groups: the ones based on hand-crafted
rules and the ones based on machine learning. The system ?SAIQA? [1], Xu et al
[2] used hand-crafted rules for question classification. However, methods based
on pattern matching have the following two drawbacks: high cost of making rules
or patterns by hand and low coverage.
Machine learning can be considered to solve these problems. Li et al [3] used
SNoW for question classification. The SNoW is a multi-class classifier that is
specifically tailored for learning in the presence of a very large number of fea-
tures. Zukerman et al [4] used decision tree. Ittycheriah et al [5] used maximum
entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared
question classification using machine learning methods (decision tree, maximum
entropy, SVM) with a rule-based method. The result showed that the accuracy
of question classification with SVM is the highest of all. According to Suzuki [6],
a lot of information is needed to improve the accuracy of question classification
and SVM is suitable for question classification, because SVM can classify ques-
tions with high accuracy even when the dimension of the feature space is large.
Moreover, Zhang et al [7] compared question classification with five machine
learning algorithms and showed that SVM outperforms the other four methods
as Suzuki [6] showed. Therefore, we also use SVM in classifying questions, as we
will explain later.
However, please note that we treat not only usual single-sentence questions,
but also multiple-sentence questions. Furthermore, our work differs from previous
work in that we treat real data on the web, not artificial data prepared for the
QA task. From these points, the results in this paper cannot be compared with
the ones in the previous work.
428 A.Tamura, H. Takamura, and M. Okumura
3 Two-Step Approach to Multiple-Sentence Question
Classification
This section describes our method for classifying multiple-sentence questions.
We first explain the entire flow of our question classification. Figure 1 shows the
proposed method.
question
preprocessing
core sentence 
extraction component 
question classification
component
a core sentence
single-sentence 
question
multiple-sentence 
question
question type
peculiar process to 
multiple-sentence questions
Fig. 1. The entire flow of question classification
An input question consisting of possibly multiple sentences is first prepro-
cessed. Parentheses parts are excluded in order to avoid errors in syntactic pars-
ing. The question is divided into sentences by punctuation marks.
The next process changes depending on whether the given question is a single-
sentence question or a multiple-sentence question. If the question consists of a
single sentence, the question is sent directly to question classification component.
If the question consists of multiple sentences, the question is sent to core sentence
extraction component. In the component, a core sentence, which is defined as
the most important sentence for question classification, is extracted. Then, the
core sentence is sent to the question classification component and the question is
classified using the information in the core sentence. In Figure 1, ?core sentence
extraction? is peculiar to multiple-sentence questions.
3.1 Core Sentence Extraction
When a multiple-sentence question is given, the core sentence of the question is
extracted. For example, if the question ?I have studied the US history. Therefore,
I am looking for the web page that tells me what day Independence Day is.? is
given, the sentence ?Therefore, I am looking for the web page that tells me what
day Independence Day is.? is extracted as the core sentence.
With the core sentence extraction, we can eliminate noisy information before
question classification. In the above example, the occurrence of the sentence
Classification of Multiple-Sentence Questions 429
?I have studied the US history.? would be a misleading information in terms of
question classification.
Here, we have based our work on the following assumption: a multiple-
sentence question can be classified using only the core sentence. Please note
that we treat only questions which require one answer.
We explain the method for extracting a core sentence. Suppose we have a
classifier, which returns Score(Si) for each sentence Si of Question. Question is
the set of sentences composing a given question. Score(Si) indicates the likeliness
of Si being the core sentence. The sentence with the largest value is selected as
the core sentence:
Core sentence = argmaxSi?QuestionScore(Si). (1)
We then extract features for constructing a classifier which returns Score(Si).
We use the information on the words as features. Only the features from the
target sentence would not be enough for accurate classification. This issue is
exemplified by the following questions (core sentences are underlined).
? Question 1:
Please advise a medication effective for hay fever. I want to relieve my
headache and stuffy nose. Especially my headache is severe.
? Question 2:
I want to relieve my headache and stuffy nose. Especially my head-
ache is severe.
While the sentence ?I want to relieve my headache and stuffy nose.? written in
bold-faced type is the core sentence in Question 2, the sentence is not suitable as
the core sentence in Question 1. These examples show that the target sentence
alone is sometimes not a sufficient evidence for core sentence extraction.
Thus, in classification of a sentence, we use its preceding and following sen-
tences. For that purpose, we introduce a notion of window size. ?Window size is
n? means ?the preceding n sentences and the following n sentences in addition to
the target sentence are used to make a feature vector?. For example, if window
size is 0, we use only the target sentence. If window size is ?, we use all the
sentences in the question.
We use SVM as a classifier. We regard the functional distance from the
separating hyperplane (i.e., the output of the separating function) as Score(Si).
Word unigrams and word bigrams of the target sentence and the sentences in
the window are used as features. A word in the target sentence and the same
word in the other sentences are regarded as two different features.
3.2 Question Classification
As discussed in Section 2, we use SVM in the classification of questions. We use
five sets of features: word unigrams, word bigrams, semantic categories of nouns,
question focuses, and semantic categories of question focuses. The semantic cat-
egories are obtained from a thesaurus (e.g., SHOP, STATION, CITY).
430 A.Tamura, H. Takamura, and M. Okumura
?Question focus? is the word that determines the answer class of the ques-
tion. The notion of question focus was described by Moldovan et al [8]. For
instance, in the question ?What country is ???, the question focus is ?coun-
try?. In many researches, question focuses are extracted with hand-crafted rules.
However, since we treat all kinds of questions including the questions which are
not in an interrogative form, such as ?Please teach me ?? and ?I don?t know ??,
it is difficult to manually create a comprehensive set of rules. Therefore, in this
paper, we automatically find the question focus in a core sentence according to
the following steps :
step 1 find the phrase5 including the last verb of the sentence or the phrase
with ??? at the end.
step 2 find the phrase that modifies the phrase found in step 1.
step 3 output the nouns and the unknown words in the phrase found in step 2.
The output of this procedure is regarded as a question focus. Although this
procedure itself is specific to Japanese, we suppose that we can extract question
focus for other languages with a similar simple procedure.
4 Experiments
We designed experiments to confirm the effectiveness of the proposed method.
In the experiments, we use data in Japanese. We use a package for SVM
computation, TinySVM 6, and a Japanese morphological analyzer, ChaSen 7 for
word segmentation of Japanese text. We use CaboCha 8 to obtain dependency
relations, when a question focus is extracted from a question. Semantic categories
are obtained from a thesaurus ?Goitaikei? [9].
4.1 Experimental Settings
We collect questions from two Japanese Q&A sites: hatena9 and
Yahoo!tiebukuro10. 2000 questions are extracted from each site and experimental
data consist of 4000 questions in total. A Q&A site is the site where a user puts a
question on the site and other users answer the question on the site. Such Q&A
sites include many multiple-sentence questions in various forms. Therefore, those
questions are appropriate for our experiments where non-artificial questions are
required.
Here, we manually exclude the following three kinds of questions from the
dataset: questions whose answers are only Yes or No, questions which require two
5 Phrase here is actually Japanese bunsetsu phrase, which is the smallest meaningful
sequence consisting of an independent word and accompanying words.
6 http://chasen.org/?taku/software/TinySVM/
7 http://chasen.naist.jp/hiki/ChaSen/
8 http://chasen.org/?taku/software/cabocha/
9 http://www.hatena.ne.jp/
10 http://knowledge.yahoo.co.jp/
Classification of Multiple-Sentence Questions 431
Table 1. The types and the distribution of 2376 questions
Nominal Answer Non-nominal Answer
Question Type Number Question Type Number
PERSON 64 REASON 132
PRODUCT 238 WAY 500
FACILITY 139 DEFINITION 73
LOCATION 393 DESCRIPTION 228
TIME 108 OPINION 173
NUMBER 53 OTHERS (TEXT) 131
OTHERS (NOUN) 144
1139 1237
TOTAL 2376
or more answers, and questions which are not actually questions. This deletion
left us 2376 questions. The question types that we used and their numbers are
shown in Table 111. Question types requiring nominal answers are determined
referring to the categories used by Sasaki et al [1].
Of the 2376 questions, 818 are single-sentence questions and 1558 are
multiple-sentence questions. The average number of sentences in a multiple-
sentence question is 3.49. Therefore, the task of core sentence extraction in our
setting is to decide a core sentence from 3.49 sentences on the average. As an eval-
uation measure for core sentence extraction, we use accuracy, which is defined
as the number of multiple-sentence questions whose core sentence is correctly
identified over the number of all the multiple-sentence questions. To calculate
the accuracy, correct core sentence of the 2376 questions is manually tagged in
the preparation of the experiments.
As an evaluation measure for question classification, we use F-measure, which
is defined as 2?Recall?Precision / (Recall+Precision). As another evaluation
measure for question classification, we use also accuracy, which is defined as the
number of questions whose type is correctly classified over the number of the
questions. All experimental results are obtained with two-fold cross-validation.
4.2 Core Sentence Extraction
We conduct experiments of core sentence extraction with four different window
sizes (0, 1, 2, and ?) and three different feature sets (unigram, bigram, and
unigram+bigram). Table 2 shows the result.
As this result shows, we obtained a high accuracy, more than 90% for this
task. The accuracy is so good that we can use this result for the succeeding task
of question classification, which is our main target. This result also shows that
large widow sizes are better for core sentence extraction. This shows that good
clues for core sentence extraction are scattered all over the question.
11 Although Sasaki et al [1] includes ORGANIZATION in question types, ORGA-
NIZATION is integrated into OTHERS (NOUN) in our work because the size of
ORGANIZATION is small.
432 A.Tamura, H. Takamura, and M. Okumura
Table 2. Accuracy of core sentence extraction with different window sizes and features
Window Size\ Features Unigram Bigram Unigram+Bigram
0 1350/1558= 0.866 1378/1558= 0.884 1385/1558= 0.889
1 1357/1558= 0.871 1386/1558= 0.890 1396/1558= 0.896
2 1364/1558= 0.875 1397/1558= 0.897 1405/1558= 0.902
? 1376/1558= 0.883 1407/1558= 0.903 1416/1558= 0.909
Table 3. Accuracy of core sentence extraction with simple methodologies
Methodology Accuracy
First Sentence 743/1558= 0.477
Last Sentence 471/1558= 0.302
Interrogative Sentence 1077/1558= 0.691
The result in Table 2 also shows that unigram+bigram features are most
effective for any window size in core sentence extraction.
To confirm the validity of our proposed method, we extract core sentences
with three simple methodologies, which respectively extract one of the following
sentences as the core sentence : (1) the first sentence, (2) the last sentence,
and (3) the last interrogative sentence (or the first sentence). Table 3 shows the
result. The result shows that such simple methodologies would not work in core
sentence extraction.
4.3 Question Classification: The Effectiveness of Core Sentence
Extraction
We conduct experiments to examine whether the core sentence extraction is
effective for question classification or not. For that purpose, we construct the
following three models:
Plain question. The given question is the input of question classification com-
ponent without core sentence extraction process.
Predicted core sentence. The core sentence extracted by the proposed
method in Section 3.1 is the input of question classification component. The
accuracy of core sentence extraction process is 90.9% as mentioned in Sec-
tion 4.2.
Correct core sentence. The correct core sentence tagged by hand is the input
of question classification component. This case corresponds to the case when
the accuracy of core sentence extraction process is 100%.
Word unigrams, word bigrams, and semantic categories of nouns are used as
features. The features concerning question focus cannot be used for the plain
question model, because the method for identifying the question focus requires
that the input be one sentence. Therefore, in order to clarify the effectiveness of
core sentence extraction itself, through fair comparison we do not use question
focus for each of the three models in these experiments.
Classification of Multiple-Sentence Questions 433
Table 4. F-measure and Accuracy of the three models for question classification
Model Plain Question Predicted Core Sentence Correct Core Sentence
Accuracy Of
Core Sentence Extraction ? 0.909 1.000
PERSON 0.462 0.434 0.505
PRODUCT 0.381 0.467 0.480
FACILITY 0.584 0.569 0.586
LOCATION 0.758 0.780 0.824
TIME 0.340 0.508 0.524
NUMBER 0.262 0.442 0.421
OTHERS (NOUN) 0.049 0.144 0.145
REASON 0.280 0.539 0.579
WAY 0.756 0.778 0.798
DEFINITION 0.643 0.624 0.656
DESCRIPTION 0.296 0.315 0.317
OPINION 0.591 0.675 0.659
OTHERS (TEXT) 0.090 0.179 0.186
Average 0.423 0.496 0.514
Accuracy 0.617 0.621 0.652
Table 4 shows the result. For most question types, the proposed method
with a predicted core sentence improves F-measure. This result shows that the
core sentence extraction is effective in question classification. We can still expect
some more improvement of performance, by boosting accuracy of core sentence
extraction.
In order to further clarify the importance of core sentence extraction, we
examine the accuracy for the questions whose core sentences are not correctly
extracted. Of 142 such questions, 54 questions are correctly classified. In short,
the accuracy is 38% and very low. Therefore, we can claim that without accurate
core sentence extraction, accurate question classification is quite hard.
4.4 Question Classification: More Detailed Investigation of Features
Here we investigate the effectiveness of each set of features and the influence
of the preceding and the following sentences of the core sentence. After that,
we conduct concluding experiments. In the first two experiments of this section,
we use only the correct core sentence tagged by hand as the input of question
classification.
The Effectiveness of Each Feature Set
First, to examine which feature set is effective in question classification, we
exclude a feature set one by one from the five feature sets described in Section
3.2 and conduct experiments of question classification. Please note that the five
feature sets can be used unlike the last experiment (Table 4), because the input
of question classification is one sentence.
434 A.Tamura, H. Takamura, and M. Okumura
Table 5. Experiments with each feature set being excluded. Here ?sem. noun? means
semantic categories of nouns. ?sem. qf? means semantic categories of question focuses.
Excluded Feature Set
All Unigram Bigram Sem. noun Qf Sem. Qf
PERSON 0.574 0.571 0.620 0.536 0.505 0.505
(-0.003) (+0.046) (-0.038) (-0.069) (-0.069)
PRODUCT 0.506 0.489 0.579 0.483 0.512 0.502
(-0.017) (+0.073) (-0.023) (+0.006) (-0.004)
FACILITY 0.612 0.599 0.642 0.549 0.615 0.576
(-0.013) (+0.03) (-0.063) (+0.003) (-0.036)
LOCATION 0.832 0.826 0.841 0.844 0.825 0.833
(-0.006) (+0.009) (+0.012) (-0.007) (+0.001)
TIME 0.475 0.506 0.548 0.420 0.502 0.517
(+0.031) (+0.073) (-0.055) (+0.027) (+0.042)
NUMBER 0.442 0.362 0.475 0.440 0.466 0.413
(-0.080) (+0.033) (-0.002) (+0.024) (-0.029)
OTHERS (NOUN) 0.210 0.182 0.267 0.204 0.198 0.156
(-0.028) (+0.057) (-0.006) (-0.012) (-0.054)
REASON 0.564 0.349 0.622 0.603 0.576 0.582
(-0.215) (+0.058) (+0.039) (+0.012) (+0.018)
WAY 0.817 0.803 0.787 0.820 0.817 0.807
(-0.014) (-0.030) (+0.003) (?0.000) (-0.010)
DEFINITION 0.652 0.659 0.603 0.640 0.647 0.633
(+0.007) (-0.049) (-0.012) (-0.005) (-0.019)
DESCRIPTION 0.355 0.308 0.355 0.363 0.357 0.334
(-0.047) (?0.000) (+0.008) (+0.002) (-0.021)
OPINION 0.696 0.670 0.650 0.703 0.676 0.685
(-0.026) (-0.046) (+0.007) (-0.020) (-0.011)
OTHERS (TEXT) 0.183 0.176 0.179 0.154 0.190 0.198
(-0.007) (-0.004) (-0.029) (+0.007) (+0.015)
Average 0.532 0.500 0.551 0.520 0.530 0.518
(-0.032) (+0.019) (-0.012) (-0.002) (-0.014)
Accuracy 0.674 0.632 0.638 0.668 0.661 0.661
Table 5 shows the result. The numbers in parentheses are differences of
F-measure compared with its original value. The decrease of F-measure suggests
the effectiveness of the excluded feature set.
We first discuss the difference of F-measure values in Table 5, by taking
PRODUCT and WAY as examples. The F-measure of PRODUCT is much
smaller than that of WAY. This difference is due to whether characteristic ex-
pressions are present in the type or not. In WAY, words and phrases such as
?method? and ?How do I - ?? are often used. Such words and phrases work as
good clues for classification. However, there is no such characteristic expressions
for PRODUCT. Although there is a frequently-used expression ?What is [noun] -
??, this expression is often used also in other types such as LOCATION and FA-
CILITY. We have to rely on currently-unavailable world knowledge of whether
the noun is a product name or not. This is the reason of the low F-measure for
PRODUCT.
We next discuss the difference of effective feature sets according to question
types. We again take PRODUCT and WAY as examples. The most effective
Classification of Multiple-Sentence Questions 435
Table 6. Experiments with different window sizes
Window Size
0 1 2 ?
PERSON 0.574 0.558 0.565 0.570
PRODUCT 0.506 0.449 0.441 0.419
FACILITY 0.612 0.607 0.596 0.578
LOCATION 0.832 0.827 0.817 0.815
TIME 0.475 0.312 0.288 0.302
NUMBER 0.442 0.322 0.296 0.311
OTHERS (NOUN) 0.210 0.123 0.120 0.050
REASON 0.564 0.486 0.472 0.439
WAY 0.817 0.808 0.809 0.792
DEFINITION 0.652 0.658 0.658 0.641
DESCRIPTION 0.355 0.358 0.357 0.340
OPINION 0.696 0.670 0.658 0.635
OTHERS (TEXT) 0.183 0.140 0.129 0.133
Average 0.532 0.486 0.477 0.463
Accuracy 0.674 0.656 0.658 0.653
feature set is semantic categories of nouns for ?PRODUCT? and bigrams for
?WAY?. Since whether a noun is a product name or not is important for PROD-
UCT as discussed before, semantic categories of nouns are crucial to PRODUCT.
On the other hand, important clues for WAY are phrases such as ?How do I?.
Therefore, bigrams are crucial to WAY.
Finally, we discuss the effectiveness of a question focus. The result in Table
5 shows that the F-measure does not change so much even if question focuses or
their semantic categories are excluded. This is because both question focuses and
their semantic categories are redundantly put in the feature sets. By comparing
Tables 4 and 5, we can confirm that question focuses improve question classifi-
cation performance (F-measure increases from 0.514 to 0.532). Please note again
that question focuses are not used in Table 4 for fair comparison.
The Influence of Window Size
Next, we clarify the influence of window size. As in core sentence extraction,
?Window size is n? means that ?the preceding n sentences and the following
n sentences in addition to the core sentence are used to make a feature vec-
tor?. We construct four models with different window sizes (0, 1, 2, and ?)
and compare their experimental results. In this experiment, we use five sets of
features and correct core sentence as the input of question classification like the
last experiment (Table 5).
Table 6 shows the result of the experiment. The result in Table 6 shows that
the model with the core sentence alone is best. Therefore, the sentences other
than the core sentence are considered to be noisy for classification and would
not contain effective information for question classification. This result suggests
that the assumption (a multiple-sentence question can be classified using only
the core sentence) described in Section 3.1 be correct.
436 A.Tamura, H. Takamura, and M. Okumura
Table 7. The result of concluding experiments
Plain Question The Proposed Method
core sentence extraction No Yes
feature sets unigram, bigram unigram,bigram,qf
sem. noun sem. noun,sem. qf
PERSON 0.462 0.492
PRODUCT 0.381 0.504
FACILITY 0.584 0.575
LOCATION 0.758 0.792
TIME 0.340 0.495
NUMBER 0.262 0.456
OTHERS (NOUN) 0.049 0.189
REASON 0.280 0.537
WAY 0.756 0.789
DEFINITION 0.643 0.626
DESCRIPTION 0.296 0.321
OPINION 0.591 0.677
OTHERS (TEXT) 0.090 0.189
Average 0.423 0.511
Accuracy 0.617 0.661
Concluding Experiments
We have so far shown that core sentence extraction and question focuses work
well for question classification. In this section, we conduct concluding experi-
ments which show that our method significantly improves the classification per-
formance. In the discussion on effective features, we used correct core sentences.
Here we use predicted core sentences.
The result is shown in Table 7. For comparison, we add to this table the
values of F-measure in Table 4, which correspond to plain question (i.e., without
core sentence extraction). The result shows that F-measure of most categories
increase, except for FACILITY and DEFINITION. From comparison of ?All?
in Table 5 with Table 7, the reason of decrease would be the low accuracies of
core sentence extraction for these categories. As shown in this table, in conclu-
sion, we obtained 8.8% increase of average F-measure of all and 4.4% increase of
accuracy, which is statistically significant in the sign-test with 1% significance-
level.
Someone may consider that the type of multiple-sentence questions can be
identified by ?one-step? approach without core sentence extraction. In a word,
the question type of each sentence in the given multiple-sentence question is
first identified by a classifier, and then the type of the sentence for which the
classifier outputs the largest score is selected as the type of the given question.
The classifier?s output indicates the likeliness of being the question type of a
given question. Therefore, we compared the proposed model with this model
in the preliminary experiment. The accuracy of question classification with the
proposed model is 66.1% (1570/2376), and that of the one-step approach is
61.7% (1467/2376). This result shows that our two-step approach is effective for
classification of multiple-sentence questions.
Classification of Multiple-Sentence Questions 437
5 Conclusions
In this paper, we proposed a method for identifying the types of multiple-
sentence questions. In our method, the core sentence is first extracted from a
given multiple-sentence question and then used for question classification.
We obtained accuracy of 90.9% in core sentence extraction and empirically
showed that larger window sizes are more effective in core sentence extraction.
We also showed that the extracted core sentences and the question focuses are
good for question classification. Core sentence extraction is quite important also
in the sense that question focuses could not be introduced without core sentences.
With the proposed method, we obtained the 8.8% increase of F-measure and
4.4% increase of accuracy.
Future work includes the following. The question focuses extracted in the
proposed method include nouns which might not be appropriate for question
classification. Therefore, we regard the improvement on the question focus detec-
tion as future work. To construct a QA system that can handle multiple-sentence
question, we are also planning to work on the other components: document re-
trieval, answer extraction.
References
1. Yutaka Sasaki, Hideki Isozaki, Tsutomu Hirao, Koji Kokuryou, and Eisaku Maeda:
NTT?s QA Systems for NTCIR QAC-1. Working Notes, NTCIR Workshop 3, Tokyo,
pp. 63?70, 2002.
2. Jinxi Xu, Ana Licuanan, and Ralph M.Weischedel: TREC 2003 QA at BBN: An-
swering Definitional Questions. TREC 2003, pp. 98?106, 2003.
3. Xin Li and Dan Roth: Learning Question Classifiers. COLING 2002, Taipei, Taiwan,
pp. 556?562, 2002.
4. Ingrid Zukerman and Eric Horvitz: Using Machine Learning Techniques to Interpret
WH-questions. ACL 2001, Toulouse, France, pp. 547?554, 2001.
5. Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and Adwait Ratnaparkhi: Ques-
tion Answering Using Maximum Entropy Components. NAACL 2001, pp. 33?39,
2001.
6. Jun Suzuki: Kernels for Structured Data in Natural Language Processing, Doctor
Thesis, Nara Institute of Science and Technology, 2005.
7. Dell Zhang and Wee Sun Lee: Question Classification using Support Vector Ma-
chines. SIGIR, Toronto, Canada, pp. 26?32, 2003.
8. Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum,
Roxana Girju, and Vasile Rus: Lasso: A Tool for Surfing the Answer Net. TREC-8,
pp. 175?184, 1999.
9. Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa,
Kentaro Ogura, Yoshifumi Oyama, and Yoshihiko Hayashi, editors: The Semantic
System, volume 1 of Goi-Taikei ? A Japanese Lexicon. Iwanami Shoten, 1997 (in
Japanese).
Identifying Cross-Document Relations between Sentences
Yasunari Miyabe ?? Hiroya Takamura ? Manabu Okumura ?
?Interdisciplinary Graduate School of Science and Engineering,
Tokyo Institute of Technology, Japan
?Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
miyabe@lr.pi.titech.ac.jp, {takamura,oku}@pi.titech.ac.jp
Abstract
A pair of sentences in different newspaper
articles on an event can have one of sev-
eral relations. Of these, we have focused on
two, i.e., equivalence and transition. Equiv-
alence is the relation between two sentences
that have the same information on an event.
Transition is the relation between two sen-
tences that have the same information except
for values of numeric attributes. We pro-
pose methods of identifying these relations.
We first split a dataset consisting of pairs
of sentences into clusters according to their
similarities, and then construct a classifier
for each cluster to identify equivalence re-
lations. We also adopt a ?coarse-to-fine? ap-
proach. We further propose using the identi-
fied equivalence relations to address the task
of identifying transition relations.
1 Introduction
A document generally consists of semantic units
called sentences and various relations hold between
them. The analysis of the structure of a document by
identifying the relations between sentences is called
discourse analysis.
The discourse structure of one document has
been the target of the traditional discourse anal-
ysis (Marcu, 2000; Marcu and Echihabi, 2002;
Yokoyama et al, 2003), based on rhetorical struc-
ture theory (RST) (Mann and Thompson, 1987).
?Yasunari Miyabe currently works at Toshiba Solutions Cor-
poration.
Inspired by RST, Radev (2000) proposed the
cross-document structure theory (CST) for multi-
document analysis, such as multi-document summa-
rization, and topic detection and tracking. CST takes
the structure of a set of related documents into ac-
count. Radev defined relations that hold between
sentences across the documents on an event (e.g., an
earthquake or a traffic accident).
Radev presented a taxonomy of cross-document
relations, consisting of 24 types. In Japanese, Etoh
et al (2005) redefined 14 CST types based on
Radev?s taxonomy. For example, a pair of sentences
with an ?equivalence relation? (EQ) has the same
information on an event. EQ can be considered to
correspond to the identity and equivalence relations
in Radev?s taxonomy. A sentence pair with a ?tran-
sition relation? (TR) contains the same numeric at-
tributes with different values. TR roughly corre-
sponds to the follow-up and fulfilment relations in
Radev?s taxonomy. We will provide examples of
CST relations:
1. ABC telephone company announced on the 9th
that the number of users of its mobile-phone
service had reached one million. Users can ac-
cess the Internet, reserve train tickets, as well
as make phone calls through this service.
2. ABC said on the 18th that the number of
users of its mobile-phone service had reached
1,500,000. This service includes Internet ac-
cess, and enables train-ticket reservations and
telephone calls.
The pair of the first sentence in 1 and the first sen-
tence in 2 is in TR, because the number of users
141
has changed from one million to 1.5 millions, while
other things remain unchanged. The pair of the sec-
ond sentence in 1 and the second sentence in 2 is
in EQ, because these two sentences have the same
information.
Identification of CST relations has attracted more
attention since the study of multi-document dis-
course emerged. Identified CST types are helpful
in various applications such as multi-document sum-
marization and information extraction. For example,
EQ is useful for detecting and eliminating redundant
information in multi-document summarization. TR
can be used to visualize time-series trends.
We focus on the two relations EQ and TR in the
Japanese CST taxonomy, and present methods for
their identification. For the identification of EQ
pairs, we first split a dataset consisting of sentence
pairs into clusters according to their similarities, and
then construct a classifier for each cluster. In addi-
tion, we adopt a coarse-to-fine approach, in which a
more general (coarse) class is first identified before
the target fine class (EQ). For the identification of TR
pairs, we use variable noun phrases (VNPs), which
are defined as noun phrases representing a variable
with a number as its value (e.g., stock prices, and
population).
2 Related Work
Hatzivassiloglou et al (1999; 2001) proposed a
method based on supervised machine learning to
identify whether two paragraphs contain similar in-
formation. However, we found it was difficult to
accurately identify EQ pairs between two sentences
simply by using similarities as features. Zhang et
al. (2003) presented a method of classifying CST
relations between sentence pairs. However, their
method used the same features for every type of
CST, resulting in low recall and precision. We thus
select better features for each CST type, and for each
cluster of EQ.
The EQ identification task is apparently related to
Textual Entailment task (Dagan et al, 2005). Entail-
ment is asymmetrical while EQ is symmetrical, in
the sense that if a sentence entails and is entailed by
another sentence, then this sentence pair is in EQ.
However in the EQ identification, we usually need
to find EQ pairs from an extremely biased dataset of
sentence pairs, most of which have no relation at all.
3 Identification of EQ pairs
This section explains a method of identifying EQ
pairs. We regarded the identification of a CST re-
lation as a standard binary classification task. Given
a pair of sentences that are from two different but
related documents, we determine whether the pair
is in EQ or not. We use Support Vector Machines
(SVMs) (Vapnik, 1998) as a supervised classifier.
Please note that one instance consists of a pair of two
sentences. Therefore, a similarity value between two
sentences is only given to one instance, not two.
3.1 Clusterwise Classification
Although some pairs in EQ have quite high similar-
ity values, others do not. Simultaneously using both
of these two types of pairs for training will adversely
affect the accuracy of classification. Therefore, we
propose splitting the dataset first according to sim-
ilarities of pairs, and then constructing a classifier
for each cluster (sub-dataset). We call this method
clusterwise classification.
We use the following similarity in the cosine mea-
sure between two sentences (s1, s2):
cos(s1, s2) = u1 ? u2/|u1||u2|, (1)
where u1 and u2 denote the frequency vectors of
content words (nouns, verbs, adjectives) for respec-
tive s1 and s2. The distribution of the sentence pairs
according to the cosine measure is summarized in
Table 1. From the table, we can see a large dif-
ference in distributions of EQ and no-relation pairs.
This difference suggests that the clusterwise classi-
fication approach is reasonable.
We split the dataset into three clusters: high-
similarity cluster, intermediate-similarity cluster,
and low-similarity cluster. Intuitively, we ex-
pected that a pair in the high-similarity cluster
would have many common bigrams, that a pair in
the intermediate-similarity cluster would have many
common unigrams but few common bigrams, and
that a pair in the low-similarity cluster would have
few common unigrams or bigrams.
3.2 Two-Stage Identification Method
The number of sentence pairs in EQ in the
intermediate- or low-similarity clusters is much
142
Table 1: The distribution of sentence pairs according to the cosine measure (NO indicates pairs with no
relation. The pairs with other relations are not on the table due to the space limitation)
cos (0.0, 0.1] (0.1, 0.2] (0.2, 0.3] (0.3, 0.4] (0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] (0.9, 1.0]
EQ 12 13 21 25 37 61 73 61 69 426
summary 5 5 25 19 22 13 16 6 6 0
refinement 3 4 15 11 12 15 6 6 3 2
NO 194938 162221 68283 28152 11306 4214 1379 460 178 455
Figure 1: Method of identifying EQ pairs
smaller than the total number of sentence pairs as
shown in Table 1. These two clusters also contain
many pairs that belong to a ?summary? and a ?re-
finement? relation, which are very much akin to EQ.
This may cause difficulties in identifying EQ pairs.
We gave a generic name, GEN(general)-EQ, to
the union of EQ, ?summary?, and ?refinement? re-
lations. For pairs in the intermediate- or low-
similarity clusters, we propose a two-stage method
using GEN-EQ on the basis of the above observa-
tions, which first identifies GEN-EQ pairs between
sentences, and then identifies EQ pairs from GEN-
EQ pairs.
This two-stage method can be regarded as a
coarse-to-fine approach (Vanderburg and Rosenfeld,
1977; Rosenfeld and Vanderbrug, 1977), which first
identifies a coarse class and then finds the target fine
class. We used the coarse-to-fine approach on top of
the clusterwise classification method as in Fig. 1.
There are by far less EQ pairs than pairs without
relation. This coarse-to-fine approach will reduce
this bias, since GEN-EQ pairs outnumber EQ pairs.
3.3 Features for identifying EQ pairs
Instances (i.e., pairs of sentences) are represented as
binary vectors. Numeric features ranging from 0.0
to 1.0 are discretized and represented by 10 binary
features (e.g., a feature value of 0.65 is transformed
into the vector 0000001000). Let us first explain ba-
sic features used in all clusters. We will then explain
other features that are specific to a cluster.
3.3.1 Basic features
1. Cosine similarity measures: We use unigram, bi-
gram, trigram, bunsetsu-chunk1 similarities at all the
sentence levels, and unigram similarities at the para-
graph and the document levels. These similarities
are calculated by replacing u1 and u2 in Eq. (1) with
the frequency vectors of each sentence level.
2. Normalized lengths of sentences: Given an in-
stance of sentence pair s1 and s2, we can define fea-
tures normL(s1) and normL(s2), which represent
(normalized) lengths of sentences, as:
normL(s) = len(s)/EventMax(s), (2)
where len(s) is the number of characters in
s. EventMax(s) is maxs??event(s) len(s?), where
event(s) is the set of sentences in the event that
doc(s) describes. doc(s) is the document contain-
ing s.
3. Difference in publication dates: This feature de-
pends on the interval between the publication dates
of doc(s1) and doc(s2) and is defined as:
DateDiff(s1, s2) = 1 ?
|Date(s1) ? Date(s2)|
EventSpan(s1, s2)
, (3)
where Date(s) is the publication date of an arti-
cle containing s, and EventSpan(s1, s2) is the time
span of the event, i.e., the difference between the
publication dates for the first and the last articles that
are on the same event. For example, if doc(s1) is
published on 1/15/99 and doc(s2) on 1/17/99, and
if the time span of the event ranges from 1/1/99 to
1/21/99, then the feature value is 1-2/20 = 0.9.
1Bunsetsu-chunks are Japanese phrasal units usually con-
sisting of a pair of a noun phrase and a case marker.
143
4. Positions of sentences in documents (Edmund-
son, 1969): This feature is defined as
Posit(s) = lenBef(s)/len(doc(s)), (4)
where lenBef(s) is the number of characters be-
fore s in the document, and len(doc(s)) is the total
number of characters in doc(s).
5. Semantic similarities: This feature is measured by
Eq. (1) with u1 and u2 being the frequency vectors
of semantic classes of nouns, verbs, and adjectives.
We used the semantic classes in a Japanese thesaurus
called ?Goi-taikei? (Ikehara et al, 1997).
6. Conjunction (Yokoyama et al, 2003): Each of 55
conjunctions corresponds to one feature. If a con-
junction appears at the beginning of the sentence,
the feature value is 1, otherwise 0.
7. Expressions at the end of sentences: Yokoyama
et al (2003) created rules that map sentence endings
to their functions. Each function corresponds to a
feature. If a function appears in the sentence, the
value of the feature for the function is 1, otherwise 0.
Functions of sentence endings are past, present, as-
sertion, existence, conjecture, interrogation, judge-
ment, possibility, reason, request, description, duty,
opinion, continuation, causation, hearsay, and mode.
8. Named entity: This feature represents sim-
ilarities measured through named entities in the
sentences. Its value is measured by Eq. (1)
with u1 and u2 being the frequency vectors of the
named entities. We used the named-entity chun-
ker bar2. The types of named entities are ARTI-
FACT?DATE?ORGANIZATION?MONEY?LO-
CATION?TIME?PERCENT?and PERSON.
9. Types of named entities with particle: This fea-
ture represents the occurrence of types of named en-
tities accompanied by a case marker (particle). We
used 11 different case markers.
3.3.2 Additional features to identify fine class
We will next explain additional features used in
identifying EQ pairs from GEN-EQ pairs.
1. Numbers of words (morphemes) and phrases:
These features represent the closeness of the num-
bers of words and bunsetsu-chunks in the two sen-
tences. This feature is defined as:
2http://chasen.naist.jp/?masayu-a/p/bar/
NumW (s1, s2) = 1 ?
|frqW (s1) ? frqW (s2)|
max(frqW (s1), frqW (s2))
, (5)
where frqW (s) indicates the number of words in
s. Similarly, NumP (s1, s2) is obtained by replac-
ing frqW in Eq. (5) with frqP , where frqP (s)
indicates the number of phrases in s.
2. Head verb: There are three features of this kind.
The first indicates whether the two sentences have
the same head verb or not. The second indicates
whether the two sentences have a semantically sim-
ilar head verb or not. If the two verbs have the
same semantic class in a thesaurus, they are re-
garded as being semantically similar. The last in-
dicates whether both sentences have a verb or not.
The head verbs are extracted using rules proposed
by Hatayama (2001).
3. Salient words: This feature indicates whether the
salient words of the two sentences are the same or
not. We approximate the salient word with the ga-
or the wa-case word that appears first.
4. Numeric expressions and units (Nanba et al,
2005): The first feature indicates whether the two
sentences share a numeric expression or not. The
second feature is similarly defined for numeric units.
4 Experiments on identifying EQ pairs
We used the Text Summarization Challenge (TSC) 2
and 3 corpora (Okumura et al, 2003) and the Work-
shop on Multimodal Summarization for Trend Infor-
mation (Must) corpus (Kato et al, 2005). These two
corpora contained 115 sets of related news articles
(10 documents per set on average) on various events.
A document contained 9.9 sentences on average.
Etoh et al (2005) annotated these two corpora with
CST types. There were 471,586 pairs of sentences
and 798 pairs of these had EQ. We conducted the
experiments with 10-fold cross-validation (i.e., ap-
proximately 425,000 pairs on average, out of which
approximately 700 pairs are in EQ, are in the train-
ing dataset for each fold). The average, maximum,
and minimum lengths of the sentences in the whole
datset are shown in Table 2. We used precision,
recall, and F-measure as evaluation measures. We
used a Japanese morphological analyzer ChaSen3 to
3http://chasen.naist.jp/hiki/Chasen/
144
Table 2: Average, max, min lengths of the sentences
in the dataset
average max min
# of words 33.27 458 1
# of characters 111.22 1107 2
extract parts-of-speech. and a dependency analyzer
CaboCha4 to extract bunsetsu-chunks.
4.1 Estimation of threshold
We split the set of sentence pairs into clusters ac-
cording to their similarities in identifying EQ pairs
as explained. We used 10-fold cross validation again
within the training data (i.e., the approximately
425,000 pairs above are split into a temporary train-
ing dataset and a temporary test dataset 10 times) to
estimate the threshold to split the set, to select the
best feature set, and to determine the degree of the
polynomial kernel function and the value for soft-
margin parameter C in SVMs. No training instances
are used in the estimation of these parameters.
4.1.1 Threshold between high- and
intermediate-similarity clusters
We will first explain how to estimate the threshold
between high- and intermediate-similarity clusters.
We expected that a pair in high-similarity cluster
would have many common bigrams, and that a pair
in intermediate-similarity cluster would have many
common unigrams but few common bigrams. We
therefore assumed that bigram similarity would be
ineffective in intermediate-similarity cluster.
We determined the threshold in the following way
for each fold of cross-validation. We decreased the
threshold by 0.01 from 1.0. We carried out 10-fold
cross-validation within the training data, excluding
one of the 14 features (6 cosine similarities and other
basic features) for each value of the threshold. If
the exclusion of a feature type deteriorates both av-
erage precision and recall obtained by the cross-
validation within the training data, we call it ineffec-
tive. We set the threshold to the minimum value for
which bigram similarity is not ineffective. We obtain
a threshold value for each fold of cross-validation.
The average value of threshold was 0.87.
4http://chasen.naist.jp/?taku/software/cabocha/
Table 3: Ineffective feature types for each threshold
threshold ineffective features
0.90 particle, bunsetsu-chunk similarity, semantic similarity
0.89
semantic similarity, expression at end of sentences,
bigram similarity, particle
0.88 bigram similarity
0.87
difference in publication dates, similarity between documents,
expression at end of sentences, number of tokens,
bigram similarity, similarity between paragraphs,
positions of sentences, particle
0.86 particle, similarity between documents, bigram similarity
Table 4: F-measure calculated by cross-validation
within the training data for each threshold in
?intermediate-similarity cluster?
threshold precision recall F-measure
0.60 49,71 14.95 22.99
0.59 52.92 15.05 23.44
0.58 55.08 16.64 25.56
0.57 52.81 16.93 25.64
0.56 49.15 14.45 22.34
0.55 51.51 14.84 23.04
0.54 51.89 15.21 23.52
0.53 54.59 13.61 21.78
As an example, we show the table of obtained
ineffective feature types for one fold of cross-
validation (Table 3). The threshold was set to 0.90
in this fold.
4.1.2 Threshold between intermediate- and
low-similarity clusters
We will next explain how to estimate the threshold
between intermediate- and low-similarity clusters.
There are numerous no-relation pairs in low-
similarity pairs. We expected that this imbalance
would adversely affect classification. We therefore
simply attemted to exclude low-similarity pairs. We
decreased the threshold by 0.01 from the threshold
between high- and intermediate-similarity clusters.
We chose a value that yielded the best average F-
measure calculated by the cross-validation within
the training data. The average value of the thresh-
old was 0.57. Table 4 is an example of thresholds
and F-measures for one fold.
4.2 Results of identifying EQ pairs
The results of EQ identification are shown in Ta-
ble 5. We tested the following models:
Bow-cos: This is the simplest baseline we used. We represented
sentences with bag-of-words model. Instances with the cosine
similarity in Eq. (1) larger than a threshold were classified as
EQ. The threshold that yielded the best F-measure in the test
145
Table 5: Results of identifying EQ pairs
precision recall F-measure
Bow-cos 87.29 57.35 69.22
basic features
Clusterwise 81.98 59.40 68.88
Non-Clusterwise 86.10 59.49 70.36
ClusterC2F 94.96 62.27 75.22
with additional features
Clusterwise 80.93 59.74 68.63
Non-Clusterwise 86.11 60.16 70.84
ClusterC2F 94.99 62.65 75.50
Table 6: Results with basic features
Results for ?high-similarity cluster?
precision recall F-measure
Clusterwise 94.23 96.83 95.51
Non-clusterwise 95.51 96.29 95.90
ClusterC2F 94.23 96.83 95.51
Results for ?intermediate-similarity cluster?
Clusterwise 42.77 23.03 29.94
Non-clusterwise 53.46 25.31 34.36
ClusterC2F 100.00 36.29 53.25
data was chosen.
Non-Clusterwise: This is a supervised method without the
clusterwise approach. One classifier was constructed regard-
less of the similarity of the instance. We used the second degree
polynomial kernel. Soft margin parameter C was set to 0.01.
Clusterwise: This is a clusterwise method without the coarse-
to-fine approach. The second degree polynomial kernel was
used. Soft margin parameter C was set to 0.1 for high-similarity
cluster and 0.01 for the other clusters.
ClusterC2F: This is our model, which integrates clusterwise
classification with the coarse-to-fine approach (Figure 1).
Table 5 shows that ClusterC2F yielded the best
F-measure regardless of presence of additional fea-
tures. The difference between ClusterC2F and the
others was statistically significant in the Wilcoxon
signed rank sum test with 5% significance level.
4.3 Results for each cluster
We examined the results for each cluster. The re-
sults with basic features are summarized in Table 6
and those with basic features plus additional fea-
tures are in Table 7. The tables show that there
are no significant differences among the models
for high-similarity cluster. However, there are sig-
nificant differences for intermediate-similarity clus-
ter. We thus concluded that the proposed model
(ClusterC2F) works especially well in intermediate-
similarity cluster.
Table 7: Results with additional features
Results for ?high-similarity cluster?
precision recall F-measure
Clusterwise 94.23 96.83 95.51
Non-clusterwise 95.70 96.76 96.23
ClusterC2F 94.23 96.83 95.51
Results for ?intermediate-similarity cluster?
Clusterwise 39.77 22.93 29.09
Non-clusterwise 55.61 26.81 36.18
ClusterC2F 100.00 38.06 55.13
5 Identification of TR pairs
We regarded the identification of the relations be-
tween sentences as binary classification, whether a
pair of sentences is classified into TR or not. We
used SVMs (Vapnik, 1998).
The sentence pairs in TR have the same numeric
attributes with different values, as mentioned in In-
troduction. Therefore, VNPs will be good clues for
the identification.
5.1 Extraction of VNPs
We extract VNPs in the following way.
1. Search for noun phrases that have numeric ex-
pressions (we call them numeric phrases).
2. Search for the phrases that the numeric phrases
depend on (we call them predicate phrases).
3. Search for the noun phrases that depend on the
predicate phrases.
4. Extract the noun phrases that depend on the
noun phrases found in step 3, except for date expres-
sions. Both the extracted noun phrases and the noun
phrases found in step 3 were regarded as VNPs.
In the example in Introduction, ?one million? and
?1,500,000? are numeric phrases, and ?had reached?
is a predicate phrase. Then, ?the number of users of
its mobile-phone service? is a VNP.
5.2 Features for identifying TR pairs
We used some features used in EQ identification:
sentence-level uni-, bi-, tirgrams, and bunsetsu-
chunk unigrams, normalized lengths of sentences,
difference in publication dates, position of sentences
in documents, semantic similarities, conjunctions,
expressions at the end of sentences, and named enti-
ties. In addition, we use the following features.
1. Similarities through VNPs: The cosine similarity
of the frequency vectors of nouns in the VNPs in s1
146
and s2 is used. If there are more than one VNP, the
largest cosine similarity is chosen.
2. Similarities through bigrams and trigrams in
VNPs: These features are defined similarly to the
previous feature, but each VNP is represented by the
frequency vector of word bi- and trigrams.
3. Similarities of noun phrases in nominative case:
Instances in TR often have similar subjects. A noun
phrase containing a ga-, wa-, or mo-case is regarded
as the subject phrase of a sentence. The similarity is
calculated by Eq. (1) with the frequency vectors of
nouns in the phrase.
4. Changes in value of numeric attributes: This fea-
ture is 1 if the values of the numeric phrases in the
two sentences are different, otherwise 0.
5. Presence of numerical units: If a numerical unit
is present in both sentences, the value of the feature
is 1, otherwise 0.
6. Expressions that mean changes in value: In-
stances in TR often contain those expressions, such
as ?reduce? and ?increase? (Nanba et al, 2005). We
have three features for each of these expressions.
The first feature is 1 if both sentences have the ex-
pression, otherwise 0. The second is 1 if s1 has the
expression, otherwise 0. The third is 1 if s2 has the
expression, otherwise 0.
7. Predicates: We define one feature for a predicate.
The value of this feature is 1 if the predicate appears
in the two sentences, otherwise 0.
8. Reporter: This feature represents who is report-
ing the incident. This feature is represented by the
cosine similarity between the frequency vectors of
nouns in phrases respectively expressing reporters in
s1 and s2. The subjects of verbs such as ?report? and
?announce? are regarded as phrases of the reporter.
5.3 Use of EQ
A pair of sentences in TR often has a high degree
of similarity. Such pairs are likely to be confused
with pairs in EQ. We used the identified EQ pairs for
the identification of TR in order to circumvent this
confusion. Pairs classified as EQ with our method
were excluded from candidates for TR.
Table 8: Results of identifying TR pairs
precision recall F-measure
Bow-cos 27.44 41.26 32.96
NANBA 19.85 45.96 27.73
WithoutEq 42.41 47.06 44.61
WithEq 43.13 48.51 45.67
WithEqActual 43.06 48.55 45.64
6 Experiments on identifying TR pairs
Most experimental settings are the same as in the ex-
periments of EQ identification. Sentence pairs with-
out numeric expressions were excluded in advance
and 55,547 pairs were left. This exclusion process
does not degrade recall at all, because TR pairs by
definition contain numberic expressions.
We used precision, recall and F-measure for eval-
uation. We employed 10-fold cross validation.
6.1 Results of identifying TR pairs
The results of the experiments are summarized in
Table 8. We compared four following models with
ours. A linear kernel was used in SVMs and soft
margin parameter C was set to 1.0 for all models:
Bow-cos (baseline): We calculated the similarity through
VPNs. If the similarity was larger than a threshold and the two
sentences had the same expressions meaning changes in value
and had different values, then this pair was classified as TR. The
threshold was set to 0.7, which yielded the best F-measure in the
test data.
NANBA (Nanba et al, 2005): If the unigram cosine similarity
between the two sentences was larger than a threshold and the
two sentences had expressions meaning changes in value, then
this pair was classified as TR. The value of the threshold was set
to 0.42, which yielded the best F-measure in the test data.
WithEq (Our method): This model uses the identified EQ
pairs.
WithoutEq: This model uses no information on EQ.
WithEqActual: This model uses the actual EQ pairs given by
oracle.
The results in Table 8 show that bow-cos is better
than NANBA in F-measure. This result suggests that
focusing on VNPs is more effective than a simple
bag-of-words approach.
WithEq and WithEqActual were better than With-
outEq. This suggests that we successfully excluded
EQ pairs, which are TR look-alikes. WithEq and
WithEqActual yielded almost the same F-measure.
This means that our EQ identifier was good enough
147
to improve the identification of TR pairs.
7 Conclusion
We proposed methods for identifying EQ and TR
pairs in different newspaper articles on an event.
We empirically demonstrated that the methods work
well in this task.
Although we focused on resolving a bias in the
dataset, we can expect that the classification perfor-
mance will improve by making use of methods de-
veloped in different but related tasks such as Textual
Entailment recognition on top of our method.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment, pages
177?190.
Harold Edmundson. 1969. New methods in automatic
extracting. Journal of ACM, 16(2):246?285.
Junji Etoh and Manabu Okumura. 2005. Making
cross-document relationship between sentences cor-
pus. In Proceedings of the Eleventh Annual Meeting
of the Association for Natural Language Processing
(in Japanese), pages 482?485.
Mamiko Hatayama, Yoshihiro Matsuo, and Satoshi Shi-
rai. 2001. Summarizing newspaper articles using ex-
tracted information and functional words. In 6th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS2001), pages 593?600.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combi-
nations via machine learning. In Proceedings of the
Empirical Methods for Natural Language Processing,
pages 203?212.
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.
Holcombe, Regina Barzilay, Min-Yen Kan, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible clus-
tering tool for summarization. In Proceedings of the
Workshop on Automatic Summarization, pages 41?49.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Oyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ? A
Japanese Lexicon (in Japanese). Iwanami Shoten.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. Must:a workshop on multimodal sum-
marization for trend information. In Proceedings of
the NTCIR-5 Workshop Meeting, pages 556?563.
William Mann and Sandra Thompson. 1987. Rhetorical
structure theory: Description and construction of text
structures. In Gerard Kempen, editor, Natural Lan-
guage Generation: New Results in Artificial Intelli-
gence, Psychology, and Linguistics, pages 85?96. Ni-
jhoff, Dordrecht.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
368?375.
Daniel Marcu. 2000. The rhetorical parsing of un-
restricted texts a surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
In Information Processing Society of Japan, Special
Interest Group on Natural Language Processing
(IPSJ-SIGNL), NL-168 (in Japanese), pages 67?74.
Manabu Okumura, Takahiro Fukushima, and Hidetsugu
Nanba. 2003. Text summarization challenge 2 -
text summarization evaluation at ntcir workshop 3.
In HLT-NAACL 2003 Workshop: Text Summarization
(DUC03), pages 49?56.
Dragomir Radev. 2000. A common theory of infor-
mation fusion from multiple text sources, step one:
Cross-document structure. In Proceedings of the 1st
ACL SIGDIAL Workshop on Discourse and Dialogue,
pages 74?83.
Azriel Rosenfeld and Gorden Vanderbrug. 1977.
Coarse-fine template matching. IEEE transactions
Systems, Man, and Cybernetics, 7:104?107.
Gorden Vanderburg and Azriel Rosenfeld. 1977. Two-
stage template matching. IEEE transactions on com-
puters, 26(4):384?393.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley, New York.
Kenji Yokoyama, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Discourse analysis using support vector
machine. In Information Processing Society of Japan,
Special Interest Group on Natural Language Process-
ing (IPSJ-SIGNL), 2003-NL-155 (in Japanese), pages
193?200.
Zhu Zhang, Jahna Otterbacher, and Dragomir R.Radev.
2003. Learning cross-document structural relation-
ships using boosting. In Proceedings of the 12th Inter-
national Conference on Information and Knowledge
Management, pages 124?130.
148
Learning to Shift the Polarity of Words for Sentiment Classification
Daisuke Ikeda? Hiroya Takamura? Lev-Arie Ratinov?? Manabu Okumura?
?Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology
ikeda@lr.pi.titech.ac.jp
??Department of Computer Science, University of Illinois at Urbana-Champaign
ratinov2@uiuc.edu
?Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura,oku}@pi.titech.ac.jp
Abstract
We propose a machine learning based
method of sentiment classification of sen-
tences using word-level polarity. The polari-
ties of words in a sentence are not always the
same as that of the sentence, because there
can be polarity-shifters such as negation ex-
pressions. The proposed method models
the polarity-shifters. Our model can be
trained in two different ways: word-wise and
sentence-wise learning. In sentence-wise
learning, the model can be trained so that the
prediction of sentence polarities should be
accurate. The model can also be combined
with features used in previous work such
as bag-of-words and n-grams. We empiri-
cally show that our method almost always
improves the performance of sentiment clas-
sification of sentences especially when we
have only small amount of training data.
1 Introduction
Due to the recent popularity of the internet, individ-
uals have been able to provide various information
to the public easily and actively (e.g., by weblogs
or online bulletin boards). The information often in-
cludes opinions or sentiments on a variety of things
such as new products. A huge amount of work has
been devoted to analysis of the information, which
is called sentiment analysis. The sentiment analysis
has been done at different levels including words,
sentences, and documents. Among them, we focus
on the sentiment classification of sentences, the task
to classify sentences into ?positive? or ?negative?,
because this task is fundamental and has a wide ap-
plicability in sentiment analysis. For example, we
can retrieve individuals? opinions that are related to
a product and can find whether they have the positive
attitude to the product.
There has been much work on the identification of
sentiment polarity of words. For instance, ?beauti-
ful? is positively oriented, while ?dirty? is negatively
oriented. We use the term sentiment words to refer
to those words that are listed in a predefined polar-
ity dictionary. Sentiment words are a basic resource
for sentiment analysis and thus believed to have a
great potential for applications. However, it is still
an open problem how we can effectively use sen-
timent words to improve performance of sentiment
classification of sentences or documents.
The simplest way for that purpose would be the
majority voting by the number of positive words and
the number of negative words in the given sentence.
However, the polarities of words in a sentence are
not always the same as that of the sentence, be-
cause there can be polarity-shifters such as nega-
tion expressions. This inconsistency of word-level
polarity and sentence-level polarity often causes er-
rors in classification by the simple majority voting
method. A manual list of polarity-shifters, which
are the words that can shift the sentiment polarity of
another word (e.g., negations), has been suggested.
However, it has limitations due to the diversity of
expressions.
Therefore, we propose a machine learning based
method that models the polarity-shifters. The model
can be trained in two different ways: word-wise
296
and sentence-wise. While the word-wise learn-
ing focuses on the prediction of polarity shifts, the
sentence-wise learning focuses more on the predic-
tion of sentence polarities. The model can also be
combined with features used in previous work such
as bag-of-words, n-grams and dependency trees. We
empirically show that our method almost always im-
proves the performance of sentiment classification
of sentences especially when we have only small
amount of training data.
The rest of the paper is organized as follows. In
Section 2, we briefly present the related work. In
Section 3, we discuss well-known methods that use
word-level polarities and describe our motivation. In
Section 4, we describe our proposed model, how to
train the model, and how to classify sentences using
the model. We present our experiments and results
in Section 5. Finally in Section 6, we conclude our
work and mention possible future work.
2 Related Work
Supervised machine learning methods including
Support Vector Machines (SVM) are often used in
sentiment analysis and shown to be very promising
(Pang et al, 2002; Matsumoto et al, 2005; Kudo and
Matsumoto, 2004; Mullen and Collier, 2004; Ga-
mon, 2004). One of the advantages of these meth-
ods is that a wide variety of features such as depen-
dency trees and sequences of words can easily be in-
corporated (Matsumoto et al, 2005; Kudo and Mat-
sumoto, 2004; Pang et al, 2002). Our attempt in this
paper is not to use the information included in those
substructures of sentences, but to use the word-level
polarities, which is a resource usually at hand. Thus
our work is an instantiation of the idea to use a re-
source on one linguistic layer (e.g., word level) to
the analysis of another layer (sentence level).
There have been some pieces of work which fo-
cus on multiple levels in text. Mao and Lebanon
(2006) proposed a method that captures local senti-
ment flow in documents using isotonic conditional
random fields. Pang and Lee (2004) proposed to
eliminate objective sentences before the sentiment
classification of documents. McDonald et al (2007)
proposed a model for classifying sentences and doc-
uments simultaneously. They experimented with
joint classification of subjectivity for sentence-level,
and sentiment for document-level, and reported that
their model obtained higher accuracy than the stan-
dard document classification model.
Although these pieces of work aim to predict not
sentence-level but document-level sentiments, their
concepts are similar to ours. However, all the above
methods require annotated corpora for all levels,
such as both subjectivity for sentences and senti-
ments for documents, which are fairly expensive to
obtain. Although we also focus on two different lay-
ers, our method does not require such expensive la-
beled data. What we require is just sentence-level
labeled training data and a polarity dictionary of sen-
timent words.
3 Simple Voting by Sentiment Words
One of the simplest ways to classify sentences us-
ing word-level polarities would be a majority voting,
where the occurrences of positive words and those
of negative words in the given sentence are counted
and compared with each other. However, this major-
ity voting method has several weaknesses. First, the
majority voting cannot take into account at all the
phenomenon that the word-level polarity is not al-
ways the same as the polarity of the sentence. Con-
sider the following example:
I have not had any distortion problems
with this phone and am more pleased with
this phone than any I?ve used before.
where negative words are underlined and positive
words are double-underlined. The example sentence
has the positive polarity, though it locally contains
negative words. The majority voting would misclas-
sify it because of the two negative words.
This kind of inconsistency between sentence-level
polarity and word-level polarity often occurs and
causes errors in the majority voting. The reason
is that the majority voting cannot take into ac-
count negation expressions or adversative conjunc-
tions, e.g., ?I have not had any ...? in the example
above. Therefore, taking such polarity-shifting into
account is important for classification of sentences
using a polarity dictionary. To circumvent this prob-
lem, Kennedy and Inkpen (2006) and Hu and Liu
(2004) proposed to use a manually-constructed list
of polarity-shifters. However, it has limitations due
to the diversity of expressions.
297
Another weakness of the majority voting is that
it cannot be easily combined with existing methods
that use the n-gram model or tree structures of the
sentence as features. The method we propose here
can easily be combined with existing methods and
show better performance.
4 Word-Level Polarity-Shifting Model
We assume that when the polarity of a word is dif-
ferent from the polarity of the sentence, the polarity
of the word is shifted by its context to adapt to the
polarity of the sentence. Capturing such polarity-
shifts will improve the classification performance of
the majority voting classifier as well as of more so-
phisticated classifiers.
In this paper, we propose a word polarity-shifting
model to capture such phenomena. This model is
a kind of binary classification model which deter-
mines whether the polarity is shifted by its context.
The model assigns a score sshift(x, S) to the senti-
ment word x in the sentence S. If the polarity of x
is shifted in S, sshift(x, S) > 0. If the polarity of x
is not shifted in S, sshift(x, S) ? 0. Let w be a pa-
rameter vector of the model and ? be a pre-defined
feature function. Function sshift is defined as
sshift(x, S) = w ? ?(x, S). (1)
Since this model is a linear discriminative model,
there are well-known algorithms to estimate the pa-
rameters of the model.
Usually, such models are trained with each occur-
rence of words as one instance (word-wise learning).
However, we can train our model more effectively
with each sentence being one instance (sentence-
wise learning). In this section, we describe how to
train our model in two different ways and how to
apply the model to a sentence classification.
4.1 Word-wise Learning
In this learning method, we train the word-level
polarity-shift model with each occurrence of sen-
timent words being an instance. Training exam-
ples are automatically extracted by finding sentiment
words in labeled sentences. In the example of Sec-
tion 3, for instance, both negative words (?distor-
tion? or ?problems?) and a positive word (?pleased?)
appear in a positive sentence. We regard ?distortion?
and ?problems?, whose polarities are different from
that of the sentence, as belonging to the polarity-
shifted class. On the contrary, we regard ?pleased?,
whose polarity is the same as that of the sentence, as
not belonging to polarity-shifted class.
We can use the majority voting by those (possi-
bly polarity-shifted) sentiment words. Specifically,
we first classify each sentiment word in the sentence
according to whether the polarity is shifted or not.
Then we use the majority voting to determine the
polarity of the sentence. If the first classifier classi-
fies a positive word into the ?polarity-shifted? class,
we treat the word as a negative one. We expect that
the majority voting with polarity-shifting will out-
perform the simple majority voting without polarity-
shifting. We actually use the weighted majority vot-
ing, where the polarity-shifting score for each senti-
ment word is used as the weight of the vote by the
word. We expect that the score works as a confi-
dence measure.
We can formulate this method as follows. Here,
N and P are respectively defined as the sets of neg-
ative sentiment words and positive sentiment words.
For instance, x ? N means that x is a negative word.
We also write x ? S to express that the word x oc-
curs in S.
First, let us define two scores, scorep(S) and
scoren(S), for the input sentence S. The scorep(S)
and the scoren(S) respectively represent the num-
ber of votes for S being positive and the number
of votes for S being negative. If scorep(S) >
scoren(S), we regard the sentence S as having the
positive polarity, otherwise negative. We suppose
that the following relations hold for the scores:
scorep(S) =
?
x?P?S
?sshift(x, S) +
?
x?N?S
sshift(x, S), (2)
scoren(S) =
?
x?P?S
sshift(x, S) +
?
x?N?S
?sshift(x, S). (3)
When either a polarity-unchanged positive word
(sshift(x, S) ? 0) or a polarity-shifted negative
word occurs in the sentence S, scorep(S) increases.
We can easily obtain the following relation between
two scores:
scorep(S) = ?scoren(S). (4)
298
Since, according to this relation, scorep(S) >
scoren(S) is equivalent to scorep(S) > 0, we use
only scorep(S) for the rest of this paper.
4.2 Sentence-wise Learning
The equation (2) can be rewritten as
scorep(S) =
?
x?S
sshift(x, S)I(x)
=
?
x?S
w ? ?(x, S)I(x)
= w ?
{
?
x?S
?(x, S)I(x)
}
, (5)
where I(x) is the function defined as follows:
I(x) =
?
?
?
?
?
+1 if x ? N ,
?1 if x ? P ,
0 otherwise.
(6)
This scorep(S) can also be seen as a linear discrimi-
native model and the parameters of the model can be
estimated directly (i.e., without carrying out word-
wise learning). Each labeled sentence in a corpus
can be used as a training instance for the model.
In this method, the model is learned so that the
predictive ability for sentence classification is opti-
mized, instead of the predictive ability for polarity-
shifting. Therefore, this model can remain indeci-
sive on the classification of word instances that have
little contextual evidence about whether polarity-
shifting occurs or not. The model can rely more
heavily on word instances that have much evidence.
In contrast, the word-wise learning trains the
model with all the sentiment words appearing in a
corpus. It is assumed here that all the sentiment
words have relations with the sentence-level polar-
ity, and that we can always find the evidence of the
phenomena that the polarity of a word is different
from that of a sentence. Obviously, this assump-
tion is not always correct. As a result, the word-wise
learning sometimes puts a large weight on a context
word that is irrelevant to the polarity-shifting. This
might degrade the performance of sentence classifi-
cation as well as of polarity-shifting.
4.3 Hybrid Model
Both methods described in Sections 4.1 and 4.2
are to predict the sentence-level polarity only with
the word-level polarity. On the other hand, sev-
eral methods that use another set of features, for ex-
ample, bag-of-words, n-grams or dependency trees,
were proposed for the sentence or document classi-
fication tasks. We propose to combine our method
with existing methods. We refer to it as hybrid
model.
In recent work, discriminative models including
SVM are often used with many different features.
These methods are generally represented as
score?p(X) = w? ? ??(X), (7)
where X indicates the target of classification, for ex-
ample, a sentence or a document. If score?p(X) > 0,
X is classified into the target class. ??(X) is a fea-
ture function. When the method uses the bag-of-
words model, ?? maps X to a vector with each ele-
ment corresponding to a word.
Here, we define new score function scorecomb(S)
as a linear combination of scorep(S), the score
function of our sentence-wise learning, and
score?p(S), the score function of an existing
method. Using this, we can write the function as
scorecomb(S) = ?scorep(S) + (1 ? ?)score?p(S)
= ?
?
x?S
w ? ?(x, S)I(x) + (1 ? ?)w? ? ??(S)
= wcomb ?
?
?
?
x?S
?(x, S)I(x), (1 ? ?)??(S)
?
. (8)
Note that ?? indicates the concatenation of two vec-
tors, wcomb is defined as ?w, w?? and ? is a param-
eter which controls the influence of the word-level
polarity-shifting model. This model is also a dis-
criminative model and we can estimate the param-
eters with a variety of algorithms including SVMs.
We can incorporate additional information like bag-
of-words or dependency trees by ??(S).
4.4 Discussions on the Proposed Model
Features such as n-grams or dependency trees can
also capture some negations or polarity-shifters. For
example, although ?satisfy? is positive, the bigram
model will learn ?not satisfy? as a feature corre-
lated with negative polarity if it appears in the train-
ing data. However, the bigram model cannot gener-
alize the learned knowledge to other features such
299
Table 1: Statistics of the corpus
customer movie
# of Labeled Sentences 1,700 10,662
Available 1,436 9,492
# of Sentiment Words 3,276 26,493
Inconsistent Words 1,076 10,674
as ?not great? or ?not disappoint?. On the other
hand, our polarity-shifter model learns that the word
?not? causes polarity-shifts. Therefore, even if there
was no ?not disappoint? in training data, our model
can determine that ?not disappoint? has correlation
with positive class, because the dictionary contains
?disappoint? as a negative word. For this reason,
the polarity-shifting model can be learned even with
smaller training data.
What we can obtain from the proposed method is
not only a set of polarity-shifters. We can also obtain
the weight vector w, which indicates the strength of
each polarity-shifter and is learned so that the pre-
dictive ability of sentence classification is optimized
especially in the sentence-wise learning. It is impos-
sible to manually determine such weights for numer-
ous features.
It is also worth noting that all the models proposed
in this paper can be represented as a kernel function.
For example, the hybrid model can be seen as the
following kernel:
Kcomb(S1, S2) = ?
?
xi?S1
?
xj?S2
K((xi, S1), (xj , S2))
+(1 ? ?)K ?(S1, S2). (9)
Here, K means the kernel function between
words and K ? means the kernel function be-
tween sentences respectively. In addition,
?
xi
?
xjK((xi, S1), (xj , S2)) can be seen as
an instance of convolution kernels, which was
proposed by Haussler (1999). Convolution kernels
are a general class of kernel functions which are
calculated on the basis of kernels between substruc-
tures of inputs. Our proposed kernel treats sentences
as input, and treats sentiment words as substructures
of sentences. We can use high degree polynomial
kernels as both K which is a kernel between sub-
structures, i.e. sentiment words, of sentences, and
K ? which is a kernel between sentences to make the
classifiers take into consideration the combination
of features.
5 Evaluation
5.1 Datasets
We used two datasets, customer reviews 1 (Hu
and Liu, 2004) and movie reviews 2 (Pang and
Lee, 2005) to evaluate sentiment classification of
sentences. Both of these two datasets are often
used for evaluation in sentiment analysis researches.
The number of examples and other statistics of the
datasets are shown in Table 1.
Our method cannot be applied to sentences which
contain no sentiment words. We therefore elimi-
nated such sentences from the datasets. ?Available?
in Table 1 means the number of examples to which
our method can be applied. ?Sentiment Words?
shows the number of sentiment words that are found
in the given sentences. Please remember that senti-
ment words are defined as those words that are listed
in a predefined polarity dictionary in this paper. ?In-
consistent Words? shows the number of the words
whose polarities conflicted with the polarity of the
sentence.
We performed 5-fold cross-validation and used
the classification accuracy as the evaluation mea-
sure. We extracted sentiment words from General
Inquirer (Stone et al, 1996) and constructed a polar-
ity dictionary. After some preprocessing, the dictio-
nary contains 2,084 positive words and 2,685 nega-
tive words.
5.2 Experimental Settings
We employed the Max Margin Online Learning
Algorithms for parameter estimation of the model
(Crammer et al, 2006; McDonald et al, 2007).
In preliminary experiments, this algorithm yielded
equal or better results compared to SVMs. As the
feature representation, ?(x, S), of polarity-shifting
model, we used the local context of three words
to the left and right of the target sentiment word.
We used the polynomial kernel of degree 2 for
polarity-shifting model and the linear kernel for oth-
1http://www.cs.uic.edu/?liub/FBS/FBS.
html
2http://www.cs.cornell.edu/people/pabo/
movie-review-data/
300
Table 2: Experimental results of the sentence classi-
fication
methods customer movie
Baseline 0.638 0.504
BoW 0.790 0.724
2gram 0.809 0.756
3gram 0.800 0.762
Simple-Voting 0.716 0.624
Negation Voting 0.733 0.658
Word-wise 0.783 0.699
Sentence-wise 0.806 0.718
Hybrid BoW 0.827 0.748
Hybrid 2gram 0.840 0.755
Hybrid 3gram 0.837 0.758
Opt 0.840 0.770
ers, and feature vectors are normalized to 1. In hy-
brid models, the feature vectors,
?
x?S ?(x, S)I(x)
and ??(S) are normalized respectively.
5.3 Comparison of the Methods
We compared the following methods:
? Baseline classifies all sentences as positive.
? BoW uses unigram features. 2gram uses uni-
grams and bigrams. 3gram uses unigrams, bi-
grams, and 3grams.
? Simple-Voting is the most simple majority vot-
ing with word-level polarity (Section 3).
? Negation Voting proposed by Hu and
Liu (2004) is the majority voting that takes
negations into account. As negations, we
employed not, no, yet, never, none, nobody,
nowhere, nothing, and neither, which are taken
from (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2006; Hu and Liu, 2004) (Section 3).
? Word-wise was described in Section 4.1.
? Sentence-wise was described in Section 4.2.
? Hybrid BoW, hybrid 2gram, hybrid 3gram
are combinations of sentence-wise model and
respectively BoW, 2gram and 3gram (Section
4.3). We set ? = 0.5.
Table 2 shows the results of these experiments.
Hybrid 3gram, which corresponds to the proposed
method, obtained the best accuracy on customer re-
view dataset. However, on movie review dataset,
the proposed method did not outperform 3gram. In
Section 5.4, we will discuss this result in details.
Comparing word-wise to simple-voting, the accu-
racy increased by about 7 points. This means that
the polarity-shifting model can capture the polarity-
shifts and it is an important factor for sentiment clas-
sification. In addition, we can see the effectiveness
of sentence-wise, by comparing it to word-wise in
accuracy.
?Opt? in Table 2 shows the results of hybrid mod-
els with optimal ? and combination of models. The
optimal results of hybrid models achieved the best
accuracy on both datasets.
We show some dominating polarity-shifters ob-
tained through learning. We obtained many nega-
tions (e.g., no, not, n?t, never), modal verbs (e.g.,
might, would, may), prepositions (e.g., without, de-
spite), comma with a conjunction (e.g., ?, but? as
in ?the case is strong and stylish, but lacks a win-
dow?), and idiomatic expressions (e.g., ?hard resist?
as in ?it is hard to resist?, and ?real snooze?).
5.4 Effect of Training Data Size
When we have a large amount of training data, the n-
gram classifier can learn well whether each n-gram
tends to appear in the positive class or the negative
class. However, when we have only a small amount
of training data, the n-gram classifier cannot capture
such tendency. Therefore the external knowledge,
such as word-level polarity, could be more valuable
information for classification. Thus it is expected
that the sentence-wise model and the hybrid model
will outperform n-gram classifier which does not
take word-level polarity into account, more largely
with few training data.
To verify this conjecture, we conducted experi-
ments by changing the number of the training ex-
amples, i.e., the labeled sentences. We evaluated
three models: sentence-wise, 3gram model and hy-
brid 3gram on both customer review and movie re-
view.
Figures 1 and 2 show the results on customer re-
view and movie review respectively. When the size
of the training data is small, sentence-wise outper-
301
Figure 1: Experimental results on customer review
Figure 2: Experimental results on movie review
forms 3gram on both datasets. We can also see that
the advantage of sentence-wise becomes smaller as
the amount of training data increases, and that the
hybrid 3gram model almost always achieved the best
accuracy among the three models. Similar behaviour
was observed when we ran the same experiments
with 2gram or BoW model. From these results, we
can conclude that, as we expected above, the word-
level polarity is especially effective when we have
only a limited amount of training data, and that the
hybrid model can combine two models effectively.
6 Conclusion
We proposed a model that captures the polarity-
shifting of sentiment words in sentences. We also
presented two different learning methods for the
model and proposed an augmented hybrid classifier
that is based both on the model and on existing clas-
sifiers. We evaluated our method and reported that
the proposed method almost always improved the
accuracy of sentence classification compared with
other simpler methods. The improvement was more
significant when we have only a limited amount of
training data.
For future work, we plan to explore new feature
sets appropriate for our model. The feature sets we
used for evaluation in this paper are not necessar-
ily optimal and we can expect a better performance
by exploring appropriate features. For example, de-
pendency relations between words or appearances of
conjunctions will be useful. The position of a word
in the given sentence is also an important factor in
sentiment analysis (Taboada and Grieve, 2004). Fur-
thermore, we should directly take into account the
fact that some words do not affect the polarity of the
sentence, though the proposed method tackled this
problem indirectly. We cannot avoid this problem
to use word-level polarity more effectively. Lastly,
since we proposed a method for the sentence-level
sentiment prediction, our next step is to extend the
method to the document-level sentiment prediction.
Acknowledgement
This research was supported in part by Overseas Ad-
vanced Educational Research Practice Support Pro-
gram by Ministry of Education, Culture, Sports, Sci-
ence and Technology.
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online Passive-
Aggressive Algorithms. In Journal of Machine Learn-
ing Research, Vol.7, Mar, pp.551?585, 2006.
Michael Gamon. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. In Proceedings of the
20th International Conference on Computational Lin-
guistics (COLING-2004) , pp.841?847, 2004.
David Haussler. Convolution Kernels on Discrete Struc-
tures, Technical Report UCS-CRL-99-10, University
of California in Santa Cruz, 1999.
Minqing Hu and Bing Liu. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteeth
National Conference on Artificial Intellgience (AAAI-
2004) , pp.755?560, San Jose, USA, July 2004.
302
Alistair Kennedy and Diana Inkpen. Sentiment Classi-
fication of Movie and Product Reviews Using Con-
textual Valence Shifters. In Workshop on the Analysis
of Formal and Informal Information Exchange during
Negotiations (FINEXIN-2005), 2005.
Taku Kudo and Yuji Matsumoto. A Boosting Algorithm
for Classification of Semi-Structured Text. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2004), pp.301?
308, 2004.
Yu Mao and Guy Lebanon. Isotonic Conditional Ran-
dom Fields and Local Sentiment Flow. In Proceedings
of the Newral Information Processing Systems (NIPS-
2006), pp.961?968, 2006.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. Sentiment Classification using Word Sub-
Sequences and Dependency Sub-Trees. In Proceed-
ings of the 9th Pacific-Asia International Conference
on Knowledge Discovery and Data Mining (PAKDD-
2005), pp.301?310 , 2005.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL-2007), pp.432?439, 2007.
Tony Mullen and Nigel Collier. Sentiment analysis us-
ing support vector machines with diverse informa-
tion sources. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pp.412?418, 2004.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
Thumbs up? Sentiment Classification using Machine
Learning Techniques. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2002), pp.76?86, 2002.
Bo Pang and Lillian Lee. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summarization
Based on Minimum Cuts. In Proceedings of the 42th
Annual Meeting of the Association for Computational
Linguistics (ACL-2004), pp.271?278, 2004.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2005), pp.115?124, 2005.
Livia Polanyi and Annie Zaenen. Contextual Valence
Shifters. In AAAI Spring Symposium on Exploring At-
titude and Affect in Text: Theories and Applications
(AAAI-EAAT2004), 2004.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. The General Inquirer: A Com-
puter Approach to Content Analysis. The MIT Press,
1996.
Maite Taboada and Jack Grieve. Analyzing Appraisal
Automatically. In AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Applica-
tions (AAAI-EAAT2004), pp.158?161, 2004.
303
Proceedings of NAACL HLT 2007, pages 292?299,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Extracting Semantic Orientations of Phrases from Dictionary
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Integrated Research Institute
Tokyo Institute of Technology
inui@iri.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose a method for extracting se-
mantic orientations of phrases (pairs of an
adjective and a noun): positive, negative,
or neutral. Given an adjective, the seman-
tic orientation classification of phrases can
be reduced to the classification of words.
We construct a lexical network by con-
necting similar/related words. In the net-
work, each node has one of the three ori-
entation values and the neighboring nodes
tend to have the same value. We adopt
the Potts model for the probability model
of the lexical network. For each adjec-
tive, we estimate the states of the nodes,
which indicate the semantic orientations
of the adjective-noun pairs. Unlike ex-
isting methods for phrase classification,
the proposed method can classify phrases
consisting of unseen words. We also pro-
pose to use unlabeled data for a seed set of
probability computation. Empirical evalu-
ation shows the effectiveness of the pro-
posed method.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial ar-
eas. It can be applied to, for example, a survey of
new products or a questionnaire analysis. Automatic
sentiment analysis enables a fast and comprehensive
investigation.
The most fundamental step for sentiment analy-
sis is to acquire the semantic orientations of words:
positive or negative (desirable or undesirable). For
example, the word ?beautiful? is positive, while the
word ?dirty? is negative. Many researchers have de-
veloped several methods for this purpose and ob-
tained good results. One of the next problems to be
solved is to acquire semantic orientations of phrases,
or multi-term expressions, such as ?high+risk? and
?light+laptop-computer?. Indeed the semantic ori-
entations of phrases depend on context just as the se-
mantic orientations of words do, but we would like
to obtain the orientations of phrases as basic units
for sentiment analysis. We believe that we can use
the obtained basic orientations of phrases for affect
analysis of higher linguistic units such as sentences
and documents.
A computational model for the semantic orienta-
tions of phrases has been proposed by Takamura et
al. (2006). However, their method cannot deal with
the words that did not appear in the training data.
The purpose of this paper is to propose a method for
extracting semantic orientations of phrases, which is
applicable also to expressions consisting of unseen
words. In our method, we regard this task as the
noun classification problem for each adjective; the
nouns that become respectively positive (negative,
or neutral) when combined with a given adjective
are distinguished from the other nouns. We create
a lexical network with words being nodes, by con-
necting two words if one of the two appears in the
gloss of the other. In the network, each node has one
of the three orientation values and the neighboring
nodes expectedly tend to have the same value. For
292
example, the gloss of ?cost? is ?a sacrifice, loss, or
penalty? and these words (cost, sacrifice, loss, and
penalty) have the same orientation. To capture this
tendency of the network, we adopt the Potts model
for the probability distribution of the lexical net-
work. For each adjective, we estimate the states of
the nodes, which indicate the semantic orientations
of the adjective-noun pairs. Information from seed
words is diffused to unseen nouns on the network.
We also propose a method for enlarging the seed
set by using the output of an existing method for the
seed words of the probability computation.
Empirical evaluation shows that our method
works well both for seen and unseen nouns, and that
the enlarged seed set significantly improves the clas-
sification performance of the proposed model.
2 Related Work
The semantic orientation classification of words has
been pursued by several researchers. Some of
them used corpora (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003), while others used
dictionaries (Kobayashi et al, 2001; Kamps et al,
2004; Takamura et al, 2005; Esuli and Sebastiani,
2005).
Turney (2002) applied an internet-based tech-
nique to the semantic orientation classification of
phrases, which had originally been developed for
word sentiment classification. In their method, the
number of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
?phrase NEAR good?) is used to determine the ori-
entation. Baron and Hirst (2004) extracted colloca-
tions with Xtract (Smadja, 1993) and classified the
collocations using the orientations of the words in
the neighboring sentences. Their method is similar
to Turney?s in the sense that cooccurrence with seed
words is used. In addition to individual seed words,
Kanayama and Nasukawa (2006) used more compli-
cated syntactic patterns that were manually created.
The four methods above are based on context infor-
mation. In contrast, our method exploits the internal
structure of the semantic orientations of phrases.
Wilson et al (2005) worked on phrase-level se-
mantic orientations. They introduced a polarity
shifter. They manually created the list of polarity
shifters. Inui (2004) also proposed a similar idea.
Takamura et al (2006) proposed to use based on
latent variable models for sentiment classification of
noun-adjective pairs. Their model consists of vari-
ables respectively representing nouns, adjectives, se-
mantic orientations, and latent clusters, as well as
the edges between the nodes. The words that are
similar in terms of semantic orientations, such as
?risk? and ?mortality? (i.e., the positive orientation
emerges when they are ?low?), make a cluster in
their model, which can be an automated version of
Inui?s or Wilson et al?s idea above. However, their
method cannot do anything for the words that did not
appear in the labeled training data. In this paper, we
call their method the latent variable method (LVM).
3 Potts Model
If a variable can have more than two values and
there is no ordering relation between the values,
the network comprised of such variables is called
Potts model (Wu, 1982). In this section, we ex-
plain the simplified mathematical model of Potts
model, which is used for our task in Section 4.
The Potts system has been used as a mathematical
model in several applications such as image restora-
tion (Tanaka and Morita, 1996) and rumor transmis-
sion (Liu et al, 2001).
3.1 Introduction to the Potts Model
Suppose a network consisting of nodes and weighted
edges is given. States of nodes are represented by c.
The weight between i and j is represented by wij .
Let H(c) denote an energy function, which indi-
cates a state of the whole network:
H(c) = ??
?
ij
wij?(ci, cj)+?
?
i?L
??(ci, ai), (1)
where ? is a constant called the inverse-temperature,
L is the set of the indices for the observed variables,
ai is the state of each observed variable indexed by i,
and ? is a positive constant representing a weight on
labeled data. Function ? returns 1 if two arguments
are equal to each other, 0 otherwise. The state is
penalized if ci (i ? L) is different from ai. Using
H(c), the probability distribution of the network is
represented as P (c) = exp{?H(c)}/Z, where Z is
a normalization factor.
However, it is computationally difficult to exactly
estimate the state of this network. We resort to a
293
mean-field approximation method that is described
by Nishimori (2001). In the method, P (c) is re-
placed by factorized function ?(c) =
?
i ?i(ci).
Then we can obtain the function with the smallest
value of the variational free energy:
F (c) =
?
c
P (c)H(c)?
?
c
?P (c) logP (c)
= ??
?
i
?
ci
?i(ci)?(ci, ai)
??
?
ij
?
ci,cj
?i(ci)?j(cj)wij?(ci, cj)
?
?
i
?
ci
??i(ci) log ?i(ci). (2)
By minimizing F (c) under the condition that ?i,?
ci ?i(ci) = 1, we obtain the following fixed point
equation for i ? L:
?i(c) =
exp(??(c, ai) + ?
?
j wij?j(c))?
n exp(??(n, ai) + ?
?
j wij?j(n))
. (3)
The fixed point equation for i /? L can be obtained
by removing ??(c, ai) from above.
This fixed point equation is solved by an itera-
tive computation. In the actual implementation, we
represent ?i with a linear combination of the dis-
crete Tchebycheff polynomials (Tanaka and Morita,
1996). Details on the Potts model and its computa-
tion can be found in the literature (Nishimori, 2001).
After the computation, we obtain the function?
i ?i(ci). When the number of classes is 2, the Potts
model in this formulation is equivalent to the mean-
field Ising model (Nishimori, 2001).
3.2 Relation to Other Models
This Potts model with the mean-field approximation
has relation to several other models.
As is often discussed (Mackay, 2003), the min-
imization of the variational free energy (Equa-
tion (2)) is equivalent to the obtaining the factorized
model that is most similar to the maximum likeli-
hood model in terms of the Kullback-Leibler diver-
gence.
The second term of Equation (2) is the entropy
of the factorized function. Hence the optimization
problem to be solved here is a kind of the maxi-
mum entropy model with a penalty term, which cor-
responds to the first term of Equation (2).
We can find a similarity also to the PageRank al-
gorithm (Brin and Page, 1998), which has been ap-
plied also to natural language processing tasks (Mi-
halcea, 2004; Mihalcea, 2005). In the PageRank al-
gorithm, the pagerank score ri is updated as
ri = (1? d) + d
?
j
wijrj , (4)
where d is a constant (0 ? d ? 1). This update
equation consists of the first term corresponding to
random jump from an arbitrary node and the sec-
ond term corresponding to the random walk from the
neighboring node.
Let us derive the first order Taylor expansion of
Equation (3). We use the equation for i /? L and
denote the denominator by Z? , for simplicity. Since
expx ? 1 + x, we obtain
?i(c) =
exp(?
?
j wij?j(c))
Z?
?
1 + ?
?
j wij?j(c)
Z?
= 1Z?
+ ?Z?
?
j
wij?j(c). (5)
Equation (5) clearly has a quite similar form as
Equation (4). Thus, the PageRank algorithm can be
regarded as an approximation of our model. Let us
clarify the difference between the two algorithms.
The PageRank is designed for two-class classifica-
tion, while the Potts model can be used for an arbi-
trary number of classes. In this sense, the PageRank
is an approximated Ising model. The PageRank is
applicable to asymmetric graphs, while the theory
used in this paper is based on symmetric graphs.
4 Potts Model for Phrasal Semantic
Orientations
In this section, we explain our classification method,
which is applicable also to the pairs consisting of an
adjective and an unseen noun.
4.1 Construction of Lexical Networks
We construct a lexical network, which Takamura et
al. (2005) call the gloss network, by linking two
words if one word appears in the gloss of the other
word. Each link belongs to one of two groups:
294
the same-orientation links SL and the different-
orientation links DL.
If a negation word (e.g., nai, for Japanese) follows
a word in the gloss of the other word, the link is a
different-orientation link. Otherwise the links is a
same-orientation link1.
We next set weights W = (wij) to links :
wij =
?
??
??
1?
d(i)d(j)
(lij ? SL)
? 1?
d(i)d(j)
(lij ? DL)
0 otherwise
, (6)
where lij denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0.
4.2 Classification of Phrases
Takamura et al (2005) used the Ising model to ex-
tract semantic orientations of words (not phrases).
We extend their idea and use the Potts model to ex-
tract semantic orientations of phrasal expressions.
Given an adjective, the decision remaining to be
made in classification of phrasal expressions con-
cerns nouns. We therefore estimate the state of the
nodes on the lexical network for each adjective. The
nouns paring with the given adjective in the train-
ing data are regarded as seed words, which we call
seen words, while the words that did not appear in
the training data are referred to as unseen words.
We use the mean-field method to estimate the
state of the system. If the probability ?i(c) of a vari-
able being positive (negative, neutral) is the highest
of the three classes, then the word corresponding to
the variable is classified as a positive (negative, neu-
tral) word.
We explain the reason why we use the Potts model
instead of the Ising model. While only two classes
(i.e., positive and negative) can be modeled by the
Ising model, three classes (i.e., positive, negative
and neutral) can be modelled by the Potts model.
For the semantic orientations of words, all the words
are sorted in the order of the average orientation
value, equivalently the probability of the word be-
ing positive. Therefore, even if the neutral class is
1For English data, a negation should precede a word, in or-
der for the corresponding link to be a different-orientation link.
not explicitly incorporated, we can manually deter-
mine two thresholds that define respectively the pos-
itive/neutral and negative/neutral boundaries. For
the semantic orientations of phrasal expressions,
however, it is impractical to manually determine
the thresholds for each of the numerous adjectives.
Therefore, we have to incorporate the neutral class
using the Potts model.
For some adjectives, the semantic orientation is
constant regardless of the nouns. We need not use
the Potts model for those unambiguous adjectives.
We thus propose the following two-step classifica-
tion procedure for a given noun-adjective pair <
n, a >.
1. if the semantic orientation of all the instances
with a in L is c, then classify < n, a > into c.
2. otherwise, use the Potts model.
We can also construct a probability model for
each noun to deal with unseen adjectives. However,
we focus on the unseen nouns in this paper, because
our dataset has many more nouns than adjectives.
4.3 Hyper-parameter Prediction
The performance of the proposed method largely de-
pends on the value of hyper-parameter ?. In order to
make the method more practical, we propose a cri-
terion for determining its value.
Takamura et al (2005) proposed two kinds of cri-
teria. One of the two criteria is an approximated
leave-one-out error rate and can be used only when a
large labeled dataset is available. The other is a no-
tion from statistical physics, that is, magnetization:
m =
?
i
x?i/N. (7)
At a high temperature, variables are randomly ori-
ented (paramagnetic phase, m ? 0). At a low
temperature, most of the variables have the same
direction (ferromagnetic phase, m 6= 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, variables are lo-
cally polarized; strongly connected nodes have the
same polarity, but not in a global way. Intuitively,
the state of the lexical network is locally polarized.
295
Therefore, they calculate values of m with several
different values of ? and select the value just before
the phase transition.
Since we cannot expect a large labeled dataset
to be available for each adjective, we use not
the approximated leave-one-out error rate, but the
magnetization-like criterion. However, the magne-
tization above is defined for the Ising model. We
therefore consider that the phase transition has oc-
curred, if a certain class c begins to be favored all
over the system. In practice, when the maximum of
the spatial averages of the approximated probabil-
ities maxc
?
i ?i(c)/N exceeds a threshold during
increasing ?, we consider that the phase transition
has occurred. We select the value of ? slightly be-
fore the phase transition.
4.4 Enlarging Seed Word Set
We usually have only a few seed words for a given
adjective. Enlarging the set of seed words will in-
crease the classification performance. Therefore, we
automatically classify unlabeled pairs by means of
an existing method and use the classified instances
as seeds.
As an existing classifier, we use LVM. Their
model can classify instances that consist of a seen
noun and a seen adjective, but are unseen as a pair.
Although we could classify and use all the nouns
that appeared in the training data (with an adjective
which is different from the given one), we do not
adopt such an alternative, because it will incorporate
even non-collocating pairs such as ?green+idea? into
seeds, resulting in possible degradation of classifi-
cation performance. Therefore, we sample unseen
pairs consisting of a seen noun and a seen adjective
from a corpus, classify the pairs with the latent vari-
able model, and add them to the seed set. The en-
larged seed set consists of pairs used in newspaper
articles and does not include non-collocating pairs.
5 Experiments
5.1 Dataset
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper arti-
cles (1995) written in Japanese, and annotated the
pairs with semantic orientation tags : positive, neu-
tral or negative. We thus obtained the labeled dataset
consisting of 12066 pair instances (7416 different
pairs). The dataset contains 4459 negative instances,
4252 neutral instances, and 3355 positive instances.
The number of distinct nouns is 4770 and the num-
ber of distinct adjectives is 384. To check the inter-
annotator agreement between two annotators, we
calculated ? statistics, which was 0.6402. This value
is allowable, but not quite high. However, positive-
negative disagreement is observed for only 0.7% of
the data. In other words, this statistics means that
the task of extracting neutral examples, which has
hardly been explored, is intrinsically difficult.
We should note that the judgment in annotation
depends on which perspective the annotator takes;
?high+salary? is positive from employee?s perspec-
tive, but negative from employer?s perspective. The
annotators are supposed to take a perspective subjec-
tively. Our attempt is to imitate annotator?s decision.
To construct a classifier that matches the decision of
the average person, we also have to address how to
create an average corpus. We do not pursue this is-
sue because it is out of the scope of the paper.
As unlabeled data, we extracted approximately
65,000 pairs for each iteration of the 10-fold cross-
validation, from the same news source.
The average number of seed nouns for each am-
biguous adjective was respectively 104 in the la-
beled seed set and 264 in the labeled+unlabeled seed
set. Please note that these figures are counted for
only ambiguous adjectives. Usually ambiguous ad-
jectives are more frequent than unambiguous adjec-
tives.
5.2 Experimental Settings
We employ 10-fold cross-validation to obtain the
averaged classification accuracy. We split the data
such that there is no overlapping pair (i.e., any pair
in the training data does not appear in the test data).
Hyperparameter ? was set to 1000, which is very
large since we regard the labels in the seed set is
reliable. For the seed words added by the classifier,
lower ? can be better. Determining a good value for
? is regarded as future work.
Hyperparameter ? is automatically selected from
2Although Kanayama and Nasukawa (2006) that ? for their
dataset similar to ours was 0.83, this value cannot be directly
compared with our value because their dataset includes both in-
dividual words and pairs of words.
296
{0.1, 0.2, ? ? ?, 2.5} for each adjective and each fold
of the cross-validation using the prediction method
described in Section 4.3.
5.3 Results
The results of the classification experiments are
summarized in Table 1.
The proposed method succeeded in classifying,
with approximately 65% in accuracy, those phrases
consisting of an ambiguous adjective and an unseen
noun, which could not be classified with existing
computational models such as LVM.
Incorporation of unlabeled data improves accu-
racy by 15.5 points for pairs consisting of a seen
noun and an ambiguous adjective, and by 3.5 points
for pairs consisting of an unseen noun and an am-
biguous adjective, approximately. The reason why
the former obtained high increase is that pairs with
an ambiguous adjective3 are usually frequent and
likely to be found in the added unlabeled dataset.
If we regard this classification task as binary clas-
sification problems where we are to classify in-
stances into one class or not, we obtain three accu-
racies: 90.76% for positive, 81.75% for neutral, and
86.85% for negative. This results suggests the iden-
tification of neutral instances is relatively difficult.
Next we compare the proposed method with
LVM. The latent variable method is applicable only
to instance pairs consisting of an adjective and a
seen noun. Therefore, we computed the accuracy
for 6586 instances using the latent variable method
and obtained 80.76 %. The corresponding accuracy
by our method was 80.93%. This comparison shows
that our method is better than or at least comparable
to the latent variable method. However, we have to
note that this accuracy of the proposed method was
computed using the unlabeled data classified by the
latent variable method.
5.4 Discussion
There are still 3320 (=12066-8746) word pairs
which could not be classified, because there are no
entries for those words in the dictionary. However,
the main cause of this problem is word segmenta-
3Seen nouns are observed in both the training and the test
datasets because they are frequent. Ambiguous adjectives are
often-used adjectives such as ?large?, ?small?, ?high?, and
?low?.
tion, since many compound nouns and exceedingly-
subdivided morphemes are not in dictionaries. An
appropriate mapping from the words found in cor-
pus to entries of a dictionary will solve this problem.
We found a number of proper nouns, many of which
are not in the dictionary. By estimating a class of a
proper noun and finding the words that matches the
class in the dictionary, we can predict the semantic
orientations of the proper noun based on the orienta-
tions of the found words.
In order to see the overall tendency of errors, we
calculated the confusion matrices both for pairs of
an ambiguous adjective and a seen noun, and for
pairs of an ambiguous adjective and an unseen noun
(Table 2). The proposed method works quite well for
positive/negative classification, though it finds still
some difficulty in correctly classifying neutral in-
stances even after enhanced with the unlabeled data.
In order to qualitatively evaluate the method,
we list several word pairs below. These word
pairs are classified by the Potts model with the la-
beled+unlabeled seed set. All nouns are unseen;
they did not appear in the original training dataset.
Please note again that the actual data is Japanese.
positive instances
noun adjective
cost low
basic price low
loss little
intelligence high
educational background high
contagion not-happening
version new
cafe many
salary high
commission low
negative instances
noun adjective
damage heavy
chance little
terrorist many
trouble many
variation little
capacity small
salary low
disaster many
disappointment big
knowledge little
For example, although both ?salary? and ?com-
mission? are kinds of money, our method captures
297
Table 1: Classification accuracies (%) for various seed sets and test datasets. ?Labeled? seed set corresponds
to the set of manually labeled pairs. ?Labeled+unlabeled? seed set corresponds to the union of ?labeled? seed
set and the set of pairs labeled by LVM. ?Seen nouns? for test are the nouns that appeared in the training
data, while ?unseen nouns? are the nouns that did not appear in the training dataset?. Please note that seen
pairs are excluded from the test data. ?Unambiguous? adjectives corresponds to the pairs with an adjective
which has a unique orientation in the original training dataset, while ?ambiguous? adjectives corresponds to
the pairs with an adjective which has more than one orientation in the original training dataset.
seed\test seen nouns unseen nouns total
labeled 68.24 73.70 69.59
(4494/6586) (1592/2160) (6086/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 61.65 94.85 61.85
(1166/1188) (3328/5398) (736/776) (856/1384)
labeled+unlabeled 80.93 75.88 79.68
(5330/6586) (1639/2160) (6969/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 77.14 94.85 65.25
(1166/1188) (4164/5398) (736/776) (903/1384)
Table 2: Confusion matrices of classification result with labeled+unlabeled seed set
Potts model
seen nouns unseen nouns
positive neutral negative sum positive neutral negative sum
positive 964 254 60 1278 126 84 30 240
Gold standard neutral 198 1656 286 2140 60 427 104 591
negative 39 397 1544 1980 46 157 350 553
sum 1201 2307 1890 5398 232 668 484 1384
the difference between them; ?high salary? is posi-
tive, while ?low (cheap) commission? is also posi-
tive.
6 Conclusion
We proposed a method for extracting semantic ori-
entations of phrases (pairs of an adjective and a
noun). For each adjective, we constructed a Potts
system, which is actually a lexical network extracted
from glosses in a dictionary. We empirically showed
that the proposed method works well in terms of
classification accuracy.
Future work includes the following:
? We assumed that each word has a semantic ori-
entation. However, word senses and subjectiv-
ity have strong interaction (Wiebe and Mihal-
cea, 2006).
? The value of ? must be properly set, because
lower ? can be better for the seed words added
by the classifier,
? To address word-segmentation problem dis-
cussed in Section 5.3, we can utilize the fact
that the heads of compound nouns often inherit
the property determining the semantic orienta-
tion when combined with an adjective.
? The semantic orientations of pairs consisting of
a proper noun will be estimated from the named
entity classes of the proper nouns such as per-
son name and organization.
298
References
Faye Baron and Graeme Hirst. 2004. Collocations as
cues to semantic orientation. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss
analysis. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM?05), pages 617?624.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics and the
8th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 174?181.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute of
Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC?04), volume IV, pages
1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?06), pages 355?363.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelligence,
SLUD-33, pages 45?50.
Zhongzhu Liu, Jun Luo, and Chenggang Shao. 2001.
Potts model for exaggeration of a simple rumor trans-
mitted by recreant rumormongers. Physical Review E,
64:046134,1?046134,9.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Rada Mihalcea. 2004. Graph-based ranking algorithms
for sentence extraction, applied to text summarization.
In The Companion Volume to the Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, (ACL?04), pages 170?173.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Conference on Human Language Technology
/ Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 411?418.
Hidetoshi Nishimori. 2001. Statistical Physics of Spin
Glasses and Information Processing. Oxford Univer-
sity Press.
Frank Z. Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143?
177.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 133?140.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orientations
of phrases. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL?06).
Kazuyuki Tanaka and Tohru Morita. 1996. Application
of cluster variation method to image restoration prob-
lem. In Theory and Applications of the Cluster Vari-
ation and Path Probability Methods, pages 353?373.
Plenum Press, New York.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), pages 417?424.
Janyce M. Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), pages 1065?
1072.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of joint confer-
ence on Human Language Technology / Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP?05), pages 347?354.
Fa-Yueh Wu. 1982. The potts model. Reviews of Mod-
ern Physics, 54(1):235?268.
299
Proceedings of the 43rd Annual Meeting of the ACL, pages 133?140,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Extracting Semantic Orientations of Words using Spin Model
Hiroya Takamura Takashi Inui Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, 226-8503 Japan
{takamura,oku}@pi.titech.ac.jp,
tinui@lr.pi.titech.ac.jp
Abstract
We propose a method for extracting se-
mantic orientations of words: desirable
or undesirable. Regarding semantic ori-
entations as spins of electrons, we use
the mean field approximation to compute
the approximate probability function of
the system instead of the intractable ac-
tual probability function. We also pro-
pose a criterion for parameter selection on
the basis of magnetization. Given only
a small number of seed words, the pro-
posed method extracts semantic orienta-
tions with high accuracy in the exper-
iments on English lexicon. The result
is comparable to the best value ever re-
ported.
1 Introduction
Identification of emotions (including opinions and
attitudes) in text is an important task which has a va-
riety of possible applications. For example, we can
efficiently collect opinions on a new product from
the internet, if opinions in bulletin boards are auto-
matically identified. We will also be able to grasp
people?s attitudes in questionnaire, without actually
reading all the responds.
An important resource in realizing such identifi-
cation tasks is a list of words with semantic orienta-
tion: positive or negative (desirable or undesirable).
Frequent appearance of positive words in a docu-
ment implies that the writer of the document would
have a positive attitude on the topic. The goal of this
paper is to propose a method for automatically cre-
ating such a word list from glosses (i.e., definition
or explanation sentences ) in a dictionary, as well as
from a thesaurus and a corpus. For this purpose, we
use spin model, which is a model for a set of elec-
trons with spins. Just as each electron has a direc-
tion of spin (up or down), each word has a semantic
orientation (positive or negative). We therefore re-
gard words as a set of electrons and apply the mean
field approximation to compute the average orienta-
tion of each word. We also propose a criterion for
parameter selection on the basis of magnetization, a
notion in statistical physics. Magnetization indicates
the global tendency of polarization.
We empirically show that the proposed method
works well even with a small number of seed words.
2 Related Work
Turney and Littman (2003) proposed two algorithms
for extraction of semantic orientations of words. To
calculate the association strength of a word with pos-
itive (negative) seed words, they used the number
of hits returned by a search engine, with a query
consisting of the word and one of seed words (e.g.,
?word NEAR good?, ?word NEAR bad?). They re-
garded the difference of two association strengths as
a measure of semantic orientation. They also pro-
posed to use Latent Semantic Analysis to compute
the association strength with seed words. An em-
pirical evaluation was conducted on 3596 words ex-
tracted from General Inquirer (Stone et al, 1966).
Hatzivassiloglou and McKeown (1997) focused
on conjunctive expressions such as ?simple and
133
well-received? and ?simplistic but well-received?,
where the former pair of words tend to have the same
semantic orientation, and the latter tend to have the
opposite orientation. They first classify each con-
junctive expression into the same-orientation class
or the different-orientation class. They then use the
classified expressions to cluster words into the pos-
itive class and the negative class. The experiments
were conducted with the dataset that they created on
their own. Evaluation was limited to adjectives.
Kobayashi et al (2001) proposed a method for ex-
tracting semantic orientations of words with boot-
strapping. The semantic orientation of a word is
determined on the basis of its gloss, if any of their
52 hand-crafted rules is applicable to the sentence.
Rules are applied iteratively in the bootstrapping
framework. Although Kobayashi et al?s work pro-
vided an accurate investigation on this task and in-
spired our work, it has drawbacks: low recall and
language dependency. They reported that the seman-
tic orientations of only 113 words are extracted with
precision 84.1% (the low recall is due partly to their
large set of seed words (1187 words)). The hand-
crafted rules are only for Japanese.
Kamps et al (2004) constructed a network by
connecting each pair of synonymous words provided
by WordNet (Fellbaum, 1998), and then used the
shortest paths to two seed words ?good? and ?bad?
to obtain the semantic orientation of a word. Limi-
tations of their method are that a synonymy dictio-
nary is required, that antonym relations cannot be
incorporated into the model. Their evaluation is re-
stricted to adjectives. The method proposed by Hu
and Liu (2004) is quite similar to the shortest-path
method. Hu and Liu?s method iteratively determines
the semantic orientations of the words neighboring
any of the seed words and enlarges the seed word
set in a bootstrapping manner.
Subjective words are often semantically oriented.
Wiebe (2000) used a learning method to collect sub-
jective adjectives from corpora. Riloff et al (2003)
focused on the collection of subjective nouns.
We later compare our method with Turney and
Littman?s method and Kamps et al?s method.
The other pieces of research work mentioned
above are related to ours, but their objectives are dif-
ferent from ours.
3 Spin Model and Mean Field
Approximation
We give a brief introduction to the spin model
and the mean field approximation, which are well-
studied subjects both in the statistical mechanics
and the machine learning communities (Geman and
Geman, 1984; Inoue and Carlucci, 2001; Mackay,
2003).
A spin system is an array of N electrons, each of
which has a spin with one of two values ?+1 (up)? or
??1 (down)?. Two electrons next to each other en-
ergetically tend to have the same spin. This model
is called the Ising spin model, or simply the spin
model (Chandler, 1987). The energy function of a
spin system can be represented as
E(x,W ) = ?12
?
ij
wijxixj , (1)
where xi and xj (? x) are spins of electrons i and j,
matrix W = {wij} represents weights between two
electrons.
In a spin system, the variable vector x follows the
Boltzmann distribution :
P (x|W ) = exp(??E(x,W ))Z(W ) , (2)
where Z(W ) = ?x exp(??E(x,W )) is the nor-
malization factor, which is called the partition
function and ? is a constant called the inverse-
temperature. As this distribution function suggests,
a configuration with a higher energy value has a
smaller probability.
Although we have a distribution function, com-
puting various probability values is computationally
difficult. The bottleneck is the evaluation of Z(W ),
since there are 2N configurations of spins in this sys-
tem.
We therefore approximate P (x|W ) with a simple
function Q(x; ?). The set of parameters ? for Q, is
determined such that Q(x; ?) becomes as similar to
P (x|W ) as possible. As a measure for the distance
between P and Q, the variational free energy F is
often used, which is defined as the difference be-
tween the mean energy with respect to Q and the
entropy of Q :
F (?) = ?
?
x
Q(x; ?)E(x;W )
134
?
(
?
?
x
Q(x; ?) logQ(x; ?)
)
. (3)
The parameters ? that minimizes the variational free
energy will be chosen. It has been shown that mini-
mizing F is equivalent to minimizing the Kullback-
Leibler divergence between P and Q (Mackay,
2003).
We next assume that the function Q(x; ?) has the
factorial form :
Q(x; ?) =
?
i
Q(xi; ?i). (4)
Simple substitution and transformation leads us to
the following variational free energy :
F (?) = ??2
?
ij
wij x?ix?j
?
?
i
(
?
?
xi
Q(xi; ?i) logQ(xi; ?i)
)
.
(5)
With the usual method of Lagrange multipliers,
we obtain the mean field equation :
x?i =
?
xi xi exp
(
?xi
?
j wij x?j
)
?
xi exp
(
?xi
?
j wij x?j
) . (6)
This equation is solved by the iterative update rule :
x?newi =
?
xi xi exp
(
?xi
?
j wij x?oldj
)
?
xi exp
(
?xi
?
j wij x?oldj
) . (7)
4 Extraction of Semantic Orientation of
Words with Spin Model
We use the spin model to extract semantic orienta-
tions of words.
Each spin has a direction taking one of two values:
up or down. Two neighboring spins tend to have the
same direction from a energetic reason. Regarding
each word as an electron and its semantic orientation
as the spin of the electron, we construct a lexical net-
work by connecting two words if, for example, one
word appears in the gloss of the other word. Intu-
ition behind this is that if a word is semantically ori-
ented in one direction, then the words in its gloss
tend to be oriented in the same direction.
Using the mean-field method developed in statis-
tical mechanics, we determine the semantic orienta-
tions on the network in a global manner. The global
optimization enables the incorporation of possibly
noisy resources such as glosses and corpora, while
existing simple methods such as the shortest-path
method and the bootstrapping method cannot work
in the presence of such noisy evidences. Those
methods depend on less-noisy data such as a the-
saurus.
4.1 Construction of Lexical Networks
We construct a lexical network by linking two words
if one word appears in the gloss of the other word.
Each link belongs to one of two groups: the same-
orientation links SL and the different-orientation
links DL. If at least one word precedes a nega-
tion word (e.g., not) in the gloss of the other word,
the link is a different-orientation link. Otherwise the
links is a same-orientation link.
We next set weights W = (wij) to links :
wij =
?
???
???
1?
d(i)d(j) (lij ? SL)
? 1?d(i)d(j) (lij ? DL)
0 otherwise
, (8)
where lij denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0. We call this net-
work the gloss network (G).
We construct another network, the gloss-
thesaurus network (GT), by linking synonyms,
antonyms and hypernyms, in addition to the the
above linked words. Only antonym links are in DL.
We enhance the gloss-thesaurus network with
cooccurrence information extracted from corpus. As
mentioned in Section 2, Hatzivassiloglou and McK-
eown (1997) used conjunctive expressions in corpus.
Following their method, we connect two adjectives
if the adjectives appear in a conjunctive form in the
corpus. If the adjectives are connected by ?and?, the
link belongs to SL. If they are connected by ?but?,
the link belongs to DL. We call this network the
gloss-thesaurus-corpus network (GTC).
135
4.2 Extraction of Orientations
We suppose that a small number of seed words are
given. In other words, we know beforehand the se-
mantic orientations of those given words. We incor-
porate this small labeled dataset by modifying the
previous update rule.
Instead of ?E(x,W ) in Equation (2), we use the
following function H(?, x,W ) :
H(?, x,W ) = ??2
?
ij
wijxixj + ?
?
i?L
(xi ? ai)2,
(9)
where L is the set of seed words, ai is the orientation
of seed word i, and ? is a positive constant. This
expression means that if xi (i ? L) is different from
ai, the state is penalized.
Using function H , we obtain the new update rule
for xi (i ? L) :
x?newi =
?
xi xi exp
(
?xisoldi ? ?(xi ? ai)2
)
?
xi exp
(
?xisoldi ? ?(xi ? ai)2
) ,
(10)
where soldi =
?
j wij x?oldj . x?oldi and x?newi are the
averages of xi respectively before and after update.
What is discussed here was constructed with the ref-
erence to work by Inoue and Carlucci (2001), in
which they applied the spin glass model to image
restoration.
Initially, the averages of the seed words are set
according to their given orientations. The other av-
erages are set to 0.
When the difference in the value of the variational
free energy is smaller than a threshold before and
after update, we regard computation converged.
The words with high final average values are clas-
sified as positive words. The words with low final
average values are classified as negative words.
4.3 Hyper-parameter Prediction
The performance of the proposed method largely de-
pends on the value of hyper-parameter ?. In order to
make the method more practical, we propose criteria
for determining its value.
When a large labeled dataset is available, we can
obtain a reliable pseudo leave-one-out error rate :
1
|L|
?
i?L
[aix??i], (11)
where [t] is 1 if t is negative, otherwise 0, and x??i is
calculated with the right-hand-side of Equation (6),
where the penalty term ?(x?i?ai)2 in Equation (10)
is ignored. We choose ? that minimizes this value.
However, when a large amount of labeled data is
unavailable, the value of pseudo leave-one-out error
rate is not reliable. In such cases, we use magnetiza-
tion m for hyper-parameter prediction :
m = 1N
?
i
x?i. (12)
At a high temperature, spins are randomly ori-
ented (paramagnetic phase, m ? 0). At a low
temperature, most of the spins have the same di-
rection (ferromagnetic phase, m 6= 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, spins are locally
polarized; strongly connected spins have the same
polarity, but not in a global way.
Intuitively, the state of the lexical network is lo-
cally polarized. Therefore, we calculate values of
m with several different values of ? and select the
value just before the phase transition.
4.4 Discussion on the Model
In our model, the semantic orientations of words
are determined according to the averages values of
the spins. Despite the heuristic flavor of this deci-
sion rule, it has a theoretical background related to
maximizer of posterior marginal (MPM) estimation,
or ?finite-temperature decoding? (Iba, 1999; Marro-
quin, 1985). In MPM, the average is the marginal
distribution over xi obtained from the distribution
over x. We should note that the finite-temperature
decoding is quite different from annealing type algo-
rithms or ?zero-temperature decoding?, which cor-
respond to maximum a posteriori (MAP) estima-
tion and also often used in natural language process-
ing (Cowie et al, 1992).
Since the model estimation has been reduced
to simple update calculations, the proposed model
is similar to conventional spreading activation ap-
proaches, which have been applied, for example, to
word sense disambiguation (Veronis and Ide, 1990).
Actually, the proposed model can be regarded as a
spreading activation model with a specific update
136
rule, as long as we are dealing with 2-class model
(2-Ising model).
However, there are some advantages in our mod-
elling. The largest advantage is its theoretical back-
ground. We have an objective function and its ap-
proximation method. We thus have a measure of
goodness in model estimation and can use another
better approximation method, such as Bethe approx-
imation (Tanaka et al, 2003). The theory tells
us which update rule to use. We also have a no-
tion of magnetization, which can be used for hyper-
parameter estimation. We can use a plenty of knowl-
edge, methods and algorithms developed in the field
of statistical mechanics. We can also extend our
model to a multiclass model (Q-Ising model).
Another interesting point is the relation to maxi-
mum entropy model (Berger et al, 1996), which is
popular in the natural language processing commu-
nity. Our model can be obtained by maximizing the
entropy of the probability distribution Q(x) under
constraints regarding the energy function.
5 Experiments
We used glosses, synonyms, antonyms and hyper-
nyms of WordNet (Fellbaum, 1998) to construct an
English lexical network. For part-of-speech tag-
ging and lemmatization of glosses, we used Tree-
Tagger (Schmid, 1994). 35 stopwords (quite fre-
quent words such as ?be? and ?have?) are removed
from the lexical network. Negation words include
33 words. In addition to usual negation words such
as ?not? and ?never?, we include words and phrases
which mean negation in a general sense, such as
?free from? and ?lack of?. The whole network con-
sists of approximately 88,000 words. We collected
804 conjunctive expressions from Wall Street Jour-
nal and Brown corpus as described in Section 4.2.
The labeled dataset used as a gold standard is
General Inquirer lexicon (Stone et al, 1966) as in the
work by Turney and Littman (2003). We extracted
the words tagged with ?Positiv? or ?Negativ?, and
reduced multiple-entry words to single entries. As a
result, we obtained 3596 words (1616 positive words
and 1980 negative words) 1. In the computation of
1Although we preprocessed in the same way as Turney and
Littman, there is a slight difference between their dataset and
our dataset. However, we believe this difference is insignificant.
Table 1: Classification accuracy (%) with various
networks and four different sets of seed words. In
the parentheses, the predicted value of ? is written.
For cv, no value is written for ?, since 10 different
values are obtained.
seeds GTC GT G
cv 90.8 (?) 90.9 (?) 86.9 (?)
14 81.9 (1.0) 80.2 (1.0) 76.2 (1.0)
4 73.8 (0.9) 73.7 (1.0) 65.2 (0.9)
2 74.6 (1.0) 61.8 (1.0) 65.7 (1.0)
accuracy, seed words are eliminated from these 3596
words.
We conducted experiments with different values
of ? from 0.1 to 2.0, with the interval 0.1, and pre-
dicted the best value as explained in Section 4.3. The
threshold of the magnetization for hyper-parameter
estimation is set to 1.0 ? 10?5. That is, the pre-
dicted optimal value of ? is the largest ? whose
corresponding magnetization does not exceeds the
threshold value.
We performed 10-fold cross validation as well as
experiments with fixed seed words. The fixed seed
words are the ones used by Turney and Littman: 14
seed words {good, nice, excellent, positive, fortu-
nate, correct, superior, bad, nasty, poor, negative,
unfortunate, wrong, inferior}; 4 seed words {good,
superior, bad, inferior}; 2 seed words {good, bad}.
5.1 Classification Accuracy
Table 1 shows the accuracy values of semantic ori-
entation classification for four different sets of seed
words and various networks. In the table, cv corre-
sponds to the result of 10-fold cross validation, in
which case we use the pseudo leave-one-out error
for hyper-parameter estimation, while in other cases
we use magnetization.
In most cases, the synonyms and the cooccurrence
information from corpus improve accuracy. The
only exception is the case of 2 seed words, in which
G performs better than GT. One possible reason of
this inversion is that the computation is trapped in a
local optimum, since a small number of seed words
leave a relatively large degree of freedom in the so-
lution space, resulting in more local optimal points.
We compare our results with Turney and
137
Table 2: Actual best classification accuracy (%)
with various networks and four different sets of seed
words. In the parenthesis, the actual best value of ?
is written, except for cv.
seeds GTC GT G
cv 91.5 (?) 91.5 (?) 87.0 (?)
14 81.9 (1.0) 80.2 (1.0) 76.2 (1.0)
4 74.4 (0.6) 74.4 (0.6) 65.3 (0.8)
2 75.2 (0.8) 61.9 (0.8) 67.5 (0.5)
Littman?s results. With 14 seed words, they achieved
61.26% for a small corpus (approx. 1? 107 words),
76.06% for a medium-sized corpus (approx. 2?109
words), 82.84% for a large corpus (approx. 1?1011
words).
Without a corpus nor a thesaurus (but with glosses
in a dictionary), we obtained accuracy that is compa-
rable to Turney and Littman?s with a medium-sized
corpus. When we enhance the lexical network with
corpus and thesaurus, our result is comparable to
Turney and Littman?s with a large corpus.
5.2 Prediction of ?
We examine how accurately our prediction method
for ? works by comparing Table 1 above and Ta-
ble 2 below. Our method predicts good ? quite well
especially for 14 seed words. For small numbers of
seed words, our method using magnetization tends
to predict a little larger value.
We also display the figure of magnetization and
accuracy in Figure 1. We can see that the sharp
change of magnetization occurs at around ? = 1.0
(phrase transition). At almost the same point, the
classification accuracy reaches the peak.
5.3 Precision for the Words with High
Confidence
We next evaluate the proposed method in terms of
precision for the words that are classified with high
confidence. We regard the absolute value of each
average as a confidence measure and evaluate the top
words with the highest absolute values of averages.
The result of this experiment is shown in Figure 2,
for 14 seed words as an example. The top 1000
words achieved more than 92% accuracy. This re-
sult shows that the absolute value of each average
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1  2  3  4  5  6  7  8  9  10
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
M
ag
ne
tiz
ati
on
Ac
cu
ra
cy
Beta
magnetization
accuracy
Figure 1: Example of magnetization and classifica-
tion accuracy(14 seed words).
 75
 80
 85
 90
 95
 100
 0  500  1000  1500  2000  2500  3000  3500  4000
Pr
ec
isi
on
Number of selected words
GTC
GT
G
Figure 2: Precision (%) with 14 seed words.
138
Table 3: Precision (%) for selected adjectives.
Comparison between the proposed method and the
shortest-path method.
seeds proposed short. path
14 73.4 (1.0) 70.8
4 71.0 (1.0) 64.9
2 68.2 (1.0) 66.0
Table 4: Precision (%) for adjectives. Comparison
between the proposed method and the bootstrapping
method.
seeds proposed bootstrap
14 83.6 (0.8) 72.8
4 82.3 (0.9) 73.2
2 83.5 (0.7) 71.1
can work as a confidence measure of classification.
5.4 Comparison with other methods
In order to further investigate the model, we conduct
experiments in restricted settings.
We first construct a lexical network using only
synonyms. We compare the spin model with
the shortest-path method proposed by Kamps et
al. (2004) on this network, because the shortest-
path method cannot incorporate negative links of
antonyms. We also restrict the test data to 697 ad-
jectives, which is the number of examples that the
shortest-path method can assign a non-zero orien-
tation value. Since the shortest-path method is de-
signed for 2 seed words, the method is extended
to use the average shortest-path lengths for 4 seed
words and 14 seed words. Table 3 shows the re-
sult. Since the only difference is their algorithms,
we can conclude that the global optimization of the
spin model works well for the semantic orientation
extraction.
We next compare the proposed method with a
simple bootstrapping method proposed by Hu and
Liu (2004). We construct a lexical network using
synonyms and antonyms. We restrict the test data
to 1470 adjectives for comparison of methods. The
result in Table 4 also shows that the global optimiza-
tion of the spin model works well for the semantic
orientation extraction.
We also tested the shortest path method and the
bootstrapping method on GTC and GT, and obtained
low accuracies as expected in the discussion in Sec-
tion 4.
5.5 Error Analysis
We investigated a number of errors and concluded
that there were mainly three types of errors.
One is the ambiguity of word senses. For exam-
ple, one of the glosses of ?costly?is ?entailing great
loss or sacrifice?. The word ?great? here means
?large?, although it usually means ?outstanding? and
is positively oriented.
Another is lack of structural information. For ex-
ample, ?arrogance? means ?overbearing pride evi-
denced by a superior manner toward the weak?. Al-
though ?arrogance? is mistakingly predicted as posi-
tive due to the word ?superior?, what is superior here
is ?manner?.
The last one is idiomatic expressions. For exam-
ple, although ?brag? means ?show off?, neither of
?show? and ?off? has the negative orientation. Id-
iomatic expressions often does not inherit the se-
mantic orientation from or to the words in the gloss.
The current model cannot deal with these types of
errors. We leave their solutions as future work.
6 Conclusion and Future Work
We proposed a method for extracting semantic ori-
entations of words. In the proposed method, we re-
garded semantic orientations as spins of electrons,
and used the mean field approximation to compute
the approximate probability function of the system
instead of the intractable actual probability function.
We succeeded in extracting semantic orientations
with high accuracy, even when only a small number
of seed words are available.
There are a number of directions for future work.
One is the incorporation of syntactic information.
Since the importance of each word consisting a gloss
depends on its syntactic role. syntactic information
in glosses should be useful for classification.
Another is active learning. To decrease the
amount of manual tagging for seed words, an active
learning scheme is desired, in which a small number
of good seed words are automatically selected.
Although our model can easily extended to a
139
multi-state model, the effectiveness of using such a
multi-state model has not been shown yet.
Our model uses only the tendency of having the
same orientation. Therefore we can extract seman-
tic orientations of new words that are not listed in
a dictionary. The validation of such extension will
widen the possibility of application of our method.
Larger corpora such as web data will improve per-
formance. The combination of our method and the
method by Turney and Littman (2003) is promising.
Finally, we believe that the proposed model is ap-
plicable to other tasks in computational linguistics.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
David Chandler. 1987. Introduction to Modern Statisti-
cal Mechanics. Oxford University Press.
Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lexi-
cal disambiguation using simulated annealing. In Pro-
ceedings of the 14th conference on Computational lin-
guistics, volume 1, pages 359?365.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database, Language, Speech, and Communi-
cation Series. MIT Press.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the Thirty-Fifth Annual Meet-
ing of the Association for Computational Linguistics
and the Eighth Conference of the European Chapter of
the Association for Computational Linguistics, pages
174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004
ACM SIGKDD international conference on Knowl-
edge discovery and data mining (KDD-2004), pages
168?177.
Yukito Iba. 1999. The nishimori line and bayesian statis-
tics. Journal of Physics A: Mathematical and General,
pages 3875?3888.
Junichi Inoue and Domenico M. Carlucci. 2001. Image
restoration using the q-ising spin glass. Physical Re-
view E, 64:036121?1 ? 036121?18.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to mea-
sure semantic orientation of adjectives. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115?1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelligence,
SLUD-33, pages 45?50.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Jose L. Marroquin. 1985. Optimal bayesian estima-
tors for image segmentation and surface reconstruc-
tion. Technical Report A.I. Memo 839, Massachusetts
Institute of Technology.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Seventh Con-
ference on Natural Language Learning (CoNLL-03),
pages 25?32.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, pages 44?49.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. The MIT
Press.
Kazuyuki Tanaka, Junichi Inoue, and Mike Titterington.
2003. Probabilistic image processing by means of the
bethe approximation for the q-ising model. Journal
of Physics A: Mathematical and General, 36:11023?
11035.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Jean Veronis and Nancy M. Ide. 1990. Word sense dis-
ambiguation with very large neural networks extracted
from machine readable dictionaries. In Proceedings
of the 13th Conference on Computational Linguistics,
volume 2, pages 389?394.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence (AAAI-
2000), pages 735?740.
140
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1153?1160,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Time Period Identification of Events in Text 
 
 
Taichi Noro? Takashi Inui?? Hiroya Takamura? Manabu Okumura?
?Interdisciplinary Graduate School of Science and Engineering 
Tokyo Institute of Technology 
4259 Nagatsuta-cho, Midori-ku, Yokohama, Kanagawa, Japan 
??Japan Society for the Promotion of Science 
?Precision and Intelligence Laboratory, Tokyo Institute of Technology 
{norot, tinui}@lr.pi.titech.ac.jp,{takamura, oku}@pi.titech.ac.jp 
 
  
 
Abstract 
This study aims at identifying when an 
event written in text occurs. In particular, 
we classify a sentence for an event into 
four time-slots; morning, daytime, eve-
ning, and night. To realize our goal, we 
focus on expressions associated with 
time-slot (time-associated words). How-
ever, listing up all the time-associated 
words is impractical, because there are 
numerous time-associated expressions. 
We therefore use a semi-supervised 
learning method, the Na?ve Bayes classi-
fier backed up with the Expectation 
Maximization algorithm, in order to it-
eratively extract time-associated words 
while improving the classifier. We also 
propose to use Support Vector Machines 
to filter out noisy instances that indicates 
no specific time period. As a result of ex-
periments, the proposed method achieved 
0.864 of accuracy and outperformed 
other methods. 
1 Introduction 
In recent years, the spread of the internet has ac-
celerated. The documents on the internet have 
increased their importance as targets of business 
marketing. Such circumstances have evoked 
many studies on information extraction from text 
especially on the internet, such as sentiment 
analysis and extraction of location information. 
In this paper, we focus on the extraction of tem-
poral information. Many authors of documents 
on the web often write about events in their daily 
life. Identifying when the events occur provides 
us valuable information. For example, we can 
use temporal information as a new axis in the 
information retrieval. From time-annotated text, 
companies can figure out when customers use 
their products. We can explore activities of users 
for marketing researches, such as ?What do 
people eat in the morning??, ?What do people 
spend money for in daytime?? 
Most of previous work on temporal processing 
of events in text dealt with only newswire text. In 
those researches, it is assumed that temporal ex-
pressions indicating the time-period of events are 
often explicitly written in text. Some examples of 
explicit temporal expressions are as follows: ?on 
March 23?, ?at 7 p.m.?. 
However, other types of text including web 
diaries and blogs contain few explicit temporal 
expressions. Therefore one cannot acquire suffi-
cient temporal information using existing meth-
ods. Although dealing with such text as web dia-
ries and blogs is a hard problem, those types of 
text are excellent information sources due to 
their overwhelmingly huge amount. 
In this paper, we propose a method for estimat-
ing occurrence time of events expressed in in-
formal text. In particular, we classify sentences 
in text into one of four time-slots; morning, day-
time, evening, and night. To realize our goal, we 
focus on expressions associated with time-slot 
(hereafter, called time-associated words), such as 
?commute (morning)?, ?nap (daytime)? and 
?cocktail (night)?. Explicit temporal expressions 
have more certain information than the time-
associated words. However, these expressions 
are rare in usual text. On the other hand, al-
though the time-associated words provide us 
only indirect information for estimating occur-
rence time of events, these words frequently ap-
pear in usual text. Actually, Figure 2 (we will 
discuss the graph in Section 5.2, again) shows 
the number of sentences including explicit tem-
1153
poral expressions and time-associated words re-
spectively in text. The numbers are obtained 
from a corpus we used in this paper. We can fig-
ure out that there are much more time-associated 
words than explicit temporal expressions in blog 
text. In other words, we can deal with wide cov-
erage of sentences in informal text by our 
method with time-associated words. 
However, listing up all the time-associated 
words is impractical, because there are numerous 
time-associated expressions. Therefore, we use a 
semi-supervised method with a small amount of 
labeled data and a large amount of unlabeled data, 
because to prepare a large quantity of labeled 
data is costly, while unlabeled data is easy to ob-
tain. Specifically, we adopt the Na?ve Bayes 
classifier backed up with the Expectation Maxi-
mization (EM) algorithm (Dempster et al, 1977) 
for semi-supervised learning. In addition, we 
propose to use Support Vector Machines to filter 
out noisy sentences that degrade the performance 
of the semi-supervised method. 
In our experiments using blog data, we ob-
tained 0.864 of accuracy, and we have shown 
effectiveness of the proposed method. 
This paper is organized as follows. In Section 
2 we briefly describe related work. In Section 3 
we describe the details of our corpus. The pro-
posed method is presented in Section 4. In Sec-
tion 5, we describe experimental results and dis-
cussions. We conclude the paper in Section 6. 
 
2 Related Work 
The task of time period identification is new 
and has not been explored much to date. 
Setzer et al (2001) and Mani et al (2000) 
aimed at annotating newswire text for analyzing 
temporal information. However, these previous 
work are different from ours, because these work 
only dealt with newswire text including a lot of 
explicit temporal expressions. 
Tsuchiya et al (2005) pursued a similar goal 
as ours. They manually prepared a dictionary 
with temporal information. They use the hand-
crafted dictionary and some inference rules to 
determine the time periods of events. In contrast, 
we do not resort to such a hand-crafted material, 
which requires much labor and cost. Our method 
automatically acquires temporal information 
from actual data of people's activities (blog). 
Henceforth, we can get temporal information 
associated with your daily life that would be not 
existed in a dictionary. 
3 Corpus 
In this section, we describe a corpus made from 
blog entries. The corpus is used for training and 
test data of machine learning methods mentioned 
in Section 4. 
The blog entries we used are collected by the 
method of Nanno et al (2004). All the entries are 
written in Japanese. All the entries are split into 
sentences automatically by some heuristic rules. 
In the next section, we are going to explain 
?time-slot? tag added at every sentence. 
3.1 Time-Slot Tag 
The ?time-slot? tag represents when an event 
occurs in five classes; ?morning?, ?daytime?, 
?evening?, ?night?, and ?time-unknown?. ?Time-
unknown? means that there is no temporal in-
formation. We set the criteria of time-slot tags as 
follows. 
Morning: 04:00--10:59 
from early morning till before noon, breakfast 
Daytime: 11:00--15:59 
from noon till before dusk, lunch 
Evening: 16:00--17:59 
from dusk till before sunset 
Night: 18:00--03:59 
from sunset till dawn, dinner 
Note that above criteria are just interpreted as 
rough standards. We think time-slot recognized 
by authors is more important. For example, in a 
case of ?about 3 o'clock this morning? we judge 
the case as ?morning? (not ?night?) with the ex-
pression written by the author ?this morning?. 
To annotate sentences in text, we used two dif-
ferent clues. One is the explicit temporal expres-
sions or time-associated words included in the 
sentence to be judged. The other is contextual 
information around the sentences to be judged. 
The examples corresponding to the former case 
are as follows: 
 
Example 1 
a. I went to post office by bicycle in the morning. 
b. I had spaghetti at restaurant at noon. 
c. I cooked stew as dinner on that day. 
 
Suppose that the two sentences in Example 2 
appear successively in a document. In this case, 
we first judge the first sentence as morning. Next, 
we judge the second sentence as morning by con-
textual information (i.e., the preceding sentence 
is judged as morning), although we cannot know 
the time period just from the content of the sec-
ond sentence itself. 
1154
4.2 Na?ve Bayes Classifier Example 2 
1. I went to X by bicycle in the morning. In this section, we describe multinomial model 
that is a kind of Na?ve Bayes classifiers. 2. I went to a shop on the way back from X. 
A generative probability of example x  given a 
category  has the form: c
3.2 Corpus Statistics 
We manually annotated the corpus. The number 
of the blog entries is 7,413. The number of sen-
tences is 70,775. Of 70,775, the number of sen-
tences representing any events1 is 14,220. The 
frequency distribution of time-slot tags is shown 
in Table 1. We can figure out that the number of 
time-unknown sentences is much larger than the 
other sentences from this table. This bias would 
affect our classification process. Therefore, we 
propose a method for tackling the problem. 
 
( ) ( ) ( ) ( )( )?= w
xwN
xwN
cwP
xxPcxP
,
|
!,|
,
?  (1) 
where ( )xP  denotes the probability that a sen-
tence of length x  occurs,  denotes the 
number of occurrences of w  in text 
( xwN , )
x . The oc-
currence of a sentence is modeled as a set of tri-
als, in which a word is drawn from the whole 
vocabulary.  
In time-slot classification, the x  is correspond 
to each sentence, the c  is correspond to one of 
time-slots in {morning, daytime, evening, night}. 
Features are words in the sentence. A detailed 
description of features will be described in Sec-
tion 4.5. 
morning 711 
daytime 599 
evening 207 
night 1,035 
time-unknown 11,668 
Total 14,220 
4.3 Incorporation of Unlabeled Data with 
the EM Algorithm 
 
Table 1: The numbers of time-slot tags. 
 The EM algorithm (Dempster et al, 1977) is a 
method to estimate a model that has the maximal 
likelihood of the data when some variables can-
not be observed (these variables are called latent 
variables). Nigam et al (2000) proposed a com-
bination of the Na?ve Bayes classifiers and the 
EM algorithm. 
4 Proposed Method 
4.1 Basic Idea 
Suppose, for example, ?breakfast? is a strong 
clue for the morning class, i.e. the word is a 
time-associated word of morning. Thereby we 
can classify the sentence ?I have cereal for 
breakfast.? into the morning class. Then ?cereal? 
will be a time-associated word of morning. 
Therefore we can use ?cereal? as a clue of time-
slot classification. By iterating this process, we 
can obtain a lot of time-associated words with 
bootstrapping method, improving sentence clas-
sification performance at the same time. 
Ignoring the unrelated factors of Eq. (1), we 
obtain 
 
( ) ( ) ( )??
w
xwNcwPcxP ,|,| ,?  (2) 
( ) ( ) ( ) ( )???
w
xwN
c
cwPcPxP .|| ,?  (3) 
We express model parameters as ? . 
If we regard c  as a latent variable and intro-
duce a Dirichlet distribution as the prior distribu-
tion for the parameters, the Q-function (i.e., the 
expected log-likelihood) of this model is defined 
as: 
To realize the bootstrapping method, we use 
the EM algorithm. This algorithm has a theoreti-
cal base of likelihood maximization of incom-
plete data and can enhance supervised learning 
methods. We specifically adopted the combina-
tion of the Na?ve Bayes classifier and the EM 
algorithm. This combination has been proven to 
be effective in the text classification (Nigam et 
al., 2000). 
 ( ) ( )( ) ( )
( ) ( ) ( ) ,|log
,|log|
, ???
????
?
?+=
?
??
?
w
xwN
Dx c
cwPcP
cxPPQ ????
 (4) 
where ( ) ( ) ( )( )( )? ? ??? c w cwPcPP 11 | ??? . ?  is a 
user given parameter and D  is the set of exam-
ples used for model estimation. 
 
                                                 
1 The aim of this study is time-slot classification of 
events. Therefore we treat only sentences expressing 
an event. 
We obtain the next EM equation from this Q-
function: 
1155
 
Figure 1: The flow of 2-step classification. 
 
 
E-step: 
( ) ( ) ( )( ) ( ),,|| ,||,| ?= c cxPcP
cxPcP
xcP ??
???  (5) 
M-step: 
( ) ( ) ( )( ) ,1 ,|1 DC xcPcP Dx +?+?= ? ?? ??  (6) 
( )
( ) ( ) ( )
( ) ( ) ( ) ,,,|1 ,,|1
|
? ?
?
?
?
+?
+?=
w Dx
Dx
xwNxcPW
xwNxcP
cwP
??
??
 (7) 
where C  denotes the number of categories, W  
denotes the number of features variety. For la-
beled example x , Eq. (5) is not used. Instead, ( )?,| xcP  is set as 1.0 if c  is the category of x , 
otherwise 0. 
Instead of the usual EM algorithm, we use the 
tempered EM algorithm (Hofmann, 2001). This 
algorithm allows coordinating complexity of the 
model. We can realize this algorithm by substi-
tuting the next equation for Eq. (5) at E-step: 
 
( ) ( ) ( ){ }( ) ( ){ } ,,|| ,||,| ?= c cxPcP
cxPcP
xcP ?
?
??
???  (8) 
where ?  denotes a hyper parameter for coordi-
nating complexity of the model, and it is positive 
value. By decreasing this hyper-parameter ? , we 
can reduce the influence of intermediate classifi-
cation results if those results are unreliable. 
Too much influence by unlabeled data some-
times deteriorates the model estimation. There-
fore, we introduce a new hyper-parameter 
( 10 ?? )??  which acts as weight on unlabeled 
data. We exchange the second term in the right-
hand-side of Eq. (4) for the next equation: 
( ) ( ) ( ) ( )
( ) ( ) ( ) ( ) ,|log,|
|log,|
,
,
? ??
? ??
?
?
???
????
?+
???
????
?
u
l
Dx w
xwN
c
Dx w
xwN
c
cwPcPxcP
cwPcPxcP
??
?
 
where lD  denotes labeled data, uD  denotes 
unlabeled data. We can reduce the influence of 
unlabeled data by decreasing the value of ? . 
We derived new update rules from this new Q-
function. The EM computation stops when the 
difference in values of the Q-function is smaller 
than a threshold. 
4.4 Class Imbalance Problem 
We have two problems with respect to ?time-
unknown? tag.  
The first problem is the class imbalance prob-
lem (Japkowicz 2000). The number of time-
unknown time-slot sentences is much larger than 
that of the other sentences as shown in Table 1. 
There are more than ten times as many time-
unknown time-slot sentences as the other sen-
tences.  
Second, there are no time-associated words in 
the sentences categorized into ?time-unknown?. 
Thus the feature distribution of time-unknown 
time-slot sentences is remarkably different from 
the others. It would be expected that they ad-
versely affect proposed method. 
There have been some methodologies in order 
to solve the class imbalance problem, such as 
Zhang and Mani (2003), Fan et al (1999) and 
Abe et al (2004). However, in our case, we have 
to resolve the latter problem in addition to the 
class imbalance problem. To deal with two prob-
lems above simultaneously and precisely, we 
develop a cascaded classification procedure. 
SVM 
NB + EM 
Step 2 
Time-Slot 
Classifier 
time-slot = time-unknown 
time-slot = morning, daytime, evening, night 
time-slot = morning 
time-slot = daytime 
time-slot = morning, daytime, evening, night, time-unknown Step1 
Time-Unknown 
Filter 
time-slot = night 
time-slot = evening 
1156
4.5 Time-Slot Classification Method 
It?s desirable to treat only ?time-known? sen-
tences at NB+EM process to avoid the above-
mentioned problems. We prepare another classi-
fier for filtering time-unknown sentences before 
NB+EM process for that purpose. Thus, we pro-
pose a classification method in 2 steps (Method 
A). The flow of the 2-step classification is shown 
in Figure 1. In this figure, ovals represent classi-
fiers, and arrows represent flow of data. 
The first classifier (hereafter, ?time-unknown? 
filter) classifies sentences into two classes; 
?time-unknown? and ?time-known?. The ?time-
known? class is a coarse class consisting of four 
time-slots (morning, daytime, evening, and 
night). We use Support Vector Machines as a 
classifier. The features we used are all words 
included in the sentence to be classified.  
The second classifier (time-slot classifier) 
classifies ?time-known? sentences into four 
classes. We use Na?ve Bayes classifier backed up 
with the Expectation Maximization (EM) algo-
rithm mentioned in Section 4.3.  
The features for the time-slot classifier are 
words, whose part of speech is noun or verb. The 
set of these features are called NORMAL in the 
rest of this paper. In addition, we use information 
from the previous and the following sentences in 
the blog entry. The words included in such sen-
tences are also used as features. The set of these 
features are called CONTEXT. The features in 
CONTEXT would be effective for estimating 
time-slot of the sentences as mentioned in Ex-
ample2 in Section 3.1. 
We also use a simple classifier (Method B) for 
comparison. The Method B classifies all time-
slots (morning ~ night, time-unknown) sentences 
at just one step. We use Na?ve Bayes classifier 
backed up with the Expectation Maximization 
(EM) algorithm at this learning. The features are 
words (whose part-of-speech is noun or verb) 
included in the sentence to be classified. 
 
5 Experimental Results and Discussion 
5.1 Time-Slot Classifier with Time-
Associated Words 
5.1.1 Time-Unknown Filter 
We used 11.668 positive (time-unknown) sam-
ples and 2,552 negative (morning ~ night) sam-
ples. We conducted a classification experiment 
by Support Vector Machines with 10-fold cross 
validation. We used TinySVM2 software pack-
age for implementation. The soft margin parame-
ter is automatically estimated by 10-fold cross 
validation with training data. The result is shown 
in Table 2. 
 
Table 2 clarified that the ?time-unknown? fil-
ter achieved good performance; F-measure of 
0.899. In addition, since we obtained a high re-
call (0.969), many of the noisy sentences will be 
filtered out at this step and the classifier of the 
second step is likely to perform well. 
 
Accuracy 0.878 
Precision 0.838 
Recall 0.969 
F-measure 0.899 
 
Table 2: Classification result of  
the time-unknown filter. 
 
5.1.2 Time-Slot Classification 
In step 2, we used ?time-known? sentences clas-
sified by the unknown filter as test data. We con-
ducted a classification experiment by Na?ve 
Bayes classifier + the EM algorithm with 10-fold 
cross validation. For unlabeled data, we used 
64,782 sentences, which have no intersection 
with the labeled data. The parameters, ?  and ? , 
are automatically estimated by 10-fold cross 
validation with training data. The result is shown 
in Table 3. 
 
Accuracy Method 
NORMAL CONTEXT
Explicit 0.109 
Baseline 0.406 
NB 0.567 0.464 
NB + EM 0.673 0.670 
Table 3: The result of time-slot classifier. 
                                                 
2 http://www.chasen.org/~taku/software/TinySVM 
1157
 
 
 
 
 
 
 
 
 
 
Table 4: Confusion matrix of output. 
 
 morning daytime evening night 
rank word p(c|w) word p(c|w) word p(c|w) word p(c|w)
1 this morning 0.729 noon 0.728 evening 0.750 last night 0.702 
2 morning 0.673 early after noon 0.674 sunset 0.557 night 0.689 
3 breakfast 0.659 afternoon 0.667 academy 0.448 fireworks 0.688 
4 early morning 0.656 daytime 0.655 dusk 0.430 dinner 0.684 
5 before noon 0.617 lunch 0.653 Hills 0.429 go to bed 0.664 
6 compacted snow 0.603 lunch 0.636 run on 0.429 night 0.641 
7 commute 0.561 lunch break 0.629 directions 0.429 bow 0.634 
8 --- 0.541 lunch 0.607 pinecone 0.429 overtime 0.606 
9 parade 0.540 noon 0.567 priest 0.428 year-end party 0.603 
10 wake up 0.520 butterfly 0.558 sand beach 0.428 dinner 0.574 
11 leave harbor 0.504 Chinese food 0.554 --- 0.413 beach 0.572 
12 rise late 0.504 forenoon 0.541 Omori 0.413 cocktail 0.570 
13 cargo work 0.504 breast-feeding 0.536 fan 0.413 me 0.562 
14 alarm clock 0.497 nap 0.521 Haneda 0.412 Tomoyuki 0.560 
15 --- 0.494 diaper 0.511 preview 0.402 return home 0.557 
16 sunglow 0.490 Japanese food 0.502 cloud 0.396 close 0.555 
17 wheel 0.479 star festival 0.502 Dominus 0.392 stay up late 0.551 
18 wake up 0.477 hot noodle 0.502 slip 0.392 tonight 0.549 
19 perm 0.474 pharmacy 0.477 tasting 0.391 night 0.534 
20 morning paper 0.470 noodle 0.476 nest 0.386 every night 0.521 
Table 5: Time-associated words examples. 
 
In Table 3, ?Explicit? indicates the result by a 
simple classifier based on regular expressions 3  
including explicit temporal expressions. The 
baseline method classifies all sentences into 
night because the number of night sentences is 
the largest. The ?CONTEXT? column shows the 
results obtained by classifiers learned with the 
features in CONTEXT in addition to the features 
                                                 
3 For example, we classify sentences matching follow-
ing regular expressions into morning class: 
[(gozen)(gozen-no)(asa) (asa-no)(am)(AM)(am-
no)(AM-no)][456789(10)] ji, [(04)(05)(06)(07)(08) 
(09)]ji, [(04)(05)(06)(07) (08) (09)]:[0-9]{2,2}, 
[456789(10)][(am)(AM)]. 
??gozen?, ?gozen?no? means before noon. ?asa?, 
?asa-no? means morning. ?ji? means o?clock.? 
in NORMAL. The accuracy of the Explicit 
method is lower than the baseline. This means 
existing methods based on explicit temporal ex-
pressions cannot work well in blog text. The ac-
curacy of the method 'NB' exceeds that of the 
baseline by 16%. Furthermore, the accuracy of 
the proposed method 'NB+EM' exceeds that of 
the 'NB' by 11%. Thus, we figure out that using 
unlabeled data improves the performance of our 
time-slot classification.  
In this experiment, unfortunately, CONTEXT 
only deteriorated the accuracy. The time-slot tags 
of the sentences preceding or following the target 
sentence may still provide information to im-
prove the accuracy. Thus, we tried a sequential 
tagging method for sentences, in which tags are 
output of time-slot classifier 
 morning daytime evening night time-unknown 
sum 
morning 332 14 1 37 327 711 
daytime 30 212 1 44 312 599 
evening 4 5 70 18 110 207 
night 21 19 4 382 609 1035 
tim
e-
sl
ot
 ta
g 
time-unknown 85 66 13 203 11301 11668 
sum 472 316 89 684 12659 14220 
1158
predicted in the order of their occurrence. The 
predicted tags are used as features in the predic-
tion of the next tag. This type of sequential tag-
ging method regard as a chunking procedure 
(Kudo and Matsumoto, 2000) at sentence level. 
We conducted time-slot (five classes) classifica-
tion experiment, and tried forward tagging and 
backward tagging, with several window sizes. 
We used YamCha4, the multi-purpose text chun-
ker using Support Vector Machines, as an ex-
perimental tool. However, any tagging direction 
and window sizes did not improve the perform-
ance of classification. Although a chunking 
method has possibility of correctly classifying a 
sequence of text units, it can be adversely biased 
by the preceding or the following tag. The sen-
tences in blog used in our experiments would not 
have a very clear tendency in order of tags. This 
is why the chunking-method failed to improve 
the performance in this task. We would like to 
try other bias-free methods such as Conditional 
Random Fields (Lafferty et al, 2001) for future 
work. 
5.1.3 2-step Classification 
Finally, we show an accuracy of the 2-step clas-
sifier (Method A) and compare it with those of 
other classifiers in Table 6. The accuracies are 
calculated with the equation: 
 
. 
 
In Table 6, the baseline method classifies all 
sentences into time-unknown because the num-
ber of time-unknown sentences is the largest. 
Accuracy of Method A (proposed method) is 
higher than that of Method B (4.1% over). These 
results show that time-unknown sentences ad-
versely affect the classifier learning, and 2-step 
classification is an effective method. 
Table 4 shows the confusion matrix corre-
sponding to the Method A (NORMAL). From 
this table, we can see Method A works well for 
classification of morning, daytime, evening, and 
night, but has some difficulty in 
 
                                                 
4 http://www.chasen.org/~taku/software/YamCha 
Table 6: Comparison of the methods for five 
class classification 
 
 
Figure 2: Change of # sentences that have time-
associated words: ?Explicit? indicates the num-
ber of sentences including explicit temporal ex-
pressions, ?NE-TIME? indicates the number of 
sentences including NE-TIME tag. 
 
classification of time-unknown. The 11.7% of 
samples were wrongly classified into ?night? or 
?unknown?. 
We briefly describe an error analysis. We 
found that our classifier tends to wrongly classify 
samples in which two or more events are written 
in a sentence. The followings are examples: 
 
Example 3 
a. I attended a party last night, and I got back 
on the first train in this morning because the 
party was running over. 
b. I bought a cake this morning, and ate it after 
the dinner. 
5.2 Examples of Time-Associated Words 
Table 5 shows some time-associated words ob-
tained by the proposed method. The words are 
sorted in the descending order of the value of ( )wcP | . Although some consist of two or three 
words, their original forms in Japanese consist of 
one word. There are some expressions appearing 
more than once, such as ?dinner?. Actually these 
expressions have different forms in Japanese. 
Meaningless (non-word) strings caused by mor-
Method Conclusive accuracy
Explicit 0.833 
Baseline 0.821 
Method A (NORMAL) 0.864 
Method A (CONTEXT) 0.862 
Method B 0.823 
0
1000
2000
3000
4000
5000
1 10 20 30 40 50 60 70 80 90 100
# time-associated words (N-best)
# 
se
nt
en
ce
s 
in
cl
ud
in
g 
ti
m
e-
as
so
ci
at
ed
 w
or
ds
   Explicit 
NE-TIME 
# time-unknown sentences correctly classi-
fied by the time-unknown filter 
# known sentences correctly classi-
fied by the time-slot classifier + 
# sentences with a time-slot tag value 
1159
phological analysis error are presented as the 
symbol ?---?. We obtained a lot of interesting 
time-associated words, such as ?commute (morn-
ing)?, ?fireworks (night)?, and ?cocktail (night)?. 
Most words obtained are significantly different 
from explicit temporal expressions and NE-
TIME expressions. 
Figure 2 shows the number of sentences in-
cluding time-associated words in blog text. The 
horizontal axis represents the number of time-
associated words. We sort the words in the de-
scending order of  and selected the top N 
words. The vertical axis represents the number of 
sentences including any N-best time-associated 
words. We also show the number of sentences 
including explicit temporal expressions, and the 
number of sentences including NE-TIME tag 
(Sekine and Isahara, 1999) for comparison. The 
set of explicit temporal expressions was ex-
tracted by the method described in Section 5.1.2. 
We used a Japanese linguistic analyzer ?Cabo-
Cha
( wcP | )
                                                
5 ? to obtain NE-TIME information. From 
this graph, we can confirm that the number of 
target sentences of our proposed method is larger 
than that of existing methods. 
 
6 Conclusion 
In our study, we proposed a method for identify-
ing when an event in text occurs. We succeeded 
in using a semi-supervised method, the Na?ve 
Bayes Classifier enhanced by the EM algorithm, 
with a small amount of labeled data and a large 
amount of unlabeled data. In order to avoid the 
class imbalance problem, we used a 2-step classi-
fier, which first filters out time-unknown sen-
tences and then classifies the remaining sen-
tences into one of 4 classes. The proposed 
method outperformed the simple 1-step method. 
We obtained 86.4% of accuracy that exceeds the 
existing method and the baseline method. 
 
References 
Naoki Abe, Bianca Zadrozny, John Langford. 2004. 
An Iterative Method for Multi-class Cost-sensitive 
Learning. In Proc. of the 10th. ACM SIGKDD, 
pp.3?11. 
Arthur P. Dempster, Nan M. laird, and Donald B. 
Rubin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the 
 
5 http://chasen.org/~taku/software/cabocha/ 
Royal Statistical Society Series B, Vol. 39, No. 1, 
pp.1?38. 
Wei Fan, Salvatore J. Stolfo, Junxin Zhang, Philip K. 
Chan. 1999. AdaCost: Misclassification Cost-
sensitive Boosting. In Proc. of ICML, pp.97?105. 
Thomas Hofmann. 2001. Unsupervised learning by 
probabilistic latent semantic analysis. Machine 
Learning, 42:177?196. 
Nathalie Japkowicz. 2000. Learning from Imbalanced 
Data Sets: A Comparison of Various Strategies. In 
Proc. of the AAAI Workshop on Learning from Im-
balanced Data Sets, pp.10 ?15. 
Taku Kudo, Yuji Matsumoto. 2000. Use of Support 
Vector Learning for Chunking Identification, In 
Proc of the 4th CoNLL, pp.142?144. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Probabil-
istic models for segmenting and labeling sequence 
data, In Proc. of ICML, pp.282?289. 
Inderjeet Mani, George Wilson 2000. Robust Tempo-
ral Processing of News. In Proc. of the 38th ACL, 
pp.69?76. 
Tomoyuki Nanno, Yasuhiro Suzuki, Toshiaki Fujiki, 
Manabu Okumura. 2004. Automatically Collecting 
and Monitoring Japanese Weblogs. Journal for 
Japanese Society for Artificial Intelligence ?
Vol.19, No.6, pp.511?520. (in Japanese) 
Kamal Nigam, Andrew McCallum, Sebastian Thrun, 
and Tom Mitchell. 2000. Text classification from 
labeled and unlabeled documents using EM. Ma-
chine Learning, Vol. 39, No.2/3, pp.103?134. 
Satoshi Sekine, Hitoshi Isahara. 1999. IREX project 
overview. Proceedings of the IREX Workshop. 
Andrea Setzer, Robert Gaizauskas. 2001.  A Pilot 
Study on Annotating Temporal Relations in Text. 
In Proc. of the ACL-2001 Workshop on Temporal 
and Spatial Information Processing, Toulose, 
France, July, pp.88?95. 
Seiji Tsuchiya, Hirokazu Watabe, Tsukasa Kawaoka. 
2005. Evaluation of a Time Judgement Technique 
Based on an Association Mechanism. IPSG SIG 
Technical Reports?2005-NL-168, pp.113?118. (in 
Japanese) 
Jianping Zhang, Inderjeet Mani. 2003. kNN Approach 
to Unbalanced Data Distributions: A Case Study 
involving Information Extraction. In Proc. of 
ICML Workshop on Learning from Imbalanced 
Datasets II., pp.42?48. 
1160
Modeling Category Structures with a Kernel Function
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama,
226-8503 Japan
takamura@pi.titech.ac.jp
Yuji Matsumoto
Department of Information Technology
Nara Institute of Science and Technology
8516-9 Takayama Ikoma Nara,
630-0101 Japan
matsu@is.aist-nara.ac.jp
Hiroyasu Yamada
School of Information Science
Japan Advanced Institute of Science and Technology
1-1 Asahidai Tatsunokuchi Ishikawa, 923-1292 Japan
h-yamada@jaist.ac.jp
Abstract
We propose one type of TOP (Tangent vector
Of the Posterior log-odds) kernel and apply it to
text categorization. In a number of categoriza-
tion tasks including text categorization, nega-
tive examples are usually more common than
positive examples and there may be several dif-
ferent types of negative examples. Therefore,
we construct a TOP kernel, regarding the prob-
abilistic model of negative examples as a mix-
ture of several component models respectively
corresponding to given categories. Since each
component model of our mixture model is ex-
pressed using a one-dimensional Gaussian-type
function, the proposed kernel has an advantage
in computational time. We also show that the
computational advantage is shared by a more
general class of models. In our experiments,
the proposed kernel used with Support Vector
Machines outperformed the linear kernel and
the Fisher kernel based on the Probabilistic La-
tent Semantic Indexing model.
1 Introduction
Recently, Support Vector Machines (SVMs) have been
actively studied because of their high generalization abil-
ity (Vapnik, 1998). In the formulation of SVMs, func-
tions which measure the similarity of two examples take
an important role. These functions are called kernel func-
tions. The usual dot-product of two vectors respectively
corresponding to two examples is often used. Although
some variants to the usual dot-product are sometimes
used (for example, higher-order polynomial kernels and
RBF kernels), the distribution of examples is not taken
into account in such kernels.
However, new types of kernels have more recently
been proposed; they are based on the probability distri-
bution of examples. One is Fisher kernels (Jaakkola and
Haussler, 1998). The other is TOP (Tangent vector Of the
Posterior log-odds) kernels (Tsuda et al, 2002). While
Fisher kernels are constructed on the basis of a genera-
tive model of data, TOP kernels are based on the class-
posterior probability, that is, the probability that the pos-
itive class occurs given an example. However, in order to
use those kernels, we have to select a probabilistic model
of data. The selection of a model will affect categoriza-
tion result. The present paper provides one solution to
this issue. Specifically, we proposed one type of TOP
kernel, because it has been reported that TOP kernels per-
form better than Fisher kernels in terms of categorization
accuracy.
We briefly explain our kernel. We focus on negative
examples in binary classification. Negative examples are
usually more common than positive examples. There
may be several different types of negative examples. Fur-
thermore, the categories of negative examples are some-
times explicitly given (for example, the situation where
we are given documents, each of which has one of three
categories ?sports?,?politics? and ?economics?, and we
are to extract documents with ?politics?). In such a situa-
tion, the probabilistic model of negative examples can be
regarded as a mixture of several component models. We
effectively use this property. Although many other mod-
els can be used, we propose a model based on the sepa-
rating hyperplanes in the original feature space. Specif-
ically, a one-dimensional Gaussian-type function normal
to a hyperplane corresponds to a category. The negative
class is then expressed as a kind of Gaussian mixture.
The reason for the selection of this model is that the re-
sulting kernel has an advantage in computational time.
The kernel based on this mixture model, what we call
Hyperplane-based TOP (HP-TOP) kernel, can be com-
puted efficiently in spite of its high dimensionality. We
later show that the computational advantage is shared by
a more general class of models.
In the experiments of text categorization, in which
SVMs are used as classifiers, our kernel outperformed
the linear kernel and the Fisher kernel based on the Prob-
abilistic Latent Semantic Indexing model proposed by
Hofmann (2000) in terms of categorization accuracy.
2 SVMs and Kernel Method
In this section, we explain SVMs and the kernel method,
which are the basis of our research. SVMs have achieved
high accuracy in various tasks including text categoriza-
tion (Joachims, 1998; Dumais et al, 1998).
Suppose a set Dl of ordered pairs consisting of a fea-
ture vector and its label
Dl = {(x1, y1), (x2, y2), ? ? ? , (xl, yl)},
(?i, xi ? R|I|, yi ? {?1, 1}) (1)
is given. Dl is called training data. I is the set of feature
indices. In SVMs, a separating hyperplane (f(x) = w ?
x ? b) with the largest margin (the distance between the
hyperplane and its nearest vectors) is constructed.
Skipping the details of SVMs? formulation, here we
just show the conclusion that, using some real numbers
??i (?i) and b?, the optimal hyperplane is expressed as
follows:
f(x) =
?
i
??i yixi ? x ? b?. (2)
We should note that only dot-products of examples are
used in the above expression.
Since SVMs are linear classifiers, their separating abil-
ity is limited. To compensate for this limitation, the
kernel method is usually combined with SVMs (Vapnik,
1998).
In the kernel method, the dot-products in (2) are re-
placed with more general inner-products K(xi, x) (kernel
functions). The polynomial kernel (xi ?xj+1)d (d ? N+)
and the RBF kernel exp{??xi ? xj?2/2?2} are often
used. Using the kernel method means that feature vectors
are mapped into a (higher dimensional) Hilbert space and
linearly separated there. This mapping structure makes
non-linear separation possible, although SVMs are basi-
cally linear classifiers.
Another advantage of the kernel method is that al-
though it deals with a high dimensional (possibly infinite)
space, explicit computation of high dimensional vectors
is not required. Only the general inner-products of two
vectors need to be computed. This advantage leads to a
relatively small computational overhead.
3 Kernels from Probabilistic Models
Recently new type of kernels which connect genera-
tive models of data and discriminative classifiers such as
SVMs, have been proposed: the Fisher kernel (Jaakkola
and Haussler, 1998) and the TOP (Tangent vector Of the
Posterior log-odds) kernel (Tsuda et al, 2002).
3.1 Fisher Kernel
Suppose we have a probabilistic generative model p(x|?)
of the data (we denote an example by x). The Fisher score
of x is defined as ?? log p(x|?), where ?? means par-
tial differentiation with respect to the parameters ?. The
Fisher information matrix is denoted by I(?) (this ma-
trix defines the geometric structure of the model space).
Then, the Fisher kernel at an estimate ?? is given by:
K(x1, x2)
= (?? log p(x1|??))tI?1(??)(?? log p(x2|??)) (3)
The Fisher score of an example approximately indicates
how the model will change if the example is added to the
training data used in the estimation of the model. That
means, the Fisher kernel between two examples will be
large, if the influences of the two examples to the model
are similar and large (Tsuda and Kawanabe, 2002).
The matrix I(?) is often approximated by the identity
matrix to avoid large computational overhead.
3.2 TOP Kernel
On the basis of a probabilistic model of the data, TOP
kernels are designed to extract feature vectors f?? which
are considered to be useful for categorization with a sep-
arating hyperplane.
We begin with the proposition that, between the gener-
alization error R(f??) and the expected error of the poste-
rior probability D(f??), the relation R(f??)?L? ? 2D(f??)
holds, where L? is the Bayes error. This inequality means
that minimizing D(f??) leads to reducing the generaliza-
tion error R(f??). D(f??) is expressed, using a logistic
function F (t) = 1/(1 + exp(?t)), as
D(f??)
= min
w,b
Ex|F (w ? f?? ? b)? P (y = +1|x, ??)|, (4)
where ?? denotes the actual parameters of the model.
The TOP kernel consists of features which can minimize
D(f??). In other words, we would like to have feature vec-
tors f?? that satisfy the following:
?x, w ? f??(x)? b = F?1(P (y = +1|x, ??)). (5)
for certain values of w and b.
For that purpose, we first define a function v(x, ?):
v(x, ?) ? F?1(P (y = +1|x, ?))
= logP (y = +1|x, ?)? logP (y = ?1|x, ?). (6)
The first-order Taylor expansion of v(x, ??) around the
estimate ?? is
v(x, ??) ? v(x, ??) +
?
i
(??i ? ??i)
?v(x, ??)
??i . (7)
If f?? is of the following form:
f??(x) =
(v(x, ??), ?v(x, ??)/??1, ? ? ? , ?v(x, ??)/??p
),
(8)
and if w and b are properly chosen as
w = (1, ??1 ? ??1, ? ? ? , ??p ? ??p), b = 0, (9)
then (5) is approximately satisfied. Thus, the TOP kernel
is defined as
K(x1, x2) = f??(x1) ? f??(x2). (10)
A detailed discussion of the TOP kernel and its theoreti-
cal analysis have been given by Tsuda et al(Tsuda et al,
2002).
4 Related Work
Hofmann (2000) applied Fisher kernels to text catego-
rization under the Probabilistic Latent Semantic Indexing
(PLSI) model (Hofmann, 1999).
In PLSI, the joint probability of document d and word
w is :
P (d, w) =
?
k
P (zk)P (d|zk)P (w|zk), (11)
where variables zk correspond to latent classes. After
the estimation of the model using the EM algorithm, the
Fisher kernel for this model is computed. The average
log-likelihood of document d normalized by the docu-
ment length is given by
l(d) =
?
j
P? (wj |d) log
?
k
P (wj |zk)P (zk|d), (12)
where
P? (wj |d) = freq(wj , d)?
m freq(wm,d)
. (13)
They use spherical parameterization (Kass and Vos,
1997) instead of the original parameters in the model.
They define parameters ?jk = 2
?P (wj |zk) and ?k =
2
?
P (zk), and obtained
?l(d)
??jk =
P? (wj |d)P (zk|d, wj)?P (wj |zk)
, (14)
?l(d)
??k ?
P (zk|d)?
P (zk)
. (15)
Thus, the Fisher kernel for this model is obtained as de-
scribed in Appendix A.
The first term of (31) corresponds to the similarity
through latent spaces. The second term corresponds to
the similarity through the distribution of each word. The
number of latent classes zk can affect the value of the
kernel function. In the experiment of (Hofmann, 2000),
they computed the kernels with the different numbers (1
to 64) of zk and added them together to make a robust
kernel instead of deciding one specific number of latent
classes zk.
They concluded that the Fisher kernel based on PLSI
is effective when a large amount of unlabeled examples
are available for the estimation of the PLSI model.
5 Hyperplane-based TOP Kernel
In this section, we explain our TOP kernel.
5.1 Derivation of HP-TOP kernel
Suppose we have obtained the parameters wc and bc
of the separating hyperplane for each category c ?
Ccategory in the original feature space, where Ccategory
denotes the set of categories.
We assume that the class-posteriors Pc(+1|d) and
Pc(?1|d) are expressed as1
Pc(+1|d) = P (c)q(d|c)?
c? P (c?)q(d|c?)
, (16)
Pc(?1|d) =
?
e 6=c P (e)q(d|e)?
c? P (c?)q(d|c?)
(17)
where, for any category x, component function q(d|x) is
of Gaussian-type:
q(d|x) = 1?2pi?2x
exp{? ((wx ? d ? bx)? ?x)
2
2?2x
},
(18)
with the mean ?x of a random variable wx ? d ? bx and
the variance ?x. Those parameters are estimated with the
maximum likelihood estimation, as follows:
?x =
?
(d,y)?Dl,y=x{wx?d?bx}
|{(d,y)?Dl|y=x}| , (19)
?x =
?
(d,y)?Dl,y=x{wx?d?bx??x}
2
|{(d,y)?Dl|y=x}| . (20)
We choose the Gaussian-type function as an exam-
ple.However, this choice is open to argument, since some
other models also have the same computational advan-
tage as described in Section 5.4.
We set ?x1 = ?x/?2x, ?x2 = ?1/2?2x. Although ?x1
and ?x2 are not the natural parameters of this model,
1We cannot say q(d|x) is a generative probability of d given
class x, because it is one-dimensional and not valid as a proba-
bility density in the original feature space.
we parameterize this model using the parameters ?x1,
?x2, wx, bx and P (x) (?x ? Ccategory) for simplic-
ity. Using this probabilistic model,we compute func-
tion v(d, ?) as described in Appendix B (? denotes
{wx, bx, ?x1, ?x2|x ? Ccategory} and wxi denotes the i-
th element of the weight vector wx).
The partial derivatives of this function with respect to
the parameters are in Appendix C.
Then we can follow the definition (10) to obtain our
version of the TOP kernel. We call this new kernel a
hyperplane-based TOP (HP-TOP) kernel.
5.2 Properties of HP-TOP kernel
In the derivatives (39), which provide the largest number
of features, original features di are accompanied by other
factors computed from probability distributions. This
form suggests that two vectors are considered to be more
similar, if they have similar distributions over categories.
In other words, an occurrence of a word can have dif-
ferent contribution to the classification result, depending
on the context (i.e., the other words in the document).
This property of the HP-TOP kernel can lead to the ef-
fect of word sense disambiguation, because ?bank? in a
financial document is treated differently from ?bank? in a
document related to a river-side park.
The derivatives (34) and (35) correspond to the first-
order differences, respectively for the positive class and
the negative class. Similarly, the derivatives (36) and (37)
for the second-order differences. The derivatives (40) and
(41) are for the first-order differences normalized by the
variances.
The derivatives other than (38) and (38) directly de-
pend on the distance from a hyperplane, rather than on
the value of each feature. These derivatives enrich the
feature set, when there are few active words, by which
we mean the words that do not occur in the training data.
For this reason, we expect that the HP-TOP kernel works
well for a small training dataset.
5.3 Computational issue
Computing the kernel in this form is time-consuming, be-
cause the number of components of type (39) can be very
large:
O(|I| ? |Ccategory|), (21)
where I denotes the set of indices for original features.
However, we can avoid this heavy computational cost
as follows. Let us compute the dot-product of deriva-
tives (39) of two vectors d1 and d2, which is shown in
Appendix D. The last expression (45) is regarded as the
scalar product of two dot-products. Thus, by preserving
vectors d and
(
?P (e)q(d|e)P?c(d)
?e ? (we ? d ? be)
?2e
)
e 6=c,e?Ccategory
, (22)
we can efficiently compute the dot-product in (39); the
computational complexity of a kernel function is
O(|I|), (23)
on the condition that the original dimension is larger than
the number of categories. Thus, from the viewpoint of
computational time, our kernel has an advantage over
some other kernels such as the PLSI-based Fisher kernel
in Section 4, which requires the computational complex-
ity of O(|I| ? |Ccluster|), where Ccluster denotes the set
of clusters.
In the PLSI-based Fisher kernel, each word has a prob-
ability distribution over latent classes. In this sense, the
PLSI-based Fisher kernel is more detailed, but detailed
models are sometimes suffer overfitting to the training
data and have the computational disadvantage as men-
tioned above.
The PLSI-based Fisher kernel can be extended to a
TOP kernel by using given categories as latent classes.
However, the problem of computational time still re-
mains.
5.4 General statement about the computational
advantage
So far, we have discussed the computational time for
the kernel constructed on the Gaussian mixture. How-
ever, the computational advantage of the kernel, in fact,
is shared by a more general class of models.
We examine the required conditions for the computa-
tional advantage. Suppose the class-posteriors have the
mixture form as Equations (16) and (17), but function
q(d|x) does not have to be a Gaussian-type function. In-
stead, function q(d|x) is supposed to be represented using
some function r parametrized by we and b, as:
q(d|x) = r(fx(d)|x), (24)
where fx is a scalar function. Then, let us obtain the
derivative of v(d, ?) with respect to wei, which is the bot-
tleneck of kernel computation:
?v(d, ?)
?wei
= ?P (e)q(d|e)P?c(d)
?r(fe(d)|e)
?wei
= ?P (e)q(d|e)P?c(d)
?r(fe(d)|e)
?fe(d)
?fe(d)
?wei . (25)
The first two factors of (25) do not depend on i. There-
fore, if the last factor of (25) is variable-separable with
respect to e and i:
?fe(d)
?wei = S(e)T (i), (26)
where S and T are some function, then the derivative
(25) is also variable-separable. In such cases, the effi-
cient computation described in Section 5.3 is possible by
preserving the vectors:
(T (i))i?I , (27)(
?P (e)q(d|e)P?c(d)
?r(fe(d)|e)
?fe(d) S(e)
)
e 6=c,e?Ccategory
. (28)
We have now obtained the required conditions for the
efficient computation: Equation (24) and the variable-
separability.
In case of Gaussian-type functions, function fe and its
derivative with respect to wei are
fe(d) = we ? d ? be, (29)
?fe(d)
?wei = di. (30)
Thus, the conditions are satisfied.
6 Experiments
Through experiments of text categorization, we empiri-
cally compare the HP-TOP kernel with the linear kernel
and the PLSI-based Fisher kernel. We use Reuters-21578
dataset2 with ModApte-split (Dumais et al, 1998). In ad-
dition, we delete some texts from the result of ModApte-
split, because those texts have no text body. After the
deletion, we obtain 8815 training examples and 3023 test
examples. The words that occur less than five times in the
whole training set are excluded from the original feature
set.
We do not use all the 8815 training examples. The
size of the actual training data ranges from 1000 to 8000.
For each dataset size, experiments are executed 10 times
with different training sets.The result is evaluated with F-
measures for the most frequent 10 categories (Table 1).
The total number of categories is actually 116. How-
ever, for small categories, reliable statistics cannot be ob-
tained. For this reason, we regard the remaining cate-
gories other than the 10 most frequent categories as one
category. Therefore, the model for negative examples is
a mixture of 10 component models (9 out of the 10 most
frequent categories and the new category consisting of the
remaining categories).
We assume uniform priors for categories as in (Tsuda
et al, 2002). We computed the Fisher kernels with differ-
ent numbers (10, 20 and 30) of latent classes and added
them together to make a robust kernel (Hofmann, 2000).
After the learning in the original feature space, the param-
eters for the probability distributions are estimated with
2Available from
http://www.daviddlewis.com/resources/.
Table 1: The categories and their sizes of Reuters-21578
category training texts test texts
earn 2725 1051
acq 1490 644
money-fx 464 141
grain 399 135
crude 353 164
trade 339 133
interest 291 100
ship 197 87
wheat 199 66
corn 161 48
maximum likelihood estimation as in Equations (19) and
(20), followed by the learning with the proposed kernel.
We used an SVM package, TinySVM3, for SVM com-
putation. The soft-margin parameter C was set to 1.0
(other values of C showed no significant changes in re-
sults).
The result is shown in Figure 1 (for macro-average)
and Figure 2 (for micro-average). The HP-TOP kernel
outperforms the linear kernel and the PLSI-based Fisher
kernel for every number of examples.
At each number of examples, we conducted a
Wilcoxon Signed Rank test with 5% significance-level,
for the HP-TOP kernel and the linear kernel, since these
two are better than the other. The test shows that the dif-
ference between the two methods is significant for the
training data sizes 1000 to 5000. The superiority of the
HP-TOP kernel for small training datasets supports our
expectation that the enrichment of feature set will lead to
better performance for few active words. Although we
also expected that the effect of word sense disambigua-
tion would improve accuracy for large training datasets,
the experiments do not provide us with an empirical ev-
idence for the expectation. One possible reason is that
Gaussian-type functions do not reflect the actual distribu-
tion of data. We leave its further investigation as future
research.
In this experimental setting, the PLSI-based Fisher ker-
nel did not work well in terms of categorization accuracy.
However, this Fisher kernel will perform better when the
number of labeled examples is small and a number of
unlabeled examples are available, as reported by Hof-
mann (2000).
We also measured computational time of each method
(Figure 3). The vertical axis indicates the average com-
putational time over 100 runs of experiments (10 runs for
each category). Please note that training time in this fig-
3Available from
http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/.
64
66
68
70
72
74
76
78
80
82
1000 2000 3000 4000 5000 6000 7000 8000
F-
m
ea
su
re
Number of Labeled Examples
HP-TOP Kernel
Linear Kernel
PLSI-based Fisher Kernel
Figure 1: Macro-average of F-measure
 84
 85
 86
 87
 88
 89
 90
 1000  2000  3000  4000  5000  6000  7000  8000
F-
m
ea
su
re
Number of Labeled Examples
HP-TOP Kernel
Linear Kernel
PLSI-based Fisher Kernel
Figure 2: Micro-average of F-measure
 0.1
 1
 10
 100
 1000
 10000
 1000  2000  3000  4000  5000  6000  7000  8000
Co
mp
uta
tio
na
l T
im
e (
se
co
nd
s)
Number of Labeled Examples
HP-TOP Kernel
Linear Kernel
PLSI-based Fisher Kernel
Figure 3: Computational time of each method
ure does not include the computational time required for
feature extraction4. This result empirically shows that the
HP-TOP kernel outperforms the PLSI-based Fisher ker-
nel in terms of computational time as theoretically ex-
pected in Section 5.3.
7 Conclusion
We proposed a TOP kernel based on separating hy-
perplanes. The proposed kernel is created from one-
dimensional Gaussians along the normal directions of the
hyperplanes. We showed that the computational advan-
tage that the proposed kernel has is shared by a more
general class of models. We empirically showed that the
proposed kernel outperforms the linear kernel in text cat-
egorization.
Although the superiority of the proposed method to the
linear kernel was shown, the proposed method has to be
further investigated. Firstly, for large data sizes (namely
7000 and 8000), the proposed method was not signifi-
cantly better than the linear kernel. The effectiveness of
the proposed method should be confirmed by more ex-
periments and theoretical analysis. Secondly, we have to
compare the proposed method with other kernels in or-
der to check the effectiveness of the kernel function con-
sisting of one-dimensional Gaussians normal to the hy-
perplanes. The use of Gaussians is open to argument,
because their symmetric form is somewhat against our
4If the computational time required for feature extraction is
included, the HP-TOP kernel cannot be faster than the linear
kernel.
intuition.
This model can be extended to incorporate unlabeled
examples, for example, using the EM algorithm. In that
sense, the combination of PLSI and the semi-supervised
EM algorithm is also one promising model. When the
category structure of the negative examples is not given,
the proposed method is not applicable. We should inves-
tigate whether unsupervised clustering can substitute for
the category structure.
References
Susan T. Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algo-
rithms and representations for text categorization. In
Proceedings of the Seventh International Conference
on Information and Knowledge Management (ACM-
CIKM98), pages 148?155.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual ACM
Conference on Research and Development in Informa-
tion Retrieval, pages 50?57, Berkeley, California, Au-
gust.
Thomas Hofmann. 2000. Learning the similarity of doc-
uments: An information geometric approach to docu-
ment retrieval and categorization. In Advances in Neu-
ral Information Processing Systems, 12, pages 914?
920.
Tommi Jaakkola and David Haussler. 1998. Exploiting
generative models in discriminative classifiers. In Ad-
vances in Neural Information Processing Systems 11,
pages 487?493.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the 10th European Con-
ference on Machine Learning, pages 137?142.
Robert E. Kass and Paul W. Vos. 1997. Geometrical
foundations of asymptotic inference. New York : Wi-
ley.
Koji Tsuda and Motoaki Kawanabe. 2002. The leave-
one-out kernel. In Proceedings of International Con-
ference on Artificial Neural Networks, pages 727?732.
Koji Tsuda, Motoaki Kawanabe, Gunnar Ra?tsch, So?ren
Sonnenburg, and Klaus-Robert Mu?ller. 2002. A new
discriminative kernel from probabilistic models. Neu-
ral Computation, 14(10):2397?2414.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley, New York.
A Fisher Kernel based on PLSI
K(d1, d2) =
?
k
P (zk|d1)P (zk|d2)
P (zk) +
?
j
P? (wj |d1)P? (wj |d2)
?
k
P (zk|d1, wj)P (zk|d2, wj)
P (wj |zk) , (31)
where P (zk|d, wj) = P (zk)P (d|zk)P (wj |zk)?
l P (zl)P (d|zl)P (wj |zl)
(
= P (zk)P (d|zk)P (wj |zk)P (d, wj)
)
. (32)
B Function v for HP-TOP Kernel
v(d, ?,w, b) = logP (+1|d)? logP (?1|d)
= log P (c)q(d|c)?
c? P (c?)q(d|c?)
? log
?
e6=c P (e)q(d|e)?
c? P (c?)q(d|c?)
= logP (c)q(d|c)? log
?
e6=c
P (e)q(d|e)
= logP (c) exp{?c1(wc ? d) + ?c2(wc ? d)2 + ?
2
c1
4?c2 ?
1
2 log
?pi
?c2 }
? log
?
e6=c
P (e) exp{?e1(we ? d) + ?e2(we ? d)2 + ?
2
e1
4?e2 ?
1
2 log
?pi
?e2 }, (33)
where ?x1 = ?x/?2x, ?x2 = ?1/2?2x.
C Partial Derivatives
?v(d, ?)
??c1 = wc ? d ? bc ? ?c, (34)
?v(d, ?)
??e1 = ?
P (e)q(d|e)?
c? 6=c P (c?)q(d|c?)
(we ? d ? be ? ?e), (35)
?v(d, ?)
??c2 = (wc ? d ? bc)
2 ? ?2c ? ?2c , (36)
?v(d, ?)
??e2 = ?
P (e)q(d|e)?
c? 6=c P (c?)q(d|c?)
{(we ? d ? be)2 ? ?2e ? ?2e}, (37)
?v(d, ?)
?wci =
?c ? (wc ? d ? bc)
?2c di, (38)
?v(d, ?)
?wei = ?
P (e)q(d|e)?
c? 6=c P (c?)q(d|c?)
?e ? (we ? d ? be)
?2e di, (39)
?v(d, ?)
?bc =
wc ? d ? bc ? ?c
?2c , (40)
?v(d, ?)
?be = ?
P (e)q(d|e)?
c? 6=c P (c?)q(d|c?)
we ? d ? be ? ?e
?2e , (41)
?v(d, ?)
P (c) =
1
P (c) , (42)
?v(d, ?)
P (e) = ?
P (d|e)?
c? 6=c P (c?)q(d|c?)
. (43)
D Dot-product of Derivatives (39) in Appendix C
?
e6=c
?
i
?v(d1, ?)
?wei
?v(d2, ?)
?wei =
?
e6=c
?
i
P (e)2q(d1|e)q(d2|e)
P?c(d1)P?c(d2)
?e ? (we ? d ? be)
?2e
?e ? (we ? d ? be)
?2e d
1
i d2i (44)
=
(?
e6=c
P (e)2q(d1|e)q(d2|e)
P?c(d1)P?c(d2)
?e ? (we ? d ? be)
?2e
?e ? (we ? d ? be)
?2e
)
d1 ? d2, (45)
where P?c(d) denotes
?
c? 6=c P (c?)q(d|c?).
Feature Space Restructuring for SVMs
with Application to Text Categorization
Hiroya Takamura and Yuji Matsumoto
Department of Information Technology
Nara Institute of Science and Technology
8516-9, Takayama, Ikoma, 630-0101 Japan
fhiroya-t,matsug@is.aist-nara.ac.jp
Abstract
In this paper, we propose a new method of text
categorization based on feature space restruc-
turing for SVMs. In our method, independent
components of document vectors are extracted
using ICA and concatenated with the original
vectors. This restructuring makes it possible
for SVMs to focus on the latent semantic space
without losing information given by the original
feature space. Using this method, we achieved
high performance in text categorization both
with small number and large numbers of labeled
data.
1 Introduction
The task of text categorization has been exten-
sively studied in Natural Language Processing.
Most successful works rely on a large number
of classied data. However, it is hard to collect
classied data, so considering real applications,
text categorization must be realized even with a
small number of labeled data. Several methods
to realize it have been proposed so far (Nigam et
al, 2000), but they need to be further developed.
For that purpose, we have to take advantage of
invaluable information oered by the property
of unlabeled data. In this paper, we propose
a new categorization method based on Sup-
port Vector Machines (SVMs) (Vapnik, 1995)
and Independent Component Analysis (ICA)
(Herault and Jutten, 1986; Bell and Sejnowski,
1995). SVM is gaining popularity as a classi-
er with high performance, and ICA is one of
the most prospective algorithms in the eld of
signal processing, which extracts independent
components from mixed signals.
SVM has been applied in many applications
such as Image Processing and Natural Language
Processing. The idea to apply SVM for text cat-
egorization was rst introduced in (Joachims,
1998). However, when the number of labeled
data are small, SVM often fails to produce a
good result, although several eorts against this
problem have been made. There are two strate-
gies for improving performance in the case of
a limited number of data. One is to modify
the learning algorithm itself (Joachims, 1999a;
Glenn and Mangasarian, 2001), and the other
is to process training data (Weston et al 2000),
including the selection of features. In this pa-
per, we focus on the latter, especially on fea-
ture space restructuring. For processing train-
ing data, Principal Component Analysis (PCA)
is often adopted in classiers such as k-Nearest
Neighbor method (Mitchell, 1997). But the con-
ventional dimension-reduction methods fail for
SVM as shown by experiments in Section 6. Un-
like the conventional ones, our approach uses
the components obtained with ICA to augment
the dimension of the feature space.
ICA is built on the assumptions that the
sources are independent of each other and that
the signals observed at multiple-points are lin-
ear mixtures of the sources. While the theoret-
ical aspects of ICA are being studied, its pos-
sibility to applications is often pointed out as
in (Bell and Sejnowski, 1997). The idea of us-
ing ICA for text clustering is adopted in sev-
eral works such as in (Isbell and Viola, 1998).
In those works, vector representation model is
adopted (i.e. each text is represented as a vector
with the word-frequencies as the elements). It
is reported however that the independent com-
ponents do not always correspond to the desired
classes, but represent some kind of characteris-
tics of texts (Kolenda et al 2000). In (Kaban
and Girolami, 2000), they showed that the num-
ber of potential components were larger than
that of human-annotated classes. These facts
imply that it is not easy to apply ICA directly
for text classication.
Taking these observations into consideration,
we take the following strategy: rst we perform
ICA on input document vectors, and second,
create the restructured information by concate-
nating the reduced vectors (i.e. the values of
the independent components) and the original
feature vectors.
PCA is an alternative restructuring method.
So we conducted experiments using SVM with
various input vectors: original feature vectors,
reduced feature vectors and restructured fea-
ture vectors (reduction and restructuring are
performed by PCA and ICA). For comparison,
we conducted experiments using Transductive
SVM (TSVM) (Joachims, 1999a) as well, which
is designed for the case of a small number of
labeled data.
Using the proposed method (SVM with ICA),
we obtain better results than ordinary SVM and
TSVM, with both small and large numbers of
labeled data.
2 Support Vector Machines
2.1 Brief Overview of Support Vector
Machines
Support Vector Machine (SVM) is one of the
large-margin classiers (Smola et al 2000).
Given a set of pairs,
(x
1
; y
1
); (x
2
; y
2
);    ; (x
n
; y
n
) (1)
8i; x
i
2 R
d
; y
i
2 f 1; 1g
of a feature vector and a label, SVM constructs
a separating hyperplane with the largest margin
(the distance between the hyperplane and the
vectors, see Figure 1):
f(x) = w  x+ b: (2)
Finding the largest margin is equivalent to min-
imizing the norm kwk, which is expressed as:
min :
1
2
kwk
2
; (3)
s:t: 8i; y
i
(x
i
w+ b)  1  0:
This is realized by solving the quadratic pro-
gram (dual problem of (3)):
max :
P
i

i
 
1
2
P
i;j

i

j
y
i
y
j
x
i
 x
j
(4)
s:t:
P
i

i
y
i
= 0;
8i; 
i
 0;
Positive example
Negative example
Margin
Figure 1: Support Vector Machine
(the solid line corresponds to the optimal hy-
perplane).
where 
i
's are Lagrange multipliers. Using the

i
's that maximize (4), w is expressed as
w =
X
i

i
y
i
x
i
: (5)
Substituting (5) into (2), we obtain
f(x) =
X
i

i
y
i
x
i
 x+ b: (6)
Unlabeled data are classied according to the
signs of (6).
2.2 Kernel Method
SVM is a linear classier and its separating abil-
ity is limited. To compensate this limitation,
Kernel Method is usually combined with SVM
(Vapnik, 1995).
In Kernel Method, the dot-product in (4) and
(6) is replaced by a more general inner-product
K(x
i
;x), called the kernel function. Polynomial
kernel (x
i
 x
j
+ 1)
d
(d 2 N
+
) and RBF ker-
nel expf kx
i
  x
j
k
2
=2
2
g are often used. Us-
ing kernel method means that feature vectors
are mapped into a (higher dimensional) Hilbert
space and linearly separated there. This map-
ping structure makes non-linear separation pos-
sible, although SVM is basically a linear classi-
er.
Another advantage of kernel method is that
although it deals with a high dimensional (pos-
sibly innite) Hilbert space, there is no need
to compute high dimensional vectors explicitly.
Only the general inner-products of two vectors
are needed. This leads to a relatively small com-
putational overhead.
2.3 Transductive SVMs
The Transductive Support Vector Machine
(TSVM) is introduced in (Joachims, 1999a),
which is one realization of transductive learning
in (Vapnik, 1995). It is designed for the classi-
cation with a small number of labeled data. Its
algorithm is approximately as follows:
1. construct a hyperplane using labeled data
in the same way as the ordinary SVMs.
2. classify the unlabeled (test) data according
to the current hyperplane.
3. select the pair of a positively classied sam-
ple and a negatively classied sample that
are nearest to the hyperplane.
4. exchange the labels of those samples, if the
margin gets larger by exchanging them.
5. terminate if a stopping-criterion is satised.
Otherwise, go back to step 2.
This is one way to search for the largest mar-
gin, permitting the relabeling of test data that
have already been labeled by the classier in the
previous iteration.
3 Independent Component Analysis
Independent Component Analysis (ICA) is a
method by which source signals are extracted
from mixed signals. It is based on the assump-
tions that the sources s 2 R
m
are statisti-
cally independent of each other and that the
observed signals x 2 R
n
are linear mixtures of
the sources:
x = As: (7)
Here the matrix A is called a mixing matrix. We
observe x as a time series and estimate both A
and s = (s
1
;    ; s
m
). So our purpose here is to
nd a demixing matrix W such that s
1
;    ; s
m
are as independent of each other as possible:
s = Wx: (8)
The computation proceeds by way of descent
learning with an objective function indicating
independence. There are several criteria of
independence and their learning rules, among
which we take here Infomax approach (Bell
and Sejnowski, 1995), but with natural gradi-
ent (Amari, 1998). Its learning rule is
?W = (I + (I   2g(Wx))(Wx)
t
)W; (9)
where; g(u) = 1=(1 + exp( u)):
4 Text Categorization Enhanced
with Feature Space Restructuring
As in most previous works, we adopt Vector
Space Model (Salton and McGill, 1983) for
representing documents. In this framework,
each document d is represented as a vector
(f
1
;    ; f
d
) with word-frequencies as its ele-
ments.
4.1 Feature Space Restructuring
First we reduce the dimension of document vec-
tors using PCA or ICA. As for PCA, we fol-
low the previous work described in , e.g., (Deer-
wester et al 1990). In (Isbell and Viola, 1998),
they use ICA for dimension reduction and ob-
tain a good result in Information Retrieval. At
the rst step of our method, where the reduced
vectors are obtained, we follow their method.
In this framework, each document d is consid-
ered as a linear mixture of sources s representing
topics. Each word plays a role of "microphone"
and receives a word-frequency in the document
as a mixed signal at each time unit. This for-
mulation is represented by the equation:
d = As; (10)
where A is a mixing matrix. Although both A
and s are unknown, they can be obtained using
the independence assumption. The source sig-
nals s are considered as a reduced expression of
this document. In the case of PCA, the restruc-
turing is processed in the same way. The only
dierence is that independent components cor-
respond to principal components for the PCA
case.
After computing a reduced vector s with PCA
or ICA, we concatenate the original vector d
and the reduced vector s:
^
d =

d
s

: (11)
This transformation means that we do not rely
only on the reduced information, but make use
of both the reduced and the original informa-
tion, that is, the restructured information.
4.2 Text Categorization
Regarding
^
d as the input feature vector of a
document, we use SVM for categorization.
Since SVMs are binary classiers themselves,
so we take here the one-versus-rest method to
apply them for multi-class classication tasks.
5 Theoretical Perspective
5.1 Validation as a Kernel Function
The proposed feature restructuring method can
be considered as the use of a certain kernel for
the pre-restructured feature space. We give an
explanation for the linear case. Given two vec-
tors, d
1
and d
2
, the kernel function K in the
restructured space is expressed as,
K(
^
d
1
;
^
d
2
) =
^
d
t
1
^
d
2
= d
t
1
d
2
+ s
t
1
s
2
= d
t
1
d
2
+ d
t
1
A
t
Ad
2
: (12)
Considering the fact that each of two terms
above is a kernel and that the sum of two kernels
is also a kernel (Vapnik, 1995), the proposed re-
structuring is equivalent to using a certain ker-
nel in the pre-restructured space.
5.2 Interpretation of Feature Space
Restructuring
The expression (12) shows that weights are put
on the latent semantic indices determined by
ICA and PCA respectively. The criterion of
meaningfulness depends on which of ICA and
PCA is used. Note that weighting is dier-
ent from reducing. In the dimension-reduction
methods, only the latent semantic space is con-
sidered, but in our method, the original feature
space still directly inuences the classication
result.
This property of our method makes it pos-
sible to focus on the information given by the
latent semantic space, without losing informa-
tion given by the original feature space.
In text categorization, classes to be predicted
are sometimes characterized by local informa-
tion such as the occurrence of a certain word,
but sometimes dominated by global information
such as the total frequency of a certain group of
words. Considering this situation and the above
property of our method, it is not surprising that
out method gives a good result.
6 Experiments
To evaluate the proposed method, we conducted
several experiments.
The data used here is the Reuters-21578
dataset. The most frequent 6 categories are ex-
tracted from the training-set of the corpus. This
leaves 4872 documents (see Table 1). Some part
of them is used as training data and the rest is
used as test data. Only the words occurring
more than twice are used. Both stemming and
stop-word removal are performed. For compu-
tation, we used SVM-light (Joachims, 1999b).
We conducted two kinds of experiments. The
rst one focuses on evaluating the performance
of the proposed method for each category, with
a xed number of labeled data (Section 6.1).
The second one is conducted to show that the
proposed method gives a good result also when
the number of labeled data increases (Section
6.2).
The results are evaluated by F-measures.
To evaluate the performance across categories,
we computed Micro-average and Macro-average
(Yang, 1999) of F-measures. Micro-average is
obtained by rst computing precision and re-
call for all categories and then using them to
compute the F-measure. Macro-average is com-
puted by rst calculating F-measures for each
category and then averaging them. Micro-
average tends to be dominated by large-sized
categories, and Macro-average by small-sized
ones.
The kernel function used here is a linear ker-
nel. The number of independent or principal
components extracted by ICA or PCA is set to
50.
6.1 Performance with a Fixed Number
of Data
In this experiment, we treated 100, 500, 1000
and 2000 samples as labeled respectively and
kept the other 4772, 4372, 3872 and 2872 sam-
ples unlabeled. The experiment was conducted
10 times for each sample-size repeatedly with
randomly selected labeled samples and their av-
erage values are computed. The result is shown
in Tables 2, 3, 4 and 5. In the row of "Method",
Table 1: Documents used in Experiments
category number of documents
earn 2673
acq 1435
trade 225
crude 223
money-fx 176
interest 140
combinations of restructuring methods are writ-
ten. "Original" means the data of original docu-
ment vectors. "PCA" and "ICA" mean the data
of only reduced vectors, respectively. "Orig-
inal+PCA" and "Original+ICA" are the re-
structured data explained in Section 4.
The proposed method yields a high F-
measure in all the categories for 1000 and 2000
labeled data and in most categories for 100 and
500 labeled data. The last two rows of Tables
2, 3, 4 and 5 show that both Micro-average
and Macro-average are the highest for the pro-
posed method. This means that the proposed
method performs well both for large-sized cat-
egories (e.g., earn) and small-sized categories
(e.g., interest), regardless with the number of
labeled data.
6.2 Performance for the Increase of the
Labeled Data
To investigate how each method behaves when
the number of labeled data increases, we con-
ducted this experiment. The number of labeled
data ranges from 100 to 2000. The results are
shown in Figure 2 and Figure 3. "PCA" gives a
good score only with a small number of data and
"Original" gives a good score only with a large
number of data. In contrast to them, the pro-
posed method produces high performance both
with small and large numbers of data.
7 Conclusions
We proposed a new method of feature space re-
structuring for SVM. In our method, indepen-
dent components are extracted using ICA and
concatenated with the original vectors. Using
this new vectors in the restructured space, we
achieved high performance both with small and
large numbers of labeled data.
The proposed method can be applied also
to other machine learning algorithms provided
78
80
82
84
86
88
90
92
94
96
0 200 400 600 800 1000 1200 1400 1600 1800 2000
M
ic
ro
-a
ve
ra
ge
 o
f F
-m
ea
su
re
s
Number of Labeled Data
Original+ICA
PCA
Original
Original(TSVM)
Figure 2: Micro-average
55
60
65
70
75
80
85
90
95
0 200 400 600 800 1000 1200 1400 1600 1800 2000
M
a
cr
o
-a
ve
ra
ge
 o
f F
-m
ea
su
re
s
Number of Labeled Data
Original+ICA
PCA
Original
Original(TSVM)
Figure 3: Macro-average
that they are robust against noise and can han-
dle a high-dimensional feature space. From this
point of view, it is expected that the proposed
method is useful for kernel-based methods, to
which SVM belongs.
As a future work, we need to nd a way to de-
cide the number of independent components to
be extracted. In this paper, we set the number
to 50 in an ad-hoc way. However, the appropri-
ate number must be predicted based on a theo-
Table 2: F-Measures (100 Labeled Data)
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 92.96 84.00 91.13 86.60 92.97 92.88
acq 85.88 81.42 85.67 80.86 85.91 87.48
trade 36.52 65.59 72.41 72.28 36.68 70.73
crude 65.69 70.90 79.75 80.67 65.93 82.87
money-fx 32.46 45.01 52.69 54.37 32.47 48.62
interest 51.30 52.69 64.44 63.48 51.30 64.84
microaverage 83.63 79.48 85.98 82.14 83.66 87.40
macroaverage 60.80 66.60 74.34 73.04 60.87 74.56
Table 3: F-Measures (500 Labeled Data)
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 96.49 93.97 94.38 93.45 96.49 96.70
acq 93.23 91.57 89.18 87.45 93.22 93.41
trade 86.31 80.81 87.42 86.58 86.37 91.70
crude 83.33 79.78 81.36 78.28 83.43 87.12
money-fx 62.94 64.88 72.83 73.45 63.17 73.99
interest 59.31 52.02 73.37 72.18 59.31 70.41
microaverage 92.17 89.75 90.54 89.33 92.19 93.48
macroaverage 80.26 77.17 83.09 81.89 80.34 85.55
Table 4: F-Measures (1000 Labeled Data)
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 97.15 95.52 96.07 95.53 97.15 97.26
acq 94.60 93.77 92.18 91.44 94.60 94.84
trade 91.19 86.11 87.13 86.87 91.23 93.25
crude 87.99 80.03 80.93 78.75 87.99 89.41
money-fx 73.68 68.85 72.96 72.68 69.96 80.99
interest 75.34 57.26 72.83 68.25 75.34 79.27
microaverage 94.23 91.79 92.31 91.54 94.09 94.90
macroaverage 86.65 80.25 83.68 82.25 86.04 89.17
Table 5: F-Measures (2000 Labeled Data)
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 97.48 95.92 97.18 97.12 97.48 97.55
acq 95.39 94.39 94.78 94.80 95.39 95.65
trade 93.81 86.33 88.61 85.28 93.81 95.90
crude 89.88 80.35 82.63 78.56 89.88 90.25
money-fx 77.44 70.60 74.84 70.69 77.49 81.56
interest 82.71 62.15 73.99 68.46 82.76 83.02
microaverage 95.19 92.43 93.93 93.26 95.20 95.58
macroaverage 89.45 81.62 85.33 82.48 89.47 90.65
retical reason. Toward this problem, theories of
model selection such as Minimum Description
Length (Rissanen, 1987) or Akaike Information
Criterion (Akaike, 1974) could be a good theo-
retical basis.
As explained in Section 4, two terms d
t
1
d
2
and d
t
1
A
t
Ad
2
are simply concatenated in our
method. But either of these terms can be mul-
tiplied with a certain constant. This means that
either of the original space and the Latent Se-
mantic Space can be weighted. Searching for
the best weighting scheme is one of the future
works.
Acknowledgment
We would like to thank Thomas Kolenda
(Technical University of Denmark) for helping
us with the code.
References
Akaike, H. 1974. A New Look at the Statis-
tical Model Identication. IEEE Trans. Au-
tom. Control, vol. AC-19, pp. 716{723.
Amari, S. 1998. Natural Gradient Works E?-
ciently in Learning. Neural Computation, vol.
10-2, pp. 251{276.
Bell, A. J. and Sejnowski, T. J. 1995. An In-
formation Maximization Approach to Blind
Separation and Blind Deconvolution. Neural
Computation, 7, 1129{1159.
Bell, A. J. and Sejnowski, T. J. 1997. The
'Independent Components' of Natural Scenes
are Edge Filters. Vision Research, 37(23), pp.
3327{3338.
Deerwester, S., Dumais, T., Landauer, T., Fur-
nas, W. and Harshman, A. 1990. Indexing by
Latent Semantic Analysis. Journal of the So-
ciety for Information Science, 41(6), pp. 391{
497.
Glenn, F. and Mangasarian, O. 2001. Semi-
Supervised Support Vector Machines for Un-
labeled Data Classication. Optimization
Methods and Software, pp. 1{14.
Herault, J. and Jutten, J. 1986. Space or Time
Adaptive Signal Processing by Neural Net-
work Models. Neural networks for computing:
AIP conference proceedings 151, pp. 206{211.
Isbell, C. and Viola. P. 1998. Restructuring
Sparse High Dimensional Data for Eective
Retrieval. Advances in Neural Information
Processing Systems, volume 11.
Joachims, T. 1998. Text Categorization with
Support Vector Machines: Learning with
Many Relevant Features. Proceedings of the
European Conference on Machine Learning,
pp. 137{142.
Joachims, T. 1999a. Transductive Inference for
Text Classication using Support Vector Ma-
chines. Machine Learning { Proc. 16th Int'l
Conf. (ICML '99), pp. 200{209.
Joachims, T. 1999b. Making large-Scale SVM
Learning Practical. Advances in Kernel
Methods - Support Vector Learning, pp. 169{
184.
Kaban, A. and Girolami, M. 2000. Unsuper-
vised Topic Separation and Keyword Identi-
cation in Document Collections: A Projection
Approach Technical Report.
Kolenda, T, Hansen, L., K. and Sigurdsson, S.
2000. Indepedent Components in Text . Ad-
vances in Independent Component Analysis,
Springer-Verlag, pp. 235{256.
Mitchell, T. 1997. Machine Learning, McGraw
Hill.
Nigam, K., McCallum, A., Thrun, S. and
Mitchell, T. 2000. Text Classication from
Labeled and Unlabeled Documents using EM.
Machine Learning, 39(2/3). pp. 103{134.
Rissanen, J. 1987. Stochastic Complexity.
Journal of Royal Statistical Society, Series B,
49(3), pp. 223{239.
Salton, G. and McGill, M. J. 1983. Introduction
to Modern Information Retrieval. McGraw-
Hill Book Company, New York.
Smola, A., Bartlett, P., Scholkopf, B. and Schu-
urmans, D. 2000. Advances in Large Margin
Classiers. MIT Press
Vapnik, V. 1995. The Nature of Statistical
Learning Theory. Springer.
Weston, J., Mukherjee, S., Chapelle, O., Pon-
til, M., Poggio, T. and Vapnik, V. 2000. Fea-
ture Selection for SVMs. In Advances in Neu-
ral Information Processing Systems, volume
13.
Yang, Y. An Evaluation of Statistical Ap-
proaches to Text Categorization. Information
Retrieval, volume 1, 1-2, pp. 69{90.
 
		Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832?842,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
An Efficient Algorithm for Unsupervised Word Segmentation with
Branching Entropy and MDL
Valentin Zhikov
Interdisciplinary Graduate School
of Science and Engineering
Tokyo Institute of Technology
zhikov@lr.pi.titech.ac.jp
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
This paper proposes a fast and simple unsuper-
vised word segmentation algorithm that uti-
lizes the local predictability of adjacent char-
acter sequences, while searching for a least-
effort representation of the data. The model
uses branching entropy as a means of con-
straining the hypothesis space, in order to ef-
ficiently obtain a solution that minimizes the
length of a two-part MDL code. An evaluation
with corpora in Japanese, Thai, English, and
the ?CHILDES? corpus for research in lan-
guage development reveals that the algorithm
achieves an accuracy, comparable to that of
the state-of-the-art methods in unsupervised
word segmentation, in a significantly reduced
computational time.
1 Introduction
As an inherent preprocessing step to nearly all NLP
tasks for writing systems without orthographical
marking of word boundaries, such as Japanese and
Chinese, the importance of word segmentation has
lead to the emergence of a micro-genre in NLP fo-
cused exclusively on this problem.
Supervised probabilistic models such as Condi-
tional Random Fields (CRF) (Lafferty et al, 2001)
have a wide application to the morphological anal-
ysis of these languages. However, the development
of the annotated training corpora necessary for their
functioning is a labor-intensive task, which involves
multiple stages of manual tagging. Because of the
scarcity of labeled data, the domain adaptation of
morphological analyzers is also problematic, and
semi-supervised algorithms that address this issue
have also been proposed (e.g. Liang, 2005; Tsuboi
et al, 2008).
Recent advances in unsupervised word segmen-
tation have been promoted by human cognition re-
search, where it is involved in the modeling of the
mechanisms that underlie language acquisition. An-
other motivation to study unsupervised approaches
is their potential to support the domain adaptation of
morphological analyzers through the incorporation
of unannotated training data, thus reducing the de-
pendency on costly manual work. Apart from the
considerable difficulties in discovering reliable cri-
teria for word induction, the practical application
of such approaches is impeded by their prohibitive
computational cost.
In this paper, we address the issue of achiev-
ing high accuracy in a practical computational time
through an efficient method that relies on a combina-
tion of evidences: the local predictability of charac-
ter patterns, and the reduction of effort achieved by
a given representation of the language data. Both of
these criteria are assumed to play a key role in native
language acquisition. The proposed model allows
experimentation in a more realistic setting, where
the learner is able to apply them simultaneously. The
832
method shows a high performance in terms of accu-
racy and speed, can be applied to language samples
of substantial length, and generalizes well to corpora
in different languages.
2 Related Work
The principle of least effort (Zipf, 1949) postulates
that the path of minimum resistance underlies all
human behavior. Recent research has recognized
its importance in the process of language acquisi-
tion (Kit, 2003). Compression-based word induc-
tion models comply to this principle, as they reor-
ganize the data into a more compact representation
while identifying the vocabulary of a text. The min-
imum description length framework (MDL) (Ris-
sanen, 1978) is an appealing means of formalizing
such models, as it provides a robust foundation for
learning and inference, based solely on compres-
sion.
The major problem in MDL-based word segmen-
tation is the lack of standardized search algorithms
for the exponential hypothesis space (Goldwater,
2006). The representative MDL models compare
favorably to the current state-of-the-art models in
terms of accuracy. Brent and Cartwright (1996) car-
ried out an exhaustive search through the possible
segmentations of a limited subset of the data. Yu
(2000) proposed an EM optimization routine, which
achieved a high accuracy, in spite of a lower com-
pression than the gold standard segmentation.
As a solution to the aforementioned issue, the pro-
posed method incorporates the local predictability of
character sequences into the inference process. Nu-
merous studies have shown that local distributional
cues can serve well the purpose of inducing word
boundaries. Behavioral science has confirmed that
infants are sensitive to the transitional probabilities
found in speech (Saffran et al, 1996). The increase
in uncertainty following a given word prefix is a
well studied criterion for morpheme boundary pre-
diction (Harris, 1955). A good deal of research has
been conducted on methods through which such lo-
cal statistics can be applied to the word induction
problem (e.g. Kempe, 1999; Huang and Powers,
2003; Jin and Tanaka-Ishii, 2006). Hutchens and
Adler (1998) noticed that entropic chunking has the
effect of reducing the perplexity of a text.
Most methods for unsupervised word segmenta-
tion based solely on local statistics presume a cer-
tain ? albeit minimum ? level of acquaintance with
the target language. For instance, the model of
Huang and Powers (2003) involves some parame-
ters (Markov chain order, numerous threshold val-
ues) that allow its adaptation to the individuality of
written Chinese. In comparison, the method pro-
posed in this paper generalizes easily to a variety of
languages and domains, and is less dependent on an-
notated development data.
The state-of-the-art in unsupervised word seg-
mentation is represented by Bayesian models. Gold-
water et al (2006) justified the importance of
context as a means of avoiding undersegmentation,
through a method based on hierarchical Dirichlet
processes. Mochihashi et al (2009) proposed ex-
tensions to this method, which included a nested
character model and an optimized inference proce-
dure. Johnson and Goldwater (2009) have proposed
a novel method based on adaptor grammars, whose
accuracy surpasses the aforementioned methods by
a large margin, when appropriate assumptions are
made regarding the structural units of a language.
3 Proposed Method
3.1 Word segmentation with MDL
The proposed two-part code incorporates some ex-
tensions of models presented in related work, aimed
at achieving a more precise estimation of the repre-
sentation length. We first introduce the general two-
part code, which consists of:
? the model, embodied by a codebook, i.e., a lexi-
con of unique word typesM = {w1, ..., w|M |},
? the source text D, obtained through encoding
the corpus using the lexicon.
The total description length amounts to the num-
ber of bits necessary for simultaneous transmission
of the codebook and the source text. Therefore, our
objective is to minimize the combined description
length of both terms:
L(D,M) = L(M) + L(D|M).
The description length of the data given M is cal-
culated using the Shannon-Fano code:
833
L(D|M) = ?
|M |?
j=1
#wj log2 P (wj),
where #wj stands for the frequency of the word wj
in the text.
Different strategies have been proposed in the lit-
erature for the calculation of the codebook cost. A
common technique in segmentation and morphology
induction models is to calculate the product of the
total length in characters of the lexicon and an esti-
mate of the per-character entropy. In this way, both
the probabilities and lengths of words are taken into
consideration. The use of a constant value is an ef-
fective and easily computable approach, but it is far
from precise. For instance, in Yu (2000) the average
entropy per character is measured against the orig-
inal corpus, but this model does not capture the ef-
fects of the word distributions on the observed char-
acter probabilities. For this reason, we propose a
different method: the codebook is modeled as a sep-
arate Markov chain of characters.
A lexicon of characters M ? is defined. The de-
scription length of the lexicon data D? given M ? is
then calculated as:
L(D?|M ?) = ?
|C|?
i=1
#ci log2 P (ci),
where #ci denotes the frequency of a character ci
in the lexicon of hypothesis M . The term L(M ?)
is constant for any choice of hypothesis, as is repre-
sents the character set of a corpus.
The total description length under the proposed
model is thus calculated as:
L(M) + L(D|M) = L(M ?) + L(D?|M ?) + L(D|M) =
?
|C|?
i=1
#ci log2 P (ci)?
|M |?
j=1
#wj log2 P (wj) +O(1).
A rigorous definition should include two addi-
tional terms, L(?|M) and L(??|M ?), which give the
representation cost of the parameters of both mod-
els. The L(?|M) can be calculated as:
L(?|M) =
|M | ? 1
2
? log2 S,
where |M | ? 1 gives the number of parameters (de-
grees of freedom), and S is the size of the dataset
(the total length of the text in characters). The para-
metric complexity term is calculated in the same
way for the lexicon. For a derivation of the above
formula, refer to e.g. Li (1998).
MDL is closely related to Bayesian inference. De-
pending on the choice of a universal code, the two
approaches can overlap, as is the case with the two-
part code discussed in this paper. It can be shown
that the model selection in our method is equiva-
lent to a MAP inference, conducted under the as-
sumption that the prior probability of a model de-
creases exponentially with its length (Goldwater,
2006). Thus, the task that we are trying to accom-
plish is to conduct a focused search through the hy-
pothesis space that will allow us to obtain an approx-
imation of the MAP solution in a reasonable time.
The MDL framework does not provide standard
search algorithms for obtaining the hypotheses that
minimize the description length. In the rest of this
section, we will describe an efficient technique suit-
able for the word segmentation task.
3.2 Obtaining an initial hypothesis
First, a rough initial hypothesis is built by an algo-
rithm that combines the branching entropy and MDL
criteria.
Given a setX , comprising all the characters found
in a text, the entropy of branching at position k of the
text is defined as:
H(Xk|xk?1, ..., xk?n) =
?
?
x?X
P (x|xk?1, ..., xk?n) log2 P (x|xk?1, ..., xk?n),
where xk represents the character found at position
k, and n is the order of the Markov model over char-
acters. For brevity, hereafter we shall denote the ob-
served sequence {xk?1, ..., xk?n} as {xk?1:k?n} .
The above definition is extended to combine the
entropy estimates in the left-to-right and right-to-
left directions, as this factor has reportedly improved
performance figures for models based on branching
entropy (Jin and Tanaka-Ishii, 2006). The estimates
in both directions are summed up, yielding a single
value per position:
834
H ?(Xk;k?1|xk?1:k?n;xk:k+n?1) =
?
?
x?X
P (x|xk?1:k?n) log2 P (x|xk?1:k?n)
?
?
x?X
P (x|xk:k+n?1) log2 P (x|xk:k+n?1).
Suffix arrays are employed during the collection
of frequency statistics. For a character model of or-
der n over a testing corpus of size t and a training
corpus of size m, suffix arrays allow these to be
acquired in O(tn logm) time. Faster implementa-
tions reduce the complexity toO(t(n+logm)). For
further discussion, see Manber and Myers (1991).
During the experiments, we did not use the caching
functionality provided by the suffix array library, but
instead kept the statistics for the current iterative
pass (n-gram order and direction) in a local table.
The chunking technique we adopt is to insert a
boundary when the branching entropy measured in
sequences of length n exceeds a certain threshold
value (H(X|xk?1:k?n) > ?). Both n and ? are fixed.
Within the described framework, the increase in
context length n promotes precision and recall at
first, but causes a performance degradation when the
entropy estimates become unreliable due to the re-
duced frequencies of long strings. High threshold
values produce a combination of high precision and
low recall, while low values result in low precision
and high recall.
Since the F-score curve obtained as decreasing
values are assigned to the threshold is typically uni-
modal as in many applications of MDL, we employ
a bisection search routine for the estimation of the
threshold (Algorithm 1).
All positions of the dataset are sorted by their en-
tropy values. At each iteration, at most two new
hypotheses are built, and their description lengths
are calculated in time linear to the data size. The
computational complexity of the described routine
is O(t log t), where t is the corpus length in charac-
ters.
The order of the Markov chain n used during the
entropy calculation is the only input variable of the
proposed model. Since different values perform the
best across the various languages, the most appro-
priate settings can be obtained with the help of a
small annotated corpus. However, the MDL objec-
tive also enables unsupervised optimization against
Algorithm 1 Generates an initial hypothesis.
thresholds[] := sorted H(Xk) values;
threshold := median of thresholds[];
step := length of thresholds[]/4;
direction := ascending;
minimum := +?;
while step > 0 do
nextThreshold := thresholds[] value one step in last
direction;
DL = calculateDL(nextThreshold);
if DL < minimum then
minimum:= DL; threshold := nextThreshold;
step := step/2; continue;
end if
reverse direction;
nextThreshold := thresholds[] value one step in last
direction;
if DL < minimum then
minimum:= DL; threshold := nextThreshold;
step := step/2; continue;
end if
reverse direction;
step := step/2;
end while
Corpus [1] [2] [3] [4]
CHILDES 394655.52 367711.66 368056.10 405264.53
Kyoto 1.291E+07 1.289E+07 1.398E+07 1.837E+07
Table 1: Length in bits of the solutions proposed by Al-
gorithm 1 with respect to the character n-gram order.
a sufficiently large unlabeled dataset. The order that
minimizes the description length of the data can be
discovered in a few iterations of Algorithm 1 with
increasing values of n, and it typically matches the
optimal value of the parameter (Table 1).
Although an acceptable initial segmentation can
be built using the described approach, it is possible
to obtain higher accuracy with an extended model
that takes into account the statistics of Markov
chains from several orders during the entropy calcu-
lation. This can be done by summing up the entropy
estimates, in the way introduced earlier for combin-
ing the values in both directions:
H ??(Xk;k?1|xk?1:k?n;xk:k+n?1) =
?
nmax?
n=1
(
?
x?X
P (x|xk?1:k?n) log2 P (x|xk?1:k?n)
+
?
x?X
P (x|xk:k+n?1) log2 P (x|xk:k+n?1)),
835
where nmax is the index of the highest order to be
taken into consideration.
3.3 Refining the initial hypothesis
In the second phase of the proposed method, we will
refine the initial hypothesis through the reorganiza-
tion of local co-occurrences which produce redun-
dant description length. We opt for greedy optimiza-
tion, as our primary interest is to further explore the
impact that description length minimization has on
accuracy. Of course, such an approach is unlikely
to obtain global minima, but it is a feasible means of
conducting the optimization process, and guarantees
a certain increase in compression.
Since a preliminary segmentation is available, it
is convenient to proceed by inserting or removing
boundaries in the text, thus splitting or merging the
already discovered tokens. The ranked positions in-
volved in the previous step can be reused here, as
this is a way to bias the search towards areas of
the text where boundaries are more likely to occur.
Boundary insertion should start in regions where the
branching entropy is high, and removal should first
occur in regions where the entropy is close to zero.
A drawback of this approach is that it omits loca-
tions where the gains are not immediately obvious,
as it cannot assess the cumulative gains arising from
the merging or splitting of all occurrences of a cer-
tain pair (Algorithm 2).
A clean-up routine, which compensates for this
shortage, is also implemented (Algorithm 3). It op-
erates directly on the types found in the lexicon pro-
duced by Algorithm 2, and is capable of modify-
ing a large number of occurrences of a given pair
in a single step. The lexicon types are sorted by
their contribution to the total description length of
the corpus. For each word type, splitting or merg-
ing is attempted at every letter, beginning from the
center. The algorithm eliminates unlikely types with
low contribution, which represent mostly noise, and
redistributes their cost among more likely ones. The
design of the merging routine makes it impossible to
produce types longer than the ones already found in
the lexicon, as an exhaustive search would be pro-
hibitive.
The evaluation of each hypothetical change in
the segmentation requires that the description length
of the two-part code is recalculated. In order to
Algorithm 2 Compresses local token co-occurrences.
path[][]:= positions sorted by H(Xk) values;
minimum := DL of model produced at initialization;
repeat
for i = max H(Xk) to min H(Xk) do
pos:= path[i][k];
if no boundary exists at pos then
leftToken := token to the left;
rightToken := token to the right;
longToken := leftToken + rightToken;
calculate DL after splitting;
if DL < minimum then
accept split, update model, update DP vari-
ables;
end if
end if
end for
for i = min H(Xk) to max H(Xk) do
merge leftToken and rightToken into longToken
if DL will decrease (analogous to splitting)
end for
until no change is evident in model
Algorithm 3 A lexicon clean-up procedure.
types[] := lexicon types sorted by cost;
minimum := DL of model produced by Algorithm 2;
repeat
for i = min cost to max cost do
for pos = middle to both ends of types[i] do
longType := types[i];
leftType := sequence from first character to
pos;
rightType:= sequence from pos to last charac-
ter;
calculate DL after splitting longType into left-
Type and rightType;
if DL < minimum then
accept split, update model, update DP vari-
ables;
break out of inner loop;
end if
end for
end for
types[] := lexicon types sorted by cost;
for i = max cost to min cost do
for pos = middle to both ends of types[i] do
merge leftType and rightType into longType if
DL will decrease (analogous to splitting)
break out of inner loop;
end for
end for
until no change is evident in model
836
make this optimization phase computationally fea-
sible, dynamic programming is employed in Algo-
rithms 2 and 3. The approach adopted for the re-
calculation of the source text term L(D|M) is ex-
plained below. The estimation of the lexicon cost is
analogous. The term L(D|M) can be rewritten as:
L(D|M) = ?
|M |?
j=1
#wj log2
#wj
N
=
?
|M |?
j=1
#wj log2 #wj +N log2N = T1 + T2,
where #wj is the frequency of wj in the segmented
corpus, and N =
?|M |
j=1 #wj is the cumulative to-
ken count. In order to calculate the new length, we
keep the values of the terms T1 and T2 obtained at
the last change of the model. Their new values are
computed for each hypothetical split or merge on the
basis of the last values, and the expected description
length is calculated as their sum. If the produced es-
timate is lower, the model is modified and the new
values of T1 and T2 are stored for future use.
In order to maintain precise token counts, Algo-
rithms 2 and 3 recognize the fact that recurring se-
quences (?byebye? etc.) appear in the corpora, and
handle them accordingly. Known boundaries, such
as the sentence boundaries in the CHILDES corpus,
are also taken into consideration.
4 Experimental Settings
We evaluated the proposed model against four
datasets. The first one is the Bernstein-Ratner cor-
pus for language acquisition based on transcripts
from the CHILDES database (Bernstein-Ratner,
1987). It comprises phonetically transcribed utter-
ances of adult speech directed to 13 through 21-
month-old children. We evaluated the performance
of our learner in the cases when the few boundaries
among the individual sentences are available to it
(B), and when it starts from a blank state (N). The
Kyoto University Corpus (Kurohashi and Nagao,
1998) is a standard dataset for Japanese morpho-
logical and dependency structure analysis, which
comprises newspaper articles and editorials from the
Mainichi Shimbun. The BEST corpus for word seg-
mentation and named entity recognition in Thai lan-
guage combines text from a variety of sources in-
Corpus Language Size
(MB)
Chars
(K)
Tokens
(K)
Types
(K)
CHILDES-
B/N
English 0.1 95.8 33.3 1.3
Kyoto Japanese 5.02 1674.9 972.9 39.5
WSJ English 5.22 5220.0 1174.2 49.1
BEST-E Thai 12.64 4360.2 1163.2 26.2
BEST-N Thai 18.37 6422.7 1659.4 36.3
BEST-A Thai 4.59 1619.9 438.7 13.9
BEST-F Thai 16.18 5568.0 1670.8 22.6
Wikipedia Japanese 425.0 169069.3 / /
Asahi Japanese 337.2 112401.1 / /
BEST-All Thai 51.2 17424.0 4371.8 73.4
Table 2: Corpora used during the evaluation. Precise to-
ken and type counts have been omitted for Wikipedia and
Asahi, as no gold standard segmentations are available.
cluding encyclopedias (E), newspaper articles (N),
scientific articles (A), and novels (F). The WSJ sub-
set of the Penn Treebank II Corpus incorporates
selected stories from the Wall Street Journal, year
1989 (Marcus et al, 1994). Both the original text
(O), and a version in which all characters were con-
verted to lower case (L) were used.
The datasets listed above were built by remov-
ing the tags and blank spaces found in the corpora,
and concatenating the remaining text. We added
two more training datasets for Japanese, which were
used in a separate experiment solely for the acqui-
sition of frequency statistics. One of them was
created from 200,000 randomly chosen Wikipedia
articles, stripped from structural elements. The
other one contains text from the year 2005 issues of
Asahi Newspaper. Statistics regarding all described
datasets are presented in Table 2.
One whole corpus is segmented in each experi-
ment, in order to avoid the statement of an extended
model that would allow the separation of training
and test data. This setting is also necessary for the
direct comparison between the proposed model and
other recent methods evaluated against the entire
CHILDES corpus.
We report the obtained precision, recall and F-
score values calculated using boundary, token and
type counts. Precision (P) and recall (R) are defined
as:
P =
#correct units
# output units
, R =
#correct units
#gold standard units
.
Boundary, token and lexicon F-scores, denoted
as B-F and T -F and L-F , are calculated as the
837
Model Corpus & Settings B-Prec B-Rec B-F T-Prec T-Rec T-F DL
(bits)
Ref.DL
(bits)
Time
(ms)
1 CHILDES, ? = 1.2, n = [1-6] 0.8667 0.8898 0.8781 0.6808 0.6990 0.6898 344781.74 1060.2
2a (H?) CHILDES, n = 2 0.7636 0.9109 0.8308 0.5352 0.6384 0.5823 367711.66 300490.52 753.1
2b (H??) CHILDES, nmax = 3 0.8692 0.8865 0.8777 0.6792 0.6927 0.6859 347633.07 885.3
1 Kyoto, ? = 0, n = [1-6] 0.8208 0.8208 0.8208 0.5784 0.5784 0.5784 1.325E+07 54958.8
2a (H?) Kyoto, n = 2 0.8100 0.8621 0.8353 0.5934 0.6316 0.6119 1.289E+07 1.120E+07 22909.7
2b (H??) Kyoto, nmax = 2 0.8024 0.9177 0.8562 0.6093 0.6969 0.6501 1.248+E07 23212.8
Table 3: Comparison of the proposed method (2a, 2b) with the model of Jin and Tanaka-Ishii (2006) (1). Execution
times include the obtaining of frequency statistics, and are represented by averages over 10 runs.
harmonic averages of the corresponding precision
and recall values (F = 2PR/(P + R)). As a
rule, boundary-based evaluation produces the high-
est scores among the three evaluation modes, as it
only considers the correspondence between the pro-
posed and the gold standard boundaries at the indi-
vidual positions of the corpora. Token-based evalua-
tion is more strict ? it accepts a word as correct only
if its beginning and end are identified accurately, and
no additional boundaries lie in between. Lexicon-
based evaluation reflects the extent to which the vo-
cabulary of the original text has been recovered.
It provides another useful perspective for the error
analysis, which in combination with token scores
can give a better idea of the relationship between the
accuracy of induction and item frequency.
The system was implemented in Java, however it
handled the suffix arrays through an external C li-
brary called Sary.1 All experiments were conducted
on a 2 GHz Core2Duo T7200 machine with 2 GB
RAM.
5 Results and Discussion
The scores we obtained using the described instan-
tiations of the branching entropy criterion at the ini-
tialization phase are presented in Table 3, along with
those generated by our own implementation of the
method presented in Jin and Tanaka-Ishii (2006),
where the threshold parameter ? was adjusted man-
ually for optimal performance.
The heuristic of Jin and Tanaka-Ishii takes advan-
tage of the trend that branching entropy decreases
as the observed character sequences become longer;
sudden rises can thus be regarded as an indication of
locations where a boundary is likely to exist. Their
method uses a common value for thresholding the
1http://sary.sourceforge.net
entropy change throughout all n-gram orders, and
combines the boundaries discovered in both direc-
tions in a separate step. These properties of the
method would lead to complications if we tried to
employ it in the first phase of our method (i.e. a step
parameter for iterative adjustment of the threshold
value, rules for combining the boundaries, etc.).
The proposed criterion with an automatically de-
termined threshold value produced slightly worse
results than that of Jin and Tanaka-Ishii at the
CHILDES corpus. However, we found out that our
approach achieves approximately 1% higher score
when the best performing threshold value is selected
from the candidate list. There are two observations
that account for the suboptimal threshold choice by
our algorithm. On one hand, the correspondence
between description length and F-score is not abso-
lutely perfect, and this may pose an obstacle to the
optimization process for relatively small language
samples. Another issue lies in the bisection search
routine, which suggests approximations of the de-
scription length minima. The edge that our method
has on the Kyoto corpus can be attributed to a better
estimation of the optimal treshold value due to the
larger amount of data.
The experimental results obtained at the comple-
tion of Algorithm 3 are summarized in Tables 4 and
5. Presented durations include the obtaining of fre-
quency statistics. The nmax parameter is set to the
value which maximizes the compression during the
initial phase, in order to make the results representa-
tive of the case in which no annotated development
corpora are accessible to the algorithm.
It is evident that after the optimization carried out
in the second phase, the description length is re-
duced to levels significantly lower than the ground
truth. In this aspect, the algorithm outperforms the
EM-based method of Yu (2000).
838
Corpus & Settings B-F T-F L-F Time
(ms)
CHILDES-B, nmax=3 0.9092 0.7542 0.5890 2597.2
CHILDES-N, nmax=3 0.9070 0.7499 0.5578 2949.3
Kyoto, nmax=2 0.8855 0.7131 0.3725 70164.6
BEST-E, nmax=5 0.9081 0.7793 0.3549 738055.0
BEST-N, nmax=5 0.8811 0.7339 0.2807 505327.0
BEST-A, nmax=5 0.9045 0.7632 0.4246 250863.0
BEST-F, nmax=5 0.9343 0.8216 0.4820 305522.0
WSJ-O, nmax=6 0.8405 0.6059 0.3338 658214.0
WSJ-L, nmax=6 0.8515 0.6373 0.3233 582382.0
Table 4: Results obtained after the termination of Algo-
rithm 3.
Corpus & Settings Description
Length (Proposed)
Description
Length (Total)
CHILDES-B, nmax=3 290592.30 300490.52
CHILDES-N, nmax=3 290666.12 300490.52
Kyoto, nmax=2 1.078E+07 1.120E+07
BEST-E, nmax=5 1.180E+07 1.252E+07
BEST-N, nmax=5 1.670E+07 1.809E+07
BEST-A, nmax=5 4438600.32 4711363.62
BEST-F, nmax=5 1.562E+07 1.634E+07
WSJ-O, nmax=6 1.358E+07 1.460E+07
WSJ-L, nmax=6 1.317E+07 1.399E+07
Table 5: Description length - proposed versus reference
segmentation.
We conducted experiments involving various ini-
tialization strategies: scattering boundaries at ran-
dom throughout the text, starting from entirely un-
segmented state, or considering each symbol of the
text to be a separate token. The results obtained
with random initialization confirm the strong rela-
tionship between compression and segmentation ac-
curacy, evident in the increase of token F-score be-
tween the random initialization and the termination
of the algorithm, where description length is lower
(Table 6). They also reveal the importance of the
branching entropy criterion to the generation of hy-
potheses that maximize the evaluation scores and
compression, as well as the role it plays in the re-
duction of computational time.
T-F-Score Description Time
Random Init Refinement Length (ms)
0.0441 (0.25) 0.3833 387603.02 6660.4
0.0713 (0.50) 0.3721 383279.86 4975.1
0.0596 (0.75) 0.2777 412743.67 3753.3
Table 6: Experimental results for CHILDES-N with ran-
domized initialization and search path. The numbers in
brackets represent the seed boundaries/character ratios.
The greedy algorithms fail to suggest any opti-
mizations that improve the compression in the ex-
treme cases when the boundaries/character ratio is
either 0 or 1. When no boundaries are given, split-
ting operations produce unique types with a low
frequency that increase the cost of both parts of
the MDL code, and are rejected. The algorithm
runs slowly, as each evaluation operates on candi-
date strings of enormous length. Similarly, when the
corpus is broken down into single-character tokens,
merging individual pairs does not produce any in-
crease in compression. This could be achieved by an
algorithm that estimates the total effect from merg-
ing all instances of a given pair, but such an algo-
rithm would be computationally infeasible for large
corpora.
Finally, we tried randomizing the search path for
Algorithm 2 after an entropy-guided initialization, to
observe a small deterioration in accuracy in the final
segmentation (less than 1% on average).
Figure 1a illustrates the effect that training data
size has on the accuracy of segmentation for the Ky-
oto corpus. The learning curves are similar through-
out the different corpora. For the CHILDES cor-
pus, which has a rather limited vocabulary, token
F-score above 70% can be achieved for datasets as
small as 5000 characters of training data, provided
that reasonable values are set for the nmax parameter
(we used the values presented in Table 4 throughout
these experiments).
Figure 1b shows the evolution of token F-score by
stage for all corpora. The initialization phase seems
to have the highest contribution to the formation of
the final segmentation, and the refinement phase is
highly dependent on the output it produces. As a
consequence, results improve when a more adequate
language sample is provided during the learning of
local dependencies at initialization. This is evident
in the experiments with the larger unlabeled Thai
and Japanese corpora.
For Japanese language with the setting for the
nmax parameter that maximized compression, we
observed an almost 4% increase in the token F-score
produced at the end of the first phase with the Asahi
corpus as training data. Only a small (less than 1%)
rise was observed in the overall performance. The
quite larger dataset of randomly chosen Wikipedia
articles achieved no improvement. We attributed this
839
Figure 1: a) corpus size / accuracy relationship (Kyoto); b) accuracy levels by phase; c) accuracy levels by phase
with various corpora for frequency statistics (Kyoto); d) accuracy levels by phase with different corpora for frequency
statistics (BEST).
to the higher degree of correspondence between the
domains of the Asahi and Kyoto corpora (Figure 1c).
Experiments with the BEST corpus reveal bet-
ter the influence of domain-specific data on the ac-
curacy of segmentation. Performance deteriorates
significantly when out-of-domain training data is
used. In spite of its size, the assorted composite cor-
pus, in which in-domain and out-of-domain training
data are mixed, produces worse results than the cor-
pora which include only domain-specific data (Fig-
ure 1d).
Finally, a comparison of the proposed method
with Bayesian n-gram models is presented in Ta-
ble 7. Through the increase of compression in the
refinement phase of the algorithm, accuracy is im-
proved by around 3%, and the scores approach those
of the explicit probabilistic models of Goldwater et
al. (2009) and Mochihashi et al (2009). The pro-
posed learner surpasses the other unsupervised word
induction models in terms of processing speed. It
should be noticed that a direct comparison of accu-
racy is not possible with Mochihashi et al (2009),
as they evaluated their system with separate datasets
for training and testing. Furthermore, different seg-
mentation standards exist for Japanese, and there-
fore the ?ground truth? provided by the Kyoto cor-
pus cannot be considered an ideal measure of accu-
racy.
6 Conclusions and Future Work
This paper has presented an efficient algorithm for
unsupervised word induction, which relies on a
combination of evidences. New instantiations of the
branching entropy and MDL criteria have been pro-
posed and evaluated against corpora in different lan-
guages. The MDL-based optimization eliminates
the discretion in the choice of the context length
and threshold parameters, common in segmenta-
tion models based on local statistics. At the same
time, the branching entropy criterion enables a con-
strained search through the hypothesis space, allow-
ing the proposed method to demonstrate a very high
840
Model Corpus T-Prec T-Rec T-F L-Prec L-Rec L-F Time
NPY(3) CHILDES 0.7480 0.7520 0.7500 0.4780 0.5970 0.5310 17 min
NPY(2) CHILDES 0.7480 0.7670 0.7570 0.5730 0.5660 0.5700 17 min
HDP(2) CHILDES 0.7520 0.6960 0.7230 0.6350 0.5520 0.5910 -
Ent-MDL CHILDES 0.7634 0.7453 0.7542 0.6844 0.5170 0.5890 2.60 sec
NPY(2) Kyoto - - 0.6210 - - - -
NPY(3) Kyoto - - 0.6660 - - - -
Ent-MDL Kyoto 0.6912 0.7365 0.7131 0.5908 0.2720 0.3725 70.16 sec
Table 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al, 2009 (NPY) and
Goldwater et al, 2009 (HDP).
performance in terms of both accuracy and speed.
Possible improvements of the proposed method
include modeling the dependencies among neigh-
boring tokens, which would allow the evaluation
of the context to be reflected in the cost func-
tion. Mechanisms for stochastic optimization imple-
mented in the place of the greedy algorithms could
provide an additional flexibility of search for such
more complex models. As the proposed approach
provides significant performance improvements, it
could be utilized in the development of more so-
phisticated novel word induction schemes, e.g. en-
semble models trained independently with different
data. Of course, we are also going to explore the
model?s potential in the setting of semi-supervised
morphological analysis.
References
Bernstein-Ratner, Nan 1987. The phonology of parent ?
child speech. Childrens Language, 6:159?174
Brent, Michael R and Timothy A. Cartwright. 1996. Dis-
tributional Regularity and Phonotactic Constraints are
Useful for Segmentation. Cognition 61: 93?125
Goldwater, Sharon. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Brown University,
Ph.D. Thesis
Goldwater, Sharon, Thomas L. Griffiths and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, Sydney, 673?680
Goldwater, Sharon, Thomas L. Griffiths and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112:1, 21?54.
Harris, Zellig. 1955. From Phoneme to Morpheme. Lan-
guage, 31(2):190-222.
Huang, Jin H. and David Powers. 2003. Chinese Word
Segmentation Based on Contextual Entropy. Proceed-
ings of 17th Pacific Asia Conference, 152?158
Hutchens, Jason L. and Michael D. Alder. 1998. Finding
structure via compression. Proceedings of the Inter-
national Conference on Computational Natural Lan-
guage Learning, 79?82
Jin, Zhihui and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised Segmentation of Chinese Text by Use of Branch-
ing Entropy. Proceedings of the COLING/ACL on
Main conference poster sessions, 428?435
Johnson, Mark and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Association for Computational Linguistics, 317?325.
Kempe, Andre. 1999. Experiments in Unsupervised
Entropy Based Corpus Segmentation. Proceedings of
CoNLL?99, pp. 371?385
Kit, Chunyu. 2003. How does lexical acquisition begin?
A cognitive perspective. Cognitive Science 1(1): 1?
50.
Kurohashi, Sadao and Makoto Nagao. 1998. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. Proceedings of the First International
Conference on Language Resources and Evaluation,
Granada, Spain, 719?724
Lafferty, John, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the International Conference on Machine
Learning.
Li, Hang. 1998. A Probabilistic Approach to Lexical
Semantic Knowledge Acquisition and Structural Dis-
ambiguation. University of Tokyo, Ph.D. Thesis
Liang, Percy. 2005. Semi-Supervised Learning for Nat-
ural Language. Massachusets Institute of Technology,
Master?s Thesis.
Manber, Udi and Gene Myers. 1991. Suffix arrays: a
new method for on-line string searches. SIAM Journal
on Computing 22:935?948
841
Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating Predicate Argument Structure. Hu-
man Language Technology, 114?119
Mochihashi, Daiichi, Takeshi Yamada and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation
of Natural Language Processing, 1: 100?108
Rissanen, Jorma. 1978. Modeling by Shortest Data De-
scription. Aulomatica, 14:465?471.
Saffran, Jenny R., Richard N. Aslin and Elissa L. New-
port. 1996. Statistical learning in 8-month-old infants
Science; 274:1926-1928
Tsuboi, Yuta, Hisashi Kashima., Hiroki Oda, Shinsuke
Mori and Yuji Matsumoto. 2008. Training Condi-
tional Random Fields Using Incomplete Annotations.
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1,897?904.
Yu, Hua. 2000. Unsupervised word induction using
MDL criterion. Proceedings of tne International Sym-
posium of Chinese Spoken Language Processing, Bei-
jing.
Zipf, George K. 1949. Human Behavior and the Princi-
ple of Least Effort. Addison-Wesley.
842
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153?158,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Word Alignment Using Frequency Constraint in Posterior
Regularized EM
Hidetaka Kamigaito
1,2
, Taro Watanabe
2
, Hiroya Takamura
1
, Manabu Okumura
1
1
Tokyo Institute of Technology, Precision and Intelligence Laboratory
4259 Nagatsuta-cho Midori-ku Yokohama, Japan
2
National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
Generative word alignment models, such
as IBM Models, are restricted to one-
to-many alignment, and cannot explicitly
represent many-to-many relationships in
a bilingual text. The problem is par-
tially solved either by introducing heuris-
tics or by agreement constraints such that
two directional word alignments agree
with each other. In this paper, we fo-
cus on the posterior regularization frame-
work (Ganchev et al., 2010) that can force
two directional word alignment models
to agree with each other during train-
ing, and propose new constraints that can
take into account the difference between
function words and content words. Ex-
perimental results on French-to-English
and Japanese-to-English alignment tasks
show statistically significant gains over the
previous posterior regularization baseline.
We also observed gains in Japanese-to-
English translation tasks, which prove the
effectiveness of our methods under gram-
matically different language pairs.
1 Introduction
Word alignment is an important component in sta-
tistical machine translation (SMT). For instance
phrase-based SMT (Koehn et al., 2003) is based
on the concept of phrase pairs that are automat-
ically extracted from bilingual data and rely on
word alignment annotation. Similarly, the model
for hierarchical phrase-based SMT is built from
exhaustively extracted phrases that are, in turn,
heavily reliant on word alignment.
The Generative word alignment models, such as
the IBM Models (Brown et al., 1993) and HMM
(Vogel et al., 1996), are popular methods for au-
tomatically aligning bilingual texts, but are re-
stricted to represent one-to-many correspondence
of each word. To resolve this weakness, vari-
ous symmetrization methods are proposed. Och
and Ney (2003) and Koehn et al. (2003) propose
various heuristic methods to combine two direc-
tional models to represent many-to-many relation-
ships. As an alternative to heuristic methods, fil-
tering methods employ a threshold to control the
trade-off between precision and recall based on
a score estimated from the posterior probabili-
ties from two directional models. Matusov et al.
(2004) proposed arithmetic means of two mod-
els as a score for the filtering, whereas Liang et
al. (2006) reported better results using geometric
means. The joint training method (Liang et al.,
2006) enforces agreement between two directional
models. Posterior regularization (Ganchev et al.,
2010) is an alternative agreement method which
directly encodes agreement during training. DeN-
ero and Macherey (2011) and Chang et al. (2014)
also enforce agreement during decoding.
However, these agreement models do not take
into account the difference in language pairs,
which is crucial for linguistically different lan-
guage pairs, such as Japanese and English: al-
though content words may be aligned with each
other by introducing some agreement constraints,
function words are difficult to align.
We focus on the posterior regularization frame-
work and improve upon the previous work by
proposing new constraint functions that take into
account the difference in languages in terms of
content words and function words. In particular,
we differentiate between content words and func-
tion words by frequency in bilingual data, follow-
ing Setiawan et al. (2007).
Experimental results show that the proposed
methods achieved better alignment qualities on the
French-English Hansard data and the Japanese-
English Kyoto free translation task (KFTT) mea-
sured by AER and F-measure. In translation eval-
uations, we achieved statistically significant gains
153
in BLEU scores in the NTCIR10.
2 Statistical word alignment with
posterior regularization framework
Given a bilingual sentence x = (x
s
,x
t
) where x
s
andx
t
denote a source and target sentence, respec-
tively, the bilingual sentence is aligned by a many-
to-many alignment of y. We represent posterior
probabilities from two directional word alignment
models as
??
p
?
(
??
y |x) and
??
p
?
(
??
y |x) with each ar-
row indicating a particular direction, and use ? to
denote the parameters of the models. For instance,
??
y is a subset of y for the alignment from x
s
to
x
t
under the model of p(x
t
,
??
y |x
s
). In the case of
IBM Model 1, the model is represented as follows:
p(xt,??y |xs) =
?
i
1
|xs|+ 1
p
t
(x
t
i
|xs??
y
i
). (1)
where we define the index of x
t
, x
s
as i, j(1 ?
i ? |x
t
|, 1 ? j ? |x
s
|) and the posterior probabil-
ity for the word pair (x
t
i
, x
s
j
) is defined as follows:
??
p (i, j|x) =
p
t
(x
t
i
|x
s
j
)
?
j
?
p
t
(x
t
i
|x
s
j
?
)
. (2)
Herein, we assume that the posterior probabil-
ity for wrong directional alignment is zero (i.e.,
??
p (
??
y |x) = 0).
1
Given the two directional mod-
els, Ganchev et al. defined a symmetric feature for
each target/source position pair, i, j as follows:
?
i,j
(x,y) =
?
?
?
+1 (
??y ? y) ? (??y
i
= j),
?1 (
??y ? y) ? (??y
j
= i),
0 otherwise.
(3)
The feature assigns 1 for the subset of word align-
ment for
??
y , but assigns ?1 for
??
y . As a result,
if a word pair i, j is aligned with equal posterior
probabilities in two directions, the expectation of
the feature value will be zero. Ganchev et al. de-
fined a joint model that combines two directional
models using arithmetic means:
p
?
(y|x) =
1
2
??
p
?
(y|x) +
1
2
??
p
?
(y|x). (4)
Under the posterior regularization framework, we
instead use q that is derived by maximizing the fol-
lowing posterior probability parametrized by ? for
each bilingual data x as follows (Ganchev et al.,
2010):
q?(y|x) =
??
p
?
(
??y |x) +??p
?
(
??y |x)
2
(5)
?
exp{?? ? ?(x,y)}
Z
1
No alignment is represented by alignment into a special
token ?null?.
=
??
q (
??y |x)
Z??
q
??
p
?
(x) +
??
q (
??y |x)
Z??
q
??
p
?
(x)
2Z
,
Z =
1
2
(
Z
??
q
??
p
?
+
Z
??
q
??
p
?
),
??
q (
??y |x) =
1
Z
??
q
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
Z
??
q
=
?
??
y
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
??
q (
??y |x) =
1
Z
??
q
??
p
?
(
??y ,x)exp{?? ? ?(x, y)},
Z
??
q
=
?
??
y
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
such that E
q? [?i,j(x,y)] = 0. In the E-step of
EM-algorithm, we employ q? instead of p? to ac-
cumulate fractional counts for its use in the M-
step. ? is efficiently estimated by the gradient as-
cent for each bilingual sentence x. Note that pos-
terior regularization is performed during parame-
ter estimation, and not during testing.
3 Posterior Regularization with
Frequency Constraint
The symmetric constraint method represented in
Equation (3) assumes a strong one-to-one rela-
tion for any word, and does not take into account
the divergence in language pairs. For linguisti-
cally different language pairs, such as Japanese-
English, content words may be easily aligned one-
to-one, but function words are not always aligned
together. In addition, Japanese is a pro-drop lan-
guage which can easily violate the symmetric con-
straint when proper nouns in the English side have
to be aligned with a ?null? word. In addition, low
frequency words may cause unreliable estimates
for adjusting the weighing parameters ?.
In order to solve the problem, we improve
Ganchev?s symmetric constraint so that it can con-
sider the difference between content words and
function words in each language. In particular, we
follow the frequency-based idea of Setiawan et al.
(2007) that discriminates content words and func-
tion words by their frequencies. We propose con-
straint features that take into account the differ-
ence between content words and function words,
determined by a frequency threshold.
3.1 Mismatching constraint
First, we propose a mismatching constraint that
penalizes word alignment between content words
and function words by decreasing the correspond-
ing posterior probabilities.
154
The constraint is represented as f2c (function to
content) constraint:
?
f2c
i,j
(x,y) = (6)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+1 (
??y ? y) ? (??y
i
= j) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
j
= i) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
i
= j) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) < 0),
?1 (
??y ? y) ? (??y
j
= i) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) < 0).
where ?
i,j
(x,y) =
??
p
?
(i, j|x) ?
??
p
?
(i, j|x) is
the difference in the posterior probabilities be-
tween the source-to-target and the target-to-source
alignment. C
s
and C
t
represent content words in
the source sentence and target sentence, respec-
tively. Similarly, F
s
and F
t
are function words
in the source and target sentence, respectively. In-
tuitively, when there exists a mismatch in content
word and function word for a word pair (i, j), the
constraint function returns a non-zero value for
the model with the highest posterior probability.
When coupled with the constraint such that the ex-
pectation of the feature value is zero, the constraint
function decreases the posterior probability of the
highest direction and discourages agreement with
each other.
Note that when this constraint is not fired, we
fall back to the constraint function in Equation (3)
for each word pair.
3.2 Matching constraint
In contrast to the mismatching constraint, our
second constraint function rewards alignment for
function to function word matching, namely f2f.
The f2f constraint function is defined as follows:
?
f2f
i,j
(x,y) = (7)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+1 (
??y ? y) ? (??y
i
= j)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
j
= i)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
i
= j)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) < 0),
?1 (
??y ? y) ? (??y
j
= i)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) < 0).
This constraint function returns a non-zero value
for a word pair (i, j) when they are function
words. As a result, the pair of function words
are encouraged to agree with each other, but not
other pairs. The content to content word matching
function c2c can be defined similarly by replac-
ing F
s
and F
t
by C
s
and C
t
, respectively. Like-
wise, the function to content word matching func-
tion f2c is defined by considering the matching
of content words and function words in two lan-
guages. As noted in the mismatch function, when
no constraint is fired, we fall back to Eq (3) for
each word pair.
4 Experiment
4.1 Experimental Setup
The data sets used in our experiments are the
French-English Hansard Corpus, and two data sets
for Japanese-English tasks: the Kyoto free trans-
lation task (KFTT) and NTCIR10. The Hansard
Corpus consists of parallel texts drawn from of-
ficial records of the proceedings of the Canadian
Parliament. The KFTT (Neubig, 2011) is derived
from Japanese Wikipedia articles related to Ky-
oto, which is professionally translated into En-
glish. NTCIR10 comes from patent data employed
in a machine translation shared task (Goto et al.,
2013). The statistics of these data are presented in
Table 1.
Sentences of over 40 words on both source and
target sides are removed for training alignment
models. We used a word alignment toolkit ci-
cada
2
for training the IBM Model 4 with our
proposed methods. Training is bootstrapped from
IBM Model 1, followed by HMM and IBM Model
4. When generating the final bidirectional word
alignment, we use a grow-diag-final heuristic for
the Japanese-English tasks and an intersection
heuristic in the French-English task, judged by
preliminary studies.
Following Bisazza and Federico (2012), we
automatically decide the threshold for word fre-
quency to discriminate between content words and
function words. Specifically, the threshold is de-
termined by the ratio of highly frequent words.
The threshold th is the maximum frequency that
satisfies the following equation:
?
w?(freq(w)>th)
freq(w)
?
w?all
freq(w)
> r. (8)
Here, we empirically set r = 0.5 by preliminary
studies. This method is based on the intuition that
content words and function words exist in a docu-
ment at a constant rate.
4.2 Word alignment evaluation
We measure the impact of our proposed meth-
ods on the quality of word alignment measured
2
https://github.com/tarowatanabe/cicada
155
Table 1: The statistics of the data sets
hansard kftt NTCIR10
French English Japanese English Japanese English
train sentence 1.13M 329.88K 2.02M
word 23.3M 19.8M 6.08M 5.91M 53.4M 49.4M
vocabulary 78.1K 57.3K 114K 138K 114K 183K
dev sentence 1.17K 2K
word 26.8K 24.3K 73K 67.3K
vocabulary 4.51K 4.78K 4.38K 5.04K
test WA sentence 447 582
word 7.76K 7.02K 14.4K 12.6K
vocabulary 1,92K 1.69K 2.57K 2.65K
TR sentence 1.16K 8.6K
word 28.5K 26.7K 334K 310K
vocabulary 4.91K 4.57K 10.4K 12.7K
Figure 1: Precision Recall graph in Hansard
French-English
Figure 2: Precision Recall graph in KFTT
Figure 3: AER in Hansard French-English Figure 4: AER in KFTT
156
Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
KFTT Hansard (French-English)
method precision recall AER F precision recall AER F
symmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822
f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844
c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804
f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848
by AER and F-measure (Och and Ney, 2003).
Since there exists no distinction for sure-possible
alignments in the KFTT data, we use only sure
alignment for our evaluation, both for the French-
English and the Japanese-English tasks. Table 2
summarizes our results.
The baseline method is symmetric constraint
(Ganchev et al., 2010) shown in Table 2. The num-
bers in bold and in italics indicate the best score
and the second best score, respectively. The dif-
ferences between f2f,f2c and baseline in KFTT are
statistically significant at p < 0.05 using the sign-
test, but in hansard corpus, there exist no signifi-
cant differences between the baseline and the pro-
posed methods. In terms of F-measure, it is clear
that the f2f method is the most effective method
in KFTT, and both f2f and f2c methods exceed the
original posterior regularized model of Ganchev et
al. (2010).
We also compared these methods with filtering
methods (Liang et al., 2006), in addition to heuris-
tic methods. We plot precision/recall curves and
AER by varying the threshold between 0.1 and
0.9 with 0.1 increments. From Figures, it can be
seen that our proposed methods are superior to
the baseline in terms of both precision-recall and
AER.
4.3 Translation evaluation
Next, we performed a translation evaluation, mea-
sured by BLEU (Papineni et al., 2002). We
compared the grow-diag-final and filtering method
(Liang et al., 2006) for creating phrase tables.
The threshold for the filtering factor was set to
0.1 which was the best setting in the word align-
ment experiment in section 4.2 under KFTT. From
the English side of the training data, we trained a
word using the 5-gram model with SRILM (Stol-
cke and others, 2002). ?Moses? toolkit was used
as a decoder (Koehn et al., 2007) and the model
parameters were tuned by k-best MIRA (Cherry
and Foster, 2012). In order to avoid tuning insta-
bility, we evaluated the average of five runs (Hop-
kins and May, 2011). The results are summarized
Table 3: Results of translation evaluation
KFTT NTCIR10
GDF Filtered GDF Filtered
symmetric 19.06 19.28 28.3 29.71
f2f 19.15 19.17 28.36 29.74
c2c 19.26 19.02 28.36 29.92
f2c 18.91 19.20 28.36 29.67
in Table 3. Our proposed methods achieved large
gains in NTCIR10 task with the filtered method,
but observed no gain in the KFTT with the filtered
method. In NTCIR10 task with GDF, the gain in
BLEU was smaller than that of KFTT. We cal-
culate p-values and the difference between sym-
metric and c2c (the most effective proposed con-
straint) are lower than 0.05 in kftt with GDF and
NTCIR10 with filtered method. There seems to
be no clear tendency in the improved alignment
qualities and the translation qualities, as shown in
numerous previous studies (Ganchev et al., 2008).
5 Conclusion
In this paper, we proposed new constraint func-
tions under the posterior regularization frame-
work. Our constraint functions introduce a
fine-grained agreement constraint considering the
frequency of words, a assuming that the high
frequency words correspond to function words
whereas the less frequent words may be treated
as content words, based on the previous work of
Setiawan et al. (2007). Experiments on word
alignment tasks showed better alignment quali-
ties measured by F-measure and AER on both the
Hansard task and KFTT. We also observed large
gain in BLEU, 0.2 on average, when compared
with the previous posterior regularization method
under NTCIR10 task.
As our future work, we will investigate more
precise methods for deciding function words and
content words for better alignment and translation
qualities.
157
References
Arianna Bisazza and Marcello Federico. 2012. Cutting
the long tail: Hybrid language models for translation
style adaptation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 439?448. Associ-
ation for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Yin-Wen Chang, Alexander M. Rush, John DeNero,
and Michael Collins. 2014. A constrained viterbi
relaxation for bidirectional word alignment. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1481?1490, Baltimore, Maryland,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 420?429, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001?2049.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-Lingual Information Access, NTCIR-10.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. In Proceedings of COLING 2004, pages 219?
225, Geneva, Switzerland, August 23?27.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th annual meeting on associ-
ation for computational linguistics, pages 712?719.
Association for Computational Linguistics.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
158
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841?851,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Part-of-Speech Induction in Dependency Trees for Statistical Machine
Translation
Akihiro Tamura?,?, Taro Watanabe?, Eiichiro Sumita?,
Hiroya Takamura?, Manabu Okumura?
? National Institute of Information and Communications Technology
{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp
? Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura, oku}@pi.titech.ac.jp
Abstract
This paper proposes a nonparametric
Bayesian method for inducing Part-of-
Speech (POS) tags in dependency trees
to improve the performance of statistical
machine translation (SMT). In particular,
we extend the monolingual infinite tree
model (Finkel et al, 2007) to a bilin-
gual scenario: each hidden state (POS tag)
of a source-side dependency tree emits a
source word together with its aligned tar-
get word, either jointly (joint model), or
independently (independent model). Eval-
uations of Japanese-to-English translation
on the NTCIR-9 data show that our in-
duced Japanese POS tags for dependency
trees improve the performance of a forest-
to-string SMT system. Our independent
model gains over 1 point in BLEU by re-
solving the sparseness problem introduced
in the joint model.
1 Introduction
In recent years, syntax-based SMT has made
promising progress by employing either depen-
dency parsing (Lin, 2004; Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008; Mi and Liu,
2010) or constituency parsing (Huang et al, 2006;
Liu et al, 2006; Galley et al, 2006; Mi and Huang,
2008; Zhang et al, 2008; Cohn and Blunsom,
2009; Liu et al, 2009; Mi and Liu, 2010; Zhang
et al, 2011) on the source side, the target side,
or both. However, dependency parsing, which
is a popular choice for Japanese, can incorporate
only shallow syntactic information, i.e., POS tags,
compared with the richer syntactic phrasal cate-
gories in constituency parsing. Moreover, exist-
ing POS tagsets might not be optimal for SMT
because they are constructed without considering
the language in the other side. Consider the ex-
amples in Figure 1. The Japanese noun ???? in
? ? ?? ?? ? ??
??? ? ??????? ? ???? ??
You can not use the Internet  .
I  pay  usage fees  .
noun particle particlenoun noun verb auxiliary verb
noun particle noun noun verbparticle
[Example 1]
[Example 2]
Japanese POS:
Japanese POS:
Figure 1: Examples of Existing Japanese POS
Tags and Dependency Structures
Example 1 corresponds to the English verb ?use?,
while that in Example 2 corresponds to the English
noun ?usage?. Thus, Japanese nouns act like verbs
in English in one situation, and nouns in English
in another. If we could discriminate POS tags for
two cases, we might improve the performance of a
Japanese-to-English SMT system.
In the face of the above situations, this pa-
per proposes an unsupervised method for inducing
POS tags for SMT, and aims to improve the perfor-
mance of syntax-based SMT by utilizing the in-
duced POS tagset. The proposed method is based
on the infinite tree model proposed by Finkel et
al. (2007), which is a nonparametric Bayesian
method for inducing POS tags from syntactic de-
pendency structures. In this model, hidden states
represent POS tags, the observations they generate
represent the words themselves, and tree structures
represent syntactic dependencies between pairs of
POS tags.
The proposed method builds on this model by
incorporating the aligned words in the other lan-
guage into the observations. We investigate two
types of models: (i) a joint model and (ii) an in-
dependent model. In the joint model, each hid-
den state jointly emits both a source word and its
aligned target word as an observation. The in-
dependent model separately emits words in two
languages from hidden states. By inferring POS
841
tags based on bilingual observations, both mod-
els can induce POS tags by incorporating infor-
mation from the other language. Consider, for ex-
ample, inducing a POS tag for the Japanese word ?
??? in Figure 1. Under a monolingual induction
method (e.g., the infinite tree model), the ????
in Example 1 and 2 would both be assigned the
same POS tag since they share the same observa-
tion. However, our models would assign separate
tags for the two different instances since the ??
?? in Example 1 and Example 2 could be disam-
biguated by encoding the target-side information,
either ?use? or ?usage?, in the observations.
Inference is efficiently carried out by beam sam-
pling (Gael et al, 2008), which combines slice
sampling and dynamic programming. Experi-
ments are carried out on the NTCIR-9 Japanese-
to-English task using a binarized forest-to-string
SMT system with dependency trees as its source
side. Our bilingually-induced tagset signifi-
cantly outperforms the original tagset and the
monolingually-induced tagset. Further, our inde-
pendent model achieves a more than 1 point gain
in BLEU, which resolves the sparseness problem
introduced by the bi-word observations.
2 Related Work
A number of unsupervised methods have been
proposed for inducing POS tags. Early methods
have the problem that the number of possible POS
tags must be provided preliminarily. This limita-
tion has been overcome by automatically adjust-
ing the number of possible POS tags using non-
parametric Bayesian methods (Finkel et al, 2007;
Gael et al, 2009; Blunsom and Cohn, 2011; Sirts
and Aluma?e, 2012). Gael et al (2009) applied
infinite HMM (iHMM) (Beal et al, 2001; Teh
et al, 2006), a nonparametric version of HMM,
to POS induction. Blunsom and Cohn (2011)
used a hierarchical Pitman-Yor process prior to the
transition and emission distribution for sophisti-
cated smoothing. Sirts and Aluma?e (2012) built a
model that combines POS induction and morpho-
logical segmentation into a single learning prob-
lem. Finkel et al (2007) proposed the infinite
tree model, which represents recursive branching
structures over infinite hidden states and induces
POS tags from syntactic dependency structures. In
the following, we overview the infinite tree model,
which is the basis of our proposed model. In par-
ticular, we will describe the independent children
H ?k
?k? z1
z2 z3
x1 x2 x3k=1,?,C
Hk
k
~
),...,(Dirichlet~|? ???pi
Figure 2: A Graphical Representation of the Finite
Tree Model
model (Finkel et al, 2007), where children are
dependent only on their parents, used in our pro-
posed model1.
2.1 Finite Tree Model
We first review the finite tree model, which can
be graphically represented in Figure 2. Let
Tt denote the tree whose root node is t. A
node t has a hidden state zt (the POS tag)
and an observation xt (the word). The prob-
ability of a tree Tt, pT (Tt), is recursively de-
fined: pT (Tt) = p(xt|zt)
?
t??c(t)
p(zt? |zt)pT (Tt?),
where c(t) is the set of the children of t.
Let each hidden state variable have C possible
values indexed by k. For each state k, there is
a parameter ?k which parameterizes the observa-
tion distribution for that state: xt|zt ? F (?zt). ?k
is distributed according to a prior distribution H:
?k ? H .
Transitions between states are governed by
Markov dynamics parameterized by pi, where
?ij = p(zc(t) = j|zt = i) and pik are the transition
probabilities from the parent?s state k. pik is dis-
tributed according to a Dirichlet distribution with
parameter ?: pik|? ? Dirichlet(?, . . . , ?). The
hidden state of each child zt? is distributed accord-
ing to a multinomial distributionpizt specific to the
parent?s state zt: zt? |zt ? Multinomial(pizt).
2.2 Infinite Tree Model
In the infinite tree model, the number of possible
hidden states is potentially infinite. The infinite
model is formed by extending the finite tree model
using a hierarchical Dirichlet process (HDP) (Teh
et al, 2006). The reason for using an HDP rather
1Finkel et al (2007) originally proposed three types of
models: besides the independent children model, the simul-
taneous children model and the markov children model. Al-
though we could apply the other two models, we leave this
for future work.
842
H ?k
?k?0 z1
z2 z3
x1 x2 x3?
? ?
Hk
k
~
),(DP~,|
)(GEM~|
00? ????pi
???
Figure 3: A Graphical Representation of the Infi-
nite Tree Model
than a simple Dirichlet process (DP)2 (Ferguson,
1973) is that we have to introduce coupling across
transitions from different parent?s states. A similar
measure was adopted in iHMM (Beal et al, 2001).
HDP is a set of DPs coupled through a shared
random base measure which is itself drawn from
a DP: each Gk ? DP(?0, G0) with a shared base
measure G0, and G0 ? DP(?,H) with a global
base measure H . From the viewpoint of the stick-
breaking construction3 (Sethuraman, 1994), the
HDP is interpreted as follows: G0 =
??
k?=1
?k???k?
and Gk =
??
k?=1
?kk???k? , where ? ? GEM(?),
pik ? DP(?0,?), and ?k? ? H .
We regard each Gk as two coindexed distribu-
tions: pik, a distribution over the transition prob-
abilities from the parent?s state k, and ?k? , an ob-
servation distribution for the state k?. Then, the
infinite tree model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt).
Figure 3 shows the graphical representation of the
infinite tree model. The primary difference be-
2DP is a measure on measures. It has two parameters, a
scaling parameter ? and a base measure H: DP (?,H).
3Sethuraman (1994) showed a definition of a measure
G ? DP(?0, G0). First, infinite sequences of i.i.d variables
(??k)?k=1 and (?k)?k=1 are generated: ??k|?0 ? Beta(1, ?0),
?k ? G0. Then, G is defined as: ?k = ??k
?k?1
l=1 (1 ? ??l),
G = ??k=1 ?k??k . If pi is defined by this process, then we
write pi ? GEM(?0).
H ?k
?k?0
?
? ? z1
z2 z3
z4 z5 z6
???
+pay? ??????+fees? ???+usage???+I? ???
Figure 4: An Example of the Joint Model
tween Figure 2 and Figure 3 is whether the number
of copies of the state is finite or not.
3 Bilingual Infinite Tree Model
We propose a bilingual variant of the infinite tree
model, the bilingual infinite tree model, which uti-
lizes information from the other language. Specifi-
cally, the proposed model introduces bilingual ob-
servations by embedding the aligned target words
in the source-side dependency trees. This paper
proposes two types of models that differ in their
processes for generating observations: the joint
model and the independent model.
3.1 Joint Model
The joint model is a simple application of the in-
finite tree model under a bilingual scenario. The
model is formally defined in the same way as in
Section 2.2 and is graphically represented simi-
larly to Figure 3. The only difference from the
infinite tree model is the instances of observations
(xt). Observations in the joint model are the com-
bination of source words and their aligned target
words4, while observations in the monolingual in-
finite tree model represent only source words. For
each source word, all the aligned target words are
copied and sorted in alphabetical order, and then
concatenated into a single observation. Therefore,
a single target word may be emitted multiple times
if the target word is aligned with multiple source
words. Likewise, there may be target words which
may not be emitted by our model, if the target
words are not aligned.
Figure 4 shows the process of generating Exam-
ple 2 in Figure 1 through the joint model, where
aligned words are jointly emitted as observations.
In Figure 4, the POS tag of ???? (z5) generates
4When no target words are aligned, we simply add a
NULL target word.
843
H ?k
?k?0
? ? z1
z2 z3
z4 z5
H? ?'k
?? pay
I?
???
??
? NONE NONEusage
fees z6
?
'~',~
),(DP~,|
)(GEM~|
00 HH kk
k ?? ????pi
???
Figure 5: A Graphical Representation of the Inde-
pendent Model
the string ???+usage? as the observation (x5).
Similarly, the POS tag of ???? in Example 1
would generate the string ???+use?. Hence, this
model can assign different POS tags to the two dif-
ferent instances of the word ????, based on the
different observation distributions in inference.
3.2 Independent Model
The joint model is prone to a data sparseness prob-
lem, since each observation is a combination of a
source word and its aligned target word. Thus, we
propose an independent model, where each hidden
state generates a source word and its aligned target
word separately. For the aligned target side, we in-
troduce an observation variable x?t for each zt and
a parameter ??k for each state k, which parame-
terizes a distinct distribution over the observations
x?t for that state. ??k is distributed according to a
prior distribution H ?. Specifically, the indepen-
dent model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H, ??k ? H ?,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt), x?t|zt ? F ?(??zt).
When multiple target words are aligned to a single
source word, each aligned word is generated sepa-
rately from observation distribution parameterized
by ??k.
Figure 5 graphs the process of generating Ex-
ample 2 in Figure 1 using the independent model.
x?t and ??k are introduced for aligned target words.
The state of ???? (z5) generates the Japanese
word ???? as x5 and the English word ?usage?
as x?5. Due to this factorization, the independent
model is less subject to the sparseness problem.
3.3 Introduction of Other Factors
We assumed the surface form of aligned target
words as additional observations in previous sec-
tions. Here, we introduce additional factors, i.e.,
the POS of aligned target words, in the observa-
tions. Note that POSs of target words are assigned
by a POS tagger in the target language and are not
inferred in the proposed model.
First, we can simply replace surface forms of
target words with their POSs to overcome the
sparseness problem. Second, we can incorporate
both information from the target language as ob-
servations. In the joint model, two pieces of in-
formation are concatenated into a single observa-
tion. In the independent model, we introduce ob-
servation variables (e.g., x?t and x??t ) and parame-
ters (e.g., ??k and ???k) for each information. Specif-
ically, x?t and ??k are introduced for the surface
form of aligned words, and x??t and ???k for the POS
of aligned words. Consider, for example, Example
1 in Figure 1. The POS tag of ???? generates the
string ???+use+verb? as the observation in the
joint model, while it generates ????, ?use?, and
?verb? independently in the independent model.
3.4 POS Refinement
We have assumed a completely unsupervised way
of inducing POS tags in dependency trees. An-
other realistic scenario is to refine the existing POS
tags (Finkel et al, 2007; Liang et al, 2007) so
that each refined sub-POS tag may reflect the in-
formation from the aligned words while preserv-
ing the handcrafted distinction from original POS
tagset. Major difference is that we introduce sep-
arate transition probabilities pisk and observation
distributions (?sk, ?
?s
k ) for each existing POS tag s.
Then, each node t is constrained to follow the dis-
tributions indicated by the initially assigned POS
tag st, and we use the pair (st, zt) as a state repre-
sentation.
3.5 Inference
In inference, we find the state set that maximizes
the posterior probability of state transitions given
observations (i.e., P (z1:n|x1:n)). However, we
cannot evaluate the probability for all possible
states because the number of states is infinite.
Finkel et al (2007) presented a sampling algo-
rithm for the infinite tree model, which is based on
the Gibbs sampling in the direct assignment rep-
resentation for iHMM (Teh et al, 2006). In the
844
Gibbs sampling, individual hidden state variables
are resampled conditioned on all other variables.
Unfortunately, its convergence is slow in HMM
settings because sequential data is likely to have
a strong correlation between hidden states (Gael
et al, 2008).
We present an inference procedure based on
beam sampling (Gael et al, 2008) for the joint
model and the independent model. Beam sam-
pling limits the number of possible state transi-
tions for each node to a finite number using slice
sampling (Neal, 2003), and then efficiently sam-
ples whole hidden state transitions using dynamic
programming. Beam sampling does not suffer
from slow convergence as in Gibbs sampling by
sampling the whole state variables at once. In ad-
dition, Gael et al (2008) showed that beam sam-
pling is more robust to initialization and hyperpa-
rameter choice than Gibbs sampling.
Specifically, we introduce an auxiliary variable
ut for each node in a dependency tree to limit
the number of possible transitions. Our procedure
alternates between sampling each of the follow-
ing variables: the auxiliary variables u, the state
assignments z, the transition probabilities pi, the
shared DP parameters ?, and the hyperparameters
?0 and ?. We can parallelize procedures in sam-
pling u and z because the slice sampling for u and
the dynamic programing for z are independent for
each sentence. See Gael el al. (2009) for details.
The only difference between inferences in the
joint model and the independent model is in com-
puting the posterior probability of state transi-
tions given observations (e.g., p(z1:n|x1:n) and
p(z1:n|x1:n, x?1:n)) in sampling z. In the follow-
ing, we describe each sampling stage. See Teh et
al., (2006) for details of sampling pi, ?, ?0 and ?.
Sampling u:
Each ut is sampled from the uniform distribu-
tion on [0, ?zd(t)zt ], where d(t) is the parent of
t: ut ? Uniform(0, ?zd(t)zt). Note that ut is a
positive number, since each transition probability
?zd(t)zt is larger than zero.
Sampling z:
Possible values k of zt are divided into the two
sets using ut: a finite set with ?zd(t)k > ut and
an infinite set with ?zd(t)k ? ut. The beam
sampling considers only the former set. Owing
to the truncation of the latter set, we can compute
the posterior probability of a state zt given ob-
servations for all t (t = 1, . . . , T ) using dynamic
programming as follows:
In the joint model, p(zt|x?(t), u?(t)) ?
p(xt|zt) ?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), u?(d(t))),
and in the independent model,
p(zt|x?(t), x??(t), u?(t)) ? p(xt|zt) ? p(x?t|zt)
?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), x??(d(t)), u?(d(t))),
where x?(t) (or u?(t)) denotes the set of xt (or ut)
on the path from the root node to the node t in a
tree.
In our experiments, we assume that F (?k)
is Multinomial(?k) and H is Dirichlet(?, . . . , ?),
which is the same in Finkel et al (2007). Un-
der this assumption, the posterior probability of an
observation is as follows: p(xt|zt) =
n?xtk + ?
n??k + N?
,
where n?xk is the number of observations x with
state k, n??k is the number of hidden states whose
values are k, and N is the total number of observa-
tions x. Similarly, p(x?t|zt) =
n?x?tk + ?
?
n??k + N ???
, where
N ? is the total number of observations x?.
When the posterior probability of a state zt
given observations for all t can be computed,
we first sample the state of each leaf node and
then perform backtrack sampling for every other
zt where the zt is sampled given the sample
for zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ?
p(zt|x?(t), u?(t))
?
t??c(t) p(zt? |zt, ut?).
Sampling pi:
We introduce a count variable nij ? n,
which is the number of observations with
state j whose parent?s state is i. Then,
we sample pi using the Dirichlet distri-
bution: (?k1, . . . , ?kK ,
??
k?=K+1 ?kk?) ?
Dirichlet(nk1 + ?0?1, . . . , nkK +
?0?K , ?0
??
k?=K+1 ?k?), where K is the
number of distinct states in z.
Sampling ?:
We introduce a set of auxiliary variables m, where
mij ? m is the number of elements of pij
corresponding to ?i. The conditional distribu-
tion of each variable is p(mij = m|z,?, ?0) ?
S(nij ,m)(?0?j)m, where S(n,m) are unsigned
Stirling numbers of the first kind5.
5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0,
S(n,m) = 0 for m > n, and S(n + 1,m) = S(n,m ?
1) + nS(n,m) for others.
845
The parameters ? are sampled using the Dirich-
let distribution: (?1, . . . , ?K ,
??
k?=K+1 ?k?) ?
Dirichlet(m?1, . . . ,m?K , ?), where m?k =?K
k?=1 mk?k.
Sampling ?0:
?0 is parameterized by a gamma hyperprior
with hyperparameters ?a and ?b. We introduce
two types of auxiliary variables for each state
(k = 1, . . . ,K), wk ? [0, 1] and vk ? {0, 1}.
The conditional distribution of each wk is
p(wk|?0) ? w?0k (1?wk)n?k?1 and that of each vk
is p(vk|?0) ? (
n?k
?0
)
vk
, where n?k =
?K
k?=1 nk?k.
The conditional distribution of ?0 given wk
and vk (k = 1, . . . ,K) is p(?0|w,v) ?
??a?1+m..?
?K
k=1 vk
0 e??0(?b?
?K
k=1 logwk), where
m?? =
?K
k?=1
?K
k??=1 mk?k?? .
Sampling ?:
? is parameterized by a gamma hyperprior with
hyperparameters ?a and ?b. We introduce an
auxiliary variable ?, whose conditional distribu-
tion is p(?|?) ? ??(1 ? ?)m???1. The con-
ditional distribution of ? given ? is p(?|?) ?
??a?1+Ke??(?b?log?).
4 Experiment
We tested our proposed models under the
NTCIR-9 Japanese-to-English patent translation
task (Goto et al, 2011), consisting of approxi-
mately 3.2 million bilingual sentences. Both the
development data and the test data consist of 2,000
sentences. We also used the NTCIR-7 develop-
ment data consisting of 2,741 sentences for devel-
opment testing purposes.
4.1 Experimental Setup
We evaluated our bilingual infinite tree model
for POS induction using an in-house developed
syntax-based forest-to-string SMT system. In
the training process, the following steps are per-
formed sequentially: preprocessing, inducing a
POS tagset for a source language, training a POS
tagger and a dependency parser, and training a
forest-to-string MT model.
Step 1. Preprocessing
We used the first 10,000 Japanese-English sen-
tence pairs in the NTCIR-9 training data for in-
ducing a POS tagset for Japanese6. The Japanese
sentences were segmented using MeCab7, and the
English sentences were tokenized and POS tagged
using TreeTagger (Schmid, 1994), where 43 and
58 types of POS tags are included in the Japanese
sentences and the English sentences, respectively.
The Japanese POS tags come from the second-
level POS tags in the IPA POS tagset (Asahara and
Matsumoto, 2003) and the English POS tags are
derived from the Penn Treebank. Note that the
Japanese POS tags are used for initialization of
hidden states and the English POS tags are used
as observations emitted by hidden states.
Word-by-word alignments for the sentence
pairs are produced by first running GIZA++ (Och
and Ney, 2003) in both directions and then com-
bining the alignments using the ?grow-diag-final-
and? heuristic (Koehn et al, 2003). Note that we
ran GIZA++ on all of the NTCIR-9 training data
in order to obtain better alignements.
The Japanese sentences are parsed using
CaboCha (Kudo and Matsumoto, 2002), which
generates dependency structures using a phrasal
unit called a bunsetsu8, rather than a word unit as
in English or Chinese dependency parsing. Since
we focus on the word-level POS induction, each
bunsetsu-based dependency tree is converted into
its corresponding word-based dependency tree us-
ing the following heuristic9: first, the last func-
tion word inside each bunsetsu is identified as
the head word10; then, the remaining words are
treated as dependents of the head word in the same
bunsetsu; finally, a bunsetsu-based dependency
structure is transformed to a word-based depen-
dency structure by preserving the head/modifier
relationships of the determined head words.
Step 2. POS Induction
A POS tag for each word in the Japanese sentences
is inferred by our bilingual infinite tree model, ei-
6Due to the high computational cost, we did not use all
the NTCIR-9 training data. We leave scaling up to a larger
dataset for future work.
7http://mecab.googlecode.com/svn/
trunk/mecab/doc/index.html
8A bunsetsu is the smallest meaningful sequence con-
sisting of a content word and accompanying function words
(e.g., a noun and a particle).
9We could use other word-based dependency trees such
as trees by the infinite PCFG model (Liang et al, 2007)
and syntactic-head or semantic-head dependency trees in
Nakazawa and Kurohashi (2012), although it is not our major
focus. We leave this for future work.
10If no function words exist in a bunsetsu, the last content
word is treated as the head word.
846
ther jointly (Joint) or independently (Ind). We
also performed monolingual induction of Finkel et
al. (2007) for comparison (Mono). In each model,
a sequence of sampling u, z, pi, ?, ?0, and ? is
repeated 10,000 times. In sampling ?0 and ?, hy-
perparameters ?a, ?b, ?a, and ?b are set to 2, 1,
1, and 1, respectively, which is the same setting in
Gael et al (2008). In sampling z, parameters ?, ??,
. . ., are set to 0.01. In the experiments, three types
of factors for the aligned English words are com-
pared: surface forms (?s?), POS tags (?P?), and the
combination of both (?s+P?). Further, two types of
inference frameworks are compared: induction
(IND) and refinement (REF ). In both frame-
works, each hidden state zt is first initialized to
the POS tags assigned by MeCab (the IPA POS
tagset), and then each state is updated through
the inference procedure described in Section 3.5.
Note that in REF , the sampling distribution over
zt is constrained to include only states that are a
refinement of the initially assigned POS tag.
Step 3. Training a POS Tagger and a
Dependency Parser
In this step, we train a Japanese dependency parser
from the 10,000 Japanese dependency trees with
the induced POS tags which are derived from Step
2. We employed a transition-based dependency
parser which can jointly learn POS tagging and
dependency parsing (Hatori et al, 2011) under an
incremental framework11. Note that the learned
parser can identify dependencies between words
and attach an induced POS tag for each word.
Step 4. Training a Forest-to-String MT
In this step, we train a forest-to-string MT model
based on the learned dependency parser in Step 3.
We use an in-house developed hypergraph-based
toolkit, cicada, for training and decoding with a
tree-to-string model, which has been successfully
employed in our previous work for system com-
bination (Watanabe and Sumita, 2011) and online
learning (Watanabe, 2012). All the Japanese and
English sentences in the NTCIR-9 training data
are segmented in the same way as in Step 1, and
then each Japanese sentence is parsed by the de-
pendency parser learned in Step 3, which simul-
taneously assigns induced POS tags and word de-
pendencies. Finally, a forest-to-string MT model
is learned with Zhang et al, (2011), which ex-
tracts translation rules by a forest-based variant of
11http://triplet.cc/software/corbit/
IND REF
BS 27.54
Mono 27.66 26.83
Joint[s] 28.00 28.00
Joint[P] 26.36 26.72
Joint[s+P] 27.99 27.82
Ind[s] 28.00 27.93
Ind[P] 28.11 28.63
Ind[s+P] 28.13 28.62
Table 1: Performance on Japanese-to-English
Translation Measured by BLEU (%)
the GHKM algorithm (Mi and Huang, 2008) af-
ter each parse tree is restructured into a binarized
packed forest. Parameters are tuned on the devel-
opment data using xBLEU (Rosti et al, 2011) as
an objective and L-BFGS (Liu and Nocedal, 1989)
as an optimization toolkit, since it is stable and less
prone to randomness, unlike MERT (Och, 2003)
or PRO (Hopkins and May, 2011). The develop-
ment test data is used to set up hyperparameters,
i.e., to terminate tuning iterations.
When translating Japanese sentences, a parse
tree for each sentence is constructed in the same
way as described earlier in this step, and then the
parse trees are translated into English sentences
using the learned forest-to-string MT model.
4.2 Experimental Results
Table 1 shows the performance for the test data
measured by case sensitive BLEU (Papineni et
al., 2002). We also present the performance of
our baseline forest-to-string MT system (BS) us-
ing the original IPA POS tags. In Table 1, num-
bers in bold indicate that the systems outperform
the baselines, BS and Mono. Under the Moses
phrase-based SMT system (Koehn et al, 2007)
with the default settings, we achieved a 26.80%
BLEU score.
Table 1 shows that the proposed systems outper-
form the baseline Mono. The differences between
the performance of Ind[s+P] and Mono are statis-
tically significant in the bootstrap method (Koehn,
2004), with a 1% significance level both in IND
and REF . The results indicate that integrating the
aligned target-side information in POS induction
makes inferred tagsets more suitable for SMT.
Table 1 also shows that the independent model
is more effective for SMT than the joint model.
This means that sparseness is a severe problem in
847
Model IND REF
Joint[s+P] 164 620
Ind[s+P] 102 517
IPA POS tags 42
Table 2: The Number of POS Tags
POS induction when jointly encoding bilingual in-
formation into observations. Additionally, all the
systems using the independent model outperform
BS. The improvements are statistically significant
in the bootstrap method (Koehn, 2004), with a 1%
significance level. The results show that the pro-
posed models can generate more favorable POS
tagsets for SMT than an existing POS tagset.
In Table 1, REF s are at least comparable to, or
better than, INDs except for Mono. This shows
that REF achieves better performance by preserv-
ing the clues from the original POS tagset. How-
ever, REF may suffer sever overfitting problem
for Mono since no bilingual information was in-
corporated. Further, when the full-level IPA POS
tags12 were used in BS, the system achieved a
27.49% BLEU score, which is worse than the re-
sult using the second-level IPA POS tags. This
means that manual refinement without bilingual
information may also cause an overfitting problem
in MT.
5 Discussion
5.1 Comparison to the IPA POS Tagset
Table 2 shows the number of the IPA POS tags
used in the experiments and the POS tags induced
by the proposed models. This table shows that
each induced tagset contains more POS tags than
the IPA POS tagset. In the experimental data,
some of Japanese verbs correspond to genuine En-
glish verbs, some are nominalized, and others cor-
respond to English past participle verbs or present
participle verbs which modify other words. Re-
spective examples are ?I use a card.?, ?Using the
index is faster.?, and ?I explain using an exam-
ple.?, where all the underlined words correspond
to the same Japanese word, ????, whose IPA
POS tag is a verb. Ind[s+P] in REF generated
the POS tagset where the three types are assigned
to separate POS groups.
The Japanese particle ??? is sometimes at-
tached to nouns to give them adverb roles. For
12377 types of full-level IPA POS tags were included in our
experimental data.
Tagging Dependency
IND REF IND REF
Original 90.37 93.62
Mono 90.75 88.04 91.77 91.51
Joint[s] 89.08 86.73 91.55 91.14
Joint[P] 80.54 79.98 91.06 91.29
Joint[s+P] 87.56 84.92 91.31 91.10
Ind[s] 87.62 84.33 92.06 92.58
Ind[P] 90.21 88.50 92.85 93.03
Ind[s+P] 89.57 86.12 92.96 92.78
Table 3: Tagging and Dependency Accuracy (%)
example, ??? (mutual) ??? is translated as
the adverb ?mutually? in English. Other times,
it is attached to words to make them the objects
of verbs. For example, ?? (he) ??????
(give)? is translated as ?give him?. The POS tags
by Ind[s+P] in REF discriminated the two types.
These examples show that the proposed mod-
els can disambiguate POS tags that have different
functions in English, whereas the IPA POS tagset
treats them jointly. Thus, such discrimination im-
proves the performance of a forest-to-string SMT.
5.2 Impact of Tagging and Dependency
Accuracy
The performance of our methods depends not only
on the quality of the induced tag sets but also on
the performance of the dependency parser learned
in Step 3 of Section 4.1. We cannot directly eval-
uate the tagging accuracy of the parser trained
through Step 3 because we do not have any data
with induced POS tags other than the 10,000-
sentence data gained through Step 2. Thus we split
the 10,000 data into the first 9,000 data for train-
ing and the remaining 1,000 for testing, and then
a dependency parser was learned in the same way
as in Step 3.
Table 3 shows the results. Original is the per-
formance of the parser learned from the training
data with the original POS tagset. Note that the de-
pendency accuracies are measured on the automat-
ically parsed dependency trees, not on the syntac-
tically correct gold standard trees. Thus Original
achieved the best dependency accuracy.
In Table 3, the performance for our bilingually-
induced POSs, Joint and Ind, are lower than
Original and Mono. It seems performing pars-
ing and tagging with the bilingually-induced POS
tagset is too difficult when only monolingual in-
848
formation is available to the parser. However, our
bilingually-induced POSs, except for Joint[P ],
with the lower accuracies are more effective for
SMT than the monolingually-induced POSs and
the original POSs, as indicated in Table 1. The
tagging accuracies for Joint[P ] both in IND and
REF are significantly lower than the others, while
the dependency accuracies do not differ signifi-
cantly. The lower tagging accuracies may directly
reflect the lower translation qualities for Joint[P ]
in Table 1.
6 Conclusion
We proposed a novel method for inducing POS
tags for SMT. The proposed method is a non-
parametric Bayesian method, which infers hidden
states (i.e., POS tags) based on observations repre-
senting not only source words themselves but also
aligned target words. Our experiments showed
that a more favorable POS tagset can be induced
by integrating aligned information, and further-
more, the POS tagset generated by the proposed
method is more effective for SMT than an existing
POS tagset (the IPA POS tagset).
Even though we employed word alignment
from GIZA++ with potential errors, large gains
were achieved using our proposed method. We
would like to investigate the influence of align-
ment errors in the future. In addition, we are plan-
ning to prove the effectiveness of our proposed
method for language pairs other than Japanese-to-
English. We are also planning to introduce our
proposed method to other syntax-based SMT, such
as a string-to-tree SMT and a tree-to-tree SMT.
Acknowledgments
We thank Isao Goto for helpful discussions and
anonymous reviewers for valuable comments. We
also thank Jun Hatori for helping us to apply his
software, Corbit, to our induced POS tagsets.
References
Masayuki Asahara and Yuji Matsumoto. 2003.
IPADIC User Manual. Technical report, Japan.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2001. The Infinite Hidden Markov Model.
In Advances in Neural Information Processing Sys-
tems, pages 577?584.
Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of
Speech Induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 865?874.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
Model of Syntax-Directed Tree to String Grammar
Induction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 352?361.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 541?548.
Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The Infinite Tree. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 272?279.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam Sampling for
the Infinite Hidden Markov Model. In Proceedings
of the 25th International Conference on Machine
Learning, pages 1088?1095.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsuper-
vised PoS tagging. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, pages 678?687.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961?968.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559?578.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental Joint POS Tag-
ging and Dependency Parsing in Chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-Directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop on
849
Computationally Hard Problemsand Joint Inference
in Speech and Language Processing, pages 1?8.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning, pages 63?69.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The Infinite PCFG using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 688?697.
Dekang Lin. 2004. A Path-based Transfer Model for
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 625?630.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improv-
ing Tree-to-Tree Translation with Packed Forests.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, pages 558?566.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 206?214.
Haitao Mi and Qun Liu. 2010. Constituency to De-
pendency Translation with Forests. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, pages 1433?1442.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by Bilingual Generation and Monolin-
gual Derivation. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1963?1978.
Radford M. Neal. 2003. Slice Sampling. Annals of
Statistics, 31:705?767.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Conference of the Association for Computa-
tional Linguistics, pages 271?279.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 159?165.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49.
Jayaram Sethuraman. 1994. A Constructive Definition
of Dirichlet Priors. Statistica Sinica, 4(2):639?650.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 577?
585.
Kairit Sirts and Tanel Aluma?e. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 407?416.
850
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
Processes. Journal of the American Statistical Asso-
ciation, 101(476):1566?1581.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
Translation System Combination by Confusion For-
est. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1249?1257.
Taro Watanabe. 2012. Optimized Online Rank Learn-
ing for Machine Translation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 253?262.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 559?
567.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized Forest to String Translation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 19?24.
851
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023?1032,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Subtree Extractive Summarization via Submodular Maximization
Hajime Morita
Tokyo Institute of Technology, Japan
morita@lr.pi.titech.ac.jp
Hiroya Takamura
Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp
Ryohei Sasano
Tokyo Institute of Technology, Japan
sasano@pi.titech.ac.jp
Manabu Okumura
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract
This study proposes a text summarization
model that simultaneously performs sen-
tence extraction and compression. We
translate the text summarization task into
a problem of extracting a set of depen-
dency subtrees in the document cluster.
We also encode obligatory case constraints
as must-link dependency constraints in or-
der to guarantee the readability of the gen-
erated summary. In order to handle the
subtree extraction problem, we investigate
a new class of submodular maximization
problem, and a new algorithm that has
the approximation ratio 12(1 ? e?1). Ourexperiments with the NTCIR ACLIA test
collections show that our approach outper-
forms a state-of-the-art algorithm.
1 Introduction
Text summarization is often addressed as a task
of simultaneously performing sentence extraction
and sentence compression (Berg-Kirkpatrick et
al., 2011; Martins and Smith, 2009). Joint mod-
els of sentence extraction and compression have
a great benefit in that they have a large degree of
freedom as far as controlling redundancy goes. In
contrast, conventional two-stage approaches (Za-
jic et al, 2006), which first generate candidate
compressed sentences and then use them to gen-
erate a summary, have less computational com-
plexity than joint models. However, two-stage ap-
proaches are suboptimal for text summarization.
For example, when we compress sentences first,
the compressed sentences may fail to contain im-
portant pieces of information due to the length
limit imposed on each sentence. On the other
hand, when we extract sentences first, an impor-
tant sentence may fail to be selected, simply be-
cause it is long. Enumerating a huge number
of compressed sentences is also infeasible. Joint
models can prune unimportant or redundant de-
scriptions without resorting to enumeration.
Meanwhile, submodular maximization has re-
cently been applied to the text summarization task,
and the methods thereof have performed very well
(Lin and Bilmes, 2010; Lin and Bilmes, 2011;
Morita et al, 2011). Formalizing summarization
as a submodular maximization problem has an im-
portant benefit inthat the problem can be solved by
using a greedy algorithm with a performance guar-
antee.
We therefore decided to formalize the task of si-
multaneously performing sentence extraction and
compression as a submodular maximization prob-
lem. That is, we extract subsentences for mak-
ing the summary directly from all available sub-
sentences in the documents and not in a stepwise
fashion. However, there is a difficulty with such
a formalization. In the past, the resulting maxi-
mization problem has been often accompanied by
thousands of linear constraints representing logi-
cal relations between words. The existing greedy
algorithm for solving submodular maximization
problems cannot work in the presence of such nu-
merous constraints although monotone and non-
monotone submodular maximization with con-
straints other than budget constraints have been
studied (Lee et al, 2009; Kulik et al, 2009; Gupta
et al, 2010). In this study, we avoid this difficulty
by reducing the task to one of extracting depen-
dency subtrees from sentences in the source doc-
uments. The reduction replaces the difficulty of
numerous linear constraints with another difficulty
wherein two subtrees can share the same word to-
1023
ken when they are selected from the same sen-
tence, and as a result, the cost of the union of the
two subtrees is not always the mere sum of their
costs. We can overcome this difficulty by tackling
a new class of submodular maximization prob-
lem: a budgeted monotone nondecreasing sub-
modular function maximization with a cost func-
tion, where the cost of an extraction unit varies
depending on what other extraction units are se-
lected. By formalizing the subtree extraction prob-
lem as this new maximization problem, we can
treat the constraints regarding the grammaticality
of the compressed sentences in a straightforward
way and use an arbitrary monotone submodular
word score function for words including our word
score function (shown later). We also propose a
new greedy algorithm that solves this new class of
maximization problem with a performance guar-
antee 12(1? e?1).
We evaluated our method on by using it to per-
form query-oriented summarization (Tang et al,
2009). Experimental results show that it is supe-
rior to state-of-the-art methods.
2 Related Work
Submodularity is formally defined as a property of
a set function for a finite universe V . The function
f : 2V ? R maps a subset S ? V to a real value.
If for any S, T ? V , f(S ? T ) + f(S ? T ) ?
f(S)+f(T ), f is called submodular. This defini-
tion is equivalent to that of diminishing returns,
which is well known in the field of economics:
f(S ?{u})? f(S) ? f(T ?{u})? f(T ), where
T ? S ? V and u is an element of V . Di-
minishing returns means that the value of an el-
ement u remains the same or decreases as S be-
comes larger. This property is suitable for sum-
marization purposes, because the gain of adding a
new sentence to a summary that already contains
sufficient information should be small. Therefore,
many studies have formalized text summarization
as a submodular maximization problem (Lin and
Bilmes, 2010; Lin and Bilmes, 2011; Morita et
al., 2011). Their approaches, however, have been
based on sentence extraction. To our knowledge,
there is no study that addresses the joint task of
simultaneously performing compression and ex-
traction through an approximate submodular max-
imization with a performance guarantee.
In the field of constrained maximization prob-
lems, Kulik et al (2009) proposed an algorithm
that solves the submodular maximization problem
under multiple linear constraints with a perfor-
mance guarantee 1? e?1 in polynomial time. Al-
though their approach can represent more flexible
constraints, we cannot use their algorithm to solve
our problem, because their algorithm needs to enu-
merate many combinations of elements. Integer
linear programming (ILP) formulations can repre-
sent such flexible constraints, and they are com-
monly used to model text summarization (McDon-
ald, 2007). Berg-Kirkpatrick et al (2011) formu-
lated a unified task of sentence extraction and sen-
tence compression as an ILP. However, it is hard to
solve large-scale ILP problems exactly in a practi-
cal amount of time.
3 Budgeted Submodular Maximization
with Cost Function
3.1 Problem Definition
Let V be the finite set of all valid subtrees in
the source documents, where valid subtrees are
defined to be the ones that can be regarded as
grammatical sentences. In this paper, we regard
subtrees containing the root node of the sentence
as valid. Accordingly, V denotes a set of all
rooted subtrees in all sentences. A subtree con-
tains a set of elements that are units in a de-
pendency structure (e.g., morphemes, words or
clauses). Let us consider the following problem
of budgeted monotone nondecreasing submodu-
lar function maximization with a cost function:
maxS?V {f(S) : c (S) ? L} , where S is a sum-
mary represented as a set of subtrees, c(?) is the
cost function for the set of subtrees, L is our bud-
get, and the submodular function f(?) scores the
summary quality. The cost function is not always
the sum of the costs of the covered subtrees, but
depends on the set of the covered elements by the
subtrees. Here, we will assume that the generated
summary has to be as long as or shorter than the
given summary length limit, as measured by the
number of characters. This means the cost of a
subtree is the integer number of characters it con-
tains.
V is partitioned into exclusive subsetsB of valid
subtrees, and each subset corresponds to the orig-
inal sentence from which the valid subtrees de-
rived. However, the cost of a union of subtrees
from different sentences is simply the sum of the
costs of subtrees, while the cost of a union of sub-
trees from the same sentence is smaller than the
sum of the costs. Therefore, the problem can be
represented as follows:
1024
max
S?V
{
f(S) :
?
B?B
c (B ? S) ? L
}
. (1)
For example, if we add a subtree t containing
words {wa,wb,wc} to a summary that already
covers words {wa, wb, wd} from the same sen-
tence, the additional cost of t is only c({wc}) be-
cause wa and wb are already covered1.
The problem has two requirements. The first
requirement is that the union of valid subtrees is
also a valid subtree. The second requirement is
that the union of subtrees and a single valid sub-
tree have the same score and the same cost if they
cover the same elements. We will refer to the sin-
gle valid subtree as the equivalent subtree of the
union of subtrees. These requirements enable us
to represent sentence compression as the extrac-
tion of subtrees from a sentence. This is because
the requirements guarantee that the extracted sub-
trees represent a sentence.
3.2 Greedy Algorithm
We propose Algorithm 1 that solves the maximiza-
tion problem (Eq.1). The algorithm is based on
ones proposed by Khuller et al (1999) and Krause
et al (2005). Instead of enumerating all candidate
subtrees, we use a local search to extract the ele-
ment that has the highest gain per cost. In the al-
gorithm, Gi indicates a summary set obtained by
adding element si to Gi?1. U means the set of
subtrees that are not extracted. The algorithm it-
eratively adds to the current summary the element
si that has the largest ratio of the objective func-
tion gain to the additional cost, unless adding it
violates the budget constraint. We set a parame-
ter r that is the scaling factor proposed by Lin and
Bilmes (2010). After the loop, the algorithm com-
pares Gi with the {s?} that has the largest value of
the objective function among all subtrees that are
under the budget, and it outputs the summary can-
didate with the largest value.
Let us analyze the performance guarantee of Al-
gorithm 12.
1Each subset B corresponds to a kind of greedoid con-
straint. V implicitly constrains the model such that it can
only select valid subtrees from a set of nodes and edges.
2Our performance guarantee is lower than that reported
by Lin and Bilmes (2010). However, their proof is er-
roneous. In their proof of Lemma 2, they derive ?u ?
S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
, for any i(1 ? i ? |G|),
from line 4 of their Algorithm 1, which selects the densest
element out of all available elements. However, the inequal-
ity does not hold for i, for which element u selected on line
4 is discarded on line 5 of their algorithm. The performance
guarantee of their algorithm is actually the same as ours, since
Algorithm 1 Modified greedy algorithm for budgeted
submodular function maximization with a cost function .
1: G0 ? ?
2: U ? V
3: i? 1
4: while U 6= ? do
5: si ? argmaxs?U f(Gi?1?{s})?f(Gi?1)(c(Gi?1?{s})?c(Gi?1))r
6: if c({si} ?Gi?1) ? L then
7: Gi ? Gi?1 ? {si}
8: i? i + 1
9: end if
10: U ? U\{si}
11: end while
12: s?? argmaxs?V,c(s)?L f({s})
13: return Gf = argmaxS?{{s?},Gi} f(S)
Theorem 1 For a normalized monotone submod-
ular function f(?), Algorithm 1 has a constant
approximation factor when r = 1 as follows:
f(Gf ) ?
(1
2(1? e
?1)
)
f(S?), (2)
where S? is the optimal solution and, Gf is the
solution obtained by Greedy Algorithm 1.
Proof. See appendix.
3.3 Relation with Discrete Optimization
We argue that our optimization problem can be
regarded as an extraction of subtrees rooted at a
given node from a directed graph, instead of from
a tree. Let D be the set of edges of the directed
graph, F be a subset of D that is a subtree. In the
field of combinatorial optimization, a pair (D, F)
is a kind of greedoid: directed branching greedoid
(Schmidt, 1991). A greedoid is a generalization of
the matroid concept. However, while matroids are
often used to represent constraints on submodular
maximization problems (Conforti and Cornue?jols,
1984; Calinescu et al, 2011), greedoids have not
been used for that purpose, in spite of their high
representation ability. To our knowledge, this is
the first study that gives a constant performance
guarantee for the submodular maximization under
greedoid (non-matroid) constraints.
the guarantee 12 (1? e?1) was already proved by Krause andGuestrin (2005). We show a counterexample. Suppose that
V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density
3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10. The
optimal solution is S? = {e1, e2}. Their algorithm selects
e1, e3, e4 in this order. However the algorithm selects e2 on
line 4 after selecting e3, and it drops e2 on line 5. As a result,
e4 selected by the algorithm does not satisfy the inequality
?u ? S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
.
1025
4 Joint Model of Extraction and
Compression
We will formalize the unified task of sentence
compression and extraction as a budgeted mono-
tone nondecreasing submodular function maxi-
mization with a cost function. In this formaliza-
tion, a valid subtree of a sentence represents a
candidate of a compressed sentence. We will re-
fer to all valid subtrees of a given sentence as a
valid set. A valid set corresponds to all candi-
dates of the compression of a sentence. Note that
although we use the valid set in the formaliza-
tion, we do not have to enumerate all the candi-
dates for each sentence. Since, from the require-
ments, the union of valid subtrees is also a valid
subtree in the valid set, the model can extract one
or more subtrees from one sentence, and generate
a compressed sentence by merging those subtrees
to generate an equivalent subtree. Therefore, the
joint model can extract an arbitrarily compressed
sentence as a subtree without enumerating all can-
didates. The joint model can remove the redundant
part as well as the irrelevant part of a sentence, be-
cause the model simultaneously extracts and com-
presses sentences. We can approximately solve the
subtree extraction problem by using Algorithm 1.
On line 5 of the algorithm, the subtree extraction
is performed as a local search that finds maximal
density subtrees from the whole documents. The
maximal density subtree is a subtree that has the
highest score per cost of subtree. We use a cost
function to represent the cost, which indicates the
length of word tokens in the subtree.
In this paper, we address the task of summariza-
tion of Japanese text by means of sentence com-
pression and extraction. In Japanese, syntactic
subtrees that contain the root of the dependency
tree of the original sentence often make gram-
matical sentences. This means that the require-
ments mentioned in Section 3.1 that a union of
valid subtrees is a valid and equivalent tree is of-
ten true for Japanese. The root indicates the pred-
icate of a sentence, and it is syntactically modi-
fied by other prior words. Some modifying words
can be pruned. Therefore, sentence compression
can be represented as edge pruning. The linguis-
tic units we extract are bunsetsu phrases, which
are syntactic chunks often containing a functional
word after one or more content words. We will re-
fer to bunsetsu phrases as phrases for simplicity.
Since Japanese syntactic dependency is generally
defined between two phrases, we use the phrases
as the nodes of subtrees.
In this joint model, we generate a compressed
sentence by extracting an arbitrary subtree from a
dependency tree of a sentence. However, not all
subtrees are always valid. The sentence generated
by a subtree can be unnatural even though the sub-
tree contains the root node of the sentence. To
avoid generating such ungrammatical sentences,
we need to detect and retain the obligatory de-
pendency relations in the dependency tree. We
address this problem by imposing must-link con-
straints if a phrase corresponds to an obligatory
case of the main predicate. We merge obligatory
phrases with the predicate beforehand so that the
merged nodes make a single large node.
Although we focus on Japanese in this pa-
per, our approach can be applied to English and
other languages if certain conditions are satisfied.
First, we need a dependency parser of the lan-
guage in order to represent sentence compression
as dependency tree pruning. Moreover, although,
in Japanese, obligatory cases distinguish which
edges of the dependency tree can be pruned or not,
we need another technique to distinguish them in
other languages. For example we can distinguish
obligatory phrases from optional ones by using se-
mantic role labeling to detect arguments of predi-
cates. The adaptation to other languages is left for
future work.
4.1 Objective Function
We extract subtrees from sentences in order to
solve the query-oriented summarization problem
as a unified one consisting of sentence compres-
sion and extraction. We thus need to allocate a
query relevance score to each node. Off-the-shelf
similarity measures such as the cosine similarity of
bag-of-words vectors with query terms would al-
locate scores to the terms that appear in the query,
but would give no scores to terms that do not ap-
pear in it. With such a similarity, sentence com-
pression extracts nearly only the query terms and
fails to contain important information. Instead,
we used Query SnowBall (QSB) (Morita et al,
2011) to calculate the query relevance score of
each phrase. QSB is a method for query-oriented
summarization, which calculates the similarity be-
tween query terms and each word by using co-
occurrences within the source documents. Al-
though the authors of QSB also provided scores
of word pairs to avoid putting excessive penalties
1026
on word overlaps, we do not score word pairs. The
score function is supermodular as a score function
of subtree extraction3, because the union of two
subtrees can have extra word pairs that are not in-
cluded in either subtree. If the extra pair has a pos-
itive score, the score of the union is greater than
the sum of the score of the subtrees. This violates
the definition of submodularity, and invalidates the
performance guarantee of our algorithms.
We designed our objective function by combin-
ing this relevance score with a penalty for redun-
dancy and too-compressed sentences. Important
words that describe the main topic should occur
multiple times in a good summary. However, ex-
cessive overlap undermines the quality of a sum-
mary, as do irrelevant words. Therefore, the scores
of overlapping words should be lower than thoseof
new words. The behavior can be represented by a
submodular objective function that reduces word
scores depending on those already included in the
summary. Furthermore, a summary consisting of
many too-compressed sentences would lack read-
ability. We thus gives a positive reward to long
sentences. The positive reward leads to a natu-
ral summary being generated with fewer sentences
and indirectly penalizes too short sentences. Our
positive reward for long sentences is represented
as
reward(S) = c(S)? |S|, (3)
where c(S) is the cost of summary S, and |S| is the
number of sentences in S. Since a sentence must
contain more than one character, the reward con-
sistently gives a positive score, and gives a higher
score to a summary that consists of fewer sen-
tences.
Let d be the damping rate, countS(w) be the
number of sentences containing word w in sum-
mary S, words(S) be the set of words included in
summary S, qsb(w) be the query relevance score
of word w, and ? be a parameter that adjusts the
rate of sentence compression. Our score function
for a summary S is as follows:
f(S) =
?
w?words(S)
?
?
?
countS(w)?1?
i=0
qsb(w)di
?
?
?+ ? reward(S).
(4)
An optimization problem with this objective
function cannot be regarded as an ILP problem be-
cause it contains non-linear terms. It is also ad-
3The score is still submodular for the purpose of sentence
extraction.
vantageous that the submodular maximization can
deal with such objective functions. Note that the
objective function is such that it can be calculated
according to the type of word. Due to the na-
ture of the objective function, we can use dynamic
programming to effectively search for the subtree
with the maximal density.
4.2 Local Search for Maximal Density
Subtree
Let us now discuss the local search used on line
5 of Algorithm 1. We will use a fast algorithm to
find the maximal density subtree (MDS) of a given
sentence for each cost in Algorithm 1.
Consider the objective function Eq. 4, We can
ignore the second term of the reward function
while looking for the MDS in a sentence because
the number of sentences is the same for every
MDS in a sentence. That is, the gain function of
adding a subtree to a summary can be represented
as the sum of gains for words:
g(t) =
?
w?t
{gainS(w) + freqt(w)c(w)?},
gainS(w) = qsb(w)dcountS(w),
where freqt(w) is the number of ws in subtree
t, and gainS(w) is the gain of adding the word
w to the summary S. Our algorithm is based on
dynamic programming, and it selects a subtree that
maximizes the gain function per cost.
When the word gain is a constant, the algorithm
proposed by Hsieh et al (2010) can be used to
find the MDS. We extended this algorithm to work
for submodular word gain functions that are not
constant. Note that the gain of a word that oc-
curs only once in the sentence, can be treated as
a constant. In what follows, we will describe an
extended algorithm to find the MDS even if there
is word overlap.
For example, let us describe how to obtain the
MDS in the case of a binary tree. First let us tackle
the case in which the gain is always constant. Let
n be a node in the tree, a and b be child nodes of n,
c(n) be the cost of n, mdsca be the MDS rooted at
a and have cost c. mdsn = {mdsc(n)n , . . . ,mdsLn}
denotes the set of MDSs for each cost and its root
node n. The valid subtrees rooted at n can be ob-
tained by taking unions of n with one or both of
t1 ? mdsa and t2 ? mdsb. mdscn is the union that
has the largest gain over the union with the cost of
c (by enumerating all the unions). The MDS for
1027
the sentence root can be found by calculating each
mdscn from the bottom of the tree to the top.
Next, let us consider the objective function that
returns the sum of values of submodular word gain
functions. When there is no word overlap within
the union, we can obtain mdscn in the same man-
ner as for the constant gain. In contrast, if the
union includes word overlap, the gain is less than
the sum of gains: g(mdscn) ? g(n) + g(mdska) +
g(mdsc?k?c(n)b ), where k and c are variables. Thescore reduction can change the order of the gains
of the union. That is, it is possible that another
union without word overlaps will have a larger
gain. Therefore, the algorithm needs to know
whether each t ? mdsn has the potential to have
word overlaps with other MDSs. Let O be the set
of words that occur twice or more in the sentence
on which the local seach focuses. The algorithm
stores MDS for each o ? O, as well as each cost.
By storing MDS for each o and cost as shown
in Fig. 1, the algorithm can find MDS with the
largest gain over the combinations of subtrees.
Algorithm 2 shows the procedure. In it, t andm
denote subtrees, words(t) returns a set of words
in the subtree, g(t) returns the gain of t, tree(n)
means a tree consisting of node n, and t ?m de-
notes the union of subtrees: t and m. subt in-
dicates a set of current maximal density subtrees
among the combinations calculated before. newt
indicates a set of temporary maximal density sub-
trees for the combinations calculated from line 4
to 8. subt[cost,ws] indicates a element of subt that
has a cost cost and contains a set of words ws.
newt[cost,ws] is defined similarly. Line 1 sets subt
to a set consisting of a subtree that indicates node
n itself. The algorithm calculates maximal den-
sity subtrees within combinations of the root node
n and MDSs rooted at child nodes of n. Line 3
iteratively adds MDSs rooted at a next child node
to the combinations; the algorithm then calculates
MDSs newt between subt and the MDSs of the
child node. The procedure from line 6 to 8 selects
a subtree that has a larger gain from the tempo-
rary maximal subtree and the union of t and m.
The computational complexity of this algorithm is
O(NC2) when there is no word overlap within the
sentence, where C denotes the cost of the whole
sentence, and N denotes the number of nodes in
the sentence. The complexity order is the same
as that of the algorithm of Hsieh et al (2010).
When we treat word overlaps, we need to count
Algorithm 2 Algorithm for finding maximal density
subtree for each cost: MDSs.
Function: MDSs
Require: root node n
1: subt[c(n),words(n)?O] = tree(n)
2: newt = ?
3: for i ? child node of n do
4: for t ?MDSs(i) do
5: for m ? subt do
6: index = [c(t ?m), words(t ?m) ? O]
7: newtindex = argmaxj?{newtindex,t?m} g(j)8: end for
9: end for
10: subt = newt
11: end for
12: return subt
Figure 1: Maximal density subtree extraction. The
right table enumerates the subtrees rooted at w2 in
the left tree for all indices. The number in each
tree node is the score of the word.
all unions of combinations of the stored MDSs.
There are at most (C2|O|) MDSs that the algo-
rithm needs to store at each node. Therefore the
total computational complexity is O(NC222|O|).
Since it is unlikely that a sentence contains many
word tokens of one type, the computational cost
may not be so large in practical situations.
5 Experimental Settings
We evaluate our method on Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain questions and
weighted answer nuggets. Our experimental set-
tings followed the settings of (Morita et al, 2011),
except for the maximum summary length. We
generated summaries consisting of 140 Japanese
characters or less, with the question as the query
terms. We did this because our aim is to use our
method in mobile situations. We used ?ACLIA1
test data? to tune the parameters, and evaluated our
method on ?ACLIA2 test? data.
We used JUMAN (Kurohashi and Kawahara,
2009a) for word segmentation and part-of-speech
tagging, and we calculated idf over Mainichi
newspaper articles from 1991 to 2005. For the de-
1028
POURPRE Precision Recall F1 F3
Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174
Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190
Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183
Table 1: Results on ACLIA2 test data.
pendency parsing, we used KNP (Kurohashi and
Kawahara, 2009b). Since KNP internally has a
flag that indicates either an ?obligatory case? or an
?adjacent case?, we regarded dependency relations
flagged by KNP as obligatory in the sentence com-
pression. KNP utilizes Kyoto University?s case
frames (Kawahara and Kurohashi, 2006) as the re-
source for detecting obligatory or adjacent cases.
To evaluate the summaries, we followed the
practices of the TAC summarization tasks (Dang,
2008) and NTCIR ACLIA tasks, and computed
pyramid-based precision with the allowance pa-
rameter, recall, and F? (where ? is 1 or 3)
scores. The allowance parameter was determined
from the average nugget length for each question
type of the ACLIA2 collection (Mitamura et al,
2010). Precision and recall are computed from the
nuggets that the summary covered along with their
weights. One of the authors of this paper man-
ually evaluated whether each nugget matched the
summary. We also used the automatic evaluation
measure, POURPRE (Lin and Demner-Fushman,
2006). POURPRE is based on word matching
of reference nuggets and system outputs. We re-
garded as stopwords the most frequent 100 words
in Mainichi articles from 1991 to 2005 (the doc-
ument frequency was used to measure the fre-
quency). We also set the threshold of nugget
matching as 0.5 and binarized the nugget match-
ing, following the previous study (Mitamura et al,
2010). We tuned the parameters by using POUR-
PRE on the development dataset.
Lin and Bilmes (2011) designed a monotone
submodular function for query-oriented summa-
rization. Their succinct method performed well
in DUC from 2004 to 2007. They proposed a
positive diversity reward function in order to de-
fine a monotone submodular objective function for
generating a non-redundant summary. The diver-
sity reward gives a smaller gain for a biased sum-
mary, because it consists of gains based on three
clusters and calculates a square root score with
respect to each sentence. The reward also con-
tains a score for the similarity of a sentence to
the query, for purposes of query-oriented summa-
Recall Length # of nuggets
Subtree extraction 0.213 11,143 100
Reconstructed (RC) 0.228 13,797 108
Table 2: Effect of sentence compression.
rization. Their objective function also includes a
coverage function based on the similarity wi,j be-
tween sentences. In the coverage function min
function limits the maximum gain ??i?V wi,j ,
which is a small fraction ? of the similarity be-
tween a sentence j and the all source documents.
The objective function is the sum of the positive
reward R and the coverage function L over the
source documents V , as follows:
F(S) = L(S) +
3?
k=1
?kRQ,k(S),
L(S) = ?
i?V
min
??
?
?
j?S
wi,j , ?
?
k?V
wi,k
??
? ,
RQ,k =
?
c?Ck
???? ?
j?S?c
( ?N
?
i?V
wi,j + (1? ?)rj,Q),
where ?, ? and ?k are parameters, and rj,Q repre-
sents the similarity between sentence j and query
Q. We tuned the parameters on the development
dataset. Lin and Bilmes (2011) used three clusters
Ck with different granularities, which were calcu-
lated in advance. We set the granularity to (0.2N ,
0.15N , 0.05N ) according to the settings of them,
where N is the number of sentences in a docu-
ment.
We also regarded as stopwords ???? (tell),?
??? (know),? ?? (what)? and their conjugated
forms, which are excessively common in ques-
tions. For the query expansion in the baseline, we
used Japanese WordNet to obtain synonyms and
hypernyms of query terms.
6 Results
Table 1 summarizes our results. ?Subtree ex-
traction (SbE)? is our method, and ?Sentence ex-
traction (NC)? is a version of our method with-
out compression. The NC has the same objec-
tive function but only extracts sentences. The F1-
measure and F3-measure of our method are 0.159
and 0.190 respectively, while those of the state-of-
1029
the-art baseline are 0.135 and 0.174 respectively.
Unfortunately, since the document set is small, the
difference is not statistically significant. Compar-
ing our method with the one without compression,
we can see that there are improvements in the F1
and F3 scores of the human evaluation, whereas
the POURPRE score of the version of our method
without compression is higher than that of our
method with compression. The compression im-
proved the precision of our method, but slightly
decreased the recall.
For the error analyses, we reconstructed the
original sentences from which our method ex-
tracted the subtrees. Table 2 shows the statistics
of the summaries of SbE and reconstructed sum-
maries (RC). The original sentences covered 108
answer nuggets in total, and 8 of these answer
nuggets were dropped by the sentence compres-
sion. Comparing the results of SbE and RC, we
can see that the sentence compression caused the
recall of SbE to be 7% lower than that of RC.
However, the drop is relatively small in light of
the fact that the sentence compression can discard
19% of the original character length with SbE.
This suggests that the compression can efficiently
prune words while avoiding pruning informative
content.
Since the summary length is short, we can select
only two or three sentences for a summary. As
Morita et al (2011) mentioned, answer nuggets
overlap each other. The baseline objective func-
tion R tends to extract sentences from various
clusters. If the answer nuggets are present in the
same cluster, the objective function does not fit the
situation. However, our methods (SbE and NC)
have a parameter d that can directly adjust overlap
penalty with respect to word importance as well
as query relevance. This may help our methods to
cover similar answer nuggets. In fact, the develop-
ment data resulted in a relatively high parameter d
(0.8) for NC compared with 0.2 for SbE.
7 Conclusions and Future Work
We formalized a query-oriented summarization,
which is a task in which one simultaneously per-
forms sentence compression and extraction, as a
new optimization problem: budgeted monotone
nondecreasing submodular function maximization
with a cost function. We devised an approximate
algorithm to solve the problem in a reasonable
computational time and proved that its approxima-
tion rate is 12(1 ? e?1). Our approach achieved
an F3-measure of 0.19 on the ACLIA2 Japanese
test collection, which is 9.2 % improvement over
a state-of-the-art method using a submodular ob-
jective function.
Since our algorithm requires that the objective
function is the sum of word score functions, our
proposed method has a restriction that we cannot
use an arbitrary monotone submodular function as
the objective function for the summary. Our fu-
ture work will improve the local search algorithm
to remove this restriction. As mentioned before,
we also plan to adapt of our system to other lan-
guages.
Appendix
Here, we analyze the performance guarantee of
Algorithm 1. We use the following notation. S? is
the optimal solution, cu(S) is the residual cost of
subtree u when S is already covered, and i? is the
last step before the algorithm discards a subtree
s ? S? or a part of the subtree s. This is because
the subtree does not belong to either the approxi-
mate solution or the optimal solution. We can re-
move the subtree s? from V without changing the
approximate rate. si is the i-th subtree obtained by
line 5 of Algorithm 1. Gi is the set obtained after
adding subtree si to Gi?1 from the valid set Bi.
Gf is the final solution obtained by Algorithm 1.
f(?) : 2V ? R is a monotone submodular func-
tion.
We assume that there is an equivalent sub-
tree with any union of subtrees in a valid set B:
?t1, t2,?te, te ? {t1, t2}. Note that for any or-
der of the set, the cost or profit of the set is fixed:?
ui?S={u1,...,u|S|} cui(Si?1) = c(S).
Lemma 1 ?X,Y ? V, f(X) ? f(Y ) +?
u?X\Y ?u(Y ), where ?u(S) = f(S ? {u}) ?
f(S).
The inequality can be derived from the definition
of submodularity. 2
Lemma 2 For i = 1, . . . , i?+1, when 0 ? r ? 1,
f(S?)?f(Gi?1)?L
r |S?|1?r
csi (Gi?1)
(f(Gi?1?{si})?f(Gi?1)),
where cu(S)=c(S?{u})?c(S).
Proof. From line 5 of Algorithm 1, we have
?u ? S?\Gi?1,
?u(Gi?1)
cu(Gi?1)r
? ?si(Gi?1)csi(Gi?1)r
.
Let B be a valid set, and union be a func-
tion that returns the union of subtrees. We have
1030
?T ? B, ?b ? B, b = union(T ), because we
have an equivalent tree b ? B for each union
of trees T in a valid set B. That is, for any
set of subtrees, we have an equivalent set of sub-
trees, where bi ? Bi. Without loss of generality,
we can replace the difference set S?\Gi?1 with
a set T ?i?1 = {b0, . . . , b|T ?i?1|} that does not con-tain any two elements extracted from the same
valid set. Thus when 0 ? r ? 1 and 0 ?
i ? i? + 1, ?s?\Gi?1 (Gi?1)cS?\Gi?1 (Gi?1)r =
?T ?i?1 (Gi?1)
cT ?i?1 (Gi?1)
r , and
?bj ? T ?i?1,
?bj (Gi?1)
cbj (Gi?1)r
? ?si (Gi?1)csi (Gi?1)r . Thus,
?T ?i?1 (Gi?1) =
?
u?T ?i?1
?u(Gi?1)
? ?si (Gi?1)csi (Gi?1)r
?
u?T ?i?1
cu(Gi?1)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|
(?
u?T ?i?1
cu(Gi?1)
|T ?i?1|
)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|1?r
(?
u?T ?i?1
cu(?)
)r
? ?si (Gi?1)csi (Gi?1)r |S
?|1?rLr,
where the second inequality is from Ho?lder?s in-
equality. The third inequality uses the submodu-
larity of the cost function,
cu(Gi?1) = c({u} ?Gi?1)? c(Gi?1) ? cu(?)
and the fact that |S?| ? |S?\Gi?1| ? |T ?i?1|, and?
u?T ?i?1 cu(?) = c(T
?
i?1) ? L .
As a result, we have
?s?\Gi?1(Gi?1) = ?T ?i?1(Gi?1)
? ?si(Gi?1)csi(Gi?1)r
|S?|1?rLr.
Let X = S? and Y = Gi?1. Applying Lemma
1 yields
f(S?) ? f(Gi?1) + ?u?S?\Gi?1(Gi?1).
? f(Gi?1) +
?si(Gi?1)
csi(Gi?1)
|S?|1?rLr.
The lemma follows as a result.
Lemma 3 For a normalized monotone submodu-
lar f(?), for i = 1, . . . , i? + 1 and 0 ? r ? 1 and
letting si be the i-th unit added into G and Gi be
the set after adding si, we have
f(Gi) ?
(
1?
i?
k=1
(
1? csk(Gk?1)
r
Lr|S?|1?r
))
f(S?).
Proof. This is proved similarly to Lemma 3 of
(Krause and Guestrin, 2005) using Lemma 2.
Proof of Theorem 1. This is proved similarly to
Theorem 1 of (Krause and Guestrin, 2005) using
Lemma 3.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
481?490, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Calinescu Calinescu, Chandra Chekuri, Martin Pa?l,
and Jan Vondra?k. 2011. Maximizing a monotone
submodular function subject to a matroid constraint.
SIAM Journal on Computing, 40(6):1740?1766.
Michele Conforti and Ge?rard Cornue?jols. 1984. Sub-
modular set functions, matroids and the greedy al-
gorithm: Tight worst-case bounds and some gener-
alizations of the rado-edmonds theorem. Discrete
Applied Mathematics, 7(3):251 ? 274.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks. In Proceedings of Text Analysis Confer-
ence.
Anupam Gupta, Aaron Roth, Grant Schoenebeck, and
Kunal Talwar. 2010. Constrained non-monotone
submodular maximization: offline and secretary
algorithms. In Proceedings of the 6th interna-
tional conference on Internet and network eco-
nomics, WINE?10, pages 246?257, Berlin, Heidel-
berg. Springer-Verlag.
Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The
weight-constrained maximum-density subtree prob-
lem and related problems in trees. The Journal of
Supercomputing, 54(3):366?380, December.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ?06, pages 176?183, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Andreas Krause and Carlos Guestrin. 2005. A
note on the budgeted maximization on submodular
functions. Technical Report CMU-CALD-05-103,
Carnegie Mellon University.
1031
Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ?09, pages 545?554,
Philadelphia, PA, USA. Society for Industrial and
Applied Mathematics.
Sadao Kurohashi and Daisuke Kawahara, 2009a.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara, 2009b. KN
parser (Kurohashi-Nagao parser) 3.0 Users Man-
ual. http://nlp.ist.i.kyoto-u.ac.jp/
EN/index.php?KNP.
Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and
Maxim Sviridenko. 2009. Non-monotone submod-
ular maximization under matroid and knapsack con-
straints. In Proceedings of the 41st annual ACM
symposium on Theory of computing, STOC ?09,
pages 323?332, New York, NY, USA. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 510?520,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jimmy Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to com-
plex questions. Information Retrieval, 9(5):565?
587, November.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR
research, ECIR?07, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-
hua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong
Ji, and Noriko Kando. 2008. Overview of the
NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual
Information Access. In Proceedings of the 7th NT-
CIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai,
Noriko Kando, Tatsunori Mori, Koichi Takeda,
Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and
Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia
tasks: Advanced cross-lingual information access.
In Proceedings of the 8th NTCIR Workshop.
Hajime Morita, Tetsuya Sakai, and Manabu Okumura.
2011. Query snowball: a co-occurrence-based ap-
proach to multi-document summarization for ques-
tion answering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 223?229, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wolfgang Schmidt. 1991. Greedoids and searches in
directed graphs. Discrete Mathmatics, 93(1):75?88,
November.
Jie Tang, Limin Yao, and Dewei Chen. 2009. Multi-
topic based query-oriented summarization. In Pro-
ceedings of 2009 SIAM International Conference
Data Mining (SDM?2009), pages 1147?1158.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summariza-
tion system. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at
NLT/NAACL 2006.
1032
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315?320,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Single Document Summarization based on Nested Tree Structure
Yuta Kikuchi
?
Tsutomu Hirao
?
Hiroya Takamura
?
?
Tokyo Institute of technology
4295, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
{kikuchi,takamura,oku}@lr.pi.titech.ac.jp
?
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
Manabu Okumura
?
Masaaki Nagata
?
Abstract
Many methods of text summarization
combining sentence selection and sen-
tence compression have recently been pro-
posed. Although the dependency between
words has been used in most of these
methods, the dependency between sen-
tences, i.e., rhetorical structures, has not
been exploited in such joint methods. We
used both dependency between words and
dependency between sentences by con-
structing a nested tree, in which nodes
in the document tree representing depen-
dency between sentences were replaced by
a sentence tree representing dependency
between words. We formulated a sum-
marization task as a combinatorial opti-
mization problem, in which the nested
tree was trimmed without losing impor-
tant content in the source document. The
results from an empirical evaluation re-
vealed that our method based on the trim-
ming of the nested tree significantly im-
proved the summarization of texts.
1 Introduction
Extractive summarization is one well-known ap-
proach to text summarization and extractive meth-
ods represent a document (or a set of documents)
as a set of some textual units (e.g., sentences,
clauses, and words) and select their subset as a
summary. Formulating extractive summarization
as a combinational optimization problem greatly
improves the quality of summarization (McDon-
ald, 2007; Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009). There has re-
cently been increasing attention focused on ap-
proaches that jointly optimize sentence extraction
and sentence compression (Tomita et al, 2009;
Qian and Liu, 2013; Morita et al, 2013; Gillick
and Favre, 2009; Almeida and Martins, 2013;
Berg-Kirkpatrick et al, 2011). We can only ex-
tract important content by trimming redundant
parts from sentences.
However, as these methods did not include the
discourse structures of documents, the generated
summaries lacked coherence. It is important for
generated summaries to have a discourse struc-
ture that is similar to that of the source docu-
ment. Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is one way of introduc-
ing the discourse structure of a document to a
summarization task (Marcu, 1998; Daum?e III and
Marcu, 2002; Hirao et al, 2013). Hirao et al
recently transformed RST trees into dependency
trees and used them for single document summa-
rization (Hirao et al, 2013). They formulated the
summarization problem as a tree knapsack prob-
lem with constraints represented by the depen-
dency trees.
We propose a method of summarizing a single
document that utilizes dependency between sen-
tences obtained from rhetorical structures and de-
pendency between words obtained from a depen-
dency parser. We have explained our method with
an example in Figure 1. First, we represent a doc-
ument as a nested tree, which is composed of two
types of tree structures: a document tree and a
sentence tree. The document tree is a tree that has
sentences as nodes and head modifier relationships
between sentences obtained by RST as edges. The
sentence tree is a tree that has words as nodes
and head modifier relationships between words
obtained by the dependency parser as edges. We
can build the nested tree by regarding each node of
the document tree as a sentence tree. Finally, we
formulate the problem of single document sum-
marization as that of combinatorial optimization,
which is based on the trimming of the nested tree.
315
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held on next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held on next month.
EDU?? ???? ??? ????
0
2
4
6
8
10
12
14
16
EDU
selsection
sentence subtree
selection
sentence
selection
reference 
summary
Nu
m
be
r o
f  
se
lec
ted
 se
nt
en
ce
s 
fro
m
 so
ur
ce
 do
cu
m
en
t
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  training  for  a  race.
The  race  is  held  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held next month.
Figure 1: Overview of our method. The source document is represented as a nested tree. Our method
simultaneously selects a rooted document subtree and sentence subtree from each node.
Our method jointly utilizes relations between sen-
tences and relations between words, and extracts
a rooted document subtree from a document tree
whose nodes are arbitrary subtrees of the sentence
tree.
Elementary Discourse Units (EDUs) in RST are
defined as the minimal building blocks of dis-
course. EDUs roughly correspond to clauses.
Most methods of summarization based on RST use
EDUs as extraction textual units. We converted
the rhetorical relations between EDUs to the re-
lations between sentences to build the nested tree
structure. We could thus take into account both
relations between sentences and relations between
words.
2 Related work
Extracting a subtree from the dependency tree of
words is one approach to sentence compression
(Tomita et al, 2009; Qian and Liu, 2013; Morita
et al, 2013; Gillick and Favre, 2009). However,
these studies have only extracted rooted subtrees
from sentences. We allowed our model to extract
a subtree that did not include the root word (See
the sentence with an asterisk ? in Figure 1). The
method of Filippova and Strube (2008) allows the
model to extract non-rooted subtrees in sentence
compression tasks that compress a single sentence
with a given compression ratio. However, it is not
trivial to apply their method to text summariza-
tion because no compression ratio is given to sen-
tences. None of these methods use the discourse
structures of documents.
Daum?e III and Marcu (2002) proposed a noisy-
channel model that used RST. Although their
method generated a well-organized summary, no
optimality of information coverage was guaran-
teed and their method could not accept large texts
because of the high computational cost. In addi-
- The scare over Alar, a growth regulator
- that makes apples redder and crunchier
- but may be carcinogenic,
- made consumers shy away from the Delicious,
- though they were less affected than the McIntosh.
Figure 2: Example of one sentence. Each line cor-
responds to one EDU.
tion, their method required large sets of data to cal-
culate the accurate probability. There have been
some studies that have used discourse structures
locally to optimize the order of selected sentences
(Nishikawa et al, 2010; Christensen et al, 2013).
3 Generating summary from nested tree
3.1 Building Nested Tree with RST
A document in RST is segmented into EDUs and
adjacent EDUs are linked with rhetorical relations
to build an RST-Discourse Tree (RST-DT) that has
a hierarchical structure of the relations. There are
78 types of rhetorical relations between two spans,
and each span has one of two aspects of a nu-
cleus and a satellite. The nucleus is more salient
to the discourse structure, while the other span, the
satellite, represents supporting information. RST-
DT is a tree whose terminal nodes correspond
to EDUs and whose nonterminal nodes indicate
the relations. Hirao et al converted RST-DTs
into dependency-based discourse trees (DEP-DTs)
whose nodes corresponded to EDUs and whose
edges corresponded to the head modifier relation-
ships of EDUs. See Hirao et al for details (Hirao
et al, 2013).
Our model requires sentence-level dependency.
Fortunately we can simply convert DEP-DTs to
obtain dependency trees between sentences. We
specifically merge EDUs that belong to the same
sentence. Each sentence has only one root EDU
that is the parent of all the other EDUs in the sen-
tence. Each root EDU in a sentence has the parent
316
max.
n
?
i
m
i
?
j
w
ij
z
ij
s.t.
?
n
i
?
m
i
j
z
ij
? L; (1)
x
parent(i)
? x
i
; ?i (2)
z
parent(i,j)
? z
ij
+ r
ij
? 0; ?i, j (3)
x
i
? z
ij
; ?i, j (4)
?
m
i
j
z
ij
? min(?, len(i))x
i
; ?i (5)
?
m
i
j
r
ij
= x
i
; ?i (6)
?
j /?R
c
(i)
r
ij
= 0; ?i (7)
r
ij
? z
ij
; ?i, j (8)
r
ij
+ z
parent(i,j)
? 1; ?i, j (9)
r
iroot(i)
= z
iroot(i)
; ?i (10)
?
j?sub(i)
z
ij
? x
i
; ?i (11)
?
j?obj(i)
z
ij
? x
i
; ?i (12)
Figure 3: ILP formulation (x
i
, z
ij
, r
ij
? {0, 1})
EDU in another sentence. Hence, we can deter-
mine the parent-child relations between sentences.
As a result, we obtain a tree that represents the
parent-child relations of sentences, and we can use
it as a document tree. After the document tree is
obtained, we use a dependency parser to obtain the
syntactic dependency trees of sentences. Finally,
we obtain a nested tree.
3.2 ILP formulation
Our method generates a summary by trimming a
nested tree. In particular, we extract a rooted docu-
ment subtree from the document tree, and sentence
subtrees from sentence trees in the document tree.
We formulate our problem of optimization in this
section as that of integer linear programming. Our
model is shown in Figure 3.
Let us denote by w
ij
the term weight of word
ij (word j in sentence i). x
i
is a variable that
is one if sentence i is selected as part of a sum-
mary, and z
ij
is a variable that is one if word ij
is selected as part of a summary. According to the
objective function, the score for the resulting sum-
mary is the sum of the term weights w
ij
that are
included in the summary. We denote by r
ij
the
variable that is one if word ij is selected as a root
of an extracting sentence subtree. Constraint (1)
guarantees that the summary length will be less
than or equal to limit L. Constraints (2) and (3)
are tree constraints for a document tree and sen-
tence trees. r
ij
in Constraint (3) allows the system
to extract non-rooted sentence subtrees, as we pre-
viously mentioned. Function parent(i) returns the
parent of sentence i and function parent(i, j) re-
turns the parent of word ij. Constraint (4) guaran-
tees that words are only selected from a selected
sentence. Constraint (5) guarantees that each se-
lected sentence subtree has at least ? words. Func-
tion len(i) returns the number of words in sentence
i. Constraints (6)-(10) allow the model to extract
subtrees that have an arbitrary root node. Con-
straint (6) guarantees that there is only one root
per selected sentence. We can set the candidate
for the root node of the subtree by using constraint
(7). The R
c
(i) returns a set of the nodes that are
the candidates of the root nodes in sentence i. It
returned the parser?s root node and the verb nodes
in this study. Constraint (8) maintains consistency
between z
ij
and r
ij
. Constraint (9) prevents the
system from selecting the parent node of the root
node. Constraint (10) guarantees that the parser?s
root node will only be selected when the system
extracts a rooted sentence subtree. The root(i) re-
turns the word index of the parser?s root. Con-
straints (11) and (12) guarantee that the selected
sentence subtree has at least one subject and one
object if it has any. The sub(i) and obj(i) return
the word indices whose dependency tag is ?SUB?
and ?OBJ?.
3.3 Additional constraint for grammaticality
We added two types of constraints to our model
to extract a grammatical sentence subtree from a
dependency tree:
z
ik
= z
il
, (13)
?
k?s(i,j)
z
ik
= |s(i, j)|x
i
. (14)
Equation (13) means that words z
ik
and z
il
have
to be selected together, i.e., a word whose depen-
dency tag is PMOD or VC and its parent word, a
negation and its parent word, a word whose de-
pendency tag is SUB or OBJ and its parent verb,
a comparative (JJR) or superlative (JJS) adjective
and its parent word, an article (a/the) and its par-
ent word, and the word ?to? and its parent word.
Equation (14) means that the sequence of words
has to be selected together, i.e., a proper noun se-
quence whose POS tag is PRP$, WP%, or POS
and a possessive word and its parent word and the
words between them. The s(i, j) returns the set of
word indices that are selected together with word
ij.
317
Table 1: ROUGE score of each model. Note that
the top two rows are both our proposals.
ROUGE-1
Sentence subtree 0.354
Rooted sentence subtree 0.352
Sentence selection 0.254
EDU selection (Hirao et al, 2013) 0.321
LEAD
EDU
0.240
LEAD
snt
0.157
4 Experiment
4.1 Experimental Settings
We experimentally evaluated the test collection for
single document summarization contained in the
RST Discourse Treebank (RST-DTB) (Carlson et
al., 2001) distributed by the Linguistic Data Con-
sortium (LDC)
1
. The RST-DTB Corpus includes
385 Wall Street Journal articles with RST anno-
tations, and 30 of these documents also have one
manually prepared reference summary. We set the
length constraint, L, as the number of words in
each reference summary. The average length of
the reference summaries corresponded to approxi-
mately 10% of the length of the source document.
This dataset was first used by Marcu et al for
evaluating a text summarization system (Marcu,
1998). We used ROUGE (Lin, 2004) as an eval-
uation criterion.
We compared our method (sentence subtree)
with that of EDU selection (Hirao et al, 2013).
We examined two other methods, i.e., rooted sen-
tence subtree and sentence selection. These two
are different from our method in the way that they
select a sentence subtree. Rooted sentence subtree
only selects rooted sentence subtrees
2
. Sentence
selection does not trim sentence trees. It simply
selects full sentences from a document tree
3
. We
built all document trees from the RST-DTs that
were annotated in the corpus.
We set the term weight, w
ij
, for our model as:
w
ij
=
log(1 + tf
ij
)
depth(i)
2
, (15)
where tf
ij
is the term frequency of word ij in a
document and depth(i) is the depth of sentence
1
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002T07
2
We achieved this by making R
c
(i) only return the
parser?s root node in Figure 7.
3
We achieved this by setting ? to a very large number.
i within the sentence-level DEP-DT that we de-
scribed in Section 3.1. For Constraint (5), we set
? to eight.
4.2 Results and Discussion
4.2.1 Comparing ROUGE scores
We have summarized the Recall-Oriented Under-
study for Gisting Evaluation (ROUGE) scores for
each method in Table 1. The score for sentence
selection is low (0.254). However, introducing
sentence compression to the system greatly im-
proved the ROUGE score (0.354). The score is
also higher than that with EDU selection, which
is a state-of-the-art method. We applied a multi-
ple test by using Holm?s method and found that
our method significantly outperformed EDU se-
lection and sentence selection. The difference be-
tween the sentence subtree and the rooted sentence
subtree methods was fairly small. We therefore
qualitatively analyzed some actual examples that
will be discussed in Section 4.2.2. We also exam-
ined the ROUGE scores of two LEAD
4
methods
with different textual units: EDUs (LEAD
EDU
)
and sentences (LEAD
SNT
). Although LEAD
works well and often obtains high ROUGE scores
for news articles, the scores for LEAD
EDU
and
LEAD
SNT
were very low.
4.2.2 Qualitative Evaluation of Sentence
Subtree Selection
This subsection compares the methods of subtree
selection and rooted subtree selection. Figure 4
has two example sentences for which both meth-
ods selected a subtree as part of a summary. The
{?} indicates the parser?s root word. The [?] indi-
cates the word that the system selected as the root
of the subtree. Subtree selection selected a root in
both examples that differed from the parser?s root.
As we can see, subtree selection only selected im-
portant subtrees that did not include the parser?s
root, e.g., purpose-clauses and that-clauses. This
capability is very effective because we have to
contain important content in summaries within
given length limits, especially when the compres-
sion ratio is high (i.e., the method has to gener-
ate much shorter summaries than the source docu-
ments).
4
LEADmethods simply take the firstK textual units from
a source document until the summary length reaches L.
318
Original sentence : John Kriz, a Moody?s vice president, {said} Boston Safe Deposit?s performance has been
hurt this year by a mismatch in the maturities of its assets and liabilities.
Rooted subtree selection : John Kriz a Moody?s vice president [{said}] Boston Safe Deposit?s performance has been
hurt this year
Subtree selection : Boston Safe Deposit?s performance has [been] hurt this year
Original sentence : Recent surveys by Leo J. Shapiro & Associates, a market research firm in Chicago,
{suggest} that Sears is having a tough time attracting shoppers because it hasn?t yet done
enough to improve service or its selection of merchandise.
Rooted subtree selection : surveys [{suggest}] that Sears is having a time
Subtree selection : Sears [is] having a tough time attracting shoppers
Figure 4: Example sentences and subtrees selected by each method.
Table 2: Average number of words that individual
extracted textual units contained.
Subtree Sentence EDU
15.29 18.96 9.98
4.2.3 Fragmentation of Information
Many studies that have utilized RST have simply
adopted EDUs as textual units (Mann and Thomp-
son, 1988; Daum?e III and Marcu, 2002; Hirao et
al., 2013; Knight and Marcu, 2000). While EDUs
are textual units for RST, they are too fine grained
as textual units for methods of extractive summa-
rization. Therefore, the models have tended to se-
lect small fragments from many sentences to max-
imize objective functions and have led to frag-
mented summaries being generated. Figure 2 has
an example of EDUs. A fragmented summary
is generated when small fragments are selected
from many sentences. Hence, the number of sen-
tences in the source document included in the re-
sulting summary can be an indicator to measure
the fragmentation of information. We counted
the number of sentences in the source document
that each method used to generate a summary
5
.
The average for our method was 4.73 and its me-
dian was four sentences. In contrast, methods
of EDU selection had an average of 5.77 and a
median of five sentences. This meant that our
method generated a summary with a significantly
smaller number of sentences
6
. In other words, our
method relaxed fragmentation without decreasing
the ROUGE score. There are boxplots of the num-
bers of selected sentences in Figure 5. Table 2 lists
the number of words in each textual unit extracted
by each method. It indicates that EDUs are shorter
than the other textual units. Hence, the number of
sentences tends to be large.
5
Note that the number for the EDU method is not equal to
selected textual units because a sentence in the source docu-
ment may contain multiple EDUs.
6
We used the Wilcoxon signed-rank test (p < 0.05).
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
  Source document                                   John was running on a track in the park.He looks very tired.Mike said he is training for a race.The race is held on next month.
  Summary                                              John was running on a track.he is training for a race.The race is held on next month.
EDU?? ???? ??? ????0
2
4
6
8
10
12
14
16
EDUselsection sentence subtreeselection sentenceselection reference summary
Num
ber o
f  sel
ected
 sent
ence
s 
from
 sour
ce do
cume
nt
Figure 5: Number of sentences that each method
selected.
5 Conclusion
We proposed a method of summarizing a sin-
gle document that included relations between sen-
tences and relations between words. We built a
nested tree and formulated the problem of summa-
rization as that of integer linear programming. Our
method significantly improved the ROUGE score
with significantly fewer sentences than the method
of EDU selection. The results suggest that our
method relaxed the fragmentation of information.
We also discussed the effectiveness of sentence
subtree selection that did not restrict rooted sub-
trees. Although ROUGE scores are widely used
as evaluation metrics for text summarization sys-
tems, they cannot take into consideration linguis-
tic qualities such as human readability. Hence, we
plan to conduct evaluations with people
7
.
We only used the rhetorical structures between
sentences in this study. However, there were also
rhetorical structures between EDUs inside individ-
ual sentences. Hence, utilizing these for sentence
compression has been left for future work. In addi-
tion, we used rhetorical structures that were man-
ually annotated. There have been related studies
on building RST parsers (duVerle and Prendinger,
2009; Hernault et al, 2010) and by using such
parsers, we should be able to apply our model to
other corpora or to multi-document settings.
7
For example, the quality question metric from the Docu-
ment Understanding Conference (DUC).
319
References
Miguel Almeida and Andre Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In ACL, pages
196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
ACL, pages 481?490, Portland, Oregon, USA, June.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In SIGDIAL, pages 1?10.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In NAACL:HLT, pages
1163?1173.
Hal Daum?e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. ACL,
pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In COLING.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In INLG,
pages 25?32.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP, pages 10?18.
Hugo Hernault, Helmut Prendinger, David duVerle,
and Mitsuru Ishizuka. 2010. Hilda: A discourse
parser using support vector machine classification.
Dialogue & Discourse, 1(3):1?30.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In EMNLP, pages 1515?1520.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In National Conference on Artificial Intelli-
gence (AAAI), pages 703?710.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, pages 74?81.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, pages 243?281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In In Proc. of the
6th Workshop on Very Large Corpora, pages 206?
215.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In ECIR, pages 557?564.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In ACL,
pages 1023?1032.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In COL-
ING, pages 910?918.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In EMNLP,
pages 1492?1502.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on the budgeted median
problem. In CIKM, pages 1589?1592.
Kohei Tomita, Hiroya Takamura, and Manabu Oku-
mura. 2009. A new approach of extractive sum-
marization combining sentence selection and com-
pression. IPSJ SIG Notes, pages 13?20.
320
