Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 604?611, Vancouver, October 2005. c?2005 Association for Computational Linguistics
  
The Use of Metadata, Web-derived Answer Patterns and Passage 
Context to Improve Reading Comprehension Performance
Yongping Du 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
ypdu@fudan.edu.cn 
 
 
Helen Meng 
Human-Computer 
Communication Laboratory 
The Chinese University of 
Hong Kong 
 HongKong. SAR. China 
hmmeng@se.cuhk.edu.hk 
 
Xuanjing Huang 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
xjhuang@fudan.edu.cn 
 
 
Lide Wu 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
ldwu@fudan.edu.cn 
Abstract 
A reading comprehension (RC) system 
attempts to understand a document and returns 
an answer sentence when posed with a 
question.  RC resembles the ad hoc question 
answering (QA) task that aims to extract an 
answer from a collection of documents when 
posed with a question.  However, since RC 
focuses only on a single document, the system 
needs to draw upon external knowledge 
sources to achieve deep analysis of passage 
sentences for answer sentence extraction.  
This paper proposes an approach towards RC 
that attempts to utilize external knowledge to 
improve performance beyond the baseline set 
by the bag-of-words (BOW) approach.  Our 
approach emphasizes matching of metadata 
(i.e. verbs, named entities and base noun 
phrases) in passage context utilization and 
answer sentence extraction. We have also 
devised an automatic acquisition process for 
Web-derived answer patterns (AP) which 
utilizes question-answer pairs from TREC QA, 
the Google search engine and the Web.  This 
approach gave improved RC performances for 
both the Remedia and ChungHwa corpora, 
attaining HumSent accuracies of 42% and 
69% respectively.  In particular, performance 
analysis based on Remedia shows that relative 
performances of 20.7% is due to metadata 
matching and a further 10.9% is due to the 
application of Web-derived answer patterns. 
1. Introduction 
A reading comprehension (RC) system attempts to 
understand a document and returns an answer 
sentence when posed with a question.  The RC 
task was first proposed by the MITRE 
Corporation which developed the Deep Read 
reading comprehension system (Hirschman et al, 
1999).  Deep Read was evaluated on the Remedia 
Corpus that contains a set of stories, each with an 
average of 20 sentences and five questions (of 
types who, where, when, what and why). The 
MITRE group also defined the HumSent scoring 
metric, i.e. the percentage of test questions for 
which the system has chosen a correct sentence as 
the answer.  HumSent answers were compiled by a 
human annotator, who examined the stories and 
chose the sentence(s) that best answered the 
questions.  It was judged that for 11% of the 
Remedia test questions, there is no single sentence 
in the story that is judged to be an appropriate 
answer sentence.  Hence the upper bound for RC 
on Remedia should by 89% HumSent accuracy.  
(Hirschman et al 1999) reported a HumSent 
accuracy of 36.6% on the Remedia test set.  
Subsequently, (Ng et al, 2000) used a machine 
learning approach of decision tree and achieved 
the accuracy of 39.3%.   Then (Riloff and Thelen, 
2000) and (Charniak et al, 2000) reported 
improvements to 39.7% and 41%, respectively.  
They made use of handcrafted heuristics such as 
the WHEN rule: 
if contain(S, TIME), then Score(S)+=4 
i.e. WHEN questions reward candidate answer 
sentences with four extra points if they contain a 
name entity TIME.  
RC resembles the ad hoc question answering 
(QA) task in TREC.1  The QA task finds answers 
to a set of questions from a collection of 
documents, while RC focuses on a single 
                                                                 
1 http://www.nist.gov. 
604
  
document.  (Light et al 1998) conducted a 
detailed compared between the two tasks.  They 
found that the answers of most questions in the 
TREC QA task appear more than once within the 
document collection.  However, over 80% of the 
questions in the Remedia corpus correspond to 
answer sentences that have a single occurrence 
only.  Therefore an RC system often has only one 
shot at finding the answer. The system is in dire 
need of extensive knowledge sources to help with 
deep text analysis in order to find the correct 
answer sentence.   
Recently, many QA systems have exploited 
the Web as a gigantic data repository in order to 
help question answering (Clarke et al, 2001; 
Kwok et al, 2001; Radev et al, 2002).  Our 
current work attempts to incorporate a similar idea 
in exploiting Web-derived knowledge to aid RC.  
In particular, we have devised an automatic 
acquisition process for Web-derived answer 
patterns. Additionally we propose to emphasize 
the importance of metadata matching in our 
approach to RC.  By metadata, we are referring to 
automatically labeled verbs, named entities as well 
as base noun phrases in the passage.  It is 
important to achieve a metadata match between 
the question and a candidate answer sentence 
before the candidate is selected as the final answer.  
The candidate answer sentence may be one with a 
high degree of word overlap with the posed 
question, or it may come from other sentences in 
the neighboring context. We apply these different 
techniques step by step and obtain better results 
than have ever previously been reported. 
Especially, we give experiment analysis for 
understanding the results. 
    In the rest of this paper, we will first describe 
three main aspects of our approach towards RC ? 
(i) metadata matching, (ii)automatic acquisition of 
Web-derived answer patterns and (iii) the use of 
passage context.  This will be followed by a 
description of our experiments, analysis of results 
and conclusions. 
2. Metadata Matching 
A popular approach in reading comprehension is 
to represent the information content of each 
question or passage sentence as a bag of words 
(BOW).  This approach incorporates stopword 
removal and stemming.  Thereafter, two words are 
considered a match if they share the same 
morphological root.  Given a question, the BOW 
approach selects the passage sentence with the 
maximum number of matching words as the 
answer.  However, the BOW approach does not 
capture the fact that the informativeness of a word 
about a passage sentence varies from one word to 
another.  For example, it has been pointed out by 
(Charniak et al 2000) that the verb seems to be 
especially important for recognizing that a passage 
sentence is related to a specific question.  In view 
of this, we propose a representation for questions 
and answer sentences that emphasizes three types 
of metadata:  
(i) Main Verbs (MVerb), identified by the link 
parser (Sleator and Temperley 1993);  
(ii) Named Entities (NE), including names of 
locations (LCN), persons (PRN) and organizations 
(ORG), identified by a home-grown named entity 
identification tool; and  
(iii) Base Noun Phrases (BNP), identified by a 
home-grown base noun phrase parser respectively. 
We attempt to quantify the relative importance 
of such metadata through corpus statistics 
obtained only from the training set of the Remedia 
corpus, which has 55 stories. The Remedia test set, 
which contains 60 stories, is set aside for 
evaluation. On average, each training story has 20 
sentences and five questions. There are 274 
questions in all in the entire training set.  Each 
question corresponds to a marked answer sentence 
within the story text.  We analyzed all the 
questions and divided them into three question 
sets (Q_SETS) based on the occurrences of 
MVerb, NE and BNP identified with the tools 
mentioned above.  The following are illustrative 
examples of the Q_SETS as well as their sizes: 
Q_SETMverb  
(Count:169) 
Who helped the Pilgrims? 
Q_SETNE    
 (Count:62) 
When was the first merry-go-
round built in the United States? 
Q_SETBNP   
(Count:232) 
Where are the northern lights? 
Table 1.  Examples and sizes of question sets (Q_SETS) 
with different metadata ? main  verb (MVerb), named 
entity (NE) and base noun phrase (BNP). 
   It may also occur that a question belongs to 
multiple Q_SETS.  For example:  
605
  
Q_SETMVerb 
 
When was the first merry-go-round built 
in the United States? 
Q_SETNE 
 
When was the first merry-go-round built 
in the United States? 
Q_SETBNP 
 
When was the first merry-go-round built 
in the United States? 
Table 2.  An example sentence that belongs to multiple 
Q_SETS. 
As mentioned earlier, each question 
corresponds to an answer sentence, which is 
annotated in the story text by MITRE.  Hence we 
can follow the Q_SETS to divide the answer 
sentences into three answer sets (A_SETS).  
Examples of A_SETS that correspond to Table 1 
include: 
A_SETMVerb 
 
An Indian named Squanto came 
to help them. 
A_SETNE 
 
The first merry-go-round in the 
United States was built in 1799.
A_SETBNP 
 
Then these specks reach the air 
high above the earth. 
Table 3.  Examples of the answer sets (A_SETS) 
corresponding to the different metadata categories, 
namely, main verb (MVerb), named entity (NE) and 
base noun phrase) (BNP). 
    In order to quantify the relative importance of 
matching the three kinds of metadata between 
Q_SET and A_SET for reading comprehension, 
we compute the following relative weights based 
on corpus statistics: 
|_|
||
Metadata
Metadata
Metadata SETA
SWeight =  ?..Eqn (1) 
where SMetadata is the set of answer sentences in 
|A_SETMetadata| that contain the metadata of its 
corresponding question.  For example, referring to 
Tables 2 and 3, the question in Q_SETNE ?When 
was the first merry-go-round built in the United 
Sates?? contains the named entity (underlined) 
which is also found in the associated answer 
sentence from A_SETNE, ?The first merry-go-
round in the United States was built in 1799.?  
Hence this answer sentence belongs to the set SNE.   
Contrarily, the question in Q_SETBNP ?Where are 
the northern lights?? contains the base noun 
phrase (underlined) but it is not found in the 
associated answer sentence from A_SETBNP, 
?Then these specks reach the air high above the 
earth.?  Hence this answer sentence does not 
belong to the set SBNP.  Based on the three sets, we 
obtain the metadata weights: 
WeightMVerb=0.64, WeightNE=0.38, WeightBNP=0.21 
To illustrate how these metadata weights are 
utilized in the RC task, consider again the 
question, ?Who helped the Pilgrims?? together 
with three candidate answers that are ?equally 
good? with a single word match when the BOW 
approach is applied.  We further search for 
matching metadata among these candidate 
answers and use the metadata weights for scoring.   
Question Who helped the Pilgrims? 
MVerb identified: ?help? 
BNP identified: ?the Pilgrams? 
Candidate 
Sentence 1 
 
An Indian named Squanto came to help. 
Matched MVerb (underlined) 
Score= WeightMVerb=0.64 
Candidate 
Sentence 2 
 
By fall, the Pilgrims had enough food for 
the winter. 
Matched BNP (underlined) 
Score= WeightBNP=0.21 
Candidate 
Sentence 3 
 
Then the Pilgrims and the Indians ate and 
played games. 
Matched BNP (underlined) 
Score= WeightBNP=0.21 
Table 4.  The use of metadata matching to extend the 
bag-of-words approach in reading comprehension.  
3. Web-derived Answer Patterns 
In addition to using metadata for RC, the proposed 
approach also leverages knowledge sources that 
are external to the core RC resources ? primarily 
the Web and other available corpora.  This section 
describes our approach that attempts to 
automatically derive answer patterns from the 
Web as well as score useful answer patterns to aid 
RC.  We utilize the open domain question-answer 
pairs (2393 in all) from the Question Answering 
track of TREC (TREC8-TREC12) as a basis for 
automatic answer pattern acquisition.   
3.1 Deriving Question Patterns 
We define a set of question tags (Q_TAGS) that 
extend the metadata above in order to represent 
question patterns.  The tags include one for main 
verbs (Q_MVerb), three for named entities 
(Q_LCN, Q_PRN and Q_ORG) and one for base 
noun phrases (Q_BNP). We are also careful to 
ensure that noun phrases tagged as named entities 
are not further tagged as base noun phrases. 
606
  
    A question pattern is expressed in terms of 
Q_TAGS.  A question pattern can be used to 
represent multiple questions in the TREC QA 
resource.  An example is shown in Table 5.  
Tagging the TREC QA resource provides us with 
a set of question patterns {QPi} and for each 
pattern, up to mi example questions. 
Question Pattern (QPi): 
When do Q_PRN Q_MVerb Q_BNP? 
Represented questions: 
Q1: When did Alexander Graham Bell invent the 
telephone? 
Q2: When did Maytag make Magic Chef 
refrigerators? 
Q3: When did Amumdsen reach the South Pole? 
(mi example questions in all) 
Table 5.  A question pattern and some example 
questions that it represents. 
3.2  Deriving Answer Patterns 
For each question pattern, we aim to derive 
answer patterns for it automatically from the Web. 
The set of answer patterns capture possible ways 
of embedding a specific answer in an answer 
sentence.  We will describe the algorithm for 
deriving answer patterns as following and 
illustrate with the following question answer pair 
from TREC QA:  
Q: When did Alexander Graham Bell invent the 
telephone? 
A: 1876 
1. Formulate the Web Query 
The question is tagged and the Web query is 
formulated as ?Q_TAG?+ ?ANSWER?, i.e. 
Question: ?When did Alexander Graham Bell 
invent the telephone?? 
QP:            When do Q_PRN Q_MVerb Q_BNP ? 
where Q_PRN= ?Alexander Graham Bell?, 
Q_MVerb= ?invent?, and  Q_BNP=  ?the 
telephone? 
hence Web query:  ?Alexander Graham Bell?+ 
?invent? + ?the telephone? + ?1876? 
2. Web Search and Snippet Selection 
The Web query is submitted to the search 
engine Google using the GoogleAPI and the top 
100 snippets are downloaded.  From each 
snippet, we select up to ten contiguous words to 
the left as well as to the right of the ?ANSWER? 
for answer pattern extraction.  The selected 
words must be continuous and do not cross the 
snippet boundary that Google denotes with ???. 
3. Answer Pattern Selection 
We label the terms in each selected snippet with 
the Q_TAGs from the question as well as the 
answer tag <A>.  The shortest string containing 
all these tags (underlined below) is extracted as 
the answer pattern (AP).  For example:  
Snippet 1: 1876, Alexander Graham Bell 
invented the telephone in the United States? 
AP 1:   <A>, Q_PRN Q_MVerb Q_BNP. 
(N.B.  The answer tag <A> denotes ?1876? in this 
example). 
Snippet 2: ?which has been invented by 
Alexander Graham Bell in 1876? 
AP 2:    Q_MVerb by Q_PRN in <A>. 
    As may be seen in above, the acquisition 
algorithm for Web-derived answer questions calls 
for specific answers, such as a factoid in a word or 
phrase.  Hence the question-answer pairs from 
TREC QA are suitable for use.  On the other hand, 
Remedia is less suitable here because it contains 
labelled answer sentences instead of factoids.  
Inclusion of whole answer sentences in Web 
query formulation generally does not return the 
answer pattern that we seek in this work. 
3.3 Scoring the Acquired Answer Patterns 
The answer pattern acquisition algorithm returns 
multiple answer patterns for every question-
answer pair submitted to the Web.   In this 
subsection we present an algorithm for deriving 
scores for these answer patterns.  The 
methodology is motivated by the concept of 
confidence level, similar to that used in data 
mining.  The algorithm is as follows: 
1. Formulate the Web Query 
For each question pattern QPi (see Table 5) 
obtained previously, randomly select an example 
question among the mi options that belongs to this 
pattern.  The question is tagged and the Web 
query is formulated in terms of the Q_TAGs only.  
(Please note that the corresponding answer is 
excluded from Web query formulation here, 
which differs from the answer pattern acquisition 
algorithm).  E.g., 
Question: ?When did Alexander Graham Bell 
invent the telephone? 
Q_TAGs: Q_PRN Q_MVerb Q_BNP 
Web query:  ?Alexander Graham Bell?+ 
?invent? + ?the telephone? 
2.   Web Search and Snippet Selection 
The Web query is submitted to the search engine 
607
  
Google and the top 100 snippets are downloaded. 
3.   Scoring each Answer Pattern APij relating to 
QPi 
Based on the question, its pattern QPi, the answer 
and the retrieved snippets, totally the following 
counts for each answer pattern APij relating to 
QPi . 
cij ? # snippets matching APij and for which the 
tag <A> matches the correct answer. 
nij ? #  snippets matching APij and for which the 
tag <A> matches any term 
Compute the ratio rij= cij / nij..........Eqn(2) 
Repeat steps 1-3 above for another example 
question randomly selected from the pool of mi 
example under QPi.  We arbitrarily set the 
maximum number of iterations to be ki = ??
???
?
im3
2  
in order to achieve decent coverage of the 
available examples.  The confidence for APij.is 
computed as          
k
r
APConfidence
k
i
ij
ij
?
== 1)( ??Eqn(3) 
Equation (3) tries to assign high confidence 
values to answer patterns APij that choose the 
correct answers, while other answer patterns are 
assigned low confidence values.  E.g.: 
<A>, Q_PRN Q_MVerb Q_BNP     (Confidence=0.8) 
Q_MVerb by Q_PRN in <A>.         (Confidence=0.76) 
3.4 Answer Pattern Matching in RC 
The Web-derived answer patterns are used in the 
RC task.  Based on the question and its QP, we 
select the related AP to match among the answer 
sentence candidates.  The candidate that matches 
the highest-scoring AP will be selected.  We find 
that this technique is very effective for RC as it 
can discriminate among candidate answer 
sentences that are rated ?equally good? by the 
BOW or metadata matching approaches, e.g.: 
Q:   When is the Chinese New Year? 
QP: When is the Q_BNP? 
where Q_BNP=Chinese New Year 
Related AP:  Q_BNP is <A> (Confidence=0.82) 
Candidate answer sentences 1: you must wait a few more 
weeks for the Chinese New Year. 
Candidate answer sentences 2: Chinese New Year is most 
often between January 20 and February 20. 
Both candidate answer sentences have the same 
number of matching terms ? ?Chinese?, ?New? 
and ?Year? and the same metadata, i.e. 
Q_BNP=Chinese New Year. The term ?is? is 
excluded by stopword removal. However the 
Web-derived answer pattern is able to select the 
second candidate as the correct answer sentence. 
Hence our system gives high priority to the 
Web-derived AP ? if a candidate answer sentence 
can match an answer pattern with confidence > 
0.6, the candidate is taken as the final answer.  No 
further knowledge constraints will be enforced. 
4. Context Assistance 
During RC, the initial application of the BOW 
approach focuses the system?s attention on a small 
set of answer sentence candidates.  However, it 
may occur the true answer sentence is not 
contained in this set.  As was observed by (Riloff 
and Thelen, 2000) and (Charniak et al, 2000), the 
correct answer sentence often precedes/follows the 
sentence with the highest number of matching 
words.  Hence both the preceding and following 
context sentences are searched in their work to 
find the answer sentence especially for why 
questions. 
Our proposed approach references this idea in 
leveraging contextual knowledge for RC.  
Incorporation of contextual knowledge is very 
effective when used in conjunction with named 
entity (NE) identification.  For instance, who 
questions should be answered with words tagged 
with Q_PRN (for persons).  If the candidate 
sentence with the highest number of matching 
words does not contain the appropriate NE, it will 
not be selected as the answer sentence.  Instead, 
our system searches among the two preceding and 
two following context sentences for the 
appropriate NE.  Table 6 offers an illustration. 
Data analysis Remedia training set shows that the 
context window size selected is appropriate for 
when, who and where questions.   
Football Catches On Fast 
(LATROBE, PA., September 4, 1895) - The new 
game of football is catching on fast, and each month new 
teams are being formed. 
Last night was the first time that a football player was 
paid.  The man's name is John Brallier, and he was paid 
$10 to take the place of someone who was hurt.? 
Question: Who was the first football player to be paid? 
Sentence with maximum # matching words: Last night 
was the first time that a football player was paid. 
Correct answer sentence: The man's name is John 
Brallier, and he was paid $10 to take the place of 
someone who was hurt. 
Table 6.  An example illustrating the use of contextual 
knowledge in RC. 
608
  
As for why questions, a candidate answer 
sentence is selected from the context window if its 
first word is one of ?this?, ?that?, ?these?, 
?those?, ?so? or ?because?.  We did not utilize 
contextual constraints for what questions. 
5. Experiments 
RC experiments are run on the Remedia corpus as 
well as the ChungHwa corpus.  The Remedia 
training set has 55 stories, each with about five 
questions.  The Remedia test set has 60 stories and 
5 questions per story.  The ChungHwa corpus is 
derived from the book, ?English Reading 
Comprehension in 100 days,? published by 
Chung Hwa Book Co., (H.K.) Ltd.  The 
ChungHwa training set includes 100 English 
stories and each has four questions on average.  
The ChungHwa testing set includes 50 stories and 
their questions.  We use HumSent as the prime 
evaluation metric for reading comprehension.   
The three kinds of knowledge sources are used 
incrementally in our experimental setup and 
results are labeled as follows: 
Result Technique 
Result_1 BOW 
Result_2 BOW+MD 
Result_3 BOW+MD+AP 
Result_4 BOW+MD+AP+Context 
Table 7.  Experimental setup in RC evaluations.  
Abbrievations are: bag-of-words (BOW), metadata 
(MD), Web-derived answer patterns (AP), contextual 
knowledge (Context). 
5.1 Results on Remedia 
Table 8 shows the RC results for various question 
types in the Remedia test set.  
 When Who What Where Why 
Result_1 32.0% 30.0% 31.8% 29.6% 18.6%
Result_2 40.0% 28.0% 39.0% 38.0% 20.0%
Result_3 52.6% 42.8% 40.6% 38.4% 21.0%
Result_4 55.0% 48.0% 40.6% 36.4% 27.6%
 Table 8.  HumSent accuracies for the Remedia test set. 
We observe that the HumSent accuracies vary 
substantially across different interrogatives. The 
system performs best for when questions and 
worst for why questions. The use of Web-derived 
answer patterns brought improvements to all the 
different interrogatives.  The other knowledge 
sources, namely, meta data and context, bring 
improvements for some question types but 
degraded others.  
Figure 1 shows the overall RC results of our 
system.  The relative incremental gains due to the 
use of metadata, Web-derived answer patterns and 
context are 20.7%, 10.9% and 8.2% respectively.  
We also ran pairwise t-tests to test the statistical 
significance of these improvements and results are 
shown in Table 9.  The improvements due to 
metadata matching and Web-derived answer 
patterns are statistically significant (p<0.05) but 
the improvement due to context is not. 
29%
35%
38.80% 42%
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
Result_1 Result_2 Result_3 Result_4
H
um
Se
nt
 P
re
ci
sio
n
 
Figure 1.  HumSent accuracies for Remedia. 
Pairwise 
Comparison 
Result_1 & 
Result_2 
Result_2 & 
Result_3 
Result_3 & 
Result_4 
t-test Results t(4)=2.207, 
p=0.046 
t(4)=2.168, 
p=0.048 
t(4)=1.5, 
p=0.104 
Table 9.  Tests of statistical significance in the 
incremental improvements over BOW among the use 
of metadata, Web-derived answer patterns and context.   
We also compared our results across various 
interrogatives with those previously reported in 
(Riloff and Thelen, 2000).  Their system is based 
on handcrafted rules with deterministic algorithms.  
The comparison (see Table 10) shows that our 
approach which is based on data-driven patterns 
and statistics can achieve comparable performance. 
Question Type Riloff &Thelen 2000 Result_4 
When 55% 55.0% 
Who 41% 48.0% 
What 28% 40.6% 
Where 47% 36.4% 
Why 28% 27.6% 
Overall 40% 42.0% 
Table 10.  Comparison of HumSent results with a 
heuristic based RC system (Riloff & Thelen 00).  
5.2 Results on ChungHwa 
Experimental results for the ChungHwa corpus are 
presented in Figure 2.  The HumSent accuracies 
obtained are generally higher than those with 
609
  
Remedia.  We observe similar trends as before, i.e. 
our approach in the use of metadata, Web-derived 
answer patterns and context bring incremental 
gains to RC performance.  However, the actual 
gain levels are much reduced. 
65%
66%
68%
69%
63%
64%
65%
66%
67%
68%
69%
70%
Result_1 Result_2 Result_3 Result_4
H
um
Se
nt
 P
re
ci
sio
n
 
Figure 2.  HumSent accuracies for ChungHwa. 
5.3. Analyses of Results 
In order to understand the underlying reason for 
reduced performance gains as we migrated from 
Remedia to Chunghwa, we analyzed the question 
lengths as well as the degree of word match 
between questions and answers among the two 
corpora.  Figure 3 shows that the average length 
of questions in Chunghwa are longer than 
Remedia.  Longer questions contain more 
information which is beneficial to the BOW 
approach in finding the correct answer. 
32.6
7.5
32.5
60
13.3
54.1
0
10
20
30
40
50
60
70
?4 5,6,7 ?8
Question Length
Pe
rc
en
t o
f Q
ue
st
io
ns
 (%
)
Remedia ChungHwa
 
Figure 3.  Distribution of question lengths among the 
Remedia and ChungHwa corpora. 
The degree of word match between questions 
and answers among the two corpora is depicted in 
Figure 4.  We observe that ChungHwa has a larger 
proportion of questions that have a match- size (i.e. 
number of matching words between a question 
and its answer) larger than 2.  This presents an 
advantage for the BOW approach in RC.  It is also 
observed that approximately 10% of the Remedia 
questions have no correct answers (i.e. match-
size=-1) and about 25% have no matching words 
with the correct answer sentence.  This explains 
the overall discrepancies in HumSent accuracies 
between Remedia and ChungHwa. 
0
5
10
15
20
25
30
35
-1 0 1 2 3 4 5 ?6
Match Size
Pe
rc
en
t o
f Q
ue
st
io
ns
 (%
)
Remedia ChungHwa
 
Figure 4.  Distribution of match-sizes (i.e. the number 
of matching words between questions and their 
answers) in the two corpora. 
While our approach has leveraged a variety of 
knowledge sources in RC, we still observe that 
our system is unable to correctly answer 58% of 
the questions in Remedia. An example of such 
elusive questions is:  
Question: When do the French celebrate their 
freedom? 
Answer Sentence: To the French, July 14 has the 
same meaning as July 4th does to the United 
States.  
6. Conclusions 
A reading comprehension (RC) system aims to 
understand a single document (i.e. story or passage) 
in order to be able to automatically answer questions 
about it.   The task presents an information retrieval 
paradigm that differs significantly from that found in 
Web search engines.  RC resembles the question 
answering (QA) task in TREC which returns an 
answer for a given question from a collection of 
documents.  However, while a QA system can 
utilize the knowledge and information in a collection 
of documents, RC systems focuses only on a single 
document only.  Consequently there is a dire need to 
draw upon a variety of knowledge sources to aid 
deep analysis of the document for answer generation.  
This paper presents our initial effort in designing an 
approach for RC that leverages a variety of 
knowledge sources beyond the context of the 
passage, in an attempt to improve RC performance 
beyond the baseline set by the bag-of-words (BOW) 
approach.  The knowledge sources include the use of 
metadata (i.e. verbs, named entities and base noun 
phrases).  Metadata matching is applied in our 
approach in answer sentence extraction as well as 
use of contextual sentences.  We also devised an 
610
  
automatic acquisition algorithm for Web-derived 
answer patterns.  The acquisition process utilizes 
question-answer pairs from TREC QA, the Google 
search engine and the Web.  These answer patterns 
capture important structures for answer sentence 
extraction in RC.  The use of metadata matching and 
Web-derived answer patterns improved reading 
comprehension performances for the both Remedia 
and ChungHwa corpora. We obtain improvements 
over previously reported results for Remedia, with 
an overall HumSet accuracy of 42%.  In particular, a 
relative gain of 20.7% is due to metadata matching 
and a further 10.9% is due to application of Web-
derived answer patterns. 
Acknowledgement 
This work is partially supported by the Direct 
Grant from The Chinese University of Hong Kong 
(CUHK) and conducted while the first author was 
visiting CUHK. This work is supported by Natural 
Science Foundation of China under Grant 
No.60435020. 
References 
Charles L.A. Clarke, Gordon V. Cormack, Thomas R. 
Lynam. 2001. Exploiting Redundancy in Question 
Answering. In Proceedings of the 24th ACM 
Conference on Research and Development in 
Information Retrieval (SIGIR-2001, New Orleans, 
LA). ACM Press. New York, 358?365. 
Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld. 2001. 
Scaling Question Answering to the Web. In 
Proceedings of the 10th World Wide Web 
Conference (WWW?2001). 150-161. 
Daniel Sleator and Davy Temperley. 1993. Parsing 
English with a Link Grammar. Third International 
Workshop on Parsing Technologies. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question 
Answering System. In Proceedings of the 40th 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2002). 41-47.  
Dell Zhang, Wee Sun Lee. 2002. Web Based Pattern 
Mining and Matching Approach to Question 
Answering. In Proceedings of the TREC-11 
Conference. 2002. NIST, Gaithersburg, MD, 505- 
512. 
Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu, 
Amardeep Grewal. 2002. Probabilistic Question 
Answering on the Web. In Proceedings of the 11th 
World Wide Web Conference (WWW?2002). 
Ellen Riloff and Michael Thelen. 2000. A Rule-based 
Question Answering System for Reading 
Comprehension Test. ANLP/NAACL-2000 
Workshop on Reading Comprehension Tests as 
Evaluation for Computer-Based Language 
Understanding Systems. 
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo 
Braz, Benjamin Garrett, Margaret Kosmala, Tomer 
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun, 
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa 
Zorn. 2000. Reading Comprehension Programs in a 
Statistical-Language-Processing Class. ANLP-
NAACL 2000 Workshop: Reading Comprehension 
Tests as Evaluation for Computer-Based Language 
Understanding Systems. 
Hwee Tou Ng, Leong Hwee Teo, Jennifer Lai Pheng 
Kwan. 2000. A Machine Learning Approach to 
Answering Questions for Reading Comprehension 
Tests.  Proceedings of the 2000 Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora 2000. 
Lynette Hirschman, Marc Light, Eric Breck, and John 
Burger. 1999. Deep Read: A Reading 
Comprehension System. Proceedings of the 37th 
Annual Meeting of the Association for 
Computational Linguistics. 
Marc Light, Gideon S. Mann, Ellen Riloff and Eric 
Break. 1998. Analyses for Elucidating Current 
Question Answering Technology.  Natural 
Language Engineering. Vol. 7, No. 4.  
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use 
of Patterns for Detection of Likely Answer Strings: 
A Systematic Approach. In Proceedings of the 
TREC-11 Conference. 2002. NIST, Gaithersburg, 
MD, 134-143. 
611
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 498 ? 506, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Answering Definition Questions Using  
Web Knowledge Bases 
Zhushuo Zhang, Yaqian Zhou, Xuanjing Huang, and Lide Wu 
Department of Computer Science and Engineering, Fudan University, Shanghai, China, 200433 
{zs_zhang, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn 
Abstract. This paper presents a definition question answering approach, which 
is capable of mining textual definitions from large collections of documents. In 
order to automatically identify definition sentences from a large collection of 
documents, we utilize the existing definitions in the Web knowledge bases in-
stead of hand-crafted rules or annotated corpus. Effective methods are adopted to 
make full use of Web knowledge bases, and they promise high quality response 
to definition questions. We applied our system in the TREC 2004 definition 
question-answering task and achieved an encouraging performance with the F-
measure score of 0.404, which was ranked second among all the submitted runs. 
1   Introduction 
When people want to learn an unknown concept from a large collection of documents, 
the most commonly used tools are the search engines. They submit a query to a search 
engine system, and the search engine returns a number of pages related to the query 
terms. Usually, the pages returned are ranked mainly based on keywords matching 
rather than their relevance to the query terms. The users have to read a lot of returned 
pages to organize the information they wanted by themselves. This procedure is time-
consuming, and the information acquired is not concentrative. The research of Ques-
tion Answering (QA) intends to resolve this problem by answering user?s questions 
with exact answers.  
Questions like ?Who is Colin Powell?? or ?What is mold?? are definition questions 
[3]. Their relatively frequent occurrences in logs of Web search engines [2] indicate 
that they are an important type of question. The Text REtrieval Conference (TREC) 
provides an entire evaluation for definition question answering from TREC2003. A 
typical definition QA system extracts definition nuggets that contain the most descrip-
tive information about the question target (the concept for which information is being 
sought is called the target term, or simply, the target) from multiple documents.  
Until recently, definition questions remained a largely unexplored area of question 
answering. Standard factoid question answering technology, designed to extract single 
answers, cannot be directly applied to this task. The solution to this interesting re-
search challenge will involve the techniques in related fields such as information 
extraction, multi-document summarization, and answer fusion. 
In order to extract definitional nuggets/sentences, most systems use various pattern 
matching approaches. Kouylekov et al [10] relied on a set of hand-crafted rules to 
 Answering Definition Questions Using Web Knowledge Bases 499 
find definitional sentences. Sasha et al [12] proposed to combine data-driven statisti-
cal method and machine learned rules to generate definitions. Cui et al [7] used soft 
patterns, which were generated by unsupervised learning. Such methods require hu-
man labor to construct patterns or to annotate corpus more or less. 
Prager et al [8] try to solve this problem through existing technology. They de-
compose a definition question into a series of factoid questions. The answers to the 
factoid questions are merged to form the answer to the original question. However, 
the performance of their system on the TREC definition QA task is unsatisfactory. 
They need a more proper framework to determine how to generate these follow-up 
questions [8]. 
Some systems [1] [7] [9] statistically rank the candidate answers based on the ex-
ternal knowledge. They all adopt a centroid-based ranking method. For each question, 
they form one centroid (i.e., vector of words and frequencies) of the information in 
the external knowledge, and then calculate the similarity between the candidate an-
swer and this centroid. The ones that have large similarity are extracted as the answers 
to this question.  
Among the abundant information on the Web, Web knowledge bases (KBs) are 
one kind of most useful resource to acquire information. Dictionary definitions often 
supply knowledge that can be exploited directly. The information from them can 
model the interests of a typical user more reliably than other information. So we go 
further in identifying and selecting definition sentences from document collection 
using Web knowledge bases.  
Our work differs from the above in that we make use of the Web knowledge bases 
in a novel and effective way. Instead of using centroid-based ranking, we try to find 
out more effective methods in ranking the candidate sentences. We consider the rela-
tionship and the difference between the definitions from different knowledge sources. 
In our first algorithm, we calculate the similarity scores between the candidate sen-
tence and the definitions from different knowledge bases respectively, and merge 
these scores to generate the weight of this candidate sentence. In another algorithm, 
we first summarize the definitions from different KBs in order to eliminate the redun-
dant information, and then use this summary to rank the candidate sentences. We have 
applied our approaches to the TREC 2004 definition question-answering task. The 
results reveal that these procedures can make better use of the knowledge in the Web 
KBs, and the extracted sentences contain the most descriptive information about the 
question target. 
The remainder of the paper is organized as follows. In Section 2, we describe the 
system architecture. Then in Section 3 we give the details of our definition extraction 
methods. The evaluation of our system and the concluding remarks are given in Sec-
tion 4 and Section 5 respectively. 
2   System Architecture 
We adopt a general architecture for definition QA. The system consists of five mod-
ules: question processing, document processing, Web knowledge acquisition, defini-
tion extraction, and an optional module corpus information acquisition. The process 
of answering a definition question is briefly described as follows. 
500 Z. Zhang et al 
Firstly, a definition question is input, and the question processing module identifies 
the question target from this question. The so called target or target term is a term for 
which information is being sought (e.g., the target of the question ?What is Hale Bopp 
comet?? is ?Hale Bopp comet?.) The target term is the input for document processing 
module and knowledge acquisition module.  
Secondly, the document processing module generates the candidate sentence set 
according to this target term. This module has three steps, document retrieval, rele-
vant sentence extraction and redundancy removal. In the first step, the documents that 
relevant to the target are retrieved from the corpus. In the second step, the sentences 
that relevant to the target are extracted from these documents. We first cut the docu-
ments into sentences, and delete the irrelevant sentences by a few heuristic rules. In 
the third step, the redundant sentences are deleted by calculating the percentage of 
shared content words between sentences. After these three steps, we get the candidate 
sentence set.  
 
Definition 
Extraction 
Question 
processing 
Web Knowledge 
Acquisition 
Answer 
Question 
Target 
Candidate sentence set 
Definitions from Web KB 
World Wide Web
Question Corpus 
Corpus Information 
Acquisition 
Document 
processing 
 
Fig. 1.  System architecture 
Thirdly, the Web knowledge acquisition module acquires the definitions of the tar-
get term from the Web knowledge base. If we can find definitions from these sources 
(the Web KBs we used will be described in Section 3.1), we use them to rank the 
candidate sentences set.  
At last, the definition extraction module extracts the definition from the candidate 
sentence set based on the knowledge which is got from the Web knowledge base. 
In very few situations, no definitions can be found from the Web KBs, and the 
module named ?corpus information acquisition? is adopted to form the centroid of the 
 Answering Definition Questions Using Web Knowledge Bases 501 
candidate sentence set. We rank candidate sentences based on this centroid. The sen-
tences that have high similarity with this centroid are extracted as the answers to the 
question. The assumption is that words co-occurring frequently with the target in the 
corpus are more important ones for answering the question.  
The system architecture is illustrated in Fig.1. 
In this paper, we focus on how to make use of the Web KBs in extracting definition 
sentences, so we will describe the detail of the definition extraction module below. 
3   Definition Extraction Based on Web Knowledge Bases 
3.1   Web Knowledge Base 
There are lots of specific websites on the Web, such as online biography dictionaries 
or online cyclopaedias. We can get biography of a person, the profile of an organiza-
tion or the definition of a generic term from them. We call this kind of website Web 
knowledge base (KB). The definitions from them often supply knowledge that can be 
exploited directly. So we answer definition questions by utilizing the existing defini-
tions in the Web knowledge bases. The results of our system reveal that the Web 
knowledge bases are quite helpful to answering definition questions. 
Usually, different knowledge bases may pay attention to different kind of concept, 
and they may have different kind of entries. For example, the biography dictionary 
(www.s9.com) is a dictionary that covers widely on biography of people, and other 
KBs may pay attention to other kinds of concept. We choose several authoritative 
KBs that cover different kinds of concept to achieve our goal.  
The Web knowledge bases we used are the Encyclopedia(www.encyclopedia.com), 
the Wikipedia(www.wikipedia.com), the Merriam-Webster dictionary (www.mw. 
com), the WordNetglossaries (www.cogsci.princeton.edu/cgi-bin/webwn) and a biog-
raphies dictionary (www.s9.com). 
0
0. 2
0. 4
0. 6
0. 8
1
TREC2003 TREC2004
pe
rc
en
t
ency
wi ki
mw
wn
s9
al l
 
Fig. 2. The Web KBs? coverage of TREC data 
These Web KBs can cover most of the target terms, and the definitions in them are 
exact and concise.This can be confirmed from the experiment on TREC?s data set. 
Fig.2 gives our experiment results on the TREC 2003 and TREC 2004?s definition 
question sets, which have 50 and 65 target terms respectively. The ?ency?, ?wiki?, 
502 Z. Zhang et al 
?mw?, ?wn? and ?s9? stand for the five online KBs we have used. Each column repre-
sents the percent of the target terms that can be found in the corresponding online 
knowledge base. The column marked ?all? represents the percent of the target terms 
that can be found in at least one of these five online knowledge bases. 
It is easy to see that a high coverage can be got by using these Web knowledge 
bases. In the Section 3.2 and Section 3.3 we will show how to use these KBs, and in 
Section 4 we can see that it boosts the performance of the system significantly. 
3.2   Definition Extraction Based on GDS 
As mentioned above, we may get most of the submitted target terms? definitions by 
utilizing multiple Web KBs. One target may find its definitions in more than one 
knowledge base. Are all of them useful? The experimental data tells that, the different 
definitions belonging to one target differ from each other in some degree. They are 
short or long, concise or detailed. 
Considering the above factor, we try to utilize all of the definitions from different 
Web KBs to accomplish our task. For one target term, the definitions from all Web 
knowledge bases compose its ?general definition set?, which is abbreviated to GDS. 
Each element of this set is a definition from one Web knowledge base, so the number 
of the elements in this set is the same as the number of the Web KBs we used. When 
we cannot find its entry in a certain Web KB, its corresponding element will be an 
empty string.  
For each target, its candidate sentence set is expressed as SA = {A1, A2,?, Am}, 
where Ak (k=1..m) is a candidate sentence in the set and m is the total number of the 
candidate sentences.  
GDS is expressed as SGD = {D1, D2 ,..., Dn}, where Dk (k=1..n) is the definition of 
the target from the kth knowledge base, and n is the number of the knowledge bases. 
Dk may be an empty string when the target has no definition in the knowledge base k. 
In this algorithm, we rank the candidate sentences set SA = {A1, A2,?, Am} using 
SGD. 
Let Sij be the similarity of Ai and Dj. The similarity is the tf.idf score, where the 
candidate sentence Ai and the definition Dj are all treated as a bag of words. The tf.idf 
function we used is described in [5]. 
For each candidate sentence Ai in the set SA, we calculate its score based on the 
GDS as follows: 
ij
n
j
ji Swscore ?
=
=
1
 (?
=
=
n
j
jw
1
1 ) . (1) 
The weights jw are fixed based on experiment, considering the authoritativeness of 
the knowledge base from which Dj comes. The sentences of set SA are ranked based 
on this score, and the top ones are chosen as the definition of the target term. 
3.3   Definition Extraction Based on EDS 
As we have seen, for a target term, different definitions in its ?general definition set? 
may overlap in some degree. We intent to modify this set by merging its elements into 
 Answering Definition Questions Using Web Knowledge Bases 503 
one concise definition. We extract the essential information from the ?general defini-
tion set? to form the ?essential definition set?, which is abbreviated to EDS.  
EDS is expressed as SED = {d1, d2 ,?, dl}, where each element dk (k=1..l) is an es-
sential definition sentence about the target, and l is the number of the essential defini-
tion sentences. We hope that each element can tell one important aspect of the target 
term, and the whole ?essential definition set? may contain as much information as 
GDS but no redundant information. 
We try to use an automatic text summary technique [11] to get EDS. This tech-
nique is based on sentence?s weight and similarity between sentences. Firstly, calcu-
late the weights of all sentences and similarities between any two sentences, and then 
extract sentence based on these weights. After one sentence has been extracted, calcu-
late the new weights of the remained sentences based on their similarities. Iterate the 
above procedure until the extracted sentences reach the required length. More detail 
of this technique can be found in [11]. In this section we will try to use the ?essential 
definition set? to extract definitions from the candidate sentence set. 
1. Initially set the result set A={}, and i=1. 
2. For the element di in the set SED:  
First get the similarity between di and Aj (j=1..m), which is ex-
pressed as Sij.  
Then let },...,,max{ 21 imiiik SSSS = . If Sik>minsim , then add Ak 
to the set A and delete Ak from the set SA . 
3. If lengthAL
m
k
k max_)(
1
>??
=
 or i equals to l, the algorithm ends; 
otherwise, i = i +1, go to step2. 
Fig. 3.   Definition extraction using EDS 
The algorithm was showed in Fig.3. The candidate sentence set is also expressed as 
SA = {A1, A2,?, Am}, where Ak (k=1..m) is a candidate sentence in the set and m is 
the total number of the candidate sentences. The similarity Sij is calculated as the 
same as in Section 3.2. ( )kAL  represents the length of string Ak in character and m? is 
the number of elements in set A. The parameters max_length and minsim were em-
pirically set based on TREC?s definition question set. The last result is set A, where 
A={A1, A2,?, Am?}. 
4   Evaluation 
In order to get comparable evaluation, we apply our approach to TREC2004 defini-
tion QA task. We can see that our approach is an effective one compared with peer 
systems in this competitive evaluation. 
In this section we present the evaluation criterion and system performance on 
TREC task, and discuss the effectiveness of our approach. 
504 Z. Zhang et al 
4.1   Evaluation Criterion  
The TREC evaluation criterion [3] is summarized here for the purpose of discussing 
the evaluation results.  
For an individual definition question, there is a list of essential nuggets and accept-
able nuggets provided by TREC. These given nuggets are used to score the definition 
generated by the system. 
An individual definition question will be scored using nugget recall (R) and an ap-
proximation to nugget precision (P) based on length. In particular, 
R = # essential nuggets returned in response/# essential nuggets 
P is defined as: if    length < allowance,  P = 1 
else    P=1-[(length-allowance)/length] 
where    allowance = 100*(# essential+acceptable nuggets returned) 
length = total # non-white-space characters in answer strings 
The F measure is:  
RP?
PR?
+
)1+(
=F 2
2
 . (2) 
where ? value is fixed three in TREC 2004, and we also use three to get comparable 
result. 
The score of a system is the arithmetic mean of F-measure scores of all the defini-
tion questions output by the system. 
4.2   Effectiveness of Web Knowledge Bases 
To compare the effectiveness of the Web knowledge bases, we experimented on the 
TREC 2004 definition question set. The result can be seen in Table1. 
Table 1 shows the F-measure scores of our two algorithms and the baseline 
method. It also shows the median of the scores of all participating systems in TREC 
2004. The baseline method is: for an input question, form the candidate sentence set 
by using the approach described in Section 2. Then put the sentence of this set into the 
answer set one by one until all the sentences in the candidate sentence set are consid-
ered or the answer length is greater than a pre-fixed length (we set the length 3000 
characters in our experiment). 
We can see that our two algorithms all outperform the median and the baseline 
method which does not use Web knowledge bases. In conclusion, the Web knowledge 
bases are effective resources to definition question answering. 
Table 1. The F- measure score of the baseline method, the median system in TREC2004, and 
our two methods on TREC 2004 data set 
 Baseline 
method 
Median Ranking 
using  GDS 
Ranking 
using EDS 
F-measure 
score (?=3) 
0.231 0.184 0.404 0.367 
 Answering Definition Questions Using Web Knowledge Bases 505 
4.3   Definition Extraction Based on GDS vs. Based on EDS 
As we have mentioned, we have tried two algorithms in the definition extraction mod-
ule, which are based on GDS and EDS respectively. The performance of these algo-
rithms is shown in Table 2. 
Table 2. Performance of our three runs on the three types of quesitions and on the whole 64 
questions of TREC 2004 
 Num Q Run A Run B Run C 
all 64 0.404 0.389 0.367 
PERSON 23 0.366 0.372 0.404 
ORG 25 0.413 0.389 0.326 
THING 16 0.446 0.415 0.379 
We have submitted three runs in TREC2004, which were generated by using dif-
ferent algorithm in the definition extraction module. Run A and run B were generated 
by using GDS with slightly different weights in formula (1), and run C was generated 
by using EDS. All the 64 questions are divided into three classes based on the entity 
types of the targets, which are person, organization and other thing. Table 2 shows the 
three runs? F-measure scores on these three types and their overall score on the whole 
64 questions. 
Two algorithms? F-measure scores are all among the best of total 63 runs. Run C?s 
score on the ?PERSON?, 0.404 is the highest of our three runs on this type. Run A does 
better on the types named ?ORG? and ?THING?. We can say that these two algorithms 
contribute to different kinds of target terms. Dividing definition questions into different 
subclass and processing them with different methods could be a proper direction. 
Considering the score on all the 64 questions, the former algorithm is slightly 
higher than the latter one. However, the result of the latter one is also encouraging. 
Since the ?essential definition set? contain the important information and less redun-
dancy, it has the potential to get the answers, which are not only concise but also have 
wide coverage about the target. We believe it is an appropriate way to extract the high 
quality definitions. A preliminary analysis shows that the major problem is how to 
improve the quality and the coverage of the essential definition set. We believe that 
the performance could be boosted through improving this technique. 
In conclusion, we can say that our methods can make better use of the external 
knowledge in answering definition question. 
5   Conclusions 
This paper proposes a definition QA approach, which makes use of Web knowledge 
bases and several complementary technology components. The experiments reveal that 
the Web knowledge bases are effective resources to definition question answering, and 
the presented method gives an appropriate framework for answering this kind of ques-
tion. Our approach has achieved an encouraging performance with the F-measure score 
of 0.404, which is ranked second among all the submitted runs in TREC2004. 
506 Z. Zhang et al 
Since definitional patterns can not only filter out those statistically highly-ranked 
sentences that are not definitional, but also bring those definition sentences that are 
written in certain styles for definitions but are not statistically significant into the 
answer set. [6] In the future work, we will employ some pattern matching methods to 
reinforce our existing method. 
Acknowledgements 
This research was partly supported by NSF (Beijing, China) under contract of 
60435020, and Key Project of Science and Technology Committee of Shanghai under 
contract of 035115028. 
References 
1. Abdessamad Echihabi, Ulf Hermjakob, Eduard Hovy: Multiple-Engine Question Answer-
ing in TextMap. In Proceedings of the Twelfth Text REtreival Conference. NIST, Gath-
ersburg, MD (2003) 772?781 
2. Ellen M. Voorhees: Overview of the TREC 2001 question answering track. In Proceedings 
of the Tenth Text REtreival Conference. NIST, Gathersburg, MD (2001) 42?51 
3. Ellen M. Voorhees: Overview of the TREC 2003 Question Answering Track. In Proceed-
ings of the Twelfth Text REtreival Conference. NIST, Gathersburg, MD (2003) 54?68 
4. Ellen M. Voorhees: Evaluating answers to definition questions. In Proceedings of the 2003 
Human Language Technology Conference of the North American Chapter of the Associa-
tion for Computational Linguistics (2003) Volume 2: 109?111 
5. G.Salton, C. Buckley: Term weighting approaches in automatic text retrieval. Information 
Processing and Management (1988) 24(5): 513?523 
6. Hang Cui, Min-Yen Kan, Tat-Seng Chua, Jing Xiao: A comparative Study o Sentence Re-
trieval for Definitional Question Answering. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference (2004) 
7. Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, Min-Yen kan: National University of 
Singapore the TREC-13 Question Answering Main Task. In Proceedings of the Thirteenth 
Text REtreival Conference. NIST, Gathersburg, MD (2004) 
8. J. M. Prager, Jennifer Chu-Carroll, Krzysztof Czuba, Christopher Welty, Abraham It-
tycheiach, Ruchi Mahindru: IBM?s PIQUANT in TREC2003. In Proceedings of the 
Twelfth Text REtreival Conference. NIST, Gathersburg, MD (2003) 283?292 
9. Jinxi Xu, Ana Licuanan, Ralph Weischedel: TREC2003 QA at BBN: Answering defini-
tional Questions. In Proceedings of the Twelfth Text REtreival Conference. NIST, Gath-
ersburg, MD (2003) 98~106 
10. Milen Kouylekov, Bernardo Magnini, Matteo Negri, Hristo Tanev: ITC-irst at TREC-
2003: the DIOGENE QA system. In Proceedings of the Twelfth Text REtreival Confer-
ence. NIST, Gathersburg, MD (2003) 349?357 
11. Qi Zhang, Xuanjing Huang, Lide Wu: A New Method for Calculating Similarity between 
Sentences and Application on Automatic Text Summarization. In Proceedings of the first 
National Conference on Information Retrieval and Content Security (2004) 
12. Sasha Blair-Goldensohn, Kathleen R. McKeown, Andrew Hazen Schlaikjer: A hybrid ap-
proach for QA track definitional questions. In Proceedings of the Twelfth Text REtreival 
Conference. NIST, Gathersburg, MD (2003) 185?192 
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165?168,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Hierarchical Multi-Class Text Categorization
with Global Margin Maximization
Xipeng Qiu
School of Computer Science
Fudan University
xpqiu@fudan.edu.cn
Wenjun Gao
School of Computer Science
Fudan University
wjgao616@gmail.com
Xuanjing Huang
School of Computer Science
Fudan University
xjhuang@fudan.edu.cn
Abstract
Text categorization is a crucial and well-
proven method for organizing the collec-
tion of large scale documents. In this pa-
per, we propose a hierarchical multi-class
text categorization method with global
margin maximization. We not only max-
imize the margins among leaf categories,
but also maximize the margins among
their ancestors. Experiments show that the
performance of our algorithm is competi-
tive with the recently proposed hierarchi-
cal multi-class classification algorithms.
1 Introduction
In the past serval years, hierarchical text catego-
rization has become an active research topic in
database area (Koller and Sahami, 1997; Weigend
et al, 1999) and machine learning area (Rousu et
al., 2006; Cai and Hofmann, 2007).
Hierarchical categorization methods can be di-
vided in two types: local and global approaches
(Wang et al, 1999; Sun and Lim, 2001). A lo-
cal approach usually proceeds in a top-down fash-
ion, which firstly picks the most relevant cate-
gories of the top level and then recursively making
the choice among the low-level categories. The
global approach builds only one classifier to dis-
criminate all categories in a hierarchy. Due that the
global hierarchical categorization can avoid the
drawbacks about those high-level irrecoverable er-
ror, it is more popular in the machine learning do-
main.
The essential idea behind global approach is
that the close classes(nodes) have some common
underlying factors. Especially, the descendant
classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997). A key problem for global hi-
erarchical categorization is how to combine these
underlying factors.
In this paper, we propose an method for hierar-
chical multi-class text categorization with global
margin maximization. We emphasize that it is im-
portant to separate all the nodes of the correct path
in the class hierarchy from their sibling node, then
we incorporate such information into the formula-
tion of hierarchical support vector machine.
The rest of the paper is organized as follows.
Section 2 describes the basic model of multi-class
hierarchical categorization with maximizing mar-
gin. Then we propose our improved versions in
section 3. Section 4 gives the experimental analy-
sis. Section 5 concludes the paper.
2 Hierarchical Multi-Class Text
Categorization
Multiclass SVM can be generalized to the problem
of hierarchical categorization (Cai and Hofmann,
2007), which has more than two categories in most
of the case. Denote Y
i
as the multilabels of x
i
and
?
Y
i
the multilabels set not in Y
i
. The separation
margin of w, with respect to x
i
, can be approxi-
mated as:
?
i
(w) = min
y?Y
i
,?y?
?
Y
i
??(x
i
,y)? ?(x
i
,
?
y),w? (1)
The loss function can be accommodated to
multi-class SVM to scale the penalties for margin
violations proportional to the loss. This is moti-
vated by the fact that margin violations involving
an incorrect class with high loss should be penal-
ized more severely. So the cost-sensitive hierar-
chical multiclass formulation takes takes the fol-
lowing form:
min
w,?
1
2
||w||
2
+ C
n
?
i=1
?
i
(2)
s.t.?w,??
i
(y,
?
y)??1?
?
i
l(y,?y)
, (?i,y?Y
i
,
?
y?
?
Y
i
)
?
i
? 0(?i)
165
where ??
i
(y,
?
y) = ?(x
i
,y) ? ?(x
i
,
?
y),
l(y,
?
y) > 0 and ?(x,y) is the joint feature of in-
put x and output y, which can be represented as:
?(x,y) = ?(y)? ?(x) (3)
where ? is the tensor product. ?(y) is the feature
representation of y.
Thus, we can classify a document x to label y
?
:
y
?
= arg max
y
F (w,?(x,y)) (4)
where F (?) is a map function.
There are different kinds of loss functions
l(y,
?
y).
One is thezero-one loss, l
0/1
(y,u) = [y 6= u].
Another is specially designed for the hierarchy
is tree loss(Dekel et al, 2004). Tree loss is defined
as the length of the path between two multilabels
with positive microlabels,
l
tr
= |path(i : y
i
= 1, j : u
j
= 1)| (5)
(Rousu et al, 2006) proposed a simplified ver-
sion of l
H
, namely l
?
H
:
l
?
H
=
?
j
c
j
[y
j
6= u
j
&y
pa
(j) = u
pa(j)
], (6)
that penalizes a mistake in a child only if the label
of the parent was correct. There are some different
choices for setting c
j
. One naive idea is to use
a uniform weighting (c
j
= 1). Another possible
choice is to divide the loss among the sibling:
c
root
= 1, c
j
= c
Parent(j)
/(|Sib(j)|+ 1) (7)
Another possible choice is to scale the loss by the
proportion of the hierarchy that is in the subtree
T (j) rooted by j:
c
j
= |T (j)|/|T (root)| (8)
Using these scaling weights, the derived losses are
referred as l
?
uni
,l
?
sib
and l
?
sub
respectively.
3 Hierarchical Multi-Class Text
Categorization with Global Margin
Maximization
In previous literature (Cai and Hofmann, 2004;
Tsochantaridis et al, 2005), they focused on sep-
arating the correct path from those incorrect path.
Inspired by the example in Figure 1, we emphasize
it is also important to separate the ancestor node in
the correct path from their sibling node.
The vector w can be decomposed in to the set
of w
i
for each node (category) in the hierarchy. In
Figure 1, the example hierarchy has 7 nodes and 4
of them are leaf nodes. The category is encode
as an integer, 1, . . . , 7. Suppose that the train-
ing pattern x belongs to category 4. Both w in
the Figure 1a and Figure 1b can successfully clas-
sify x into category 4, since F (w,?(x,y
4
)) =
?
1,2,4
?w
i
,x? is the maximal among all the possi-
ble discriminate functions. So both learned param-
eter w is acceptable in current hierarchical support
vector machine.
Here we claim the w in Figure 1b is better than the
w in Figure 1a. Since we notice in Figure 1a, the
discriminate function ?w
2
,x? is smaller than the
discriminate function ?w
3
,x?. The discriminate
function ?w
i
,x? measures the similarity of x to
category i. The larger the discriminate function is,
the more similar x is to category i. Since category
2 is in the path from the root to the correct cate-
gory and category 3 is not, intuitively, x should be
closer to category 2 than category 3. But the dis-
criminate function in Figure 1a is contradictive to
this assumption. But such information is reflected
correctly in Figure 1b. So we conclude w in Fig.
1b is superior to w in 1a.
Here we propose a novel formulation to incor-
porate such information. Denote A
i
as the mul-
tilabel in Y
i
that corresponds to the nonleaf cate-
gories and Sib(z) denotes the sibling nodes of z,
that is the set of nodes that have the same parent
with z, except z itself. Implementing the above
idea, we can get the following formulation:
min
w,?,?
1
2
?w?
2
+ C
1
?
i
?
i
+ C
2
?
i
?
i
(9)
s.t.?w, ??
i
(y,
?
y)? ? 1?
?
i
l(y,
?
y)
, (?i,
y ? Y
i
?
y ?
?
Y
i
)
?w, ??
i
(z,
?
z)? ? 1?
?
i
l(z,
?
z)
, (?i,
z ? A(i)
?
z ? Sib(z)
)
?
i
? 0(?i)
?
i
? 0(?i)
It arrives at the following Lagrangian:
L(w, ?
1
, ..., ?
n
, ?
1
, ..., ?
n
)
=
1
2
?w?
2
+ C
1
X
i
?
i
+ C
2
X
i
?
i
?
X
i
X
y?Y
i
?y?
?
Y
i
?
iy?y
(?w, ??
i
(y,
?
y)? ? 1 +
?
i
l(y,
?
y)
)
166
1
2 3
4 5 6 7
10,1  xw
3,2  xw 5,3  xw
5,4  xw 1,7  xw1,5  xw 2,6  xw
1
2 3
4 5 6 7
10,1  xw
7,2  xw 3,3  xw
5,4  xw 1,7  xw1,5  xw 2,6  xw
a) b)
Figure 1: Two different discriminant function in a hierarchy
?
X
i
X
z?A
i
?z?Sib(z)
?
iz?z
(?w, ??
i
(z,
?
z)? ? 1 +
?
i
l(z,
?
z)
)
?
X
i
c
i
?
i
?
X
i
d
i
?
i
(10)
The dual QP becomes
max
?
?(?) =
?
i
?
y?Y
i
?y?
?
Y
i
?
iy
?
y
+
?
i
?
z?A
i
?z?Sib(z)
?
iz
?
z
?
1
2
?
i,j
?
y?Y
i
?y?
?
Y
i
?
r?Y
j
?r?
?
Y
j
?
1
i,j,y,
?
y,r,
?
r
(11)
?
1
2
?
i,j
?
z?A
i
?z?Sib(z)
?
k?A
j
?
k?Sib(k)
?
2
i,j,z,
?
z,k,
?
k
,
s.t.?
iy
?
y
? 0, (12)
?
jz
?
z
? 0, (13)
?
y?Y
i
?y?
?
Y
i
?
iy
?
y
l(y,
?
y)
? C
1
, (14)
?
z?A
i
?z?Sib(z)
?
iz
?
z
l(z,
?
z)
? C
2
, (15)
where ?
1
i,j,y,
?
y,r,
?
r
=
?
iy
?
y
?
jr
?
r
???
i
(y,
?
y), ??
j
(r,
?
r)? and ?
2
i,j,z,
?
z,k,
?
k
=
?
iz
?
z
?
jk
?
k
???
i
(z,
?
z), ??
j
(k,
?
k)?.
3.1 Optimization Algorithm
The derived QP can be very large, since the num-
ber of ? and ? variables is up to O(n?2
N
), where
n is number of training pattern and N is the num-
ber of nodes in the hierarchy. But two properties
of the dual problem can be exploited to design a
much more efficient optimization.
First, the constraints in the dual problem Eq. 11
- Eq. 15 factorize over the instance index for both
?-variables and ?-variables. The constraints in
Eq. 14 do not couple ?-variables and ?-variables
together. Further, dual variables ?
iy
?
y
and ?
jy
?
?
y
?
belonging to different training instances i and j do
not join in a same constraints. This inspired an
optimization procedure which iteratively performs
subspace optimization over all dual variables ?
iy
?
y
belonging to the same training instance. This will
in general reduced to a much smaller QP, since
it freezes all ?
jy
?
y
with j 6= i and ?-variables at
their current values. This strategy can be applied
in solving ?-variables.
Secondly, the number of active constraints at the
solution is expected to be relatively small, since
only a small fraction of categories
?
y ?
?
Y
i
( or
?
y ? Sib(y) when y ? A
i
) will typically fail to
achieve the required margin. The expected sparse-
ness of the variable for the dual problem can be
exploited by employing a variable selection strat-
egy. Equivalently, this corresponds to a cutting
plane algorithm for the primal QP. Intuitively, we
will identify the most violated margin constraint
with index (i,y,
?
y) and then add the correspond-
ing variable to the optimization problem. This
means that we start with extremely sparse prob-
lems and only successively increase the number of
variables in the active set. This general approach
to deal with large linear or quadratic optimization
problems is also known as column selection. In
practice, it is often not necessary to optimize until
final convergence, which adds to the attractiveness
of this approach.
We have used the LOQO optimization package
(Vanderbei, 1999) in our experiments.
4 Experiment
We evaluate our proposed model on the section D
in the WIPO-alpha collection
1
, which consists of
the 1372 training and 358 testing document. The
1
World Intellectual Property Organization (WIPO)
167
Table 1: Prediction losses (%) obtained on WIPO.
The values per column is calculated with the dif-
ferent loss function.
X
X
X
X
X
X
X
Train
Test
l
0/1
l
?
l
tr
l
uni
l
sib
l
sub
HSVM 48.6 188.8 94.4 97.2 5.4 7.5
l
0/1
HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4
HSVM 49.7 187.7 93.9 99.4 5.0 7.1
l
?
HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9
HM3 70.9 167.0 - 89.1 5.0 7.0
HSVM 49.4 186.0 93.0 98.9 5.0 7.5
l
tr
HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1
HSVM 47.2 181.0 90.5 94.4 5.0 7.0
l
?
uni
HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9
HM3 70.1 172.1 - 88.8 5.2 7.4
HSVM 49.4 184.9 92.5 98.9 4.8 7.4
l
?
sib
HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4
HM3 64.8 172.9 - 92.7 4.8 7.1
HSVM 50.6 189.9 95.0 101.1 5.2 7.5
l
?
sub
HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6
HM3 65.0 170.9 - 91.9 4.8 7.2
number of nodes in the hierarchy is 188, with max-
imum depth 3.
We compared the performance of our proposed
method HSVM-S with two algorithms: HSVM(Cai
and Hofmann, 2007) and HM3(Rousu et al, 2006).
4.1 Effect of Different Loss Function
We compare the methods based on different loss
functions, l
0/1
, l
?
, l
tr
, l
u?ni
, l
s?ib
and l
s?ub
. The per-
formances for three algorithms can be seen in Ta-
ble 1. Those empty cells, denoted by ?-?, are not
available in (Rousu et al, 2006).
As expected, l
0/1
is inferior to other hierarchi-
cal losses by getting poorest performance in all the
testing losses, since it can not take into account the
hierarchical information between categories. The
results suggests that training with a hierarchical
losses function, like l
s?ib
or l
u?ni
, would lead to a
better reduced l
0/1
on the test set as well as in
terms of the hierarchical loss. In Table 1, we can
also point out that when training with the same
hierarchical loss, the performance of HSVM-S is
better than HSVM under the measure of most hier-
archical losses, since HSVM-S includes more hier-
archical information,the relationship between the
sibling categories, than HSVM which only separate
the leave categories.
5 Conclusion
In this paper we present a hierarchical multi-class
document categorization, which focus on maxi-
mize the margin of the classes at the different
levels in the class hierarchy. In future work, we
plan to extend the proposed hierarchical learning
method to the case where the hierarchy is a DAG
instead of tree and scale up the method further.
Acknowledgments
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302.
References
L. Cai and T Hofmann. 2004. Hierarchical docu-
ment categorization with support vector machines.
In Proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine
Learning, 28(1):41?75.
Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004.
Large margin hierarchical classification. In Pro-
ceedings of the 21 st International Conference on
Machine Learning.
D. Koller and M Sahami. 1997. Hierarchically classi-
fying documents using very few words. In Proceed-
ings of the International Conference on Machine
Learning (ICML).
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning
of hierarchical multilabel classification models. In
Journal of Machine Learning Research.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining (ICDM).
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning.
R. J. Vanderbei. 1999. Loqo: An interior point code
for quadratic programming. In Optimization Meth-
ods and Software.
K. Wang, S. Zhou, and S Liew. 1999. Building hier-
archical classifiers using class proximities. In Pro-
ceedings of the International Conference on Very
Large Data Bases (VLDB).
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
168
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 203?212, Dublin, Ireland, August 23-29 2014.
Time-aware Personalized Hashtag Recommendation on Social Media
Qi Zhang, Yeyun Gong, Xuyang Sun, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qz, 12110240006, 13210240106, xjhuang}@fudan.edu.cn
Abstract
The task of recommending hashtags for microblogs has been received considerable attention in
recent years, and many applications can reap enormous benefits from it. Various approaches have
been proposed to study the problem from different aspects. However, the impacts of temporal and
personal factors have rarely been considered in the existing methods. In this paper, we propose a
novel method that extends the translation based model and incorporates the temporal and personal
factors. To overcome the limitation of only being able to recommend hashtags that exist in the
training data of the existing methods, the proposed method also incorporates extraction strategies
into it. The results of experiments on the data collected from real world microblogging services
by crawling demonstrate that the proposed method outperforms state-of-the-art methods that do
not consider these aspects. The relative improvement of the proposed method over the method
without considering these aspects is around 47.8% in F1-score.
1 Introduction
Over the past few years, social media services have become one of the most important communication
channels for people. According to the statistic reported by the Pew Research Center?s Internet &
American Life Project in Aug 5, 2013, about 72% of adult internet users are also members of at least
one social networking site. Hence, microblogs have also been widely used as data sources for public
opinion analyses (Bermingham and Smeaton, 2010; Jiang et al., 2011), prediction (Asur and Huberman,
2010; Bollen et al., 2011), reputation management (Pang and Lee, 2008; Otsuka et al., 2012), and many
other applications (Sakaki et al., 2010; Becker et al., 2010; Guy et al., 2010; Guy et al., 2013). In
addition to the limited number of characters in the content, microblogs also contain a form of metadata
tag (hashtag), which is a string of characters preceded by the symbol (#). Hashtags are used to mark the
keywords or topics of a microblog. They can occur anywhere in a microblog, at the beginning, middle, or
end. Hashtags have been proven to be useful for many applications, including microblog retrieval (Efron,
2010), query expansion (A.Bandyopadhyay et al., 2011), sentiment analysis (Davidov et al., 2010; Wang
et al., 2011). However, only a few microblogs contain hashtags provided by their authors. Hence, the
task of recommending hashtags for microblogs has become an important research topic and has received
considerable attention in recent years.
Existing works have studied discriminative models (Ohkura et al., 2006; Heymann et al., 2008) and
generative models (Krestel et al., 2009; Blei and Jordan, 2003; Ding et al., 2013) based on textual
information from a single microblog. However, from a dataset containing 282.2 million microblogs
crawled from Sina Weibo
1
, we observe that different users may have different perspectives when picking
hashtags, and the perspectives of users are impacted by their own interests or the global topic trend.
Meanwhile,the global topic distribution is likely to change over time. To better understand how the
topics vary over time, we aggregate the microblog posts published in a month as a document. Then, we
use a Latent Dirichlet Allocation (LDA) to estimate their topics. Figure 1 illustrates an example, where
ten active topics are selected. We can observe that the topics distribution varies greatly over time.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.weibo.com. It is one of the most popular microblog services in China.
203
2012-04 2012-06 2012-08 2012-10 2012-12 2013-02 2013-04
0
200
400
600
800
1000
1200
pay 
official 
staff 
support 
ministry 
statistics 
tomorrow
reproduce 
research
financial 
network 
Japanese 
culture 
together 
yourself life
incentive street
Pisces AriesLeo Horoscope
Pluto
iphoneAppledesign
food 
water
Taurus
Venus
charges
 life
universities 
education
husband
  own
  women
  home
  
successson like
saint 
sunday
employees
film 
loyalty husbandchildren
parent
happyness
like
egg  pumpkin
Figure 1: An example of the topics of retweets in each month. Each colored stripe represents a topic,
whose height is the number of words assigned to the topic. For each topic, the top words of this topic in
each month are placed on the stripe.
Motivated by the methods proposed to handle the vocabulary gap problem for keyphrase extrac-
tion (Liu et al., 2012) and hashtag suggestion (Ding et al., 2013), in this work, we also assume that
the hashtags and textual content in a microblog are parallel descriptions of the same thing in different
languages. To model the document themes, in this paper, we adopt the topical translation model to
facilitate the translation process. Topic-specific word triggers are used to bridge the gap between the
words and hashtags. Since existing topical translation models can only recommend hashtags learned
from the training data, we also incorporate an extraction process into the model.
This work makes three main contributions. First, we incorporate temporal and personal factors into
considerations. Most of the existing works on hashtag recommendation tasks have focused on textual
information. Second, we adopt a topical translation model to combine extraction and translation methods.
This makes it possible to suggest hashtags that are not included in the training data. Third, to evaluate
the task, we construct a large collection of microblogs from a real microblogging service. All of the
microblogs in the collection contain textual content and hashtags labeled by their authors. This can
benefit other researchers investigating the same task or other topics using author-centered data.
The remaining part of this paper is structured as follows: We briefly review existing methods in
related domains in Section 2. Section 3 gives an overview of the proposed generation model. Section
4 introduces the dataset construction, experimental results and analyses. In Section 5, we will conclude
the paper.
2 Related Works
Due to the usefulness of tag recommendation, many methods have been proposed from different
perspectives (Heymann et al., 2008; Krestel et al., 2009; Rendle et al., 2009; Liu et al., 2012; Ding et al.,
2013). Heymann et al. (Heymann et al., 2008) investigated the tag recommendation problem using the
data collected from social bookmarking system. They introduced an entropy-based metric to capture the
generality of a particular tag. In (Song et al., 2008), a Poisson Mixture Model based method is introduced
to achieve the tag recommendation task. Krestel et al. (Krestel et al., 2009) introduced a Latent Dirichlet
Allocation to elicit a shared topical structure from the collaborative tagging effort of multiple users for
recommending tags. Based on the the observation that similar webpages tend to have the same tags, Lu et
al. proposed a method taking both tag information and page content into account to achieve the task (Lu
et al., 2009). Ding et al. proposed to use translation process to model this task (Ding et al., 2013). They
extended the translation based method and introduced a topic-specific translation model to process the
various meanings of words in different topics. In (Tariq et al., 2013), discriminative-term-weights were
used to establish topic-term relationships, of which users? perception were learned to suggest suitable
hashtags for users. To handle the vocabulary problem in keyphrase extraction task, Liu et al. proposed a
204
topical word trigger model, which treated the keyphrase extraction problem as a translation process with
latent topics (Liu et al., 2012).
Most of the works mentioned above are based on textual information. Besides these methods,
personalized methods for different recommendation tasks have also been paid lots of attentions (Liang
et al., 2007; Shepitsen et al., 2008; Garg and Weber, 2008; Li et al., 2010; Liang et al., 2010; Rendle and
Schmidt-Thieme, 2010). Shepitsen et al. (2008) proposed to use hierarchical agglomerative clustering
to take into account personalized navigation context in cluster selection. In (Garg and Weber, 2008),
the problem of personalized, interactive tag recommendation was also studied based on the statics of the
tags co-occurrence. Liang et al. (2010) proposed to the multiple relationships among users, items and
tags to find the semantic meaning of each tag for each user individually and used this information for
personalized item recommendation.
From the brief descriptions given above, we can observe that most of the previous works on hashtag
suggestion focused on textual information. In this work, we propose to incorporate temporal and personal
information into the generative methods. Further more, to over the limitation that translation based
method can only recommend hashtags learned from the training data, we also propose to incorporate an
extraction process into the model.
3 The Proposed Methods
In this section, we firstly introduce the notation and generation process of the proposed method. Then,
we describe the method used for learning parameters. Finally, we present the methods of how do we
apply the learned model to achieve the hashtag recommendation task.
3.1 The Generation Process
We use D to represent the number of microblogs in the given corpus, and the microblogs have been
divided into T epoches. Let t = 1, 2, ..., T be the index of an epoches, ?
t
is the topic distribution of the
epoch t. Each microblog is generated by a user u
i
, where u
i
is an index between 1 and U , and U is the
total number of users. A microblog is a sequence of N
d
words denoted by w
d
= {w
d1
, w
d2
, ..., w
dN
d
}.
Each microblog contains a set of hashtags denoted by h
d
= {h
d1
, h
d2
, ..., h
dM
d
}. A word is defined as
an item from a vocabulary with W distinct words indexed by w = {w
1
, w
2
, ..., w
W
}. Each hashtag is
from the vocabulary with V distinct hashtags indexed by h = {h
1
, h
2
, ..., h
V
}. The notations in this
paper are summarized in Table 1.
The original LDA assumes that a document is contains a mixture of topics, which is represented by a
topic distribution, and each word has a hidden topic label. Although, it is sensible for long document,
due to the limitations of the length of characters in a single microblog, it tends to be about a single topic.
Hence, we associate a single hidden variable with each microblog to indicate its topic. Similar idea of
assigning a single topic to a short sequence of words has also been used for modeling Twitters (Zhao et
al., 2011)
The hashtag recommendation task is to discover a list of hashtags for each unlabeled microblog, In
our method, we first learn a topical translation model, and then we estimate the latent variables for each
microblog, finaly recommending hashtags accord to the learned model.
Fig. 2 shows the graphical representation of the generation process. The generative story for each
microblog is as follows:
3.2 Learning
To learn the parameters of our model, we use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to
sample the topics assignment z, latent variables assignment x and y.
Given the current state of all but the variable x
d
and z
d
for the dth microblog, we can jointly sample
205
1. Draw pi ? Beta(?), ? ? Beta(?)
2. Draw background word distribution ?
B
? Dirichlet(?
w
)
3. Draw global trendy topic distribution ?
t
? Dirichlet(?) for each time epoch t = 1, 2, ..., T
4. Draw personal topic distribution ?
u
? Dirichlet(?) for each user u = 1, 2, ..., U
5. Draw word distribution ?
z
? Dirichlet(?
w
) for each topic z = 1, 2, ...,K
6. Draw hashtag distribution ?
z,w
? Dirichilet(?
h
) for each topic z = 1, 2, ...,K and each word
w = 1, 2, ...,W
7. For each microblog d = 1, 2, ..., D
a. Draw x
d
? Bernoulli(?)
b. If x
d
= 0 then
Draw a topic z
d
?Multinomial(?
u
)
End if
If x
d
= 1 then
Draw a topic z
d
?Multinomial(?
t
)
End if
c. For each word n = 1, ..., N
d
i. Draw y
dn
? Bernoulli(pi)
ii. If y
dn
= 0 then
Draw a word w
dn
?Multinomial(?
B
)
End if
If y
dn
= 1 then
Draw a word w
dn
?Multinomial(?
z
d
)
End if
d. For each hashtag m = 1, ...,M
d
i. Draw h
dm
? P (h
dm
|w
d
, z
d
, ?
z
d
,w
d
)
w
dn
z
d
?
t
?
u
t
d
u
d
x
d
?
?
? ?
h
dm
y
dn
pi
?
?
z
?
B
?
w
?
w
?
z,w
?
h
T
M
d
N
d
D
K
U
W
K
Figure 2: The graphical representation of the proposed model. Shaded circles are observations or
constants. Unshaded ones are hidden variables.
206
Table 1: The notations used in this work.
D The number of training data set
W The number of unique word in the corpus
V The number of unique hashtag in the corpus
K The number of topics
T The total number of time epoches
U The total number of users
N
d
The number of words in the dth microblog
M
d
The number of hashtags in the dth microblog
z
d
The topic of the dth microblog
x
d
The latent variable decided the distribution category of z
d
y
dn
The latent variable decided the distribution category of w
dn
pi The distribution of latent variable y
dn
? The distribution of latent variable x
d
?
z
The distribution of topic words
?
B
The distribution of background words
?
t
The distribution of topics for time epoch t
?
u
The distribution of topics for user u
t
d
The time epoch for microblog d
u
d
The user of the microblog d
? The topic-specific word alignment table between word and hashtag or itself
x
d
and z
d
, the conditional probability of x
d
= p,z
d
= k is calculated as follows:
Pr(x
d
= p, z
d
= k|z
?d
,x
?d
,y,w,h)
?
N
?
p
+ ?
N
?
(.)
+ 2?
?
N
l
k
+ ?
N
l
(.)
+K?
?
N
d
?
n=1
N
k
w
dn
+ ?
w
N
k
(.)
+W?
w
?
M
d
?
m=1
N
d
?
n=1
M
w
dn
,h
dm
?d,k
+ ?
h
M
w
dn
,(.)
?d,k
+ V ?
h
,
(1)
where l = u
d
when p = 0 and l = t
d
when p = 1. N
?
0
is the number of microblog generated by personal
interests, while N
?
1
is the number of microblog coming from global topical trends, N
?
(.)
= N
?
0
+ N
?
1
.
N
u
d
k
is the number of microblogs generated by user u
d
and under topic k. N
u
d
(.)
is the total number of
microblogs generated by user u
d
. N
t
d
k
=
?
t
d
t
?
=1
e
?t
?
?
N
?
t?t
?
k
,N
?
t?t
?
k
is the number of microblogs assigned
to topic k at time epoch t ? t
?
, e
?t
?
?
is decay factory, and N
t
d
(.)
=
?
K
k=1
N
t
d
k
. N
k
w
dn
is the times of word
w
dn
assigned to topic k, N
k
(.)
is the times of all the word assigned to topic k, M
w
dn
,h
dm
?d,k
is the number of
occurrences that word w
dn
is translated to hashtag h
dm
given topic k. All the counters mentioned above
are calculated with the dth microblog excluded.
We sample y
dn
for each word w
dn
in the dth microblog using the following equation:
Pr(y
dn
= q|z,x,y
?dn
,w,h) ?
N
pi
q
+ ?
N
pi
(.)
+ 2?
?
N
l
w
dn
+ ?
w
N
l
(.)
+W?
w
,
(2)
where l = B when q = 0 and l = z
d
when q = 1. N
pi
0
is the number of words assigned to background
words and N
pi
1
is the number of words under any topic respectively. N
pi
(.)
= N
pi
0
+N
pi
1
, N
B
w
dn
is a count
of word w
dn
occurs as a background word. N
z
d
w
dn
is the number of word w
dn
is assigned to topic z
d
, and
N
z
d
(.)
is the total number of words assigned to topic z
d
. All counters are calculated with taking no account
of the current word w
dn
.
In many cases, hashtag dose not appear in the training data, to solve this problem, we assume that each
word in the microblog can translate to a hashtag in the training data or itself. We assume that each word
207
have aligned ? (we set ? = 1 in this paper after trying some number) times with itself under the specific
topic. After all the hidden variables become stable, we can estimate the alignment probability as follows:
?
h,w,z
=
?
?
?
N
h
z,w
+?
h
N
(.)
z,w
+?+(V+1)?
h
if h is a hashtag in the training data
?+?
h
N
(.)
z,w
+?+(V+1)?
h
if h is the word itself
(3)
where N
h
z,w
is the number of the hashtag h co-occurs with the word w under topic z in the microblogs.
For the probability alignment ? between hashtag and word, the potential size is W ? V ? K. The
data sparsity poses a more serious problem in estimating ? than the topic-free word alignment case.
To remedy the problem, we use interpolation smoothing technique for ?. In this paper, we emplogy
smoothing as follows:
?
?
h,w,z
= ??
h,w,z
+ (1? ?)P (h|w),
(4)
where ?
?
h,w,z
is the smoothed topical alignment probabilities, ?
h,w,z
is the original topical alignment
probabilities. P (h|w) is topic-free word alignment probability. Here we obtain P (h|w) by exploring
IBM model-1 (Brown et al., 1993). ? is trade-off of two probabilities ranging from 0.0 to 1.0. When
? = 0.0, ?
?
h,w,z
will be reduce to topic-free word alignment probability; and when ? = 1.0, there will be
no smoothing in ?
?
h,w,z
. For the word itself there are no smoothing, because it is a pseudo-count.
3.3 Hashtag Extraction
We perform hashtag extraction as follows. Suppose given an unlabeled dataset, we perform Gibbs
Sampling to iteratively estimate the topic and determine topic/background words for each microblog.
The process is the same as described in Section 3.2. After the hidden variables of topic/background
words and the topic of each microblog become stable, we can estimate the distribution of topics for the
dth microblog in unlabeled data by:?
?
dk
=
p(k)p(w
d1
|k)...p(w
dN
d
|k)
Z
where p(w
dn
|k) =
N
pi
1
+?
N
pi
(.)
+2?
?
N
k
w
dn
+?
w
N
k
(.)
+W?
w
and N
k
w
dn
is the number of words w
dn
that are assigned to topic k in the corpus, and p(k) =
N
?
0
+?
N
?
(.)
+2?
?
N
u
k
+?
N
u
(.)
+K?
+
N
?
1
+?
N
?
(.)
+2?
?
N
t
k
+?
N
t
(.)
+K?
is regarded as a prior for topic distribution, Z is the normalized
factor. With topic distribution ?
?
and topical alignment table ?
?
, we can rank hashtags for the dth
microblog in unlabeled data by computing the scores:
P (h
dm
|w
d
, ?
?
d
, ?
?
) ?
K
?
z
d
=1
N
d
?
n=1
P (h
dm
|z
d
, w
dn
, ?
?
) ? P (z
d
|?
?
d
) ? P (w
dn
|w
d
),
(5)
where h
dm
can be a hashtag in the training data or a word in the dth microblog, p(w
dn
|w
d
) is the weight
of the word w
dn
in the microblog, which can be estimated by the IDF score of the word. According to
the ranking scores, we can suggest the top-ranked hashtags for each microblog to users.
4 Experiments
In this section, we introduce the experimental results and the data collection we constructed for training
and evaluation. Firstly, we describe how do we construct the collection and statics of it. Then we
introduce the experiment configurations and baseline methods. Finally, the evaluation results and
analysis are given.
4.1 Data Collection
We use a dataset collected from Sina Weibo to evaluate the proposed approach and alternative methods.
We random select 166,864 microblogs from Aug. 2012 to June 2013. The unique number of hashtags
in the corpus is 17,516. We use the microblogs posted from Aug. 2012 to May 2013 as the training
data. The other microblogs are used for evaluation. The hashtags marked in the original microblogs are
considered as the golden standards.
208
Figure 3: Precision-recall curves of different
methods on this task.
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Prec
ision
Recall
TWTMTTMT-TTMU-TTMTU-TTMK-TTMTUK-TTM
Table 2: Evaluation results of different methods
on the evaluation collection.
Methods Precision Recall F
1
TWTM 0.231 0.202 0.215
SVM 0.418 0.366 0.390
TTM 0.319 0.279 0.297
T-TTM 0.338 0.301 0.319
U-TTM 0.341 0.307 0.323
K-TTM 0.386 0.337 0.360
TU-TTM 0.355 0.310 0.331
TUK-TTM 0.452 0.415 0.433
4.2 Experiment Configurations
We use precision (P ), recall (R), and F1-score (F
1
) to evaluate the performance. Precision is calculated
based on the percentage of ?hashtags truly assigned? among ?hashtags assigned by system?. Recall
is calculated based on the ?hashtags truly assigned? among ?hashtags manually assigned?. F1-score
is the harmonic mean of precision and recall. We do 500 iterations of Gibbs sampling to train the
model. For optimize the hyperparmeters of the proposed method and alternative methods, we use 5-fold
cross-validation in the training data to do it. The number of topics is set to 70. The other settings of
hyperparameters are as follows: ? = 50/K, ?
w
= 0.1, ?
h
= 0.1, ? = 0.01, and ? = 0.01. The
smoothing factor ? in Eq.(3) is set to 0.6. For estimating the translation probability without topical
information, we use GIZA++ 1.07 to do it (Och and Ney, 2003).
For baselines, we compare the proposed model with the following alternative models.
? TWTM: Topical word trigger model (TWTM) was proposed by Liu et al. for keyphrase extraction
using only textual information (Liu et al., 2012). We implemented the model and used it to achieve
the task.
? TTM: Ding et al. (2013) proposed the topical translation model (TTM) for hash tag extraction. We
implemented and extended their method for evaluating it on the corpus constructed in this work.
4.3 Experimental Results
Table 2 shows the comparisons of the proposed method with the state-of-the-art methods on the
constructed evaluation dataset. ?TUK-TTM? denotes the method proposed in this paper. ?T-TTM?
and ?U-TTM? represent the methods incorporating temporal and personal information respectively. ?K-
TTM? represents the method incorporating the extraction factor. From the results, we can observe that
the proposed method is significantly better than other methods at 5% significance level (two-sided).
Comparing to results of the TTM, we can observe that the temporal information, personal information
and extraction strategy can all benefit the task. Among the three additional factors, the extraction strategy
achieves the best result. The limitation of only being able to recommend hashtags that exist in the training
data can be overcome in some degree by the proposed method. The relative improvement of proposed
TUK-TTM over TTM is around 47.8% in F1-score.
Table 3 shows the comparisons of the proposed method with the method ?K-TTM? in two corpus NE-
Corpus and E-Corpus. NE-Corpus include microblogs whose hashtags are not contained in the training
data. E-Corpus include the microblogs whose hashtags appear in the training data. We can observe that
the proposed method significantly better than ?K-TTM? in the E-Corpus. Another observation is that
the method incorporating the extraction factor achieves better performances on the NE-Corpus than E-
Corpus. We think that the reason is that the fewer times hashtag appear, the greater weight it has. Hence,
we can extract this kind of hashtags more easier.
Figure 3 shows the precision-recall curves of TWTW, TTM, T-TTM, U-TTM, TU-TTM, K-TTM,
and TUK-TTM on the evaluation dataset. Each point of a precision-recall curve represents extracting
209
Table 3: Evaluation results of two different corpus.
Corpus Methods P R F
NE-Corpus
K-TTM 0.631 0.553 0.589
TUK-TTM 0.641 0.561 0.598
E-Corpus
K-TTM 0.172 0.162 0.167
TUK-TTM 0.288 0.271 0.279
Table 4: The influence of the number of topics
K of TUK-TTM.
K Precision Recall F
1
10 0.410 0.382 0.396
30 0.435 0.380 0.406
50 0.448 0.413 0.430
70 0.452 0.415 0.433
100 0.439 0.404 0.421
Table 5: The influence of the smoothing
parameter ? of TUK-TTM.
? Precision Recall F
1
0.0 0.379 0.354 0.366
0.2 0.405 0.372 0.388
0.4 0.433 0.398 0.415
0.6 0.452 0.415 0.433
0.8 0.426 0.386 0.405
1.0 0.423 0.381 0.401
different number of hashtags ranging from 1 to 5 respectively. In the figure, curves which are close
to the upper right-hand corner of the graph indicate the better performance. From the results, we can
observe that the performance of TUK-TTM is in the upper right-hand corner. It also demonstrates that
the proposed method achieves better performances than other methods.
From the description of the proposed model, we can know that there are several hyperparameters in
the proposed TUK-TTM. To evaluate the impacts of them, we evaluate two crucial ones, the number of
topics K and the smoothing factor ?. Table 4 shows the influence of the number of topics. From the
table, we can observe that the proposed model obtains the best performance when K is set to 70. And
performance decreases with more number of topics. We think that data sparsity may be one of the main
reasons. With much more topic number, the data sparsity problem will be more serious when estimating
topic-specific translation probability. Table 5 shows the influence of the translation probability smoothing
parameter ?. When ? is set to 0.0, it means that the topical information is omitted. Comparing the results
of ? = 0.0 and other values, we can observe that the topical information can benefit this task. When ? is
set to 1.0, it represents the method without smoothing. The results indicate that it is necessary to address
the sparsity problem through smoothing.
5 Conclusions
In this paper, we propose a novel method which incorporates temporal and personal factors into the
topical translation model for hashtag recommendation task. Since existing translation model based
methods for this task can only recommend hashtags that exist in the training data of the topical translation
model, we also incorporate extraction strategies into the model. To evaluate the proposed method, we
also construct a dataset from real world microblogging services. The results of experiments on the dataset
demonstrate that the proposed method outperforms state-of-the-art methods that do not consider these
aspects.
6 Acknowledgement
The authors wish to thank the anonymous reviewers for their helpful comments. This work was
partially funded by 973 Program (2010CB327900), National Natural Science Foundation of China
(61003092,61073069), Shanghai Leading Academic Discipline Project (B114) and ?Chen Guang?
project supported by Shanghai Municipal Education Commission and Shanghai Education Development
Foundation(11CG05).
210
References
A.Bandyopadhyay, M. Mitra, and P. Majumder. 2011. Query expansion for microblog retrieval. In Proceedings
of The Twentieth Text REtrieval Conference, TREC 2011.
S. Asur and B.A. Huberman. 2010. Predicting the future with social media. In WI-IAT?10, volume 1, pages
492?499.
Hila Becker, Mor Naaman, and Luis Gravano. 2010. Learning similarity metrics for event identification in social
media. In Proceedings of WSDM ?10.
Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In
Proceedings of CIKM?10.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated data. In Proceedings of SIGIR, pages 127?134.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and
smileys. In Proceedings of COLING ?10.
Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Learning topical translation model for
microblog hashtag suggestion. In Proceedings of IJCAI 2013.
Miles Efron. 2010. Hashtag retrieval in a microblogging environment. In Proceedings of SIGIR ?10.
Nikhil Garg and Ingmar Weber. 2008. Personalized, interactive tag recommendation for flickr. In Proceedings of
RecSys ?08.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences.
Ido Guy, Naama Zwerdling, Inbal Ronen, David Carmel, and Erel Uziel. 2010. Social media recommendation
based on people and tags. In Proceedings of SIGIR ?10.
Ido Guy, Uri Avraham, David Carmel, Sigalit Ur, Michal Jacovi, and Inbal Ronen. 2013. Mining expertise and
interests from social media. In Proceedings of WWW ?13.
Paul Heymann, Daniel Ramage, and Hector Garcia-Molina. 2008. Social tag prediction. In Proceedings of SIGIR
?08.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment
classification. In Proceedings of ACL 2011, Portland, Oregon, USA.
Ralf Krestel, Peter Fankhauser, and Wolfgang Nejdl. 2009. Latent dirichlet allocation for tag recommendation. In
Proceedings of RecSys ?09.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide web, pages
661?670. ACM.
Ting-Peng Liang, Hung-Jen Lai, and Yi-Cheng Ku. 2007. Personalized content recommendation and user
satisfaction: Theoretical synthesis and empirical findings. Journal of Management Information Systems,
23(3):45?70.
Huizhi Liang, Yue Xu, Yuefeng Li, Richi Nayak, and Xiaohui Tao. 2010. Connecting users and items with
weighted tags for personalized item recommendations. In Proceedings of the 21st ACM conference on Hypertext
and hypermedia, pages 51?60. ACM.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012. Topical word trigger model for keyphrase extraction. In
Proceedings of COLING.
Yu-Ta Lu, Shoou-I Yu, Tsung-Chieh Chang, and Jane Yung-jen Hsu. 2009. A content-based method to enhance
tag recommendation. In Proceedings of IJCAI?09.
211
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Tsutomu Ohkura, Yoji Kiyota, and Hiroshi Nakagawa. 2006. Browsing system for weblog articles based on
automated folksonomy. Workshop on the Weblogging Ecosystem Aggregation Analysis and Dynamics at WWW.
Takanobu Otsuka, Takuya Yoshimura, and Takayuki Ito. 2012. Evaluation of the reputation network using realistic
distance between facebook data. In Proceedings of WI-IAT ?12.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag
recommendation. In Proceedings of the third ACM international conference on Web search and data mining,
pages 81?90. ACM.
Steffen Rendle, Leandro Balby Marinho, Alexandros Nanopoulos, and Lars Schmidt-Thieme. 2009. Learning
optimal ranking with tensor factorization for tag recommendation. In Proceedings of KDD ?09.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of WWW ?10.
Andriy Shepitsen, Jonathan Gemmell, Bamshad Mobasher, and Robin Burke. 2008. Personalized
recommendation in social tagging systems using hierarchical clustering. In Proceedings of the 2008 ACM
Conference on Recommender Systems, RecSys ?08, pages 259?266, New York, NY, USA. ACM.
Yang Song, Ziming Zhuang, Huajing Li, Qiankun Zhao, Jia Li, Wang-Chien Lee, and C. Lee Giles. 2008. Real-
time automatic tag recommendation. In Proceedings of SIGIR ?08.
Amara Tariq, Asim Karim, Fernando Gomez, and Hassan Foroosh. 2013. Exploiting topical perceptions over
multi-lingual text for hashtag suggestion on twitter. In FLAIRS Conference.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and Ming Zhang. 2011. Topic sentiment analysis in twitter:
a graph-based hashtag sentiment classification approach. In Proceedings of CIKM ?11.
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li.
2011. Topical keyphrase extraction from twitter. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies-Volume 1, pages 379?388. Association for
Computational Linguistics.
212
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 688?697, Dublin, Ireland, August 23-29 2014.
A Generative Model for Identifying Target Companies of Microblogs
Yeyun Gong, Yaqian Zhou, Ya Guo, Qi Zhang, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{12110240006, zhouyaqian, 13210240002, qz, xjhuang}@fudan.edu.cn
Abstract
Microblogging services have attracted hundreds of millions of users to publish their status, ideas
and thoughts, everyday. These microblog posts have also become one of the most attractive and
valuable resources for applications in different areas. The task of identifying the main targets of
microblogs is an important and essential step for these applications. In this paper, to achieve this
task, we propose a novel method which converts the target company identification problem to
the translation process from content to targets. We introduce a topic-specific generative method
to model the translation process. Topic specific trigger words are used to bridge the vocabulary
gap between the words in microblogs and targets. We examine the effectiveness of our approach
via datasets gathered from real world microblogs. Experimental results demonstrate a 20.2%
improvement in terms of F1-score over the state-of-the-art discriminative method.
1 Introduction
With the rapid growth of social media, about 72% of adult internet users are also members of
a social networking site
1
. Over the past few years, microblogging has become one of the most
popular services. Meanwhile, microblogs have also been widely used as sources for analyzing public
opinions (Bermingham and Smeaton, 2010; Jiang et al., 2011), prediction (Asur and Huberman, 2010;
Bollen et al., 2011), reputation management (Pang and Lee, 2008; Otsuka et al., 2012), and many other
applications (Bian et al., 2008; Sakaki et al., 2010; Becker et al., 2010; Guy et al., 2010; Lee and Croft,
2013; Guy et al., 2013). For most of these applications, identifying the microblogs that are relevant to
the targets of interest is one of the basic steps (Lin and He, 2009; Amig?o et al., 2010; Qiu et al., 2011;
Liu et al., 2013). Let us firstly consider the following example:
Example 1: 11? MacBook Air can run for up to five hours on a single charge.
?MacBook Air? can be considered to be the target being discussed on the microblog, and we can also
infer from the microblog that it is related to Apple Inc. The ability to discriminate which company is
being referred to in a microblog is required by many applications.
Previous studies on fine-grained sentiment analysis and aspect-based opinion mining proposed
supervised (Popescu and Etzioni, 2005; Liu et al., 2012a; Liu et al., 2013) and unsupervised methods (Hu
and Liu, 2004; Wu et al., 2009; Zhang et al., 2010) to extract targets of opinion expressions. Based on
the associations between opinion targets and opinion words, some methods were also introduced to
simultaneously solve the opinion expression and target extraction problems (Qiu et al., 2011; Liu et al.,
2012a). However, most of the existing methods in this area only focus on extracting items about which
opinions are expressed in a given domain. The implicated information of targets is rarely considered.
Moreover, domain adaptation is another big challenge for these fine-grained methods in processing
different domains.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
It is reported by the Pew Research Center?s Internet & American Life Project in Aug 5, 2013.
688
The WePS-3
2
(Amig?o et al., 2010) and RepLab 2013
3
(Amig?o et al., 2013) evaluation campaigns also
addressed the problem from the perspective of the disambiguation of company names in microblogs.
Microblogs that contain company names at a lexical level are classified based on whether it refers
to the company or not. Various approaches have been proposed to address the task with different
methods (Pedersen et al., 2006; Yerva et al., 2010; Zhang et al., 2012; Spina et al., 2012; Spina et
al., 2013). However, the microblogs that do not contain company names cannot be correctly processed
using these methods. From analyzing the data, we observe that a variety of microblog posts belong to
this type. They only contain products names, slang terms, and other related company content.
To achieve this task, in this paper, we propose the use of a translation based model to identify the targets
of microblogs. We assume that the microblog posts and targets describe the same topic using different
languages. Hence, the target identification problem can be regarded as a translation process from the
content of the microblogs to the targets. We integrate latent topical information into the translation
model to facilitate the translation process. Because product names, series, and other related information
are important indicators for this task, we also incorporate this background knowledge into the model. To
evaluate the proposed method, we collect a large number of microblogs and manually annotate a subset
of these as golden standards. We compare the proposed method with state-of-the-art methods using the
constructed dataset. Experimental results demonstrate that the proposed approach can achieve better
performance than the other approaches.
2 The Proposed Method
2.1 The Generation Process
Given a corpus D = {d
i
, 1 ? i ? |D|}, which contains a list of microblogs {d
i
}. A microblog is a
sequence of N
d
words denoted by w
d
= {w
d1
, w
d2
, ..., w
dN
d
}. Each microblog contains a set of targets
denoted by c
d
= {c
d1
, c
d2
, ..., c
dM
d
}. A word is defined as an item from a vocabulary with V distinct
words indexed by w = {w
1
, w
2
, ..., w
V
}. The nth word in the dth microblog is associated with not only
one topic z
dn
, but also an indicator variable l
dn
which indicates whether w
dn
belongs to the ontology
(l
dn
= 1), which contains company names, product names, series, and other related information, or is a
common word (l
dn
= 0). Each target is from the vocabulary with C distinct company names indexed by
c = {c
1
, c
2
, ..., c
C
}. The mth target in the dth microblog is associated with a topic z
dm
. The notations
used in this paper are summarized in Table 1. Fig. 1 shows the graphical representation of the generation
process. The generative story for each microblog is as follows:
1. Sample word distribution ?
t,l
fromDir(?
l
) for each topic t = 1, 2, ..., T and each label l = 1, ..., L.
2. For each microblog d=1,2,...,|D|
a. Sample topic distribution ?
d
from Dir(?)
b. For each word n = 1, 2, ..., N
d
i. Sample a topic z
dn
= t from Multinomial(?
d
)
ii. Sample a label l
dn
= l from the distribution over labels, v
d,n
iii. Sample a wordw according to multinomial distribution P (w
dn
= w|z
dn
= t, l
dn
= l, ?
t,l
)
c. For each target m = 1, 2, ...,M
d
i. Sample a topic z
dm
= t from Multinomial(?
d
)
ii. Sample a target c
dm
= c according to probability P (c
dm
= c|w
d
, l
d
, z
dm
= t, B)
As described above, we use l
dn
to incorporate the ontology information into the model. In this work,
we construct an ontology which contains 4,926 company names, 7,632 abbreviations, and 26,732 product
names. These companies names are collected based on the top search queries in different categories
4
.
We propose to use the distribution v
d,n
to indicate the probability of variable l
dn
. We set v
d,n
by applying
2
http://nlp.uned.es/weps/weps-3
3
http://www.limosine-project.eu/events/replab2013
4
http://top.baidu.com/boards
689
wdn
z
dn
?
d
?
c
dm
z
dm
B
?
t,l
?
l
l
dn
v
d,n
f
dn
?
M
d
N
d
|D|
V
T
L
Figure 1: The graphical representation of the proposed model. Shaded circles are observations or
constants. Unshaded ones are hidden variables.
various sources of ontology (presented by ?) and the context features of the word w
dn
(presented by f
dn
).
In this work, we only consider the word itself as its context feature. This information is encoded into
the hyperparameters {?
w
|w ? {w
1
, w
2
, ..., w
V
}}, where ?
w
is hyperparameter for the word w, and
?
w
0
+ ?
w
1
= 1. For each word w in the ontology, we set ?
w
1
to a value 0.9, ?
w
0
to a value 0.1. For each
word w not contained by ontology, we set ?
w
1
to a value 0 and ?
w
0
to a value 1. Based on the ontology,
v
d,n
could be set as follows:
P (l
dn
= l|w
dn
= w) = v
d,n
l
=
?
w
l
?
w
1
+ ?
w
0
, l ? {0, 1}
(1)
2.2 Model Inference
We use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to obtain samples of hidden variable
assignment and to estimate the model parameters from these samples.
On the microblog content side, the conditional probability of a latent topic and label for the nth word
in the dth microblog is:
Pr(z
dn
= t, l
dn
= l|w
dn
= w,w
?n
, z
?n
, l
?n
) ?
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
?
N
t,?n
d
+ ?
N
?n
d
+ T?
,
(2)
where N
w,?n
t,l
is the number of the word w that are assigned to topic t under the label l; N
?n
t,l
is the
number of all the words that are assigned to topic t under the label l; N
t,?n
d
is the number of topic t in
the microblog d; N
?n
d
is the number of all the topics in the document d; ?n indicates taking no account
of the current position n.
Given the conditional probability of z
dn
= t, l
dn
= l, we formalize the marginal probability of z
dn
= t
as follows:
Pr(z
dn
= t|w
dn
= w,w
?n
, z
?n
, l
?n
) ?
L?1
?
l=0
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
?
N
t,?n
d
+ ?
N
?n
d
+ T?
(3)
690
Table 1: The notation used in the proposed model.
|D| The number of microblogs in the data set
V The number of unique words in the vocabulary
C The number of companies
T The number of topics
L The number of labels
N
d
The number of words in the dth microblog
M
d
The number of companies in the dth microblog
w
d
All the words in the dth microblog
c
d
All the targets in the dth microblog
z
d
The topic of the words in the dth microblog
l
d
The label of the words in the dth microblog
B The topic-specific word alignment table between a word and a target
?
t,l
Distribution of words for each topic t and each label l
?
d
Distribution of topics in microblog d
v
d,n
Distribution of labels for word w
dn
N
w,?n
t,l
The number of the word w that is assigned to topic t under the label l except the position n
N
?n
t,l
The number of all the words that are assigned to topic t under the label l. except the position n
N
t,?n
d
The number of topic t in the microblog d except the position n
N
?n
d
The number of all the topics in the microblog d except the position n
N
c,w
t,l
The number of the target c that co-occurs with the word w labeled as l under topic t
After re-assigning the topic z
dn
= t for the current word, the conditional probability of ontology label
for the nth word in the dth microblog is:
Pr(l
dn
= l|w
dn
= w, z
dn
= t,w
?n
, z
?n
, l
?n
) ?
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
(4)
On the target side, we perform topic assignments for each target as follows:
Pr(z
dm
= t|c
dm
= c, c
?m
,w, l, z
?m
) ?
N
d
?
n=1
?
l
dn
N
c,w
dn
,?m
t,l
dn
N
w
dn
t,l
dn
+ ?C
?
N
t,?m
d
+ ?
N
?m
d
+ T?
,
(5)
where ?
l
dn
is the weight for the label (?
1
> 1, ?
0
= 1); N
c,w
dn
,?m
t,l
dn
is the number of the company c that
co-occurs with the word w
dn
labeled as l
dn
under topic t; ?C is a smoothing part; N
w
dn
t,l
dn
is the number of
the word w
dn
labeled as l
dn
under topic t; N
t,?m
d
is the number of occurrences of topic t in the document
d; N
?m
d
is the number of occurrences of all the topics in the document d; ?m indicates taking no account
of the current position m.
Based on the above equations, after enough sampling iterations, we can estimate word alignment table
B, B
c,w,t,l
= ?
l
N
c,w
t,l
N
w
t,l
+?C
. Some companies just occur few times, and most of the words co-occur with
them also alignment with other companies, for this case, we use ?C to smooth, where C represent the
number of company c. And also we can estimate topic distribution ? for each document, and word
distribution ? for each topic and each label, as follows:
?
t
d
=
N
t
d
+ ?
N
d
+ T?
, ?
t,l
w
=
N
w
t,l
+ ?
l
N
t,l
+ V ?
l
The possibility table B
c,w,t,l
has a potential size of V ?C ?T ?L. The data sparsity may pose a problem
in estimating B
c,w,t,l
. To reduce the data sparsity problem, we introduce the remedy in our model. We
691
employ a linear interpolation with topic-free word alignment probability to avoid data sparsity problem:
B
?
c,w,t,l
= ?B
c,w,t,l
+ (1? ?)P (c|w),
(6)
where P (c|w) is topic-free word alignment probability between the word w and the company c. ? is
trade-off of two probabilities ranging from 0.0 to 1.0.
2.3 Target Company Extraction
Just like standard LDA, the proposed method itself finds a set of topics but does not directly extract
targets. Suppose we have a dataset which contains microblogs without targets, we can use the collapsed
Gibbs sampling to estimate the topic and label for the words in each microblog. The process is the same
as described in Section 3.2.
After the hidden topics and label of the words in each microblog become stable, we can estimate the
distribution of topics for the dth microblog by: P (t|w
d
) = ?
t
d
=
N
t
d
+?
N
d
+T?
. With the word alignment table
B
?
, we can rank companies for the dth microblog in unlabeled data by computing the scores:
Pr(c
dm
|w
d
) ?
T
?
t=1
N
d
?
n=1
P (c
dm
|t, w
dn
, l
dn
, B
?
) ? P (t|w
d
)P (w
dn
|w
d
),
(7)
where P (w
dn
|w
d
) is the weight of the word w
dn
in the microblog content w
d
. In this paper, we use
inverse document frequency (IDF) score to estimate it. Based on the ranking scores calculated by Eq.(7),
we can extract the top-ranked targets for each microblog to users.
3 Experiments
In this section, we will introduce the experimental results and datasets we constructed for training and
evaluation. We will firstly describe the how we construct the datasets and their statistics. Then we
will introduce the experiment configurations and baseline methods. Finally, the evaluation results and
analysis will be given.
3.1 Datasets
We started by using Sina Weibo?s API
5
to collect public microblogs from randomly selected users. The
dataset contains 282.2M microblogs published by 1.1M users. We use RAW-Weibo to represent it in the
following sections. Based on the collected raw microblogs, we constructed three datasets for evaluation
and training.
3.1.1 Training data
Since social media users post thoughts, ideas, or status on various topics in social medias, there are a
huge number of related companies. Manually constructing training data is a time consuming and cost
process. In this work, we propose a weakly manual method based on ontology and hashtag. A hashtag is
a string of characters preceded by the symbol #. In most cases, hashtags can be viewed as an indication
to the context of the tweet or as the core idea expressed in the tweet. Hence, we can use hashtag as the
targets.
We extract the microblogs whose hashtags contain ontology items as training data and the
corresponding ontology items as targets. Obviously, the training data constructed based on this method
is not perfect. However, since this method can effectively generate a great quantity of data, we think
that general characteristics can be modeled with the generated training data. To evaluate the corpus,
we randomly selected 100 microblogs from the training data and manually labeled their targets. The
accuracy of the sampled dataset is 91%. It indicates that the proposed training data generation method
is effective. From the RAW-Weibo dataset, we extracted a total of 1.79M microblogs whose hashtags
contain more than one target. Training instances for 2,574 target companies are included in the training
data.
5
http://open.weibo.com/
692
3.1.2 Test data
For evaluation, we manually constructed a dataset RAN-Weibo, which contains 2,000 microblogs selected
from RAW-Weibo. Three annotators were asked to label the target companies for each microblog. To
evaluate the quality of annotated dataset, we validate the agreements of human annotations using Cohen?s
kappa coefficient. The average ? among all annotators is 0.626. It indicates that the annotations are
reliable.
Since some targets are ambiguous, inspired by the evaluation campaigns WePS-3 and RepLab 2013,
we also constructed a dataset AMB-Weibo, where microblogs include 10 popular company names which
may cause ambiguity. For each target, we randomly selected and annotated 200 microblogs as golden
standards. Three annotators were also asked to label whether the microblog is related the given target or
not. The agreements of human annotations were also validated through Cohen?s kappa coefficient. The
average ? among all annotators is 0.692.
3.2 Experiment Configurations
We use precision (P ), recall (R), and F1-score (F
1
) to evaluate the performance. We ran our model
with 500 iterations of Gibbs sampling. We use 5-fold cross-validation in the training data to optimize
hyperparameters. The number of topics is set to 30. The other settings of hyperparameters are as follows:
? = 50/T , ? = 0.1, ? = 20, ? = 0.5. The smoothing parameter ? is set to 0.8.
For baselines, we compare the proposed model with the following baseline methods.
? Naive Bayes (NB): The target identification task can be easily formalized as a classification task,
where each target is considered as a classification label. Hence, we applied Naive Bayes to model
the posterior probability of each target given a microblog.
? Support Vector Machine (SVM): The content of microblogs are represented as vectors and SVM
is used to model the classification problem.
? IBM1: Translation model (IBM model-1) is applied to obtain the alignment probability between
words and targets.
? TTM: Topical translation model (TTM) was proposed by Ding et al. (2013) to achieve microblog
hashtag suggestion task. We adopted it to estimate the alignment probability between words and
targets.
3.3 Experimental Results
We evaluate the proposed method from the following perspectives: 1) comparing the proposed method
with the state-of-the-art methods on the two evaluation datasets; 2) identifying the impacts of parameters.
Table 2 shows the comparisons of the proposed method with the state-of-the-arts discriminative
and generative methods on the evaluation dataset RAN-Weibo. ?Our? denotes the method proposed
in previous sections. ?Our w/o BG? represents the proposed method without background knowledge.
From the results, we can observe that the proposed method is better than other methods. Discriminative
methods achieve worse results than generative methods. We think that the large number of targets is
one of the main reasons of the low performances. The results of the proposed models with and without
ontology information also show that background knowledge can benefit both the precision and recall.
TTM achieves better performance than IBM1. It indicates that topical information is useful for this
task. The performances of our method are significantly better than TTM. It illustrates that our smoothing
method and incorporation of background knowledge are effective.
From the description of the proposed model, we can know that there are several hyperparameters in
the proposed model. To evaluate the impacts of them, we evaluate two crucial ones among all of them,
the number of topics T and the smoothing factor ?. Table 3 shows the influence of the number of topics.
From the table, we can observe that the proposed model obtains the best performance when T is set to
30. And performance decreases with more number of topics. We think that data sparsity may be one of
the main reasons. With much more topic number, the data sparsity problem will be more serious when
693
Table 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the evaluation dataset RAN-
Weibo.
Methods Precision Recall F
1
NB 0.168 0.154 0.161
SVM 0.312 0,286 0.298
IBM1 0.236 0.214 0.220
TTM 0.356 0.327 0.341
Our w/o BG 0.488 0.448 0.467
Our 0.522 0.479 0.500
Table 3: The influence of the number of topics T of the proposed method.
T Precision Recall F
1
10 0.516 0.473 0.493
30 0.522 0.479 0.500
50 0.508 0.466 0.486
70 0.489 0.449 0.468
100 0.488 0.448 0.467
estimating topic-specific translation probability. Table 4 shows the influence of the translation probability
smoothing parameter ?. When ? is set to 0.0, it means that the topical information is omitted. Comparing
the results of ? = 0.0 and other values, we can observe that the topical information can benefit this task.
When ? is set to 1.0, it represents the method without smoothing. The results indicate that it is necessary
to address the sparsity problem through smoothing.
Figure 2 shows the results of different methods on the dataset AMB-Weibo. All the models are trained
with same dataset as the above experiments. From the results, we can observe that the F1-scores vary
from less than 0.40 up to almost 0.60. The performances? variations of other methods are also huge. We
think that training data size and difficulty level are two main reasons. The size of training data of different
targets vary greatly in the dataset. However, comparing with other method, the proposed method is the
most stable one. Comparing with other methods, the proposed method achieves better performance than
other methods for all targets.
4 Related Work
Organization name disambiguation task is fundamental problems in many NLP applications. The task
aims to distinguish the real world relevant of a given name with the same surface in context. WePS-
3
6
(Amig?o et al., 2010) and RepLab 2013
7
(Amig?o et al., 2013) evaluation campaigns have also addressed
the problem from the perspective of disambiguation organization names in microblogs. Pedersen et
al. (2006) proposed an unsupervised method for name discrimination. Yerva et al. (2010) used support
vector machines (SVM) classifier with various external resources, such as WordNet, metadata profile,
category profile, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet
allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for
this task. However, most of these methods focused on the text with predefined surface words. The
documents which do not contain organization names or person names can not be well processed by these
methods.
To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use
translation model to handle it. They modeled the tag suggestion task as a translation process from
6
http://nlp.uned.es/weps/weps-3
7
http://www.limosine-project.eu/events/replab2013
694
Table 4: The influence of the smoothing parameter ? of the propose method.
? Precision Recall F
1
0.0 0.471 0.432 0.451
0.2 0.490 0.449 0.469
0.4 0.495 0.454 0.474
0.6 0.511 0.468 0.489
0.8 0.522 0.479 0.500
1.0 0.519 0.476 0.496
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10
F1
-S
co
re
NB SVM IBM1 TTM Our w/o BG Our
Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the different companies in
the test dataset AMB-Weibo.
document content to tags. Ding et al. (2013) extended the translation based method and introduced a
topic-specific translation model to process the multiple meanings of words in different topics. Motivated
by these methods, we also propose to use topic-specific translation model to handle vocabulary problem.
Based on the model, in this work, we incorporate the background knowledge information into the model.
5 Conclusions
To identify target companies of microblogs, in this paper, we propose a novel topical translation
model to achieve the task. The main assumption is that the microblog posts and targets describe
the same thing with different languages. We convert the target identification problem to a translation
process from content of microblogs to targets. We integrate latent topical information into translation
model to hand the themes of microblogs in facilitating the translation process. We also incorporate
background knowledge (such as product names, series, et al.) into the generation model. Experimental
results on a large corpus constructed from a real microblog service and a number of manually labeled
golden standards of easily ambiguous entities demonstrate that the proposed method can achieve better
performance than other approaches.
6 Acknowledgement
The authors wish to thank the anonymous reviewers for their helpful comments. This work was
partially funded by 973 Program (2010CB327900), National Natural Science Foundation of China
(61003092,61073069), Shanghai Leading Academic Discipline Project (B114) and ?Chen Guang?
project supported by Shanghai Municipal Education Commission and Shanghai Education Development
Foundation(11CG05).
695
References
Enrique Amig?o, Javier Artiles, Julio Gonzalo, Damiano Spina, Bing Liu, and Adolfo Corujo. 2010.
Weps3 evaluation campaign: Overview of the on-line reputation management task. In CLEF (Notebook
Papers/LABs/Workshops).
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Tamara Mart??n, Edgar
Meij, Maarten Rijke, and Damiano Spina. 2013. Overview of replab 2013: Evaluating online reputation
monitoring systems. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization,
volume 8138 of Lecture Notes in Computer Science, pages 333?352. Springer Berlin Heidelberg.
S. Asur and B.A. Huberman. 2010. Predicting the future with social media. In Proceedings of WI-IAT 2010.
Hila Becker, Mor Naaman, and Luis Gravano. 2010. Learning similarity metrics for event identification in social
media. In Proceedings of WSDM ?10.
Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In
Proceedings of CIKM ?10.
Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the right facts in the crowd:
factoid question answering over social media. In Proceedings of WWW ?08.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Learning topical translation model for
microblog hashtag suggestion. In Proceedings of IJCAI 2013.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy
of Sciences, volume 101, pages 5228?5235.
Ido Guy, Naama Zwerdling, Inbal Ronen, David Carmel, and Erel Uziel. 2010. Social media recommendation
based on people and tags. In Proceedings of SIGIR ?10.
Ido Guy, Uri Avraham, David Carmel, Sigalit Ur, Michal Jacovi, and Inbal Ronen. 2013. Mining expertise and
interests from social media. In Proceedings of WWW ?13.
Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI?04.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment
classification. In Proceedings of ACL-HLT 2011, Portland, Oregon, USA.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsupervised name ambiguity resolution using a generative model.
In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ?11, pages 105?112,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Chia-Jung Lee and W. Bruce Croft. 2013. Building a web test collection using social media. In Proceedings of
SIGIR ?13, SIGIR ?13.
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIKM
?09.
Kang Liu, Liheng Xu, and Jun Zhao. 2012a. Opinion target extraction using word-based translation model. In
Proceedings of EMNLP-CoNLL ?12.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012b. Topical word trigger model for keyphrase extraction. In
Proceedings of COLING.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion targets
from online reviews. In Proceedings of ACL 2013, Sofia, Bulgaria.
Takanobu Otsuka, Takuya Yoshimura, and Takayuki Ito. 2012. Evaluation of the reputation network using realistic
distance between facebook data. In Proceedings of WI-IAT ?12, Washington, DC, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
696
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zornitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name discrimination using second order co-occurrence features.
In Computational Linguistics and Intelligent Text Processing, pages 208?222.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In
Proceedings of HL-EMNLP 2005, Vancouver, British Columbia, Canada.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Comput. Linguist., 37(1):9?27, March.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of the 19th international conference on World wide web, WWW
?10, pages 851?860, New York, NY, USA. ACM.
Damiano Spina, Edgar Meij, Maarten de Rijke, Andrei Oghina, Minh Thuong Bui, and Mathias Breuss. 2012.
Identifying entity aspects in microblog posts. In Proceedings of SIGIR ?12.
Damiano Spina, Julio Gonzalo, and Enrique Amig?o. 2013. Discovering filter keywords for company name
disambiguation in twitter. Expert Systems with Applications, 40(12):4986 ? 5003.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP 2009, Singapore.
Surender Reddy Yerva, Zoltn Mikls, and Karl Aberer. 2010. It was easy, when apples and blackberries
were only fruits. In Martin Braschler, Donna Harman, and Emanuele Pianta, editors, CLEF (Notebook
Papers/LABs/Workshops).
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain. 2010. Extracting and ranking product features
in opinion documents. In Proceedings of COLING ?10.
Shu Zhang, Jianwei Wu, Dequan Zheng, Yao Meng, and Hao Yu. 2012. An adaptive method for organization name
disambiguation with feature reinforcing. In Proceedings of the 26th Pacific Asia Conference on Language,
Information, and Computation, pages 237?245, Bali,Indonesia, November. Faculty of Computer Science,
Universitas Indonesia.
697
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1154?1164, Dublin, Ireland, August 23-29 2014.
Automatic Corpus Expansion for Chinese Word Segmentation
by Exploiting the Redundancy of Web Information
Xipeng Qiu, ChaoChao Huang and Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University, Shanghai, China
xpqiu@fudan.edu.cn, superhuang007@gmail.com, xjhuang@fudan.edu.cn
Abstract
Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are
based on supervised learning, which depend on large scale annotated corpus. However,
these supervised methods do not work well when we deal with a new different domain
without enough annotated corpus. In this paper, we propose a method to automatically
expand the training corpus for the out-of-domain texts by exploiting the redundant in-
formation on Web. We break up a complex and uncertain segmentation by resorting to
Web for an ample supply of relevant easy-to-segment sentences. Then we can pick out
some reliable segmented sentences and add them to corpus. With the augmented corpus,
we can re-train a better segmenter to resolve the original complex segmentation. The
experimental results show that our approach can more effectively and stably improve the
performance of CWS. Our method also provides a new viewpoint to enhance the perfor-
mance of CWS by automatically expanding corpus rather than developing complicated
algorithms or features.
1 Introduction
Word segmentation is a fundamental task for Chinese language processing. In recent years,
Chinese word segmentation (CWS) has undergone great development. The popular method is
to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004).
The goal of sequence labeling is to assign labels to all elements in a sequence, which can be
handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al.,
1996), Conditional Random Fields (CRF)(Lafferty et al., 2001).
After years of intensive researches, Chinese word segmentation achieves a quite high precision.
However, the performance of segmentation is not so satisfying for the practical demands to
analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news
texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain
texts.
Since these supervised approaches often has a high requirement on the quality and quantity of
annotated corpus, which is always not easy to create. As a result, many methods were proposed
to utilize the information of unlabeled data.
There are three kinds of methods for domain adaptation problem in CWS.
The first is to use unsupervised learning algorithm to segment texts, like branching entropy
(BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry
and Sagot, 2012).
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
1154
The second is to use unsupervised or domain-independent features in supervised learning for
Chinese word segmentation, such as punctuation and mutual information(MI), word accessory
variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011)
The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the
difference in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and
Isozaki, 2008).
Although these methods improve the performance of out-of-domain texts, the performance is
still worse than that of in-domain texts obviously.
We firstly investigate the reasons of lower performance in new domain for state-of-the-art
CWS systems and find that most of error segmentation were caused by out-of-vocabulary (OOV)
words, also called new words or unknown words (see details in Section 3). It is difficult to devote
efforts to building a corpus for out-of-domain texts, since new words are produced frequently as
the development of the society, especially the Internet society. It is also impractical to manually
maintain an up-to-date corpus to include all geographical names, person names, organization
names, technical terms, etc.
In this paper, we propose a method to automatically expand the training corpus for the out-
of-domain texts by exploiting the redundant information on Web. When we meet a complex
and potentially difficult-to-segment sentence, we do not expect to solve it with more complicated
learning algorithm or elaborate features. We assume that there are some relevant sentences that
are relatively easy to process. These simple sentences can help to solve the complex one.
For example, the sentence ??????? (L?Oreal, Maybelline)?is difficult to segment if
both ???? (L?Oreal)?and ???? (Maybelline)?are unknown words. However, we can
always find some easy-to-segment sentences, such as??????? (I use Maybelline)?,???
???? (production of L?Oreal)?, and so on. When we use these simple sentences to re-train
the segmenter, we can solve the previous complex sentence.
Our method relies on breaking up the complex problems into relevant smaller, simpler prob-
lems that can be solved easily. Fortunately, we can resort to the scale and redundancy of the
web for an ample supply of simple sentences that are relatively easy to process.
Our method is very easy to implement upon a trainable base segmenter. Given the out-of-
domain texts, we firstly choose some uncertain segmentations and select the candidate expansion
seeds. Secondly, we use these seeds to get the relevant texts from Web search engine. Then we
segment these texts and add the texts with high confidence to training corpus. Finally, we can
get a better segmenter with the new corpus.
The rest of the paper is organized as follows: we review the related works in section 2. In
section 3, we analyze the influence factor for CWS. Then we describe our method in section 4.
Section 5 introduces the base segmenter. Section 6 gives the experimental results. Finally we
conclude our work in section 7.
2 Related Works
The idea of exploring information redundancy on Web was introduced in question answering
system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information
extraction system KNOWITALL(Etzioni et al., 2004). However, this idea is rarely mentioned
in Chinese word segmentation.
Nonetheless, there are three kinds of related methods on Chinese word segmentation.
One is active learning. Both (Li et al., 2012) and (Sassano, 2002) try to use active learning
method to expand annotated corpus, but they still need to manually label some new raw texts
in order to enlarge the training corpus. Different with these methods, our method do not require
any manual oracle labeling at all.
Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005). Self-training
is a general semi-supervised learning approach. In self-training, a classifier is first trained with
the small amount of labeled data. The classifier is then used to classify the unlabeled data.
1155
0.7
0.75
0.8
0.85
0.9
0.95
1
0 1 2 3 4 5
F1
Number of Continuous OOV Words
(a) Number of continuous OOV
words
0.100.20
0.300.40
0.500.60
0.700.80
0.901.00
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.6 0.7 1
F1
OOV Rate
(b) OOV rate
0.100.20
0.300.40
0.500.60
0.700.80
0.901.00
1 2 3 4 5 6 7 8 9 10
F1
Word Length 
(c) Word Length
The blue horizontal line is the overall F1 score, and the red line is the F1 scores with different values of the
factor.
Figure 1: Analysis of Influence Factors
Typically the most confident unlabeled points, together with their predicted labels, are added
to the training set. The classifier is re-trained and the procedure repeated. Note that the
classifier uses its own predictions to teach itself. Self-training has been applied to several natural
language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POS-
tagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman
et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information
extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve
system performance on the target domain by simultaneously modeling annotated source-domain
data and unannotated target domain data in the training process. However, the data on target
domain cannot always help itself (Steedman et al., 2003).
The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the
massive manual natural annotations or punctuation information on the Internet to improve the
performance of CWS. However, these natural annotations are just partial annotations and their
roles depend on the qualities of the selected resource, such as Wikipedia.
In this paper, we wish to propose a method to obtain new fully-annotated data in more
aggressive way, which can combine the advantages of the above works.
3 Analysis of Influence Factors for CWS
Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV)
words for segmentation. We first conduct experiments on the Chinese Treebank (CTB6.0)
dataset (Xue et al., 2005) (The detailed information of dataset is shown in Section 6).
Table 1 shows the performance of base segmenter. The F1 score of OOV words is significantly
lower than that of in-vocabulary (INV) words.
Precision Recall F1
INV 95.86 96.58 96.21
OOV 74.12 66.77 70.25
Total 94.64 94.73 94.69
Table 1: Performances of INV and OOV words
We also investigate the impacts of three different factors: number of continuous OOV words,
OOV rate and word length. Figure 1 shows the F1 scores with the changes of the different
factors. We find that OOV words significantly improve the difficulty of segmentation, while the
word length does not always harm the accuracy.
These findings also indicate that we can improve the performance of CWS if we have a
dictionary or annotated corpus including these OOV words. With the redundancy of the Web
information, it is not difficult to automatically obtain the expected dictionary or corpus.
1156
4 Our Method
In this section, we describe our method to automatically expand the training corpus.
4.1 Framework of Automatic Corpus Expansion
Our framework of automatic corpus expansion is similar to standard process self-training or
active learning for domain adaptation. Given a trainable base segmenter, the texts in out-
of-domain, we firstly choose some uncertain segmentations and select the candidate expansion
seeds. Secondly, we use these seeds to get the relevant texts from Web search engine. Then we
segment these texts and add the texts with high confidence to training corpus. Finally, we can
get a better segmenter with the new corpus.
Algorithm 1 illustrates the framework of automatic corpus expansion.
Algorithm 1 Framework of Automatic Corpus Expansion
Input:
Annotated Corpus C
A
Unannotated Corpus in Target domain C
T
Uncertainty Threshold T
u
Seed Extraction Threshold T
se
Acceptation Threshold T
a
Maximum Iteration Number: M
Output: Expanded Annotated Corpus C
A
1: for i = 1 to M do
2: Train a basic segmenter using current C
A
with base learner
3: Use the basic segmenter to do segmentation for each sentence in C
T
and calculate its
confidence.
4: Choose out the sentences collection C
TS
, in which the segmentation confidence of each
sentence is less than T
u
.
5: Extract the expansion seeds collection C
seeds
from C
TS
and use search engine to acquire
relevant raw texts C
RRT
.
6: Segment and calculate the confidence for each sentence in C
RRT
.
7: Pick the reliable segmentations C
new
with confidence more than T
a
from C
RRT
.
8: Add C
new
into C
A
.
9: end for
10: return C
A
;
4.2 Uncertainty Sampling
The first key step in our method is to find the uncertain segmentations. There are many proposed
uncertainty measures in the literature of active learning (Settles, 2010), such as entropy and
query-by-committee (QBC) algorithm.
In our works, we investigate four following uncertainty measures for each sentence x. We use
S
1
(x), S
2
(x), ? ? ? , S
N
(x) to represent the top N scores given by the segmenter.
Normalized Score U
NS
The first measures is normalized score by the length of x, the normalized score U
NS
is calcu-
lated by
U
NS
=
S
1
(x)
L
(1)
where L is the length of x.
Standard Deviation U
SD
1157
The standard deviation is calculate with the top N scores.
U
SD
=
?
?
?
?
1
N
N
?
i=1
(S
i
(x) ? ?)
2 (2)
where ? = 1
N
?
N
i=1
S
i
(x) is the average or expected value of S
i
(x).
Entropy U
Entropy
Entropy is a measure of unpredictability or information content. Since we use character-based
method for word segmentation, each character is labeled as one of {B, M, E, S} to indicate
the segmentation. {B, M, E} represent Begin, Middle, End of a multi-character segmentation
respectively, and S represents a Single character segmentation.
Given the top N labeled results for a sentence, each labeled sequence consists of the labels
{B, M, E, S}. We define l ? {B,M,E, S} to represent the label variable, and count
j
(l) to be
the number of occurrences of l on position j among the top N results. Thus, we can calculate
the entropy for the labeling uncertainty of each character.
The entropy H
j
(l) for the character on position j is calculated by
H
j
(l) = ?
?
l
count
j
(l)
N
log countj(l)
N
, (3)
where ?
l
count
j
(l) = N .
The entropy of sentence U
Entropy
is the sum of the entropies of all the characters in the
sentence.
U
Entropy
=
L
?
j=1
H
j
(l). (4)
Margin U
Margin
Margin is the deviation of top 2 scores, which is often used in machine learning algorithms,
such as support vector machine (Cristianini and Shawe-Taylor, 2000) and passive-aggressive
algorithm (Crammer et al., 2006).
U
Margin
= S
1
(x) ? S
2
(x) (5)
Among the above four measures, the larger the entropy is, the more uncertain the result is.
For the rest three factors, the less the score is, the more uncertain the result is.
We test these four uncertainty measures on the development set in order to choose the best
one as our confidence measure.
In figure 2, we illustrate the relationship between each uncertainty measure and the OOV
count. We assume that the more OOV words are, the more uncertainty is. Meanwhile, a steep
learning curve imply a good ability to distinguish whether the result is uncertain.
Obviously, the entropy is not helpful according to our assumption. The normalized score is
okay but not good, and both the standard deviation and margin seem to be useful because they
can give a better threshold to distinguish uncertain segmentation. Finally, we choose margin as
our uncertainty measure.
4.3 Expansion Seeds Extraction
For the uncertain segmentation, not every word is unreliable. We just pick the suspicious
fragments. Therefore, we need to extract some seed phrases to get the relevant texts. It is
notable that these seed phrases do not need to be words. They can be the combinations of
several words or only parts of words.
Take the following sentence for example.
1158
0.3450.35
0.3550.36
0.3650.37
0.3750.38
0.385
0 2 4 6 8 10
U NS
OOV count
(a) NScore
00.001
0.0020.003
0.0040.005
0.0060.007
0 2 4 6 8 10
U SD
OOV count
(b) STDEV
00.5
11.5
22.5
33.5
44.5
5
0 2 4 6 8 10
U Entropy
OOV count
(c) Entropy
0
0.002
0.004
0.006
0.008
0.01
0.012
0 2 4 6 8 10
U Margin
OOV count
(d) Margin
Figure 2: Different Uncertainty Measures
??????????????
(L?Oreal, Maybelline, Lancome are good brands)
The first fragment??????????is difficult to segment if these words does not appear in
training corpus. Conversely, the second fragment is easy to segment since the containing words
are very common.
We use base segmenter to get the top five results as follows:
? ? ? ? ? ? ? ? ? ? ? ? ? ?
1 B M M M E B M E S B E S B E
2 B M M E B E B E S B E S B E
3 B M E B E B M E S B E S B E
4 S B M M E B E S S B E S B E
5 S B M M M E B E S B E S B E
(Li et al., 2012) proposed a good way to select the candidate words for active learning with
diversity measurement to avoid duplicate annotation. However, their method is not suitable for
our work. The reason is that they regarded CWS as a binary classification problem, while our
base segmenter uses 1st-order sequence labeling.
In our work, we choose the expansion seeds by calculating the entropy of each character. If
the entropy of the character is larger than threshold T
se
, we say that this character may be in
an uncertain context. Thus, we extract the consecutive uncertain characters and their contexts
as the expansion seeds.
For the above example, we select the ????????? (L?Oreal, Maybelline, Lancome)?
and its context ?? (is)?as a seed ???????????.
4.4 Collect relevance texts by using Web Search Engines
After obtaining the expansion seeds, we collect the relevant texts on multiple search engines
including Google, Baidu and Bing.
For the seed ???????????, we can get the following relevance sentence, which is
easy to segment.
??????????????????? 500 ????
(L?Oreal owns more than 500 brands, including Lancome, L?Oreal, Maybelline, Vichy, etc.)
In our work, we just get the top 100 relevant texts returned by each search engine without
manual intervention. We do not use any search API and directly use the returned webpages by
search engine, then extract the snippets and titles. Therefore, we just write a simple program
to collect the webpages and clean them.
4.5 Expand Training Corpus
Since the qualities of these relevant texts are spotty, we just pick the reliable texts with high
confidence scores. In contrast to uncertainty sampling, we find the certain segmentations from
the collecting raw texts and add them to training corpus. Here, we also use a margin to find
the reliable ones as new training data.
In our experiments, the number of selected sentence is 1 ? 5 for each seed.
Thus, we can re-train a new segmenter on the expanded corpus. After several iteration, we
will get a segmenter with the best performance.
1159
5 Base Segmenter
We use discriminative character-based sequence labeling for base word segmentation. Each
character is labeled as one of {B, M, E, S} to indicate the segmentation.
We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et
al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used
to avoid the overfitting problem.
6 Experiment
To evaluate our algorithm, we use both CTB6.0 and CTB7.0 datasets in our experiments. CTB
is a segmented, part-of-speech tagged, and fully bracketed corpus in the constituency formalism.
It is also a popular data set to evaluate word segmentation methods, such as (Sun and Xu, 2011).
Since CTB dataset is collected from different sources, such as newswire, magazine, broadcast
news and web blogs, it is suitable to evaluate the performance of CWS systems on different
domains.
We conduct two experiments on different divisions of datasets.
1. The first experiment is performed on CTB6.0 for comparison with state-of-the-art systems
which also utilize the unlabeled data for word segmentation.
2. The second experiment is performed on CTB7.0 for better evaluation on out-of-domain
texts. CTB7.0 contains some newer news texts and web blogs texts, which is more suitable
to evaluate our method for out-of-domain data.
In our experiments, we set C = 0.01 for PA algorithm. We also try to use the different values
of C, and found that larger values of C imply a more aggressive update step and result to fast
convergence, but it has little influence on the final accuracy. The maximum iteration number
M
? of PA algorithm is set to 50.
The feature templates are C
i
T
0
, (i = ?1, 0, 1),C
?1,0
T
0
, C
0,1
T
0
, C
?1,1
T
0
, T
?1,0
. C represents a
Chinese character, and the subscript of C indicates its position relative to the current character,
whose subscript is 0. T represents the character-based tag.
The evaluation measure are reported are precision, recall, and an evenly-weighted F
1
.
6.1 Experiments on CTB6.0
Train Dev Test
81-325, 400-454, 500-554, 590-596,
600-885, 900, 1001-1017, 1019,
1021-1035, 1037-1043, 1045-1059,
1062-1071, 1073-1078, 1100-1117,
1130-1131 1133-1140, 1143-1147,
1149-1151,2000-2139, 2160-2164,
2181-2279,2311-2549, 2603-2774,
2820-3079
41-80,
1120-1129,
2140-2159,
2280-2294,
2550-2569,
2775-2799,
3080-3109
(1-40,901-931 newswire)
(1018, 1020, 1036,
1044,1060-1061, 1072,
1118-1119, 1132,1141-1142,
1148 magazine) (2165-2180,
2295-2310, 2570-2602, 2800-
2819, 3110-3145 broadcast
news)
Table 2: CTB6.0 Dataset Division
On CTB 6.0, we divide the training, development and test sets according to (Yang and Xue,
2012). , which are shown in Table 2 The detailed statistical information is shown in Table 3.
Firstly, We use the development set to determine the parameters in Algorithm 1. For T
u
, T
se
and T
a
, we have three rounds to determine the parameters. In first round, we find the best value
t1 in the range to 0 ? 1 with the interval of 0.1. In second round, we find the best value t2 in
range t1? 0.1 ? t1+0.1 with the interval of 0.01. In third round, we find the final best value t3
1160
94.594.794.9
95.195.395.5
95.795.996.1
0 1 2 3 4 5
F1
Iterations
(a) F1 score
7072
7476
7880
8284
0 1 2 3 4 5
Roov
Iterations
(b) OOV Recall
Figure 3: Iterative Learning Curve on
CTB6.0
92.592.792.9
93.193.393.5
93.793.994.1
94.394.5
0 1 2 3 4 5
F1
Iterations
(a) F1 score
60.562.564.5
66.568.570.5
72.574.576.5
78.5
0 1 2 3 4 5
Roov
Iterations
(b) OOV Recall
Figure 4: Iterative Learning Curve on
CTB7.0
in the range to t2? 0.01 ? t1+ 0.01 with the interval of 0.001. The maximum iteration number
M is just determined based on convergence with the range 1 ? 10.
Finally, we set these parameters as following: uncertainty threshold T
u
= 0.003, seed extrac-
tion threshold T
se
= 0.65, acceptation threshold T
a
= 0.004 and maximum iteration number
M = 5.
Figure 3 shows the changing curve of F1 and OOV recall in the process of corpus expansion.
The performance of the baseline segmenter is shown at iteration 0. The curve shows that the
F1 score and OOV recall have continuous improvement with the increasing of train corpus. The
maximum performance is achieved at the 5th iteration. The detailed results are shown in Table
4. Compared with the baseline, the expanded corpus leads to a segmenter with significantly
higher accuracy. The relative error reductions are 26.37% and 43.63% in terms of the balanced
F-score and the recall of OOV words respectively.
Dataset Sents Words Chars OOV Rate
Train. 22757 639506 1053426 -
Dev. 2003 59764 100038 5.45%
Test 2694 81304 133798 5.58%
Table 3: Corpus Information of CTB 6.0
Test P R F1 R
oov
Baseline 94.64 94.73 94.69 70.25
Final 95.66 96.51 96.09 83.23
(Sun and Xu, 2011) 95.86 95.62 95.74 79.28
Table 4: Performance on CTB6.0
6.2 Experiments on CTB7.0
CTB7.0 includes documents from newswire, magazine articles, broadcast news, broadcast con-
versations, newsgroups and weblogs. The newly added documents contains texts from web
blogs, which is very different with news texts. Therefore, we use the documents (No. 4198 4411,
weblogs) as test dataset, and the rest as training dataset. The detailed statistical information
is shown in Table 5. We can see that the OOV rate is higher than the dataset in the first
experiment.
Dataset Sents Words Chars OOV Rate
Train. 40425 987307 1601142 -
Test 10177 209827 342061 7.09%
Table 5: Corpus Information of CTB 7.0
Test P R F1 R
oov
Baseline 93.58 92.40 92.98 60.72
Final 94.47 94.40 94.43 79.24
Table 6: Performance on CTB7.0
Figure 4 shows the changing curve of F1 and OOV recall in the process of corpus expansion.
The performance of the baseline segmenter is shown at iteration 0. The curve shows that the
F1 score and OOV recall have continuous improvement with the increasing of train corpus. The
maximum performance is achieved at iteration 5. The detailed results are shown in Table 6.
Compared with the baseline, the expanded corpus leads to a segmenter with significantly higher
accuracy. The relative error reductions are 20.66% and 47.15% in terms of the balanced F-score
and the recall of OOV words respectively.
1161
6.3 Analysis
The experimental results show that our method is very effective to improve the performance of
Chinese word segmentation. Especially, our method gives a significant boost on OOV words.
For the words such as ???????? (Borussia Moenchengladbach)?, ??????
(catalase)?,???? (Yi ZhongTian, a Chinese person name)?and???? (prime time)?,
it is still difficult to segment them correctly even if we can obtain useful features from unlabeled
data. When we take advantage of the redundant information from Web, we can easily collect
the relevant easy-to-segment sentences to expand the training corpus.
Our method can result to a segmenter significantly better than the systems which finds the
informative features derived from unlabeled data, such as (Sun and Xu, 2011). This also suggests
that expanding corpus is more effective than developing complicated algorithm or well-design
features. Of course, our method is compatible with these technologies, which can further improve
the performance of CWS by combining the Web redundancy.
7 Conclusion
In this paper, we propose a method to automatically expand the training corpus for the out-
of-domain texts. Given the out-of-domain texts, we first choose some uncertain segmentations
as candidate expansion seeds, and use these seeds to get the relevant texts from search engine.
Then we segment the texts and add the texts with high confidence to training corpus. We can
always obtain some easily-segmented texts due to the large amount of redundancy texts on Web,
especially for new words. Our experimental results show that our proposed method can more
effectively and stably utilize the unlabeled examples to improve the performance. Our method
also provides a new viewpoint to enhance the performance of CWS by expanding corpus rather
than developing complicated algorithms or features.
The long term goal of our method is to build an online and constant learning system, which
can identify the difficult tasks and seek help from crowdsourcing. Search engines are special
cases of crowdsourcing. In the future, we wish to investigate our method for other NLP tasks,
such as POS tagging, Named Entity Recognition, and so on.
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments. This work was
funded by NSFC (No.61003091), Science and Technology Commission of Shanghai Municipality
(14ZR1403200) and Shanghai Leading Academic Discipline Project (B114).
References
Y. Altun, D. McAllester, and M. Belkin. 2006. Maximum margin semi-supervised learning for structured
variables. Advances in neural information processing systems, 18:33.
Michele Banko, Eric Brill, Susan Dumais, and Jimmy Lin. 2002. AskMSR: Question answering using the
worldwide web. In Proceedings of 2002 AAAI Spring Symposium on Mining Answers from Texts and
Knowledge Bases, pages 7?9.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural
language processing. Computational Linguistics, 22(1):39?71.
Stephen Clark, James R Curran, and Miles Osborne. 2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-
Volume 4, pages 49?55. Association for Computational Linguistics.
C.L.A. Clarke, G.V. Cormack, and T.R. Lynam. 2001. Exploiting redundancy in question answering.
Proceedings of the 24th annual international ACM SIGIR conference on Research and development in
information retrieval, pages 358?365.
1162
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing.
K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine Learning Research, 7:551?585.
N. Cristianini and J. Shawe-Taylor. 2000. An introduction to support Vector Machines: and other
kernel-based learning methods. Cambridge Univ Pr.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates. 2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th international conference on World Wide Web, WWW
?04, pages 100?110, New York, NY, USA. ACM.
H. Feng, K. Chen, X. Deng, and W. Zheng. 2004. Accessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75?93.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL,
volume 2007, page 22.
Wenbin Jiang, Meng Sun, Yajuan L?, Yating Yang, and Qun Liu. 2013. Discriminative learning with
natural annotations: Word segmentation as a case study. In ACL, pages 761?769.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semi-supervised
conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th annual meeting of the Association
for Computational Linguistics, pages 209?216. Association for Computational Linguistics.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branch-
ing entropy. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 428?435.
Association for Computational Linguistics.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling question answering to the web. Proceedings of
the 10th international conference on World Wide Web, pages 150?161.
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmentation.
Computational Linguistics, 35(4):505?512.
Shoushan Li, Guodong Zhou, and Chu-Ren Huang. 2012. Active learning for Chinese word segmentation.
In COLING (Posters), pages 683?692.
Yang Liu and Yue Zhang. 2012. Unsupervised domain adaptation for joint segmentation and pos-tagging.
In COLING (Posters), pages 745?754.
Pierre Magistry and Beno?t Sagot. 2012. Unsupervized word segmentation: the case for mandarin
chinese. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 383?387. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In
Proceedings of the main conference on human language technology conference of the North American
Chapter of the Association of Computational Linguistics, pages 152?159. Association for Computational
Linguistics.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th international conference on Computational Linguistics.
Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statis-
tical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 616?623, Prague, Czech Republic, June. Association for Compu-
tational Linguistics.
1163
Kenji Sagae. 2010. Self-training without reranking for parser domain adaptation and its impact on
semantic role labeling. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural
Language Processing, pages 37?44. Association for Computational Linguistics.
Manabu Sassano. 2002. An empirical study of active learning with support vector machines for japanese
word segmentation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, pages 505?512. Association for Computational Linguistics.
Burr Settles. 2010. Active learning literature survey. University of Wisconsin, Madison.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul
Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets.
In Proceedings of the tenth conference on European chapter of the Association for Computational
Linguistics-Volume 1, pages 331?338. Association for Computational Linguistics.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970?979.
Association for Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-
word scale unlabeled data. In ACL, pages 665?673. Citeseer.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural language engineering, 11(2):207?238.
Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Yaqin Yang and Nianwen Xue. 2012. Chinese comma disambiguation for discourse analysis. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume
1, pages 786?794. Association for Computational Linguistics.
D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Computational Linguistics, pages 189?196. Association
for Computational Linguistics.
H. Zhao and C. Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging
for word segmentation and named entity recognition. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111. Citeseer.
Xiaojin Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
1164
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187?195,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Training and Decoding Using Virtual Nodes for Cascaded
Segmentation and Tagging Tasks
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn
Abstract
Many sequence labeling tasks in NLP require
solving a cascade of segmentation and tag-
ging subtasks, such as Chinese POS tagging,
named entity recognition, and so on. Tradi-
tional pipeline approaches usually suffer from
error propagation. Joint training/decoding in
the cross-product state space could cause too
many parameters and high inference complex-
ity. In this paper, we present a novel method
which integrates graph structures of two sub-
tasks into one using virtual nodes, and per-
forms joint training and decoding in the fac-
torized state space. Experimental evaluations
on CoNLL 2000 shallow parsing data set and
Fourth SIGHAN Bakeoff CTB POS tagging
data set demonstrate the superiority of our
method over cross-product, pipeline and can-
didate reranking approaches.
1 Introduction
There is a typical class of sequence labeling tasks
in many natural language processing (NLP) applica-
tions, which require solving a cascade of segmenta-
tion and tagging subtasks. For example, many Asian
languages such as Japanese and Chinese which
do not contain explicitly marked word boundaries,
word segmentation is the preliminary step for solv-
ing part-of-speech (POS) tagging problem. Sen-
tences are firstly segmented into words, then each
word is assigned with a part-of-speech tag. Both
syntactic parsing and dependency parsing usually
start with a textual input that is tokenized, and POS
tagged.
The most commonly approach solves cascaded
subtasks in a pipeline, which is very simple to im-
plement and allows for a modular approach. While,
the key disadvantage of such method is that er-
rors propagate between stages, significantly affect-
ing the quality of the final results. To cope with this
problem, Shi and Wang (2007) proposed a rerank-
ing framework in which N-best segment candidates
generated in the first stage are passed to the tag-
ging model, and the final output is the one with the
highest overall segmentation and tagging probabil-
ity score. The main drawback of this method is that
the interaction between tagging and segmentation is
restricted by the number of candidate segmentation
outputs. Razvan C. Bunescu (2008) presented an
improved pipeline model in which upstream subtask
outputs are regarded as hidden variables, together
with their probabilities are used as probabilistic fea-
tures in the downstream subtasks. One shortcom-
ing of this method is that calculation of marginal
probabilities of features may be inefficient and some
approximations are required for fast computation.
Another disadvantage of these two methods is that
they employ separate training and the segmentation
model could not take advantages of tagging infor-
mation in the training procedure.
On the other hand, joint learning and decoding
using cross-product of segmentation states and tag-
ging states does not suffer from error propagation
problem and achieves higher accuracy on both sub-
tasks (Ng and Low, 2004). However, two problems
arises due to the large state space, one is that the
amount of parameters increases rapidly, which is apt
to overfit on the training corpus, the other is that
the inference by dynamic programming could be in-
efficient. Sutton (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs) to perform joint
training/decoding of subtasks using much fewer pa-
rameters than the cross-product approach. How-
187
ever, DCRFs do not guarantee non-violation of hard-
constraints that nodes within the same segment get
a single consistent tagging label. Another draw-
back of DCRFs is that exact inference is generally
time consuming, some approximations are required
to make it tractable.
Recently, perceptron based learning framework
has been well studied for incorporating node level
and segment level features together (Kazama and
Torisawa, 2007; Zhang and Clark, 2008). The main
shortcoming is that exact inference is intractable
for those dynamically generated segment level fea-
tures, so candidate based searching algorithm is
used for approximation. On the other hand, Jiang
(2008) proposed a cascaded linear model which has
a two layer structure, the inside-layer model uses
node level features to generate candidates with their
weights as inputs of the outside layer model which
captures non-local features. As pipeline models, er-
ror propagation problem exists for such method.
In this paper, we present a novel graph structure
that exploits joint training and decoding in the fac-
torized state space. Our method does not suffer
from error propagation, and guards against viola-
tions of those hard-constraints imposed by segmen-
tation subtask. The motivation is to integrate two
Markov chains for segmentation and tagging sub-
tasks into a single chain, which contains two types of
nodes, then standard dynamic programming based
exact inference is employed on the hybrid struc-
ture. Experiments are conducted on two different
tasks, CoNLL 2000 shallow parsing and SIGHAN
2008 Chinese word segmentation and POS tagging.
Evaluation results of shallow parsing task show
the superiority of our proposed method over tradi-
tional joint training/decoding approach using cross-
product state space, and achieves the best reported
results when no additional resources at hand. For
Chinese word segmentation and POS tagging task, a
strong baseline pipeline model is built, experimental
results show that the proposed method yields a more
substantial improvement over the baseline than can-
didate reranking approach.
The rest of this paper is organized as follows: In
Section 2, we describe our novel graph structure. In
Section 3, we analyze complexity of our proposed
method. Experimental results are shown in Section
4. We conclude the work in Section 5.
2 Multi-chain integration using Virtual
Nodes
2.1 Conditional Random Fields
We begin with a brief review of the Conditional Ran-
dom Fields(CRFs). Let x = x1x2 . . . xl denote the
observed sequence, where xi is the ith node in the
sequence, l is sequence length, y = y1y2 . . . yl is a
label sequence over x that we wish to predict. CRFs
(Lafferty et al, 2001) are undirected graphic mod-
els that use Markov network distribution to learn the
conditional probability. For sequence labeling task,
linear chain CRFs are very popular, in which a first
order Markov assumption is made on the labels:
p(y|x) = 1Z(x)
?
i
?(x,y, i)
,where
?(x,y, i) = exp
(
wT f(x, yi?1, yi, i)
)
Z(x) =
?
y
?
i
?(x,y, i)
f(x, yi?1, yi, i) =
[f1(x, yi?1, yi, i), . . .,fm(x, yi?1, yi, i)]T , each ele-
ment fj(x, yi?1, yi, i) is a real valued feature func-
tion, here we simplify the notation of state feature
by writing fj(x, yi, i) = fj(x, yi?1, yi, i), m is the
cardinality of feature set {fj}. w = [w1, . . . , wm]T
is a weight vector to be learned from the training
set. Z(x) is the normalization factor over all label
sequences for x.
In the traditional joint training/decoding approach
for cascaded segmentation and tagging task, each
label yi has the form si-ti, which consists of seg-
mentation label si and tagging label ti. Let s =
s1s2 . . . sl be the segmentation label sequence over
x. There are several commonly used label sets such
as BI, BIO, IOE, BIES, etc. To facilitate our dis-
cussion, in later sections we will use BIES label set,
where B,I,E represents Beginning, Inside and End of
a multi-node segment respectively, S denotes a sin-
gle node segment. Let t = t1t2 . . . tl be the tagging
label sequence over x. For example, in named entity
recognition task, ti ? {PER, LOC, ORG, MISC,
O} represents an entity type (person name, loca-
tion name, organization name, miscellaneous entity
188
x2
s?t
2
2
x1
s?t
1
1
S-P S-O
x3
s?t
3
3
S-O
x4
s?t
4
4
B-P
x5
s?t
5
5
E-P
Hendrix ?s girlfriend Kathy Etchingham
Figure 1: Graphical representation of linear chain CRFs
for traditional joint learning/decoding
name and other). Graphical representation of lin-
ear chain CRFs is shown in Figure 1, where tagging
label ?P? is the simplification of ?PER?. For nodes
that are labeled as other, we define si =S, ti =O.
2.2 Hybrid structure for cascaded labeling
tasks
Different from traditional joint approach, our
method integrates two linear markov chains for seg-
mentation and tagging subtasks into one that con-
tains two types of nodes. Specifically, we first
regard segmentation and tagging as two indepen-
dent sequence labeling tasks, corresponding chain
structures are built, as shown in the top and mid-
dle sub-figures of Figure 2. Then a chain of twice
length of the observed sequence is built, where
nodes x1, . . . , xl on the even positions are original
observed nodes, while nodes v1, . . . , vl on the odd
positions are virtual nodes that have no content in-
formation. For original nodes xi, the state space is
the tagging label set, while for virtual nodes, their
states are segmentation labels. The label sequence
of the hybrid chain is y = y1 . . . y2l = s1t1 . . . sltl,
where combination of consecutive labels siti repre-
sents the full label for node xi.
Then we let si be connected with si?1 and si+1
, so that first order Markov assumption is made
on segmentation states. Similarly, ti is connected
with ti?1 and ti+1. Then neighboring tagging and
segmentation states are connected as shown in the
bottom sub-figure of Figure 2. Non-violation of
hard-constraints that nodes within the same seg-
ment get a single consistent tagging label is guar-
anteed by introducing second order transition fea-
tures f(ti?1, si, ti, i) that are true if ti?1 6= ti and
si ? {I,E}. For example, fj(ti?1, si, ti, i) is de-
fined as true if ti?1 =PER, si =I and ti =LOC.
In other words, it is true, if a segment is partially
tagging as PER, and partially tagged as LOC. Since
such features are always false in the training corpus,
their corresponding weights will be very low so that
inconsistent label assignments impossibly appear in
decoding procedure. The hybrid graph structure can
be regarded as a special case of second order Markov
chain.
Hendrix ?s girlfriend Kathy Etchingham
x1 x2 x3 x4 x5
s1 s2 s3 s4 s5
S S S B E
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
s2s1 s3 s4 s5
S S S B E
v1 v2 v3 v4 v5
Integrate
Figure 2: Multi-chain integration using Virtual Nodes
2.3 Factorized features
Compared with traditional joint model that exploits
cross-product state space, our hybrid structure uses
factorized states, hence could handle more flexible
features. Any state feature g(x, yi, i) defined in
the cross-product state space can be replaced by a
first order transition feature in the factorized space:
f(x, si, ti, i). As for the transition features, we
use f(si?1, ti?1, si, i) and f(ti?1, si, ti, i) instead
of g(yi?1, yi, i) in the conventional joint model.
Features in cross-product state space require that
segmentation label and tagging label take on partic-
ular values simultaneously, however, sometimes we
189
want to specify requirement on only segmentation or
tagging label. For example, ?Smith? may be an end
of a person name, ?Speaker: John Smith?; or a sin-
gle word person name ?Professor Smith will . . . ?. In
such case, our observation is that ?Smith? is likely a
(part of) person name, we do not care about its seg-
mentation label. So we could define state feature
f(x, ti, i) = true, if xi is ?Smith? with tagging la-
bel ti=PER.
Further more, we could define features like
f(x, ti?1, ti, i), f(x, si?1, si, i), f(x, ti?1, si, i),
etc. The hybrid structure facilitates us to use
varieties of features. In the remainder of the
paper, we use notations f(x, ti?1, si, ti, i) and
f(x, si?1, ti?1, si, i) for simplicity.
2.4 Hybrid CRFs
A hybrid CRFs is a conditional distribution that fac-
torizes according to the hybrid graphical model, and
is defined as:
p(s, t|x) = 1Z(x)
?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
Where
?(x, s, t, i) = exp
(
wT1 f(x, si?1, ti?1, si)
)
?(x, s, t, i) = exp
(
wT2 f(x, ti?1, si, ti)
)
Z(x) =
?
s,t
(?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
)
Where w1, w2 are weight vectors.
Luckily, unlike DCRFs, in which graph structure
can be very complex, and the cross-product state
space can be very large, in our cascaded labeling
task, the segmentation label set is often small, so
far as we known, the most complicated segmenta-
tion label set has only 6 labels (Huang and Zhao,
2007). So exact dynamic programming based algo-
rithms can be efficiently performed.
In the training stage, we use second order forward
backward algorithm to compute the marginal proba-
bilities p(x, si?1, ti?1, si) and p(x, ti?1, si, ti), and
the normalization factor Z(x). In decoding stage,
we use second order Viterbi algorithm to find the
best label sequence. The Viterbi decoding can be
Table 1: Time Complexity
Method Training Decoding
Pipeline (|S|2cs + |T |2ct)L (|S|2 + |T |2)U
Cross-Product (|S||T |)2cL (|S||T |)2U
Reranking (|S|2cs + |T |2ct)L (|S|2 + |T |2)NU
Hybrid (|S| + |T |)|S||T |cL (|S| + |T |)|S||T |U
used to label a new sequence, and marginal compu-
tation is used for parameter estimation.
3 Complexity Analysis
The time complexity of the hybrid CRFs train-
ing and decoding procedures is higher than that of
pipeline methods, but lower than traditional cross-
product methods. Let
? |S| = size of the segmentation label set.
? |T | = size of the tagging label set.
? L = total number of nodes in the training data
set.
? U = total number of nodes in the testing data
set.
? c = number of joint training iterations.
? cs = number of segmentation training itera-
tions.
? ct = number of tagging training iterations.
? N = number of candidates in candidate rerank-
ing approach.
Time requirements for pipeline, cross-product, can-
didate reranking and hybrid CRFs are summarized
in Table 1. For Hybrid CRFs, original node xi has
features {fj(ti?1, si, ti)}, accessing all label subse-
quences ti?1siti takes |S||T |2 time, while virtual
node vi has features {fj(si?1, ti?1, si)}, accessing
all label subsequences si?1ti?1si takes |S|2|T | time,
so the final complexity is (|S|+ |T |)|S||T |cL.
In real applications, |S| is small, |T | could be
very large, we assume that |T | >> |S|, so for
each iteration, hybrid CRFs is about |S| times slower
than pipeline and |S| times faster than cross-product
190
Table 2: Feature templates for shallow parsing task
Cross Product CRFs Hybrid CRFs
wi?2yi, wi?1yi, wiyi wi?1si, wisi, wi+1si
wi+1yi, wi+2yi wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
wi?1wiyi, wiwi+1yi wi?1wisi, wiwi+1si
wi?1witi, wiwi+1ti
pi?2yi, pi?1yi, piyi pi?1si, pisi, pi+1si
pi+1yi, pi+2yi pi?2ti, pi?1ti, pi+1ti, pi+2ti
pi?2pi?1yi, pi?1piyi, pipi+1yi,
pi+1pi+2yi
pi?2pi?1si, pi?1pisi, pipi+1si, pi+1pi+2si
pi?3pi?2ti, pi?2pi?1ti, pi?1piti, pipi+1ti,
pi+1pi+2ti, pi+2pi+3ti, pi?1pi+1ti
pi?2pi?1piyi, pi?1pipi+1yi,
pipi+1pi+2yi
pi?2pi?1pisi, pi?1pipi+1si, pipi+1pi+2si
wipiti
wisi?1si
wi?1ti?1ti, witi?1ti, pi?1ti?1ti, piti?1ti
yi?1yi si?1ti?1si, ti?1siti
method. When decoding, candidate reranking ap-
proach requires more time if candidate number N >
|S|.
Though the space complexity could not be com-
pared directly among some of these methods, hybrid
CRFs require less parameters than cross-product
CRFs due to the factorized state space. This is sim-
ilar with factorized CRFs (FCRFs) (Sutton et al,
2004).
4 Experiments
4.1 Shallow Parsing
Our first experiment is the shallow parsing task. We
use corpus from CoNLL 2000 shared task, which
contains 8936 sentences for training and 2012 sen-
tences for testing. There are 11 tagging labels: noun
phrase(NP), verb phrase(VP) , . . . and other (O), the
segmentation state space we used is BIES label set,
since we find that it yields a little improvement over
BIO set.
We use the standard evaluation metrics, which are
precision P (percentage of output phrases that ex-
actly match the reference phrases), recall R (percent-
age of reference phrases returned by our system),
and their harmonic mean, the F1 score F1 = 2PRP+R
(which we call F score in what follows).
We compare our approach with traditional cross-
product method. To find good feature templates,
development data are required. Since CoNLL2000
does not provide development data set, we divide
the training data into 10 folds, of which 9 folds for
training and 1 fold for developing. After selecting
feature templates by cross validation, we extract fea-
tures and learn their weights on the whole training
data set. Feature templates are summarized in Table
2, where wi denotes the ith word, pi denotes the ith
POS tag.
Notice that in the second row, feature templates
of the hybrid CRFs does not contain wi?2si, wi+2si,
since we find that these two templates degrade per-
formance in cross validation. However, wi?2ti,
wi+2ti are useful, which implies that the proper con-
text window size for segmentation is smaller than
tagging. Similarly, for hybrid CRFs, the window
size of POS bigram features for segmentation is 5
(from pi?2 to pi+2, see the eighth row in the sec-
ond column); while for tagging, the size is 7 (from
pi?3 to pi+3, see the ninth row in the second col-
umn). However for cross-product method, their win-
dow sizes must be consistent.
For traditional cross-product CRFs and our hybrid
CRFs, we use fixed gaussian prior ? = 1.0 for both
methods, we find that this parameter does not signifi-
191
Table 3: Results for shallow parsing task, Hybrid CRFs
significantly outperform Cross-Product CRFs (McNe-
mar?s test; p < 0.01)
Method Cross-Product
CRFs
Hybrid
CRFs
Training Time 11.6 hours 6.3 hours
Feature Num-
ber
13 million 10 mil-
lion
Iterations 118 141
F1 93.88 94.31
cantly affect the results when it varies between 1 and
10. LBFGS(Nocedal and Wright, 1999) method is
employed for numerical optimization. Experimen-
tal results are shown in Table 3. Our proposed CRFs
achieve a performance gain of 0.43 points in F-score
over cross-product CRFs that use state space while
require less training time.
For comparison, we also listed the results of pre-
vious top systems, as shown in Table 4. Our pro-
posed method outperforms other systems when no
additional resources at hand. Though recently semi-
supervised learning that incorporates large mounts
of unlabeled data has been shown great improve-
ment over traditional supervised methods, such as
the last row in Table 4, supervised learning is funda-
mental. We believe that combination of our method
and semi-supervised learning will achieve further
improvement.
4.2 Chinese word segmentation and POS
tagging
Our second experiment is the Chinese word seg-
mentation and POS tagging task. To facilitate com-
parison, we focus only on the closed test, which
means that the system is trained only with a des-
ignated training corpus, any extra knowledge is not
allowed, including Chinese and Arabic numbers, let-
ters and so on. We use the Chinese Treebank (CTB)
POS corpus from the Fourth International SIGHAN
Bakeoff data sets (Jin and Chen, 2008). The train-
ing data consist of 23444 sentences, 642246 Chinese
words, 1.05M Chinese characters and testing data
consist of 2079 sentences, 59955 Chinese words,
0.1M Chinese characters.
We compare our hybrid CRFs with pipeline and
candidate reranking methods (Shi and Wang, 2007)
Table 4: Comparison with other systems on shallow pars-
ing task
Method F1 Additional Re-
sources
Cross-Product CRFs 93.88
Hybrid CRFs 94.31
SVM combination 93.91
(Kudo and Mat-
sumoto, 2001)
Voted Perceptrons 93.74 none
(Carreras and Mar-
quez, 2003)
ETL (Milidiu et al,
2008)
92.79
(Wu et al, 2006) 94.21 Extended features
such as token fea-
tures, affixes
HySOL 94.36 17M words unla-
beled
(Suzuki et al, 2007) data
ASO-semi 94.39 15M words unla-
beled
(Ando and Zhang,
2005)
data
(Zhang et al, 2002) 94.17 full parser output
(Suzuki and Isozaki,
2008)
95.15 1G words unla-
beled data
using the same evaluation metrics as shallow pars-
ing. We do not compare with cross-product CRFs
due to large amounts of parameters.
For pipeline method, we built our word segmenter
based on the work of Huang and Zhao (2007),
which uses 6 label representation, 7 feature tem-
plates (listed in Table 5, where ci denotes the ith
Chinese character in the sentence) and CRFs for pa-
rameter learning. We compare our segmentor with
other top systems using SIGHAN CTB corpus and
evaluation metrics. Comparison results are shown
in Table 6, our segmenter achieved 95.12 F-score,
which is ranked 4th of 26 official runs. Except for
the first system which uses extra unlabeled data, dif-
ferences between rest systems are not significant.
Our POS tagging system is based on linear chain
CRFs. Since SIGHAN dose not provide develop-
ment data, we use the 10 fold cross validation de-
scribed in the previous experiment to turning feature
templates and Gaussian prior. Feature templates are
listed in Table 5, where wi denotes the ith word in
192
Table 5: Feature templates for Chinese word segmentation and POS tagging task
Segmentation feature templates
(1.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(1.2) ci?1cisi, cici+1si, ci?1ci+1si
(1.3) si?1si
POS tagging feature templates
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti, wi+1wi+2ti, wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti, c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.6) ti?1ti
Joint segmentation and POS tagging feature templates
(3.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(3.2) ci?1cisi, cici+1si, ci?1ci+1si
(3.3) ci?3ti, ci?2ti, ci?1ti, citi, ci+1ti, ci+2ti, ci+3ti
(3.4) ci?3ci?2ti, ci?2ci?1ti, ci?1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci?2citi, cici+2ti
(3.5) cisiti
(3.6) citi?1ti
(3.7) si?1ti?1si, ti?1siti
Table 6: Word segmentation results on Fourth SIGHAN
Bakeoff CTB corpus
Rank F1 Description
1/26 95.89? official best, using extra un-
labeled data (Zhao and Kit,
2008)
2/26 95.33 official second
3/26 95.17 official third
4/26 95.12 segmentor in pipeline sys-
tem
Table 7: POS results on Fourth SIGHAN Bakeoff CTB
corpus
Rank Accuracy Description
1/7 94.29 POS tagger in pipeline sys-
tem
2/7 94.28 official best
3/7 94.01 official second
4/7 93.24 official third
the sentence, cj(wi), j > 0 denotes the jth Chinese
character of word wi, cj(wi), j < 0 denotes the jth
last Chinese character, l(wi) denotes the word length
of wi. We compare our POS tagger with other top
systems on Bakeoff CTB POS corpus where sen-
tences are perfectly segmented into words, our POS
tagger achieved 94.29 accuracy, which is the best of
7 official runs. Comparison results are shown in Ta-
ble 7.
For reranking method, we varied candidate num-
bers n among n ? {10, 20, 50, 100}. For hybrid
CRFs, we use the same segmentation label set as
the segmentor in pipeline. Feature templates are
listed in Table 5. Experimental results are shown
in Figure 3. The gain of hybrid CRFs over the
baseline pipeline model is 0.48 points in F-score,
about 3 times higher than 100-best reranking ap-
proach which achieves 0.13 points improvement.
Though larger candidate number can achieve higher
performance, such improvement becomes trivial for
n > 20.
Table 8 shows the comparison between our work
and other relevant work. Notice that, such com-
parison is indirect due to different data sets and re-
193
0 20 40 60 80 10090.3
90.4
90.5
90.6
90.7
90.8
90.9
candidate number
F s
cor
e
 
 
candidate reranking
Hybrid CRFs
Figure 3: Results for Chinese word segmentation and
POS tagging task, Hybrid CRFs significantly outperform
100-Best Reranking (McNemar?s test; p < 0.01)
Table 8: Comparison of word segmentation and POS tag-
ging, such comparison is indirect due to different data
sets and resources.
Model F1
Pipeline (ours) 90.40
100-Best Reranking (ours) 90.53
Hybrid CRFs (ours) 90.88
Pipeline (Shi and Wang, 2007) 91.67
20-Best Reranking (Shi and Wang,
2007)
91.86
Pipeline (Zhang and Clark, 2008) 90.33
Joint Perceptron (Zhang and Clark,
2008)
91.34
Perceptron Only (Jiang et al, 2008) 92.5
Cascaded Linear (Jiang et al, 2008) 93.4
sources. One common conclusion is that joint mod-
els generally outperform pipeline models.
5 Conclusion
We introduced a framework to integrate graph struc-
tures for segmentation and tagging subtasks into one
using virtual nodes, and performs joint training and
decoding in the factorized state space. Our approach
does not suffer from error propagation, and guards
against violations of those hard-constraints imposed
by segmentation subtask. Experiments on shal-
low parsing and Chinese word segmentation tasks
demonstrate our technique.
6 Acknowledgements
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
The National High Technology Research and De-
velopment Program of China (2009AA01A346),
Shanghai Leading Academic Discipline Project
(B114), Doctoral Fund of Ministry of Education of
China (200802460066), National Natural Science
Funds for Distinguished Young Scholar of China
(61003092), and Shanghai Science and Technology
Development Funds (08511500302).
References
R. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Pro-
ceedings of ACL, pages 1?9.
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In Proceedings
of EMNLP, Waikiki, Honolulu, Hawaii.
X Carreras and L Marquez. 2003. Phrase recognition by
filtering and ranking with perceptrons. In Proceedings
of RANLP.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21:8?19.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL, Columbus, Ohio, USA.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing, India.
Junichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP, pages 315?
324, Prague, June.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL, pages
647?655.
194
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
ofspeech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs
based joint decoding method for cascaded segmenta-
tion and labeling tasks. In Proceedings of IJCAI, pages
1707?1712, Hyderabad, India.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In Proceedings of ICML.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL, pages
665?673.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Proceedings of EMNLP, Prague.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of Intel-
ligent Text Processing and Computational Linguistics,
pages 144?155.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL, Columbus, Ohio, USA.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. ma-
chine learning research. Machine Learning Research,
2:615?637.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111.
195
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1332?1341,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structural Opinion Mining for Graph-based Sentiment Representation
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qz,xjhuang,ldwu}@fudan.edu.cn
Abstract
Based on analysis of on-line review corpus
we observe that most sentences have compli-
cated opinion structures and they cannot be
well represented by existing methods, such as
frame-based and feature-based ones. In this
work, we propose a novel graph-based rep-
resentation for sentence level sentiment. An
integer linear programming-based structural
learning method is then introduced to produce
the graph representations of input sentences.
Experimental evaluations on a manually la-
beled Chinese corpus demonstrate the effec-
tiveness of the proposed approach.
1 Introduction
Sentiment analysis has received much attention in
recent years. A number of automatic methods have
been proposed to identify and extract opinions, emo-
tions, and sentiments from text. Previous researches
on sentiment analysis tackled the problem on vari-
ous levels of granularity including document, sen-
tence, phrase and word (Pang et al, 2002; Riloff et
al., 2003; Dave et al, 2003; Takamura et al, 2005;
Kim and Hovy, 2006; Somasundaran et al, 2008;
Dasgupta and Ng, 2009; Hassan and Radev, 2010).
They mainly focused on two directions: sentiment
classification which detects the overall polarity of a
text; sentiment related information extraction which
tries to answer the questions like ?who expresses
what opinion on which target?.
Most of the current studies on the second direc-
tion assume that an opinion can be structured as a
frame which is composed of a fixed number of slots.
Typical slots include opinion holder, opinion expres-
sion, and evaluation target. Under this representa-
tion, they defined the task as a slots filling prob-
lem for each of the opinions. Named entity recog-
nition and relation extraction techniques are usually
applied in this task (Hu and Liu, 2004; Kobayashi
et al, 2007; Wu et al, 2009).
However, through data analysis, we observe that
60.5% of sentences in our corpus do not follow the
assumption used by them. A lot of important infor-
mation about an opinion may be lost using those rep-
resentation methods. Consider the following exam-
ples, which are extracted from real online reviews:
Example 1: The interior is a bit noisy on the free-
way1.
Example 2: Takes good pictures during the day-
time. Very poor picture quality at night2.
Based on the definition of opinion unit proposed
by Hu and Liu (2004), from the first example, the
information we can get is the author?s negative opin-
ion about ?interior? using an opinion expression
?noisy?. However, the important restriction ?on the
freeway?, which narrows the scope of the opinion,
is ignored. In fact, the tuple (?noisy?,?on the free-
way?) cannot correctly express the original opinion:
it is negative but under certain condition. The sec-
ond example is similar. If the conditions ?during the
daytime? and ?at night? are dropped, the extracted
elements cannot correctly represent user?s opinions.
Example 3: The camera is actually quite good for
outdoors because of the software.
Besides that, an opinion expression may induce
other opinions which are not expressed directly. In
example 3, the opinion expression is ?good? whose
1http://reviews.carreview.com/blog/2010-ford-focus-
review-the-compact-car-that-can/
2http://www.dooyoo.co.uk/digital-camera/sony-cyber-shot-
dsc-s500/1151680/
1332
target is ?camera?. But the ?software? which trig-
gers the opinion expression ?good? is also endowed
with a positive opinion. In practice, this induced
opinion on ?software? is actually more informative
than its direct counterpart. Mining those opinions
may help to form a complete sentiment analysis re-
sult.
Example 4: The image quality is in the middle of
its class, but it can still be a reasonable choice for
students.
Furthermore, the relations among individual opin-
ions also provide additional information which is
lost when they are considered separately. Example
4 is such a case that the whole positive comment of
camera is expressed by a transition from a negative
opinion to a positive one.
In order to address those issues, this paper de-
scribes a novel sentiment representation and analysis
method. Our main contributions are as follows:
1. We investigate the use of graphs for repre-
senting sentence level sentiment. The ver-
tices are evaluation target, opinion expression,
modifiers of opinion. The Edges represent
relations among them. The semantic rela-
tions among individual opinions are also in-
cluded. Through the graph, various informa-
tion on opinion expressions which is ignored
by current representation methods can be well
handled. And the proposed representation is
language-independent.
2. We propose a supervised structural learning
method which takes a sentence as input and the
proposed sentiment representation for it as out-
put. The inference algorithm is based on in-
teger linear programming which helps to con-
cisely and uniformly handle various properties
of our sentiment representation. By setting ap-
propriate prior substructure constraints of the
graph, the whole algorithm achieves reasonable
performances.
The remaining part of this paper is organized as
follows: In Section 2 we discuss the proposed rep-
resentation method. Section 3 describes the com-
putational model used to construct it. Experimental
results in test collections and analysis are shown in
Section 4. In Section 5, we present the related work
and Section 6 concludes the paper.
2 Graph-based Sentiment Representation
In this work, we propose using directed graph to
represent sentiments. In the graph, vertices are
text spans in the sentences which are opinion ex-
pressions, evaluation targets, conditional clauses etc.
Two types of edges are included in the graph: (1)
relations among opinion expressions and their mod-
ifiers; (2) relations among opinion expressions. The
edges of the first type exist within individual opin-
ions. The second type of the edges captures the re-
lations among individual opinions. The following
sections detail the definition.
2.1 Individual Opinion Representation
Let r be an opinion expression in a sentence, the rep-
resentation unit for r is a set of relations {(r, dk)}.
For each relation (r, dk), dk is a modifier which is a
span of text specifying the change of r?s meaning.
The relations between modifier and opinion ex-
pression can be the type of any kind. In this work,
we mainly consider two basic types:
? opinion restriction. (r, dk) is called an opin-
ion restriction if dk narrows r?s scope, adds a
condition, or places limitations on r?s original
meaning.
? opinion expansion. (r, dk) is an opinion expan-
sion if r?s scope expands to dk, r induces an-
other opinion on dk, or the opinion on dk is im-
plicitly expressed by r.
Mining the opinion restrictions can help to get ac-
curate meaning of an opinion, and the opinion ex-
pansions are useful to cover more indirect opinions.
As with previous sentiment representations, we ac-
tually consider the third type of modifier which dk is
the evaluation target of r.
Figure 1 shows a concrete example. In this ex-
ample, there are three opinion expressions: ?good?,
?sharp?, ?slightly soft?. The modifiers of ?good?
are ?indoors? and ?Focus accuracy?, where relation
(?good?,?indoors?) is an opinion restriction because
?indoors? is the condition under which ?Focus ac-
curacy? is good. On the other hand, the relation
1333
(?sharp?, ?little 3x optical zooms?) is an opinion ex-
pansion because the ?sharp? opinion on ?shot? im-
plies a positive opinion on ?little 3x optical zooms?.
It is worth to remark that: 1) a modifier dk can re-
late to more than one opinion expression. For exam-
ple, multiple opinion expressions may share a same
condition; 2) dk itself can employ a set of relations,
although the case appears occasionally. The follow-
ing is an example:
Example 5: The camera wisely get rid of many
redundant buttons.
In the example, ?redundant buttons? is the eval-
uation target of opinion expression ?wisely get rid
of?, but itself is a relation between ?redundant?
and ?buttons?. Such nested semantic structure is
described by a path: ?wisely get rid of? target?????
[?redundant? target??????buttons?]nested target.
2.2 Relations between Individual Opinion
Representation
Assume ?ri? are opinion expressions ordered by
their positions in sentence, and each of them has
been represented by relations {(ri, dik)} individu-
ally (the nested relations for dik have also been de-
termined). Then we define two relations on adja-
cent pair ri, ri+1: coordination when the polarities
of ri and ri+1 are consistent, and transition when
they are opposite. Those relations among ri form a
set B called opinion thread. In Figure 1, the opin-
ion thread is: {(?good?, ?sharp?), (?sharp?, ?slightly
soft?)}.
The whole sentiment representation for a sentence
can be organized by a direct graphG = (V,E). Ver-
tex set V includes all opinion expressions and mod-
ifiers. Edge set E collects both relations of each
individual opinion and relations in opinion thread.
The edges are labeled with relation types in label set
L={?restriction?, ?expansion?, ?target?, ?coordina-
tion?, ?transition?} 3.
Compared with previous works, the advantages of
using G as sentiment representation are: 1) for in-
dividual opinions, the modifiers will collect more
information than using opinion expression alone.
3We don?t define any ?label? on vertices: if two span of text
satisfy a relation in L, they are chosen to be vertices and an
edge with proper label will appear inE. In other words, vertices
are identified by checking whether there exist relations among
them.
Focus accuracy was good indoors, and although the 
little 3x optical zooms produced sharp shots, the 
edges were slightly soft on the Canon. 
Focus 
accuracy 
edges 
slightly soft shots 
sharp 
little 3x optical 
zooms 
indoors 
good 
Expansion 
Target 
Coordinate 
Transition 
Target 
Target 
Restriction 
r
1 
r
2 
r
3 
d
11 
d
12 
d
21 
d
22 
d
31 
Figure 1: Sentiment representation for an example sen-
tence
Thus G is a relatively complete and accurate rep-
resentation; 2) the opinion thread can help to catch
global sentiment information, for example the gen-
eral polarity of a sentence, which is dropped when
the opinions are separately represented.
3 System Description
To produce the representation graph G for a sen-
tence, we need to extract candidate vertices and
build the relations among them to get a graph struc-
ture. For the first task, the experimental results in
Section 4 demonstrate that the standard sequential
labeling method with simple features can achieve
reasonable performance. In this section, we focus
on the second task, and assume the vertices in the
graph have already been correctly collected in the
following formulation of algorithm.
3.1 Preliminaries
In order to construct graph G, we use a structural
learning method. The framework is from the first or-
der discriminative dependency parsing model (Mc-
donald and Pereira, 2005). A sentence is denoted by
s; x are text spans which will be vertices of graph;
xi is the ith vertex in x ordered by their positions in
s. For a set of vertices x, y is the graph of its sen-
timent representation, and e = (xi, xj) ? y is the
direct edge from xi to xj in y. In addition, x0 is a
1334
virtual root node without inedge. G = {(xn,yn)}Nn
is training set.
Following the edge based factorization, the score
of a graph is the sum of its edges? scores,
score(x,y) =
?
(xi,xj)?y
score(xi, xj)
=
?
(xi,xj)?y
?T f(xi, xj), (1)
f(xi, xj) is a high dimensional feature vector of the
edge (xi, xj). The components of f are either 0 or 1.
For example the k-th component could be
fk(xi, xj) =
?
?
?
1 if xi.POS = JJ and xj .POS = NN
and label of (xi, xj)is restriction
0 otherwise
.
Then the score of an edge is the linear combination
of f ?s components, and the coefficients are in vector
?.
Algorithm 1 shows the parameter learning pro-
cess. It aims to get parameter ? which will assign
the correct graph y with the highest score among all
possible graphs of x (denoted by Y).
Algorithm 1 Online structural learning
Training Set:G = {(xn, yn)}Nn
1: ?0 = 0, r = 0, T =maximum iteration
2: for t = 0 to T do
3: for n = 0 to N do
4: y? = argmaxy?Y score(xn, y) B Inference
5: if y? 6= yn then
6: update ?t to ?t+1 B PA
7: r = r + ?t+1
8: end if
9: end for
10: end for
11: return ? = r/(N ? T )
3.2 Inference
Like other structural learning tasks, the ?argmax?
operation in the algorithm (also called inference)
y? = argmax
y?Y
score(x,y)
= argmax
y?Y
?
(xi,xj)?y
?T f(xi, xj) (2)
is hard because all possible values of y form a huge
search space. In our case, Y is all possible directed
acyclic graphs of the given vertex set, which num-
ber is exponential. Directly solving the problem of
finding maximum weighted acyclic graph is equiva-
lent to finding maximum feedback arc set, which is a
NP-hard problem (Karp, 1972). We will use integer
linear programming (ILP) as the framework for this
inference problem.
3.2.1 Graph Properties
We first show some properties of graph G either
from the definition of relations or corpus statistics.
Property 1. The graph is connected and without
directed cycle. From individual opinion represen-
tation, each subgraph of G which takes an opinion
expression as root is connected and acyclic. Thus
the connectedness is guaranteed for opinion expres-
sions are connected in opinion thread; the acyclic is
guaranteed by the fact that if a modifier is shared by
different opinion expressions, the inedges from them
always keep (directed) acyclic.
Property 2. Each vertex can have one outedge
labeled with coordination or transition at most. The
opinion thread B is a directed path in graph.
Property 3. The graph is sparse. The average
in-degree of a vertex is 1.03 in our corpus, thus the
graph is almost a rooted tree. In other words, the
cases that a modifier connects to more than one opin-
ion expression rarely occur comparing with those
vertices which have a single parent. An explaination
for this sparseness is that opinions in online reviews
always concentrate in local context and have local
semantic connections.
3.2.2 ILP Formulation
Based on the property 3, we divide the inference
algorithm into two steps: i) constructing G?s span-
ning tree (arborescence) with property 1 and 2; ii)
finding additional non-tree edges as a post process-
ing task. The first step is close to the works on ILP
formulations of dependency parsing (Riedel and
Clarke, 2006; Martins et al, 2009). In the second
step, we use a heuristic method which greedily adds
non-tree edges. A similar approximation method
is also used in (Mcdonald and Pereira, 2006) for
acyclic dependency graphs.
Step 1. Find MST. Following the multicommodity
1335
flow formulation of maximum spanning tree (MST)
problem in (Magnanti and Wolsey, 1994), the ILP
for MST is:
max.
?
i,j
yij ? score(xi, xj) (3)
s.t.
?
i,j
yij = |V | ? 1 (4)
?
i
fuij ?
?
k
fujk = ?uj ,1 ? u, j ? |V | (5)
?
k
fu0k = 1, 1 ? u ? |V | (6)
fuij ? yij , 1 ? u, j ? |V |,
0 ? i ? |V | (7)
fuij ? 0, 1 ? u, j ? |V |,
0 ? i ? |V | (8)
yij ? { 0, 1}, 0 ? i, j ? |V |. (9)
In this formulation, yij is an edge indicator vari-
able that (xi, xj) is a spanning tree edge when yij =
1, (xi, xj) is a non-tree edge when yij = 0. Then
output y is represented by the set {yij , 0 ? i, j ?
|V |} 4. Eq(4) ensures that there will be exactly
|V | ? 1 edges are chosen. Thus if the edges cor-
responding to those non zero yij is a connected sub-
graph, y is a well-formed spanning tree. Objective
function just says the optimal solution of yij have
the maximum weight.
The connectedness is guaranteed if for every ver-
tex, there is exactly one path from root to it. It is for-
mulated by using |V | ? 1 flows {fu, 1 ? u ? |V |}.
fu starts from virtual root x0 towards vertex xu.
Each flow fu = {fuij , 0 ? i, j ? |V |}. fuij indi-
cates whether flow fu is through edge (xi, xj). so
it should be 0 if edge (xi, xj) does not exist (by
(7)). The Kronecker?s delta ?uj in (5) guarantees fu
is only assumed by vertex xu, so fu is a well-formed
path from root to xu. (6) ensures there is only one
flow (path) from root to xu. Thus the subgraph is
connected. The following are our constraints:
c1: Constraint on edges in opinion thread (10)-
(11).
From the definition of opinion thread, we impose
a constraint on every vertex?s outedges in opinion
thread, which are labeled with ?coordination? or
4For simplicity, we overload symbol y from the graph of the
sentiment represetation to the MST of it.
?transition?. Let Iob be a characteristic function on
edges: Iob((j, k)) = 1 when edge (xj , xk) is labeled
with ?coordination? or ?transition?, otherwise 0. We
denote q variables for vertices:
qj =
?
k
yjk ? Iob((j, k)), 0 ? j ? |V |. (10)
Then following linear inequalities bound the number
of outedges in opinion thread (? 1) on each vertex:
qj ? 1, 0 ? j ? |V |. (11)
c2: Constraint on target edge (12).
We also bound the number of evaluation targets
for a vertex in a similar way. Let It be characteris-
tic function on edges identifing whether it is labeled
with ?target?,
?
k
yjk ? It((j, k)) ? Ct, 0 ? j ? |V |. (12)
The parameter Ct can be adjusted according to the
style of document. In online reviews, authors tend
to use simple and short comments on individual tar-
gets, so Ct could be set small.
c3: Constraint on opinion thread (13)-(18).
From graph property 2, the opinion thread should
be a directed path. It implies the number of con-
nected components whose edges are ?coordination?
or ?transition? should be less than 1. Two set of ad-
ditional variables are needed: {cj , 0 ? j ? |V |} and
{hj , 0 ? j ? |V |}, where
cj =
{
1 if an opinion thread starts at xj
0 otherwise ,
and
hj =
?
i
yij ? Iob((i, j)). (13)
Then cj = ?hj ? qj , which can be linearized by
cj? qj ? hj , (14)
cj? 1 ? hj , (15)
cj? qj , (16)
cj? 0. (17)
If the sum of cj is no more than 1, the opinion thread
of graph is a directed path.
?
j
cj ? 1. (18)
1336
1 
2 
3 
4 
5 
6 
7 
(a) 
(b) 
1 
2 
3 
4 
5 
6 7 
(c) 
1 
2 3 
4 
5 
6 
7 
Figure 2: The effects of c1 and c3. Assume solid lines
are edges labeled with ?coordination? and ?transition?,
dot lines are edges labeled with other types. (a) is an
arbitrary tree. (b) is a tree with c1 constraints. (c) is a
tree with c1 and c3. It shows c1 are not sufficient for
graph property 2: the edges in opinion thread may not be
connected.
Figure 2 illustrates the effects of c1 and c3.
Equations (10)-(18), together with basic multi-
commodity flow model build up the inference algo-
rithm. The entire ILP formulation involves O(|V |3)
variables and O(|V |2) constraints. Generally, ILP
falls into NPC, but as an important result, in the mul-
ticommodity flow formulation of maximum span-
ning tree problem, the integer constraints (9) on yij
can be dropped. So the problem reduces to a linear
programming which is polynomial solvable (Mag-
nanti and Wolsey, 1994). Unfortunately, with our
additional constraints the LP relaxation is not valid.
Step 2. Adding non-tree edges. We examine the
case that a modifier attaches to different opinion ex-
pressions. That often occurs as the result of the
sharing of modifiers among adjacent opinion expres-
sions. We add those edges in the following heuristic
way: If a vertex ri in opinion thread does not have
any modifier, we search the modifiers of its adjacent
vertices ri+1, ri?1 in the opinion thread, and add
edge (ri, d?) where
d? = argmax
d?S
score(ri, d),
and S are the modifiers of ri?1 and ri+1.
3.3 Training
We use online passive aggressive algorithm (PA)
with Hamming cost of two graphs in training (Cram-
mer et al, 2006).
Unigram Feature Template
xi.text w0.text w1.text
w0.POS w1.POS
wk?1.text wk.text
Inside wk?1.POS wk.POS
Features xi.hasDigital
xi.isSingleWord
xi.hasSentimentWord
xi.hasParallelPhrase
w?1.text w?2.text
w?1.POS w?2.POS
wk+1.text wk+2.text
Outside wk+1.POS wk+2.POS
Features c?1.text c?2.text
c?1.POS c?2.POS
cl+1.text cl+2.text
cl+1.POS cl+2.POS
Other Features
distance between parent and child
dependency parsing relations
Table 1: Feature set
3.4 Feature Construction
For each vertex xi in graph, we use 2 sets of fea-
tures: inside features which are extracted inside the
text span of xi; outside features which are outside
the text span of xi. A vertex xi is described both in
word sequence (w0, w1, ? ? ? , wk) and character se-
quence (c0, c1, ? ? ? , cl), for the sentences are in Chi-
nese.
? ? ? , w?1, w0, w1, w2, ? ? ? , wk?1, wk? ?? ?
xi
, wk+1 ? ? ?
? ? ? , c?1, c0, c1, c2, ? ? ? , cl?1, cl? ?? ?
xi
, cl+1 ? ? ?
For an edge (xi, xj), the high dimensional feature
vector f(xi, xj) is generated by using unigram fea-
tures in Table 1 on xi and xj respectively. The dis-
tance between parent and child in sentence is also
attached in features. In order to involve syntactic
information, whether there is certain type of depen-
dency relation between xi and xj is also used as a
feature.
1337
4 Experiments
4.1 Corpus
We constructed a Chinese online review corpus from
Pcpop.com, Zol.com.cn, and It168.com, which have
a large number of reviews about digital camera. The
corpus contains 138 documents and 1735 sentences.
Since some sentences do not contain any opinion,
1390 subjective sentences were finally chosen and
manually labeled.
Two annotators labeled the corpus independently.
The annotators started from locating opinion expres-
sions, and for each of them, they annotated other
modifiers related to it. In order to keep the relia-
bility of annotations, another annotator was asked
to check the corpus and determine the conflicts. Fi-
nally, we extracted 6103 elements, which are con-
nected by 6284 relations.
Relation Number
Target 2479
Coordinate 1173
Transition 154
Restriction 693
Expansion 386
Table 2: Statistics of relation types
Table 2 shows the number of various relation
types appearing in the labeled corpus. We observe
60.5% of sentences and 32.1% of opinion expres-
sions contain other modifiers besides ?target?. Thus
only mining the relations between opinion expres-
sions and evaluation target is actually at risk of inac-
curate and incomplete results.
4.2 Experiments Configurations
In all the experiments below, we take 90% of the cor-
pus as training set, 10% as test set and run 10 folder
cross validation. In feature construction, we use
an external Chinese sentiment lexicon which con-
tains 4566 positive opinion words and 4370 nega-
tive opinion words. For Chinese word segment, we
use ctbparser 5. Stanford parser (Klein and Man-
ning, 2003) is used for dependency parsing. In the
settings of PA, the maximum iteration number is
5http://code.google.com/p/ctbparser/
set to 2, which is chosen by maximizing the test-
ing performances, aggressiveness parameter C is set
to 0.00001. For parameters in inference algorithm,
Ct = 2, the solver of ILP is lpsolve6.
We evaluate the system from the following as-
pects: 1) whether the structural information helps
to mining opinion relations. 2) How the proposed
inference algorithm performs with different con-
straints. 3) How the various features affect the sys-
tem. Except for the last one, the feature set used for
different experiments are the same (?In+Out+Dep?
in Table 5). The criteria for evaluation are simi-
lar to the unlabeled attachment score in parser eval-
uations, but due to the equation |E| = |V | ? 1
is not valid if G is not a tree, we evaluate pre-
cision P = #true edges in result graph#edges in result graph , recall
R = #true edges in result graph#edges in true graph , and F-score
F = 2P ?RP+R .
4.3 Results
1. The effects of structural information. An alter-
native method to extract relations is directly using
a classifier to judge whether there is a relation be-
tween any two elements. Those kinds of methods
were used in previous opinion mining works (Wu
et al, 2009; Kobayashi et al, 2007). To show the
entire structural information is important for min-
ing relations, we use SVM for binary classification
on candidate pairs. The data point representing a
pair (xi, xj) is the same as the high dimensional fea-
ture vectors f(xi, xj). The setting of our algorithm
?MST+c1+c2+c3? is the basic MSTwith all the con-
straints. The results are shown in the Table 3.
P R F
SVM 64.9 24.0 35.0
MST+c1+c2+c3-m 61.5 74.0 67.2
MST+c1+c2+c3 73.1 71.0 72.1
Table 3: Binary classifier and structural learning
From the results, the performance of SVM (espe-
cially recall) is relatively poor. A possible reason
is that the huge imbalance of positive and negative
training samples (only ?(n) positive pairs among
all n2 pairs). And the absence of global structural
6http://sourceforge.net/projects/lpsolve/
1338
knowledge makes binary classifier unable to use
the information provided by classification results of
other pairs.
In order to examine whether the complicated sen-
timent representation would disturb the classifier in
finding relations between opinion expressions and
its target, we evaluate the system by discarding the
modifiers of opinion restriction and expansion from
the corpus. The result is shown in the second row of
Table 3. We observe that ?MST+c1+c2+c3? is still
better which means at least on overall performance
the additional modifiers do not harm.
2. The effect of constraints on inference algo-
rithm. In the inference algorithm, we utilized the
properties of graph G and adapted the basic multi-
commodity flow ILP to our specific task. To evaluate
how the constraints affect the system, we decompose
the algorithm and combine them in different ways.
P R F
MST 69.3 67.3 68.3
MST+c1 70.0 68.0 69.0
MST+c2 69.8 67.8 68.8
MST+c1+c2 70.6 68.6 69.6
MST+c1+c3 72.4 70.4 71.4
MST+c1+c2+c3 73.1 71.0 72.1
MST+c1+c2+c3+g 72.5 72.3 72.4
Table 4: Results on inference methods. ?MST? is the ba-
sic multicommodity flow formulation of maximum span-
ning tree; c1, c2, c3 are groups of constraint from Section
3.2.2; ?g? is our heuristic method for additional non span-
ning tree edges.
From Table 4, we observe that with any additional
constraints the inference algorithm outperforms the
basic maximum spanning tree method. It implies al-
though we did not use high order model (e.g. involv-
ing grandparent and sibling features), prior struc-
tural constraints can also help to get a better out-
put graph. By comparing with different constraint
combinations, the constraints on opinion thread (c1,
c3) are more effective than constraints on evaluation
targets (c2). It is because opinion expressions are
more important in the entire sentiment representa-
tion. The main structure of a graph is clear once the
relations between opinion expressions are correctly
determined.
3. The effects of various features. We evaluate the
performances of different feature configurations in
Table 5. From the results, the outside feature set is
more effective than inside feature set, even if it does
not use any external resource. A possible reason is
that the content of a vertex can be very complicated
(a vertex even can be a clause), but the features sur-
rounding the vertex are relatively simple and easy
to identify (for example, a single preposition can
identify a complex condition). The dependency fea-
ture has limited effect, due to that lots of online re-
view sentences are ungrammatical and parsing re-
sults are unreliable. And the complexity of vertices
also messes the dependency feature.
P R F
In-s 66.3 66.3 66.3
In 66.7 66.4 66.6
Out 67.8 67.4 67.6
In+Out 72.0 70.5 71.0
In+Out+Dep 72.5 72.3 72.4
Table 5: Results with different features. ?In? repre-
sents the result of inside feature set; ?In-s? is ?In? with-
out the external opinion lexicon feature; ?Out? uses the
outside feature set; ?In+Out? uses both ?In? and ?Out?,
?In+Out+Dep? adds the dependency feature. The infer-
ence algorithm is ?MST+c1+c2+c3+g? in Table 4.
We analyze the errors in test results. A main
source of errors is the confusion of classifier be-
tween ?target? relations and ?coordination?, ?tran-
sition? relations. The reason may be that for a mod-
ification on opinion expression (r, dk), we allow
dk recursively has its own modifiers (Example 5).
Thus an opinion expression can be a modifier which
brings difficulties to classifier.
4. Extraction of vertices. Finally we conduct an
experiment on vertex extraction using standard se-
quential labeling method. The tag set is simply {B,
I, O} which are signs of begin, inside, outside of a
vertex. The underlying model is conditional random
field 7. Feature templates involved are in Table 6.
We only use basic features in the experiment. 10
folder cross validation results are in table 7. We sus-
pect that the performances (especially recall) could
be improved if some external resources(i.e. ontol-
ogy, domain related lexicon, etc.) are involved.
7We use CRF++ toolkit, http://crfpp.sourceforge.net/
1339
Unigram Template
ci.char character
ci.isDigit digit
ci.isAlpha english letter
ci.isPunc punctuation
ci.inDict in a sentiment word
ci.BWord start of a word
ci.EWord end of a word
Table 6: Features for vertex extraction. The sequential
labeling is conducted on character level (ci). The senti-
ment lexicon used in ci.inDict is the same as Table1. We
also use bigram feature templates on ci.char, ci.isAlpha,
ci.inDict with respect to ci?1 and ci+1.
P R F
E+Unigram 56.8 45.1 50.3
E+Unigram+Bigram 57.3 47.9 52.1
O+Unigram 71.9 57.2 63.7
O+Unigram+Bigram 72.3 60.2 65.6
Table 7: Results on vertices extraction with 10 folder
cross validation. We use two criterion: 1) the vertex is
correct if it is exactly same as ground truth(?E?), 2) the
vertex is correct if it overlaps with ground truth(?O?).
5 Related Work
Opinion mining has recently received considerable
attentions. Large amount of work has been done on
sentimental classification in different levels and sen-
timent related information extraction. Researches on
different types of sentences such as comparative sen-
tences (Jindal and Liu, 2006) and conditional sen-
tences (Narayanan et al, 2009) have also been pro-
posed.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. They used slots
to represent evaluations, converted the task to two
kinds of relation extraction tasks and proposed a ma-
chine learning-based method which used both con-
textual and statistical clues.
Jindal and Liu (2006) studied the problem of iden-
tifying comparative sentences. They analyzed dif-
ferent types of comparative sentences and proposed
learning approaches to identify them.
Sentiment analysis of conditional sentences were
studied by Narayanan et al (2009). They aimed
to determine whether opinions expressed on dif-
ferent topics in a conditional sentence are posi-
tive, negative or neutral. They analyzed the con-
ditional sentences in both linguistic and computi-
tional perspectives and used learning method to do
it. They followed the feature-based sentiment anal-
ysis model (Hu and Liu, 2004), which also use flat
frames to represent evaluations.
Integer linear programming was used in many
NLP tasks (Denis and Baldridge, 2007), for its
power in both expressing and approximating various
inference problems, especially in parsing (Riedel
and Clarke, 2006; Martins et al, 2009). Martins
etc. (2009) also applied ILP with flow formulation
for maximum spanning tree, besides, they also han-
dled dependency parse trees involving high order
features(sibling, grandparent), and with projective
constraint.
6 Conclusions
This paper introduces a representation method for
opinions in online reviews. Inspections on corpus
show that the information ignored in previous sen-
timent representation can cause incorrect or incom-
plete mining results. We consider opinion restric-
tion, opinion expansions, relations between opin-
ion expressions, and represent them with a directed
graph. Structural learning method is used to produce
the graph for a sentence. An inference algorithm is
proposed based on the properties of the graph. Ex-
perimental evaluations with a manually labeled cor-
pus are given to show the importance of structural
information and effectiveness of proposed inference
algorithm.
7 Acknowledgement
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
National Natural Science Foundation of China
(61003092, 61073069),863 Program of China
(2009AA01A346), Shanghai Science and Tech-
nology Development Funds(10dz1500104), Doc-
toral Fund of Ministry of Education of China
(200802460066), Shanghai Leading Academic Dis-
cipline Project (B114), and Key Projects in
the National Science & Technology Pillar Pro-
1340
gram(2009BAH40B04).
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: A semi-supervised approach to auto-
matic sentiment classification. In Proceedings of ACL-
IJCNLP.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In Proceedings
of ACL, pages 395?403, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of SIGKDD.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR.
R. Karp. 1972. Reducibility among combinatorial prob-
lems. In R. Miller and J. Thatcher, editors, Complex-
ity of Computer Computations, pages 85?103. Plenum
Press.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING-ACL.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL-IJCNLP.
R. Mcdonald and F. Pereira. 2005. Identifying gene
and protein mentions in text using conditional random
fields. BMC Bioinformatics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc. of EACL, pages 81?88.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of EMNLP 2002.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
1341
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1379?1388, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Part-of-Speech Tagging for Chinese-English Mixed Texts
with Dynamic Features
Jiayi Zhao? Xipeng Qiu? Shu Zhang? Feng Ji? Xuanjing Huang?
School of Computer Science, Fudan University, Shanghai, China ? ?
Fujitsu Research and Development Center, Beijing, China?
zjy.fudan@gmail.com?
{xpqiu,fengji,xjhuang}@fudan.edu.cn?
zhangshu@cn.fujitsu.com ?
Abstract
In modern Chinese articles or conversations,
it is very popular to involve a few English
words, especially in emails and Internet liter-
ature. Therefore, it becomes an important and
challenging topic to analyze Chinese-English
mixed texts. The underlying problem is how
to tag part-of-speech (POS) for the English
words involved. Due to the lack of specially
annotated corpus, most of the English words
are tagged as the oversimplified type, ?foreign
words?. In this paper, we present a method
using dynamic features to tag POS of mixed
texts. Experiments show that our method
achieves higher performance than traditional
sequence labeling methods. Meanwhile, our
method also boosts the performance of POS
tagging for pure Chinese texts.
1 Introduction
Nowadays, Chinese-English mixed texts are
prevalent in modern articles or emails. More and
more English words are used in Chinese texts as
names of organizations, products, terms and abbre-
viations, such as ?eBay?, ?iPhone?, ?GDP?, ?An-
droid? etc. On the other hand, it is also a common
phenomenon to use Chinese-English mixed texts
in daily conversation, especially in communication
among employers in large international corporations.
There are some challenges for analyzing Chinese-
English mixed texts:
1. How to define the POS tags for English words
in these mixed texts. Since the standard of
POS tags for English and Chinese are different,
we cannot use English POS to tag the English
words in mixed texts.
2. Due to lack of annotated corpus for mixed texts,
most of the English words are tagged as ?for-
eign words?, which is oversimplified. So we
cannot use them in further processing for the
syntactic and semantic analysis.
3. Most English words used in mixed texts are of-
ten out-of-vocabulary (OOV), which thus in-
creases the difficulties to tag them.
Currently, the mainstream method of Chinese
POS tagging is joint segmentation & tagging with
cross-labels, which can avoid the problem of error
propagation and achieve higher performance on both
subtasks(Ng and Low, 2004). Each label is the cross-
product of a segmentation label and a tagging la-
bel, e.g. {B-NN, I-NN, E-NN, S-NN, ...}. The fea-
tures are generated by position-based templates on
character-level.
Since the main part of mixed texts is in Chinese
and the role of English word is more like Chinese,
we use Chinese POS tags (Xia, 2000) to tag English
words. Since the categories of the most commonly
used English words are nouns, verbs and adjectives,
we can use ?NN?, ?NR?, ?VV?, ?VA?, ?JJ? to label
their POS tags.
For the English proper nouns and verbs, there
are no significant differences in Chinese and En-
glish POS tags except that English features plural
and tense forms.
For the English nouns, these are some English
nouns used as verbs, such as ??? [fan/VV]??(I
adore him very much.)? where ?fan? means ?adore?
and is used as a verb.
For the English adjectives, there are two corre-
sponding Chinese POS tags ?VA? and ?JJ?. For ex-
ample, the roles of some English words in Table 1,
1379
Table 1: The POS tags of English Adjectives in Mixed
Texts
Chinese English
? ? ? [profes-
sional/VA]?
I am very profes-
sional.
??? [high/VA]? Feel very high.
?? [super/JJ] [star/NN] He is a super star.
such as ?professional? and ?high?, are different with
their original ones.
Therefore, the POS tagging for mixed texts cannot
be settled with simple methods, such as looking up
in a dictionary.
One of the main differences between Chinese and
English in POS tagging is that the two languages
have character-based features and word-based fea-
tures respectively. To ensure the consistency of tag-
ging models, we prefer to use word-level informa-
tion in Chinese, which is both useful for Chinese-
English mixed texts and Chinese-only texts. For in-
stance, in a sentence ?X ?? Y ... (X or Y ...)?,
the word Y ought to have the same POS tag as the
word X . Another example is that the word follow-
ing a pronoun is usually a verb, and adjectives of-
ten describe nouns. Some related works show that
word-level features can improve the performance of
Chinese POS tagging (Jiang et al 2008; Sun, 2011).
In this paper, we propose a method to tag mixed
texts with dynamic features. Our method combines
these dynamic features, which are dynamically gen-
erated at the decoding stage, with traditional static
features. For Chinese-English mixed texts, the tra-
ditional features cannot yield a satisfied result due to
lack of training data. The proposed dynamic features
can improve the performance by using the informa-
tion of a word, such as POS tag or length of the whole
word, which is proven effective by experiments.
The rest of the paper is organized as follows: In
section 2, we introduce the sequence labeling mod-
els, thenwe describe our method of dynamic features
in section 3 and analyze its complexity in section 4.
Section 5 describes the training method. The exper-
imental results are manifested in section 6. Finally,
We review the relevant research works in section 7
and conclude our work in section 8.
2 Sequence Labeling Models
Sequence labeling is the task of assigning labels
y = y1, . . . , yn to an input sequence x = x1, . . . , xn.
Given a sample x, we define the feature ?(x, y).
Thus, we can label x with a score function,
y? = argmax
y
F (w,?(x, y)), (1)
where w is the parameter of function F (?).
For sequence labeling, the feature can be denoted
as?k(yi, yi?1, x, i), where i stands for the position in
the sequence and k stands for the number of feature
templates.
we use online Passive-Aggressive (PA) algorithm
(Crammer and Singer, 2003; Crammer et al 2006)
to train the model parameters. Following (Collins,
2002), the average strategy is used to avoid the over-
fitting problem.
3 Dynamic Features
The form of traditional features is shown in Table
2, where C represents a Chinese character, and T
represents the character-based tag. The subscript i
indicates its position related to the current character.
Table 2: Traditional Feature Templates
Ci, T0(i = ?2,?1, 0, 1, 2)
Ci, Cj , T0(i, j = ?2,?1, 0, 1, 2 and i ?= j)
T?1, T0
Traditional features are generated by position-
fixed templates. Since the length of Chinese word
is unfixed, their meanings are incomplete. We cat-
egorize them as ?static? features since they can be
calculated before tagging (except ?T?1, T0?).
The form of dynamic features is shown in Table
3, where WORD represents a Chinese word, and
POS (LEN ) is the POS tag (length) of the word.
The subscript of dynamic feature template indicates
its position related to the current word.
Table 4 shows an example. If the current posi-
tion is ? Apple?, then {POS?1=CC, POS?2=NR,
WORD?1=???, LEN?2=2}. Since these features
are unavailable before tagging, we call them ?dy-
namic? features.
1380
Table 3: Examples of Dynamic Feature Templates
POSi, POSj , T0(i, j = ?2,?1, 0 and i ?= j)
POSi,WORDj , T0(i, j = ?2,?1, 0)
WORDi, LENj , POSk, T0(i, j, k = ?2,?1, 0)
?
Dynamic features are more flexible because the
number of involved characters is dependent on the
length of previous words. Unlike static features, dy-
namic features do not merely rely on the input se-
quence C1:n, so the weights of dynamic features, in
which POS/LEN are involved, can be trained by
Chinese-only texts and used by mixed texts, which
resolve the problem of the lack of training data.
4 Tagging with Dynamic Features
In the tagging stage, we use the current best result
to approximately calculate the unknown tag infor-
mation. For an input sequence C1:n, the current best
tags from index 0 to i?1 can be calculated by Viterbi
algorithm and they can be used to generate dynamic
features for index i. The specific algorithm is shown
in Algorithm 1.
Here is an example to explain the time com-
plexity of the dynamic features. Normal template
xi?2xi?1yi requires to look for the positions of
i ? 2 and i ? 1 related to the current character
xi, but dynamic template posi?2posi?1yi needs to
know the pos tags of two words. If the length of
wordi?1/wordi?2 is 2, then the positions of i?4, i?
3, i?2, i?1 are needed to generate the dynamic fea-
tures.
For all dynamic features, it is unnecessary to
repetitively calculate the POS/WORD/LEN ar-
ray. Apart from that one time calculation of the ar-
ray, no distinction can be found between the time
complexity of the dynamic features and the tradi-
tional features. For input C1:n, the time complexity
isO(n?[O(op.2)+(Ts.num+Td.num)?O(op.1)+
O(op.4)]), n.b. O(op.1) = O(op.3). Universally
the dynamic features only require the information of
position i ? 2 and i ? 1, so the time complexity of
calculating the POS/WORD/LEN array can be
ignored as compared with the complexity of Viterbi
algorithm and feature extraction. The approximate
algorithm is thus faster than the Brute-Force way by
input : character sequence C1:n
static templates Ts
dynamic templates Td
number of labelsm
trans matrixM
output: resultsMax & Vp
Initialize: weight matrixW (n?m)
viterbi score matrix Vs (n?m)
viterbi path matrix Vp (n?m)
the index of current best labelMax
for i = 1 ? ? ?n do
for ts in Ts do
// create feature string Fs (Op.1)
Fs = createFeature(C1:n, ts);
W [i] += getWeightVector(Fs);
end
// create a list of <posk,wordk,lenk>
// (k = 0,?1,?2 . . .) (Op.2)
dList = getCurrentBestPath(Max, Vp);
for td in Td do
// create dynamic features string Fd
// (Op.3)
Fd = createFeature(C1:n, td, dList);
W [i] += getWeightVector(Fd);
end
// Update Vs[i], Vp[i] (Op.4)
viterbi_OneStep(Vs[i? 1],W [i],M );
Max = argmaxi(Vs[i]) ;
end
Algorithm 1: Tagging Algorithm with Dynamic
Features
using word-level information.
5 Training
Given an example (x, y), y? are denoted as the in-
correct labels with the highest score
y? = argmax
z ?=y
wT?(x, z). (2)
The margin ?(w; (x, y)) is defined as
?(w; (x, y)) = wT?(x, y)? wT?(x, y?). (3)
Thus, we calculate the hinge loss ?(w; (x, y), (ab-
breviated as ?w) by
1381
Table 4: Example for Chinese-English Mixed POS Tagging
? ? ? Apple ? OS ? ? ? ? ?
B-NR E-NR S-CC S-NR S-DEG S-NN B-NN E-NN B-VA E-VA S-PU
?w =
{
0, ?(w; (x, y)) > 1
1? ?(w; (x, y)), otherwise (4)
In round k, the new weight vector wk+1 is calcu-
lated by
wk+1 = argminw
1
2
||w? wk||2 + C ? ?,
s.t. ?(w; (xk, yk)) <= ? and ? >= 0 (5)
where ? is a non-negative slack variable, and C is
a positive parameter which controls the influence of
the slack term on the objective function.
Following the derivation in PA (Crammer et al
2006), we can get the update rule,
wk+1 = wk + ?k(?(xk, yk)? ?(xk, y?k)), (6)
where
?k = min(C,
?wk
??(xk, yk)? ?(xk, y?k)?2
) (7)
Our algorithm based on PA algorithm is shown in
Algorithm 2.
6 Experiments
We implement our system based on FudanNLP1.
We employ the commonly used label set {B, I, E,
S} for the segmentation part of cross-labels. {B,
I, E} represent Begin, Inside, End of a multi-node
segmentation respectively, and S represents a Single
node segmentation.
The F1 score is used for evaluation, which is the
harmonic mean of precision P (percentage of pre-
dict phrases that exactlymatch the reference phrases)
and recallR (percentage of reference phrases that re-
turned by system).
The feature templates, which are used to extract
features, are listed in Table 5. We set traditional
method (static features) as the baseline. The detailed
experimental settings and results are reported in the
following subsections.
1Available at http://code.google.com/p/fudannlp/
input : training data sets:
(xi, yi), i = 1, ? ? ? , N , and parameters:
C,K
output: wK
Initialize: wTemp? 0,w? 0;
for k = 0 ? ? ?K ? 1 do
for i = 1 ? ? ?N do
receive an example (xi, yi);
predict: y?i = argmax
y
?wk,?(xi, y)?;
if y?i ?= yi then
update wk+1 with Eq. 6;
end
end
wTemp = wTemp+ wk+1 ;
end
wK = wTemp/K ;
Algorithm 2: Training Algorithm
Table 5: Feature Templates
Static
xi?2yi, xi?1yi, xiyi, xi+1yi, xi+2yi
xi?1xiyi, xi+1xiyi, xi?1xi+1yi,
yi?1yi
Dynamic
posi?2posi?1yi, posi?1posiyi
posi?2wordi?1yi, posi?1wordiyi
posi?1wordi?1yi, posiwordiyi
wordi?2wordi?1yi, wordi?1wordiyi
wordileniyi
6.1 POS Tagging for Chinese-only Texts
Before the experiments onChinese-Englishmixed
texts, we evaluate the performance of our method on
Chinese-only texts. We use the CTB dataset from
the POS tagging task of the Fourth International Chi-
nese Language Processing Bakeoff (SIGHAN Bake-
off 2008)(Jin and Chen, 2008). The details are
shown in Table 6.
The performance comparison on joint segmenta-
tion & POS tagging is shown in Table 7. Our method
obtains an error reduction of 6.7% over the baseline.
The reason is that our dynamic features can utilize
1382
Table 6: POS Tagging Dataset in SIGHAN Bakeoff 2008
Train Set Test Set
(number) (number)
Sentence 23444 2079
Word
Total 642246 59955
NN 168896 16793
NR 42906 3970
VV 92887 8641
VA 9106 649
JJ 15640 1581
word-level information effectively and the feature
templates are more flexible.
Table 7: Performances of POS Tagging on Chinese-only
Texts with Static and Dynamic Features
Method P R F1
Baseline 89.68 89.60 89.64
Our 90.35 90.31 90.33
6.2 POS Tagging for Chinese-English Mixed
Texts
Without annotated corpus for Chinese-English
mixed texts, we use synthetic data as the alternative.
In Chinese-English mixed texts, English words of
noun(NN/NR), verb(VV/VA) and adjective(JJ) cat-
egories are the most commonly used, so we ran-
domly transform a certain percentage of Chinese
words with these POS tags in the SIGHAN Bakeoff
2008 dataset(Jin and Chen, 2008) into their English
counterparts.
6.2.1 Synthetic Data
Before trying out an experiment, we first study
how to generate the data of mixed texts.
We use two ways to produce the synthetic data:
?Respective Replacement? and ?Unified Replace-
ment?.
Respective Replacement We replace the selected
Chinese words into their corresponding English
counterparts.
Unified Replacement We replace the selected Chi-
nese words with a unified labelENG. The rea-
son we use the labelENG instead of real words
is that we want to consider the context of these
words but not the words themselves and over-
come the problem of out-of-vocabulary (OOV)
English words.
For our experiments, we just select 5% of the Chi-
nese nouns and verbs from SIGHAN dataset, and re-
place them in the above two ways. After replace-
ment, the training and test data have 12780 and 1254
English words, respectively. 5189 words are gener-
ated by way of ?Respective Replacement?. In the
test data, 326words are OOV, which comprises 25%
of the whole vocabulary. The information of gener-
ated data is shown in Table 8.
Table 8: The Synthetic Chinese-English Mixed Dataset
H
Dataset Numbers of ENGNN VV
H Train Set 8191 4589Test Set 842 412
We use H1 to represent the dataset generated by
way of ?Respective Replacement?, and H2 for the
dataset by way of with ?Unified Replacement?. The
experimental results on these two datasets are shown
in Table 9.
Table 9: Performances of POS Tagging on Dataset H1
and H2
Method Dataset ENG OOV TotalF1 F1oov F1
Baseline H1 73.60 54.91 88.93H2 77.59 73.93 89.11
Our H1 75.60 54.60 89.79H2 79.82 77.61 89.81
From Table 9, we can see that the ?Unified
Replacement? way is better than the ?Respective
Replacement? way for both the baseline and our
method. The main reason is that the ?Unified Re-
placement? way can greatly improve the tagging per-
formance of OOV words.
6.2.2 Detail Comparisons
For detail comparisons of all situations of
mixed texts, we design six synthetic datasets,
A/B/C/D1/D2/E by randomly selecting 10% or
15% of Chinese words (?NN/NR/VV/VA/JJ?) in the
1383
above SIGHANBakeoff 2008 dataset, and replacing
them with English label ENG.
The differences of these datasets are as following:
? Dataset A only contains English words with
tags ?NN/VV?.
? Dataset B contains English words with tags
?NN/VV/VA?.
? Dataset C contains one more tag ?NR? than
Dataset B.
? Datasets D1 and D2 contain one more tag ?JJ?
than Dataset B. The difference between D1
andD2 is thatD2 has about 50%more English
words than D1 in training set.
? Dataset E contains English words with all the
tags ?NN/NR/VV/VA/JJ?.
The detailed information of datasets
A/B/C/D1/D2/E is shown in Table 10.
Table 10: The Synthetic Chinese-English Mixed Dataset
Dataset Numbers of ENGNN NR VV VA JJ
A Train 16302 0 9007 0 0Test 1675 0 841 0 0
B Train 16116 0 8882 906 0Test 1573 0 830 58 0
C Train 16312 4057 9067 899 0Test 1549 400 795 61 0
D1
Train 16042 0 8957 855 1539
Test 1588 0 845 58 150
D2
Train 23705 0 13154 1300 2211
Test 1588 0 845 58 150
E Train 16066 4162 9156 886 1547Test 1647 415 809 57 141
The results are shown in Table 11. On dataset E,
our method achieves 6.78% higher performance on
tagging ENG labels than traditional static features.
This result is reasonable because our model can use
more flexible feature templates to extract features
and reduce the problem of being dependent on spe-
cific English words.
Tables 12/13/14/15/16/17 show the detailed re-
sults on datasets A/B/C/D1/D2/E.
Table 11: Performances of POS Tagging on Datasets
A/B/C/D1/D2/E
Dataset Method ENG labels TotalF1 F1
A Baseline 80.25 88.74Our 83.03 89.72
B Baseline 76.72 88.51Our 80.54 89.55
C Baseline 68.16 88.13Our 70.34 88.99
D1
Baseline 71.30 88.33
Our 74.02 89.15
D2
Baseline 69.59 88.09
Our 74.10 89.15
E Baseline 61.58 87.71Our 68.36 88.83
Experiment on dataset A gets the best result be-
cause ?NN? and ?VV? can be easily distinguished by
its context. Sometimes, ?VA? has the similar context
with ?VV?, experiment on datasetB shows its influ-
ence. The performances on datasetsB/C/E descend
in turn. The reason is that words with tag ?NN? or
?NR/JJ? have the similar usage/contexts in Chinese.
Since we use the same form ENG instead of real
words, there are no differences between these words,
which leads to some errors. Though the datasets is
generated randomly, we can see our method perform
better on every dataset than the baseline.
Table 12: Performances on Dataset A
POS tag Method P R F1
NN Baseline 84.36 86.33 85.33Our 85.37 89.91 87.58
VV Baseline 71.45 68.13 69.75Our 77.53 69.32 73.20
Table 13: Performances on Dataset B
POS tag Method P R F1
NN Baseline 84.89 80.36 82.56Our 83.51 88.87 86.11
VV Baseline 65.90 72.65 69.11Our 75.75 67.35 71.30
VA Baseline 36.84 36.21 36.52Our 51.02 43.10 46.73
1384
Table 14: Performances on Dataset C
POS tag Method P R F1
NN Baseline 73.77 78.24 75.94Our 76.84 77.99 77.41
VV Baseline 61.67 66.79 64.13Our 64.94 67.80 66.34
NR Baseline 55.22 37.00 44.31Our 55.65 50.50 52.95
VA Baseline 63.64 34.43 44.68Our 60.00 39.34 47.52
Table 15: Performances on DatasetD1
POS tag Method P R F1
NN Baseline 77.15 81.42 79.23Our 76.70 88.54 82.20
VV Baseline 67.53 64.50 65.98Our 79.65 59.76 68.29
JJ Baseline 25.00 18.00 20.93Our 22.92 14.67 17.89
VA Baseline 36.00 31.03 33.33Our 28.57 37.93 32.59
Table 16: Performances on DatasetD2
POS tag Method P R F1
NN Baseline 79.11 74.87 76.93Our 79.29 82.68 80.95
VV Baseline 55.77 72.78 65.64Our 69.17 70.89 70.02
JJ Baseline 27.27 12.00 16.67Our 34.38 22.00 26.83
VA Baseline 37.21 27.59 31.68Our 52.17 20.69 29.63
6.3 POS Tagging for Mixed Texts with a Real
Dataset
To investigate the actual performance, we collect
a real dataset from Web, which consists of 142 rep-
resentative Chinese-English mixed sentences. This
dataset contains 4, 238 Chinese characters and 275
English words. Since we focus on the performance
for English words, we only label the POS tags of the
English words. Table 18 shows some examples in
the real dataset of mixed texts.
Table 17: Performances on Dataset E
POS tag Method P R F1
NN Baseline 72.41 68.85 70.59Our 71.18 84.88 77.43
VV Baseline 63.65 59.09 61.28Our 76.19 55.38 64.14
JJ Baseline 28.57 25.53 26.97Our 30.21 20.57 24.47
VA Baseline 44.83 45.61 45.22Our 60.42 50.88 55.24
NR Baseline 38.03 52.05 43.95Our 52.01 46.75 49.24
Table 18: Examples in Real Dataset of Mixed Texts
?? [Ninja Cloud/NR] ????? [Ninja
Blocks/NR] ? ? [Facebook/NR]? [Twit-
ter/NR]?[Dropbox/NR]??????
By using [Ninja Cloud/NR], [Ninja
Blocks/NR] can connect to [Facebook/NR],
[Twitter/NR], [Dropbox/NR].
?? [follow/VV]?????????
You should [follow/VV] this man?s work.
?????????? [COOL/VA]?
... very [COOL/VA]!
The information of the real dataset is shown in Ta-
ble 19. If all involved English words are tagging as
?NN?, the precision is just 56%.
Table 19: The Numbers of English Words with Different
Tags in Dataset R
Dataset NN VV VA NR
R 154 58 28 35
Since there is no noun-modifier ?JJ? in our col-
lected data. We use the models trained on dataset
B and C to tag the real data. The results are shown
in Table 20. The difference between model B and
C is that model B regards all words with tag ?NR?
as ?NN?. Since it is difficult to distinguish between
?NR? and ?NN? merely according to the context,
model B performs better than model C.
The detail results of model B and C are shown in
Table 21 and 22.
1385
Table 20: Performances of POS Tagging on R
Model Method ENGF1
B Baseline 74.91Our 82.55
C Baseline 70.91Our 74.91
Table 21: Performances of Model B on Dataset R
POS tag Method P R F1
NN Baseline 88.62 78.31 83.15Our 91.67 87.30 89.43
VV Baseline 48.31 74.14 58.50Our 60.53 79.31 68.66
VA Baseline 78.95 53.57 63.83Our 84.21 57.14 68.09
Table 22: Performances of Model C on Dataset R
POS tag Method P R F1
NN Baseline 80.25 81.82 81.03Our 84.56 81.82 83.17
VV Baseline 54.88 77.59 64.29Our 61.25 84.48 71.01
VA Baseline 84.62 39.29 53.66Our 88.24 53.57 66.67
NR Baseline 56.52 37.14 44.83Our 55.17 45.71 50.00
7 Related Works
In recent years, POS tagging has undergone great
development. The mainstream method is to regard
POS tagging as sequence labeling problems (Ra-
biner, 1990; Xue, 2003; Peng et al 2004; Ng and
Low, 2004).
However, the analysis of Chinese-English mixed
texts is rarely involved in previous literature. In
the aspect of the general multilingual POS tagging,
most works focus on modeling cross-lingual corre-
lations and tagging multilingual POS on respective
monolingual texts, not on mixed texts (Cucerzan and
Yarowsky, 2002; Yarowsky et al 2001; Naseem et
al., 2009).
Since we choose to use dynamic word-level fea-
tures to improve the performance of POS tagging,
we also review some works on word-level features.
Semi-Markov Conditional Random Fields (semi-
CRF) (Sarawagi and Cohen, 2004) is a model in
which segmentation task is implicitly included into
the decoding algorithm. In this model, feature rep-
resentation would be more flexible than traditional
CRFs, since features can be extracted from the previ-
ous/the next segmentation within a window of vari-
able size. The problem of this approach lies in that
the decoding algorithm depends on the predefined
window size to exploit the boundaries of segmenta-
tions but not the real length of words.
Bunescu (2008) presents an improved pipeline
model in which the output of the previous subtasks
are considered as hidden variables, and the hidden
variables together with their probabilities denoting
the confidence are used as probabilistic features in
the next subtasks. One shortcoming of this method
is inefficiency caused by the calculation of marginal
probabilities of features. The other disadvantages
of the pipeline method are error propagation and the
need of separate training of different subtasks in the
pipeline. Another disadvantage of pipeline method
is error propagation.
Jiang et al(2008) proposes a cascaded linear
model for joint Chinese word segmentation and POS
tagging. With a character-based perceptron as the
core, combinedwith real-valued features such as lan-
guage models, the cascaded model can efficiently
utilize knowledge sources that are inconvenient to
incorporate into the perceptron directly. However,
they use POS tags or word information in a Brute-
Force way, which may suffer from the problem of
time complexity.
Sun (2011) presents a stacked sub-word model for
joint Chinese word segmentation and POS tagging.
By merging the outputs of the three predictors (in-
cluding one word-based segmenter) into sub-word
sequences, rich contextual features can be approx-
imately derived. The experiments are conducted to
show the effectiveness of using word-based informa-
tion.
The difference between the above methods and
ours is that our word-level features are dynamically
generated in the decoding stage without exhaustive
or preprocessed word segmentation.
1386
8 Conclusion
In this paper, we focus on Chinese-English mixed
texts and use dynamic features for POS tagging.
To overcome the problem of the lack of annotated
corpus on mixed texts, our features use both lo-
cal and non-local information and take advantage of
the characteristics of Chinese-English mixed texts.
The experiments demonstrate the effectiveness of
our method. It should be noted that our method is
also effective for the mixed texts of Chinese and any
foreign languages since we use ?Unified Replace-
ment?.
For future works, we plan to improve our approx-
imate tagging algorithm to reduce error propagation.
In addition, we will refer to an English dictionary
to generate some useful features to distinguish be-
tween ?NR? and ?NN? in Chinese-English mixed
texts and add some statistical features derived from
English resources, such as the most common tag of
each English word. We would also like to investi-
gate these features in more applications of natural
language processing, such as name entity recogni-
tion, information extraction, etc.
Acknowledgements
We would like to thank the anonymous reviewers
for their valuable comments. We also thanks Amy
Zhou for her help in spell and grammar checking.
This work was funded by NSFC (No.61003091 and
No.61073069), 863 Program (No.2011AA010604)
and 973 Program (No.2010CB327900).
References
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In EMNLP,
pages 670?679. ACL.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, EMNLP ?02, pages
1?8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. J. Mach. Learn. Res.,
7:551?585, December.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In proceedings of the 6th conference on
Natural language learning - Volume 20, COLING-
02, pages 1?7, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Kath-
leen McKeown, Johanna D. Moore, Simone Teufel,
James Allan, and Sadaoki Furui, editors, ACL, pages
897?904. The Association for Computer Linguistics.
C. Jin and X. Chen. 2008. The fourth international chi-
nese language processing bakeoff: Chinese word seg-
mentation, named entity recognition and chinese pos
tagging. In Sixth SIGHAN Workshop on Chinese Lan-
guage Processing, page 69.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two unsu-
pervised approaches. Journal of Artificial Intelligence
Research, 36(1):341?385.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: One-at-a-time or all-at-once? word-based or
character-based. In Proceedings of EMNLP, volume
2004, page 277.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Lawrence R. Rabiner. 1990. Readings in speech recog-
nition. chapter A tutorial on hidden Markov mod-
els and selected applications in speech recognition,
pages 267?296. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In NIPS.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Dekang Lin, Yuji Matsumoto, and Rada
Mihalcea, editors, ACL, pages 1385?1394. The Asso-
ciation for Computer Linguistics.
F. Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0).
N. Xue. 2003. Chinese word segmentation as character
tagging. Computational Linguistics and Chinese Lan-
guage Processing, 8(1):29?48.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of
1387
the first international conference on Human language
technology research, pages 1?8. Association for Com-
putational Linguistics.
1388
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 658?668,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Chinese Word Segmentation and POS Tagging on
Heterogeneous Annotated Corpora with Multiple Task Learning
Xipeng Qiu, Jiayi Zhao, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, zjy.fudan@gmail.com, xjhuang@fudan.edu.cn
Abstract
Chinese word segmentation and part-of-
speech tagging (S&T) are fundamental
steps for more advanced Chinese language
processing tasks. Recently, it has at-
tracted more and more research interests
to exploit heterogeneous annotation cor-
pora for Chinese S&T. In this paper, we
propose a unified model for Chinese S&T
with heterogeneous annotation corpora.
We first automatically construct a loose
and uncertain mapping between two rep-
resentative heterogeneous corpora, Penn
Chinese Treebank (CTB) and PKU?s Peo-
ple?s Daily (PPD). Then we regard the
Chinese S&T with heterogeneous corpora
as two ?related? tasks and train our model
on two heterogeneous corpora simultane-
ously. Experiments show that our method
can boost the performances of both of the
heterogeneous corpora by using the shared
information, and achieves significant im-
provements over the state-of-the-art meth-
ods.
1 Introduction
Currently, most of statistical natural language
processing (NLP) systems rely heavily on manu-
ally annotated resources to train their statistical
models. The more of the data scale, the better
the performance will be. However, the costs are
extremely expensive to build the large scale re-
sources for some NLP tasks. Even worse, the ex-
isting resources are often incompatible even for a
same task and the annotation guidelines are usu-
ally different for different projects, since there
are many underlying linguistic theories which
explain the same language with different per-
spectives. As a result, there often exist multi-
ple heterogeneous annotated corpora for a same
task with vastly different and incompatible an-
notation philosophies. These heterogeneous re-
sources are waste on some level if we cannot fully
exploit them.
However, though most of statistical NLP
methods are not bound to specific annota-
tion standards, almost all of them cannot deal
simultaneously with the training data with
different and incompatible annotation. The
co-existence of heterogeneous annotation data
therefore presents a new challenge to utilize
these resources.
The problem of incompatible annotation stan-
dards is very serious for many tasks in NLP,
especially for Chinese word segmentation and
part-of-speech (POS) tagging (Chinese S&T). In
Chinese S&T, the annotation standards are of-
ten incompatible for two main reasons. One is
that there is no widely accepted segmentation
standard due to the lack of a clear definition
of Chinese words. Another is that there are no
morphology for Chinese word so that there are
many ambiguities to tag the parts-of-speech for
Chinese word. For example, the two commonly-
used corpora, PKU?s People?s Daily (PPD) (Yu
et al, 2001) and Penn Chinese Treebank (CTB)
(Xia, 2000), use very different segmentation and
POS tagging standards.
For example, in Table 1, it is very different
to annotate the sentence ?????????
?? (Liu Xiang reaches the national final in
China)? with guidelines of CTB and PDD. PDD
breaks some phrases, which are single words in
658
Liu Xiang reachs China final
CTB ??/NR ??/VV ???/NN ???/NN
PDD ?/nrf ?/nrg ??/v ??/ns ?/n ?/b ??/vn
Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD
CTB, into two words. The POS tagsets are also
significantly different. For example, PDD gives
diverse tags ?n? and ?vn? for the noun, while
CTB just gives ?NN?. For proper names, they
may be tagged as ?nr?, ?ns?, etc in PDD, while
they are just tagged as ?NR? in CTB.
Recently, it has attracted more and more re-
search interests to exploit heterogeneous anno-
tation data for Chinese word segmentation and
POS tagging. (Jiang et al, 2009) presented a
preliminary study for the annotation adapta-
tion topic. (Sun and Wan, 2012) proposed a
structure-based stacking model to fully utilize
heterogeneous word structures. They also re-
ported that there is no one-to-one mapping be-
tween the heterogeneous word classification and
the mapping between heterogeneous tags is very
uncertain.
These methods usually have a two-step pro-
cess. The first step is to train the preliminary
taggers on heterogeneous annotations. The sec-
ond step is to train the final taggers by using
the outputs of the preliminary taggers as fea-
tures. We call these methods as ?pipeline-
based? methods.
In this paper, we propose a method for joint
Chinese word segmentation and POS tagging
with heterogeneous annotation corpora. We re-
gard the Chinese S&T with heterogeneous cor-
pora as two ?related? tasks which can improve
the performance of each other. Since it is impos-
sible to establish an exact mapping between two
annotations, we first automatically construct a
loose and uncertain mapping the heterogeneous
tagsets of CTB and PPD. Thus we can tag a sen-
tence in one style with the help of the ?related?
information in another heterogeneous style. The
proposed method can improve the performances
of joint Chinese S&T on both corpora by using
the shared information of each other, which is
proven effective by experiments.
There are three main contributions of our
model:
? First, we regard these two joint S&T tasks
on different corpora as two related tasks
which have interdependent and peer rela-
tionship.
? Second, different to the pipeline-based
methods, our model can be trained simul-
taneously on the heterogeneous corpora.
Thus, it can also produce two different
styles of POS tags.
? Third, our model do not depend on the
exactly correct mappings between the two
heterogeneous tagsets. The correct map-
ping relations can be automatically built in
training phase.
The rest of the paper is organized as follows:
We first introduce the related works in section 2
and describe the background of character-based
method for joint Chinese S&T in section 3. Sec-
tion 4 presents an automatic method to build
the loose mapping function. Then we propose
our method on heterogeneous corpora in 5 and
6. The experimental results are given in section
7. Finally, we conclude our work in section 8.
2 Related Works
There are some works to exploit heteroge-
neous annotation data for Chinese S&T.
(Gao et al, 2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
However, this converter need human designed
transformation templates, and is hard to be gen-
eralized to POS tagging.
(Jiang et al, 2009) proposed an automatic
adaptation method of heterogeneous annotation
standards, which depicts a general pipeline to in-
tegrate the knowledge of corpora with different
659
TaggerPPD
TaggerCTB
Input: x
Output: f(x)
Output: CTB-style Tags
z=f(x)
y=h(x,f(x))
Figure 1: Traditional Pipeline-based Strategy for
Heterogeneous POS Tagging
underling annotation guidelines. They further
proposed two optimization strategies, iterative
training and predict-self re-estimation, to fur-
ther improve the accuracy of annotation guide-
line transformation (Jiang et al, 2012).
(Sun and Wan, 2012) proposed a structure-
based stacking model to fully utilize heteroge-
neous word structures.
These methods regard one annotation as the
main target and another annotation as the com-
plementary/auxiliary purposes. For example, in
their solution, an auxiliary tagger TaggerPPD
is trained on a complementary corpus PPD, to
assist the target CTB-style TaggerCTB. To re-
fine the character-based tagger, PPD-style char-
acter labels are directly incorporated as new
features. The brief sketch of these methods is
shown in Figure 1.
The related work in machine learning liter-
ature is multiple task learning (Ben-David and
Schuller, 2003), which learns a problem together
with other related problems at the same time,
using a shared representation. This often leads
to a better model for the main task, because
it allows the learner to use the commonality
among the tasks. Multiple task learning has
been proven quite successful in practice and has
been also applied to NLP (Ando and Zhang,
2005). We also preliminarily verified that mul-
tiple task learning can improve the performance
on this problem in our previous work (Zhao et
al., 2013), which is a simplified case of the work
in this paper and has a relative low complexity.
Different with the multiple task learning,
whose tasks are actually different labels in the
same classification task, our model utilizes the
shared information between the real different
tasks and can produce the corresponding differ-
ent styles of outputs.
3 Joint Chinese Word Segmentation
and POS Tagging
Currently, the mainstream method of Chi-
nese POS tagging is joint segmentation & tag-
ging with character-based sequence labeling
models(Lafferty et al, 2001), which can avoid
the problem of segmentation error propagation
and achieve higher performance on both sub-
tasks(Ng and Low, 2004; Jiang et al, 2008; Sun,
2011; Qiu et al, 2012).
The label of each character is the cross-
product of a segmentation label and a tagging
label. If we employ the commonly used label set
{B, I, E, S} for the segmentation part of cross-
labels ({B, I, E} represent Begin, Inside, End of
a multi-node segmentation respectively, and S
represents a Single node segmentation), the la-
bel of character can be in the form of {B-T}(T
represents POS tag). For example, B-NN indi-
cates that the character is the begin of a noun.
4 Automatically Establishing the
Loose Mapping Function for the
Labels of Characters
To combine two human-annotated corpora,
the relationship of their guidelines should be
found. A mapping function should be estab-
lished to represent the relationship between two
different annotation guidelines. However, the
exact mapping relations are hard to establish.
As reported in (Sun and Wan, 2012), there is
no one-to-one mapping between their heteroge-
neous word classification, and the mapping be-
tween heterogeneous tags is very uncertain.
Fortunately, there is a loose mapping
can be found in CTB annotation guide-
line1 (Xia, 2000). Table 2 shows some
1Available at http://www.cis.upenn.edu/ ?chi-
660
CTB?s Tag PDD? Tag1
Total tags 33 26
verbal noun NN v[+nom]
proper noun NR n
? (shi4) VC v
? (you3) VE, VV v
conjunctions CC, CS c
other verb VV, VA v, a, z
number CD, OD m
1 The tag set of PDD just includes the 26 broad
categories in the mapping table. The whole tag set
of PDD has 103 sub categories.
Table 2: Examples of mapping between CTB and
PDD?s tagset
mapping relations in CTB annotation guide-
line. These loose mapping relations are
many-to-many mapping. For example, the
mapping may be ?NN/CTB?{n,nt,nz}/PDD?,
?NR/CTB?{nr,ns}/PDD?, ?v/PDD?{VV,
VA}/CTB? and so on.
We define T1 and T2 as the tag sets for two
different annotations, and t1 ? T1 and t2 ? T2
are the corresponding tags in two tag sets re-
spectively.
We first establish a loose mapping function
m : T1 ? T2 ? {0, 1} between the tags of CTB
and PDD.
m(t1, t2) =
{
1 if t1 and t2 have mapping relation
0 else
(1)
The mapping relations are automatically
build from the CTB guideline (Xia, 2000). Due
to the fact that the tag set of PPD used in
the CTB guideline is just broad categories, we
expand the mapping relations to include the
sub categories. If a PPD?s tag is involved
in the mapping, all its sub categories should
be involved. For example, for the mapping
?NR/CTB?nr/PDD?, the relation of NR and
nrf/nrg should be added in the mapping rela-
tions too (nrf/nrg belong to nr).
Since we use the character-based joint S&T
model, we also need to find the mapping func-
tion between the labels of characters.
nese/posguide.3rd.ch.pdf
In this paper, we employ the commonly used
label set {B, I, E, S} for the segmentation part
of cross-labels and the label of character can be
in the form of {B-T}(T represents POS tag).
Thus, each mapping relation t1 ? t2 can be
automatically transformed to four forms: B-
t1 ?B-t2, I-t1 ?I-t2, E-t1 ?E-t2 and S-t1 ?S-
t2. (?B-NR/CTB?{B-nr,B-ns}/PPD? for ex-
ample).
Beside the above transformation, we also
give a slight modification to adapt the dif-
ferent segmentation guidelines. For in-
stance, the person name ??? (Mo Yan)?
is tagged as ?B-NR, E-NR? in CTB but
?S-nrf, S-nrg? in PPD. So, some spe-
cial mappings may need to be added like
?B-NR/CTB?S-nrf/PPD?, ?E-NR/CTB?{S-
nrg, E-nrg}/PPD?, ?M-NR/CTB?{B-nrg, M-
nrg}/PPD? and so on. Although these spe-
cial mappings are also established automatically
with an exhaustive solution. In fact, we give seg-
mentation alignment only to proper names due
to the limitation of computing ability.
Thus, we can easily build the loose bidirec-
tional mapping function m? for the labels of
characters. An illustration of our construction
flowchart is shown in Figure 2.
Finally, total 524 mappings relationships are
established.
5 Joint Chinese S&T with
Heterogeneous Data with Multiple
Task Learning
Inspired by the multiple task learning (Ben-
David and Schuller, 2003), we can regard the
joint Chinese S&T with heterogeneous data as
two ?related? tasks, which can improve the
performance of each other simultaneously with
shared information.
5.1 Sequence Labeling Model
We first introduce the commonly used se-
quence labeling model in character-based joint
Chinese S&T.
Sequence labeling is the task of assigning la-
bels y = y1, . . . , yn(yi ? Y) to an input sequence
x = x1, . . . , xn. Y is the set of labels.
661
PPD-style
CTB-style NR
nr
NR
nrf nrg
B-NR S-NR...
B-nrf B-nrg S-nrgS-nrg...
mapping function m() 
between tags
mapping function m() 
between labels
~
Figure 2: An Illustration of Automatically Establishing the Loose Mapping Function
Given a sample x, we define the feature
?(x,y). Thus, we can label x with a score func-
tion,
y? = arg max
y
S(w,?(x,y)), (2)
where w is the parameter of score function S(?).
The feature vector ?(x,y) consists of lots of
overlapping features, which is the chief benefit of
discriminative model. Different algorithms vary
in the definition of S(?) and the corresponding
objective function. S(?) is usually defined as lin-
ear or exponential family function.
For first-order sequence labeling, the feature
can be denoted as ?k(x, yi?1:i), where i stands
for the position in the sequence and k stands for
the number of feature templates. For the linear
classifier, the score function can be rewritten in
detail as
y? = arg max
y
L
?
i=1
(?u, f(x, yi)?+ ?v,g(x, yi?1:i)?) ,
(3)
where yi:j denotes label subsequence
yiyi+1 ? ? ? yj ; f and g denote the state and
transition feature vectors respectively, u and v
are their corresponding weight vectors; L is the
length of x.
5.2 The Proposed Model
Different to the single task learning, the het-
erogeneous data have two sets of labels Y and
Z.
The heterogeneous datasets Ds and Ds con-
sist of {xi,yi}(i = 0, ? ? ? ,m) and {xi, zi}(i =
0, ? ? ? , n) respectively.
For a sequence x = x1, . . . , xL with length
L. , there may have two output sequence labels
y = y1, . . . , yL and z = z1, . . . , zL, where yi ? Y
and zi ? Z.
We rewrite the loose mapping function m? be-
tween two label sets into the following forms,
?(y) = {z|m?(y, z) = 1}, (4)
?(z) = {y|m?(y, z) = 1}, (5)
where ?(z) ? Y and ?(y) ? Z are the subsets
of Y and Z. Give a label y(or z) in an annota-
tion, the loose mapping function ? returns the
corresponding mapping label set in another het-
erogeneous annotation.
Our model for heterogeneous sequence label-
ing can be write as
y? = arg max
y,yi?Y
L
?
i=1
(
?u, f(x, yi)?
+ ?s,
?
z??(yi)
h(x, z)?
+ ?v1,g1(x, yi?1:i)?
+ ?v2,
?
zi?1??(yi?1)
zi??(yi)
g2(x, zi?1:i)?
)
, (6)
and
z? = arg max
z,zi?Z
L
?
i=1
(
?u,
?
y??(zi)
f(x, y)?+
?s,h(x, zi)?
+ ?v1,
?
yi?1??(zi?1)
yi??(zi)
g1(x, yi?1:i)?
+ ?v2,g2(x, zi?1:i)?
)
, (7)
where f and h represent the state feature vectors
on two label sets Y and Z respectively.
In Eq.(6) and (7), the score of the label of
every character is decided by the weights of the
corresponding mapping labels and itself.
662
Input sequence: x
Output: PPD-style Tags
TaggerPPD TaggerCTBSharedInformation
Output: CTB-style Tags
Figure 3: Our model for Heterogeneous POS Tagging
The main challenge of our model is the effi-
ciency of decoding algorithm, which is similar to
structured learning with latent variables(Liang
et al, 2006) (Yu and Joachims, 2009). Most
methods for structured learning with latent vari-
ables have not expand all possible mappings.
In this paper, we also only expand the map-
ping that with highest according to the current
model.
Our model is shown in Figure 3 and the
flowchart is shown in Algorithm 1. If given the
output type of label T , we only consider the la-
bels in T to initialize the Viterbi matrix, and
the score of each node is determined by all the
involved heterogeneous labels according to the
loose mapping function.
input : character sequence x1:L
loose mapping function ?
output type: T (T ? {Ty, Tz})
output: label sequence ls
if T == Ty then
calculate ls using Eq. (6);
else if T == Tz then
calculate ls using Eq. (7) ;
else
return null;
end
return ls
Algorithm 1: Flowchart of the Tagging pro-
cess of the proposed model
6 Training
We use online Passive-Aggressive (PA) algo-
rithm (Crammer and Singer, 2003; Crammer et
al., 2006) to train the model parameters. Fol-
lowing (Collins, 2002), the average strategy is
used to avoid the overfitting problem.
For the sake of simplicity, we merge the Eq.(6)
and (7) into a unified formula.
Given a sequence x and the expect type of
tags T , the merged model is
y? = arg max
y
t(y)=T
?w,
?
z??(y)
?(x, z)?, (8)
where t(y) is a function to judge the type of
output tags; ?(y) represents the set {?(y1) ?
?(y2) ? ? ? ? ? ?(yL)} ? {y}, where ? means
Cartesian product; w = (uT , sT ,vT1 ,vT2 )T and
? = (fT ,hT ,gT1 ,gT2 )T .
We redefine the score function as
S(w,x,y) = ?w,
?
z??(y)
?(x, z)?. (9)
Thus, we rewrite the model into a unified for-
mula
y? = arg max
y
t(y)=T
S(w,x,y). (10)
Given an example (x,y), y? is denoted as the
incorrect label sequence with the highest score
y? = arg max
y? ?=y
t(y?)=t(y)
S(w,x, y?). (11)
The margin ?(w; (x,y)) is defined as
?(w; (x,y)) = S(w,x,y)? S(w,x, y?). (12)
Thus, we calculate the hinge loss
?(w; (x,y)), (abbreviated as ?w) by
?w =
{
0, ?(w; (x,y)) > 1
1? ?(w; (x,y)), otherwise
(13)
In round k, the new weight vector wk+1 is
calculated by
wk+1 = arg min
w
1
2 ||w?wk||
2 + C ? ?,
s.t. ?(w; (xk,yk)) <= ? and ? >= 0 (14)
663
where ? is a non-negative slack variable, and C
is a positive parameter which controls the influ-
ence of the slack term on the objective function.
Following the derivation in PA (Crammer et
al., 2006), we can get the update rule,
wk+1 = wk + ?kek, (15)
where
ek =
?
z??(yk)
?(xk, z)?
?
z??(y?k)
?(xk, z),
?k = min(C,
?wk
?ek?2
).
As we can see from the Eq. (15), when we up-
date the weight vector, the update information
includes not only the features extracted from
current input, but also that extracted from the
loose mapping sequence of input. For each fea-
ture, the weights of its corresponding related
features derived from the loose mapping func-
tion will be updated with the same magnitude
as well as itself.
Our method regards two annotations to be in-
terdependence and peer relationship. Therefore,
the two heterogeneous annotated corpora can be
simultaneously used as the input of our training
algorithm. Because of the tagging and training
algorithm, the weights and tags of two corpora
can be used separately with the only dependent
part built by the loose mapping function.
Our training algorithm based on PA is shown
in Algorithm 2.
6.1 Analysis
Although our mapping function between two
heterogeneous annotations is loose and uncer-
tain, our online training method can automat-
ically increase the relative weights of features
from the beneficial mapping relations and de-
crease the relative weights of features from the
unprofitable mapping relations.
Consider an illustrative loose mapping re-
lation ?NN/CTB?n,nt,nz/PDD?. For an in-
put sequence x and PDD-style output is ex-
pected. If the algorithm tagging a charac-
ter as ?n/PDD?(with help of the weight of
?NN/CTB?) and the right tag isn?t one of
input : mixed heterogeneous datasets:
(xi,yi), i = 1, ? ? ? , N ;
parameters: C,K;
loose mapping function: ? ;
output: wK
Initialize: wTemp? 0,w? 0;
for k = 0 ? ? ?K ? 1 do
for i = 1 ? ? ?N do
receive an example (xi,yi);
predict: y?i with Eq.(11);
if hinge loss ?w > 0 then
update w with Eq. (15);
end
end
wTemp = wTemp + w ;
end
wK = wTemp/K ;
Algorithm 2: Training Algorithm
?n,nt,nz/PDD?, the weight of ?NN/CTB? will
also be decreased, which is reasonable since
it is beneficial to distinguish the right tag.
And if the right tag is one of ?n,nt,nz/PDD?
but not ?n/PDD? (for example, ?nt/PDD?),
which means it is a ?NN/CTB?, the weight of
?NN/CTB? will remain unchanged according to
the algorithm (updating ?n/PDD? changes the
?NN/CTB?, but updating ?nt/PDD? changes it
back).
Therefore, after multiple iterations, useful fea-
tures derived from the mapping function are
typically receive more updates, which take rela-
tively more responsibility for correct prediction.
The final model has good parameter estimates
for the shared information.
We implement our method based on Fu-
danNLP(Qiu et al, 2013).
7 Experiments
7.1 Datasets
We use the two representative corpora men-
tioned above, Penn Chinese Treebank (CTB)
and PKU?s People?s Daily (PPD) in our ex-
periments.
664
Dataset Partition Sections Words
CTB-5
Training 1?270 0.47M
400?931
1001?1151
Develop 301?325 6.66K
Test 271?300 7.82K
CTB-S Training 0.64MTest - 59.96K
PPD Training - 1.11MTest - 0.16M
Table 3: Data partitioning for CTB and PD
7.1.1 CTB Dataset
To better comparison with the previous
works, we use two commonly used criterions to
partition CTB dataset into the train and test
sets.
? One is the partition criterion used in (Jin
and Chen, 2008; Jiang et al, 2009; Sun and
Wan, 2012) for CTB 5.0.
? Another is the CTB dataset from the
POS tagging task of the Fourth Interna-
tional Chinese Language Processing Bake-
off (SIGHAN Bakeoff 2008)(Jin and Chen,
2008).
7.1.2 PPD Dataset
For the PPD dataset, we use the PKU dataset
from SIGHAN Bakeoff 2008.
The details of all datasets are shown in Table
3. Our experiment on these datasets may lead to
a fair comparison of our system and the related
works.
7.2 Setting
We conduct two experiments on CTB-5 +
PPD and CTB-S + PPD respectively.
The form of feature templates we used is
shown in Table 7.2, where C represents a Chi-
nese character, and T represents the character-
based tag. The subscript i indicates its position
related to the current character.
Our method can be easily combined with
some other complicated models, but we only use
the simple one for the purpose of observing the
Ci, T0(i = ?2,?1, 0, 1, 2)
Ci, Ci+1, T0(i = ?1, 0)
T?1, T0
Table 4: Feature Templates
sole influence of our unified model. The parame-
ter C is tested on develop dataset, and we found
that it just impact the speed of convergence and
have no effect on the accuracy. Moreover, since
we use the averaged strategy, we wish more iter-
ations to avoid overfitting and set a small value
0.01 to it. The maximum number of iterations
K is 50.
The F1 score is used for evaluation, which is
the harmonic mean of precision P (percentage of
predict phrases that exactly match the reference
phrases) and recall R (percentage of reference
phrases that returned by system).
7.3 Evaluation on CTB-5 + PPD
The experiment results on the heterogeneous
corpora CTB-5 + PPD are shown in Table
5. Our method obtains an error reductions of
24.08% and 90.8% over the baseline on CTB-5
and PDD respectively.
Our method also gives better performance
than the pipeline-based methods on heteroge-
neous corpora, such as (Jiang et al, 2009) and
(Sun and Wan, 2012).
The reason is that our model can utilize the
information of both corpora effectively, which
can boost the performance of each other.
Although the loose mapping function are bidi-
rectional between two annotation tagsets, we
may also use unidirectional mapping. Therefore,
we also evaluate the performance when we use
unidirectional mapping. We just use the map-
ping function ?PDD?CTB, which means we ob-
tain the PDD-style output without the informa-
tion from CTB in tagging stage. Thus, in train-
ing stage, there are no updates for the weights of
CTB-features for the instances from PDD cor-
pus, while instances from CTB corpus can result
to updates for PDD-features.
Surprisedly, we find that the one-way map-
ping can also improve the performances of both
corpora. The results are shown in Table 7. The
665
Method Training Dataset Test Dataset P R F1
(Jiang et al, 2009) CTB-5, PDD CTB-5 - - 94.02
(Sun and Wan, 2012) CTB-5, PDD CTB-5 94.42 94.93 94.68
Our Model CTB-5 CTB-5 93.28 93.35 93.31
Our Model PDD PDD 89.41 88.58 88.99
Our Model CTB-5, PDD CTB-5 94.74 95.11 94.92
Our Model CTB-5, PDD PDD 90.25 89.73 89.99
Table 5: Performances of different systems on CTB-5 and PPD.
Method Training Dataset Test Dataset P R F1
Our Model CTB-S CTB-S 89.11 89.16 89.13
Our Model PDD PDD 89.41 88.58 88.99
Our Model CTB-S, PDD CTB-S 89.86 90.02 89.94
Our Model CTB-S, PDD PDD 90.5 89.82 90.16
Table 6: Performances of different systems on CTB-S and PPD.
modelPPD?CTB obtains an error reductions of
14.63% and 6.12% over the baseline on CTB-5
and PDD respectively.
Method P R F1
ModelS on CTB-5 93.86 94.73 94.29
ModelS on PDD 90.05 89.28 89.66
?ModelS? is the model which is trained on both CTB-
5 and PDD training datasets with just just using the
unidirectional mapping function ?PDD?CTB.
Table 7: Performances of unidirectional PPD?CTB
mapping on CTB-5 and PPD.
7.4 Evaluation on CTB-S + PPD
Table 6 shows the experiment results on the
heterogeneous corpora CTB-S + PPD. Our
method obtains an error reductions of 7.41% and
10.59% over the baseline on CTB-S and PDD re-
spectively.
7.5 Analysis
As we can see from the above experiments,
our proposed unified model can improve the
performances of the two heterogeneous corpora
with unidirectional or bidirectional loose map-
ping functions. Different to the pipeline-based
methods, our model can use the shared infor-
mation between two heterogeneous POS tag-
gers. Although the mapping function is loose
and uncertain, it is still can boost the perfor-
mances. The features derived from the wrong
mapping function take relatively less responsi-
bility for prediction after multiple updates of
their weights in training stage. The final model
has good parameter estimates for the shared in-
formation.
Another phenomenon is that the performance
of one corpus can gains when the data size of an-
other corpus increases. In our two experiments,
the training set?s size of CTB-S is larger than
CTB-5, so the performance of PDD is higher in
latter experiment.
8 Conclusion
We proposed a method for joint Chinese word
segmentation and POS tagging with heteroge-
neous annotation data. Different to the previous
pipeline-based works, our model is learned on
heterogeneous annotation data simultaneously.
Our method also does not require the exact
corresponding relation between the standards
of heterogeneous annotations. The experimen-
tal results show our method leads to a signif-
icant improvement with heterogeneous annota-
tions over the best performance for this task.
Although our work is for a specific task on joint
Chinese word segmentation and POS, the key
idea to leverage heterogeneous annotations is
very general and applicable to other NLP tasks.
666
In the future, we will continue to refine the
proposed model in two ways: (1) We wish to use
the unsupervised method to extract the loose
mapping relation between the different annota-
tion standards, which is useful to the corpora
without loose mapping guideline. (2) We will
analyze the shared information (weights of the
features derived from the tags which have the
mapping relation) in detail and propose a more
effective model. Besides, we would also like to
investigate for other NLP tasks which have dif-
ferent annotation-style corpora.
Acknowledgments
We would like to thank the anonymous re-
viewers for their valuable comments. This
work was funded by NSFC (No.61003091), Key
Projects in the National Science & Technol-
ogy Pillar Program (2012BAH18B01), Shang-
hai Municipal Science and Technology Com-
mission (12511504500), Shanghai Leading Aca-
demic Discipline Project (B114) and 973 Pro-
gram (No.2010CB327900).
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from mul-
tiple tasks and unlabeled data. J. Mach. Learn.
Res., 6:1817?1853, December.
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. Learning
Theory and Kernel Machines, pages 567?580.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
J. Gao, A. Wu, M. Li, C.N. Huang, H. Li, X. Xia,
and H. Qin. 2004. Adaptive chinese word segmen-
tation. In Proceedings of ACL-2004.
W. Jiang, L. Huang, Q. Liu, and Y. Lu. 2008. A cas-
caded linear model for joint Chinese word segmen-
tation and part-of-speech tagging. In In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics. Citeseer.
W. Jiang, L. Huang, and Q. Liu. 2009. Automatic
adaptation of annotation standards: Chinese word
segmentation and POS tagging: a case study. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language
Processing, pages 522?530.
Wenbin Jiang, Fandong Meng, Qun Liu, and Ya-
juan L?. 2012. Iterative annotation transfor-
mation with predict-self reestimation for Chinese
word segmentation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 412?420, Jeju Is-
land, Korea, July. Association for Computational
Linguistics.
C. Jin and X. Chen. 2008. The fourth interna-
tional Chinese language processing bakeoff: Chi-
nese word segmentation, named entity recognition
and Chinese pos tagging. In Sixth SIGHAN Work-
shop on Chinese Language Processing, page 69.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Percy Liang, Alexandre Bouchard-C?t?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 761?768. Association for Computa-
tional Linguistics.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: one-at-a-time or all-at-once? word-based
or character-based. In Proceedings of EMNLP,
volume 4.
Xipeng Qiu, Feng Ji, Jiayi Zhao, and Xuanjing
Huang. 2012. Joint segmentation and tagging
with coupled sequences labeling. In Proceedings
of COLING 2012, pages 951?964, Mumbai, India,
December. The COLING 2012 Organizing Com-
mittee.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
FudanNLP: A toolkit for Chinese natural language
processing. In Proceedings of ACL.
Weiwei Sun and Xiaojun Wan. 2012. Reducing
approximation and estimation errors for Chinese
lexical processing with heterogeneous annotations.
In Proceedings of the 50th Annual Meeting of the
667
Association for Computational Linguistics, pages
232?241.
W. Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
1385?1394.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn Chinese treebank (3.0).
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169?1176.
ACM.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern Chinese corpus. Technical
report, Technical report.
Jiayi Zhao, Xipeng Qiu, and Xuanjing Huang. 2013.
A unified model for joint chinese word segmen-
tation and pos tagging with heterogeneous anno-
tation corpora. In International Conference on
Asian Language Processing, IALP.
668
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946?957,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Discourse Level Explanatory Relation Extraction from Product Reviews
Using First-order Logic
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, P.R. China
{qz, 12110240030, 12210240054, 12210240059, xjhuang}@fudan.edu.cn
Abstract
Explanatory sentences are employed to clarify
reasons, details, facts, and so on. High quality
online product reviews usually include not
only positive or negative opinions, but also a
variety of explanations of why these opinions
were given. These explanations can help
readers get easily comprehensible informa-
tion of the discussed products and aspect-
s. Moreover, explanatory relations can also
benefit sentiment analysis applications. In
this work, we focus on the task of identi-
fying subjective text segments and extracting
their corresponding explanations from prod-
uct reviews in discourse level. We propose
a novel joint extraction method using first-
order logic to model rich linguistic features
and long distance constraints. Experimental
results demonstrate the effectiveness of the
proposed method.
1 Introduction
Through analyzing product reviews with high help-
fulness ratings assigned by readers, we find that a
large number of explanatory sentences are used to
clarify the causes, details, or consequences of opin-
ions. According to the statistic based on the dataset
we crawled from a popular product review website,
more than 56.1% opinion expressions are further
explained by other sentences. Since most consumers
are not experts, these explanations would bring lots
of helpful and easy comprehension information for
them. Suggestions about writing a product review
also advise authors to include not only whether they
like or dislike a product, but also why.1
1http://www.reviewpips.com/
http://www.amazon.com/gp/community-help/customer-
For example, let us consider the following snip-
pets extracted from online reviews:
Example 1: TVs with lower refresh rates may
suffer from motion blur. If you?re watching
a fast-paced football game, for example, you
may notice a bit of blurring as the players run
around the field.
Example 2: The LED screen is highly reflec-
tive. The reflection of my own face makes it very
hard to see the subject I am trying to shoot.
The first sentence of example 1 expresses negative
opinion about refresh rate, which is one of the most
important attributes of TV. The second sentence
describes the consequence of it through an example.
In example 2, detail descriptions are used to explain
the reflection problem of the camera screen.
Although, explanations provide valuable infor-
mation, to the best of our knowledge, there is no
existing work that deals with explanation extraction
for opinions in discourse level. We think that if
explanatory relations can be automatically identified
from reviews, sentiment analysis applications may
benefit from it. Existing opinion mining approaches
mainly focus on subjective text. They try to de-
termine the subjectivity and polarity of fragments
of documents (e.g. a paragraph, a sentence, a
phrase and a word) (Pang et al, 2002; Riloff et
al., 2003; Takamura et al, 2005; Mihalcea et al,
2007; Dasgupta and Ng, ; Hassan and Radev, 2010;
Meng et al, 2012; Dragut et al, 2012). Fine-grained
methods were also introduced to extract opinion
holder, opinion expression, opinion target, and other
opinion elements (Kobayashi et al, 2007; Wu et al,
reviews-guidelines
946
2011; Xu et al, 2013; Yang and Cardie, 2013). Ma-
jor research directions and challenges of sentiment
analysis can also be found in surveys (Pang and Lee,
2008; Liu, 2012).
In this work, we aim to identify subjective tex-
t segments and extract their corresponding expla-
nations from product reviews in discourse level.
We propose to use Markov Logic Networks (ML-
N) (Richardson and Domingos, 2006) to learn the
joint model for subjective classification and explana-
tory relation extraction. MLN has been applied in
several natural language processing tasks (Singla
and Domingos, 2006; Poon and Domingos, 2008;
Yoshikawa et al, 2009; Andrzejewski et al, 2011;
Song et al, 2012) and demonstrated its advantages.
It can easily incorporate rich linguistic features and
global constraints by designing various logic for-
mulas, which can also be viewed as templates or
rules. Logic formulas are combined in a proba-
bilistic framework to model soft constraints. Hence,
the proposed approach can benefit a lot from this
framework.
To evaluate the proposed method, we crawled a
large number of product reviews and constructed a
labeled corpus through Amazon?s Mechanical Turk.
Two tasks were deployed for labeling the corpus.
We compared the proposed method with state-of-
the-art methods on the dataset. Experimental results
demonstrate that the proposed approach can achieve
better performance than state-of-the-art methods.
The remaining part of this paper is organized as
follows: In Section 2, we define the problem and
give some examples to show the challenges of this
task. Section 3 describes the proposed MLN based
method. Dataset construction, experimental results
and analyses are given in Section 4. In Section 5, we
present the related work and Section 6 concludes the
paper.
2 Problem Statement
Motivated by the argument structure of discourse
relations used in Penn Discourse Treebank (Rash-
mi Prasad and Webber, 2008), in this work, we
adopt the clause unit-based definition. It means that
clauses are treated as the basic units of opinion ex-
pressions and explanations. Let d = {c1, c2, ...cn}
be the clauses of document d. Directed graph
G = (V,E) is used to represent the subjectivity
of clauses and explanatory relationships between
them. In the graph, vertices represent clauses,
whose categories are specified by the vertex at-
tributes. Directed edges describe the explanatory
relationships between them, of which the heads are
explanatory clauses. If clause ca describes a set of
facts which clarify the causes, context, situation, or
consequences of another clause cb, ca ?? cb is used
to indicate that clause ca explains cb.
Adopting clause unit-based definition is based
on the following reasons: 1) clause is normally
considered as the smallest grammatical unit which
can express a complete proposition (Kroeger, 2005);
2) from analyzing online reviews, we observe that
a clause can express a complete opinion about one
aspect in most of cases; 3) in Penn Discourse
Treebank, the basic unit of discourse relations (with
a few exceptions) is also taken to be a clause (Rash-
mi Prasad and Webber, 2008).
Figure 1(a) illustrates a sample document. Figure
1(b) is the corresponding output of the given docu-
ment. In the graph, vertices whose color are black
stand for subjective clauses. The other clauses are
represented by white vertices. Edges describe the
explanatory relationships between them, of which
the heads are explanatory clauses.
Although the explanatory relation extraction task
has been studied from the view of linguistic and
discourse representation by existing works (Carston,
1993; Lascarides and Asher, 1993), the automatic
extraction task is still an open question. Consider the
following examples extracting from online reviews:
Example 3: It takes great pictures. Color ren-
ditions, skin tones, exposure levels are all first rate.
From the example, we can observe that the second
sentence explains the first one. However, the second
sentence itself also expresses opinion on various
opinion targets. In other words, both subjective and
objective sentences can be used as explanations.
Example 4: When we called their service center
they made us wait for them the whole day and no
one turned up. This level of service is simply not
acceptable. The first sentence in example 4 explains
the second one. Hence, the feature of relative
location between two sentences does not always
work well in all cases.
Example 5: This backpack is great! its very big
947
(c1) I have both the Panasonic LX3 and the Canon
S90. (c2) Both cameras are quite different but truly
excellent. (c3) The S90 is a true pocket camera.
(c4) It is very compact. (c5) The build quality is
also top notch. (c6) It feels solid and it is easy to
grip. (c7) It is so small and convenient, (c8) you
will find that you will always carry it with you.
C
5
C
3
C
2
C
4
C
6
C
7
C
1
(a) Example Review (b) Directed Graph Representation
C
8
Figure 1: Directed graph representation of a sample document.
and fits more than enough stuff. Many sentences,
which express explanatory relation, do not contain
any connectives (e.g. ?because?, ?the reason is?,
and so on). Lin et al(2009) generalized four chal-
lenges (include ambiguity, inference, context, and
world knowledge) to automated implicit discourse
relation recognition. In this task, we also need to
address those challenges.
From the these examples, we can observe that ex-
tracting explanatory relations from product reviews
is a challenging task. Both linguistic and global
constraints should be carefully studied.
3 The Proposed Approach
In this section, we present our method for jointly
classifying the subjectivity of text segments and
extracting explanatory relations. Firstly, we briefly
describe the framework of Markov Logic Networks.
Then, we introduce the clause extraction method
based on the definition described in the Section 2.
Finally, we present the first-order logic formulas
including local formulas and global formulas used
for joint modeling in this work.
3.1 Markov Logic Networks
A MLN consists of a set of logic formulas that
describe first-order knowledge base. Each formula
consists of a set of first-order predicates, logical
connectors and variables. Different with first-order
logic, these hard logic formulas are softened and
can be violated with some penalty (the weight of
formula) in MLN.
We use M to represent a MLN and {(?i, wi)}
to represent formula ?i and its weight wi. These
weighted formulas define a probability distribution
over sets of possible worlds. Let y denote a possible
world, the p(y) is defined as follows (Richardson
and Domingos, 2006):
p(y) = 1
Z
exp
?
?
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
?
? ,
where each c is a binding of free variable in ?i to
constraints; f?ic (y) is a binary feature function that
returns 1 if the true value is obtained in the ground
formula we get by replacing the free variables in
?i with the constants in c under the given possible
world y, and 0 otherwise; Cn?i is all possible
bindings of variables to constants, and Z is a nor-
malization constant.
Many methods have been proposed to learn the
weights of MLN using both generative and dis-
criminative approaches (Richardson and Domingos,
2006; Singla and Domingos, 2006). There are
also several MLN learning packages available online
such as thebeast2, Tuffy3, PyMLNs4, Alchemy5, and
so on.
2http://code.google.com/p/thebeast
3http://hazy.cs.wisc.edu/hazy/tuffy/
4http://www9-old.in.tum.de/people/jain/mlns/
5http://alchemy.cs.washington.edu/
948
Describing the attributes of words
subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al,
2010).
relationLexicon(w) The word w belongs to the lexicon of explanation relation
connectives (Pitler and Nenkova, 2009).
Describing the attributes of the clause ci
word(i, w) The clause ci has word w.
firstWord(i, w) The first word of clause ci is word w.
pos(i, w, t) The POS tag of word w is t in clause ci.
dep(i, h,m) Word m and h are governor and dependent of a dependency
relation in clause ci.
Describing the attributes of relations between clause ci and clause cj
clauseDistance(i, j,m) Distance between clause ci and clause cj in clauses is m.
sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n.
Table 1: Descriptions of observed predicates.
3.2 Clause Identification
We model the clause boundary identification prob-
lem through sequence labeling and use Conditional
Random Fields (CRFs) to identify clause bound-
aries. Words and part-of-speech (POS) tags are used
as feature sets. Since we do not allow embedded
segments, the performance of our method is promis-
ing, which achieves the F1 score of 92.8%. The
result is comparable with the best results obtained
during the CoNLL-2001 campaign (Tjong et al,
2001).
3.3 Formulas
In this work, we propose to use predicate subj(i)
to indicate that the ith clause is subjective and
explain(i, j) to indicate that the jth clause explains
the ith clause. Both subj and explain are hidden
predicates and jointly modeled by MLN. We use
local and global formulas to model rich linguistic
features and long distance constraints.
3.3.1 Local Formulas
The local formulas relate one or more observed
predicates to exactly one hidden predicate. In this
work, we define a list of observed predicates to
describe the properties of individual clauses and
attributes of relations between two clauses. The
observed predicates and descriptions are shown in
Table 1. The observed predicates can be categorized
into 3 groups: words, clauses, and relations between
clauses. We use two lexicons to capture background
knowledge of words. Lexical, part-of-speech tag,
and dependency relation are used to describe a single
clause. We also propose two predicates to model
distance between clauses.
Table 2 lists the local formulas used in this work.
The ?+? notation in the formulas indicates that each
constant of the logic variable should be weighted
separately. For subjective classification and relation
extraction, we construct a number of formulas re-
spectively.
For subjective classification, the first two formu-
las model the influence of lexical and POS tag. It
is similar as the bag-of-words model, which is a
simplifying representation and has been successfully
used for various natural language processing tasks.
Since words which provide positive or negative
opinions may provide important information for
subjectivity classification, we combine predicates of
words and lexicon of opinion words. Bigrams are
also proved to be useful for textual classification in
several NLP tasks. Hence, we also combine predi-
cates about individual word and POS tag to capture
this kind of information. Word-level relations are
explicitly presented at the dependency trees, we
949
Formulas for subjective classification
word(i,w+)? subj(i)
pos(i,w+,t+)? subj(i)
word(i,w+) ? subjLexicon(w)? subj(i)
pos(i,w+,t+) ? subjLexicon(w)? subj(i)
word(i,w1+) ? word(i,w2+)? subj(i)
pos(i,w1+,t+) ? pos(i,w2+,t+)? subj(i)
word(i,w1+) ? word(i,w2+) ? subjLexicon(w1)? subj(i)
word(i,w1+) ? word(i,w2+) ? subjLexicon(w2)? subj(i)
dep(i,w1+,w2+)? subj(i)
dep(i,w1+,w2+) ? subjLexicon(w1)? subj(i)
dep(i,w1+,w2+) ? subjLexicon(w2)? subj(i)
Formulas for explanatory relation extraction
word(i,w1+) ? word(j,w2+) ? j ?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+)? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? sentenceDistance(i,j,n+) ? j ?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? sentenceDistance(i,j,n+) ? j ?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? firstWord(j,w+) ? j?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? firstWord(j,w+) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? firstWord(j,w+) ? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? subjLexicon(w1) ? j?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? subjLexicon(w1) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? subjLexicon(m1) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? sentenceDistance(i,j,n+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? pos(j,w,t+) ? clauseDistance(i,j,m+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? pos(j,w,t+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(i,w1+) ? clauseDistance(i,j,m+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(i,w1+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(j,w1+) ? clauseDistance(i,j,m+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(j,w1+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
Table 2: Descriptions of local formulas.
950
also construct local formulas based on predicates
extracted from dependency trees of clauses.
For explanatory relation extraction, we firstly use
formulas to capture lexical and syntactic information
from both of the clauses. Since distances between
clauses are helpful in determining the relation, we
incorporate two kinds of distance features with lex-
ical and syntactic predicates. Connective words
such as for example, since, explicitly signal the
presence of the explanation relation. Although some
connective words are ambiguous in terms of relation
they mark (Pitler and Nenkova, 2009), they may still
be useful for explanation relation extraction. Hence,
we construct local formulas with relation lexicon
and other predicates.
3.3.2 Global Formulas
Local formulas are designed to deal with sub-
jective classification of a single clause or relation
determination of a single pair of clauses. Global
formulas are designed to handle global constraints
of multiple clauses. From the definition of explana-
tory relation and corpus statistics, we observe the
following properties:
Property 1: One clause can only serve as the
explanation of one subjective clause.
Property 2: Explanatory clauses occur immedi-
ately before or after their corresponding subjective
clauses.
Property 3: The positions of explanatory clauses
are consecutive. In other words, if clause ck and
ck+2 explain clause cj , the clause ck+1 would also
be explanatory clause of cj .
For property 1, we use the following global for-
mula to make sure that one clause only explains at
most one another clause.
explain(i, j) ? ?explain(k, j) ?k ?= i, j (1)
Based on the property 2 and 3, explanatory claus-
es are consecutive and immediately before or after
their corresponding subjective clauses. We use the
following formulas to guarantee the property:
explain(i, i+ k) ? explain(i, i+m),
1 ? m ? k ? 1 (2)
explain(i, i? k) ? explain(i, i?m),
1 ? m ? k ? 1 (3)
Since our aim is to extract explanatory for subjec-
tive clauses, we also use the following formulas to
make sure that the clauses which are explained are
subjective ones.
explain(i, j) ? subj(i) (4)
4 Experiments
4.1 Data Set
We crawled a number of reviews about digital cam-
eras from Buzzillions6, which is a product review
site and contains more than 16 million reviews.
We randomly select 100 reviews whose usefulness
ratings are 5 on a 5-point scale. They contain 1137
sentences, which are composed by 1665 clauses.
Amazon?s Mechanical Turk is used to deploy two
tasks for labeling the corpus. 694 clauses are labeled
subjective and 478 clauses explain other ones. More
than 56.1% opinion expressions are explained by
their corresponding explanatory sentences.
The two projects we deployed on Amazon?s Me-
chanical Turk are: 1) Determine whether a clause
contains opinion expressions or not; 2) Determine
whether a clause clarifies causes, reasons, or conse-
quences of another given clause. In order to control
the labeling quality, we configured parameters of
the project to make sure that all the tasks should
be judged by at least 20 annotators. Most of the
annotators can complete a task within 25 seconds.
Figure 2 shows the screenshots of the two projects.
Over all, 127 workers participated in the project.
About 72% of them submitted more than 5 tasks.
Although we listed several examples on the project
descriptions, different people may have their own
understanding and criteria for those tasks. In order
to measure the quality of the labeling task, we use
perplexity to evaluate each task. If the perplexity
of a task is below 0.51, which means that more than
80% of the workers submitted the same decision, the
result of the task will be used as training or testing
data. From the statistic of the corpus, we observe
that only 6.2% of the clauses? subjectiveness and
15.6% of explanation relations can not be certainly
decided. For the first project, we treated those claus-
es as objective one. And, those clause pairs in the
second project were not considered as explanation
relations.
6www.buzzillions.com
951
Task2: Help us check whether a sentence is an 
explanation of the opinion sentence. 
The opinion sentence (red one) is extracted from product reviews and 
express opinion towards some attributes/parts of a product. Please 
help us check whether the following blue sentences describe a set of 
facts which clarifies the causes, reason, and consequences of the 
opinion given in the opinion sentence. 
 
click "yes" if there is an explanation relation between them, "no" 
otherwise. 
 
     The battery life is something I come to expect from this line of camera. 
     I can leave the camera on for better than 8 hours shooting  
     ()YES 
     ()NO 
 
     The battery life is something I come to expect from this line of camera. 
     and I have the camera set to shut off the sensor after about 30 seconds   
     ()YES 
     ()NO 
Task1: Help us determine whether a sentence is 
subjective or objective. 
The following sentences are extracted from product reviews. Please 
help us check whether the following sentences  expressing opinion 
towards some attributes/parts of a product. 
 
     The battery life is something I come to expect from this line of camera. 
     ()Subjective 
     ()Objective 
 
     I have the camera set to shut off the sensor after about 30 seconds   
     ()Subjective 
     ()Objective 
Figure 2: Screenshots of the two tasks on Amazon
Mechanical Turk.
4.2 Experiments Configurations
Stanford parser (Klein and Manning, 2003) is used
for extracting features from dependency parse trees.
For resolving Markov logic network, we use the
toolkit thebeast 7. The detailed setting of thebeast
engine is as follows: The inference algorithm is
the MAP inference with a cutting plane approach.
For parameter learning, the weights for formulas are
updated by an online learning algorithm with MIRA
update rule. All the initial weights are set to zeros.
The number of iterations is set to 10 epochs.
Evaluation metrics used for subjectivity classifi-
cation and relation extraction throughout the experi-
ments include: Precision, Recall, and F1-score. We
randomly select 80% reviews as training set and the
others as testing set.
Since the dataset is newly created for this task, to
compare the performance of the proposed method to
other models, we also reimplemented several state-
7http://code.google.com/p/thebeast
of-the-art methods for comparison.
? CRF-Subj: We follow the method proposed by
Zhao et al (2008), which regard the subjec-
tivity of all clauses throughout a paragraph as
a sequential flow of sentiments and use CRFs
to model it. The feature sets are similar as
the local formulas for MLN including words,
POS tags, dependency relations, and opinion
lexicon.
? RAE-Subj: Socher et al (2011) proposed to
use recursive autoencoders for sentence-level
predication of sentiment label distributions. To
compare with it, we also reimplement their
method without any hand designed lexicon.
? PDTB-Rel: For discourse relation extraction,
we use ?PDTB-Styled End-to-End Discourse
Parser? (Lin et al, 2010) to extract discourse
level relations as baseline. Since it is a gener-
al discourse relations identification algorithms,
?Cause?, ?Pragmatic Cause?, ?Instantiation?,
and ?Restatement? relation types are treated as
explanatory relation in this work.
? SVM-Rel: We also use LibSVM (Chang and
Lin, 2011) to classify the relations between
clauses. Following the configurations reported
by Feng and Hirst (2012), we use linear kernel
and probability estimation to model it.
4.3 Results
Table 3 shows the comparisons of the proposed
method with the state-of-the-art systems on subjec-
tivity classification and explanatory relation extrac-
tion. From the results, we can observe that recur-
sive autoencoders based subjectivity classification
method achieves slightly better performance than
our method and conditional random fields based
method. The performances of the proposed method
are similar as CRFs?. We think that the main reason
is that only lexical features are used in MLN models
for subjective classification. However, conditional
random fields consider not only lexical information
but also inference of the contexts of sentences.
RAE method learns vector space representations for
multi-word phrases and uses compositional seman-
tics to understand sentiment.
952
Methods
Subjective Classification
P R F1
CRF-Subj 83.5% 76.9% 80.1%
RAE-Subj 85.3% 79.1% 82.1%
MLN 79.2% 80.6% 79.9%
Methods
Relation Extraction
P R F1
RAE-Subj + PDTB-Rel 28.5% 38.6% 32.8%
RAE-Subj + SVM-Rel 32.4% 89.7% 47.6%
MLN 56.2% 72.9% 63.5%
Table 3: Performance comparisons between the proposed
method and state-of-the-art methods. ?MLN? represents
the method proposed in this work.
For evaluating the performance of relation extrac-
tion, we combine the results of RAE with PDTB-
Rel and SVM-Rel. For all the subjective clauses
identified by RAE, PDTB-Rel and SVM-Rel are
used to extract corresponding explanatory clauses.
The results are shown in the last three rows in
the Table 3. From the results, we can observe
that the proposed joint model achieves best F1
score and precision among all methods. Although
the proposed method achieve slightly worse result
in processing subjectivity classification. We think
that the error propagation is the main reason for
worse results of cascaded methods. The relative
improvement of MLN over SVM-Rel is more than
33.4%.
To show the effectiveness of different observed
predicates, we evaluate the performances of the
proposed method with different predicate sets. We
subtract one observed predicate and its correspond-
ing local formulas from the original sets at a time.
The results of both subjectivity classification and
relation extraction are shown in Table 4. The first
row shows the result of the MLN based method with
all observed predicates and local formulas. From the
results we can observe that the observed predicates
which are not used in the local formulas for sub-
jectivity classification also impact the performance
of subjectivity classification. We think that the per-
formance is effected by the global formulas, which
combine the procedure of subjectivity classification
and relation extraction. Among all predicates, we
observe that words and dependency relations play
the most important roles. Without word predicate,
the F1 score of subjectivity classification and re-
lation extraction significantly drop to 51.2% and
42.9% respectively. For subjectivity classification,
subjective lexicon contributes a lot for recall. For
relation extraction, the impacts of clause distance
and sentence distance are not as significant as the
other features.
5 Related Work
Our work relates to three research areas: sentiment
analysis/opinion mining, discourse-level relation ex-
traction, andMarkov logic networks. Along with the
increasing requirement, subjectivity classification
has recently received considerable attention from
both the industry and researchers. A variety of
approaches and methods have been proposed for
this task from different aspects. Among them, a
number of approaches focus on classifying senti-
ments of text in different levels (e.g. words (Kim
and Hovy, 2004), phrases (Wilson et al, 2005),
sentences (Zhao et al, 2008), documents (Pang et
al., 2002) and so on.), and detecting the overall
polarity of them.
Another research direction tries to convert the
sentiment analysis task into entity identification and
relation extraction. Hu and Liu (2004) proposed
to use a set of methods to produce feature-based
summary of a large number of customer reviews.
Kobayashi et al (2007) assumed that evaluative
opinions could be structured as a frame which is
composed by opinion holder, subject, aspect, and
evaluation. They converted the task to two kinds
of relation extraction tasks and proposed a machine
learning-based method which used both contextual
and statistical clues.
Analysis of some special types of sentences were
also introduced in recent years. Jindal and Li-
u (2006) studied the problem of identifying com-
parative sentences. They analyzed different types
of comparative sentences and proposed learning
approaches to identify them. Conditional sentences
were studied by Narayanan et al(2009). They
analyzed the conditional sentences in both linguistic
and computitional perspectives and used learning
953
Subjective Classification Relation Extraction
P R F1 P R F1
MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5%
?subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4%
?relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0%
?word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9%
?firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7%
?pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0%
?dep(i, h,m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8%
?clauseDistance(i, j,m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3%
?sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2%
Table 4: Performance comparisons of different observed predicates
method to do it. They followed the feature-based
sentiment analysis model (Hu and Liu, 2004), which
also use flat frames to represent evaluations.
Since the cross sentences relations are considered
in this work, the discourse-level relation extrac-
tion methods are also related to ours. Marcu and
Echihabi (2002) proposed to use an unsupervised
approach to recognizing discourse relations. Lin et
al.(2009) analyzed the impacts of features extracted
from contextual information, constituent parse trees,
dependency parse trees, and word pairs. Asher
et al(2009) studied discourse segments containing
opinion expressions from the perspective of linguis-
tics. Chen et al (2010) introduced a multi-label
model to detect emotion causes. They developed
two sets of linguistic features for this task base on
linguistic cues. Zirn et al (2011) proposed to use
MLN framework to capture the context information
in analysing (sub-)sentences.
The most similar work to ours was proposed by
Somasundaran et al(2009). They proposed to use it-
erative classification algorithm to capture discourse-
level associations. However different to us, they
focused on pairwise relationships between opinion
expressions. In this paper, we used MLN framework
to capture another different discourse-level relation,
which exists between subject clauses or subject
clause and objective clause.
Richardson and Domingos (2006) proposed
Markov Logic Networks, which combines first-
order logic and probabilistic graphical models. In
recent years, MLN has been adopted for several
natural language processing tasks and achieved
a certain level of success (Singla and Domingos,
2006; Riedel and Meza-Ruiz, 2008; Yoshikawa
et al, 2009; Andrzejewski et al, 2011; Jiang
et al, 2012; Huang et al, 2012). Singla and
Domingos (2006) modeled the entity resolution
problem with MLN. They demonstrated the
capability of MLN to seamlessly combine a number
of previous approaches. Poon and Domingos (2008)
proposed to use MLN for joint unsupervised
coreference resolution. Yoshikawa et al (2009)
proposed to use Markov logic to incorporate both
local features and global constraints that hold
between temporal relations. Andrzejewski et
al. (2011) introduced a framework for incorporating
general domain knowledge, which is represented by
First-Order Logic (FOL) rules, into LDA inference
to produce topics shaped by both the data and the
rules.
6 Conclusions
In this paper, we propose to use Markov logic
networks to identify subjective text segments and ex-
tract their corresponding explanations in discourse
level. We use MLN to jointly model subjectivity
classification and explanatory relation extraction.
Rich linguistic features and global constraints are
incorporated by various logic formulas and global
formulas. To evaluate the proposed method, we
collected a large number of product reviews and
954
constructed a labeled corpus through Amazon?s Me-
chanical Turk. Experimental results demonstrate
that the proposed approach achieve better perfor-
mance than state-of-the-art methods.
7 Acknowledgement
The authors wish to thank the anonymous reviewers
for their helpful comments and Kang Han for
preparing the corpus. This work was partially
funded by National Natural Science Foundation
of China (61003092, 61073069), Key Projects
in the National Science & Technology Pillar
Program(2012BAH18B01), National Major
Science and Technology Special Project of China
(2014ZX03006005), Shanghai Municipal Science
and Technology Commission (12511504502) and
?Chen Guang? project supported by Shanghai
Municipal Education Commission and Shanghai
Education Development Foundation(11CG05).
References
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirichlet
allocation using first-order logic. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence - Volume Volume Two, IJCAI?11,
pages 1171?1177. AAAI Press.
Nicholas Asher, Farah Benamara, and Yannick Mathieu.
2009. Appraisal of opinion expressions in discourse.
Lingvistic? Investigationes.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
R. Carston. 1993. Conjunction, explanation and
relevance. Lingua 90, pages 27?48.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Ying Chen, Sophia Yat Mei Lee, Shoushan Li, and
Chu-Ren Huang. 2010. Emotion cause detection
with linguistic constructions. In Proceedings ofColing
2010.
Sajib Dasgupta and Vincent Ng. Mine the easy, classify
the hard: A semi-supervised approach to automatic
sentiment classification. In Proceedings of ACL-
IJCNLP 2009.
Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency
checking for sentiment dictionaries. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 997?1005, Jeju Island, Korea, July. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic features.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 60?68, Jeju Island, Korea, July.
Association for Computational Linguistics.
Ahmed Hassan and Dragomir R. Radev. 2010.
Identifying text polarity using random walks. In
Proceedings of ACL 2010, Uppsala, Sweden, July.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
SIGKDD 2004.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence.
Shangpu Jiang, D. Lowd, and Dejing Dou. 2012. Learn-
ing to refine an automatically extracted knowledge
base using markov logic. In Data Mining (ICDM),
2012 IEEE 12th International Conference on, pages
912?917.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR
2006.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COLING
2004.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Proceedings of NIPS 2003.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
Paul Kroeger. 2005. Analyzing Grammar: An
Introduction. Cambridge.
Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations, and common sense
entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of EMNLP 2009.
955
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR,
abs/1011.0835.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining. Morgan & Claypool Publishers.
Daniel Marcu and Abdessamad Echihabi. 2002.
An unsupervised approach to recognizing discourse
relations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 368?375.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-
lingual mixture model for sentiment classification.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 572?581, Jeju Island, Korea, July.
Association for Computational Linguistics.
RadaMihalcea, Carmen Banea, and JanyceWiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL 2007.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP 2009.
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends in
Information Retrieval, 2:1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of
EMNLP 2002.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08, pages
650?659, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In
Bente Maegaard Joseph Mariani Jan Odjik Stelios
Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of L-
REC?08, Marrakech, Morocco, may. http://www.lrec-
conf.org/proceedings/lrec2008/.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with markov logic. In
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, CoNLL ?08,
pages 193?197, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of HLT-NAACL 2003.
P. Singla and P. Domingos. 2006. Entity resolution with
markov logic. In Data Mining, 2006. ICDM ?06. Sixth
International Conference on, pages 572?582.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 151?161,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion graphs for
polarity and discourse classification. In Proceedings
of TextGraphs-4.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint learning for coreference
resolution with markov logic. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1245?1254, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL 2005.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001.
Introduction to the conll-2001 shared task: clause
identification. In Proceedings of the 2001 workshop
on Computational Natural Language Learning -
Volume 7, ConLL ?01, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP
2005.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide
Wu. 2011. Structural opinion mining for graph-based
sentiment representation. In Proceedings of EMNLP
2011.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and
Jun Zhao. 2013. Walk and learn: a two-stage
approach for opinion words and opinion targets co-
extraction. In Proceedings of the 22nd international
conference on World Wide Web companion, WWW
?13 Companion, pages 95?96, Republic and Canton of
Geneva, Switzerland. International World Wide Web
Conferences Steering Committee.
956
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings of
ACL 2013.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly
identifying temporal relations with markov logic. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 405?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding
redundant features for crfs-based sentence sentiment
classification. In Proceedings of EMNLP 2008.
Ca?cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of 5th
International Joint Conference on Natural Language
Processing, pages 336?344, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
957
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598?602,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Hierarchical Text Classification with Latent Concepts
Xipeng Qiu, Xuanjing Huang, Zhao Liu and Jinlong Zhou
School of Computer Science, Fudan University
{xpqiu,xjhuang}@fudan.edu.cn, {zliu.fd,abc9703}@gmail.com
Abstract
Recently, hierarchical text classification has
become an active research topic. The essential
idea is that the descendant classes can share
the information of the ancestor classes in a
predefined taxonomy. In this paper, we claim
that each class has several latent concepts and
its subclasses share information with these d-
ifferent concepts respectively. Then, we pro-
pose a variant Passive-Aggressive (PA) algo-
rithm for hierarchical text classification with
latent concepts. Experimental results show
that the performance of our algorithm is com-
petitive with the recently proposed hierarchi-
cal classification algorithms.
1 Introduction
Text classification is a crucial and well-proven
method for organizing the collection of large scale
documents. The predefined categories are formed
by different criterions, e.g. ?Entertainment?, ?Sport-
s? and ?Education? in news classification, ?Junk E-
mail? and ?Ordinary Email? in email classification.
In the literature, many algorithms (Sebastiani, 2002;
Yang and Liu, 1999; Yang and Pedersen, 1997) have
been proposed, such as Support Vector Machines
(SVM), k-Nearest Neighbor (kNN), Na??ve Bayes
(NB) and so on. Empirical evaluations have shown
that most of these methods are quite effective in tra-
ditional text classification applications.
In past serval years, hierarchical text classification
has become an active research topic in database area
(Koller and Sahami, 1997; Weigend et al, 1999)
and machine learning area (Rousu et al, 2006; Cai
and Hofmann, 2007). Different with traditional clas-
sification, the document collections are organized
as hierarchical class structure in many application
fields: web taxonomies (i.e. the Yahoo! Directory
http://dir.yahoo.com/ and the Open Direc-
tory Project (ODP) http://dmoz.org/), email
folders and product catalogs.
The approaches of hierarchical text classification
can be divided in three ways: flat, local and global
approaches.
The flat approach is traditional multi-class classi-
fication in flat fashion without hierarchical class in-
formation, which only uses the classes in leaf nodes
in taxonomy(Yang and Liu, 1999; Yang and Peder-
sen, 1997; Qiu et al, 2011).
The local approach proceeds in a top-down fash-
ion, which firstly picks the most relevant categories
of the top level and then recursively making the
choice among the low-level categories(Sun and Lim,
2001; Liu et al, 2005).
The global approach builds only one classifier to
discriminate all categories in a hierarchy(Cai and
Hofmann, 2004; Rousu et al, 2006; Miao and Qiu,
2009; Qiu et al, 2009). The essential idea of global
approach is that the close classes have some com-
mon underlying factors. Especially, the descendan-
t classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997; Xue et al, 2007).
Because the global hierarchical categorization can
avoid the drawbacks about those high-level irrecov-
erable error, it is more popular in the machine learn-
ing domain.
However, the taxonomy is defined artificially and
is usually very difficult to organize for large scale
taxonomy. The subclasses of the same parent class
may be dissimilar and can be grouped in differen-
t concepts, so it bring great challenge to hierarchi-
598
Sports
Football
Basketball
Swimming
Surfing
Sports
Water
Football
Basketball
Swimming
Surfing
Ball
(a) (b)
College
High 
School
College
High 
SchoolAcade
my
Figure 1: Example of latent nodes in taxonomy
cal classification. For example, the ?Sports? node
in a taxonomy have six subclasses (Fig. 1a), but
these subclass can be grouped into three unobserv-
able concepts (Fig. 1b). These concepts can show
the underlying factors more clearly.
In this paper, we claim that each class may have
several latent concepts and its subclasses share in-
formation with these different concepts respectively.
Then we propose a variant Passive-Aggressive (PA)
algorithm to maximizes the margins between latent
paths.
The rest of the paper is organized as follows. Sec-
tion 2 describes the basic model of hierarchical clas-
sification. Then we propose our algorithm in section
3. Section 4 gives experimental analysis. Section 5
concludes the paper.
2 Hierarchical Text Classification
In text classification, the documents are often rep-
resented with vector space model (VSM) (Salton et
al., 1975). Following (Cai and Hofmann, 2007),
we incorporate the hierarchical information in fea-
ture representation. The basic idea is that the notion
of class attributes will allow generalization to take
place across (similar) categories and not just across
training examples belonging to the same category.
Assuming that the categories is ? =
[?1, ? ? ? , ?m], where m is the number of the
categories, which are organized in hierarchical
structure, such as tree or DAG.
Give a sample x with its class path in the taxono-
my y, we define the feature is
?(x,y) = ?(y)? x, (1)
where ?(y) = (?1(y), ? ? ? , ?m(y))T ? Rm and ?
is the Kronecker product.
We can define
?i(y) =
{
ti if ?i ? y
0 otherwise , (2)
where ti >= 0 is the attribute value for node v. In
the simplest case, ti can be set to a constant, like 1.
Thus, we can classify x with a score function,
y? = argmax
y
F (w,?(x,y)), (3)
where w is the parameter of F (?).
3 Hierarchical Text Classification with
Latent Concepts
In this section, we first extent the Passive-
Aggressive (PA) algorithm to the hierarchical clas-
sification (HPA), then we modify it to incorporate
latent concepts (LHPA).
3.1 Hierarchical Passive-Aggressive Algorithm
The PA algorithm is an online learning algorithm,
which aims to find the new weight vectorwt+1 to be
the solution to the following constrained optimiza-
tion problem in round t.
wt+1 = arg min
w?Rn
1
2
||w ?wt||2 + C?
s.t. ?(w; (xt, yt)) <= ? and ? >= 0. (4)
where ?(w; (xt, yt)) is the hinge-loss function and ?
is slack variable.
Since the hierarchical text classification is loss-
sensitive based on the hierarchical structure. We
need discriminate the misclassification from ?near-
ly correct? to ?clearly incorrect?. Here we use tree
induced error ?(y,y?), which is the shortest path
connecting the nodes yleaf and y?leaf . yleaf repre-
sents the leaf node in path y.
Given a example (x,y), we look for the w to
maximize the separation margin ?(w; (x,y)) be-
tween the score of the correct path y and the closest
error path y?.
?(w; (x,y)) = wT?(x,y)?wT?(x, y?), (5)
599
where y? = argmaxz ?=y wT?(x, z) and ? is a fea-
ture function.
Unlike the standard PA algorithm, which achieve
a margin of at least 1 as often as possible, we wish
the margin is related to tree induced error ?(y, y?).
This loss is defined by the following function,
?(w; (x,y)) =
{
0, ?(w; (x,y)) > ?(y, y?)
?(y, y?)? ?(w; (x,y)), otherwise (6)
We abbreviate ?(w; (x,y)) to ?. If ? = 0 then wt
itself satisfies the constraint in Eq. (4) and is clearly
the optimal solution. We therefore concentrate on
the case where ? > 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (4) to be,
L(w, ?, ?, ?) = 1
2
||w?wt||2+C?+?(???)???
s.t. ?, ? >= 0. (7)
where ?, ? is a Lagrange multiplier.
We set the gradient of Eq. (7) respect to ? to zero.
? + ? = C. (8)
The gradient of w should be zero.
w ?wt ? ?(?(x,y)? ?(x, y?)) = 0 (9)
Then we get,
w = wt + ?(?(x,y)? ?(x, y?)). (10)
Substitute Eq. (8) and Eq. (10) to objective func-
tion Eq. (7), we get
L(?) = ?1
2
?2||?(x,y)? ?(x, y?)||2
+ ?wt(?(x,y)? ?(x, y?)))? ??(y, y?) (11)
Differentiate Eq. (11 with ?, and set it to zero, we
get
?? = ?(y, y?)?wt(?(x,y)? ?(x, y?)))
||?(x,y)? ?(x, y?)||2 (12)
From ? + ? = C, we know that ? < C, so
?? = min(C, ?(y, y?)?wt(?(x,y)? ?(x, y?)))
||?(x,y)? ?(x, y?)||2
).
(13)
3.2 Hierarchical Passive-Aggressive Algorithm
with Latent Concepts
For the hierarchical taxonomy ? = (?1, ? ? ? , ?c),
we define that each class ?i has a set H?i =
h1?i , ? ? ? , h
m
?i with m latent concepts, which are un-
observable.
Given a label path y, it has a set of several latent
paths Hy. For a latent path z ? Hy, a function
Proj(z) .= y is the projection from a latent path z
to its corresponding path y.
Then we can define the predict latent path h? and
the most correct latent path h?:
h? = arg max
proj(z)?=y
wT?(x, z), (14)
h? = arg max
proj(z)=y
wT?(x, z). (15)
Similar to the above analysis of HPA, we re-define
the margin
?(w; (x,y) = wT?(x,h?)? wT?(x, h?), (16)
then we get the optimal update step
??L = min(C,
?(wt; (x,y))
||?(x,h?)? ?(x, h?)||2
). (17)
Finally, we get update strategy,
w = wt + ??L(?(x,h?)? ?(x, h?)). (18)
Our hierarchical passive-aggressive algorithm
with latent concepts (LHPA) is shown in Algorith-
m 1. In this paper, we use two latent concepts for
each class.
4 Experiment
4.1 Datasets
We evaluate our proposed algorithm on two datasets
with hierarchical category structure.
WIPO-alpha dataset The dataset1 consisted of the
1372 training and 358 testing document com-
prising the D section of the hierarchy. The
number of nodes in the hierarchy was 188, with
maximum depth 3. The dataset was processed
into bag-of-words representation with TF?IDF
1World Intellectual Property Organization, http://www.
wipo.int/classifications/en
600
input : training data set: (xn,yn), n = 1, ? ? ? , N ,
and parameters: C,K
output: w
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
get (xt,yt) from data set;
predict h?,h?;
calculate ?(w; (x,y)) and?(yt, y?t);
if ?(w; (x,y)) ? ?(yt, y?t) then
calculate ??L by Eq. (17);
update wt+1 by Eq. (18). ;
end
end
cw = cw +wT ;
end
w = cw/K ;
Algorithm 1:Hierarchical PA algorithmwith la-
tent concepts
weighting. No word stemming or stop-word
removal was performed. This dataset is used
in (Rousu et al, 2006).
LSHTC dataset The dataset2 has been constructed
by crawling web pages that are found in the
Open Directory Project (ODP) and translating
them into feature vectors (content vectors) and
splitting the set of Web pages into a training,
a validation and a test set, per ODP category.
Here, we use the dry-run dataset(task 1).
4.2 Performance Measurement
Macro Precision, Macro Recall and Macro F1 are
the most widely used performance measurements
for text classification problems nowadays. The
macro strategy computes macro precision and re-
call scores by averaging the precision/recall of each
category, which is preferred because the categories
are usually unbalanced and give more challenges to
classifiers. The Macro F1 score is computed using
the standard formula applied to the macro-level pre-
cision and recall scores.
MacroF1 = P ?R
P +R
, (19)
2Large Scale Hierarchical Text classification Pascal Chal-
lenge, http://lshtc.iit.demokritos.gr
Table 1: Results on WIPO-alpha Dataset.?-? means that
the result is not available in the author?s paper.
Accuracy F1 Precision Recall TIE
PA 49.16 40.71 43.27 38.44 2.06
HPA 50.84 40.26 43.23 37.67 1.92
LHPA 51.96 41.84 45.56 38.69 1.87
HSVM 23.8 - - - -
HM3 35.0 - - - -
Table 2: Results on LSHTC dry-run Dataset
Accuracy F1 Precision Recall TIE
PA 47.36 44.63 52.64 38.73 3.68
HPA 46.88 43.78 51.26 38.2 3.73
LHPA 48.39 46.26 53.82 40.56 3.43
where P is the Macro Precision and R is the Macro
Recall. We also use tree induced error (TIE) in the
experiments.
4.3 Results
We implement three algorithms3: PA(Flat PA), H-
PA(Hierarchical PA) and LHPA(Hierarchical PA
with latent concepts). The results are shown in Table
1 and 2. For WIPO-alpha dataset, we also compared
LHPA with two algorithms used in (Rousu et al,
2006): HSVM and HM3.
We can see that LHPA has better performances
than the other methods. From Table 2, we can see
that it is not always useful to incorporate the hierar-
chical information. Though the subclasses can share
information with their parent class, the shared infor-
mation may be different for each subclass. So we
should decompose the underlying factors into dif-
ferent latent concepts.
5 Conclusion
In this paper, we propose a variant Passive-
Aggressive algorithm for hierarchical text classifi-
cation with latent concepts. In the future, we will
investigate our method in the larger and more noisy
data.
Acknowledgments
This work was (partially) funded by NSFC (No.
61003091 and No. 61073069), 973 Program (No.
3Source codes are available in FudanNLP toolkit, http:
//code.google.com/p/fudannlp/
601
2010CB327906) and Shanghai Committee of Sci-
ence and Technology(No. 10511500703).
References
L. Cai and T. Hofmann. 2004. Hierarchical document
categorization with support vector machines. In Pro-
ceedings of CIKM.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine Learn-
ing, 28(1):41?75.
D. Koller and M Sahami. 1997. Hierarchically classify-
ing documents using very few words. In Proceedings
of the International Conference on Machine Learning
(ICML).
T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and W.Y.
Ma. 2005. Support vector machines classification
with a very large-scale taxonomy. ACM SIGKDD Ex-
plorations Newsletter, 7(1):43.
Youdong Miao and Xipeng Qiu. 2009. Hierarchical
centroid-based classifier for large scale text classifica-
tion. In Large Scale Hierarchical Text classification
(LSHTC) Pascal Challenge.
Xipeng Qiu, Wenjun Gao, and Xuanjing Huang. 2009.
Hierarchical multi-class text categorization with glob-
al margin maximization. In Proceedings of the ACL-
IJCNLP 2009 Conference, pages 165?168, Suntec,
Singapore, August. Association for Computational
Linguistics.
Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang. 2011.
An effective feature selection method for text catego-
rization. In Proceedings of the 15th Pacific-Asia Con-
ference on Knowledge Discovery and Data Mining.
Juho Rousu, Craig Saunders, Sandor Szedmak, and John
Shawe-Taylor. 2006. Kernel-based learning of hierar-
chical multilabel classification models. In Journal of
Machine Learning Research.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM computing surveys, 34(1):1?47.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining.
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. 2007.
Multi-task learning for classification with dirichlet
process priors. The Journal of Machine Learning Re-
search, 8:63.
Y. Yang and X. Liu. 1999. A re-examination of text
categorization methods. In Proc. of SIGIR. ACMPress
New York, NY, USA.
Y. Yang and J.O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proc. of
Int. Conf. on Mach. Learn. (ICML), volume 97.
602
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?439,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Latent Semantic Tensor Indexing
for Community-based Question Answering
Xipeng Qiu, Le Tian, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, tianlefdu@gmail.com, xjhuang@fudan.edu.cn
Abstract
Retrieving similar questions is very
important in community-based ques-
tion answering(CQA). In this paper,
we propose a unified question retrieval
model based on latent semantic index-
ing with tensor analysis, which can cap-
ture word associations among different
parts of CQA triples simultaneously.
Thus, our method can reduce lexical
chasm of question retrieval with the
help of the information of question con-
tent and answer parts. The experimen-
tal result shows that our method out-
performs the traditional methods.
1 Introduction
Community-based (or collaborative) ques-
tion answering(CQA) such as Yahoo! An-
swers1 and Baidu Zhidao2 has become a pop-
ular online service in recent years. Unlike tra-
ditional question answering (QA), information
seekers can post their questions on a CQA
website which are later answered by other
users. However, with the increase of the CQA
archive, there accumulate massive duplicate
questions on CQA websites. One of the pri-
mary reasons is that information seekers can-
not retrieve answers they need and thus post
another new question consequently. There-
fore, it becomes more and more important to
find semantically similar questions.
The major challenge for CQA retrieval is the
lexical gap (or lexical chasm) among the ques-
tions (Jeon et al, 2005b; Xue et al, 2008),
1http://answers.yahoo.com/
2http://zhidao.baidu.com/
Query:
Q: Why is my laptop screen blinking?
Expected:
Q1: How to troubleshoot a flashing
screen on an LCD monitor?
Not Expected:
Q2: How to blinking text on screen
with PowerPoint?
Table 1: An example on question retrieval
as shown in Table 1. Since question-answer
pairs are usually short, the word mismatch-
ing problem is especially important. However,
due to the lexical gap between questions and
answers as well as spam typically existing in
user-generated content, filtering and ranking
answers is very challenging.
The earlier studies mainly focus on generat-
ing redundant features, or finding textual clues
using machine learning techniques; none of
them ever consider questions and their answers
as relational data but instead model them as
independent information. Moreover, they only
consider the answers of the current question,
and ignore any previous knowledge that would
be helpful to bridge the lexical and se mantic
gap.
In recent years, many methods have been
proposed to solve the word mismatching prob-
lem between user questions and the questions
in a QA archive(Blooma and Kurian, 2011),
among which the translation-based (Riezler et
al., 2007; Xue et al, 2008; Zhou et al, 2011)
or syntactic-based approaches (Wang et al,
2009) methods have been proven to improve
the performance of CQA retrieval.
However, most of these approaches used
434
pipeline methods: (1) modeling word asso-
ciation; (2) question retrieval combined with
other models, such as vector space model
(VSM), Okapi model (Robertson et al, 1994)
or language model (LM). The pipeline meth-
ods often have many non-trivial experimental
setting and result to be very hard to repro-
duce.
In this paper, we propose a novel unified
retrieval model for CQA, latent semantic
tensor indexing (LSTI), which is an exten-
sion of the conventional latent semantic index-
ing (LSI) (Deerwester et al, 1990). Similar
to LSI, LSTI can integrate the two detached
parts (modeling word association and question
retrieval) into a single model.
In traditional document retrieval, LSI is an
effective method to overcome two of the most
severe constraints on Boolean keyword queries:
synonymy, that is, multiple words with similar
meanings, and polysemy, or words with more
than one meanings.
Usually in a CQA archive, each en-
try (or question) is in the following triple
form:?question title, question content,
answer?. Because the performance based
solely on the content or the answer part is
less than satisfactory, many works proposed
that additional relevant information should be
provided to help question retrieval(Xue et al,
2008). For example, if a question title contains
the keyword ?why?, the CQA triple, which
contains ?because? or ?reason? in its answer
part, is more likely to be what the user looks
for.
Since each triple in CQA has three parts, the
natural representation of the CQA collection
is a three-dimensional array, or 3rd-order ten-
sor, rather than a matrix. Based on the tensor
decomposition, we can model the word associ-
ation simultaneously in the pairs: question-
question, question-body and question-answer.
The rest of the paper is organized as fol-
lows: Section 3 introduces the concept of LSI.
Section 4 presents our method. Section 5 de-
scribes the experimental analysis. Section 6
concludes the paper.
2 Related Works
There are some related works on question re-
trieval in CQA. Various query expansion tech-
niques have been studied to solve word mis-
match problems between queries and docu-
ments. The early works on question retrieval
can be traced back to finding similar ques-
tions in Frequently Asked Questions (FAQ)
archives, such as the FAQ finder (Burke et al,
1997), which usually used statistical and se-
mantic similarity measures to rank FAQs.
Jeon et al (2005a; 2005b) compared four
different retrieval methods, i.e., the vector
space model(Jijkoun and de Rijke, 2005),
the Okapi BM25 model (Robertson et al,
1994), the language model, and the trans-
lation model, for question retrieval on CQA
data, and the experimental results showed
that the translation model outperforms the
others. However, they focused only on similar-
ity measures between queries (questions) and
question titles.
In subsequent work (Xue et al, 2008), a
translation-based language model combining
the translation model and the language model
for question retrieval was proposed. The
results showed that translation models help
question retrieval since they could effectively
address the word mismatch problem of ques-
tions. Additionally, they also explored an-
swers in question retrieval.
Duan et al (2008) proposed a solution that
made use of question structures for retrieval
by building a structure tree for questions in
a category of Yahoo! Answers, which gave
more weight to important phrases in question
matching.
Wang et al (2009) employed a parser to
build syntactic trees for questions, and ques-
tions were ranked based on the similarity be-
tween their syntactic trees and that of the
query question.
It is worth noting that our method is to-
tally different to the work (Cai et al, 2006)
of the same name. They regard documents
as matrices, or the second order tensors to
generate a low rank approximations of ma-
trices (Ye, 2005). For example, they convert
a 1, 000, 000-dimensional vector of word space
into a 1000 ? 1000 matrix. However in our
model, a document is still represented by a
vector. We just project a higher-dimensional
vector to a lower-dimensional vector, but not
a matrix in Cai?s model. A 3rd-order tensor is
435
also introduced in our model for better repre-
sentation for CQA corpus.
3 Latent Semantic Indexing
Latent Semantic Indexing (LSI) (Deer-
wester et al, 1990), also called Latent Seman-
tic Analysis (LSA), is an approach to auto-
matic indexing and information retrieval that
attempts to overcome these problems by map-
ping documents as well as terms to a represen-
tation in the so-called latent semantic space.
The key idea of LSI is to map documents
(and by symmetry terms) to a low dimen-
sional vector space, the latent semantic space.
This mapping is computed by decomposing
the term-document matrix N with SVD, N =
U?V t, where U and V are orthogonal matri-
ces U tU = V tV = I and the diagonal matrix
? contains the singular values of N . The LSA
approximation of N is computed by just keep
the largest K singular values in ?, which is
rank K optimal in the sense of the L2-norm.
LSI has proven to result in more robust word
processing in many applications.
4 Tensor Analysis for CQA
4.1 Tensor Algebra
We first introduce the notation and basic
definitions of multilinear algebra. Scalars are
denoted by lower case letters (a, b, . . . ), vectors
by bold lower case letters (a,b, . . . ), matri-
ces by bold upper-case letters (A,B, . . . ), and
higher-order tensors by calligraphic upper-case
letters (A,B, . . . ).
A tensor, also known as n-way array, is a
higher order generalization of a vector (first
order tensor) and a matrix (second order ten-
sor). The order of tensor D ? RI1?I2?????IN is
N . An element of D is denoted as di1,...,N .
An Nth-order tensor can be flattened into
a matrix by N ways. We denote the matrix
D(n) as the mode-n flattening of D (Kolda,
2002).
Similar with a matrix, an Nth-order tensor
can be decomposed through ?N -mode singu-
lar value decomposition (SVD)?, which is a an
extension of SVD that expresses the tensor as
the mode-n product of N -orthogonal spaces.
D = Z?1 U1?2 U2 ? ? ??n Un ? ? ??N UN . (1)
Tensor Z, known as the core tensor, is analo-
gous to the diagonal singular value matrix in
conventional matrix SVD. Z is in general a
full tensor. The core tensor governs the in-
teraction between the mode matrices Un, for
n = 1, . . . , N . Mode matrix Un contains the
orthogonal left singular vectors of the mode-n
flattened matrix D(n).
The N -mode SVD algorithm for decompos-
ing D is as follows:
1. For n = 1, . . . , N , compute matrix Un in
Eq.(1) by computing the SVD of the flat-
tened matrix D(n) and setting Un to be
the left matrix of the SVD.
2. Solve for the core tensor as follows Z =
D ?1 UT1 ?2 UT2 ? ? ? ?n UTn ? ? ? ?N UTN .
4.2 CQA Tensor
Given a collection of CQA triples, ?qi, ci, ai?
(i = 1, . . . ,K), where qi is the question and
ci and ai are the content and answer of qi
respectively. We can use a 3-order tensor
D ? RK?3?T to represent the collection, where
T is the number of terms. The first dimension
corresponds to entries, the second dimension,
to parts and the third dimension, to the terms.
For example, the flattened matrix of CQA
tensor with ?terms? direction is composed
by three sub-matrices MTitle, MContent and
MAnswer, as was illustrated in Figure 1. Each
sub-matrix is equivalent to the traditional
document-term matrix.
Figure 1: Flattening CQA tensor with ?terms?
(right matrix)and ?entries? (bottom matrix)
Denote pi,j to be part j of entry i. Then we
436
have the term frequency, defined as follows.
tfi,j,k =
ni,j,k?
i ni,j,k
, (2)
where ni,j,k is the number of occurrences of the
considered term (tk) in pi,j , and the denomi-
nator is the sum of number of occurrences of
all terms in pi,j .
The inverse document frequency is a mea-
sure of the general importance of the term.
idfj,k = log
|K|
1 +?i I(tk ? pi,j)
, (3)
where |K| is the total number of entries and
I(?) is the indicator function.
Then the element di,j,k of tensor D is
di,j,k = tfi,j,k ? idfj,k. (4)
4.3 Latent Semantic Tensor Indexing
For the CQA tensor, we can decompose it
as illustrated in Figure 2.
D = Z ?1 UEntry ?2 UPart ?3 UTerm, (5)
where UEntry, UPart and UTerm are left sin-
gular matrices of corresponding flattened ma-
trices. UTerm spans the term space, and we
just use the vectors corresponding to the 1, 000
largest singular values in this paper, denoted
as U?Term.
Figure 2: 3-mode SVD of CQA tensor
To deal with such a huge sparse data set, we
use singular value decomposition (SVD) im-
plemented in Apache Mahout3 machine learn-
ing library, which is implemented on top
of Apache Hadoop4 using the map/reduce
paradigm and scalable to reasonably large
data sets.
3http://mahout.apache.org/
4http://hadoop.apache.org
4.4 Question Retrieval
In order to retrieve similar question effec-
tively, we project each CQA triple Dq ?
R1?3?T to the term space by
D?i = Di ?3 U?TTerm. (6)
Given a new question only with title part,
we can represent it by tensor Dq ? R1?3?T ,
and its MContent and MAnswer are zero ma-
trices. Then we project Dq to the term space
and get D?q.
Here, D?q and D?i are degraded tensors and
can be regarded as matrices. Thus, we can cal-
culate the similarity between D?q and D?i with
normalized Frobenius inner product.
For two matrices A and B, the Frobenius
inner product, indicated as A : B, is the
component-wise inner product of two matrices
as though they are vectors.
A : B =
?
i,j
Ai,jBi,j (7)
To reduce the affect of length, we use the
normalized Frobenius inner product.
A : B = A : B?
A : A?
?
B : B
(8)
While given a new question both with title
and content parts, MContent is not a zero ma-
trix and could be also employed in the question
retrieval process. A simple strategy is to sum
up the scores of two parts.
5 Experiments
5.1 Datasets
We collected the resolved CQA triples from
the ?computer? category of Yahoo! Answers
and Baidu Zhidao websites. We just selected
the resolved questions that already have been
given their best answers. The CQA triples are
preprocessed with stopwords removal (Chinese
sentences are segmented into words in advance
by FudanNLP toolkit(Qiu et al, 2013)).
In order to evaluate our retrieval system, we
divide our dataset into two parts. The first
part is used as training dataset; the rest is used
as test dataset for evaluation. The datasets are
shown in Table 2.
437
DataSet training
data size
test data
size
Baidu Zhidao 423k 1000
Yahoo! Answers 300k 1000
Table 2: Statistics of Collected Datasets
Methods MAP
Okapi 0.359
LSI 0.387
(Jeon et al, 2005b) 0.372
(Xue et al, 2008) 0.381
LSTI 0.415
Table 3: Retrieval Performance on Dataset
from Yahoo! Answers
5.2 Evaluation
We compare our method with two baseline
methods: Okapi BM25 and LSI and two state-
of-the-art methods: (Jeon et al, 2005b)(Xue
et al, 2008). In LSI, we regard each triple
as a single document. Three annotators are
involved in the evaluation process. Given a
returned result, two annotators are asked to
label it with ?relevant? or ?irrelevant?. If an
annotator considers the returned result seman-
tically equivalent to the queried question, he
labels it as ?relevant?; otherwise, it is labeled
as ?irrelevant?. If a conflict happens, the third
annotator will make the final judgement.
We use mean average precision (MAP)
to evaluate the effectiveness of each method.
The experiment results are illustrated in Ta-
ble 3 and 4, which show that our method out-
performs the others on both datasets.
The primary reason is that we incorpo-
rate the content of the question body and
the answer parts into the process of ques-
tion retrieval, which should provide addi-
tional relevance information. Different to
Methods MAP
Okapi 0.423
LSI 0.490
(Jeon et al, 2005b) 0.498
(Xue et al, 2008) 0.512
LSTI 0.523
Table 4: Retrieval Performance on Dataset
from Baidu Zhidao
the translation-based methods, our method
can capture the mapping relations in three
parts (question, content and answer) simulta-
neously.
It is worth noting that the problem of data
sparsity is more crucial for LSTI since the size
of a tensor in LSTI is larger than a term-
document matrix in LSI. When the size of data
is small, LSTI tends to just align the common
words and thus cannot find the correspond-
ing relations among the focus words in CQA
triples. Therefore, more CQA triples may re-
sult in better performance for our method.
6 Conclusion
In this paper, we proposed a novel re-
trieval approach for community-based QA,
called LSTI, which analyzes the CQA triples
with naturally tensor representation. LSTI
is a unified model and effectively resolves the
problem of lexical chasm for question retrieval.
For future research, we will extend LSTI to
a probabilistic form (Hofmann, 1999) for bet-
ter scalability and investigate its performance
with a larger corpus.
Acknowledgments
We would like to thank the anony-
mous reviewers for their valuable com-
ments. This work was funded by NSFC
(No.61003091 and No.61073069) and 973 Pro-
gram (No.2010CB327900).
References
M.J. Blooma and J.C. Kurian. 2011. Research
issues in community based question answering.
In PACIS 2011 Proceedings.
R. Burke, K. Hammond, V. Kulyukin, S. Lytinen,
N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question
files: Experiences with the faq finder system.
AI Magazine, 18(2):57?66.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Ten-
sor space model for document analysis. In SI-
GIR ?06: Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research
and development in information retrieval.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. 1990. Index-
ing by latent semantic analysis. Journal of
the American society for information science,
41(6):391?407.
438
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and
Yong Yu. 2008. Searching questions by iden-
tifying question topic and question focus. In
Proceedings of ACL-08: HLT, pages 156?164,
Columbus, Ohio, June. Association for Compu-
tational Linguistics.
T. Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
50?57. ACM Press New York, NY, USA.
J. Jeon, W.B. Croft, and J.H. Lee. 2005a. Find-
ing semantically similar questions based on their
answers. In Proceedings of the 28th annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
617?618. ACM.
J. Jeon, W.B. Croft, and J.H. Lee. 2005b. Finding
similar questions in large question and answer
archives. Proceedings of the 14th ACM interna-
tional conference on Information and knowledge
management, pages 84?90.
V. Jijkoun and M. de Rijke. 2005. Retrieving an-
swers from frequently asked questions pages on
the web. Proceedings of the 14th ACM interna-
tional conference on Information and knowledge
management, pages 76?83.
T.G. Kolda. 2002. Orthogonal tensor decompo-
sitions. SIAM Journal on Matrix Analysis and
Applications, 23(1):243?255.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
Fudannlp: A toolkit for chinese natural lan-
guage processing. In Proceedings of ACL.
S. Riezler, A. Vasserman, I. Tsochantaridis,
V. Mittal, and Y. Liu. 2007. Statistical ma-
chine translation for query expansion in answer
retrieval. In Proceedings of the Annual Meeting
of the Association for Computational Linguis-
tics.
S.E. Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1994.
Okapi at trec-3. In TREC, pages 109?126.
K. Wang, Z. Ming, and T.S. Chua. 2009. A syn-
tactic tree matching approach to finding similar
questions in community-based QA services. In
Proceedings of the 32nd international ACM SI-
GIR conference on Research and development in
information retrieval, pages 187?194. ACM.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval
models for question and answer archives. In Pro-
ceedings of the 31st annual international ACM
SIGIR conference on Research and development
in information retrieval, pages 475?482. ACM.
J.M. Ye. 2005. Generalized low rank approxima-
tions of matrices. Mach. Learn., 61(1):167?191.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011.
Phrase-based translation model for question re-
trieval in community question answer archives.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages
653?662. Association for Computational Lin-
guistics.
439
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 49?54,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
FudanNLP: A Toolkit for Chinese Natural Language Processing
Xipeng Qiu, Qi Zhang, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, qz@fudan.edu.cn, xjhuang@fudan.edu.cn
Abstract
The growing need for Chinese natural
language processing (NLP) is largely
in a range of research and commer-
cial applications. However, most of
the currently Chinese NLP tools or
components still have a wide range
of issues need to be further improved
and developed. FudanNLP is an open
source toolkit for Chinese natural lan-
guage processing (NLP), which uses
statistics-based and rule-based meth-
ods to deal with Chinese NLP tasks,
such as word segmentation, part-of-
speech tagging, named entity recogni-
tion, dependency parsing, time phrase
recognition, anaphora resolution and so
on.
1 Introduction
Chinese is one of the most widely used lan-
guages in this world, and the proportion that
Chinese language holds on the Internet is also
quite high. Under the current circumstances,
there are greater and greater demands for in-
telligent processing and analyzing of the Chi-
nese texts.
Similar to English, the main tasks in Chi-
nese NLP include word segmentation (CWS),
part-of-speech (POS) tagging, named en-
tity recognition (NER), syntactic parsing,
anaphora resolution (AR), and so on. Al-
though the general ways are essentially the
same for English and Chinese, the implemen-
tation details are different. It is also non-
trivial to optimize these methods for Chinese
NLP tasks.
There are also some toolkits to be used
for NLP, such as Stanford CoreNLP1, Apache
OpenNLP2, Curator3 and NLTK4. But these
toolkits are developed mainly for English and
not optimized for Chinese.
In order to customize an optimized system
for Chinese language process, we implement
an open source toolkit, FudanNLP5, which is
written in Java. Since most of the state-of-the-
art methods for NLP are based on statistical
learning, the whole framework of our toolkit
is established around statistics-based meth-
ods, supplemented by some rule-based meth-
ods. Therefore, the quality of training data
is crucial for our toolkit. However, we find
that there are some drawbacks in currently
most commonly used corpora, such as CTB
(Xia, 2000) and CoNLL (Haji? et al, 2009)
corpora. For example, in CTB corpus, the set
of POS tags is relative small and some cate-
gories are derived from the perspective of En-
glish grammar. And in CoNLL corpus, the
head words are often interrogative particles
and punctuations, which are unidiomatic in
Chinese. These drawbacks bring more chal-
lenges to further analyses, such as informa-
tion extraction and semantic understanding.
Therefore, we first construct a corpus with
a modified guideline, which is more in ac-
cordance with the common understanding for
Chinese grammar.
In addition to the basic Chinese NLP tasks
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://incubator.apache.org/opennlp/
3http://cogcomp.cs.illinois.edu/page/
software_view/Curator
4http://www.nltk.org/
5http://fudannlp.googlecode.com
49
Figure 1: System Structure of FudanNLP
mentioned above, the toolkit also provides
many minor functions, such as text classifi-
cation, dependency tree kernel, tree pattern-
based information extraction, keywords ex-
traction, translation between simplified and
traditional Chinese, and so on.
Currently, our toolkit has been used by
many universities and companies for various
applications, such as the dialogue system, so-
cial computing, recommendation system and
vertical search.
The rest of the demonstration is organized
as follows. We first briefly describe our system
and its main components in section 2. Then we
show system performances in section 3. Sec-
tion 4 introduces three ways to use our toolkit.
In section 5, we summarize the paper and give
some directions for our future efforts.
2 System Overview
The components of our system have three
layers of structure: data preprocessing, ma-
chine learning and natural language process-
ing, which is shown in Figure 1. We will in-
troduce these components in detail in the fol-
lowing subsections.
2.1 Data Preprocessing Component
In the natural language processing system,
the original input is always text. However,
the statistical machine learning methods often
deal with data with vector-based representa-
tion. So we firstly need to preprocess the input
texts and transform them to the required for-
mat. Due to the fact that text data is usually
discrete and sparse, the sparse vector struc-
ture is largely used. Similar to Mallet (Mc-
Callum, 2002), we use the pipeline structure
for a flexible transformation of various data.
The pipeline consists of several serial or par-
allel modules. Each module, called ?pipe?, is
aimed at a single and simple function.
For example, when we transform a sentence
into a vector with ?bag-of-words?, the trans-
formation process would involve the following
serial pipes:
1. String2Token Pipe: to transform a string
into word tokens.
2. Token2Index Pipe: to look up the word
alphabet to get the indices of the words.
3. WeightByFrequency Pipe: to calculate
the vector weight for each word accord-
ing to its frequency of occurrence.
With the pipeline structure, the data pre-
processing component has good flexibility, ex-
tensibility and reusability.
2.2 Machine Learning Component
The outputs of NLP are often structured,
so the structured learning is our core module.
Structured learning is the task of assigning a
structured label y to an input x. The label y
can be a discrete variable, a sequence, a tree
or a more complex structure.
To illustrate by a sample x, we define the
feature as ?(x,y). Thus, we can label x with
a score function,
y? = arg max
y
F (w,?(x,y)), (1)
where w is the parameter of function F (?).
The feature vector ?(x,y) consists of lots of
overlapping features, which is the chief benefit
of a discriminative model.
For example, in sequence labeling, both x =
x1, . . . , xL and y = y1, . . . , yL are sequences.
For first-order Markov sequence labeling, the
feature can be denoted as ?k(yi?1, yi,x, i),
where i is the position in the sequence. Then
the score function can be rewritten as
y? = arg max
y
F (
L?
i=1
?
k
wk?k(yi?1, yi,x, i)), (2)
where L is the length of x.
Different algorithms vary in the definition of
F (?) and the corresponding objective function.
50
F (?) is usually defined as a linear or exponen-
tial family function. For example, in condi-
tional random fields (CRFs) (Lafferty et al,
2001), F (?) is defined as:
Pw(y|x) =
1
Zw
exp(wT?(x,y)), (3)
where Zw is the normalization constant such
that it makes the sum of all the terms one.
In FudanNLP, the linear function is univer-
sally used as the objective function. Eq. (1) is
written as:
y? = arg max
y
< w,?(x,y) > . (4)
2.2.1 Training
In the training stage, we use the passive-
aggressive algorithm to learn the model pa-
rameters. Passive-aggressive (PA) algorithm
(Crammer et al, 2006) was proposed for nor-
mal multi-class classification and can be easily
extended to structure learning (Crammer et
al., 2005). Like Perceptron, PA is an online
learning algorithm.
2.2.2 Inference
For consistency with statistical machine
learning, we call the process to calculate the
Eq.(1) as ?inference?. In structured learning,
the number of possible solutions is very huge,
so dynamic programming or approximate ap-
proaches are often used for efficiency. For NLP
tasks, the most popular structure is sequence.
To label the sequence, we use Viterbi dynamic
programming to solve the inference problem in
Eq. (4).
Our system can support any order of Viterbi
decoding. In addition, we also implement a
constrained Viterbi algorithm to reduce the
number of possible solutions by pre-defined
rules. For example, when we know the prob-
able labels, we delete the unreachable states
from state transition matrix. It is very useful
for CWS and POS tagging with sequence la-
beling. When we have a word dictionary or
know the POS for some words, we can get
more accurate results.
2.2.3 Other Algorithms
Apart from the core modules of structured
learning, our system also includes several tra-
ditional machine learning algorithms, such as
Perceptron, Adaboost, kNN, k-means, and so
on.
2.3 Natural Language Processing
Components
Our toolkit provides the basic NLP func-
tions, such as word segmentation, part-of-
speech tagging, named entity recognition, syn-
tactic parsing, temporal phrase recognition,
anaphora resolution, and so on. These func-
tions are trained on our developed corpus. We
also develop a visualization module to display-
ing the output. Table 1 shows the output rep-
resentation of our toolkit.
2.3.1 Chinese Word Segmentation
Different from English, Chinese sentences
are written in a continuous sequence of char-
acters without explicit delimiters such as the
blank space. Since the meanings of most Chi-
nese characters are not complete, words are
the basic syntactic and semantic units. There-
fore, it is indispensable step to segment the
sentence into words in Chinese language pro-
cessing.
We use character-based sequence labeling
(Peng et al, 2004) to find the boundaries of
words. Besides the carefully chosen features,
we also use the meaning of character drawn
from HowNet(Dong and Dong, 2006), which
improves the performance greatly. Since un-
known words detection is still one of main chal-
lenges of Chinese word segmentation. We im-
plement a constrained Viterbi algorithm to al-
low users to add their own word dictionary.
2.3.2 POS tagging
Chinese POS tagging is very different from
that in English. There are no morphological
changes for a word among its different POS
tags. Therefore, most of Chinese words may
have multiple POS tags. For example, there
are different morphologies in English for the
word ??? (destroy)?, such as ?destroyed?,
?destroying? and ?destruction?. But in Chi-
nese, there is just one same form(Xia, 2000).
There are two popular guidelines to tag the
word?s POS: CTB (Xia, 2000) and PKU (Yu
et al, 2001). We take into account both
the weaknesses and the strengths of these two
guidelines, and propose our guideline for bet-
ter subsequent analyses, such as parser and
named entity recognition. For example, the
proper name is labeled as ?NR? in CTB, while
we label it with one of four categories: person,
51
Input:
??????????? 1980 ??
John is from Washington, and he was born in 1980.
Output:
.
.?? .?? .??? .? .? .?? .1980 ? .?
.John .is from .Washington ., .he .was born in .1980 ..
.PER .VV .LOC .PU .PRN .NN .PU
.1 .2 .3 .4 .5 .6 .7 .8
Root
SUB
CS:COO1
OBJ
PUN
SUB OBJ
PUN
NER:
1 ? PER
3 ? LOC
AR:
5 ? 1
TIME:
7 ? 1980
1 CS:COO means the coordinate complex sentence.
Table 1: Example of the output representation of our toolkit
location, organization and other proper name.
Conversely, we merge the ?VC? and ?VE? into
?VV? since there is no link verb in Chinese.
Finally, we use a tag set with 39 categories in
total.
Since a POS tag is assigned to each word,
not to each character, Chinese POS tag-
ging has two ways: pipeline method or joint
method. Currently, the joint method is more
popular and effective because it uses more flex-
ible features and can reduce the error propa-
gation (Ng and Low, 2004). In our system,
we implement both methods for POS tagging.
Besides, we also use some knowledge to im-
prove the performance, such as Chinese sur-
name and the common suffixes of the names
of locations and organizations.
2.3.3 Named Entity Recognition
In Chinese named entity recognition (NER),
there are usually three kinds of named enti-
ties (NEs) to be dealt with: names of per-
sons (PER) , locations (LOC) and organiza-
tions (ORG). Unlike English, there is no obvi-
ous identification for NEs, such as initial capi-
tals. The internal structures are also different
for different kinds of NEs, so it is difficult to
build a unified model for named entity recog-
nition.
Our NER is based on the results of POS
tagging and uses some customize features to
detect NEs. First, the number of NEs is very
large and the new NEs are endlessly emerg-
ing, so it is impossible to store them in dic-
tionary. Since the internal structures are rela-
tively more important, we use language mod-
els to capture the internal structures. Second,
we merge the continuous NEs with some rule-
based strategies. For example, we combine the
continuous words ???/NN???/NN? into
? ?????/LOC?.
2.3.4 Dependency parsing
Our syntactic parser is currently a depen-
dency parser, which is implemented with the
shift-reduce deterministic algorithm based on
the work in (Yamada and Matsumoto, 2003).
The syntactic structure of Chinese is more
complex than that of English, and semantic
meaning is more dominant than syntax in Chi-
nese sentences. So we select the dependency
parser to avoid the minutiae in syntactic con-
stituents and wish to pay more attention to
the subsequent semantic analysis. Since the
structure of the Chinese language is quite dif-
ferent from that of English, we use more effec-
tive features according to the characteristics of
Chinese sentences.
The common used corpus for Chinese de-
pendency parsing is CoNLL corpus (Haji? et
al., 2009). However, there are some illogical
cases in CoNLL corpus. For example, the
head words are often interrogative particles
and punctuations. Our guideline is based on
common understanding for Chinese grammar.
The Chinese syntactic components usually in-
clude subject, predicate, object, attribute, ad-
verbial modifier and complement. Figure 2
and 3 show the differences between the trees of
CoNLL and our Corpus. Table 2 shows some
52
primary dependency relations in our guideline.
..? .? .??? .? .? .? .?.want to .go to .Hehuanshan .to see .the snow . .?
.VV .VV .NR .VV .NN .SP .PU
Root
COMP
ADV
COMP
COMP
COMP UNK
Figure 2: Dependency Tree in CoNLL Corpus
..? .? .??? .? .? .? .?.want to .go to .Hehuanshan .to see .the snow . .?
.MD .VV .LOC .VV .NN .SP .PU
Root
ADV
OBJ
OBJ
OBJ
VOC
PUN
Figure 3: Dependency Tree in Our Corpus
Relations Chinese Definitions
SUB ?? Subject
PRED ?? Predicate
OBJ ?? Object
ATT ?? Attribute
ADV ?? Adverbial Modifier
COMP ?? Complement
SVP ?? Serial Verb Phrases
SUB-OBJ ?? Pivotal Construction
VOC ?? Voice
TEN ?? Tense
PUN ?? Punctuation
Table 2: Some primary dependency relations
2.3.5 Temporal Phrase Recognition
and Normalization
Chinese temporal phrases is more flexible
than English. Firstly, there are two calendars:
Gregorian and lunar calendars. Both of them
are frequently used. Secondly, the forms of
same temporal phrase are various, which often
consists of Chinese characters, Arabic numer-
als and English letters, such as ??? 10 ??
and ?10:00 PM?.
Different from the general process based
on machine learning, we implement the time
phrase recognizer with a rule-based method.
These rules include 376 regular expressions
and nearly a hundred logical judgments.
After recognizing the temporal phrases, we
normalize them with a standard time format.
For a phrase indicating a relative time , such
as ????? and ? ?????, we first find the
base time in the context. If no base time is
found, or there is also no temporal phrase to
indicate the base time (such as ????), we
set the base time to the current system time.
Table 3 gives examples for our temporal phrase
recognition module.
Input:
08 ?????????8 ? 8 ??????????
????????????
The Beijing Olympic Games took place from Au-
gust 8, 2008. Four years later, the London Olympic
Games took place from July 21.
???????? 9 ?????????????
I?m busy today, and have to come off duty after 9:00
PM. And I also have to work this Sunday.
Output:
08 ? (2008) 2008
8 ? 8 ? (August 8) 2008-8-8
?????? (July 21) 2012-7-27
?? (today) 2012-2-221
?? 9 ? (9:00 PM) 2012-2-22 21:00
?? (this Sunday) 2012-2-26
1 The base time is 2012-02-22 10:00AM.
Table 3: Examples for Temporal Phrase
Recognition
2.3.6 Anaphora Resolution
Anaphora resolution is to detect the pro-
nouns and find what they are referring to.
We first find all pronouns and entity names,
then use a classifier to predict whether there
is a relation between each pair of pronoun and
entity name. Table 4 gives examples for our
anaphora resolution module.
Input:
??????? 1167 ?????????????
?????????????
Oxford University is founded in 1167. It is located
in Oxford, UK. The university has nurtured a lot
of good students.
Output:
? (It) ????
???? (The
university)
???? (Oxford University)
Table 4: Examples for Anaphora Resolution
3 System Performances
In this section, we investigate the per-
formances for the six tasks: Chinese word
segmentation (CWS), POS tagging (POS),
53
named entity recognition (NER) and de-
pendency parser(DePar), Temporal Phrase
Recognition (TPR) and Anaphora Resolution
(AR). We use 5-fold cross validation on our
developed corpus. The corpus includes 65, 745
sentences and 959, 846 words. The perfor-
mances are shown in Table 5.
Task Accuracy Speed1 Memory
CWS 97.5% 98.9K 66M
POS 93.4% 44.5K 110M
NER 98.40% 38K 30M
DePar 85.3% 21.1 80M
TPR 95.16% 22.9k 237K
AR 70.3% 35.7K 52K
1 characters per second. Test environment:
CPU 2.67GHz, JRE 7.
Table 5: System Performances
4 Usages
We provide three ways to use our toolkit.
Firstly, our toolkit can be used as library.
Users can call application programming inter-
faces (API) in their own applications.
Secondly, users can also invoke the main
NLP modules to process the inputs (strings
or files) from the command line directly.
Thirdly, the web services are provided
for platform-independent and language-
independent use. We use a REST (Represen-
tational State Transfer) architecture, in which
the web services are viewed as resources and
can be identified by their URLs.
5 Conclusions
In this demonstration, we have described
the system, FudanNLP, which is a Java-based
open source toolkit for Chinese natural lan-
guage processing. In the future, we will add
more functions, such as semantic parsing. Be-
sides, we will also optimize the algorithms and
codes to improve the system performances.
Acknowledgments
We would like to thank all the people6
involved with our FudanNLP project. This
work was funded by NSFC (No.61003091
6https://code.google.com/p/fudannlp/wiki/
People
and No.61073069) and 973 Program
(No.2010CB327900).
References
K. Crammer, R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for struc-
tured classification. In NIPS Workshop on
Learning With Structured Outputs. Citeseer.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of
Machine Learning Research, 7:551?585.
Z. Dong and Q. Dong. 2006. Hownet And the
Computation of Meaning. World Scientific Pub-
lishing Co., Inc. River Edge, NJ, USA.
J. Haji?, M. Ciaramita, R. Johansson, D. Kawa-
hara, M.A. Mart?, L. M?rquez, A. Meyers,
J. Nivre, S. Pad?, J. ?t?p?nek, et al 2009. The
CoNLL-2009 shared task: Syntactic and seman-
tic dependencies in multiple languages. In Pro-
ceedings of the Thirteenth Conference on Com-
putational Natural Language Learning: Shared
Task, pages 1?18. Association for Computa-
tional Linguistics.
John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
H.T. Ng and J.K. Low. 2004. Chinese part-
of-speech tagging: one-at-a-time or all-at-once?
word-based or character-based. In Proceedings
of EMNLP, volume 4.
F. Peng, F. Feng, and A. McCallum. 2004. Chi-
nese segmentation and new word detection us-
ing conditional random fields. Proceedings of the
20th international conference on Computational
Linguistics.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0).
H. Yamada and Y. Matsumoto. 2003. Statis-
tical dependency analysis with support vector
machines. In Proceedings of the International
Workshop on Parsing Technologies (IWPT),
volume 3.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern chinese corpus. Technical
report, Technical report.
54
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 32?39,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Detecting Hedge Cues and their Scopes with Average Perceptron
Feng Ji, Xipeng Qiu, Xuanjing Huang
Fudan University
{fengji,xpqiu,xjhuang}@fudan.edu.cn
Abstract
In this paper, we proposed a hedge de-
tection method with average perceptron,
which was used in the closed challenge
in CoNLL-2010 Shared Task. There are
two subtasks: (1) detecting uncertain sen-
tences and (2) identifying the in-sentence
scopes of hedge cues. We use the unified
learning algorithm for both subtasks since
that the hedge score of sentence can be de-
composed into scores of the words, espe-
cially the hedge words. On the biomedical
corpus, our methods achieved F-measure
with 77.86% in detecting in-domain un-
certain sentences, 77.44% in recognizing
hedge cues, and 19.27% in identifying the
scopes.
1 Introduction
Detecting hedged information in biomedical lit-
eratures has received considerable interest in the
biomedical natural language processing (NLP)
community recently. Hedge information indicates
that authors do not or cannot back up their opin-
ions or statements with facts (Szarvas et al, 2008),
which exists in many natural language texts, such
as webpages or blogs, as well as biomedical liter-
atures.
For many NLP applications, such as question
answering and information extraction, the infor-
mation extracted from hedge sentences would be
harmful to their final performances. Therefore,
the hedge or speculative information should be
detected in advance, and dealt with different ap-
proaches or discarded directly.
In CoNLL-2010 Shared Task (Farkas et al,
2010), there are two different level subtasks: de-
tecting sentences containing uncertainty and iden-
tifying the in-sentence scopes of hedge cues.
For example, in the following sentence:
These results suggest that the IRE motif
in the ALAS mRNA is functional and
imply that translation of the mRNA is
controlled by cellular iron availability
during erythropoiesis.
The words suggest and imply indicate that the
statements are not supported with facts.
In the first subtask, the sentence is considered
as uncertainty.
In the second subtask, suggest and imply are
identified as hedge cues, while the consecutive
blocks suggest that the IRE motif in the ALAS
mRNA is functional and imply that translation of
the mRNA is controlled by cellular iron availabil-
ity during erythropoiesis are recognized as their
corresponding scopes.
In this paper, we proposed a hedge detec-
tion method with average perceptron (Collins,
2002), which was used in the closed challenges in
CoNLL-2010 Shared Task (Farkas et al, 2010).
Our motivation is to use a unified model to de-
tect two level hedge information (word-level and
sentence-level) and the model is easily expanded
to joint learning of two subtasks. Since that the
hedge score of sentence can be decomposed into
scores of the words, especially the hedge words,
we chosen linear classifier in our method and used
average perceptron as the training algorithm.
The rest of the paper is organized as follows. In
Section 2, a brief review of related works is pre-
sented. Then, we describe our method in Section
3. Experiments and results are presented in the
section 4. Finally, the conclusion will be presented
in Section 5.
2 Related works
Although the concept of hedge information has
been introduced in linguistic community for a
long time, researches on automatic hedge detec-
tion emerged from machine learning or compu-
32
tational linguistic perspective in recent years. In
this section, we give a brief review on the related
works.
For speculative sentences detection, Medlock
and Briscoe (2007) report their approach based
on weakly supervised learning. In their method,
a statistical model is initially derived from a seed
corpus, and then iteratively modified by augment-
ing the training dataset with unlabeled samples
according the posterior probability. They only
employ bag-of-words features. On the public
biomedical dataset1, their experiments achieve the
performance of 0.76 in BEP (break even point).
Although they also introduced more linguistic fea-
tures, such as part-of-speech (POS), lemma and
bigram (Medlock, 2008), there are no significant
improvements.
In Ganter and Strube (2009), the same task on
Wikipedia is presented. In their system, score of a
sentence is defined as a normalized tangent value
of the sum of scores over all words in the sentence.
Shallow linguistic features are introduced in their
experiments.
Morante and Daelemans (2009) present their re-
search on identifying hedge cues and their scopes.
Their system consists of several classifiers and
works in two phases, first identifying the hedge
cues in a sentence and secondly finding the full
scope for each hedge cue. In the first phase, they
use IGTREE algorithm to train a classifier with
3 categories. In the second phase, three different
classifiers are trained to find the first token and last
token of in-sentence scope and finally combined
into a meta classifier. The experiments shown
that their system achieves an F1 of nearly 0.85
of identifying hedge cues in the abstracts sub cor-
pus, while nearly 0.79 of finding the scopes with
predicted hedge cues. More experiments could
be found in their paper (Morante and Daelemans,
2009). They also provide a detail statistics on
hedge cues in BioScope corpus2.
3 Hedge detection with average
perceptron
3.1 Detecting uncertain sentences
The first subtask is to identify sentences con-
taining uncertainty information. In particular,
1http://www.benmedlock.co.uk/
hedgeclassif.html
2http://www.inf.u-szeged.hu/rgai/
bioscope
this subtask is a binary classification problem at
sentence-level.
We define the score of sentence as the confi-
dence that the sentence contains uncertainty infor-
mation.
The score can be decomposed as the sum of the
scores of all words in the sentence,
S(x, y) = ?
xi?x
s(xi, y) =
?
xi?x
wT?(xi, y)
where, x denotes a sentence and xi is the i-
th word in the sentence x, ?(xi, y) is a sparse
high-dimensional binary feature vector of word xi.
y ? {uncertain, certain} is the category of the
sentence. For instance, in the example sentence,
if current word is suggest while the category of
this sentence is uncertain, the following feature is
hired,
?n(xi, y) =
{
1, if xi=??suggest??y=??uncertain?? ,
0, otherwise
where n is feature index.
This representation is commonly used in struc-
tured learning algorithms. We can combine the
features into a sparse feature vector ?(x, y) =?
i ?(xi, y).
S(x, y) = wT?(x, y) = ?
xi?x
wT?(xi, y)
In the predicting phase, we assign x to the cate-
gory with the highest score,
y? = argmax
y
wT?(x, y)
We learn the parameters w with online learning
framework. The most common online learner is
the perceptron (Duda et al, 2001). It adjusts pa-
rameters w when a misclassification occurs. Al-
though this framework is very simple, it has been
shown that the algorithm converges in a finite
number of iterations if the data is linearly separa-
ble. Moreover, much less training time is required
in practice than the batch learning methods, such
as support vector machine (SVM) or conditional
maximum entropy (CME).
Here we employ a variant perceptron algorithm
to train the model, which is commonly named
average perceptron since it averages parameters
w across iterations. This algorithm is first pro-
posed in Collins (2002). Many experiments of
33
NLP problems demonstrate better generalization
performance than non averaged parameters. More
theoretical proofs can be found in Collins (2002).
Different from the standard average perceptron al-
gorithm, we slightly modify the average strategy.
The reason to this modification is that the origi-
nal algorithm is slow since parameters accumulate
across all iterations. In order to keep fast training
speed and avoid overfitting at the same time, we
make a slight change of the parameters accumu-
lation strategy, which occurs only after each iter-
ation over the training data finished. Our training
algorithm is shown in Algorithm 1.
input : training data set:
(xn, yn), n = 1, ? ? ? , N ,
parameters: average number: K,
maximum iteration number: T .
output: average weight: cw
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt, yt);
predict: y?t = argmaxy wTt ?(xt, y) ;
if y?t 6= yt then
wt+1 = wt+?(xt, yt)??(xt, y?t)
end
end
cw = cw +wT ;
end
cw = cw/K ;
Algorithm 1: Average Perceptron algorithm
Binary context features are extracted from 6
predefined patterns, which are shown in Figure 1.
By using these patterns, we can easily obtain the
complicate features. As in the previous example,
if the current word is suggest, then a new com-
pound feature could be extracted in the form of
w?1 =results//w0 =suggest by employing the pat-
tern w?1w0. // is the separate symbol.
3.2 Identifying hedge cues and their scopes
Our approach for the second subtask consists of
two phases: (1) identifying hedge cues in a sen-
tence, then (2) recognizing their corresponding
scopes.
3.2.1 Identifying hedge cues
Hedge cues are the most important clues for de-
termining whether a sentence contains uncertain
? unigram: w0,p0
? bigram: w0w1, w0p0, p0p1
? trigram: w?1w0w1
Figure 1: Patterns employed in the sentence-level
hedge detection. Here w denotes single word, p is
part of speech, and the subscript denotes the rela-
tive offset compared with current position.
? unigram: w?2, w?1, w0, w1, w2, p0
? bigram: w?1w0, w0w1, w0p0, p?1p0, p0p1
? trigram: w?1w0w1
Figure 2: Patterns employed in the word-level
hedge detection.
information. Therefore in this phase, we treat the
problem of identifying hedge cues as a classifica-
tion problem. Each word in a sentence would be
predicted a category indicating whether this word
is a hedge cue word or not. In the previous ex-
ample, there are two different hedge cues in the
sentence (show in bold manner). Words suggest
and imply are assigned with the category CUE de-
noting hedge cue word, while other words are as-
signed with label O denoting non hedge cue word.
In our system, this module is much similar to
the module of detecting uncertain sentences. The
only difference is that this phase is word level. So
that each training sample in this phase is a word,
while in detecting speculative sentences training
sample is a sentence. The training algorithm is the
same as the algorithm shown in Algorithm 1. 12
predefined patterns of context features are shown
in Figure 2.
3.2.2 Recognizing in-sentence scopes
After identifying the hedge cues in the first phase,
we need to recognize their corresponding in-
sentence scopes, which means the boundary of
scope should be found within the same sentence.
We consider this problem as a word-cue pair
classification problem, where word is any word
in a sentence and cue is the identified hedge cue
word. Similar to the previous phase, a word-level
linear classifier is trained to predict whether each
34
word-cue pair in a sentence is in the scope of the
hedge cue.
Besides base context features used in the pre-
vious phase, we introduce additional syntactic de-
pendency features. These features are generated
by a first-order projective dependency parser (Mc-
Donald et al, 2005), and listed in Figure 3.
The scopes of hedge cues are always covering
a consecutive block of words including the hedge
cue itself. The ideal method should recognize only
one consecutive block for each hedge cue. How-
ever, our classifier cannot work so well. Therefore,
we apply a simple strategy to process the output
of the classifier. The simple strategy is to find a
maximum consecutive sequence which covers the
hedge cue. If a sentence is considered to contain
several hedge cues, we simply combine the con-
secutive sequences, which have at least one com-
mon word, to a large block and assign it to the
relative hedge cues.
4 Experiments
In this section, we report our experiments on
datasets of CoNLL-2010 shared tasks, including
the official results and our experimental results
when developing the system.
Our system architecture is shown in Figure 4,
which consists of the following modules.
1. corpus preprocess module, which employs a
tokenizer to normalize the corpus;
2. sentence detection module, which uses a bi-
nary sentence-level classifier to determine
whether a sentence contains uncertainty in-
formation;
3. hedge cues detection module, which identi-
fies which words in a sentence are the hedge
cues, we train a binary word-level classifier;
4. cue scope recognition module, which recog-
nizes the corresponding scope for each hedge
cue by another word-level classifier.
Our experimental results are obtained on the
training datasets by 10-fold cross validation. The
maximum iteration number for training the aver-
age perceptron is set to 20. Our system is imple-
mented with Java3.
3http://code.google.com/p/fudannlp/
biomedical Wikipedia
#sentences 14541 11111
#words 382274 247328
#hedge sentences 2620 2484
%hedge sentences 0.18 0.22
#hedge cues 3378 3133
average number 1.29 1.26
average cue length 1.14 2.45
av. scope length 15.42 -
Table 1: Statistical information on annotated cor-
pus.
4.1 Datasets
In CoNLL-2010 Shared Task, two different
datasets are provided to develop the system: (1)
biological abstracts and full articles from the Bio-
Scope corpus, (2) paragraphs from Wikipedia. Be-
sides manually annotated datasets, three corre-
sponding unlabeled datasets are also allowed for
the closed challenges. But we have not employed
any unlabeled datasets in our system.
A preliminary statistics can be found in Ta-
ble 1. We make no distinction between sen-
tences from abstracts or full articles in biomedi-
cal dataset. From Table 1, most sentences are cer-
tainty while about 18% sentences in biomedical
dataset and 22% in Wikipedia dataset are spec-
ulative. On the average, there exists nearly 1.29
hedge cues per sentence in biomedical dataset and
1.26 in Wikipedia. The average length of hedge
cues varies in these two corpus. In biomedical
dataset, hedge cues are nearly one word, but more
than two words in Wikipedia. On average, the
scope of hedge cue covers 15.42 words.
4.2 Corpus preprocess
The sentence are processed with a maximum-
entropy part-of-speech tagger4 (Toutanova et al,
2003), in which a rule-based tokenzier is used to
separate punctuations or other symbols from reg-
ular words. Moreover, we train a first-order pro-
jective dependency parser with MSTParser5 (Mc-
Donald et al, 2005) on the standard WSJ training
corpus, which is converted from constituent trees
to dependency trees by several heuristic rules6.
4http://nlp.stanford.edu/software/
tagger.shtml
5http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
6http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
35
? word-cue pair: current word and the hedge cue word pair,
? word-cue POS pair: POS pair of current word and the hedge cue word,
? path of POS: path of POS from current word to the hedge cue word along dependency
tree,
? path of dependency: relation path of dependency from current word to the hedge cue
word along dependency tree,
? POS of hedge cue word+direction: POS of hedge cue word with the direction to the
current word. Here direction can be ?LEFT? if the hedge cue is on the left to the current
word, or ?RIGHT? on the right,
? tree depth: depth of current in the corresponding dependency tree,
? surface distance: surface distance between current word and the hedge cue word. The
value of this feature is always 10 in the case of surface distance greater than 10,
? surface distance+tree depth: combination of surface distance and tree depth
? number of punctuations: number of punctuations between current word and the hedge
cue word,
? number of punctuations + tree depth: combination of number of punctuations and tree
depth
Figure 3: Additional features used in recognizing in-sentence scope
4.3 Uncertain sentences detection
In the first subtask, we carry out the experiments
within domain and cross domains. As previously
mentioned, we do not use the unlabeled datasets
and make no distinction between abstracts and full
articles in biomedical dataset. This means we
train the models only with the official annotated
datasets. The model for cross-domain is trained
on the combination of annotated biomedical and
Wikipedia datasets.
In this subtask, evaluation is carried out on the
sentence level and F-measure of uncertainty sen-
tences is employed as the chief metric.
Table 2 shows the results within domain. Af-
ter 10-fold cross validation over training dataset,
we achieve 84.39% of F1-measure on biomedical
while 56.06% on Wikipedia.
We analyzed the low performance of our sub-
mission result on Wikipedia. The possible rea-
son is our careless work when dealing with the
trained model file. Therefore we retrain a model
for Wikipedia and the performance is listed on the
bottom line (Wikipedia?) in Table 2.
Dataset Precision Recall F1
10-fold cross validation
biomedical 91.03 78.66 84.39
Wikipedia 66.54 48.43 56.06
official evaluation
biomedical 79.45 76.33 77.86
Wikipedia 94.23 6.58 1.23
Wikipedia? 82.19 32.86 46.95
Table 2: Results for in-domain uncertain sentences
detection
Table 3 shows the results across domains. We
split each annotated dataset into 10 folds. Then
training dataset is combined by individually draw-
ing 9 folds out from the split datasets and the
rests are used as the test data. On biomedical
dataset, F1-measure gets to 79.24% while 56.16%
on Wikipedia dataset. Compared with the results
within domain, over 5% performance decreases
from 84.39% to 79.24% on biomedical, but a
slightly increase on Wikipedia.
36
Figure 4: System architecture of our system
Dataset Precision Recall F1
10-fold cross validation
biomedical 87.86 72.16 79.24
Wikipedia 67.78 47.95 56.16
official evaluation
biomedical 62.81 79.11 70.03
Wikipedia 62.66 55.28 58.74
Table 3: Results for across-domain uncertain sen-
tences detection
4.3.1 Results analysis
We investigate the weights of internal features and
found that the words, which have no uncertainty
information, also play the significant roles to pre-
dict the uncertainty of the sentence.
Intuitively, the words without uncertainty infor-
mation should just have negligible effect and the
corresponding features should have low weights.
However, this ideal case is difficult to reached by
learning algorithm due to the sparsity of data.
In Table 4, we list the top 10 words involved
in features with the largest weights for each cate-
gory. These words are ranked by the accumulative
scores of their related features.
In Table 5, we list the top 10 POS involved in
features with the largest weight for each category.
4.4 Hedge cue identification
Hedge cues identification is one module for the
second subtask, we also analyze the performance
on this module.
Since we treat this problem as a binary classi-
fication problem, we evaluate F-measure of hedge
cue words. The results are listed in Table 6.
We have to point out that our evaluation is
Dataset Precision Recall F1
10-fold cross validation(word-level)
biomedical 90.15 84.43 87.19
Wikipedia 57.74 39.81 47.13
official evaluation(phrase-level)
biomedical 78.7 76.22 77.44
Table 6: Results for in-domain hedge cue identifi-
cation
based on word while official evaluation is based
on phrase. That means our results would seem
to be higher than the official results, especially on
Wikipedia dataset because average length of hedge
cues in Wikipedia dataset is more than 2 words.
4.4.1 Result Analysis
We classify the results into four categories: false
negative, false positive, true positive and true neg-
ative. We found that most mistakes are made be-
cause of polysemy and collocation.
In Table 7, we list top 10 words for each cate-
gory. For the false results, the words are difficult to
distinguish without its context in the correspond-
ing sentence.
4.5 Scopes recognition
For recognizing the in-sentence scopes, F-measure
is also used to evaluate the performance of the
word-cue pair classifier. The results using gold
hedge cues are shown in Table 8. From the re-
sults, F-measure achieves respectively 70.44% and
75.94% when individually using the base context
features extracted by 12 predefined patterns (see
Figure 1) and syntactic dependency features (see
Figure 3), while 79.55% when using all features.
The results imply that syntactic dependency
37
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
whether show probably the other suggest show
may demonstrate some often whether used to
suggest will many patients probably was
likely can one of another indicate CFS
indicate role believed days appear demonstrate
possible found possibly CFS putative the other
putative human considered are some of all
appear known to such as any other thought ?:?
thought report several western possibly people
potential evidence said to pop likely could not
Table 4: Top 10 significant words in detecting uncertain sentences
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
MD SYM RB VBZ JJS SYM
VBG PRP JJS CD RBS ?:?
VB NN RBS ?:? RB JJR
VBZ CD FW WRB EX WDT
IN WDT VBP PRP CC CD
Table 5: Top 5 significant POS in detecting uncertain sentences
Dataset Precision Recall F1
base context features
biomedical 66.04 75.48 70.44
syntactic dependency features
biomedical 93.77 63.05 75.94
all features
biomedical 78.72 80.41 79.55
Table 8: Results for scopes recognizing with gold
hedge cues (word-level)
features contribute more benefits to recognize
scopes than surface context features.
Official results evaluated at block level are also
listed in Table 9.
dataset Precision Recall F1
biomedical 21.87 17.23 19.27
Table 9: Official results for scopes recognizing
(block level)
From Table 9 and the official result on hedge
cue identification in Table 6, we believe that our
post-processing strategy would be responsible for
the low performance on recognizing scopes. Our
strategy is to find a maximum consecutive block
covering the corresponding hedge cue. This strat-
egy cannot do well with the complex scope struc-
ture. For example, a scope is covered by another
scope. Therefore, our system would generate a
block covering all hedge cues if there exists more
than one hedge cues in a sentence.
5 Conclusion
We present our implemented system for CoNLL-
2010 Shared Task in this paper. We introduce
syntactic dependency features when recognizing
hedge scopes and employ average perceptron al-
gorithm to train the models. On the biomedi-
cal corpus, our system achieves F-measure with
77.86% in detecting uncertain sentences, 77.44%
in recognizing hedge cues, and 19.27% in identi-
fying the scopes.
Although some results are low and beyond our
expectations, we believe that our system can be at
least improved within the following fields. Firstly,
we would experiment on other kinds of features,
such as chunk or named entities in biomedical.
Secondly, the unlabeled datasets would be ex-
plored in the future.
38
False Negative False Positive True Positive True Negative
support considered suggesting chemiluminescence
of potential may rhinitis
demonstrate or proposal leukemogenic
a hope might ribosomal
postulate indicates indicating bp
supports expected likely nc82
good can appear intronic/exonic
advocates should possible large
implicated either speculate allele
putative idea whether end
Table 7: Top 10 words with the largest scores for each category in hedge cue identification
Acknowledgments
This work was funded by Chinese NSF (Grant
No. 60673038), 973 Program (Grant No.
2010CB327906, and Shanghai Committee of Sci-
ence and Technology(Grant No. 09511501404).
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, pages 1?8. Associa-
tion for Computational Linguistics.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley, New York.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176. Association for Com-
putational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the ACL, pages
91?98. Association for Computational Linguistics.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999. Association for Computational Lin-
guistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36.
Association for Computational Linguistics.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38?45. Association for Computa-
tional Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
39
Adaptive Chinese Word Segmentation with
Online Passive-Aggressive Algorithm
Wenjun Gao
School of Computer Science
Fudan University
Shanghai, China
wjgao616@gmail.com
Xipeng Qiu
School of Computer Science
Fudan University
Shanghai, China
xpqiu@fudan.edu.cn
Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, China
xjhuang@fudan.edu.cn
Abstract
In this paper, we describe our system1
for CIPS-SIGHAN-2010 bake-off task of
Chinese word segmentation, which fo-
cused on the cross-domain performance
of Chinese word segmentation algorithms.
We use the online passive-aggressive al-
gorithm with domain invariant informa-
tion for cross-domain Chinese word seg-
mentation.
1 Introduction
In recent years, Chinese word segmentation
(CWS) has undergone great development (Xue,
2003; Peng et al, 2004). The popular method is
to regard word segmentation as a sequence label-
ing problems. The goal of sequence labeling is to
assign labels to all elements of a sequence.
Due to the exponential size of the output
space, sequence labeling problems tend to be
more challenging than the conventional classifi-
cation problems. Many algorithms have been
proposed and the progress has been encourag-
ing, such as SVMstruct (Tsochantaridis et al,
2004), conditional random fields (CRF) (Lafferty
et al, 2001), maximum margin Markov networks
(M3N) (Taskar et al, 2003) and so on. After years
of intensive researches, Chinese word segmenta-
tion achieves a quite high precision. However, the
performance of segmentation is not so satisfying
for out-of-domain text.
There are two domains in domain adaption
problem, a source domain and a target domain.
When we use the machine learning methods for
1Available at http://code.google.com/p/
fudannlp/
Chinese word segmentation, we assume that train-
ing and test data are drawn from the same distri-
bution. This assumption underlies both theoreti-
cal analysis and experimental evaluations of learn-
ing algorithms. However, the assumption does
not hold for domain adaptation(Ben-David et al,
2007; Blitzer et al, 2006). The challenge is the
difference of distribution between the source and
target domains.
In this paper, we use online margin max-
imization algorithm and domain invariant fea-
tures for domain adaptive CWS. The online learn-
ing algorithm is Passive-Aggressive (PA) algo-
rithm(Crammer et al, 2006), which passively ac-
cepts a solution whose loss is zero, while it ag-
gressively forces the new prototype vector to stay
as close as possible to the one previously learned.
The rest of the paper is organized as follows.
Section 2 introduces the related works. Then we
describe our algorithm in section 3 and 4. The
feature templates are described in section 5. Sec-
tion 6 gives the experimental analysis. Section 7
concludes the paper.
2 Related Works
There are several approaches to deal with the do-
main adaption problem.
The first approach is to use semi-supervised
learning (Zhu, 2005).
The second approach is to incorporate super-
vised learning with domain invariant information.
The third approach is to improve the present
model with a few labeled domain data.
Altun et al (2006) investigated structured clas-
sification in a semi-supervised setting. They pre-
sented a discriminative approach that utilizes the
intrinsic geometry of inputs revealed by unlabeled
data points and we derive a maximum-margin for-
mulation of semi-supervised learning for struc-
tured variables.
Self-training (Zhu, 2005) is also a popular tech-
nology. In self-training a classifier is first trained
with the small amount of labeled data. The clas-
sifier is then used to classify the unlabeled data.
Typically the most confident unlabeled points, to-
gether with their predicted labels, are added to the
training set. The classifier is re-trained and the
procedure repeated. Note the classifier uses its
own predictions to teach itself. Yarowsky (1995)
uses self-training for word sense disambiguation,
e.g. deciding whether the word plant means a liv-
ing organism or a factory in a given context.
Zhao and Kit (2008) integrated unsupervised
segmentation and CRF learning for Chinese word
segmentation and named entity recognition. They
found word accessory variance (Feng et al, 2004)
is useful to CWS.
3 Online Passive-Aggressive Algorithm
Sequence labeling, the task of assigning labels
y = y1, . . . , yL to an input sequence x =
x1, . . . , xL.
Give a sample (x,y), we define the feature is
?(x,y). Thus, we can label x with a score func-
tion,
y? = argmax
z
F (w,?(x, z)), (1)
where w is the parameter of function F (?).
The score function of our algorithm is linear
function.
Given an example (x,y), y? is denoted as the
incorrect label with the highest score,
y? = argmax
z 6=y
wT?(x, z). (2)
The margin ?(w; (x,y)) is defined as
?(w; (x,y)) = wT?(x,y)?wT?(x, y?). (3)
Thus, we calculate the hinge loss.
`(w; (x,y) =
{
0, ?(w; (x,y)) > 1
1? ?(w; (x,y)), otherwise
(4)
We use the online PA learning algorithm to
learn the weights of features. In round t, we find
new weight vector wt+1 by
wt+1 = arg min
w?Rn
1
2
||w ?wt||2 + C ? ?,
s.t. `(w; (xt,yt)) <= ? and ? >= 0 (5)
where C is a positive parameter which controls
the influence of the slack term on the objective
function.
The algorithms goal is to achieve a margin at
least 1 as often as possible, thus the Hamming loss
is also reduced indirectly. On rounds where the
algorithm attains a margin less than 1 it suffers an
instantaneous loss.
We abbreviate `(wt; (x, y)) to `t. If `t = 0
then wt itself satisfies the constraint in Eq. (5)
and is clearly the optimal solution. We therefore
concentrate on the case where `t > 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (5) to be
L(w, ?, ?, ?) = 1
2
||w ?wt||2 + C ? ?
+ ?(`t ? ?)? ??
s.t. ? >= 0, ? >= 0. (6)
where ?, ? is a Lagrange multiplier.
Setting the partial derivatives of L with respect
to the elements of ? to zero gives
? + ? = C. (7)
The gradient of w should be zero,
w ? wt ? ?(?(x,y) ? ?(x, y?)) = 0, (8)
we get
w = wt + ?(?(x,y) ? ?(x, y?)). (9)
Substitute Eq. (7) and Eq. (9) to dual objective
function Eq. (6), we get
L(?) = ?1
2
||?(?(x,y)? ?(x, y?))||2
? ?(wtT (?(x,y)? ?(x, y?)) + ? (10)
Differentiate with ?, and set it to zero, we get
?||?(x,y)? ?(x, y?)||2
+wtT (?(x,y)? ?(x, y?))? 1 = 0. (11)
So,
?? = 1?wt
T (?(x,y)? ?(x, y?))
||?(x,y)? ?(x, y?)||2
. (12)
From ? + ? = C, we know that ? < C, so
??? = min(C, ??). (13)
Finally, we get update strategy,
wt+1 = wt + ???(?(x,y)? ?(x, y?)). (14)
Our final algorithm is shown in Algorithm 1. In
order to avoiding overfitting, the averaging tech-
nology is employed.
input : training data set:
(xn,yn), n = 1, ? ? ? , N , and
parameters: C,K
output: w
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt,yt);
predict:
y?t = argmax
z 6=yt
?wt,?(xt, z)?;
calculate `(w; (x,y));
update wt+1 with Eq.(14);
end
cw = cw +wT ;
end
w = cw/K ;
Algorithm 1: Labelwise Margin Maxi-
mization Algorithm
4 Inference
The PA algorithm is used to learn the weights of
features in training procedure. In inference pro-
cedure, we use Viterbi algorithm to calculate the
maximum score label.
Let ?(n) be the best score of the partial label
sequence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(n):
?(n) = max
n?1
(
?(n? 1) +wT?(x, yn, yn?1)
)
(15)
+wt?(x, yn)
Using this recursive definition, we can evalu-
ate ?(N) for all yN , where N is the input length.
This results in the identification of the best label
sequence.
The computational cost of the Viterbi algorithm
is O(NL2), where L is the number of labels.
5 Feature Templates
All feature templates used in this paper are shown
in Table 1. C represents a Chinese character while
the subscript of C indicates its position in the sen-
tence relative to the current character, whose sub-
script is 0. T represents the character-based tag:
?B?, ?B2?, ?B3?, ?M?, ?E? and ?S?, which repre-
sent the beginning, second, third, middle, end or
single character of a word respectively.
The type of character includes: digital, letter,
punctuation and other.
We also use the word accessor variance for do-
main adaption. Word accessor variance (AV) was
proposed by (Feng et al, 2004) and was used to
evaluate how independently a string is used, and
thus how likely it is that the string can be a word.
The accessor variety of a string s of more than one
character is defined as
AV (s) = min{Lav(s), Rav(s)} (16)
Lav(s) is called the left accessor variety and is
defined as the number of distinct characters (pre-
decessors) except ?S? that precede s plus the num-
ber of distinct sentences of which s appears at
the beginning. Similarly, the right accessor va-
riety Rav(s) is defined as the number of distinct
characters (successors) except ?E? that succeed s
plus the number of distinct sentences in which s
appears at the end. The characters ?S? and ?E?
are defined as the begin and end of a sentence.
The word accessor variance was found effective
for CWS with unsegmented text (Zhao and Kit,
2008).
Table 1: Feature templates
Ci, T0, (i = ?1, 0, 1, 2)
Ci, Ci+1, T0, (i = ?2,?1, 0, 1)
T?1,0
Tc: Type of Character
AV : word accessor variance
6 CIPS-SIGHAN-2010 Bakeoff
CIPS-SIGHAN-2010 bake-off task of Chinese
word segmentation focused on the cross-domain
performance of Chinese word segmentation algo-
rithms. There are two subtasks for this evaluation:
(1) Word Segmentation for Simplified Chinese
Text;
(2) Word Segmentation for Traditional Chinese
Text.
The test corpus of each subtask covers four do-
mains: literature, computer science, medicine and
finance.
We participate in closed training evaluation of
both subtasks.
Firstly, we calculate the word accessor variance
AVL(s)of the continuous string s from labeled
corpus. Here, we set the largest length of string
s to be 4.
Secondly, we train our model with feature tem-
ples and AVL(s).
Thirdly, when we process the different domain
unlabeled corpus, we recalculate the word ac-
cessory variance AVU (s) from the corresponding
corpus.
Fourthly, we segment the domain corpus with
new word accessory variance AVU (s) instead of
AVL(s).
The results are shown in Table 2 and 3. The
results show our method has a poor performance
in OOV ( Out-Of-Vocabulary) word.
The running environment is shown in Table 4.
Table 4: Experimental environment
OS Win 2003
CPU Intel Xeon 2.0G
Memory 4G
We set the max iterative number is 20. Our run-
ning time is shown in Table 5. ?s? represents sec-
ond, ?chars? is the number of Chinese character,
and ?MB? is the megabyte. In practice, we found
the system can achieve the same performance af-
ter 7 loops. Therefore, we just need less half the
time in Table 5 actually.
7 Conclusion
In this paper, we describe our system in CIPS-
SIGHAN-2010 bake-off task of Chinese word
segmentation. Although our method just achieve
a consequence of being average and not outstand-
ing, it has an advantage of faster training than
other batch learning algorithm, such as CRF and
M3N.
In the future, we wish to improve our method
in the following aspects. Firstly, we will investi-
gate more effective domain invariant feature rep-
resentation. Secondly, we will integrate our algo-
rithm with self-training and other semi-supervised
learning methods.
Acknowledgments
This work was (partially) funded by 863 Pro-
gram (No. 2009AA01A346), 973 Program (No.
2010CB327906), and Shanghai Science and Tech-
nology Development Funds (No. 08511500302).
References
Altun, Y., D. McAllester, and M. Belkin. 2006. Max-
imum margin semi-supervised learning for struc-
tured variables. Advances in neural information
processing systems, 18:33.
Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira.
2007. Analysis of representations for domain adap-
tation. Advances in Neural Information Processing
Systems, 19:137.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128. Association for Computational
Linguistics.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Table 2: Evaluation results on simplified corpus
R P F1 OOV RR IV RR
Best 0.945 0.946 0.946 0.816 0.954
Literature Our 0.915 0.925 0.92 0.577 0.94
Best 0.953 0.95 0.951 0.827 0.975
Computer Our 0.934 0.919 0.926 0.739 0.969
Best 0.942 0.936 0.939 0.75 0.965
Medicine Our 0.927 0.924 0.925 0.714 0.953
Best 0.959 0.96 0.959 0.827 0.972
Finance Our 0.94 0.942 0.941 0.719 0.961
Table 3: Evaluation results on traditional corpus
R P F1 OOV RR IV RR
Best 0.942 0.942 0.942 0.788 0.958
Literature Our 0.869 0.91 0.889 0.698 0.887
Best 0.948 0.957 0.952 0.666 0.977
Computer Our 0.933 0.949 0.941 0.791 0.948
Best 0.953 0.957 0.955 0.798 0.966
Medicine Our 0.908 0.932 0.92 0.771 0.919
Best 0.964 0.962 0.963 0.812 0.975
Finance Our 00.925 0.939 0.932 0.793 0.935
Table 5: Execution time of training and test phase.
Task A B C D
Training Simp 817.2s 795.6s 774.0s 792.0s
Trad 903.6s 889.2s 885.6s 874.8s
Test 20327 chars/s, or 17.97 s/MB
Feng, H., K. Chen, X. Deng, and W. Zheng. 2004. Ac-
cessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75?93.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ?01: Proceedings of the
Eighteenth International Conference on Machine
Learning.
Peng, F., F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th inter-
national conference on Computational Linguistics.
Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Proceed-
ings of Neural Information Processing Systems.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Pro-
ceedings of the International Conference on Ma-
chine Learning(ICML).
Xue, N. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189?196. Associ-
ation for Computational Linguistics.
Zhao, H. and C. Kit. 2008. Unsupervised segmenta-
tion helps supervised learning of character tagging
for word segmentation and named entity recogni-
tion. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111. Citeseer.
Zhu, Xiaojin. 2005. Semi-supervised learning
literature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
Triplet-Based Chinese Word Sense Induction
Zhao Liu
School of Computer Science
Fudan University
Shanghai, China
ZLiu.fd@gmail.com
Xipeng Qiu
School of Computer Science
Fudan University
Shanghai, China
xpqiu@fudan.edu.cn
Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, China
xjhuang@fudan.edu.cn
Abstract
This paper describes the implementa-
tion of our system at CLP 2010 bake-
off of Chinese word sense induction.
We first extract the triplets for the tar-
get word in each sentence, then use
the intersection of all related words of
these triplets from the Internet. We
use the related word to construct fea-
ture vectors for the sentence. At last
we discriminate the word senses by
clustering the sentences. Our system
achieved 77.88% F-score under the of-
ficial evaluation.
1 Introduction
The goal of the CLP 2010 bake-off of Chi-
nese word sense induction is to automati-
cally discriminate the senses of Chinese target
words by the use of only un-annotated data.
The use of word senses instead of word
forms has been shown to improve perfor-
mance in information retrieval, information
extraction and machine translation. Word
Sense Disambiguation generally requires the
use of large-scale manually annotated lexical
resources. Word Sense Induction can over-
come this limitation, and it has become one
of the most important topics in current com-
putational linguistics research.
In this paper we introduce a method to
solve the problem of Chinese word sense in-
duction.For this task, Firstly we constructed
triplets containing the target word in every
instance, then searched the intersection of all
the three words from the Internet with web
searching engine and constructed feature vec-
tors.Then we clustered the vectors with the
sIB clustering algorithm and at last discrimi-
nated the word senses.
This paper is organized as following: firstly
we introduce the related works. Then we talk
about the methods in features selection and
clustering. The method of evaluation and the
result of our system is following. At last we
discuss the improvement and the weakness of
our system.
2 Related Works
Sense induction is typically treated as a
clustering problem, by considering their co-
occurring contexts, the instances of a target
word are partitioned into classes. Previous
methods have used the first or second or-
der co-occurrence (Pedersen and Bruce, 1997;
Sch?tze, 1998), parts of speech, and local col-
locations (Niu et al, 2007). The size of con-
text window is also various, it can be as small
as only two words before and after the target
words. It may be the sentence where the tar-
get word is in. Or it will be 20 surrounding
words on either side of the target words and
even more words.
After every instance of the target word is
represented as a feature vector, it will be the
input of the clustering methods. Many clus-
tering methods have been used in the task
of word sense induction. For example, k-
means and agglomerative clustering (Sch?tze,
1998). sIB (Sequential Information Bottle-
neck) a variation of Information Bottleneck is
applied in (Niu et al, 2007). In (Dorow and
Widdows, 2003) Graph-based clustering algo-
rithm is employed that in a graph a node rep-
resents a noun and two nodes have an edge be-
tween them if they co-occur in list more than
a given number of times. A generative model
based on LDA is proposed in (Brody and La-
pata, 2009).
In our method, we use the triplets (Bordag,
2006) and their intersections from the Internet
to construct the feature vectors then sIB is
used as the clustering method.
3 Feature Selection
Our method select the features of the words
similar to (Bordag, 2006) is also using the
triplets. In Chinese there are no natural sep-
arators between the words as English, so the
first step in Chinese language processing is of-
ten the Chinese word segmentation. In our
system we use the FudanNLP toolkit1 to split
the words.
At the first stage, we split the instance of
the target word and filter out the numbers,
English words and stop words from it. So we
get a sequence of the words. Then we select
two words before the target and another two
words after it. If there are no words before or
after then leave it empty. After that we enu-
merate two words from the selected four words
to construct a triplets together with the target
words. So we can get several triplets for every
instance of the target. Because the faulty of
Chinese word segmentation and some special
target word for example a single Chinese char-
acter as a word, there are some errors finding
the position of the target words. If the word
is a single Chinese character and the toolkit
combine it with other Chinese characters to
be a word, we will use that word as the tar-
get instead of the character to construct the
triplets.
The second stage is obtaining corpus from
the Internet. For every triplet we search the
three words sequence in it with a pair of dou-
ble quotation marks in Baidu web searching
engine2. It gives the snippets of the webs
1http://code.google.com/p/fudannlp/
2http://www.baidu.com
which have all the three words in it. We se-
lect the first 50 snippets of each triplets. If
the number of the snippets is less than 50,
we will ignore that triplet. For some rare
words the snippets searched from the Internet
for all the triplets of the instance is less than
50. In that situation we will search the target
word and another one co-occurring word in
the searching engine to achieve enough snip-
pets as features. After searching the triplets
we select the first three triplets (or doublets)
with largest amount of the webs searched by
the searching engine. For every instance there
are three or less triplets (or doublets) and we
have obtained many snippets for them. After
segmenting and filtering these snippets we use
the bag of words from them as the feature for
this instance.
The last stage of feature selection is to con-
struct the feature vector for every instances
containing the target word. In the previous
stage we get a bag of words for each instance.
For all the instances of one target word we
make a statistic of the frequence of each word
in the bags. In our system we select the words
whose frequence is more than 50 as the dimen-
sions for the feature vectors. From the tests
we find that when this thread varies from 50 to
120 the result of our system is nearly the same,
but outside that bound the result will become
rather bad. So we use 50 as the thread. Af-
ter constructing the dimension of that target
word, we can get a feature vector for each in-
stance that at each dimension the number is
the frequence of that word occurs in that po-
sition.
We obtain the feature vectors for the target
words by employing these three stage. The
following work is clustering these vector to get
the classes of the word senses.
4 The Clustering Algorithm
There are many classical clustering methods
such as k-means, EM and so on. In (Niu et
al., 2007) they applied the sIB (Slonim et al,
2002) clustering algorithm at SemEval-2007
for task 2 and it achieved a quite good result.
And at first this algorithm is also introduced
for the unsupervised document classification
problem. So we use the sIB algorithm for clus-
tering the feature vectors in our system.
Unlike the situation in (Niu et al, 2007),
the number of the sense classes is provided in
CLP2010 task 4. So we can apply the sIB
algorithm directly without the sense number
estimation procedure in that paper. sIB algo-
rithm is a variant of the information bottle-
neck method.
Let d represent a document, and w repre-
sent a feature word, d ? D,w ? F . Given
the joint distribution p(d,w), the document
clustering problem is formulated as looking for
a compact representation T for D, which re-
serves as much information as possible about
F . T is the document clustering solution. For
solving this optimization problem, sIB algo-
rithm was proposed in (Slonim et al, 2002),
which found a local maximum of I(T, F ) by:
given a initial partition T, iteratively draw-
ing a d ? D out of its cluster t(d), t ? T ,
and merging it into tnew such that tnew =
argmaxt?Td(d, t). d(d, t) is the change of
I(T, F ) due to merging d into cluster tnew,
which is given by
d(d, t) = (p(d)+p(t))JS(p(w|d), p(w|t)). (1)
JS(p, q) is the Jensen-Shannon divergence,
which is defined as
JS(p, q) = pipDKL(p||p) + piqDKL(q||p), (2)
DKL(p||p) =
?
y
p log p
p
, (3)
DKL(q||p) =
?
y
q log q
p
, (4)
{p, q} ? {p(w|d), p(w|t)}, (5)
{pip, piq} ? {
p(d)
p(d) + p(t)
, p(t)
p(d) + p(t)
}, (6)
p = pipp(w|d) + piqp(w|t). (7)
In our system we use the sIB algorithm in
the Weka 3.5.8 cluster package to cluster the
feature vectors obtained in the previous sec-
tion. The detailed description of the sIB algo-
rithm in weka can refer to the website 3. And
the parameters for this Weka class is that: the
number of clusters is the number of senses pro-
vided by the task, the random number seed is
zero and the other parameters like maximum
number of iteration and so on is set as default.
5 CLP 2010 Bake-Off of Chinese
Word Sense Induction
5.1 Evaluation Measure
The evaluation measure is described as fol-
lowing:
We consider the gold standard as a solu-
tion to the clustering problem. All examples
tagged with a given sense in the gold standard
form a class. For the system output, the clus-
ters are formed by instances assigned to the
same sense tag (the sense tag with the highest
weight for that instance). We will compare
clusters output by the system with the classes
in the gold standard and compute F-score as
usual. F-score is computed with the formula
below.
Suppose Cr is a class of the gold standard,
and Si is a cluster of the system generated,
then
1. F ? score(Cr, Si) = 2?P?RP+R
2. P =the number of correctly labeled ex-
amples for a cluster/total cluster size
3. R =the number of correctly labeled ex-
amples for a cluster/total class size
Then for a given class Cr,
FScore(Cr) = max
Si
(F ? score(Cr, Si)) (8)
Then
FScore =
c
?
r=1
nr
n
FScore(Cr) (9)
3http://pacific.mpi-cbg.de/javadoc/Weka/
clusterers/sIB.html
Where c is total number of classes, nr is the
size of class Cr , and n is the total size.
5.2 DataSet
The data set includes 100 ambiguous Chi-
nese words and for every word it provided 50
instances. Besides that they also provided a
sample test set of 2500 examples of 50 target
words with the answers to illustrate the data
format.
Besides the sIB algorithm we also apply the
k-means and EM algorithm to cluster the fea-
ture vectors. These algorithms are separately
using the simpleKMeans class and the EM
class in the Weka 3.5.8 cluster package. Ex-
cept the number of clusters set as the given
number of senses and number of seeds set as
zero, all other parameters are set as default.
For the given sample test set with answers the
result is illustrated in the Table 1 below.
algorithm F-score
k-means 0.7025
EM 0.7286
sIB 0.8132
Table 1: Results of three clustering algorithms
From Table 1 we can see the sIB clustering
algorithm improves the result of the Chinese
word sense induction evidently.
In the real test data test containing 100 am-
biguous Chinese words, our system achieves a
F-score 0.7788 ranking 6th among the 18 sys-
tems submitted. The best F-score of these 18
systems is 0.7933 and the average of them is
0.7128.
5.3 Discussion
In our system we only use the local collo-
cations and the co-occurrences of the target
words. But the words distance for the target
word in the same sentence and the parts of
speech of the neighboring word together with
the target word is also important in this task.
In our experiment we used the parts of
speech for the target word and each word be-
fore and after it achieved by the Chinese word
segmentation system as part of the features
vectors for clustering. With a proper weight
on each POS dimension in the feature vectors,
the F score for some word in the given sample
test set with answers improved evidently. For
example the Chinese word ????, the F score
of it was developed from 0.5983 to 0.7573. But
because of the fault of the segmentation sys-
tem and other reasons F score of other words
fell and the speed of the system was rather
slower than before, we gave up this improve-
ment finally.
Without the words distance for the target
word in the same sentence the feature vectors
maybe lack some information useful. So if we
can calculate the correlation between the tar-
get word and other words, we will use these
word sufficiently. However because of quan-
tity of the Internet corpus is unknown, we
didn?t find the proper method to weigh the
correlation.
From the previous section we find that the
F score for the real test data test is lower than
that for the sample test set. It is mainly be-
cause there are more single Chinese charac-
ters (as words) in the real test data set. Our
system does not process these characters spe-
cially. For most of the Chinese characters we
can?t judge their correct senses only from the
context where they appear. Their meaning
always depends on the collocations with the
other Chinese characters with which they be-
come a Chinese word. However our system
discriminates the senses of them only refer-
ring to the context of them, it can?t judge the
meaning of these Chinese characters properly.
Maybe the best way is to search them in the
dictionary.
However our system does not always have a
very poor performance for any single Chinese
character (as a word). The result is quite good
for some Chinese characters. For example the
Chinese character ??? which has three mean-
ing: valley, millet and a family name, the pre-
cision (P) of our system is 0.760. But for most
of single Chinese characters such as ??? and
???, it is so bad that the result in the sample
test worked rather better than the real test.
In Chinese the former character ??? tends
to express a complete meaning and the other
characters in the word which they combine of-
ten modify it such as the characters ??? and
??? in the word ???? and ????. So this
character can have a relatively high correla-
tion with the words around and our system
can deal with such characters like it. Unfor-
tunately most characters need other charac-
ters to represent a complete meaning as the
the latter ??? and ??? so they almost have
no correlation with the words around but with
those characters in the word in which they oc-
cur. But our system only uses the context fea-
tures and even doesn?t do any special process
about these single Chinese characters. There-
fore our system can not address those char-
acters appropriately and we need to find a
proper method to solve it, using a dictionary
may be a choice.
This method works better for nouns and ad-
jectives (in the sample test data set there are
only 4 adjectives), but for verbs F score falls
a little, illustrated in the Table 2 below.
POS F-score
nouns 0.8473
adjectives 0.8543
verbs 0.7921
Table 2: Results of each POS in the sample
test data set
Only using the local collocations in our sys-
tem the F score is achieve above 80% (in the
sample test), it demonstrates to some extent
the information of collocations is so important
that we should pay more attention to it.
6 Conclusion
The triplet-based Chinese word sense induc-
tion method is fitted to the task of Chinese
word sense induction and obtain rather good
result. But for some single characters word
and some verbs, this method is not appropri-
ate enough. In the future work, we will im-
prove the method with more reasonable triplet
selection strategies.
Acknowledgments
This work was (partially) funded by 863
Program (No. 2009AA01A346), 973 Pro-
gram (No. 2010CB327906), and Shanghai Sci-
ence and Technology Development Funds (No.
08511500302).
References
Bordag, S. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. Pro-
ceedings of EACL-06. Trento.
Brody, S. and M. Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 103?
111. Association for Computational Linguistics.
Dorow, B. and D. Widdows. 2003. Discovering
corpus-specific word senses. In Proceedings of
the tenth conference on European chapter of
the Association for Computational Linguistics-
Volume 2, pages 79?82. Association for Compu-
tational Linguistics.
Niu, Z.Y., D.H. Ji, and C.L. Tan. 2007. I2r: Three
systems for word sense discrimination, chinese
word sense disambiguation, and english word
sense disambiguation. In Proceedings of the 4th
International Workshop on Semantic Evalua-
tions, pages 177?182. Association for Compu-
tational Linguistics.
Pedersen, T. and R. Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of
the Second Conference on Empirical Methods in
Natural Language Processing, volume 2, pages
197?207.
Sch?tze, H. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97?
123.
Slonim, N., N. Friedman, and N. Tishby. 2002.
Unsupervised document classification using se-
quential information maximization. In Proceed-
ings of the 25th annual international ACM SI-
GIR conference on Research and development
in information retrieval, pages 129?136. ACM.
