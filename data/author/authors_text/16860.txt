Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152?156,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Regression with Phrase Indicators for Estimating MT Quality?
Chunyang Wu Hai Zhao?
Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering, Shanghai Jiao Tong University
chunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
Abstract
We in this paper describe the regression sys-
tem for our participation in the quality estima-
tion task of WMT12. This paper focuses on
exploiting special phrases, or word sequences,
to estimate translation quality. Several feature
templates on this topic are put forward sub-
sequently. We train a SVM regression model
for predicting the scores and numerical results
show the effectiveness of our phrase indicators
and method in both ranking and scoring tasks.
1 Introduction
The performance of machine translation (MT) sys-
tems has been considerable promoted in the past two
decades. However, since the quality of the sentence
given by MT decoder is not guaranteed, an impor-
tant issue is to automatically predict or identify its
characteristics. Recent studies on quality estimation
or confidence estimation have focused on measuring
the translating quality at run-time, instead of involv-
ing reference corpus. Researches on this topic con-
tribute to offering advices or warnings for users even
without knowledge about either side of languages
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119 and
Grant No. 61170114), the National Research Foundation for
the Doctoral Program of Higher Education of China under
Grant No. 20110073120022, the National Basic Research Pro-
gram of China (Grant No. 2009CB320901), the Science and
Technology Commission of Shanghai Municipality (Grant No.
09511502400), and the European Union Seventh Framework
Program (Grant No. 247619).
?corresponding author
and illuminating some other potential MT applica-
tions.
This paper describes the regression system for our
participation in the WMT12 quality estimation task.
In this shared task, we analyzed the pattern of trans-
lating errors and studied on capturing such patterns
among the corpus. The basic objective in this pa-
per is to recognize those phrases, or special word se-
quence combinations which can indicate the quality
of a translation instance. By introducing no exter-
nal NLP toolkits, we exploited several feasible tech-
niques to extract such patterns directly on the cor-
pus. One contribution of this paper is those feature
templates on the basis of this topic. Numerical re-
sults show their positive effects on both ranking and
scoring subtasks.
The rest of this paper is organized as follows: In
Section 2, we show the related work. In Section
3, we specify the details of our system architecture.
The experimental results are reported in Section 4.
Finally, the conclusion is given in Section 5.
2 Related Work
Compared with traditional MT metrics such as
BLEU (Papineni et al, 2002), the fundamental goal
of quality estimation (QE) is predicting the quality
of output sentences without involving reference sen-
tences.
Early works (Quirk, 2004; Gamon et al, 2005)
have demonstrated the consistency of the automatic
score and human evaluation. Several further works
aimed at predicting automatic scores in order to bet-
ter select MT n-best candidates (Specia and Farzin-
dar, 2010), measure post-editing effort (Specia et
152
al., 2011) or combine SMT and TM systems (He et
al., 2010). Instead of estimating on word or sen-
tence levels, Soricut and Echihabi (2010) proposed
a document-level ranking system which grants the
user to set quality threshold. Besides, recent stud-
ies on the QE topic introduced syntactic and linguis-
tic information for better estimating the quality such
as dependency preservation checking (Bach et al,
2011).
3 System Description
We specify the details of our system in this section.
Following previous approaches for quality estima-
tion, it is first trained on the corpus with labeled
quality scores and then it is able to predict the score
for unlabeled instances.
A major challenge for this estimating task is to ex-
ploit effective indicators, or features, to identify the
quality of the translating results. In this paper, all the
features are extracted from the official corpora, in-
volving no external tools such as pre-trained parsers
or POS taggers. Most of the feature templates focus
on special phrases or word sequences. Some of the
phrases could introduce translation errors and other-
s might declare the merit of the MT output. Their
weights are automatically given by the regressor.
3.1 Regression Model
For obtaining MT quality predictor, we utilize
SVM light (Joachims, 1999)1 to train this regression
model. The radial basis function kernel is chosen as
the kernel of this model. The label for each instance
is the score annotated manually and the input vector
consists of a large amount of indicators described in
Section 3.2.
3.2 Features
For training the regression model, we utilize the
17 baseline features: number of source/target to-
kens, average source token length, source/target
LM probability, target-side average of target word
occurrences, original/inverse frequency average of
translations per source word, source/target percent-
age of uni-/bi-/tri-grams in quartile 1 or 4, source
percentage of unigrams in the training corpus and
1http://svmlight.joachims.org/
source/target number of punctuation. Besides, sev-
eral features and templates are proposed as follows:
? Inverted Automatic Scores: For each Span-
ish system output sentence, we translate it to
English and get its scores of BLEU and ME-
TEOR (Denkowski and Lavie, 2011). These s-
cores are treated as features named inverted au-
tomatic scores. In order to obtain these numer-
als, we train a Spanish-to-English phrase-based
Moses2 (Koehn et al, 2007) decoder with de-
fault parameters on the official parallel corpus.
The original training corpus is split into a de-
veloping set containing the last 3000 sentence
pairs at the end of the corpus and a training set
with the remained pairs. The word alignment
information is generated by GIZA++ (Och and
Ney, 2003) and the feature weights are tuned on
the developing set by Z-MERT (Zaidan, 2009).
? Minimal/Maximal link likelihood of gener-
al language model: In the word graph of
each decoding instance, denote the minimal
and maximal general language model likeli-
hood of links as lmin and lmax. We treat
exp(lmin) and exp(lmax) as features respec-
tively.
? Trace Density: Define the trace density ?T as
the quotient of decoding trace length and sen-
tence length:
?T = TraceLength / SentenceLength. (1)
? Average of Phrase Length: This feature is
also obtained from the decoding trace informa-
tion.
? Number of Name Entity: This feature can-
not be obtained exactly due to the resource con-
strains. We in this task count the number of the
word whose first letter is capitalized, and that
is not the first word in the sentence.
We also extract several special phrases or se-
quences. The total of each phrase/sequence type
and each pattern are respectively defined as features.
When an instance matches a pattern, the entry rep-
resenting this pattern in its vector is set to |1/Z|. In
2http://www.statmt.org/moses/
153
this paper the regressor term Z is the size of the tem-
plate which the pattern belongs to. The detail de-
scription of such templates is presented as follows:
? Reference 2?5-grams: All the 2?5-grams of
the reference sentences provided in the official
data are generated as features.
? Bi-gram Source Splitting: This template
comes from the GIZA alignment document.
We scan the parallel corpus: for each bi-gram
in the source sentence, if its words? counter-
parts in the target side are separated, we add it
to the bi-gram source splitting feature template.
The part-of-speech tags of the words seem to be
effective to this task. Since it is not provided, we uti-
lize a trick design for obtaining similar information:
? Target Functional Word Patterns: On the
target corpus, we scan those words whose
length is smaller than or equal to three. Such
a word w is denoted as functional word. Any
bi-gram in the corpus starting or ending with w
is added to a dictionary D. For each system-
output translation instance, we compare the
analogous bi-grams in it with this dictionary,
all bi-grams not in D are extracted as features.
Denote the collection of 2?5-grams of the
system-output sentences scored lower than 1.5 as B;
that with scores higher than 4.5 as G. Here the ?s-
core? is the manual score provided in the official re-
source.
? Target Bad 2?5-grams: B?G
? Target Good 2?5-grams: G? B
? Source Bad/Good 2?5-grams: Analogous
phrases on the source side are also extracted
by the same methods as Target Bad/Good n-
grams.
For each output-postedit sentence pair, we con-
struct a bipartite graph by aligning the same words
between these two sentences. By giving a maximal
matching, the output sentence can be split to several
segments by the unmatched words.
? Output-Postedit Different 2?5-grams: For
each unaligned segments, we attach the previ-
ous word to the left side and the next word to
the right. 2?5-grams in this refined segment
are extracted as features.
? Output-Postedit Different Anchor: Denote
the refined unaligned segment as
sr = (prevWord, s1, s2, . . . , sn, nextWord).
A special sequence with two word segments
prevWord s1 . . . sn nextWord
is given as a feature.
In the source-side scenario with the inverted trans-
lations, similar feature templates are extracted as
well:
? Source-Invert Different 2?5-grams/Anchor
A significant issue to be considered in this shared
task is that the training data set is not a huge one,
containing about two thousand instances. Although
carefully designed, the feature templates however
cannot involve enough cases. In order to overcome
this drawback, we adopt the following strategy:
For any template T, we compare its patterns with
the items in the phrase table. If the phrase item p is
similar enough with the pattern g, p is added to the
template T. Two similarity metrics are utilized: De-
note the longest common sequence as LCSQ(p, g)
and the longest common segment as LCSG(p, g) 3,
LCSQ(p, g)2
|p||g|
> 0.6, (2)
LCSG(p, g) ? 3. (3)
Besides, when training the regression model or
testing, the entry representing the similar items in
the feature vector are also set to 1/|template size|.
4 Experiments
4.1 Data
In order to conduct this experiment, we randomly
divide the official training data into two parts: one
3To simplify, the sequence allows separation while the seg-
ment should be contiguous. For example, LCSQ(p, g) and
LCSG(p, g) for ?I am happy? and ?I am very happy? are ?I,
am, happy? and ?I, am?, respectively.
154
Data Set
Score Distribution
[1-2) [2-3) [3-4) [4-5)
Original 3.2% 24.1% 38.7% 34.0%
Train 3.2% 24.2% 38.3% 34.4%
Dev 3.3% 24.0% 40.3% 32.4%
Table 1: The comparison of the score distributions among
three data sets: Original, Training (Train) and Develop-
ment (Dev).
Ranking Scoring
DA SC MAE RMSE
Baseline 0.47 0.49 0.61 0.79
This paper 0.49 0.52 0.60 0.77
Table 2: The experiment results on the ranking and scor-
ing tasks. In this table, DA, SC, MAE and RMSE are
DeltaAvg, Spearman Correlation, Mean-Average-Error
and Root-Mean-Squared-Error respectively.
training set with about 3/4 items and one develop-
ment set with the other 1/4 items. The comparison
of the score distribution among these data sets is list-
ed in Table 1.
4.2 Results
The baseline of this experiment is the regression
model trained on the 17 baseline features. The pa-
rameters of the classifier are firstly tuned on the
baseline features. Then the settings for both the
baseline and our model remain unchanged. The
numerical results for the ranking and scoring tasks
are listed in Table 2. The ranking task is evalu-
ated on the DeltaAvg metric (primary) and Spear-
man correlation (secondary) and the scoring task is
evaluated on Mean-Average-Error and Root-Mean-
Squared-Error. For the ranking task, our system out-
performs 0.02 on DeltaAvg and 0.03 on Spearman
correlation; for the scoring task, 0.01 lower on MAE
and 0.02 lower on RMSE.
The official evaluation results are listed in Table 3.
The official LibSVM4 model is a bit better than our
submission. Our system was further improved af-
ter the official submission. Different combinations
of the rates defined in Equation 2?3 and regressor
parameter settings are tested. As a result, the ?Re-
fined? model in Table 3 is the results of the refined
4http://www.csie.ntu.edu.tw/ cjlin/libsvm/
Ranking Scoring
DA SC MAE RMSE
SJTU 0.53 0.53 0.69 0.83
Official SVM 0.55 0.58 0.69 0.82
Refined 0.55 0.57 0.68 0.81
Best Workshop 0.63 0.64 0.61 0.75
Table 3: The official evaluation results.
version. Compared with the official model, it gives
similar ranking results and performs better on the s-
coring task.
5 Conclusion
We presented the SJTU regression system for the
quality estimation task in WMT 2012. It utilized
a support vector machine approach with several fea-
tures or feature templates extracted from the decod-
ing and corpus documents. Numerical results show
the effectiveness of those features as indicators for
training the regression model. This work could be
extended by involving syntax information for ex-
tracting more effective indicators based on phrases
in the future.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: a method for measuring machine transla-
tion confidence. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, HLT
?11, pages 211?219, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level mt evaluation without reference
translations: Beyond language modeling. In In Euro-
pean Association for Machine Translation (EAMT).
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
155
Thorsten Joachims. 1999. Advances in kernel method-
s. chapter Making large-scale support vector machine
learning practical, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics Companion Demo and Poster Sessions, pages
177?180, Prague, Czech Republic. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, ACL ?02, pages 311?318, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence metric. LREC, 4:2004.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations vi-
a ranking. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 612?621, Uppsala, Sweden, July. Association
for Computational Linguistics.
Lucia Specia and Atefeh Farzindar. 2010. Estimat-
ing machine translation post-editing effort with hter.
In AMTA 2010- workshop, Bringing MT to the User:
MT Research and the Translation Industry. The Ninth
Conference of the Association for Machine Transla-
tion in the Americas, nov.
Lucia Specia, Hajlaoui N., Hallett C., and Aziz W. 2011.
Predicting machine translation adequacy. In Machine
Translation Summit XIII.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
156
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 95?99,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Chinese Coreference Resolution via Ordered Filtering?
Xiaotian Zhang1,2 Chunyang Wu1,2 Hai Zhao1,2?
1Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
xtian.zh@gmail.com, chunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
Abstract
We in this paper present the model for our
participation (BCMI) in the CoNLL-2012
Shared Task. This paper describes a pure
rule-based method, which assembles dif-
ferent filters in a proper order. Different
filters handle different situations and the
filtering strategies are designed manually.
These filters are assigned to different or-
dered tiers from general to special cases.
We participated in the Chinese and En-
glish closed tracks, scored 51.83 and 59.24
respectively.
1 Introduction
In this paper, we describes the approaches we u-
tilized for our participation in the CoNLL-2012
Shared Task. This year?s shared task targets at
modeling coreference resolution for multiple lan-
guages. Following (Lee et al, 2011), we extend-
s the methodology of deterministic coreference
model, using manually designed rules to rec-
ognize expressions with corresponding entities.
The deterministic coreference model (Raghu-
? This work was partially supported by the Na-
tional Natural Science Foundation of China (Grant No.
60903119 and Grant No. 61170114), the National Re-
search Foundation for the Doctoral Program of Higher E-
ducation of China under Grant No. 20110073120022, the
National Basic Research Program of China (Grant No.
2009CB320901), the Science and Technology Commission
of Shanghai Municipality (Grant No. 09511502400), and
the European Union Seventh Framework Program (Grant
No. 247619).
? Corresponding author.
nathan et al, 2010) has shown good perfor-
mance in the shared task of CoNLL-2011. This
kind of model focuses on filtering with ordered
tiers: One filter is applied at one time, from
highest to lowest precision. However, compared
with learning approaches (Soon et al, 2001), s-
ince effective rules are quite heterogeneous in
different languages, several filtering methods
should be redesigned when different languages
are considered. We modified the original Stan-
ford English coreference system1 to adapt to the
Chinese scenario. For the English participation,
we implemented the full strategies and interface
of the semantic-based filters which are not ob-
tained from the open source toolkit.
The rest of this paper is organized as follows:
In Section 2, we review the related work; In Sec-
tion 3, we describe the detail of our model of
handling coreference resolution in Chinese; Ex-
periment results are reported in Section 4 and
the conclusion is presented in Section 5.
2 Related Work
Many existing works have been published on
learning relation extractors via supervised (Soon
et al, 2001) or unsupervised (Haghighi and K-
lein, 2010; Poon and Domingos, 2008) approach-
es. For involving semantics, (Rahman and Ng,
2011) proposed a coreference resolution model
with world knowledge; By using word associa-
tions, (Kobdani et al, 2011) showed its effec-
tiveness to coreference resolution. Compared
1http://nlp.stanford.edu/software/dcoref.shtml
95
with machine learning methods, (Raghunathan
et al, 2010) proposed rule-base models which
have been witnessed good performance.
Researchers began to work on Chinese coref-
erence resolution at a comparatively late date
and most of them adopt a machine learning
approach. (Guochen and Yunfei, 2005) based
their Chinese personal pronoun coreference res-
olution system on decision trees and (Naiquan et
al., 2009) realized a Chinese coreference resolu-
tion system based on maximum entropy model.
(Weixuan et al, 2010) proposes a SVM-based
approach to anaphora resolution of noun phrases
in Chinese and achieves the F-measure of 63.3%
in the evaluation on ACE 2005. (Guozhi et al,
2011) presented a model for personal pronouns
anaphora resolution based on corpus,which us-
ing rule pretreatment combined with maximum
entropy.
3 Model for Chinese
In general, we adapt Stanford English corefer-
ence system to Chinese by making necessary
changes. The sketch of this deterministic model
is to extract mentions and relevant information
firstly; then several manually designed rules, or
filtering sieves are applied to identify the corefer-
ence. Moreover, these sieves are utilized in a pre-
designed order, which are sorted from highest to
lowest precision. The ordered filtering sieves are
listed in Table 1.
Ordered Sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve
6. Head Matching Sieves
7. Proper Head Word Match Sieve
8. Pronouns Sieve
9. Post-Processing Sieve
Table 1: Ordered filtering sieves for Chinese. Modi-
fied sieves are bold.
We remove the semantic-based sieves due to
the resource constraints. The simplified version
consists of nine filtering sieves. The bold ones
in Table 1 are the modified sieves for Chinese.
First of all, we adopt the head finding rules for
Chinese used in (Levy and Manning, 2003), and
this affects sieve 4, 6 and 7 which are all take
advantage of the head words. And our change
to other sieves are described as follows.
? Mention Detection Sieve: We in this
sieve first extract all the noun phrases,
pronouns (the words with part-of-speech
(POS) tag PN), proper nouns (the word-
s with POS tag NR) and named entities.
Thus a mention candidate set is produced.
We then refine this set by removing several
types of candidates listed as follows:
1. Themeasure words, a special word pat-
tern in Chinese such as ? ?? (a year
of), ???? (a ton of).
2. Cardinals, percents and money.
3. A mention if a larger mention with the
same head word exists.
? Discourse Processing & Pronouns
Sieve: In these two sieves, we adapt
the common pronouns to Chinese. It in-
cludes ?\? (you), ??? (I or me),?? (he
or him),??? (she or her),??? (it),?\??
(plural of ?you?), ???? (we or us),???
(they, gender: male),???? (they, gender:
female),???? (plural of ?it?) and relative
pronoun ?gC? (self). Besides these, we
enrich the pronouns set by adding ?4?, ?4
??, ? T? and ?T?? which are more often
to appear in spoken dialogs as first person
pronouns and ? s? which is used to show
respect for ?you? and the third person pro-
noun ???.
Besides, for mention processing of the original
system, whether a mention is singular or plural
should be given. Different from English POS
tags, in Chinese plural nouns couldn?t be distin-
guished from single nouns in terms of the POS.
Therefore, we add two rules to judge whether a
noun is plural or not.
? A noun that ends with ??? (plural marker
for pronouns and a few animate nouns), and
?? (and so on) is plural.
96
? A noun phrase that involves the coordinat-
ing conjunction words such as ? ?? (and)
is plural.
4 Experiments
4.1 Modification for the English system
We implement the semantic-similarity sieves
proposed in (Lee et al, 2011) with the WordNet.
These modifications consider the alias sieve and
lexical chain sieve. For the alias sieve, two men-
tions are marked as aliases if they appear in the
same synset in WordNet. For the lexical chain
sieve, two mentions are marked as coreference if
linked by a WordNet lexical chain that traverses
hypernymy or synonymy relations.
4.2 Numerical Results
Lang. Coref Anno. R P F
Ch
Before gold 87.78 40.63 55.55auto 80.37 38.95 52.47
After gold 69.56 62.77 65.99auto 65.02 59.76 62.28
En
Before gold 93.65 42.32 58.30auto 88.84 40.17 55.32
After gold 77.49 74.59 76.01auto 72.88 74.53 73.69
Table 2: Performance of the mention detection com-
ponent, before and after coreference resolution, with
both gold and auto linguistic annotations on devel-
opment set.
Lang. R P F
Ch 61.11 62.12 61.61
En 75.23 72.24 73.71
Table 3: Performance of the mention detection com-
ponent, after coreference resolution, with auto lin-
guistic annotations on test set.
Table 2 shows the performance of mention de-
tection both before and after the coreference res-
olution with gold and predicted linguistic anno-
tations on development set. The performance of
mention detection on test set is presented in Ta-
ble 3. The recall is much higher than the preci-
sion so as to make sure less mentions are missed,
Metric R P F1 avg F1
Ch
MUC 50.02 49.64 49.83
51.83BCUBED 65.81 65.50 65.66CEAF (M) 49.88 49.88 49.88
CEAF (E) 40.39 43.47 41.88
BLANC 67.12 65.83 66.45
En
MUC 64.08 63.57 63.82
59.24BCUBED 66.45 70.71 68.51CEAF (M) 57.24 57.24 57.24
CEAF (E) 45.13 45.67 45.40
BLANC 71.12 77.92 73.95
Table 5: Results on the official test set (closed track).
and because spurious mentions will be left as s-
ingletons and removed at last, a low precision
will not affect the final result. The performance
of mention detection for Chinese is worse than
that of English, and this is a direction for future
improvement for Chinese.
Our results on the development set for both
languages are listed in Table 4 and the official
test results are in Table 5. Avg F1 is the arith-
metic mean of MUC, B3, and CEAFE.
We further examine the performance by test-
ing on different data types (broadcast con-
versations, broadcast news, magazine articles,
newswire, conversational speech, and web da-
ta) of the development set, and the results are
shown in Table 6. The system do better on bn,
mz, tc than bc, nw, wb for both Chinese and
English. And it performs the worst on wb due
to a relative lower recall in mention detection.
For Chinese, we also compare the performance
when handling the three different mention types,
proper nominal, pronominal, and other nominal.
Table 7 shows the scores output by the official
scorer when only each kind of mentions are pro-
vided in the keys file and response file each time
and both the quality of the coreference links a-
mong the nominal of each mention type and the
corresponding performance of mention detection
are presented. The performance of coreference
resolution among proper nominal and pronomi-
nal is significant higher than that of other nom-
inal which highly coincides with the results in
Table 6.
97
MUC BCUBED CEAF (E) avg F1Lang. Setting R P F1 R P F1 R P F1
Ch
AUTO 52.38 47.44 49.79 68.25 62.36 65.17 37.43 41.89 39.54 51.50
GOLD 58.16 53.55 55.76 70.66 68.65 69.64 41.44 45.60 43.42 56.27
GMB 63.60 87.63 73.70 62.71 88.32 73.34 74.08 42.83 54.28 67.11
En
AUTO 64.24 64.95 64.59 68.22 73.16 70.60 47.03 46.29 46.66 60.61
GOLD 67.45 66.94 67.20 69.76 73.62 71.64 47.86 48.42 48.14 62.33
GMB 71.78 90.55 80.08 65.45 88.95 75.41 77.42 46.47 58.08 71.19
Table 4: Results on the official development set (closed track). GMB stands for Gold Mention Boundaries
Lang. Anno. bc bn mz nw pt tc wb
Ch AUTO 50.31 53.87 52.80 47.82 - 55.10 47.54GOLD 53.19 63.63 58.23 50.65 - 58.96 50.15
En AUTO 59.26 62.40 63.17 57.57 65.24 60.91 56.88GOLD 60.34 64.51 64.36 59.71 67.07 62.44 58.47
Table 6: Results (Avg F1) on different data types of the development set (closed track).
Proper nominal Pronominal Other nominal
Data Type MD (Recall) avg F1 MD (Recall) avg F1 MD (Recall) avg F1
bc 94.5 (550/582) 68.06 94.5 (1372/1452) 66.40 80.5 (1252/1555) 47.74
bn 96.7 (1213/1254) 67.46 97.8 (264/270) 77.39 83.7 (1494/1786) 53.51
mz 92.0 (526/572) 67.05 94.8 (91/96) 56.89 76.1 (834/1096) 53.68
nw 91.4 (402/440) 67.44 90.6 (29/32) 83.54 51.0 (1305/2559) 44.86
tc 100 (23/23) 95.68 84.5 (572/677) 61.96 71.2 (272/382) 53.88
wb 93.2 (218/234) 72.23 95.9 (397/414) 72.55 77.1 (585/759) 43.37
all 94.4 (2932/3105) 68.30 92.7 (2725/2941) 68.10 70.6 (5742/8137) 49.56
Table 7: Results ( Recall of mention detection and Avg F1) on different data types and different mention
types of the development set with linguistic annotations (closed track).
98
5 Conclusion
We presented the rule-base approach for the BC-
MI?s participation in the shared task of CoNLL-
2012. We extend the work by (Lee et al, 2011)
and modified several tiers to adapt to Chinese.
Numerical results show the effectiveness in the
evaluation for Chinese and English. For the
Chinese scenario, we firstly show it is possible
to consider special POS-tags and common pro-
nouns as indicators for improving the perfor-
mance. This work could be extended by involv-
ing more feasible filtering tiers or utilizing some
automatic rule generating methods.
References
Li Guochen and Luo Yunfei. 2005. ?^`k?J?
???<?????). (Personal pronoun
coreference resolution in Chinese using a priority
selection strategy). Journal of Chinese Informa-
tion Processings, pages 24?30.
Dong Guozhi, Zhu Yuquan, and Cheng Xianyi. 2011.
Research on personal pronoun anaphora resolution
in chinese. Application Research of Computers,
28:1774?1776.
Aria Haghighi and Dan Klein. 2010. Coreference
resolution in a modular, entity-centered model. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT
?10, pages 385?393, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrapping
coreference resolution using word associations. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
783?792, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 28?34, Portland, Oregon,
USA, June. Association for Computational Lin-
guistics.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics - Volume
1, ACL ?03, pages 439?446, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Hu Naiquan, Kong Fang, Wang Haidong, and Zhou
Guodong. 2009. Realization on chinese corefer-
ence resolution system based on maximum entropy
model. Application Research of Computers, pages
2948?2951.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov log-
ic. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 650?659, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan
Rangarajan, Nate Chambers, Mihai Surdeanu,
Dan Jurafsky, and Christopher Manning. 2010.
A multi-pass sieve for coreference resolution. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
492?501, Cambridge, MA, October. Association
for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 814?824,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Y-
ong Lim. 2001. A machine learning approach to
coreference resolution of noun phrases. Comput.
Linguist., 27(4):521?544, December.
Tan Weixuan, Kong Fang, Wang Haidong, and Zhou
Guodong. 2010. Svm-based approach to chinese
anaphora resolution. High Performance Comput-
ing Technology, pages 30?36.
99
