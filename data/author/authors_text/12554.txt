Balancing Expressiveness and Simplicity
in an Interlingua for Task Based Dialogue
Lori Levin, Donna Gates, Dorcas Wallace,
Kay Peterson, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
email: lsl@cs.cmu.edu
Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, Nadia Mana
IRST-itc, Italy
Abstract
In this paper we compare two interlin-
gua representations for speech transla-
tion. The basis of this paper is a distri-
butional analysis of the C-star II and
Nespole databases tagged with inter-
lingua representations. The C-star II
database has been partially re-tagged
with the Nespole interlingua, which
enables us to make comparisons on the
same data with two types of interlin-
guas and on two types of data (C-
star II and Nespole) with the same
interlingua. The distributional infor-
mation presented in this paper show
that the Nespole interlingua main-
tains the language-independence and
simplicity of the C-star II speech-act-
based approach, while increasing se-
mantic expressiveness and scalability.
1 Introduction
Several speech translation projects have chosen
interlingua-based approaches because of its con-
venience (especially in adding new languages)
in multi-lingual projects. However, interlingua
design is notoriously dicult and inexact. The
main challenge is deciding on the grain size of
meaning to represent and what facets of mean-
ing to include. This may depend on the do-
main and the contexts in which the translation
system is used. For projects that take place at
multiple research sites, another factor becomes
important in interlingua design: if the interlin-
gua is too complex, it cannot be used reliably by
researchers at remote sites. Furthermore, the in-
terlingua should not be biased toward one fam-
ily of languages. Finally, an interlingua should
clearly distinguish general and domain specic
components for easy scalability and portability
between domains.
Sections 2 and 3 describe how we balanced
the factors of grain-size, language independence,
and simplicity in two interlinguas for speech
translation projects | the C-star II Inter-
change Format (Levin et al, 1998) and the Ne-
spole Interchange Format. Both interlinguas
are based in the framework of domain actions
as described in (Levin et al, 1998). We will
show that the Nespole interlingua has a ner
grain-size of meaning, but is still simple enough
for collaboration across multiple research sites,
and still maintains language-independence.
Section 4 will address the issue of scalabil-
ity of interlinguas based on domain actions to
larger domains. The basis of Section 4 is a dis-
tributional analysis of the C-star II and Ne-
spole databases tagged with interlingua repre-
sentations. The C-star II database has been
partially re-tagged with the Nespole interlin-
gua, which enables us to make comparisons on
the same data with two types of interlinguas and
on two types of data (C-star II and Nespole)
with the same type of interlingua.
2 The C-star II Domain, Database,
and Interlingua
The C-star II interlingua (Levin et al, 1998)
was developed between 1997 and 1999 for use
in the C-star II 1999 demo (www.c-star.org).
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 53-60.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
c: can I have some flight times
that would leave some time around June sixth
a: the there are several flights leaving D C
there?d be one at one twenty four
there?s a three fifty nine flight
that arrives at four fifty eight
...
what time would you like to go
c: I would take the last one that you mentioned
...
a: what credit card number would you like
to reserve this with
c: I have a visa card
and the number is double oh five three
three one one six
ninety nine eighty seven
a okay
c: the expiration date is eleven ninety seven
...
a okay they should be ready tomorrow
c: okay thank you very much
Figure 1: Excerpt from a C-star II dialogue
with six participating research sites. The seman-
tic domain was travel, including reservations
and payments for hotels, tours, and transporta-
tion. Figure 1 shows a sample dialogue from
the C-star II database. (C is the client and a
is the travel agent.) The C-star II database
contains 2278 English sentences and 7148 non-
English (Japanese, Italian, Korean) sentences
tagged with interlingua representations. Most
of the database consists of transcripts of role-
playing conversations.
The driving concept behind the C-star II
interlingua is that there are a limited num-
ber of actions in the domain | requesting the
price of a room, telling the price of a room,
requesting the time of a flight, giving a credit
card number, etc. | and that each utter-
ance can be classied as an instance of one
of these domain actions . Figure 2 illustrates
the components of the C-star II interlingua:
(1) the speaker tag, in this case c for client,
(2) a speech act (request-action), (3) a list
of concepts (reservation, temporal, hotel),
(4) arguments (e.g., time), and (5) values of ar-
guments. The C-star II interlingua specica-
tion document contains denitions for 44 speech
acts, 93 concepts, and 117 argument names.
The domain action is the part of the interlin-
gua consisting of the speech act and concepts, in
this case request-action+reservation+tem-
poral+hotel. The domain action does not in-
clude the list of argument-value pairs.
First it is important to point out that do-
main actions are created compositionally. A do-
main action consists of a speech act followed by
zero or more concepts. (Recall that argument-
value pairs are not part of the domain action.)
The Nespole interlingua includes 65 speech
acts and 110 concepts. An interlingua speci-
cation document denes the legal combinations
of speech acts and arguments.
The linguistic justication for an interlingua
based on domain-actions is that many travel do-
main utterances contain xed, formulaic phrases
(e.g., can you tell me; I was wondering; how
about; would you mind, etc.) that signal domain
actions, but either do not translate literally into
other languages or have a meaning that is su-
ciently indirect that the literal meaning is irrele-
vant for translation. To take two examples, how
about as a signal of a suggestion does not trans-
late into other languages with the words corre-
sponding to how and about . Also, would you
mind might translate literally into some Euro-
pean languages as a way of signaling a request,
but the literal meaning of minding is not rel-
evant to the translation, only the fact that it
signals politeness.
The measure of success for the domain-action
based interlingua (as described in (Levin et al,
2000a)) is that (1) it covers the data in the C-
star II database with less than 8% no-tag rate,
(2) inter-coder agreement across research sites
is reasonably high: 82% for speech acts, 88%
for concepts, and 65% for domain actions, and
(3) end-to-end translation results using an an-
alyzer and generator written at dierent sites
were about the same as end-to-end translation
results using an analyzer and generator written
at the same site.
3 The Nespole Domain, Database,
and Interlingua
The Nespole interlingua has been under devel-
opment for the last two years as part of the Ne-
spole project (http://nespole.itc.it). Fig-
I would like to make a hotel reservation for the fourth through
the seventh of july
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Figure 2: Example of a C-star II interlingua representation
ure 3 shows a Nespole dialogue. The Ne-
spole domain does not include reservations and
payments, but includes more detailed inquiries
about hotels and facilities for ski vacations and
summer vacations in Val di Fiemme, Italy. (The
tourism board of the Trentino area is a partner
of the Nespole project.) Most of the database
consists of transcripts of dialogues between an
Italian-speaking travel agent and an English or
German speaker playing the role of a traveller.
There are fewer xed, formulaic phrases in the
Nespole domain, prompting us to move toward
domain actions that are more general, and also
requiring more detailed interlingua representa-
tions. Changes from the C-star II interlingua
fall into several categories:
1. Extending semantic expressivity and
syntactic coverage: Increased coverage of
modality, tense, aspect, articles, fragments,
coordinate structures, number, and rhetor-
ical relations. In addition, we have added
more explicit representation of grammati-
cal relations and improved capabilities for
representing modication and embedding.
2. Additional Domain-Specic Con-
cepts: New concepts include giving
directions, describing sizes and dimensions
of objects, traveling routes, equipment and
gear, airports, tourist services, facilities,
vehicles, information objects (brochures,
web pages, rules and regulations), hours
of operation of businesses and attractions,
etc.
3. Utterances that accompany multi-
modal gestures: The Nespole system
includes capabilities to share web pages
and draw marks such as circles and arrows
on web pages. The interlingua was ex-
tended to cover colord, descriptions of two-
dimensional objects, and actions of show-
ing.
4. General concept names from Word-
Net: The Nespole interlingua includes
conventions for making new concept names
based on WordNet synsets.
5. More general domain actions replac-
ing specic ones: For example, replacing
hotel with accommodation.
Interlinguas based on domain actions con-
trast with interlinguas based on lexical seman-
tics (Dorr, 1993; Lee et al, 2001; Goodman and
Nirenburg, 1991). A lexical-semantic interlingua
includes a representation of predicates and their
arguments. For example, the sentence I want to
take a vacation has a predicate want with two
arguments I and to take a vacation, which in
turn has a predicate take and two arguments, I
and a vacation. Of course, predicates like take
may be represented as word senses that are less
language-dependent like participate-in. The
strength and weakness of the lexical-semantic
approach is that it is less domain dependent
than the domain-action approach.
In order to cover the less formulaic utterances
of the Nespole domain, we have taken a step
closer to the lexical-semantic approach. How-
ever, we have maintained the overall framework
of the domain-action approach because there are
still many formulaic utterances that are better
represented in a non-literal way. Also, in or-
der to abstract away from English syntax, con-
cepts such as disposition, eventuality, and obli-
gation are not represented in the interlingua as
argument-taking main verbs in order to accom-
modate languages in which these meanings are
c: and I have some questions about coming about a trip I?m gonna be taking to Trento
a: okay what are your questions
c: I currently have a hotel booking at the
Panorama-Hotel in Panchia but at the moment I have no idea how to get to my hotel from Trento
and I wanted to ask what would be the best way for me to get there
a: okay I?m gonna show you a map that and then describe the directions to you
okay so right so you will arrive in the train station in Trento
the that is shown in the middle of the map stazione FFSS
and just below that here is a bus stop labeled number forty
so okay on the map that I?m showing you here
the hotel is the orange building off on the right hand side
...
c: I also wanted to ask about skiing in the area once I?m in Panchia
a: all right just a moment and I?ll show you another map
c: okay
a: okay so on the map you see now Panchia is right in the center of the map
c: I see it
Figure 3: Excerpt from a Nespole dialogue
represented as adverbs or suxes on verbs. Fig-
ure 4 shows the Nespole interlingua represen-
tation corresponding to the C-star II interlin-
gua in Figure 2. The specication document for
the Nespole interlingua denes 65 speech acts,
110 concepts, 292 arguments, and 7827 values
grouped into 222 value classes. As in the C-
star II interlingua, domain actions are dened
compositionally from speech acts and arguments
in combinations that are allowed by the interlin-
gua specication.
3.1 Comparison of Nespole and
C-star II Interlinguas
It is useful to compare the Nespole and C-
star II Interlinguas in expressivity, language in-
dependence, and simplicity.
Expressivity of the Nespole interlingua,
Argument 1: The metric we use for expres-
sivity is the no-tag rate in the databases. The
no-tag rate is the percentage of sentences that
cannot be assigned an interlingua representation
by a human expert. The C-star II database
tagged with C-star II interlingua had a no-
tag rate of 7.3% (Levin et al, 2000a). The
C-star II database tagged with Nespole in-
terlingua has a no-tag rate of 2.4%. More than
300 English sentences in the C-star II database
that were not covered by the C-star II interlin-
gua are now covered by the Nespole interlin-
gua. (See Table 2.) We conclude from this that
the Nespole interlingua is more expressive in
that it covers more data.
Language-independence of the Nespole
interlingua: We do not have a numerical
measure of language-independence, but we note
that interlinguas based on domain actions are
particularly suitable for avoiding translation
mismatches (Dorr, 1994), particularly head-
switching mismatches (e.g., I just arrived and
Je vient d?arriver where the meaning of recent
past is expressed by an adverb just or a syn-
tactic verb vient (venir).) Interlinguas based
on domain actions resolve head-switching mis-
matches by identifying the types of meanings
that are often involved in mismatches | modal-
ity, evidentiality, disposition, and so on | and
assigning them a representation that abstracts
away from predicate argument structure. In-
terlinguas based on domain actions also neu-
tralize the dierent ways of expressing indirect
speech acts within and across languages (for ex-
ample, Would you mind..., I was wondering if
you could...., and Please.... as ways of request-
ing an action). Although Nespole domain ac-
tions are more general than C-star II domain
actions, they maintain language independence
by abstracting away from predicate-argument
structure.
Simplicity and cross-site reliability of the
Nespole interlingua: Simplicity of an inter-
lingua is measured by cross-site reliability in
I would like to make a hotel reservation for the fourth through
the seventh of july
C-star II Interlingua:
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Nespole Interlingua:
c:give-information+disposition+reservation+accommodation
(disposition=(who=i, desire),
reservation-spec=(reservation, identifiability=no),
accommodation-spec=hotel,
object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))}
Figure 4: Example of Nespole interlingua representation
inter-coder agreement and end-to-end transla-
tion performance. At the time of writing this pa-
per we have not conducted cross-site inter-coder
agreement experiments using the Nespole in-
terlingua. We have, however, conducted cross-
site evaluations (Lavie et al, 2002), in which the
analyzer and generator were written at dier-
ent sites. Experiments at the end of C-star II
showed that cross-site evaluations were compa-
rable to intra-site evaluations (analyzer and gen-
erator written at the same site) (Levin et al,
2000b). Nespole evaluations so far show a loss
of cross-site reliability: intra-site evaluations are
noticeably better than cross-site evaluations, as
reported in (Lavie et al, 2002). This seems to
indicate that developers at dierent sites have
a lower level of agreement on the Nespole in-
terlingua. However there are other possible ex-
planations for the discrepancy | for example
developers at dierent sites may have focused
their development on dierent sub-domains |
that are currently under investigation.
4 Scalability of the Nespole
Interlingua
The rest of this paper addresses the scalability
of the Nespole interlingua. A possible criti-
cism of domain actions is that they are domain
dependent and that the number of domain ac-
tions might increase too quickly with the size
of the domain. In this section, we will examine
the rate of increase in the number of domain ac-
tions as a function of the amount of data and
the diversity of the data.
Dierences in the C-star and Nespole Do-
mains: We will rst show that the C-star
and Nespole domains are signicantly dierent
even though they both pertain to travel. The
combination of the two domains is therefore sig-
nicantly larger than either domain alone.
In order to demonstrate the dierences be-
tween the C-star travel domain and the Ne-
spole travel domain, we measured the overlap
in vocabulary. The numbers in Table 4 are based
on the rst 7900 word tokens in the C-star En-
glish database and the rst 7900 word tokens
in the Nespole English database. The table
shows the number of unique word types in each
database, the number of word types that occur
in both databases, and the number of word types
that occur in one of the databases, but not in the
other. In each database, about half of the word
types overlap with the other database. The non-
overlapping vocabulary (402 C-star word types
and 344 Nespole word types) indicates that the
two databases cover quite dierent aspects of the
travel domain.
Scalability: Argument 1: We will now be-
gin to address the issue of scalability of the
domain action approach to interlingua design.
Our rst argument concerns the number of
Number of unique word types
CSTAR English 745
Nespole English 687
Word types in both CSTAR and Nespole 343
Words types in CSTAR not in Nespole 402
Words types n Nespole not in CSTAR 344
Table 1: Number of overlapping word types in the C-star English and Nespole English
databases
SA Con. Snts. Domain Ac-
tions
Old C-star English 44 93 2278 358
New C-star English 65 110 2564 452
Nespole English 65 110 1446 337
Nespole German 65 110 3298 427
Nespole Italian 65 110 1063 206
Table 2: Number of unique domain actions in interlingua databases
speech acts and concepts in the combined C-
star/Nespole domain. The C-star II in-
terlingua, designed for coverage of the C-star
travel domain, included 44 speech acts and 93
concepts. The Nespole interlingua, designed
for coverage of the combined C-star and Ne-
spole domains, has 65 speech acts and 110 con-
cepts. Thus a relatively small increase in the
number of speech acts and concepts is required
to cover a signicantly larger domain.
The increased size of the C-star/Nepsole
domain is reflected in the number of arguments
and values. The C-star II interlingua contained
denitions for 117 arguments, whereas the Ne-
spole interlingua contains denitions for 292 ar-
guments. The number of values for arguments
also has increased signicantly in the Nespole
domain. There are 7827 values grouped into 222
classes (airport names, days of the week, etc.).
Distributional Data: number of domain
actions in each database: Next we will
present distributional data concerning the num-
ber of domain actions as a function of database
size. We will compare several databases: Old
C-star English (around 2278 sentences tagged
with C-star II interlingua), New C-star En-
glish (2564 sentences tagged with Nespole in-
terlingua, including the 2278 sentences from Old
C-star English), Nespole English, Nespole
German, and Nespole Italian. Table 2 shows
the number of sentences and the number of do-
main actions in each database. The number of
domain actions refers to the number of types,
not tokens, of domain actions.
Distributional data: Coverage of the top
50 domain actions: Table 3 shows the per-
centage of each database that is covered by the
5, 10, 20, and 50 most frequent domain actions
in that database. For each database, the do-
main actions were ordered by frequency. The
percentage of sentences covered by the top-n
domain actions was then calculated. For this
experiment, we separated sentences spoken by
the traveller (client) and sentences spoken by
the travel agent (agent). C-star data in Ta-
ble 3 refers to 2564 English sentences from the
C-star database that were tagged with Ne-
spole interlingua. Nespole data refers to the
English portion of the Nespole database (1446
sentences). Combined data refers to the combi-
nation of the two (4014 sentences).
Two points are worth noting about Table 3.
First, the Nespole agent data has a higher cov-
erage rate than the Nespole client data. That
is, more data is covered by the top-n domain
actions. This may be because there was was
Domain Actions Top 5 Top 10 Top 20 Top 50
Client
C-star data 33.6 42.7 53.1 66.7
Nespole data 31.7 43.5 53.9 66.5
Combined data 31.6 40.0 50.3 62.9
Agent
C-star data 33.8 42.8 54.1 67.3
Nespole data 39.0 47.8 56.1 71.4
Combined data 33.6 41.5 51.7 64.0
Table 3: DA Coverage using Nespole interlingua on English data for both C-star and
Nespole
only a small amount of English agent data and
it was spoken by non-native speakers. Second,
the combined data has a slightly lower cover-
age rate than either the C-star or Nespole
databases alone. This is expected because, as
shown above, the combined domain is signi-
cantly more diverse than either domain by itself.
Scalability: Argument 2: Table 3 provides
additional evidence for the scalability of the Ne-
spole interlingua to larger domains. In the
combined C-star and Nespole domain, the
top 50 domain actions cover only slightly less
data than the top 50 domain actions in either
domain separately. There is not, in fact, an ex-
plosion of domain actions when the two C-star
and Nespole domains are combined.
Distributional Data: domain actions as a
function of database size: Table 3 shows
that in each of our databases, the 50 most fre-
quent domain actions cover approximately 65%
of the sentences. The next issue we address is
the nature of the \tail" of less frequent domain
actions covering the remainder of the data.
Figure 5 shows the number of domain actions
as a function of data set size. Sampling was done
for intervals of 25 sentences starting at 100 sen-
tences. For each sample size s there was ten-fold
cross-validation. Ten random samples of size s
were chosen, and the number of dierent domain
actions in each sample was counted. The aver-
age of the number of domain actions in each of
the ten samples of size s are plotted in Figure 5.
The four databases represented in Figure 5 are
IF Coverage of Four Datasets
0
100
200
300
400
500
600
700
10
0
70
0
13
00
19
00
25
00
31
00
number of SDUs in sample
av
er
ag
e 
nu
m
be
r o
f u
ni
qu
e 
DA
s 
o
ve
r 
10
 ra
nd
om
 s
am
pl
es
Old CSTAR
New CSTAR
NESPOLE
Combined
Figure 5: Number of domain actions as a function of
database size
the C-star English database tagged with C-
star II interlingua, the C-star II database
tagged with Nespole interlingua, the Nespole
English database, and the combined C-star
and Nespole English databases.
Expressivity, Argument 2: Figure 5 pro-
vides evidence for the increased expressivity of
the Nespole interlingua. In contrast to Ta-
ble 3, which deals with samples containing the
most frequent domain actions, the samples plot-
ted in Figure 5 contain random mixtures of fre-
quent and non-frequent domain actions. The
curve representing the C-star data with C-
star II interlingua is the slowest growing of the
four curves. This is because the grain-size of
meaning represented in the C-star II interlin-
gua was larger than in the Nespole interlin-
gua. Also many infrequent domain actions were
not covered by the C-star II interlingua. The
faster growth of the curve representing the C-
star data with Nespole interlingua indicates
improved expressivity of the Nespole interlin-
gua | it covers more of the infrequent domain
actions. The highest curve in Figure 5 repre-
sents the combined C-star and Nespole do-
mains. This curve is higher than the others be-
cause, as shown above, the two travel domains
are signicantly dierent from each other.
Expressivity and Simplicity, the right bal-
ance: Comparing Table 3 and Figure 5, we ar-
gue that the Nespole interlingua strikes a good
balance between expressivity and simplicity. Ta-
ble 3 shows evidence for the simplicity of the Ne-
spole interlingua: Only 50 domain actions are
needed to cover 60-70% of the sentences in the
database. Figure 5 shows evidence for expressiv-
ity: because domain actions are compositionally
formed from speech acts and concepts, it is pos-
sible to form a large number of low-frequency
domain actions in order to cover the domain.
Over 600 domain actions are used in the com-
bined C-star and Nespole domains.
5 Conclusions
We have presented a comparison of a purely
domain-action-based interlingua (the C-star II
interlingua) and a more expressive, but still
domain-action-based interlingua (the Nespole
interlingua). The data that we have presented
show that the more expressive interlingua has
better coverage of the domain (a decrease from
7.3% to 2.4% uncovered data in the C-star II
domain) and can also scale up to larger domains
without an explosion of domain actions. Thus
we have a reasonable compromise between sim-
plicity and expressiveness of the interlingua.
Acknowledgments
We would like to acknowledge Hans-Ulrich Block
for rst proposing the domain-action-based in-
terlingua to the C-star consortium. We would
also like to thank all of the C-star and Ne-
spole partners who have participated in the de-
sign of the interlingua. This work was supported
by NSF Grant 9982227 and EU Grant IST 1999-
11562 as part of the joint EU/NSF MLIAM re-
search initiative.
References
Bonnie J. Dorr. 1993. Machine Translation: A View
from the Lexicon. The MIT Press, Cambridge,
Massachusetts.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597{633.
Kenneth Goodman and Sergei Nirenburg. 1991.
The KBMT Project: A Case Study in Knowledge-
Based Machine Translation. Morgan Kaufmann,
San Mateo, CA.
Alon Lavie, Florian Metze, Roldano Cattoni, and Er-
ica Constantini. 2002. A Multi-Perspective Eval-
uation of the NESPOLE! Speech-to-Speech Trans-
lation System. In Proceedings of Speech-to-Speech
Translation: Algorithms and Systems.
Young-Suk Lee, W. Yi, Cliord Weinstein, and
Stephanie Sene. 2001. Interlingua-based broad-
coverage korean-to-english translation. In Pro-
ceedings of HLT, San Diego.
Lori Levin, Donna Gates, Alon Lavie, and Alex
Waibel. 1998. An Interlingua Based on Domain
Actions for Machine Translation of Task-Oriented
Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP?98), pages Vol. 4, 1155{1158, Sydney, Aus-
tralia.
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi,
Dorcas Wallace, Taro Watanabe, and Monika
Woszczyna. 2000a. Evaluation of a Practical In-
terlingua for Task-Oriented Dialogue. In Work-
shop on Applied Interlinguas: Practical Applica-
tions of Interlingual Approaches to NLP, Seattle.
Lori Levin, Alon Lavie, Monika Woszczyna, Donna
Gates, Marsal Gavalda, Detlef Koll, and Alex
Waibel. 2000b. The Janus-III Translation Sys-
tem. Machine Translation.
A Web-based Demonstrator of a Multi-lingual Phrase-based
Translation System
Roldano Cattoni, Nicola Bertoldi, Mauro Cettolo, Boxing Chen and Marcello Federico
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
38050 Povo - Trento, Italy
{surname}@itc.it
Abstract
This paper describes a multi-lingual
phrase-based Statistical Machine Transla-
tion system accessible by means of a Web
page. The user can issue translation re-
quests from Arabic, Chinese or Spanish
into English. The same phrase-based sta-
tistical technology is employed to realize
the three supported language-pairs. New
language-pairs can be easily added to the
demonstrator. The Web-based interface al-
lows the use of the translation system to
any computer connected to the Internet.
1 Introduction
At this time, Statistical Machine Translation
(SMT) has empirically proven to be the most
competitive approach in international competi-
tions like the NIST Evaluation Campaigns1 and
the International Workshops on Spoken Language
Translation (IWSLT-20042 and IWSLT-20053).
In this paper we describe our multi-lingual
phrase-based Statistical Machine Translation sys-
tem which can be accessed by means of a Web
page. Section 2 presents the general log-linear
framework to SMT and gives an overview of
our phrase-based SMT system. In section 3
the software architecture of the demo is out-
lined. Section 4 focuses on the currently supported
language-pairs: Arabic-to-English, Chinese-to-
English and Spanish-to-English. In section 5 the
Web-based interface of the demo is described.
1http://www.nist.gov/speech/tests/mt/
2http://www.slt.atr.jp/IWSLT2004/
3http://www.is.cs.cmu.edu/iwslt2005/
2 SMT System Description
2.1 Log-Linear Model
Given a string f in the source language, the goal of
the statistical machine translation is to select the
string e in the target language which maximizes
the posterior distribution Pr(e | f). By introduc-
ing the hidden word alignment variable a, the fol-
lowing approximate optimization criterion can be
applied for that purpose:
e? = arg max
e
Pr(e | f)
= arg max
e
?
a
Pr(e,a | f)
? arg max
e,a
Pr(e,a | f)
Exploiting the maximum entropy (Berger et
al., 1996) framework, the conditional distribu-
tion Pr(e,a | f) can be determined through
suitable real valued functions (called features)
hr(e, f ,a), r = 1 . . . R, and takes the parametric
form:
p?(e,a | f) ? exp{
R
?
r=1
?rhr(e, f ,a)}
The ITC-irst system (Chen et al, 2005) is
based on a log-linear model which extends the
original IBM Model 4 (Brown et al, 1993)
to phrases (Koehn et al, 2003; Federico and
Bertoldi, 2005). In particular, target strings e are
built from sequences of phrases e?1 . . . e?l. For each
target phrase e? the corresponding source phrase
within the source string is identified through three
random quantities: the fertility ?, which estab-
lishes its length; the permutation pii, which sets
its first position; the tablet f? , which tells its word
string. Notice that target phrases might have fer-
tility equal to zero, hence they do not translate any
91
source word. Moreover, uncovered source posi-
tions are associated to a special target word (null)
according to specific fertility and permutation ran-
dom variables.
The resulting log-linear model applies eight fea-
ture functions whose parameters are either esti-
mated from data (e.g. target language models,
phrase-based lexicon models) or empirically fixed
(e.g. permutation models). While feature func-
tions exploit statistics extracted from monolingual
or word-aligned texts from the training data, the
scaling factors ? of the log-linear model are esti-
mated on the development data by applying a min-
imum error training procedure (Och, 2004).
2.2 Decoding Strategy
The translation of an input string is performed by
the SMT system in two steps. In the first pass a
beam search algorithm (decoder) computes a word
graph of translation hypotheses. Hence, either
the best translation hypothesis is directly extracted
from the word graph and output, or an N-best list
of translations is computed (Tran et al, 1996). The
N-best translations are then re-ranked by applying
additional features and the top ranking translation
is finally output.
The decoder exploits dynamic programming,
that is the optimal solution is computed by expand-
ing and recombining previously computed partial
theories. A theory is described by its state which is
the only information needed for its expansion. Ex-
panded theories sharing the same state are recom-
bined, that is only the best scoring one is stored
for further expansions. In order to output a word
graph of translations, backpointers to all expanded
theories are mantained, too.
To cope with the large number of generated the-
ories some approximations are introduced during
the search: less promising theories are pruned off
(beam search) and a new source position is se-
lected by limiting the number of vacant positions
on the left-hand and the distance from the left most
vacant position (re-ordering constraints).
2.3 Phrase extraction and model training
Training of the phrase-based translation model
requires a parallel corpus provided with word-
alignments in both directions, i.e. from source
to target positions, and viceversa. This pre-
processing step can be accomplished by applying
the GIZA++ toolkit (Och and Ney, 2003) that pro-
vides Viterbi alignments based on IBM Model-4.
Starting from the parallel training corpus, pro-
vided with direct and inverted alignments, the so-
called union alignment (Och and Ney, 2003) is
computed.
Phrase-pairs are extracted from each sentence pair
which correspond to sub-intervals of the source
and target positions, J and I , such that the union
alignment links all positions of J into I and all
positions of I into J . In general, phrases are ex-
tracted with maximum length in the source and tar-
get defined by the parameters Jmax and Imax. All
such phrase-pairs are efficiently computed by an
algorithm with complexity O(lImaxJ2max) (Cet-
tolo et al, 2005).
Given all phrase-pairs extracted from the train-
ing corpus, lexicon probabilities and fertility prob-
abilities are estimated.
Target language models (LMs) used by the de-
coder and rescoring modules are, respectively,
estimated from 3-gram and 4-gram statistics
by applying the modified Kneser-Ney smoothing
method (Goodman and Chen, 1998). LMs are es-
timated with an in-house software toolkit which
also provides a compact binary representation of
the LM which is used by the decoder.
3 Demo Architecture
Figure 1 shows the two-layer architecture of the
demo. At the bottom lie the programs that provide
the actual translation services: for each language-
pair a wrapper coordinates the activity of a special-
ized pre-processing tool and a MT decoder. The
translation programs run on a grid-based cluster
of high-end PCs to optimize the processing speed.
All the wrappers communicate with the MT front-
end whose main task is to forward translation re-
quests to the appropriate language-pair wrapper
and to report an error in case of wrong requests
(e.g. unsupported language-pair). It is worth
noticing here that a new language-pair can be eas-
ily added to the system with a minimal interven-
tion on the code of the MT front-end.
At the top of the architecture are the programs
that provide the interface with the user. This layer
is separated from the translation layer (hosted by
internal machines only) by means of a firewall.
The user interface is implemented as a Web page
in which a translation request (a source sentence
and a language-pair) is input by means of an
HTML form. The cgi script invocated by the form
manages the interaction with the MT front-end.
92
Web Page
(form)
script
CGI
lang 1
wrapper
prepro?
cessing
MT
decoder
prepro?
cessing
MT
decoder
wrapper
lang 2
prepro?
cessing
MT
decoder
wrapper
lang N
...
MT
front?end
firewall
external host
internal hosts
fast machines
Figure 1: Architecture of the demo. For each
language-pair a set of programs (in particular the
MT decoder) provides the translation service. The
request issued by the user on the Web page is
sent by the cgi script to the MT front-end. The
translation is then performed on the appropriate
language-pair service and the output sent back to
the Web browser.
When a user issues a translation request after
filling the form fields, the cgi script sends the re-
quest to the MT front-end and waits for its reply.
The input sentence is then forwarded to the wrap-
per of the appropriate language-pair. After a pre-
processing step, the actual translation is performed
by the specific MT decoder. The output in the tar-
get language is then sent back to the user?s Web
browser through the chain in the reverse order.
From a technical point of view, the inter-process
communication is realized by means of standard
TCP-IP sockets. As far as the encoding of texts is
concerned, all the languages are encoded in UTF-
8: this allows to manage the processing phase in
an uniform way and to render graphically different
character sets.
4 The supported language-pairs
Although there is no theoretical limit to the num-
ber of supported language-pairs, the current ver-
sion of the demo provides translations to English
from three source languages: Arabic, Chinese and
Spanish. For demonstration purpose, three differ-
ent application domains are covered too.
Arabic-to-English (Tourism)
The Arabic-to-English system has been trained
with the data provided by the International Work-
shop on Spoken Language Translation 2005 The
context is that of the Basic Traveling Expres-
sion Corpus (BTEC) task (Takezawa et al, 2002).
BTEC is a multilingual speech corpus which con-
tains sentences coming from phrase books for
tourists. Training set includes 20k sentences con-
taining 159K Arabic and 182K English running
words; vocabulary size is 18K for Arabic, 7K for
English.
Chinese-to-English (Newswire)
The Chinese-to-English system has been trained
with the data provided by the NIST MT Evaluation
Campaign 2005 , large-data condition. In this case
parallel data are mainly news-wires provided by
news agencies. Training set includes 71M Chinese
and 77M English running words; vocabulary size
is 157K for Chinese, 214K for English.
Spanish-to-English (European Parliament)
The Spanish-to-English system has been trained
with the data provided by the Evaluation Cam-
paign 2005 of the European integrated project TC-
STAR4. The context is that of the speeches of
the European Parliament Plenary sessions (EPPS)
from April 1996 to October 2004. Training set for
the Final Text Edition transcriptions includes 31M
Spanish and 30M English running words; vocabu-
lary size is 140K for Spanish, 94K for English.
5 The Web-based Interface
Figure 2 shows a snapshot of the Web-based in-
terface of the demo ? the URL has been removed
to make this submission anonymous. In the upper
part of the page the user provides the two informa-
tion required for the translation: the source sen-
tence can be input in a 80x5 textarea html struc-
ture, while the language-pair can be selected by
means of a set a radio-buttons. The user can reset
the input area or send the translation request by
means of standard reset and submit buttons. Some
examples of bilingual sentences are provided in
the lower part of the page.
4http://www.tc-star.org
93
Figure 2: A snapshot of the Web-based interface.
The user provides the sentence to be translated
in the desired language-pair. Some examples of
bilingual sentences are also available to the user.
The output of a translation request is simple: the
requested source sentence, the translation in the
target language and the selected language-pair are
presented to the user. Figure 3 shows an example
of an Arabic sentence translated into English.
We plan to extend the interface with the pos-
sibility for the user to ask additional information
about the translation ? e.g. the number of explored
theories or the score of the first-best translation.
6 Acknowledgements
This work has been funded by the European Union
under the integrated project TC-STAR - Technol-
ogy and Corpora for Speech to Speech Translation
- (IST-2002-FP6-506738, http://www.tc-star.org).
References
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39?71.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?313.
Figure 3: Example of an Arabic sentence trans-
lated into English.
Mauro Cettolo, Marcello Federico, Nicola Bertoldi,
Roldano Cattoni, and Boxing Chen. 2005. A look
inside the itc-irst smt system. In Proceedings of the
10th Machine Translation Summit, pages 451?457,
Phuket, Thailand, September.
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and
M. Federico. 2005. The ITC-irst SMT System for
IWSLT-2005. In Proceedings of the IWSLT 2005,
Pittsburgh, USA.
M. Federico and N. Bertoldi. 2005. A Word-to-Phrase
Statistical Translation Model. ACM Transactions on
Speech and Language Processing. to appear.
J. Goodman and S. Chen. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACl 2003, pages 127?133, Edmonton, Canada.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2004. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, Sapporo, Japan.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a Broad-Coverage
Bilingual Corpus for Speech Translation of Travel
Conversations in the Real World. In Proceedings of
3rd LREC, pages 147?152, Las Palmas, Spain.
B. H. Tran, F. Seide, and V. Steinbiss. 1996. A Word
Graph based N-Best Search in Continuous Speech
Recognition. In Proceedings of ICLSP, Philadel-
phia, PA, USA.
94
ADAM:  An  Archi tecture for xml -based Dia logue Annotat ion  
on Mult ip le levels 
Claud ia  Sor ia  
ILC - CNR 
Pisa, Italy 
soria@ilc.pi.cnr.it 
Ro ldano  Cat ton i  
ITC-irst 
Trento, Italy 
cattoni@itc.it 
Morena  Danie l i  
CSELT 
Torino, Italy 
Morena.Danieli@cselt.it 
Abst rac t  
In this paper annotation modular- 
ity and use of annotation meta- 
schemes are identified as basic re- 
quirements for achieving actual cor- 
pora reusability. We discuss these 
concepts and the way they are 
implemented in the architectural 
framework of the ADAM corpus, 
which is a corpus of 450 Italian spon- 
taneous dialogues. The design of 
ADAM architecture is compatible 
with as many practices of dialogue 
annotation as possible, as well as ap- 
proaches to annotation at different 
levels. 
1 In t roduct ion  
In this paper we describe the methodological 
assumptions and general architectural frame- 
work underlying the ADAM Corpus, which 
is currently being developed as part of the 
Italian national project SI-TAL (Autori vari, 
2000) 1. Annotated ialogue corpora are of 
crucial importance for the development of vo- 
cal applications. Because of their cost, how- 
ever, it is essential that their acquisition and 
annotation be designed in order to maximize 
their reusability as much as possible. The 
main assumption behind the design of the 
ADAM corpus is that actual reusability of 
corpora cruciMly depends on the strategic 
choices concerning the architectural design of 
the corpus, i.e. the way in which annotation 
tThe final version of the ADAM Corpus will be 
released by the end of 2001. A pilot set, consisting of
30 dialogues, i  currently available. 
is organized and structured, and the way in 
which this is represented in a given physical 
format 2. Corpus reusability is traditionally 
seen as mainly a by-product of either corpus 
representativeness anddegree of standardiza- 
tion of annotation schemes. We claim that 
two other requirements should be taken into 
account when designing and building an anno- 
tated corpus that aims at reusability, namely 
modularity of annotation, and use of annota- 
tion meta-schemes. The meaning of these no- 
tions, as well as the way in which they are 
realized by the ADAM Corpus will be de- 
scribed in detail throughout the paper. Sec- 
tion 2 introduces the corpus; section 3 focuses 
on ADAM: the motivations underlying its ar- 
chitecture are presented and the adoption of 
XML as mark-up language is discussed. Fi- 
nally, the various annotation schemes are il- 
lustrated in section 4. 
2 Corpus  Descr ip t ion  
The ADAM Corpus, which is currently be- 
ing developed, will consist of 450 Italian 
dialogues, both human-human and human- 
machine, belonging to the tourist domain. 
The human-machine component of the corpus 
is represented by human-machine dialogues 
over the phone, which are recordings of ac- 
tual interactions occurring between customers 
and the Italian national railway information 
system (FS-Informa, (Bahia et al, 2000)). 
The human-human component is represented 
by task-oriented dialogues between a person 
playing the role of a travel agent and another 
playing the role of a customer. Each dia- 
logue in the corpus is represented by an or- 
2For a similar view see (Ide and Brew, 2000). 
9 
thographic transcription, recording the words 
uttered by the speakers plus any other non 
linguistic sound. The transcription is linked 
to an audio file in PCM format. In addi- 
tion, each dialogue is annotated according to 
five annotation levels, namely prosody, mor- 
phosyntax, syntax, semantics and pragmatics 
(see below). 
3 ADAM:  Arch i tec tura l  P r inc ip les  
3.1 Reusabi l i ty  Requ i rements  
The ADAM approach is mainly driven by the 
need of meeting the requirements of poten- 
tial users of annotated corpora, with a par- 
ticular emphasis on corpora reusability. An 
annotated corpus is reusable as far as it com- 
plies with several requirements, such as cor- 
pus representativeness and use of standard- 
ized annotation schemes (see some widely 
renoWned standardization efforts uch as EA- 
GLES, MATE, DRI, ATLAS, etc.). The 
physical format or mark-up language for cor- 
pus encoding is another crucial issue, as it 
will be argued in section 3.4. In addition to 
this, we claim that an annotated corpus is 
useful beyond the immediate particular ap- 
plication aims only to the extent o which it 
is designed so as to meet two other important 
needs. A fundamental requirement appears 
to revolve around the way annotation is orga- 
nized, structured and represented in a corpus. 
In short, it is essential that the annotation, 
i.e. the linguistic information added to the 
data, should be easily and quickly modifiable 
at a moderately ow cost by subsequent users 
of the corpus. We can think of at least two 
possible scenarios, referring to two orthogo- 
nal dimensions of customization operations. 
First, it might be the case that a user wishes 
to reuse a corpus which is annotated for sev- 
eral types of linguistic information, but lacks 
of a particular annotation type; the poten- 
tial user could nevertheless be interested in 
the existing annotations, and would like to 
supplement them with a new one. On the 
other hand, it might be the case that a user 
is interested in some annotation only (e.g., 
pos-tagging or syntactic structure) and s/he 
might want to leave aside other annotation 
types. Reusability of an annotated corpus 
can thus be thought of as a function of the 
extent o which new levels of linguistic infor- 
mation can be added, or uninteresting ones 
can be removed. This is what we call the ver- 
tical dimension of customization i annotated 
corpora. Second, for each level of linguistic 
analysis, an annotated corpus is likely to be 
reused depending on the extent o which ex- 
isting annotation can be changed, so as to ac- 
commodate different annotation practices. It 
is often the case that a corpus which is anno- 
tated with a given annotation scheme "hard- 
wires" the annotation so as it is impossible 
to replace the annotation without reverting 
to the raw text and rebuilding the annotation 
from scratch, which is enormously expensive. 
This is what we call the horizontal dimension 
off customization of an annoted corpus. 
The extent to which an annotated corpus 
can be compliant with these two requirements 
clearly depends on the architectural choices 
made at the design level: if, for instance, all 
types of annotation are flattened onto a single 
representation level, it is clear that the cus- 
tomizing operations above become hardly fea- 
sible. We claim that the vertical and horizon- 
tal customization requirements can be easily 
achieved on the one hand by appealing to the 
two related notions of modularity of annota- 
tion and use of annotation meta-schemes, and 
on the other by exploiting a physical format 
of encoding that fully supports them. In the 
next three sections we illustrate the two con- 
cepts as well as the way they are implemented 
in the ADAM Corpus. 
3.2 Annotation Modularity 
In an annotated corpus, several different 
types of annotation or linguistic information 
may be present in relation to the same in- 
put data. These types of information can be 
thought of as independent, yet related, levels 
or dimensions of linguistic description . We 
thus can think of a level of prosodic analy- 
sis, another of pos-tagging, another of seman- 
tic analysis, etc. By annotat ion  modular -  
i ty  we mean that the different layers of an- 
10 
notation are to be kept independent one of 
another. In the ADAM Corpus we provide 
five layers of annotation: prosodic, morpho- 
syntactic (pos tagging), syntactic, conceptual- 
semantic, and pragmatic. Each dialogue in 
the corpus is annotated at the above five lev- 
els of analysis; synchronization among the dif- 
ferent analyses and between these and the 
speech signal is ensured by the different anno- 
tations (stored as separate files) making ref- 
erence to the same input file. This file, con- 
taining the transcription ofthe dialogue, is in 
turn linked to the audio file in PCM (a-low 
or u-low) format. As it will be argued in sec- 
tion 3.3, support for this structure isprovided 
by the use of XML as mark-up language. By 
adopting this structure, annotation layers are 
linguistically heterogeneous and mutually or- 
thogonal, so that changing one of them af- 
fects others only to a limited extent; layers 
are nevertheless indirectly related through a) 
their hinging on a common reference file (the 
"raw" text represented by the transcription 
file); b) the indirect correlation of the lin- 
guistic information they convey. This verti- 
cal modularity of the ADAM approach has 
interesting consequences for the purposes of 
reusability. A potential user of the ADAM 
Corpus is left free to select, among the pro- 
posed levels of annotation, those which best 
reflect his/her theoretical nd practical inter- 
ests. (S)he can also feel the need for adding 
a new layer of information, ot contemplated 
in today's ADAM realization. By the way, 
level modularity is also of theoretical inter- 
est, since most annotation schemes we know 
differ mainly in the way pieces of linguistic in- 
formation categorized, rather than in the in- 
trinsic nature of these levels. Moreover, level 
modularity seems to have a useful impact on 
our theoretical understanding of the linguistic 
phenomena at stake, since it is capable of ex- 
pressing correlations between layers, and ulti- 
mately between dimensions of linguistic anal- 
ysis. 
3.3 Annotat ion meta-schemes 
Horizontal customization i annotated cor- 
pora can be enhanced by implementing the 
concept of annotation meta-schemes. The 
different layers of linguistic description ira- 
pried by the concept of annotation modular- 
ity presuppose as many annotation schemes. 
As it will be made clear in section 4, for 
each of the five annotation layers envisaged 
for the ADAM Corpus, a particular annota- 
tion scheme has been designed and applied. 
However, it should be emphasized how the 
ADAM specifications do not merely amount 
to another set of ready-made, off-the-shelf an- 
notation schemes. Rather, we would like to 
focus the attention on what we call an an- 
notation meta-scheme, and on the implica- 
tions of this choice. According to our view, 
an annotation meta-scheme is a general de- 
scriptive framework in which different annota- 
tion schemes can be accommodated. In many 
cases the same unit of linguistic information 
can be annotated in different, arguably mutu- 
ally incompatible ways, which are nonetheless 
all compatible with the recommended vertical 
modularity described above: so it is better 
to provide the potential user with the pos- 
sibility of adopting any arbitrary annotation 
scheme without being forced to re-build the 
annotation from scratch or to forcefully com- 
ply with some other annotation scheme, no 
matter how standardized. To do so, it is nec- 
essary to have a representation format for the 
annotation that is general enough for com- 
peting schemes to be mutually substitutable. 
In ADAM we achieve this aim by building 
a general scheme where those features that 
are common to several competing schemes be- 
come slots or descriptive lement ags to be 
associated with linguistic elements; the val- 
ues of these attributes can be any arbitrary 
set of tags. Let's consider, for instance, the 
case of pragmatic annotation. The main dif- 
ference between annotation schemes for this 
level of analysis lies in the particular types of 
dialogue act chosen rather than in the notion 
of dialogue act itself, which appears to be un- 
controversial. If, however, we adopt a scheme 
where the basic descriptive element of any ar- 
bitrarily long set of words is the general tag 
<dialogue act>, further described by an at- 
tribute type, different schemes can be applied 
11 
to the same corpus without totally discarding 
the existing annotation: a substitution i  the 
set of values will be enough. It is our belief 
that enforcing this practice in the design of 
annotation schemes will bring us to more ef- 
fective corpora exchange and reuse 3 
3.4 XML-based  Annotat ion  
In fact, actual corpus reusability also cru- 
cially depends on the physical \]ormat or mark- 
up language used for corpus encoding. The 
mark-up language used for the ,encoding of the 
ADAM Corpus is XML. XML proved to be 
the ideal candidate for a number of reasons, 
all related to corpus reusability. First, it is 
an emerging and widespread standard, which 
ensures a good degree of corpus reusability 
in the times to come. Second, because of 
its platform-independence it enhances the po- 
tential for wide circulation of the annotated 
material, together with a considerable flexi- 
bility of use. More crucially, however, XML 
proved essential for implementation f the ar- 
chitectural choices described above. Anno- 
tation modularity is supported via extensive 
use of Xlink elements (DeRose et al, 2000). 
Each XML element in the annotation files is 
actually an hypertextual link which refers to 
an element (or set of elements) in the tran- 
scription file. All annotations for each di- 
alogue are thus connected to the same in- 
put reference source (the transcription), thus 
ensuring synchronization of the different an- 
notations and still preserving their indepen- 
dence. On the other hand, the concept of 
annotation meta-scheme is implemented by 
making the XML translation of the different 
annotation schemes content-independent. In 
other words, a general preference was given 
towards representing the different ~nnotation 
tags as values of generic, scheme-independent 
attributes of XML elements. In this way the 
different annotation schemes (represented as 
different DTDs) are represented in a generic 
enough way, so that a future user of the cor- 
pus will only need to change the values of 
Sin addition, the meta-scheme can be seen as a 
tool for effective compariso n of alternative annotation 
schemes. 
the different attributes for the entire annota- 
tion scheme to be changed. We believe that 
this approach represents a further value of the 
ADAM Corpus. 
3.5 Prev ious  and re lated work  
Our work builds on some important standard- 
ization efforts which were going on during the 
past few years in the field of dialogue an- 
notation (DRI, EAGLES, and MATE). We 
are also indebted to the experience gained 
in other projects using stand-off XML an- 
notation, and in particular to the MATE 
project. The multi-level markup framework 
adopted in ADAM closely reflects the MATE 
approach (Dybkjaer et al, 1998). In addi- 
tion, in our project we are using the MATE 
workbench (Dybkjaer and Bernsen, 2000) for 
visualization and information extraction pur- 
poses. However, at the best of our knowl- 
edge ADAM is the first corpus being archi- 
tecturally designed by explicitly adopting the 
concept of annotation modularity and meta- 
scheme at different levels. A recent standard- 
ization project in the annotation field is con- 
stituted by the ATLAS (Bird et al, 2000) con- 
sortium, including NIST, LDC and MITRE. 
The ATLAS architecture is based on a formal 
model for annotating linguistic data (Bird 
and Liberman, 1999). ATLAS offers a three- 
layers solution to the problem of integrating 
different data storage formats by providing 
a logical level which consists of the language 
formalism and the API. The architecture we 
are proposing for the ADAM corpus is not 
a software architecture such as the one im- 
plemented by ATLAS. While the latter one 
meets the requirement of flexible and dynamic 
extension of the sofware modules, the ADAM 
architecture mainly refers to functional orga- 
nization of the different annotation layers. In 
ADAM the flexibility requirement is about 
the possible xtensions of those layers. In ad- 
dition, the ATLAS architecture covers a large 
variety of possibly annotated ata (not only 
linguistic data, but visual data of different 
kinds too), while ADAM is only focused on 
linguistic and speech annotations. 
12 
4 Leve ls  o f  Annotat ion  
The ADAM's five levels of annotation were 
mainly chosen in consideration of their inter- 
est for practical applications of the annotated 
material. In spite of the number of levels con- 
sidered, and their sometimes conflicting re- 
quirements, we tried to develop a coherent, 
unitary approach to design and application 
of annotation schemes. In particular, in de- 
veloping the different annotation schemes for 
the five levels envisaged, attention was paid 
to be consistent with criteria of robustness, 
wide coverage and compliance with existing 
standards. 
4.1 The prosodic  level 
For the annotation of the prosodic phenomena 
of dialogue we are adopting the meta-scheme 
for prosody annotation developed by Quazza 
and Garrido (Klein et al, 1998) within the 
MATE project. This meta-scheme allows to 
annotate the prosodic phenomena of natural 
dialogue by distinguishing the following four 
sub-levels of prosodic annotation: 
? phonetic transcription 
? phonetic representation f intonation 
? phonological representation f intonation 
? prosodic phrasing 
The four levels do not represent a fixed hi- 
erarchy. The two phonetic levels are directly 
aligned with the speech signal and in this 
sense may be considered as base levels. The 
two phonological levels keep a natural rela- 
tionship both with the base prosodic levels 
and with other linguistic units. In the actual 
use of the scheme, the levels and their links 
can be fully or partially specified. In a lin- 
guistic text-oriented analysis, prosody could 
be considered in its function, leaving out the 
details of its realization. In this case, the sole 
phonological levels may be filled and linked 
to the orthographic level of words. Com- 
plex schemes like ToBI could be used in this 
way, or simpler schemes providing labels to 
distinguish types of accents, associated with 
words, and types of intonation boundaries. In 
a speech technology context, a more signal- 
oriented approach could be adopted. In or- 
der to recognize or synthesize prosodic pat- 
terns, detailed phonetic descriptions are nec- 
essary, requiring both phonetic segmentation 
and phonetic representation f intonation - in 
terms of pitch movements or target f0 levels. 
The ADAM prosodic annotation is done at 
the level of the prosodic phrasing. The third 
section of Appendix B reports an excerpt 
of the prosodic annotation file of a human- 
human dialogue of the corpus. These are four 
dialogue turns whose text is translated in the 
first section of the Appendix. The element 
breakindex allows to encode the ToBI labels 
which constitute the values of the attribute 
type. For example, the brkndx_O07 is an- 
notated with type label 3p to mark-up an 
hesitation pause. 
4.2 The Morpho~Syntact ic  and 
Syntact ic  Levels 
The ADAM proposal for the morphosyntac- 
tic level is a two-layer annotation structure, 
containing respectively information on word 
category and morphosyntactic features (pos 
tagging), and non recursive phrasal nuclei 
(called chunks). Robustness and coverage 
were a crucial aspect in the development of 
the two schemes, in particular for what con- 
cerns i) syntactic onstructions specific of spo- 
ken dialogues (ellipses, anacolutha, non ver- 
bal predicative sentences etc.), and ii) dis- 
fluencies (repetitions, false starts, trailing off 
etc.). The morphosyntactic annotation level 
encodes the following information: a) iden- 
tification of morphological words and linking 
to their corresponding orthographic ounter- 
parts; b) annotation of their pos-category; c)
annotation of morphosyntactic features (such 
as number, gender, person, tense, etc.); d) an- 
notation of their corresponding lemma. The 
partic~ar tag set, though adapted to repre- 
sentation of Italian, is compliant with EA- 
GLES recommendations (Gibbon, 1999). In 
addition, the tag set is structured into a core 
scheme, supplying basic means for annotating 
morphological information, and a periphery 
tag set, which serves the purpose of making 
13 
provision for further linguistic annotation to 
be added to obligatory information. The syn- 
tactic annotation level is built on top of the 
previous one and consists in identification of 
non-recursive phrasal nuclei (called chunks) 
and annotation of their category (Mengel et 
al., 1999). The preference given to shallow 
parsing over, e.g., phrase structure trees is 
chiefly motivated by the locality of the anal- 
ysis offered by this approach, a useful feature 
if one wants to prevent a local parsing failure 
from backfiring and causing the entire parse 
of an utterance to fail. This is particularly 
desirable when dealing with particularly noisy 
and fragmented input such as spoken dialogue 
transcripts. For an illustration of morphosyn- 
tactic and syntactic annotation, see examples 
4 and 5 in Appendix B. 
4.3 The Conceptua l  Level  
The annotation scheme for the conceptual 
level has been designed on the following re- 
quirements and assumptions: 
? portabi l i ty :  although most of concepts 
encode strictly domain-dependent i for- 
mation, the annotation scheme should be 
domain-independent as much as possible; 
? expressiveness:  the scheme should al- 
low the representation of the content of 
complex dialogues; 
? minimal i ty:  a turn should be annotated 
in a unique way; 
? simplicity: the syntactical complexity 
of the concept is to be minlmized; 
? locality: the annotation should not take 
in account he history of the dialogue. 
The proposed annotation scheme takes inspi- 
ration from the so called "Frame-based De- 
scription Languages" (Cattoni and Franconi, 
1990), a well established framework in the 
field of the Knowledge Representation. In 
our annotation scheme a concept is encoded 
like a "frame", a typed structure with "slots". 
Slots represent the properties of the concept 
and its relations with Other concepts. Slots 
are encoded with the couple <slot-name, slot- 
value>: the former contains the name of a 
property, the latter either a simple value or 
a reference to another concept. This recur- 
sion allows the encoding of complex and struc- 
tured semantics information. Concepts are 
typed: different ypes of concepts (e.g. "trip", 
"room") encode different contents to be rep- 
resented. 
For example given the sentence to be an- 
notated "the train leaves from rome at eigth 
o'clock of monday fifteen", its conceptual an- 
notation is: 
<concept id="c_O01" ctype="trip"> 
<slot shame= "transport at ion-t ype" 
svalue =" train"/> 
<slot sname="origin" svalue="rome"/> 
<slot sname="departure-time" svalue="*c_O02"/> 
</concept> 
<concept id="c_O02" ctype="time"> 
<slot sname="hour" svalue="8:00"/> 
<slot sname="week-day" svalue="monday"/> 
<slot sname="month-day " svalue="md15"/> 
</concept> 
where the concept c_001 of type trip 
has tre slots; the slot representing the 
departure- t ime encodes a reference (intro- 
duced by the character '*') to the other con- 
cept c_002 of type time. 
The annotation scheme is domain indepen- 
den/: the tag set does not change when the 
domain changes ince the domain-dependent 
information is encoded in the values of the 
attributes. The user is free to adopt the pre- 
ferred ontology, although a good reference are 
the symbols adopted by the C-STAR consor- 
tium (Waibel, 1996) for the inter-lingua: they 
have been developed on the basis of the ex- 
perience on six different (Asiatic and Euro- 
pean) languages and this appears to guaran- 
tee a good portability inter-lingua. 
4.4 The Pragmat ic  Level 
For annotating the pragmatic level of dia- 
logue, we base our work on the concept of 
dialogue act. Informally speaking, a dia- 
logue act tag is a label belonging to a tag 
set which refers to a given iUocutionary di- 
mension that may be performed by uttering 
a sentence. A dialogue utterance may be an- 
notated with a dialogue act label for repre- 
senting the discourse function it plays in the 
14 
dialogue. The annotation scheme used for the 
pragmatic level of the ADAM corpus is an 
extension of both DAMSL (Core and Allen, 
1997) and SWITCHBOARD-DAMSL (Juraf- 
sky et al, 1997). The extension was not mo- 
tivated by domain, rather by the dependency 
on the dialogue type. Actually, most of dia- 
logue acts encode information that is strictly 
dependent on whether the communication is 
task-oriented, familiar, formal, and so on. So 
the inventory of dialogue acts labei should be 
sufficiently wide to cover different types of di- 
alogues, and sufficiently open to add new dia- 
logue act labels for different annotation tasks. 
For the design of the extended tag set we have 
identified the following requirements and as- 
sumptious: 
? m in ima l i ty :  an utterance should be 
tagged with an unique dialogue act label; 
? context-sensit iveness: each turn is 
managed by considering the previous 
turns, that is the annotation should take 
into account he history of the dialogue. 
The tag set used in the corpus is reported in 
Table 1 (see the Appendix). In Appendix B, 
Section 7 the four dialogue turns translated at 
the beginning of the Appendix are annotated 
by using the ADAM pragmatic tag set. Each 
dialogue turn is annotated as a whole by tag- 
ging the communicative level of the turn itself 
(if it is about the task or about managing the 
task, for example). Within the turn the differ- 
ent communicative intentions are labeled on 
the basis of the dialogue act tag set. 
5 Conc lus ions  
In this paper we have identified two ba- 
sic requirements for achieving actual cor- 
pora reusability, namely annotation modu- 
larity and use of annotation meta-schemes , 
and described how they are addressed in the 
ADAM Corpus. We claim that, for effective 
circulation and re-use of corpora, it is essen- 
tial to make provision for as many practices 
of dialogue annotation as possible, as well as 
approaches to annotation at different levels, 
instead of providing fixed levels and schemes 
of analysis, no matter how standardized. Cor- 
pora will have a chance to be reused as far as 
it will be easy and relatively inexpensive to 
adapt hem to different needs and application 
purposes. Use of XML as mark-up language 
is a further step toward this end. 
Re ferences  
Autori Vari 2000. SITAL: Manuale Operativo. 
Deliverable 1.1, Italy. 
Baggia, P., Castagneri, G. and M. Danieli. 
2000. Field Trials of the Italian ARISE Train 
Timetable System. Speech Communication, 31, 
pages 355-367. 
Bird, S. and M. Liberman. 1999. A Formal 
Framework for Linguistic Annotation. Techni- 
cal Report MS-CIS-99-01, Department ofCom- 
puter and Information Science, University of 
Pennsylvania. 
Bird, S., Day, D., Garofolo, J., Henderson, J., 
Laprun, C. and M. Liberman. 2000. ATLAS: 
A Flexible and Extensible Architecture for Lin- 
guistic Annotation. In Proceedings of LREC 
2000, Athens, Greece. 
Cattoni, R. and E. Franconi. 1990. Walking 
through the Semantics of Frame-Based Descrip- 
tion Languages: A Case Study. In Proceed- 
ings of the Fifth International Symposium IS- 
MIS '90, pages 234--241, Knoxville, TN. 
Core, M. and J. Allen. 1997. Coding Dia- 
logues with the DAMSL Annotation Scheme. 
In Working Notes of the AAAI Fall Symposium 
on Communicative Actions in Humans and Ma- 
chines, pages 28-35. Cambridge, MA. 
DeRose, S., Maler, E., Orchard, D. and B. 
Trafford. 2000. XML Linking Language 
(Xlink). W3C Working Draft, 21 February 
2000. http ://www. w3. org/TR/xlink/. 
Dybkjaer, L., Berusen, N. O., Dybkjaer, H., McK- 
elvie, D., and A. Mengel. 1998. The MATE 
Markup Framework. MATE Deliverable 1.2. 
http://mate, hiS. sdu. dk. 
Dybkjaer, L. and N.-O. Bernsen. 2000. The 
MATE Workbench. In Proceedings of LREC 
P000, Athens, Greece. 
Gibbon, D. (ed.). 1999. Handbook of Standards 
and Resources for Spoken Language Systems. 
First supplement, EAGLES LE3-4244, Spoken 
Language Working Group. 
15 
Ide, N. and C. Brew. 2000. Requirements, Tools, 
and Architectures for Annotwted Corpora. In 
Proceedings o /LREC 2000, Athens, Greece. 
Jura?sky, D., Shriberg, E. and D. Briasca. 1997. 
Switchboard DAMSL Labelhlg Project Coder's 
Manual. Technical Report 97-02, University of 
Colorado, Institute of Cognitive Science, Boul- 
der, CO. http://www, colorado, edu/ling/~ 
juraf sky/manual, august I. html. 
Klein, M., Bernsen, N. 0., Davi~, S., Dybkjaer, 
L., Garrido, J., Kasch, H., Mengel, A., Pirrelli, 
V., Poesio, M., Quazza, S. and\[ C. Sofia. 1998. 
Supported Coding Schemes. MATE Deliver- 
able i.I. http://mate .his. sdu.d.k. 
Mengel, A., Dybkjaer, L., Garrido, J., Heid, U., 
Klein, M., Pirrelli, V., Poesio, M., Quazza, S., 
Schiffrin, A. and C. Sofia. 1999. MATE Dia- 
logue Annotation Guidelines. MATE Deliver- 
able 2.1. http://mate, nis. sdu. dk. 
Waibel, A. 1996. Interactive Translation of Con- 
versational Speech. Computer, 29(7):41-48. 
Append ix  A. D ia logue-Act  Tag Set  
LABEL EXAMPLE 
Statement I 'm leaving today 
Request I 'd need a double 
room 
Accept The flight leaving at 
ten is nice for me 
Accept-Part Yes, but I 'd need an 
extra-bed for my child 
Open-Option Do you want me to 
reserve the return 
flight? 
A~ion-Dire~ive 
Repeat-Rephrase 
Collaborative-Completion 
Conventional-Opening 
Please, reserve two 
seats on the BA3476 
Oh, you said 
BA3476, the one 
leaving at I0 pm 
...and I want to leave 
from NY next Sunday 
Hello, this is the 
Tourist Information 
Desk 
Conventional-Closing Good-bye 
Backchannel/Acknowledge Y s, of course 
Backchannel/Question Is that ok? 
Or-Question Do you prefer a room 
with view on the gar- 
den or on the street? 
Apology Excuse me 
Thanking Thank you for calling 
Offer-Commit 
Yes/No-Question 
I 've to check i f  there 
is a reduced fare 
available 
Do you want to re- 
serve the return flight 
on Thursday? 
Open-Question Which company do 
you prefer to travel? 
Reject No, I don't like to 
travel with this air 
company 
Yes-Answer Yes 
No-Answer No 
Response- 
Acknowledgement 
Dispreferred-Answers 
I agree 
No, I 'd prefer to have 
a smoking room 
Opinion I believe this is the 
best solution 
Appreciation 
Abandoned/Uninterpretabh 
Suggestion 
Signal-Non-Understanding 
Signal-Understanding 
3rd-Paxty-Conversation 
I enjoyed very much 
to work with you 
I t in... 
Perhaps we could try 
with another travel 
agent 
Pardon f
I see 
Fido, stop barking, I 
can't hear a word/ 
Other You know, I 'd need to 
take a week off 
16 
Append ix  B.  Annotat ion  Examples  
1. An example of a short dialogue 
Turn  I (spk A): ~lobearo??er viag~i buongiorno 
globetrotter travel good mo~,'ning 
Turn  2,  ( spk  B) :  buon~iorno sono aDl~ama.v~a de~asperi  eeh 
vo~rei prenotare un(n) v i~ io  in Creno da(a) 1~a a 
ve~na 
good morning my name is Annarnar ia  DegoJpem ehm I 
would like to book a trip by train f rom Roma to Verona 
Turn  3~ (spk  A) :  t reno da ro~aa a verona quando? 
t ra in / rom Roma to Verona when.  ~
2. The XML Transcription File 
<?ranscrip~ion id="d~al_002" type:"hh~"> 
<~uzn ~d""t_001" vho-"A" searS-"0" 0nd="2255" 
f~le:"d~al_002_001 .p~"> 
<spk id="s_001" desc='*brea~:h" S?~="O"  end-"85"/> 
<word ~d="w_001 " S~'~:"85"  end="58~">~lobearo~ter</wo~d> 
<word ~d:"w_002" #~=* '585"  end:"915">v~aEsi</~ord> 
<word ~d="~_003" starts'*915" end:"1445">buo~iorD0</~ord> 
<spk ~d-"s_002" desc="puff" saar~="1445" end="1545"/> 
<pause id="p_001 ' s?~T~="1545" end-"1555"/> 
<spk Id-"s_O03" desc='*p~f" s~ar~="155~" e~d="2255"l> 
</turn> 
<?uzn id="?_002" vho-"B" sears="0 " end="7137" 
fi le="dial_002_002, pcm"> 
<f~l Id="~_001" sCat"L="75" end:"235">e</f~l> 
<word ~d""v_004" seal"t="235" end:"905">buon~orno</word> 
<word id="w_005 " scare="905" end="l145">sono</word> 
<voz~4 id="v_006" ~t~="1145"  end=" lS35">-~la</word> 
<word id="v_O07" s~szt:"1535" end:"2325">de~asperi</word> 
<spk id="s_004" des?="brea~h " start="2325" end="2665"/> 
<pause id="p_002" saar~-"2665" end="2675"/> 
<f~l id-"f .002" saal~="2675" end="2805">e</f~l> 
<word ~d="w.008" saart-"2805" end="3135">vorre~</vord> 
<~oz~d ~d="v_009 " staR="3135" ond-"3835'*>prenc~aro</vord> 
<voz~ id="w 010 " saaz~="3835" end*"4275">un(n)</vord> 
<pause ~d="p_003" start="4275" ~nd-"42S5"/> 
<word id="w_011" steA't:"4285" end="4675">vie, EEio</vord> 
<word ~d:"v_012" start='*4675 '* end=n4slS">in</vord> 
<word id='*v.O13'* sta~="4815" e~d:"543,5">treno</vord> 
<word ~d~"v.014 '* start="5435" *nd-"5815">da(a)</word> 
<word id="v_O~5 " s t~="5815"  end="6105'*>ro~a<l~ord> 
<word id:"w_016" s~a~="6105" snd="6135">a</word> 
<word id:"v_017" s~="6135"  end:"5785">verona</word> 
<pause ~d="p_004" staR="6785" end-"6825"/> 
<spk id~"s.005" desc="puff" stare:"6825" end="7135"/> 
</?urn> 
<?ul-n 1d="t.O03" ~ho="A" s'tax'?="O" end~"2831" 
f ~le="dial_O02.003. pcm"> 
<word id"**~_018" s ta r t : "O"  end:"555">~reno</word> 
<word id='*~_019" stars:"555" end:"895*'>da</word> 
<word ~d'"v_020" s~m="895" ond:"l195">rc~a</wor~ 
<word ~d:"w_021" s~aZ~-"1195" end:"1225">a</vord> 
<word Id="~_022" s~az~:"1225" end="1675">verona</vord> 
<vord ~d="~_023" s~art="1675" end:"2175">quando</vord> 
<spk ~d="s_006" dosc="puf~" s~ez~="2175" end="2465"/> 
<spk id="s_007" desc="pu~f" staz~:*'2465" ends"2815"/> 
<pause id:"p_OOS" stale="2815" end="2835"/> </?u~n> 
</transcript ion> 
3. The XML Prosodic Annotation File 
<prosod~cphrasln 8 id-"d~al_O02"> 
<breakAndex id-"brkndx_O01" aype="l ''
h~ef="d~al.OO2_~:a.~l~1d(v_O01)" start="585" ~nd:"585"/> 
<break,lxulex id="b=kndx_002" type:"1" 
hz~fs-~al_002_?ra.z~l~id(~_002)" searS-"915" end="915"/> 
<breaklndex id="brkndx_O03" aype-"4" 
h~ef="dial_OO2_1~ra.~m1#Id(v.O03)" st~r~="~445" end-"1445"/> 
<bz~ak~dex id="br\]mdx_O04" aype="2" 
~Tef-Odial_002_?ra.=ml#id(~_004)" sta~="905" end="905"/> 
<breakindex id-"br\]mdx_O05" type="1" 
h~ef*"d~al_002_?ra.~l~d(v_O05)*' sear't="1145" en4-"1145"/> 
<hreak/ndez id="brk~dx_O06" tTpe~"l" 
href="dla1_002_tra.~B~d(v_006)" s~r~="1535" ~="I~35"/> 
~oroaYAndex ?d="brlmdx_O07" typo="3p" 
href~"~Lia~_002_tra.~l~d(v_007)" S?~T~="2325 " e~d="232~"/> 
~breaklndox ~du"brknd~_008" aTpo="1" 
h~efe"~al.002_?ra.~iS~d(v_008)" stare-"3135" end="3135"/> 
<breakindex Id='*br~dx_O09" aTpe="l" 
b~ef-"~al.002_~ra.~1#Id(v_009)" s~az~-"3835" end="3835"/> 
<breakindex ~d="t~dx_010" ~ype-"3p" 
href="dla l .OO2_tra.xml~d(v_OlO)"  s tar t : "4275"  end="4275"/> 
? breaklndox id="brkndx_011" CTpe="l" 
h~ef="d~a1.002 ara.xm1#id(v.011)"  saar~="4675" end-"4675"/> 
<~reaXindex Sd="b~dx_012 " aype="2" 
href="d~a1_002_?ra.xml$~d(~_012)" s~az~="4815" ends"4815"/> 
<l~-eak~ndex Id="b~kndx_013" ~ype="2p" 
hrefs"d~al_002 ara.~15~d(~.013)" saart="543S" end="5435"/> 
<break~ndex ~d="brkndx_014" tTpe="lp" 
href-"d~a1_002_?ra.~l~Id(v_014)" start:"5815" end:"5815"/> 
<hreak/ndo: ~d="br~dx_015" ~Tpe="l TM 
hre~:"dial_002_?ra.~lSid(v_015)" s~a~="6105" end:"6105"/> 
<brea2Ande: id="brk=~dx_016" type="1" 
h~ef="dial_002_?ra.~l#id(v_016)" sta~-~="6135" end-"6135"/> 
~broak~=dex Id="brkndx_0/7" aTpe="4 ''
href="d~al_002_?ra.~1=Id(v_017)" san-t="6785" end-"6785"/> 
~oreaklndox id="br~dx.018"  aypo="2p" 
href-"d~al_O02 ara.x~1#~d(v_Ol8)" stars="555" ~nd-"555"/> 
<breaklndex id-"br\]mdx_Ol9" aype="2" 
hre~="d~al_OO2_?ra.~l#id(w_Ol9)" stax~-"$95" ~nd="895"/> 
<break~ndez id="br~mdx.020" aype=-l,, 
hzef-"d~al.002_?ra.~ml#~d(v.020)" staz~="1195" end:"llgS"/> 
~bre~ndox id="br~dx_021" aypeu"l" 
href=,,dial_002 ara.~1$id(v_02|)" star~="1225" end="1225"/> 
~breakindex Id="br~dx_022" Cy!~s"l" 
b1-ef=.d~a.l_OO2_?ra.~l#id(w_022)" s~art:"1675" end="1675"/> 
<break/ride= Id:"brk=~dx_023" aype="4" 
href-"d~al_OO2_?ra.~m1#~d(~_023)" saar~="2~T5" end="2175"/>' 
</phosod~ cp~rasing> 
4. The XML Morphosyntactic Annotation 
File 
~morpho log iaa1~ot  at  ion Id~"dial_O02"> 
<my id="mv_O01" i~ma="~LOBETROTTER" pos="SP" mfeats="NN" 
href=,,dial_OO2_~ra, xmlt id  (v_O01 ) ,,>globearo??ex~/mw> 
<my Id='~w_O02" Ieama=*'VIAGGZO" pos="S" mfea~s="~" 
hrefs"dlal_OO2_tra,  xmlZid(w_O02) ">vla~i</mw> 
<row id="mw_O03" 1/~a="BUONGZOR.NO" pos="I" mfea~s="Z" 
href="dia1_OO2_tra, ml#Id  (w_O03) ">buo~iorno</~v> 
<~v Id="mw.0?4" Inma="BU~GIORN0" pos="I" =~eats-"X" 
hr-ef="dial_002_?ra, mlSid(v_004) ">buongioz~o</mv> 
<my id='~J_00S" I~a="ESSERE" pos="V" mfeats="SIIP" 
href="dia~_002_?ra, zm181d(v.005) ">sono</mw> 
<my ids'~_006" ie~a="ANNAMARIA" pos="SP" mfeats="FS" 
href -"d~al.002_?ra. ~I#i d (v. 006) "> . . . . .  ~a</mv> 
<row Id='~.007" iemma~"DEGASPE~I" posm"$P" mfea~$="NN" 
h~ef=" dial.002_?ra, retold (w_007) ">deEas~er~</~> 
<row Id="mv_008" ie~a="VOLERE " pos="V " mfeats="S|DP" 
href=" d~al_002, tra. ~m1#Id (,_008) ">vozTsi</mw> 
<m~ id="mv_O09" Ie~a-"PRENOT?RE" pos-"V" ~fea~ss"F" 
h~ef="d~al_002_?ra, zml$~d(v_009) ">prenota~e</m~> 
~mv id='hnw_010" lemma="~" pos="RI" mfeats="MS" 
hre~="dla1_002_~ra. ~lSld(w_0~O) ">~m</~> 
id="m~_011" Ie~ma="VIAGGI0" pos="S" mfea~s="MS" 
href="d~a1_002.tra. ~1~d(,_011 ) ">vi~gEio</~v> 
<m~ id="mw_012 " 1~ma="IN" pos="E" mfea~s="X TM 
b~f=-d~al_002.?ra. ~l$$d(w.012) ">in</m~> 
h~f=" d/a1.002, t ra. xm1~id (-_013) ">?re~o</~> 
<row id-'~_014" 1~a="D?" pos="E" mfeats="X" 
h~ef="d~l_002_t ra. m1#id(,_014) ">da</m~> 
<m~ id='~w_015" I~a-"ROM~" pos-"SP" mfea$s="F~" 
hre~="d~al_002_?ra. ~=l~Id(,_015) ">~oma</~> 
<~v id-'~_0~6" le~a-"A" pos="E" mfeats="X" 
href -"d~al.002.t ra. ~l#Id(w_016) ">da</m~> 
<m~ Id="mv_017" le~a="1~t0N?" pos="SP" =feats="FN" 
href ="dla1_002_tra. ~l#Id (v.017) ">verona</mv> 
hze~=-dlal.002_tra, ml$1d (w_018) ">?reno</~> 
<m~ id='~v_019" l~nma="D?" pos="E" ~eats="X"  
hre~,,dla1_OO2_~ra. ~l~id(w_019)  ">da</my> 
<mY ~d:'~_020" Ie~a="ROM?" pos="SP" mfeats="~" 
h~ef:"d/al_002_tra, z~l$~d(v_020) ">~oma</~> 
id="m'~_O~l" e~a=,,A - pcs="E" m~ea~s="Z" 
h~ef-" d~al.002.tra, xmlSld(v_021) ">da<Imv> 
<my Ids"mv_022" ie~a="VEROR?" ~os="SP" ~oaSs="T~" 
hre~=-di~l.OO2_?ra. ~151d (w_022) ">verona</mv> 
<m~ ids"m~_023" lemma="~JE~D0" pos="B" infector"X" 
href =-d~al.002.?ra, x~l$1d(~_023) ">qUaDdO</~> 
</morphol s i ca l  au~ot anion> 
5. The XML Syntactic Annotation File 
<synaac~Icmmot at ion Id="dlal.O02"> 
<pn Id='~n_O01 m aypem"N" href=,,dlal.OO2.mo~.xmlSid(m~.O01)"> 
g lobetrot  t e r  
<h Id="h_O01" href-"dla1_OO2.mor.xmlSld(~v.O01)"/> </pn> 
<pn Id="pn_O02" aypo-'*N" hzef=-dial_OO2_mor.xmlSld(mv_O02)"> 
~h id-"h_O02" h~ef="d/al.OO2_mor.xml#Id(mv.O02)"/> </pn> 
17 
<pn id:"pn.003" ~yp,="INT" href:"dtal_002.mor.xmllid(mv_003)"> 
t~o~ioz~o 
<h id="h_003" href="dial.002_mor.xmll|.d(mv_003)"/> </pn> 
<~n Id:"pn_004" type="INT" hre~:"~al..002.mo~.xmllid(:~_004)"> 
buon~orno 
<h ~d*'*h_004" h~ef="dial_002_mor.zmllld(mv_004)"l> </pn> 
<pn ~d="pn_005" type:"F~" hzef~xd~al_(~2_mor.~mllid(mg_005)"> 
s~no 
<h id-"h.005" href-"d~al_OO2_mor.xmllld(m,a.O05)"l> </pn> 
<pn Id="pn_O06" ?ype:"N" hzef="d~a1.~_mor.xmllld(m~_006)'*> 
<h id:"h.006" h~e~="d~al_002_mor.xml~id(~.006)"/> </~n> 
<pn ~d:"pn.007" ~e=*'F~" href="d~al.(~2_mor.~l#~d(mv_007)"> 
degasper~ 
<h ~d'"h_007" href~"~al_002.mor.xmllJd(mv.007)"/> </pn> 
<pn $d=*'~_008" ~ype-"FV" 
href="d~al_002_m~:, xml$id (my_008).. ~.d (m~.0O9) ">vorrei prenoCaro 
<d ~d:"d.001" Cype='~oda~"-h:ef-"d~al..002.mor.xml#~d(m~.008)"/> 
<h ~d=~h_008" hx~f-"d~al_OO2_m~r.xmlli.d(mw.O09)"l> <IF
<pn ~d="~q~.009" ~ype-'*N" 
href:"d~al_002.mo=, xmll~d (my_010 ).. $ d (row.011 ) ">un v~as~io 
<h id="h_009" hzefa"d~al_002_mor.:~l#~d(m~.011)"/> </~n> 
<pn id="pn_010" Cy~e="P" 
hzof="dlal _002_too=. ::llld (:~_012).. ~ d (row_013) ">In t~r.mo 
<d Stir"d_002" Cype='~rep" hzef*"41al.?~2.mor.~znllid(m~_0$2)"/> 
<h Id-"h_010" href="dial_002.mor.zm1#Id(~_013)"/> </pn> 
<pn id-"~.011" ~ype="P" 
href-"d~al.002.mor .~ll~d(mv_014).. Id (my.015) ">da z~a 
<d td="d_003" ~Fpe-"prep" bzef="d~a1_002_mor.~l#id(mv_014)"/> 
<h id="h_011" hr*~="dlal_OO2.mor.xml#Id(m~_015)"l> <Ipn
<pn ~dz"pn.012" ?~=" I  c'" 
hzef:*'d~a/_002_mor. =~llid (~_016).. i d (m~_0~ 7 ) ">a verona 
<d id'"d_004" ~ype='*prep" href="dial_?~2_mor.~ll~d(mv_016)"/> 
<h ?d:"h_012" href*"d?al_002.mor.xmllJd(mv.0~7)"/> </p~> 
<pn Sd-"pn~013" Cype:"N" href-"dtal.0?2_mor.~lS~d(mv.018)'> 
~reno 
<h id='~_013" hzef="dial_002.mor.xml#~d(~v.018)"/> </F~> 
<pn Sd="l~t_O~4" type:"P" 
~.Tef: "d/a1_002.~or. ~mllid (~.019).. $ d (m~_020) ">da ~oma 
<d id'"d_O05" ~ype="p~ep" hzefz"~al.002_mor.~l#~d(m~.0~9)'*/> 
<h id:"h_014" hzef="d~a/_002.mo:.~llld(mv_020)'t/> </pn> 
<pn ~d="pn_015" ~ype="P" 
bzef = "Hal .  002_mot. ~=nl~id (rag_021).. ~d (~.022) ">a vez~na 
<d ?d:"d_006" ~Tpe="p~ep" href-"d~al_002_~o=.~m1:~d(:~_021)"/> 
<h ~d="h_015" href="d~a1_002_mo=.xml#$d(m~_022)"/> </pn> 
<pn id="pn_016" ~y~:+'ADV" href:"dial.O02_mor.=mll~d(~v_023)"> 
q~do 
<h ?d=*'h_016" bzef="dia1_002_mor.~l#~d(mw_023)"/> < pn> 
</synCact $ca~no~a~on> 
<pr~sat ??a~not a~ton td="d?al_002"> 
<~m ~d""~_00X" hzef-"dlal_002_~ra.xm1~?d(t_001)" 
c~mlevel:'*~ask"> 
g lobet ro t ter  v la~i  bu~nsiorno 
<dialo~ac+ ids"da_OOi" ~ype-"conv-ope" 
href :"d~al.202_~ra. xmlZSd(~_001)., id (v_003) "1> 
</turn> 
<txn~ Id:"~n_002" bxwf=*'d~al_002_zra.rm1:id(t_002)" 
co~level:"~ask"> 
buongiorno sono a ~ . a  desuper i  VOZTei ~renot~re un 
v ia~.o  in treno da x'oma a verona 
<dialogac~ id-"da.002" typ~=*'conv-o~*" 
hr~f:"d~al .~02.~ra,  xml#~d (~_004).. Id(v_0OT)*'l> 
<d~alogac~ id="da.003" ~ype="req" 
href= "dial_202_?ra. ~lS~d (~_008).. ~d(v.017)"/> 
</turn> 
<turn id="?n_003 *' href="~al_002_~ra.:~ll~d(~.003)" 
co~level:*'~ask"> 
? r~o  da r~a a verona quando 
<dL..alogac~ d:"da_O04" type="backc-ack" :rela~ion~o.:*'da_O03*' 
href="dial_202.~ra. :x::ll ?d (~_018).. id(v_022) "I> 
<dialoEac~ ?d-"da_005" ~ype="ope-ques" 
href="d~al_~O2_~ra. l l i d  (v.023) "/> 
</tur.> 
</pra~ma~ica~not at ~ 
6. The XML Conceptual-Semantic Annota- 
tion File 
< concept ualaDno~ at ion ld:"dAal_002" > 
<~n id:"~_001" href :"dia l .002_tra.~Sid(?_001)"> 
g lo~trot t~ vi~g, gi buo~o~o 
<concept id="c_001" ?~ype~"aff i l?att~"> 
<slo~ sname="value" svalue*"globe~ro~er_viaEg?"/> 
</co~cop~> 
<Itux~> 
<~urn ?d="~_002 ~ href*"dia/_002_~ra.zml#?d(~.002)'*> 
buonE?orno sono *~-~-~ia  degasperl VOZTO? px~no~ro 
tm vt~o in t~no da roma a vezona 
<?xmcept id="?_002" ctype:"perl~-n~me"> 
<slot sna~e:"given-n~e" sva lue=* '~ la" /> 
<sloe s~ame:"fa~ly-n~e" svalue="degasper?"/> 
<Ic~copt> 
<concept id="c_003" ?~ypeo"~rip"> 
<slot sn~e-'*?=ansporta~?on-?ype" svLlue-"?ratn"/> 
<Slo~ *~'*0~**  svalue="=~ae"/> 
<slO~ ~-~="des~ina~ion" svalue="verona"/> 
</concept> 
</gum> 
<turn id='~_003" hzef-"dlsl_002_~ra.~lSid(t_003)"> 
' t~o  de. :~a  a verona quando 
<c~cep~ Id*"c_004" c~pes"tz4p"> 
<s /~ sname~"oz~Igln" ~raluo-"~e"/> 
<sloe ~a~el"des~ina~on " svalu~:**verona"/> 
</coacep~> 
<c~cep~ $dm"?.005" c~ype~"~ime "> 
<slot re - "va lue"  svalue="quest$on"/> 
</c~mcept> 
</~zn> 
< / co:,cep~ualal~o~ a~ $ o:> 
7. The XML Pragmatic Annotation File 
18 
A Multi-Perspective Evaluation of the NESPOLE!
Speech-to-Speech Translation System
Alon Lavie
Carnegie Mellon University, Pittsburgh, PA, USA
alavie@cs.cmu.edu
Florian Metze
University of Karlsruhe, Germany
Roldano Cattoni
ITC-irst, Trento, Italy
Erica Costantini
University of Trieste, Trieste, Italy
Abstract
Performance and usability of real-
world speech-to-speech translation sys-
tems, like the one developed within
the Nespole! project, are aected by
several aspects that go beyond the
pure translation quality provided by
the underlying components of the sys-
tem. In this paper we describe these
aspects as perspectives along which
we have evaluated the Nespole! sys-
tem. Four main issues are investigated:
(1) assessing system performance un-
der various network trac conditions;
(2) a study on the usage and utility of
multi-modality in the context of multi-
lingual communication; (3) a compar-
ison of the features of the individual
speech recognition engines, and (4) an
end-to-end evaluation of the system.
1 Introduction
Nespole!1 is a speech-to-speech machine tran-
slation project designed to provide fully func-
tional speech-to-speech capabilities within real-
world settings of common users involved in e-
commerce applications. The project is a collab-
oration between three European research groups
1Nespole! { NEgotiation through SPOken Lan-
guage in E-commerce. See the project web-site at
http://nespole.itc.it for further details.
(IRST in Trento, Italy; ISL at Universita?t Karl-
sruhe (TH); and CLIPS at Universite Joseph
Fourier in Grenoble, France), one US research
group (ISL at Carnegie Mellon University in
Pittsburgh, PA) and two industrial partners
(APT; Trento, Italy { the Trentino provincial
tourism board, and AETHRA; Ancona, Italy {
a tele-communications company). The project is
funded jointly by the European Commission and
the US NSF. Over the past two years, we have
developed a fully functional showcase of the Ne-
spole! system within the domain of travel and
tourism, and have signicantly improved system
performance and usability based on a series of
studies and evaluations with real users. Our ex-
perience has shown that improving translation
quality is only one of several important issues
that must be addressed in achieving a practical
real-world speech-to-speech translation system.
This paper describes how we tackled these is-
sues and evaluates their eect on system per-
formance and usability. We focus on four main
issues: (1) assessing system performance under
various network trac conditions and architec-
tural congurations; (2) a study on the usage
and utility of multi-modality in the context of
multi-lingual communication; (3) a comparison
of the features of the individual speech recogni-
tion engines, and (4) an end-to-end evaluation
of the demonstration system.
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 121-128.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
2 The NESPOLE! System
The Nespole! system (Lazzari, 2001) uses a
client-server architecture to allow a common
user, who is initially browsing through the web
pages of a service provider on the Internet, to
connect seamlessly to a human agent of the ser-
vice provider who speaks another language, and
provides speech-to-speech translation service be-
tween the two parties. Standard commercially
available PC video-conferencing technology such
as Microsoft?s NetMeeting r? is used to connect
between the two parties in real-time.
In the rst showcase which we describe in this
paper, the scenario is the following: a client
is browsing through the web-pages of APT {
the tourism bureau of the province of Trentino
in Italy { in search of tour-packages in the
Trentino region. If more detailed information
is desired, the client can click on a dedicated
\button" within the web-page in order to estab-
lish a video-conferencing connection to a human
agent located at APT. The client is then pre-
sented with an interface consisting primarily of
a standard video-conferencing application win-
dow and a shared whiteboard application. Us-
ing this interface, the client can carry on a con-
versation with the agent, where the Nespole!
server provides two-way speech-to-speech trans-
lation between the parties. In the current setup,
the agent speaks Italian, while the client can
speak English, French or German.
2.1 System Architecture
The Nespole! system architecture is shown
in Figure 1. A key component in the Ne-
spole! system is the \Mediator" module,
which is responsible for mediating the commu-
nication channel between the two parties as
well as interfacing with the appropriate Human
Language Technology (HLT) speech-translation
servers. The HLT servers provide the actual
speech recognition and translation capabilities.
This system design allows for a very flexible and
distributed architecture: Mediators and HLT-
servers can be run in various physical locations,
so that the optimal conguration, given the lo-
cations of the client and the agent and antic-
Figure 1: The Nespole! System Architecture
ipated network trac, can be taken into ac-
count at any time. A well-dened API allows
the HLT servers to communicate with each other
and with the Mediator, while the HLT modules
within the servers for the dierent languages are
implemented using very dierent software pack-
ages. Further details of the design principles of
the system are described in (Lavie et al, 2001).
The computationally intensive part of speech
recognition and translation is done on dedicated
server machines, whose nature and location is
of no concern to the user. A wide range of
client-machines, even portable devices or pub-
lic information kiosks, are therefore able to run
the client software, so that the service can be
made available nearly everywhere.
The system architecture shown in Figure 1
contains two dierent types of Internet connec-
tions with dierent characteristics. The connec-
tion between Client/Agent PCs and the Media-
tor is a standard video-conferencing connection
that uses H323 and UDP protocols. In cases
of insucient network bandwidth, these proto-
cols compromise performance by allowing de-
layed or lost packets of data to be \dropped" on
the receiving side, in order to minimize delays
and ensure close to real-time performance. The
connection between the Mediator and the HLT
servers uses TCP over IP in order to achieve loss-
less communication between the Mediator and
the translation components. For practical rea-
sons, Mediator and HLT servers in our current
system usually run in separate and distant loca-
tions, which can introduce some additional time
delay. System response times in recent demon-
strations have been about three times real-time.
2.2 User interface
The user interface display is designed for
Windows r?and consists of four windows: (1)
a Microsoft r?Internet Explorer web browser;
(2) a Microsoft r?Windows NetMeeting video-
conferencing application; (3) the AeWhite-
board; and (4) the Nespole Monitor. Using
Internet Explorer, the client initiates the au-
dio and video call with an agent of the service
provider, by a simple click of a button on the
browser page. Microsoft Windows Netmeeting is
automatically opened and the audio and video
connection is established. The two additional
displays { the AeWhiteboard and the Nespole
Monitor are also launched at the same time.
Client and agent can then proceed in carrying
out a dialogue with the help of the speech trans-
lation system. For a screen snapshot of these
four displays, see (Metze et al, 2002).
We found it important to visually present as-
pects of the speech-translation process to the
end users. This is accomplished via the Ne-
spole Monitor display. Three textual represen-
tations are displayed in clearly identied elds:
(1) a transcript of their spoken input (the output
from the speech recognizer); (2) a paraphrase of
their input { the result of translating the recog-
nized input back into their own language; and
(3) the translated textual output of the utter-
ance spoken by the other party. These textual
representations provide the users with the capa-
bility to identify mis-translations and indicate
errors to the other party. A bad paraphrase is
often a good indicator of a signicant error in
the translation process. When a mis-translation
is detected, the user can press a dedicated but-
ton that informs the other party to ignore the
translation being displayed, by highlighting the
textual translation in red on the monitor display
of the other party. The user can then repeat the
turn. The current system also allows the partic-
ipants to correct speech recognition and transla-
tion errors via keyboard input, a feature which
is very eective when bandwidth limitations de-
grade the system performance.
3 Multi-Perspective Evaluations
Several dierent evaluation experiments have
been conducted, targeting dierent aspects of
our system: (1) the impact of network trac
and the consequences of real packet-loss on sys-
tem performance; (2) the impact and usability of
multi-modality; (3) a comparison of the features
of the various speech recognition engines, devel-
oped independently for dierent languages with
dierent techniques; and (4) end-to-end perfor-
mance evaluations. The data used in the evalu-
ations is part of a database collected during the
project (Burger et al, 2001).
3.1 Network Trac Impact
In our various user studies and demonstrations,
we have been forced to deal with the detrimental
eects of network congestion on the transmission
of Voice-over-IP in our system. The critical net-
work paths are the H323 connections between
the Mediator and the client and agent, which
rely on the UDP protocol in order to guaran-
tee real-time, but potentially lossy, human-to-
human communication. This can potentially be
very detrimental to the performance of speech
recognizers (Metze et al, 2001). The commu-
nication between the Mediator and HLT servers
can, in principle, be within a local network, al-
though we currently run the HLT servers at the
sites of the developing partners. This introduces
time delays, but no packet loss, due to the use
of TCP, rather than the UDP used for the H323
connections.
To quantify the influence of UDP packet-loss
on system performance, we ran a number of tests
with German client installations in the USA
(CMU at Pittsburgh) and Germany (UKA at
Karlsruhe) calling a Mediator in Italy (IRST),
which in turn contacted the German HLT server
located at UKA. The tests were conducted by
feeding a high-quality recording of the German
36
37
38
39
40
41
42
0 1 2 3 4 5 6
W
or
d 
Er
ro
r R
at
e 
(%
)
Packet Loss (%)
ITA-US
ITA-GER
Figure 2: Influence of packet loss on word accuracy of
the German Nespole! recognizer
development test-set collected at the beginning
of the project into a computer set-up for a video-
conference, i.e. we replaced the microphone by
a DAT recorder (or a computer) playing a tape,
while leaving everything else as it would be for
sessions with real subjects. In particular, seg-
mentation was based on silence detection per-
formed automatically by NetMeeting. Each test
consisted of several dialogues, lasting about an
hour. These tests (a total of more than 16 hours)
were conducted at dierent times of the day on
dierent days of the week, in an attempt to in-
vestigate a wide as possible variety of real-life
network conditions.
We were able to run 16 complete tests, re-
sulting in an average word accuracy of 60.4%,2
with single values in the 63% to 59% range for
packet-loss conditions between 0.1% and 5.2%.
The results of these tests are presented in graph-
ical from in Figure 2. On a couple of occasions
we experienced abnormally bad network condi-
tions for short periods of time. These led to a
breakdown of the Client-Mediator or Mediator-
HLT server link due to time-out conditions being
reached, or the inability to establish a connec-
tion at all. We were able, however, to record one
full test with 21.0% packet loss, which resulted
in a word accuracy of 50.3%. These dialogues
are very dicult to understand even for humans.
Our conclusion from the packet loss experi-
2The word accuracy on the clean 16kHz recording is
71.2%.
ment is that our speech recognition engine is
relatively robust to packet loss rates of up to
5%, since there is no clear degradation in the
word accuracy of the recognizer as a function
of packet loss rate (in this range). This is very
good news, since our experience indicates that
packet loss rates of over 5% are quite rare un-
der normal network trac conditions. For 20%
packet-loss, the increase in WER is signicant,
but the degradation is less severe than that re-
ported in (Milner and Semnani, 2000) on syn-
thetic data. We suspect that this is due to the
non-random distribution of lost packets.
The tests described above were the rst phase
of our research on the impact of network trac
on system performance. We are currently in the
process of conducting several further experimen-
tal investigations concerning dierent conditions
in which the system may run:
Transmission of video in addition to audio
through the video-conferencing communi-
cation channel: in this case we expect a sub-
stantial increase in UDP packet-loss rates due
to audio and video competing for the network
bandwidth over the H323 connections. It is not
clear, however, how this competition takes place
in practice and what are the resulting repercus-
sions on the audio quality (and consequently on
the recognizers? performance).
The use of low-bandwidth network con-
nections (such as standard 56Kbps
modems): This is the most common network
scenario for real client users using a home in-
stalled computer. We are currently exploring
how the bandwidth limitations in this setting
aect audio quality and system usability. In low
bandwidth conditions, NetMeeting supports en-
coding the speech with the G.723 codec, which
can consume a much lower bandwidth (less
then 6.4Kbps) compared to the G.711 codec
(64Kbps), which we currently use in our system.
We are in the process of testing the G.723 codec
within our system. Preliminary results indicate
that the recognizers used in the Nespole! sys-
tem are quite robust with respect to this new
front-end processing.
3.2 Experiments on Multi-Modality
The nature of the e-commerce scenario and ap-
plication in which our system is situated re-
quires that speech-translation be well-integrated
with additional modalities of communication
and information exchange between the agent
and client. Signicant eort has been devoted
to this issue within the project. The main
multi-modal component in the current version
of our system is the AeWhiteboard { a special
whiteboard, which allows users to share maps
and web-pages. The functionalities provided
by the AeWhiteboard include: image loading,
free-hand drawing, area selecting, color choos-
ing, scrolling the image loaded, zooming the im-
age loaded, URL opening, and Nespole! Monitor
activation. The most important feature of the
whiteboard is that each gesture performed by a
user is mirrored on the whiteboard of the other
user. Both users communicate while viewing the
same images and annotated whiteboards.
Typically, the client asks for spatial informa-
tion regarding locations, distances, and naviga-
tion directions (e.g., how to get from a hotel
to the ski slopes). By using the whiteboard,
the agent can indicate the locations and draw
routes on the map, point at areas, select items,
draw connections between dierent locations us-
ing a mouse or an optical pen, and accompany
his/her gestures with verbal explanations. Sup-
porting such combined verbal and gesture in-
teractions has required modications and exten-
sions of both HLT modules and the IF.
During July 2001, we conducted a detailed
study to evaluate the eect of multi-modality on
the communication eectiveness and usability of
our system. The goals of the experiment were
to test: (1) whether multi-modality increases
the probability of successful interaction, espe-
cially when spatial information is the focus of
the communicative exchange; (2) whether multi-
modality helps reduce mis-communications and
disfluencies; and (3) whether multi-modality
supports a faster recovery from recognition and
translation errors. For these purposes, two ex-
perimental conditions were devised: a speech-
only condition (SO), involving multilingual com-
munication and the sharing of images; and a
multi-modal condition (MM), where users could
additionally convey spatial information by pen-
based gestures on shared maps.
The setting for the experiment was the sce-
nario described earlier, involving clients search-
ing for winter tour-package information in the
Trentino province. The client?s task was to se-
lect an appropriate resort location and hotel
within the specied constraints concerning the
relevant geographical area, the available bud-
get, etc. The agent?s task was to provide the
necessary information. Novice subjects, previ-
ously unfamiliar with the system and task were
recruited to play the role of the clients. Subjects
wore a head-mounted microphone, using it in a
push-to-talk mode, and drew gestures on maps
by means of a table-pen device or a mouse. Each
subject could only hear the translated speech of
the other party (original audio was disabled in
this experiment). 28 dialogues were collected,
with 14 dialogues each for English and for Ger-
man clients, and Italian agents in all cases. Each
group contained 7 SO and 7 MM dialogues. The
dialogue transcriptions include: orthographical
transcription, annotations for spontaneous phe-
nomena and disfluencies, turn information and
annotations for gestures. Translated turns were
classied into successful, partially successful and
unsuccessful by comparing the translated turns
with the responses they generated. Repeated
turns were also counted.
The average duration of dialogues was 35 min-
utes (35.8 for SO and 35.5 for MM). On aver-
age, a dialogue contained 35 turns, 247 tokens
and 97 token types per speaker. Average val-
ues and variance of all measures are very similar
for agents and clients and across conditions and
Languages. ANOVA tests (p=0.05) on the num-
ber of turns and the number of spontaneous phe-
nomena and disfluencies, agents and customers
separately, did not produce any evidence that
modality or language aected these variables.
Hence the spoken input is homogeneous across
groups. Details on the experimental database
collected and the various statistical analyses per-
formed appear in (Costantini et al, 2002). The
analysis of the results indicated that both the
SO and MM versions of the system were eec-
tive for goal completion: 86% of the users were
able to complete the task?s goal by choosing a
hotel meeting the pre-specied budget and loca-
tion constraints.
In the MM dialogues, there were 7.6 gestures
per dialogue on average. The agents performed
almost all gestures (98%), with a clear prefer-
ence for area selections (61% of total gestures).
Most gestures (79%) followed a dialogue con-
tribution; none of the gestures were performed
during speech. Overall, few or no deictics were
used. We believe that these ndings are related
to the push-to-talk procedure and to the time
needed to transfer gestures across the network:
agents often preceded gestures with appropriate
verbal cues e.g., \I?ll show you the hotel on the
map", in order to notify the other party of an
upcoming gesture. These verbal cues indicate
that gestures were well integrated in the com-
munication.
We found signicant dierences between the
SO and MM dialogues in terms of unsuccessful
and repeated turns, particularly so in the spatial
segments of the dialogues. In the English-Italian
dialogues the MM dialogues contained 19% un-
successful turns versus 30% for the SO dialogues.
For German-Italian dialogues we found 18% in
MM versus 31% in SO. English-Italian MM dia-
logues contained 11% repeated turns versus 17%
for SO. For German-Italian dialogues repeated
turns amounted to 18% for MM versus 23% for
SO. In addition we found smoother dialogues
under MM condition, with fewer returns to al-
ready discussed topics for MM (one return every
19 turns in SO versus one return every 31 turns
in MM). MM also exhibited a lower number of
dialogue segments containing identiable misun-
derstandings between the two parties (one such
segment in each of 3 of the MM dialogues, ver-
sus a total of seven such segments in the SO dia-
logues { one dialogue with 3 segments, one with
two, and a third with a single segment of mis-
communication). Furthermore, the misunder-
standings in MM conditions were often immedi-
ately solved by resorting to MM resources, while
in case of SO ambiguous or mis-understood sub-
dialogues often remained unresolved. Finally,
the experiment subjects, given the choice be-
tween the MM and the SO system, expressed
a clear preference for the former. In summary,
we found strong supporting evidence that mul-
timodality has a positive eect on the quality
of interaction by reducing ambiguity, making it
easier to resolve ambiguous utterances and to re-
cover from system errors, improving the flow of
the dialogue, and enhancing the mutual compre-
hension between the parties, in particular when
spatial information is involved.
3.3 Features of Automatic Speech
Recognition Engines
The Speech Recognition modules of the Nespo-
le! system were developed separately at the dif-
ferent participating sites, using dierent toolk-
its, but communicate with the Mediator using
a standardized interface. The French and Ger-
man ASR modules are described in more detail
in (Vaufreydaz et al, 2001; Metze et al, 2001).
The German engine was derived from the UKA
recognizer developed for the German Verbmobil
Task (Soltau et al, 2001).
All systems were derived from existing
LVCSR recognizers and adapted to the Nespo-
le! task using less than 2 hours of adaptation
data. This data was collected during an initial
user-study, in which clients from all countries
communicated with an APT agent fluent in their
mother tongue through the Nespole! system,
but without recognition and translation compo-
nents in place. Segmentation of input speech is
done based on automatic silence detection per-
formed by NetMeeting at the site of the origi-
nating audio. The audio is encoded according
to the G.711 standard at a sampling frequency
of 8kHz. The characteristics of the dierent rec-
ognizers are summarized in Table 1. The word
accuracy rates of the recognizers are presented
in Section 3.4.
3.4 End-to-End System Evaluation
In December 2001, we conducted a large scale
multi-lingual end-to-end translation evaluation
of the Nespole! rst-showcase system. For
each of the three language pairs (English-Italian,
German-Italian and French-Italian), four previ-
English French German Italian
Vocabulary size 8,000 20,000 12,000 4,000
OOV rate 0.3% <1% 3.0%
LM training Verbmobil (E), C-Star Internet Verbmobil (D) C-Star
Data 550k words 1,500M words 500k words 100k words
+ adaptation Nespole none Nespole Nespole
Perplexity 33 98 150
Microphone type head-set head-set table-top head-set
Speaking style spontaneous read spontaneous read
Ac. training 16kHz G711 recoded 16kHz G711 recoded
Data 90h 12h 65h Verbmobil-II 11h C-Star
+ adaptation Up-sampling of G711 MLLR 80min. + FSA
Real-time factor 2.5, 1GHz P-III 1.1, 1GHz P-III 1.8, 650Mhz P-III
Memory consumption 280Mb 200Mb 100Mb 100Mb
WER on clean data 19.9% 28% 29.8% 31.5%
Table 1: Features of the Speech Recognition Engines
Language WARs SR Graded (% Acc)
English 61.9% 66.0%
German 63.5% 68.0%
French 71.2% 65.0%
Italian 76.5% 70.6%
Table 2: Speech Recognition Word Accuracy Rates and
Results of Human Grading (Percent Acceptable) of Recog-
nition Output as a Paraphrase
Language Transcribed Speech Rec.
English-to-English 58% 45%
German-to-German 46% 40%
French-to-French 54% 41%
Italian-to-Italian 61% 48%
Table 3: Monolingual End-to-End Translation Results
(Percent Acceptable) on Transcribed and Speech Recog-
nized Input
ously unseen test dialogues were used to evaluate
the performance of the translation system. The
dialogues included two scenarios: one covering
winter ski vacations, the other about summer
resorts. One or two of the dialogues for each lan-
guage contained multi-modal expressions. The
test data included a mixture of dialogues that
were collected mono-lingually prior to system
development (both client and agent spoke the
same language), and data collected bilingually
(during the July 2001 MM experiment), using
the actual translation system. This mixture of
data conditions was intended primarily for com-
prehensiveness and not for comparison of the dif-
ferent conditions.
We performed an extensive suite of evalua-
Language Transcribed Speech Rec.
English-to-Italian 55% 43%
German-to-Italian 32% 27%
French-to-Italian 44% 34%
Italian-to-English 47% 37%
Italian-to-German 47% 31%
Italian-to-French 40% 27%
Table 4: Cross-lingual End-to-End Translation Results
(Percent Acceptable) on Transcribed and Speech Recog-
nized Input
tions on the above data. The evaluations were
all end-to-end, from input to output, not as-
sessing individual modules or components. We
performed both mono-lingual evaluation (where
generated output language was the same as the
input language), as well as cross-lingual evalu-
ation. For cross-lingual evaluations, translation
from English German and French to Italian was
evaluated on client utterances, and translation
from Italian to each of the three languages was
evaluated on agent utterances. We evaluated on
both manually transcribed input as well as on
actual speech-recognition of the original audio.
We also graded the speech recognized output as
a \paraphrase" of the transcriptions, to measure
the levels of semantic loss of information due
to recognition errors. Speech recognition word
accuracies and the results of speech graded as
a paraphrase appear in Table 2. Translations
were graded by multiple human graders at the
level of Semantic Dialogue Units (SDUs). For
each data set, one grader rst manually seg-
mented each utterance into SDUs. All graders
then used this segmentation in order to assign
scores for each SDU present in the utterance.
We followed the three-point grading scheme pre-
viously developed for the C-STAR consortium,
as described in (Levin et al, 2000). Each SDU is
graded as either \Perfect" (meaning translated
correctly and output is fluent), \OK" (meaning
is translated reasonably correct but output may
be disfluent), or \Bad" (meaning not properly
translated). We calculate the percent of SDUs
that are graded with each of the above cate-
gories. \Perfect" and \OK" percentages are also
summed together into a category of \Accept-
able" translations. Average percentages are cal-
culated for each dialogue, each grader, and sep-
arately for client and agent utterances. We then
calculated combined averages for all graders and
for all dialogues for each language pair.
Table 3 shows the results of the monolingual
end-to-end translation for the four languages,
and Table 4 shows the results of the cross-
lingual evaluations. The results indicate accept-
able translations in the range of 27{43% of SDUs
(interlingua units) with speech recognized in-
puts. While this level of translation accuracy
cannot be considered impressive, our user stud-
ies and system demonstrations indicate that it is
already sucient for achieving eective commu-
nication with real users. We expect performance
levels to reach a range of 60{70% within the next
year of the project.
Acknowledgements
Additional Authors: S. Burger, D. Gates, C.
Langley, K. Laskowski, L. Levin, K. Peterson, T.
Schultz, A. Waibel, D. Wallace, Carnegie Mel-
lon University; J. McDonough, H. Soltau, Uni-
versity of Karlsruhe, Germany; G. Lazzari, N.
Manna, F. Pianesi, E. Pianta, ITC-irst, Trento,
Italy; L. Besacier, H. Blanchon, D. Vaufreydaz,
Universite Joseph Fourier, Grenoble, France; L.
Taddei, AETHRA, Ancona, Italy.
This work was supported by NSF Grant
9982227 and EU Grant IST 1999-11562 as part
of the joint EU/NSF MLIAM research initiative.
References
Susanne Burger, Laurent Besacier, Paolo Coletti,
Florian Metze, and Celine Morel. 2001. The NE-
SPOLE! VoIP Dialogue Database. In Proc. Eu-
roSpeech 2001, Aalborg, Denmark. ISCA.
Erica Costantini, Susanne Burger, and Fabio Pianesi.
2002. Nespole!?s multilingual and multimodal cor-
pus. In Proceedings of the Third International
Conference on Language Resources and Evalua-
tion (LREC-2002), Grand Canary Island, Spain,
June. To appear.
Alon Lavie, Fabio Pianesi, and al. 2001. Architec-
ture and Design Considerations in NESPOLE!: a
Speech Translation System for E-Commerce Ap-
plications. In Proc. of the HLT2001, San Diego,
CA. ACM.
Gianni Lazzari. 2001. Spoken translation: chal-
lenges and opportunities. In Proc. ICSLP 2001,
Beijing, China, 10.
Lori Levin, Donna Gates, Fabio Pianesi, Donna Wal-
lace, Takeshi Watanabe, and Monika Woszczyna.
2000. Evaluation of a Practical Interlingua for
Task-Oriented Dialogues. In Proceedings NAACL-
2000 Workshop On Interlinguas and Interlingual
Approaches, Seattle, WA. AMTA.
Florian Metze, John McDonough, and Hagen Soltau.
2001. Speech Recognition over NetMeeting Con-
nections. In Proc. EuroSpeech 2001, Aalborg,
Denmark. ISCA.
Florian Metze, John McDonough, Hagen Soltau,
Alex Waibel, Alon Lavie, Susan Burger, Chad
Langley, Kornel Laskowski, Lori Levin, Tanja
Schultz, Fabio Pianesi, Roldano Cattoni, Gianni
Lazzari, Nadia Mana, Emanuele Pianta, Laurent
Besacier, Herve Blanchon, Dominique Vaufreydaz,
and Loredana Taddei. 2002. The NESPOLE!
Speech-to-Speech Translation System. In Proc.
HLT 2002, San Diego, CA, 3.
Ben Milner and Sharam Semnani. 2000. Robust
Speech Recognition over IP Networks. In Pro-
ceedings of International Conference on Acoustics
Speech and Signal Processing (ICASSP-00), Istan-
bul, Turkey, June.
Hagen Soltau, Thomas Schaaf, Florian Metze, and
Alex Waibel. 2001. The ISL Evaluation System
for Verbmobil - II. In Proc. ICASSP 2001, Salt
Lake City, USA, 5.
D. Vaufreydaz, L. Besacier, C. Bergamini, and
R. Lamy. 2001. presented at ISCA ITRW Work-
shop on Adaptation Methods for Speech Recogni-
tion, August. Sophia-Antipolis, France.
