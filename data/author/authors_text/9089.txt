Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 181?184,
New York, June 2006. c?2006 Association for Computational Linguistics
Improved Affinity Graph Based Multi-Document Summarization 
Xiaojun Wan, Jianwu Yang 
Institute of Computer Science and Technology, Peking University 
Beijing 100871, China 
{wanxiaojun, yangjianwu}@icst.pku.edu.cn 
Abstract
This paper describes an affinity graph 
based approach to multi-document sum-
marization. We incorporate a diffusion 
process to acquire semantic relationships 
between sentences, and then compute in-
formation richness of sentences by a 
graph rank algorithm on differentiated in-
tra-document links and inter-document 
links between sentences. A greedy algo-
rithm is employed to impose diversity 
penalty on sentences and the sentences 
with both high information richness and 
high information novelty are chosen into 
the summary. Experimental results on 
task 2 of DUC 2002 and task 2 of DUC 
2004 demonstrate that the proposed ap-
proach outperforms existing state-of-the-
art systems. 
1 Introduction 
Automated multi-document summarization has 
drawn much attention in recent years. Multi-
document summary is usually used to provide con-
cise topic description about a cluster of documents 
and facilitate the users to browse the document 
cluster. A particular challenge for multi-document 
summarization is that the information stored in 
different documents inevitably overlaps with each 
other, and hence we need effective summarization 
methods to merge information stored in different 
documents, and if possible, contrast their differ-
ences.
A variety of multi-document summarization 
methods have been developed recently. In this 
study, we focus on extractive summarization, 
which involves assigning saliency scores to some 
units (e.g. sentences, paragraphs) of the documents 
and extracting tKe sentences with highest scores. 
MEAD is an implementation of the centroid-based 
method (Radev et al, 2004) that scores sentences 
based on sentence-level and inter-sentence features, 
including cluster centroids, position, TF*IDF, etc. 
NeATS (Lin and Hovy, 2002) selects important 
content using Ventence position, term frequency, 
topic signature and term clustering, and then uses 
MMR (Goldstein et al, 1999) to remove redun-
dancy. XDoX (Hardy et al, 1998) identifies the 
most salient themes within the set by passage clus-
tering and then composes an extraction summary, 
which reflects these main themes. Harabagiu and 
Lacatusu (2005) investigate different topic repre-
sentations and extraction methods.
Graph-based methods have been proposed to 
rank sentences or passages. Websumm (Mani and 
Bloedorn, 2000) uses a graph-connectivity model 
and operates under the assumption that nodes 
which are connected to many other nodes are likely 
to carry salient information. LexPageRank (Erkan 
and Radev, 2004) is an approach for computing 
sentence importance based on the concept of ei-
genvector centrality. Mihalcea and Tarau (2005) 
also propose similar algorithms based on PageR-
ank and HITS to compute sentence importance for 
document summarization.  
In this study, we extend the above graph-based 
works by proposing an integrated framework for 
considering both information richness and infor-
mation novelty of a sentence based on sentence 
affinity graph. First, a diffusion process is imposed 
on sentence affinity graph in order to make the af-
finity graph reflect true semantic relationships be-
tween sentences. Second, intra-document links and 
inter-document links between sentences are differ-
entiated to attach more importance to inter-
document links for sentence information richness 
computation. Lastly, a diversity penalty process is 
imposed on sentences to penalize redundant sen-
tences. Experiments on DUC 2002 and DUC 2004 
data are performed and we obtain encouraging re-
sults and conclusions. 
181
2 The Affinity Graph Based Approach 
The proposed affinity graph based summarization
method consists of three steps: (1) an affinity graph
is built to reflect the semantic relationship between
sentences in the document set; (2) information
richness of each sentence is computed based on the
affinity graph; (3) based on the affinity graph and 
the information richness scores, diversity penalty is 
imposed to sentences and the affinity rank score 
for each sentence is obtained to reflect both infor-
mation richness and information novelty of the 
sentence. The sentences with high affinity rank 
scores are chosen to produce the summary.
2.1 Affinity Graph Building
Given a sentence collection S={si | 1?i?n}, the af-
finity weight aff(si, sj) between a sentence pair of si
and sj  is calculated using the cosine measure. The 
weight associated with term t is calculated with the
tft*isft formula, where tft is the frequency of term t
in the corresponding sentence and isft is the inverse 
sentence frequency of term t, i.e. 1+log(N/nt),
where N is the total number of sentences and nt is 
the number of sentences containing term t.  If sen-
tences are considered as nodes, the sentence collec-
tion can be modeled as an undirected graph by
generating the link between two sentences if their
affinity weight exceeds 0, i.e. an undirected link
between si and sj (i?j) with affinity weight aff(si,sj)
is constructed if aff(si,sj)>0; otherwise no link is 
constructed. Thus, we construct an undirected
graph G reflecting the semantic relationship be-
tween sentences by their content similarity. The
graph is called as Affinity Graph. We use an adja-
cency (affinity) matrix M to describe the affinity
graph with each entry corresponding to the weight 
of a link in the graph. M = (Mi,j)n?nis defined as 
follows:
)s,s(affM jij,i  (1)
Then M is normalized to make the sum of each
row equal to 1. Note that we use the same notation 
to denote a matrix and its normalized matrix.
However, the affinity weight between two sen-
tences in the affinity graph is currently computed
simply based on their own content similarity and 
ignore the affinity diffusion process on the graph.
Other than the direct link between two sentences,
the possible paths with more than two steps be-
tween the sentences in the graph also convey more
or less semantic relationship. In order to acquire 
the implicit semantic relationship between sen-
tences, we apply a diffusion process Kandola et 
al., 2002 on the graph to obtain a more appropri-
ate affinity matrix. Though the number of possible 
paths between any two given nodes can grow ex-
ponentially, recent spectral graph theory (Kondor
and Lafferty, 2002) shows that it is possible to
compute the affinity between any two given nodes
efficiently without examining all possible paths. 
The diffusion process on the graph is as follows: 
t1t
1t
~ MM
-?f  J
(2)
where ?(0<?<1) is the decay factor set to 0.9. 
is the t-th power of the initial affinity matrix
and the entry in it is given by
tM
M
? ?
  
?

 

 
ju,iu
n}{1,...,u
1t
1
u,u
t
ji
t1
t
1
MM
"
"",
(3)
that is the sum of the products of the weights over 
all paths of length t that start at node i and finish at 
node j in the graph on the examples. If the entries 
satisfy that they are all positive and for each node 
the sum of the connections is 1, we can view the 
entry as the probability that a random walk begin-
ning at node i reaches node j after t steps.  The ma-
trix M is normalized to make the sum of each row
equal to 1. t is limited to 5 in this study.
~
2.2 Information Richness Computation 
The computation of information richness of sen-
tences is based on the following three intuitions: 1) 
the more neighbors a sentence has, the more in-
formative it is; 2) the more informative a sen-
tence?s neighbors are, the more informative it is; 3) 
the more heavily a sentence is linked with other
informative sentences, the more informative it is.
Based on the above intuitions, the information
richness score InfoRich(si) for a sentence si can be
deduced from those of all other sentences linked
with it and it can be formulated in a recursive form
as follows: 
?
z
?? 
ijall
i,jji n
)d1(M~)s(InfoRichd)s(InfoRich (4)
And the matrix form is: 
e
n
)d1(~d T &
&&  OO M (5)
182
where 1ni )]s(InfoRich[ u O
&
is the eigenvector of 
. is a unit vector with all elements equaling 
to 1. d is the damping factor set to 0.85.
T~M e&
Note that given a link between a sentence pair of 
si and sj, if si and sj comes from the same document,
the link is an intra-document link; and if si and sj
comes from different documents, the link is an in-
ter-document link. We believe that inter-document
links are more important than intra-document links 
for information richness computationDifferent
weights are assigned to intra-document links and 
inter-document links respectively, and the new af-
finity matrix is:
interintra
~~? MMM ED  (6)
where intra
~M is the affinity matrix containing only
the intra-document links (the entries of inter-
document links are set to 0) and inter
~M is the affin-
ity matrix containing only the inter-document links 
(the entries of intra-document links are set to 0). ?,
? are weighting parameters and we let 0??, ??1.
7he matrix is normalized and now the matrix  is 
replaced by  in Equations (4) and (5). 
M~
M?
2.3 Diversity Penalty Imposition 
Based on the affinity graph and obtained informa-
tion richness scores, a greedy algorithm is applied
to impose the diversity penalty and compute the
final affinity rank scores of sentences as follows: 
1. Initialize two sets A=?, B={si | i=1,2,?,n}, and
each sentence?s affinity rank score is initialized to 
its information richness score, i.e. ARScore(si) = 
InfoRich(si), i=1,2,?n.
2. Sort the sentences in B by their current affinity rank
scores in descending order.
3. Suppose si is the highest ranked sentence, i.e. the
first sentence in the ranked list. Move sentence si
from B to A, and then a diversity penalty is im-
posed to the affinity rank score of each sentence
linked with si as follows:
For each sentence sj  in B, we have
)InfoRich(sM~?)ARScore(s)ARScore(s iij,jj ?? (7)
where ?>0 is the penalty degree factor. The larger
? is, the greater penalty is imposed to the affinity
rank score. If ?=0, no diversity penalty is imposed
at all. 
4. Go to step 2 and iterate until B= ? or the iteration
count reaches a predefined maximum number.
After the affinity rank scores are obtained for all
sentences, the sentences with highest affinity rank 
scores are chosen to produce the summary accord-
ing to the summary length limit.
3 Experiments and Results
We compare our system with top 3 performing
systems and two baseline systems on task 2 of 
DUC 2002 and task 4 of DUC 2004 respectively.
ROUGE (Lin and Hovy, 2003) metrics is used for
evaluation1 and we mainly concern about ROUGE-
1. The parameters of our system are tuned on DUC 
2001 as follows: ?=7, ?=0.3 and ?=1.
We can see from the tables that our system out-
performs the top performing systems and baseline 
systems on both DUC 2002 and DUC 2004 tasks 
over all three metrics. The performance improve-
ment achieved by our system results from three
factors: diversity penalty imposition, intra-
document and inter-document link differentiation
and diffusion process incorporation. The ROUGE-
1 contributions of the above three factors are 
0.02200, 0.00268 and 0.00043 respectively.
System ROUGE-1 ROUGE-2 ROUGE-W
Our System 0.38125 0.08196 0.12390
S26 0.35151 0.07642 0.11448
S19 0.34504 0.07936 0.11332
S28 0.34355 0.07521 0.10956
Coverage Baseline 0.32894 0.07148 0.10847
Lead Baseline 0.28684 0.05283 0.09525
Table 1. System comparison on task 2 of DUC 2002
System ROUGE-1 ROUGE-2 ROUGE-W
Our System 0.41102 0.09738 0.12560
S65 0.38232 0.09219 0.11528
S104 0.37436 0.08544 0.11305
S35 0.37427 0.08364 0.11561
Coverage Baseline 0.34882 0.07189 0.10622
Lead Baseline 0.32420 0.06409 0.09905
Table 2. System comparison on task 2 of DUC 2004
Figures 1-4 show the influence of the parameters 
in our system. Note that ?: ? denotes the real val-
ues ? and ? are set to. ?w/ diffusion? is the system
with the diffusion process (our system) and  ?w/o
diffusion? is the system without the diffusion proc-
1 We use ROUGEeval-1.4.2 with ?-l? or ?-b? option for trun-
cating longer summaries, and ?-m? option for word stemming. 
183
ess. The observations demonstrate that ?w/ diffu-
sion? performs better than ?w/o diffusion? for most
parameter settings. Meanwhile, ?w/ diffusion? is
more robust than ?w/o diffusion? because the 
ROUGE-1 value of ?w/ diffusion? changes less
when the parameter values vary. Note that in Fig-
ures 3 and 4 the performance decreases sharply 
with the decrease of the weight ? of inter-
document links and it is the worst case when inter-
document links are not taken into account (i.e. ?:
?=1:0), while if intra-document links are not taken 
into account (i.e. ?:?=0:1), the performance is still
good, which demonstrates the great importance of 
inter-document links. 



Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552?559,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Towards an Iterative Reinforcement Approach for Simultaneous 
Document Summarization and Keyword Extraction 
Xiaojun Wan                     Jianwu Yang                     Jianguo Xiao 
Institute of Computer Science and Technology  
Peking University, Beijing 100871, China 
{wanxiaojun,yangjianwu,xiaojianguo}@icst.pku.edu.cn
Abstract 
Though both document summarization and 
keyword extraction aim to extract concise 
representations from documents, these two 
tasks have usually been investigated inde-
pendently. This paper proposes a novel it-
erative reinforcement approach to simulta-
neously extracting summary and keywords 
from single document under the assump-
tion that the summary and keywords of a 
document can be mutually boosted. The 
approach can naturally make full use of the 
reinforcement between sentences and key-
words by fusing three kinds of relation-
ships between sentences and words, either 
homogeneous or heterogeneous. Experi-
mental results show the effectiveness of the 
proposed approach for both tasks. The cor-
pus-based approach is validated to work 
almost as well as the knowledge-based ap-
proach for computing word semantics.  
1 Introduction 
Text summarization is the process of creating a 
compressed version of a given document that de-
livers the main topic of the document. Keyword 
extraction is the process of extracting a few salient 
words (or phrases) from a given text and using the 
words to represent the text. The two tasks are simi-
lar in essence because they both aim to extract 
concise representations for documents. Automatic 
text summarization and keyword extraction have 
drawn much attention for a long time because they 
both are very important for many text applications, 
including document retrieval, document clustering, 
etc.  For example, keywords of a document can be 
used for document indexing and thus benefit to 
improve the performance of document retrieval, 
and document summary can help to facilitate users 
to browse the search results and improve users? 
search experience.  
Text summaries and keywords can be either 
query-relevant or generic. Generic summary and 
keyword should reflect the main topics of the 
document without any additional clues and prior 
knowledge. In this paper, we focus on generic 
document summarization and keyword extraction 
for single documents. 
Document summarization and keyword extrac-
tion have been widely explored in the natural lan-
guage processing and information retrieval com-
munities. A series of workshops and conferences 
on automatic text summarization (e.g. SUMMAC, 
DUC and NTCIR) have advanced the technology 
and produced a couple of experimental online sys-
tems. In recent years, graph-based ranking algo-
rithms have been successfully used for document 
summarization (Mihalcea and Tarau, 2004, 2005; 
ErKan and Radev, 2004) and keyword extraction 
(Mihalcea and Tarau, 2004). Such algorithms make 
use of ?voting? or ?recommendations? between 
sentences (or words) to extract sentences (or key-
words). Though the two tasks essentially share 
much in common, most algorithms have been de-
veloped particularly for either document summari-
zation or keyword extraction.  
Zha (2002) proposes a method for simultaneous 
keyphrase extraction and text summarization by 
using only the heterogeneous sentence-to-word 
relationships. Inspired by this, we aim to take into 
account all the three kinds of relationships among 
sentences and words (i.e. the homogeneous rela-
tionships between words, the homogeneous rela-
tionships between sentences, and the heterogene-
ous relationships between words and sentences) in 
552
a unified framework for both document summari-
zation and keyword extraction. The importance of 
a sentence (word) is determined by both the impor-
tance of related sentences (words) and the impor-
tance of related words (sentences). The proposed 
approach can be considered as a generalized form 
of previous graph-based ranking algorithms and 
Zha?s work (Zha, 2002).  
In this study, we propose an iterative reinforce-
ment approach to realize the above idea. The pro-
posed approach is evaluated on the DUC2002 
dataset and the results demonstrate its effectiveness 
for both document summarization and keyword 
extraction. Both knowledge-based approach and 
corpus-based approach have been investigated to 
compute word semantics and they both perform 
very well.  
The rest of this paper is organized as follows: 
Section 2 introduces related works. The details of 
the proposed approach are described in Section 3. 
Section 4 presents and discusses the evaluation 
results. Lastly we conclude our paper in Section 5. 
2 Related Works 
2.1 Document Summarization 
Generally speaking, single document summariza-
tion methods can be either extraction-based or ab-
straction-based and we focus on extraction-based 
methods in this study. 
Extraction-based methods usually assign a sali-
ency score to each sentence and then rank the sen-
tences in the document. The scores are usually 
computed based on a combination of statistical and 
linguistic features, including term frequency, sen-
tence position, cue words, stigma words, topic sig-
nature (Hovy and Lin, 1997; Lin and Hovy, 2000), 
etc. Machine learning methods have also been em-
ployed to extract sentences, including unsupervised 
methods (Nomoto and Matsumoto, 2001) and su-
pervised methods (Kupiec et al, 1995; Conroy and 
O?Leary, 2001; Amini and Gallinari, 2002; Shen et 
al., 2007). Other methods include maximal mar-
ginal relevance (MMR) (Carbonell and Goldstein, 
1998), latent semantic analysis (LSA) (Gong and 
Liu, 2001). In Zha (2002), the mutual reinforce-
ment principle is employed to iteratively extract 
key phrases and sentences from a document.   
Most recently, graph-based ranking methods, in-
cluding TextRank ((Mihalcea and Tarau, 2004, 
2005) and LexPageRank (ErKan and Radev, 2004) 
have been proposed for document summarization. 
Similar to Kleinberg?s HITS algorithm (Kleinberg, 
1999) or Google?s PageRank (Brin and Page, 
1998), these methods first build a graph based on 
the similarity between sentences in a document and 
then the importance of a sentence is determined by 
taking into account global information on the 
graph recursively, rather than relying only on local 
sentence-specific information. 
2.2 Keyword Extraction 
Keyword (or keyphrase) extraction usually in-
volves assigning a saliency score to each candidate 
keyword by considering various features. Krulwich 
and Burkey (1996) use heuristics to extract key-
phrases from a document. The heuristics are based 
on syntactic clues, such as the use of italics, the 
presence of phrases in section headers, and the use 
of acronyms. Mu?oz (1996) uses an unsupervised 
learning algorithm to discover two-word key-
phrases. The algorithm is based on Adaptive Reso-
nance Theory (ART) neural networks. Steier and 
Belew (1993) use the mutual information statistics 
to discover two-word keyphrases. 
Supervised machine learning algorithms have 
been proposed to classify a candidate phrase into 
either keyphrase or not. GenEx (Turney, 2000) and 
Kea (Frank et al, 1999; Witten et al, 1999) are 
two typical systems, and the most important fea-
tures for classifying a candidate phrase are the fre-
quency and location of the phrase in the document. 
More linguistic knowledge (such as syntactic fea-
tures) has been explored by Hulth (2003). More 
recently, Mihalcea and Tarau (2004) propose the 
TextRank model to rank keywords based on the 
co-occurrence links between words. 
3 Iterative Reinforcement Approach 
3.1 Overview 
The proposed approach is intuitively based on the 
following assumptions: 
Assumption 1: A sentence should be salient if it 
is heavily linked with other salient sentences, and a 
word should be salient if it is heavily linked with 
other salient words. 
Assumption 2: A sentence should be salient if it 
contains many salient words, and a word should be 
salient if it appears in many salient sentences. 
The first assumption is similar to PageRank 
which makes use of mutual ?recommendations? 
553
between homogeneous objects to rank objects. The 
second assumption is similar to HITS if words and 
sentences are considered as authorities and hubs 
respectively. In other words, the proposed ap-
proach aims to fuse the ideas of PageRank and 
HITS in a unified framework.  
In more detail, given the heterogeneous data 
points of sentences and words, the following three 
kinds of relationships are fused in the proposed 
approach: 
SS-Relationship: It reflects the homogeneous 
relationships between sentences, usually computed 
by their content similarity. 
WW-Relationship: It reflects the homogeneous 
relationships between words, usually computed by 
knowledge-based approach or corpus-based ap-
proach. 
SW-Relationship: It reflects the heterogeneous 
relationships between sentences and words, usually 
computed as the relative importance of a word in a 
sentence. 
Figure 1 gives an illustration of the relationships.  
 
Figure 1. Illustration of the Relationships 
 
The proposed approach first builds three graphs 
to reflect the above relationships respectively, and 
then iteratively computes the saliency scores of the 
sentences and words based on the graphs. Finally, 
the algorithm converges and each sentence or word 
gets its saliency score. The sentences with high 
saliency scores are chosen into the summary, and 
the words with high saliency scores are combined 
to produce the keywords. 
3.2 Graph Building 
3.2.1  Sentence-to-Sentence Graph ( SS-Graph)  
Given the sentence collection S={si | 1IiIm} of a 
document,  if each sentence is considered as a node, 
the sentence collection can be modeled as an undi-
rected graph by generating an edge between two 
sentences if their content similarity exceeds 0, i.e. 
an undirected link between si and sj (iKj) is con-
structed and the associated weight is their content 
similarity. Thus, we construct an undirected graph 
GSS to reflect the homogeneous relationship be-
tween sentences. The content similarity between 
two sentences is computed with the cosine measure. 
We use an adjacency matrix U to describe GSS with 
each entry corresponding to the weight of a link in 
the graph. U= [Uij]m?m is defined as follows: 



?

?
otherwise,
j, if iss
ss
U ji
ji
ij
0
rr
rr
(1) 
where is and js
r are the corresponding term vec-
tors of sentences si and sj respectively. The weight 
associated with term t is calculated with tft.isft,
where tft is the frequency of term t in the sentence 
and isft is the inverse sentence frequency of term t,
i.e. 1+log(N/nt), where N is the total number of 
sentences and nt is the number of sentences con-
taining term t in a background corpus. Note that 
other measures (e.g. Jaccard, Dice, Overlap, etc.) 
can also be explored to compute the content simi-
larity between sentences, and we simply choose the 
cosine measure in this study. 
Then U is normalized to U~ as follows to make 
the sum of each row equal to 1: 


 ?
erwise , oth 
U, if UUU
m
j
ij
m
j
ijij
ij
0
0~
11 (2) 
3.2.2  Word-to-Word Graph ( WW-Graph)  
Given the word collection T={tj|1IjIn } of a docu-
ment1 , the semantic similarity between any two 
words ti and tj can be computed using approaches 
that are either knowledge-based or corpus-based 
(Mihalcea et al, 2006).   
Knowledge-based measures of word semantic 
similarity try to quantify the degree to which two 
words are semantically related using information 
drawn from semantic networks. WordNet (Fell-
baum, 1998) is a lexical database where each 
 
1 The stopwords defined in the Smart system have been re-
moved from the collection. 
sentence
word
SS
WW
SW
554
unique meaning of a word is represented by a 
synonym set or synset. Each synset has a gloss that 
defines the concept that it represents. Synsets are 
connected to each other through explicit semantic 
relations that are defined in WordNet. Many ap-
proaches have been proposed to measure semantic 
relatedness based on WordNet. The measures vary 
from simple edge-counting to attempt to factor in 
peculiarities of the network structure by consider-
ing link direction, relative path, and density, such 
as  vector, lesk, hso, lch, wup, path, res, lin and jcn 
(Pedersen et al, 2004). For example, ?cat? and 
?dog? has higher semantic similarity than ?cat? 
and ?computer?. In this study, we implement the 
vector measure to efficiently evaluate the similari-
ties of a large number of word pairs. The vector 
measure (Patwardhan, 2003) creates a co?
occurrence matrix from a corpus made up of the 
WordNet glosses. Each content word used in a 
WordNet gloss has an associated context vector. 
Each gloss is represented by a gloss vector that is 
the average of all the context vectors of the words 
found in the gloss. Relatedness between concepts 
is measured by finding the cosine between a pair of 
gloss vectors. 
 Corpus-based measures of word semantic simi-
larity try to identify the degree of similarity be-
tween words using information exclusively derived 
from large corpora. Such measures as mutual in-
formation (Turney 2001), latent semantic analysis 
(Landauer et al, 1998), log-likelihood ratio (Dun-
ning, 1993) have been proposed to evaluate word 
semantic similarity based on the co-occurrence 
information on a large corpus. In this study, we 
simply choose the mutual information to compute 
the semantic similarity between word ti and tj as 
follows: 
)()(
)(log)(
ji
ji
ji tptp
,ttpN,ttsim
 (3) 
which indicates the degree of statistical depend-
ence between ti and tj. Here, N is the total number 
of words in the corpus and p(ti) and p(tj) are re-
spectively the probabilities of the occurrences of ti
and tj, i.e. count(ti)/N and count(tj)/N, where 
count(ti) and count(tj) are the frequencies of ti and tj.
p(ti, tj) is the probability of the co-occurrence of ti
and tj within a window with a predefined size k, i.e. 
count(ti, tj)/N, where count(ti, tj) is the number of 
the times ti and tj co-occur within the window.  
Similar to the SS-Graph, we can build an undi-
rected graph GWW to reflect the homogeneous rela-
tionship between words, in which each node corre-
sponds to a word and the weight associated with 
the edge between any different word ti and tj is 
computed by either the WordNet-based vector 
measure or the corpus-based mutual information 
measure. We use an adjacency matrix V to de-
scribe GWW with each entry corresponding to the 
weight of a link in the graph. V= [Vij]n?n, where Vij 
=sim(ti, tj) if iKj and Vij=0 if i=j.
Then V is similarly normalized to V~ to make 
the sum of each row equal to 1. 
3.2.3  Sentence-to-Word Graph ( SW-Graph)  
Given the sentence collection S={si | 1IiIm} and 
the word collection T={tj|1IjIn } of a document, 
we can build a weighted bipartite graph GSW from S
and T in the following way: if word tj appears in 
sentence si, we then create an edge between si and 
tj. A nonnegative weight aff(si,tj) is specified on the 
edge, which is proportional to the importance of 
word tj in sentence si, computed as follows: 

i
jj
st
tt
tt
ji isftf
isftf
,tsaff )(  (4)
where t represents a unique term in si and tft, isft
are respectively the term frequency in the sentence 
and the inverse sentence frequency.  
We use an adjacency (affinity) matrix 
W=[Wij]m?n to describe GSW  with each entry Wij 
corresponding to aff(si,tj). Similarly, W is normal-
ized to W~ to make the sum of each row equal to 1. 
In addition, we normalize the transpose of W, i.e. 
WT, to W? to make the sum of each row in WT
equal to 1. 
3.3 Reinforcement Algorithm 
We use two column vectors u=[u(si)]m?1 and v
=[v(tj)]n?1 to denote the saliency scores of the sen-
tences and words in the specified document. The 
assumptions introduced in Section 3.1 can be ren-
dered as follows: 
 j jjii suUsu )(~)( (5) 
 i iijj tvVtv )(~)( (6) 
 j jjii tvWsu )(?)( (7) 
555
 i iijj suWtv )(~)( (8) 
After fusing the above equations, we can obtain 
the following iterative forms: 
 n
j
jji
m
j
jjii tvW)suU*su
11
)(?)(~)( (9)
 m
i
iij
n
i
iijj suW)tvV*tv
11
)(~)(~)( (10)
And the matrix form is: 
vWuUu TT )* ?~ (11)
uWvVv TT )* ~~ (12) 
where * and ) specify the relative contributions to 
the final saliency scores from the homogeneous 
nodes and the heterogeneous nodes and we have 
*+)=1. In order to guarantee the convergence of 
the iterative form, u and v are normalized after 
each iteration. 
For numerical computation of the saliency 
scores, the initial scores of all sentences and words 
are set to 1 and the following two steps are alter-
nated until convergence, 
1. Compute and normalize the scores of sen-
tences: 
)(n-T)(n-T(n) )* 11 ?~ vWuUu ,
1
(n)(n)(n) / uuu
2. Compute and normalize the scores of words: 
)(n-T)(n-T(n) )* 11 ~~ uWvVv ,
1
(n)(n)(n) / vvv
where u(n) and v(n) denote the vectors computed at 
the n-th iteration.   
Usually the convergence of the iteration algo-
rithm is achieved when the difference between the 
scores computed at two successive iterations for 
any sentences and words falls below a given 
threshold (0.0001 in this study).  
4 Empirical Evaluation 
4.1 Summarization Evaluation 
4.1.1 Evaluation Setup 
We used task 1 of DUC2002 (DUC, 2002) for 
evaluation. The task aimed to evaluate generic 
summaries with a length of approximately 100 
words or less. DUC2002 provided 567 English 
news articles collected from TREC-9 for single-
document summarization task. The sentences in 
each article have been separated and the sentence 
information was stored into files.  
In the experiments, the background corpus for 
using the mutual information measure to compute 
word semantics simply consisted of all the docu-
ments from DUC2001 to DUC2005, which could 
be easily expanded by adding more documents. 
The stopwords were removed and the remaining 
words were converted to the basic forms based on 
WordNet. Then the semantic similarity values be-
tween the words were computed.   
We used the ROUGE (Lin and Hovy, 2003) 
toolkit (i.e.ROUGEeval-1.4.2 in this study) for 
evaluation, which has been widely adopted by 
DUC for automatic summarization evaluation. It 
measured summary quality by counting overlap-
ping units such as the n-gram, word sequences and 
word pairs between the candidate summary and the 
reference summary. ROUGE toolkit reported sepa-
rate scores for 1, 2, 3 and 4-gram, and also for 
longest common subsequence co-occurrences. 
Among these different scores, unigram-based 
ROUGE score (ROUGE-1) has been shown to 
agree with human judgment most (Lin and Hovy, 
2003). We showed three of the ROUGE metrics in 
the experimental results: ROUGE-1 (unigram-
based), ROUGE-2 (bigram-based), and ROUGE-
W (based on weighted longest common subse-
quence, weight=1.2).  
In order to truncate summaries longer than the 
length limit, we used the ?-l? option 2 in the 
ROUGE toolkit. 
4.1.2 Evaluation Results 
For simplicity, the parameters in the proposed ap-
proach are simply set to *=)=0.5, which means 
that the contributions from sentences and words 
are equally important. We adopt the WordNet-
based vector measure (WN) and the corpus-based 
mutual information measure (MI) for computing 
the semantic similarity between words.  When us-
ing the mutual information measure, we heuristi-
cally set the window size k to 2, 5 and 10, respec-
tively.  
The proposed approaches with different word 
similarity measures (WN and MI) are compared 
 
2 The ?-l? option is very important for fair comparison. Some 
previous works not adopting this option are likely to overes-
timate the ROUGE scores.  
556
with two solid baselines: SentenceRank and Mutu-
alRank. SentenceRank is proposed in Mihalcea and 
Tarau (2004) to make use of only the sentence-to-
sentence relationships to rank sentences, which 
outperforms most popular summarization methods. 
MutualRank is proposed in Zha (2002) to make use 
of only the sentence-to-word relationships to rank 
sentences and words. For all the summarization 
methods, after the sentences are ranked by their 
saliency scores, we can apply a variant form of the 
MMR algorithm to remove redundancy and choose 
both the salient and novel sentences to the sum-
mary. Table 1 gives the comparison results of the 
methods before removing redundancy and Table 2 
gives the comparison results of the methods after 
removing redundancy. 
 
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN) 0.47100
*# 0.20424*# 0.16336#
Our Approach
(MI:k=2) 0.46711
# 0.20195# 0.16257#
Our Approach
(MI:k=5) 0.46803
# 0.20259# 0.16310#
Our Approach
(MI:k=10) 0.46823
# 0.20301# 0.16294#
SentenceRank 0.45591 0.19201 0.15789 
MutualRank 0.43743 0.17986 0.15333 
Table 1. Summarization Performance before Re-
moving Redundancy (w/o MMR) 
 
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN) 0.47329
*# 0.20249# 0.16352#
Our Approach
(MI:k=2) 0.47281
# 0.20281# 0.16373#
Our Approach
(MI:k=5) 0.47282
# 0.20249# 0.16343#
Our Approach
(MI:k=10) 0.47223
# 0.20225# 0.16308#
SentenceRank 0.46261 0.19457 0.16018 
MutualRank 0.43805 0.17253 0.15221 
Table 2. Summarization Performance after Remov-
ing Redundancy (w/ MMR) 
 (* indicates that the improvement over SentenceRank is sig-
nificant and # indicates that the improvement over Mutual-
Rank is significant, both by comparing the 95% confidence 
intervals provided by the ROUGE package.)
Seen from Tables 1 and 2, the proposed ap-
proaches always outperform the two baselines over 
all three metrics with different word semantic 
measures. Moreover, no matter whether the MMR 
algorithm is applied or not, almost all performance 
improvements over MutualRank are significant 
and the ROUGE-1 performance improvements 
over SentenceRank are significant when using 
WordNet-based measure (WN). Word semantics 
can be naturally incorporated into the computation 
process, which addresses the problem that Sen-
tenceRank cannot take into account word seman-
tics, and thus improves the summarization per-
formance. We also observe that the corpus-based 
measure (MI) works almost as well as the knowl-
edge-based measure (WN) for computing word 
semantic similarity. 
In order to better understand the relative contri-
butions from the sentence nodes and the word 
nodes, the parameter * is varied from 0 to 1. The 
larger * is, the more contribution is given from the 
sentences through the SS-Graph, while the less 
contribution is given from the words through the 
SW-Graph. Figures 2-4 show the curves over three 
ROUGE scores with respect to *. Without loss of 
generality, we use the case of k=5 for the MI 
measure as an illustration. The curves are similar 
to Figures 2-4 when k=2 and k=10.   
 
0.435
0.44
0.445
0.45
0.455
0.46
0.465
0.47
0.475
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1*
RO
UG
E-
1
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 2. ROUGE-1 vs. *
0.17
0.175
0.18
0.185
0.19
0.195
0.2
0.205
0.21
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
RO
UG
E-
2
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 3. ROUGE-2 vs. *
557
0.151
0.153
0.155
0.157
0.159
0.161
0.163
0.165
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
RO
UG
E-
W
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 4. ROUGE-W vs. *
Seen from Figures 2-4, no matter whether the 
MMR algorithm is applied or not (i.e. w/o MMR 
or w/ MMR), the ROUGE scores based on either 
word semantic measure (MI or WN) achieves the 
peak when * is set between 0.4 and 0.6. The per-
formance values decrease sharply when * is very 
large (near to 1) or very small (near to 0). The 
curves demonstrate that both the contribution from 
the sentences and the contribution from the words 
are important for ranking sentences; moreover, the 
contributions are almost equally important. Loss of 
either contribution will much deteriorate the final 
performance.  
Similar results and observations have been ob-
tained on task 1 of DUC2001 in our study and the 
details are omitted due to page limit. 
4.2 Keyword Evaluation 
4.1.1   Evaluation Setup 
In this study we performed a preliminary evalua-
tion of keyword extraction. The evaluation was 
conducted on the single word level instead of the 
multi-word phrase (n-gram) level, in other words, 
we compared the automatically extracted unigrams 
(words) and the manually labeled unigrams 
(words). The reasons were that: 1) there existed 
partial matching between phrases and it was not 
trivial to define an accurate measure to evaluate 
phrase quality; 2) each phrase was in fact com-
posed of a few words, so the keyphrases could be 
obtained by combining the consecutive keywords.  
We used 34 documents in the first five docu-
ment clusters in DUC2002 dataset (i.e. d061-d065).  
At most 10 salient words were manually labeled 
for each document to represent the document and 
the average number of manually assigned key-
words was 6.8. Each approach returned 10 words 
with highest saliency scores as the keywords. The 
extracted 10 words were compared with the manu-
ally labeled keywords. The words were converted 
to their corresponding basic forms based on 
WordNet before comparison. The precision p, re-
call r, F-measure (F=2pr/(p+r)) were obtained for 
each document and then the values were averaged 
over all documents for evaluation purpose. 
4.1.2 Evaluation Results 
Table 3 gives the comparison results. The proposed 
approaches are compared with two baselines: 
WordRank and MutualRank. WordRank is pro-
posed in Mihalcea and Tarau (2004) to make use 
of only the co-occurrence relationships between 
words to rank words, which outperforms tradi-
tional keyword extraction methods. The window 
size k for WordRank is also set to 2, 5 and 10, re-
spectively. 
 
System Precision Recall F-measure
Our Approach
(WN) 0.413 0.504 0.454 
Our Approach
(MI:k=2) 0.428 0.485 0.455 
Our Approach
(MI:k=5) 0.425 0.491 0.456 
Our Approach
(MI:k=10) 0.393 0.455 0.422 
WordRank 
(k=2) 0.373 0.412 0.392 
WordRank 
(k=5) 0.368 0.422 0.393 
WordRank 
(k=10) 0.379 0.407 0.393 
MutualRank 0.355 0.397 0.375 
Table 3. The Performance of Keyword Extraction  
Seen from the table, the proposed approaches 
significantly outperform the baseline approaches. 
Both the corpus-based measure (MI) and the 
knowledge-based measure (WN) perform well on 
the task of keyword extraction. 
A running example is given below to demon-
strate the results: 
Document ID: D062/AP891018-0301 
Labeled keywords:
insurance earthquake insurer damage california Francisco 
pay 
Extracted keywords:
WN: insurance earthquake insurer quake california 
spokesman cost million wednesday damage 
MI(k=5): insurance insurer earthquake percent benefit 
california property damage estimate rate 
558
5 Conclusion and Future Work 
In this paper we propose a novel approach to si-
multaneously document summarization and key-
word extraction for single documents by fusing the 
sentence-to-sentence, word-to-word, sentence-to-
word relationships in a unified framework. The 
semantics between words computed by either cor-
pus-based approach or knowledge-based approach 
can be incorporated into the framework in a natural 
way. Evaluation results demonstrate the perform-
ance improvement of the proposed approach over 
the baselines for both tasks. 
In this study, only the mutual information meas-
ure and the vector measure are employed to com-
pute word semantics, and in future work many 
other measures mentioned earlier will be investi-
gated in the framework in order to show the ro-
bustness of the framework. The evaluation of key-
word extraction is preliminary in this study, and 
we will conduct more thorough experiments to 
make the results more convincing. Furthermore, 
the proposed approach will be applied to multi-
document summarization and keyword extraction, 
which are considered more difficult than single 
document summarization and keyword extraction. 
Acknowledgements 
This work was supported by the National Science 
Foundation of China (60642001). 
References 
M. R. Amini and P. Gallinari. 2002. The use of unlabeled data to 
improve supervised learning for text summarization. In Pro-
ceedings of SIGIR2002, 105-112. 
S. Brin and L. Page. 1998. The anatomy of a large-scale hypertex-
tual Web search engine. Computer Networks and ISDN Sys-
tems, 30(1?7). 
J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-
based reranking for reordering documents and producing 
summaries. In Proceedings of SIGIR-1998, 335-336. 
J. M. Conroy and D. P. O?Leary. 2001. Text summarization via 
Hidden Markov Models. In Proceedings of SIGIR2001, 406-
407. 
DUC. 2002. The Document Understanding Workshop 2002. 
http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html 
T. Dunning. 1993. Accurate methods for the statistics of surprise 
and coincidence. Computational Linguistics 19, 61?74. 
G. ErKan and D. R. Radev. 2004. LexPageRank: Prestige in 
multi-document text summarization. In Proceedings of 
EMNLP2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. 
The MIT Press.  
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. 
Nevill-Manning. 1999. Domain-specific keyphrase extraction. 
Proceedings of IJCAI-99, pp. 668-673.  
Y. H. Gong and X. Liu. 2001. Generic text summarization using 
Relevance Measure and Latent Semantic Analysis. In Proceed-
ings of SIGIR2001, 19-25. 
E. Hovy and C. Y. Lin. 1997. Automated text summarization in 
SUMMARIST. In Proceeding of ACL?1997/EACL?1997 Wor-
shop on Intelligent Scalable Text Summarization.
A. Hulth. 2003. Improved automatic keyword extraction given 
more linguistic knowledge. In Proceedings of EMNLP2003,
Japan, August. 
J. M. Kleinberg. 1999. Authoritative sources in a hyperlinked 
environment. Journal of the ACM, 46(5):604?632. 
B. Krulwich and C. Burkey. 1996. Learning user information 
interests through the extraction of semantically significant 
phrases. In AAAI 1996 Spring Symposium on Machine Learn-
ing in Information Access.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A.trainable document 
summarizer. In Proceedings of SIGIR1995, 68-73. 
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to 
latent semantic analysis. Discourse Processes 25. 
C. Y. Lin and  E. Hovy. 2000. The automated acquisition of topic 
signatures for text Summarization. In Proceedings of ACL-
2000, 495-501. 
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summa-
ries using n-gram co-occurrence statistics. In Proceedings of 
HLT-NAACL2003, Edmonton, Canada, May. 
R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based 
and knowledge-based measures of text semantic similarity. In 
Proceedings of AAAI-06.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into 
texts. In Proceedings of EMNLP2004.
R. Mihalcea and P.Tarau. 2005. A language independent algo-
rithm for single and multiple document summarization. In 
Proceedings of IJCNLP2005.
A. Mu?oz. 1996. Compound key word generation from document 
databases using a hierarchical clustering ART model. Intelli-
gent Data Analysis, 1(1). 
T. Nomoto and Y. Matsumoto. 2001. A new approach to unsuper-
vised text summarization. In Proceedings of SIGIR2001, 26-34. 
S. Patwardhan. 2003. Incorporating dictionary and corpus infor-
mation into a context vector measure of semantic relatedness. 
Master?s thesis, Univ. of Minnesota, Duluth. 
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Word-
Net::Similarity ? Measuring the relatedness of concepts. In 
Proceedings of AAAI-04.
D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document 
Summarization using Conditional Random Fields. In Proceed-
ings of IJCAI 07.
A. M. Steier and R. K. Belew. 1993. Exporting phrases: A statisti-
cal analysis of topical language.  In Proceedings of Second 
Symposium on Document Analysis and Information Retrieval,
pp. 179-190. 
P. D. Turney. 2000. Learning algorithms for keyphrase extraction. 
Information Retrieval, 2:303-336. 
P. Turney. 2001. Mining the web for synonyms: PMI-IR versus 
LSA on TOEFL. In Proceedings of ECML-2001.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. 
Nevill-Manning. 1999. KEA: Practical automatic keyphrase 
extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 
254-256. 
H. Y. Zha. 2002. Generic summarization and keyphrase extraction 
using mutual reinforcement principle and sentence clustering. 
In Proceedings of SIGIR2002, pp. 113-120. 
559
 
 
 
 
Xiao Qin, Liang Zong, Yuqian Wu, Xiaojun Wan and Jianwu Yang 
Institute of Computer Science and Technology 
Peking University, China, 100871 
{qinxiao,zongliang,wuyuqian,wanxiaojun,yangjianwu} 
@cist.pku.edu.cn 
 
 
 
Abstract 
This paper describes our experiments on 
the cross-domain Chinese word segmen-
tation task at the first CIPS-SIGHAN 
Joint Conference on Chinese Language 
Processing. Our system is based on the 
Conditional Random Fields (CRFs) 
model. Considering the particular prop-
erties of the out-of-domain data, we pro-
pose some novel steps to get some im-
provements for the special task.  
1 Introduction 
Chinese word segmentation is one of the most   
important tasks in the field of Chinese informa-
tion processing and it is meaningful to intelligent 
information processing technologies. After a lot 
of researches, Chinese word segmentation has 
achieved a high accuracy. Many methods have 
been presented, among which the CRFs model 
has attracted more and more attention. Zhao?s 
group used the CRFs model in the task of Chi-
nese word segmentation in Bakeoff-4 and they 
ranked at the top in all closed tests of word seg-
mentation (Zhao and Kit, 2008). The CRFs 
model has been widely used because of its excel-
lent performance. However, finding a better 
segmentation algorithm for the out-of-domain 
text is the focus of CIP-SIGHAN-2010 bakeoff. 
We still consider word segmentation as a se-
quence labeling problem. What we concern is 
how to use the unlabeled corpora to enrich the 
supervised CRFs learning. So we take some 
strategies to make use of the information of the 
texts in the unlabeled corpora.  
2 System Description 
In this section, we will describe our system in 
details. The system is based on the CRFs model 
and we propose some novel steps for some im-
provements. It mainly consists of three steps: 
preprocessing, CRF-based labeling, and re-
labeling.  
2.1 Preprocessing 
This step mainly includes two operations. First, 
we should cut the whole text into a series of sen-
tences. We regard ???, ???, ??? and ??? as the 
symbols of the boundary between sentences. 
Then we do atomic segmentation to all the sen-
tences. Here Atomic segmentation represents 
that we should regard the continuous non-
Chinese characters as a whole. Take the word 
?computer? as an example, we should regard 
?computer? as a whole, but not treat it as 8 sepa-
rate letters of ?c?, ?o?, ?m?, ?p?, ?u?, ?t?, ?e?, and 
?r?.    
2.2 CRF-based Labeling 
Conditional random field (CRF) is an extension 
of both Maximum Entropy Model (MEMs) and 
Hidden Markov Models (HMMs), which was 
firstly introduced by Lafferty (Lafferty et al, 
2001). It is an undirected graphical model 
trained to maximize the conditional probability 
of the desired outputs given the corresponding 
inputs. This model has achieved great successes 
in word segmentation. 
In the CRFs model, the conditional distribu-
tion P(y|x) of the labels Y givens observations X 
directly is defined: 
CRF-based Experiments for Cross-Domain Chinese 
Word Segmentation at CIPS-SIGHAN-2010 
1
1
1( / ) exp{ ( , , , )}
T
k k t t
t kx
P y x f y y x tZ ? ??? ??
y is the label sequence, x is observation sequence, 
Zx is a normalization term that makes the proba-
bility of all state sequences sum to one; fk(yt-1, yt, 
t) is often a binary-valued feature function and 
? k is the weight of fk. 
In our system, we choose six types of tags ac-
cording to character position in a word. Accord-
ing to Zhao?s work (Zhao et al, 2006a), the 6-
tag set enables our system to generate a better 
CRF model than the 4-tag set. In our experi-
ments, we test both the 6-tag set and the 4-tag 
set, and the 6-tag set truly has a better result. The 
6-tag set is defined as below: 
T = {B, B2, B3, M, E, S} 
Here B, B2, B3, M, E represent the first, 
second, third, continuing and end character posi-
tions in a multi-character word, and S is the sin-
gle-character word tag. 
We adopt 6 n-gram feature templates as fea-
tures. Some researches have proved that the 
combination of 6-tag set and 6 n-gram feature 
template can achieve a better performance (Zhao 
et al, 2006a; Zhao et al, 2006b; Zhao and Kit, 
2007). 
The 6 n-gram feature templates used in our 
system are C-1, C0, C1, C-1C0, C0C1, C-1C1. Here 
C stands for a character and the subscripts -1, 0 
and 1 stand for the previous, current and next 
character, respectively. 
Furthermore, we try to take advantage of the 
types for the characters. For example, in our sys-
tem D stands for the date, N stands for the num-
ber, L stands for the letter, P stands for the punc-
tuation and C stands for the other characters. 
Introducing these features is beneficial to the 
CRFs learning.  
2.3 Re-labeling step 
Since the unlabeled corpora belong to different 
domains, traditional methods have some limita-
tions. In this section, we propose an additional 
step to make good use of the unlabelled data for 
this special task. This step is based on the out-
puts of the CRFs model in the previous step. 
After CRFs learning, we get a training mod-
el. With this model, we can label the literature, 
computer, medicine and finance corpora. Ac-
cording to the outputs of the CRFs model, we 
choose some labeled sentences with high confi-
dence and add them to the training corpus. Here 
the selection of high confidence must guarantee 
that the probability of the sentences selected be-
ing correct segmentations is rather high and the 
number of the sentences selected is not too little 
or they will make no difference to the generation 
of the new CRF model. Since the existing train-
ing model does not contain the information in 
the out-of-domain data, we treat the labeled sen-
tences with high confidence as additional train-
ing corpus. Then we re-train the CRFs model 
with the new training data. With the training da-
ta extracted from different domains, the training 
model incorporates more cross-domain informa-
tion and it can work better in the corresponding 
cross-domain prediction task. 
3 Experiments 
3.1 Experiment Setup 
There are two sources for the corpora: the train-
ing corpora and the test corpus. And in the train-
ing corpora, there exist two types of corpus in 
this task. The labeled corpus is Chinese text 
which has been segmented into words while the 
unlabelled corpus covers two domains: literature 
and computer science. The test corpus contains 4 
domains, which are literature, computer science, 
medicine and finance. 
There are four evaluation metrics used in 
this bake-off task: Precision, Recall, F1 measure 
(F1 = 2RP/(R+P)) and OOV measure, where R 
and P are the recall and precision of the segmen-
tation and OOV (Out-Of-Vocabulary Word) is a 
word which occurs in the reference corpus but 
does not occur in the labeled training corpus. 
Our system uses the CRF++ package Ver-
sion 0.49 implemented by Taku Kudo 1  from 
sourceforge. 
3.2 Results and Discussions 
We test the techniques described in section 2 
with the given data. Now we will show the re-
sults of each operation. 
3.2.1  Preprocessing 
As we have mentioned in section 2.1, the first 
step is to cut the text into a series of sentences. 
                                               
1 http://crfpp.sourceforge.net/ 
Then we should give each character in one sen-
tence a label. Before this step, it is necessary to 
do atomic segmentation. And we will regard the 
continuous non-Chinese characters as a whole 
and give the whole part a single label. This is 
meaningful to those corpora containing a lot of 
English words. Due to the diversity of the Eng-
lish words, segmenting the sentences with a lot 
non-Chinese characters correctly is rather diffi-
cult only through CRF learning. We should do 
atomic segmentation to all training and test cor-
pora. This may achieve a higher accuracy in a 
certain degree. 
The results of word segmentation are re-
ported in Table 1. ?Clouse+/-? indicates whether 
text clause has been done. 
 
Table 1: Results with clause and without clause 
 corpus Precision Recall F 
Literature 
Clause+ 0.922 0.916 0.919 
Clause- 0.921 0.915 0.918 
Computer 
Clause+ 0.934 0.939 0.937 
Clause- 0.934 0.939 0.936 
Medicine 
Clause+ 0.911 0.917 0.914 
Clause- 0.509 0.511 0.510 
Finance 
Clause+ 0.940 0.943 0.941 
Clause- 0.933 0.940 0.937 
 
From Table 1, we can see there is some im-
provement in different degree and the effect in 
the medicine corpus is the most obvious. So we 
can conclude that our preprocessing is useful to 
the word segmentation. 
3.2.2 CRF-based labeling 
After preprocessing, we can use CRF++ package 
to learn and test.  
The selection of feature template is also an 
important factor. For the purpose of comparison, 
we test two kinds of feature templates in our sys-
tem. The one is showed in Table 2 and the other 
one is showed in Table 3. 
Table 2: Template 1 
# Unigram 
U00:%x[-1,0] 
U01:%x[0,0] 
U02:%x[1,0] 
U03:%x[-1,0]/%x[0,0] 
U04:%x[0,0]/%x[1,0] 
U05:%x[-1,0]/%x[1,0] 
# Bigram 
B 
 
Table 3: Template 2 
# Unigram 
U00:%x[-1,0] 
U01:%x[0,0] 
U02:%x[1,0] 
U03:%x[-1,0]/%x[0,0] 
U04:%x[0,0]/%x[1,0] 
U05:%x[-1,0]/%x[1,0] 
U10:%x[-1,1] 
U11:%x[0,1] 
U12:%x[1,1] 
U13:%x[-1,1]/%x[0,1] 
U14:%x[0,1]/%x[1,1] 
U15:%x[-1,1]/%x[1,1] 
# Bigram 
B 
 
Now we will explain the meanings of the 
templates. Here is an example. In table 4, we 
show the format of the input file. The first col-
umn represents the word itself and the second 
represents the feature of the word, where there 
are five kinds of features: date (D), number (N), 
letter (L), punctuation (P) and others (C). The 
meanings of the templates are showed in table 5. 
 
Table 4: the format of the input file for CRF 
? C 
? D 
? C 
? C 
? P 
? C 
? C 
? C 
1 N 
? C 
? P 
 
Table 5: the example of the templates 
template Expanded feature 
%x[0,0] ? 
%x[0,1] C 
%x[1,0] ? 
%x[-1,0] ? 
%x[-1,0]/ %x[0,0] ?/? 
%x[0,0]/ %x[0,1] ?/C 
With two different feature templates, we con-
tinue our experiments in the four different do-
mains. The segmentation performances of our 
system on test corpora using different feature 
templates are presented in Table 6.  
 
Table 6: Results with different feature templates 
 corpus Precision Recall F 
Literature 
T1 0.917 0.909 0.913 
T2 0.922 0.916 0.919 
Computer 
T1 0.914 0.902 0.908 
T2 0.934 0.939 0.937 
Medicine 
T1 0.906 0.905 0.905 
T2 0.911 0.917 0.914 
Finance 
T1 0.937 0.925 0.931 
T2 0.940 0.943 0.941 
 
Here T1 stands for Template 1 while T2 
stands for Template 2. 
From the Table 4 we can see the second fea-
ture templates make the results of the segmenta-
tion improved more significantly. 
At the same time we need get the outputs with 
confidence measure by setting some parameters 
in CRF test. 
3.2.3 Re-labeling 
As for the outputs with confidence measure 
generated by previous step, we should do some 
special processes. Here we set a particular value 
as our standard and choose the sentences with 
confidence above the value. As we know, the 
test corpora are limited, the higher confidence 
may cause the corpora meeting our requirements 
are less. The lower confidence may not guaran-
tee the reliability. So the setting of the confi-
dence value is very significant. In our experi-
ments, we set the parameter at 0.8. 
Then we add the sentences whose confidence 
is above 0.8 to the training corpus. We should 
re-learn with new corpora, generate the new 
model and re-test the corpora related with 4 do-
mains. The segmentation performances after re-
labeling are represented in Table 7. 
 
Table 7: Results with re-labeling and without re-
labeling 
 corpus Precision Recall F 
Literature 
Re + 0.922 0.916 0.919 
Re - 0.921 0.916 0.918 
Computer 
Re + 0.934 0.939 0.937 
Re - 0.932 0.934 0.933 
Medicine 
Re + 0.911 0.917 0.914 
Re - 0.912 0.918 0.915 
Finance 
Re + 0.940 0.943 0.941 
Re - 0.937 0.941 0.939 
 
Here Re+/- indicates whether the re-labeling 
step is to be done. 
From the results we know, even though the re-
labeling step makes the results in the medicine 
corpus a little worse, it has much better effect in 
the other corpora. Overall, the operation of re-
labeling is necessary. 
3.3 Our results in this bakeoff 
In this task, our results are showed in Table 8. 
 
Table 8: our results in this bakeoff 
 Precision Recall F 
Literature 0.922 0.916 0.919 
Computer 0.934 0.939 0.937 
Medicine 0.911 0.917 0.914 
Finance 0.940 0.943 0.941 
 
From Table 6, we can see our system can 
achieve a high precision, especially in the do-
mains of computer and finance.  This proves our 
methods are fairly effective. 
4 Discussion 
4.1 Segmentation Features 
In our system, we only take advantage of the 
features of the words. We try to add other fea-
tures to our experiments such as AV feature 
(Feng et al, 2004a; Feng et al, 2004b; Hai Zhao 
et al, 2007) with the expectation of improving 
the results. But the results are not satisfying. We 
believe that the feature of words frequency may 
be an important factor, but how to use it is worth 
studying. So finding some meaningful and effec-
tive features is the crucial point. 
4.2 OOV 
In our system, we do not process the words 
out of vocabulary in the special way. The recog-
nition of OOV is still a problem. In a word, there 
is still much to be done to improve our system. 
In the present work, we make use of some sur-
face features, and further study should be con-
tinued to find more effective features. 
5 Conclusion 
In this paper, we have briefly described the 
Chinese word segmentation for out-of-domain 
texts. The CRFs model is implemented. In order 
to make the best use of the test corpora, some 
special strategies are introduced. Further im-
provement is made with these strategies. How-
ever, there is still much to do to achieve more 
improvement. From the results, we got good ex-
perience and knew the weaknesses of our system. 
These all help to improve the performance of our 
system in the future. 
Acknowledgements 
The research described in this paper was sup-
ported by NSFC (Grant No. 60875033). 
References 
Hai Zhao, Changning Huang, and Mu Li. 2006. An 
improved Chinese Word Segmentation System 
with Conditional Random Field. Proceedings of 
the Fifth SIGHAN Workshop on Chinese Language 
Processing, Sydney, Australia. 
Hai Zhao and Chunyu Kit. 2007. Incorporating global 
information into supervised for Chinese word 
segmentation. In PACALING-2007, Melbourne, 
Australia. 
Hai Zhao, Changning Huang, Mu Li and Bao-Liang 
Lu. 2006b. Effective tag set selection in Chinese 
word segmentation via conditional random field 
modeling. In PACLIC-20, Wuhan, China. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of ICML 2001, Mor-
gan Kaufmann, San Francisco, CA 
Hai Zhao and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character 
Tagging for Word Segmentation and Named Enti-
ty Recognition. Processing of the Sixth SIGHAN 
Workshop on Chinese Language Processing, Hy-
derabad, India. 
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin 
Zheng. 2004a. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics. 
Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie 
Deng. 2004b. Unsupervised segmentation of Chi-
nese corpus using accessor variety. In First Inter-
national Joint Conference on Natural Language 
Processing. Sanya, Hainan Island, China. 
