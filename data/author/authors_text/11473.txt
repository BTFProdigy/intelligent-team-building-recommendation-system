Proceedings of the 8th International Conference on Computational Semantics, pages 45?60,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Automatic identification of semantic relations
in Italian complex nominals
Fabio Celli
CLIC-CIMeC
University of Trento
fabio.celli@email.unitn.it
Malvina Nissim
Dipartimento di Studi Linguistici e Orientali
University of Bologna
malvina.nissim@unibo.it
Abstract
This paper addresses the problem of the identification of the seman-
tic relations in Italian complex nominals (CNs) of the type N+P+N.
We exploit the fact that the semantic relation, which is underspeci-
fied in most cases, is partially made explicit by the preposition. We
develop an annotation framework around five different semantic rela-
tions, which we use to create a corpus of 1700 Italian CNs, obtaining an
inter-annotator agreement of K=.695. Exploiting this data, for each
preposition p we train a classifier to assign one of the five semantic
relations to any CN of the type N+p+N, by using both string and
supersense features. To obtain supersenses, we experiment with a se-
quential tagger as well as a plain lookup in MultiWordNet, and find
that using information obtained from the former yields better results.
1 Introduction
Complex nominals are pervasive in language, and include noun-noun (N+N)
and adjective-noun (A+N) combinations (Levi, 1978), as in Ex. 1 and 2.
(1) dessert fork
(2) medieval historian
45
A ?dessert fork? is ?a fork for eating dessert?, and a ?medieval historian?
can be also described as ?a historian who studies medieval times?.
1
In
both cases the relation is not overtly marked. Indeed, syntactically, there is
nothing that tells us that the semantic relation between ?dessert? and ?fork?
in Ex. 1 is different than the one binding ?plastic? and ?fork? in Ex. 3.
(3) plastic fork
However, it is well known that whereas English composes CNs of the type
N+N, Romance languages must glue the two nouns by means of a prepo-
sition, thus yielding CNs of the form N+P+N, thereby partially making
explicit the underlying semantic relation (Busa and Johnston, 1996). So, in
Ex. 4, the ?purpose? relation between dessert and fork is (partially) made
explicit by the preposition ?da?. In contrast, the ?property? relation binding
plastic and fork (a fork made of plastic) is expressed using ?di? (Ex. 5).
(4) forchetta da dessert (en: dessert fork)
(5) forchetta di plastica (en: plastic fork)
Recently, Girju (2007) has exploited this observation including cross-language
information in a system for the automatic interpretation of NN compounds
in English. However, whereas it is true that the overt preposition restricts
the set of possible relations, it is also true that prepositions are still se-
mantically ambiguous, since there is no one-to-one correspondence between
prepositions and relations. So, ?di?, used in a ?property? relation above,
can also express a ?part-whole? (Ex. 6), a ?theme? (Ex. 7), and several other
relations.
(6) dorso della mano (the back of the hand)
(7) suonatore di chitarra (guitar player)
In this work, we also exploit the presence of a preposition in Italian CNs as
an aid to detect the semantic relation. We extract and annotate CNs in a
corpus of written Italian, and develop a supervised system for determining
the semantics of the CN, comparing the contribution of plain nouns with
that of hypernym classes, and different ways in which such hypernyms can be
obtained. In the next section, we discuss previous work on the semantics of
complex nominals. In Section 3, we define a set of five semantic relations for
the annotation of Italian CNs and the details of the annotation framework,
and discuss the corpus distribution. In Section 4 we describe the experiments
for the automatic identification of semantic relations, and discuss the results.
We conclude with ideas for future work in Section 5.
1
In this work we will only consider N+N CNs, thereby excluding A+N CNs.
46
2 Previous work
Given their underspecified nature, CNs, especially in English, have received
a large amount of attention in the linguistic and computational linguistic
literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; John-
ston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007,
among others). Current interest in NLP is also shown in the organisation of
a SemEval task especially dedicated to noun-noun compound interpretation
(Task 4, (Girju et al, 2007)). Indeed, NLP systems which aim at full text
understanding for higher NLP tasks, such as question answering, recognis-
ing textual entailment and machine translation, need to grasp the semantic
relation which noun compounds mostly leave underspecified.
One main issue in noun-noun compound interpretation is the lack of
general agreement on a well-defined set of semantic relations. Nastase and
Szpakowicz (2003), for instance, propose a two-level taxonomy, in which
fifteen fine-grained relations are subsumed into five general classes (causal,
participant, spatial, temporal, quality). An example of a causal relation
(with subtype ?purpose?) is ?concert hall?, and an example of a participant
relation (with subtype ?beneficiary?) is ?student discount?.
Girju et al (2007) propose the smaller set reported in Table 1, which
was tested on English N+N complex nominals within the SemEval 2007 task.
They specifically spell out semantic relations as two-poles relationships: for
example an effect is an effect always with respect to a cause.
Table 1: The set of 7 semantic relations from Girju et al (2007)
Semantic relation Examples
Cause-Effect laugh (cause) wrinkles (effect)
Instrument-Agency laser (instrument) printer (agency)
Product-Producer honey (product) bee (producer)
Origin-Entity message (entity) from outer-space (origin)
Theme-Tool news (theme) conference(tool)
Part-Whole the door (part) of the car (whole)
Content-Container apples (content) in the basket (container)
As far as relation detection is concerned, Johnston and Busa (1996),
working specifically on Italian, have suggested using information included
in qualia structures (Pustejovsky, 1995) for deriving the compound?s inter-
pretation. The use of qualia structures for this task is appropriate and
semantically sound but absolutely not straightforward to implement, since
there does not exist an electronic repository of qualias, so that the structures
47
would need to be constructed by hand, thereby involving a large amount of
manual work. Recent work has shown that the automatic acquisition of
qualias can be performed with reasonable success exploiting information
obtained using lexico-syntactic patterns over the Web (Cimiano and Wen-
deroth, 2005). For our purposes, though, if lexico-syntactic patterns can be
used successfully to induce qualia roles, we could directly use the informa-
tion we obtain from them, thus bypassing the qualia structure representa-
tion. We plan to include features based on such kinds of patterns in future
development of this work (see also (Nakov and Hearst, 2008)).
More purely computational approaches include both supervised (Lauer,
1995) as well as unsupervised models, such as (Lapata and Keller, 2005),
who use frequencies obtained over the Web. Some researchers also suggest
solutions to the data sparsness problem, which affects our approach as well,
by using lexical similarity (Turney, 2006) or clustering techniques (Pantel
and Pennacchiotti, 2006).
Finally, there exists specific work on compound nouns whose head is
derived from a verb (Lapata, 2002), and information about verbs deverbal
nouns are linked to has proved a useful feature in previous approaches (Girju,
2007). Whereas we have exploited this information in the annotation phase,
we have not included corresponding features yet in the statistical model we
use, but we plan to do so in future extensions.
3 Annotation Framework and Data
For developing an annotation framework, we built on Italian grammars,
existing classifications (see Section 2), and a preliminary study of corpus
data.
3.1 Annotation framework
In determining the set of relations to be annotated, following (Girju, 2007),
we also define two-pole relations between the involved nominals.
We assume that relations can be extracted and subsumed in general
classes starting from ?-roles, which are partially made explicit by the prepo-
sitional phrase. Since there is no general agreement on a complete list of
?-roles we chose to work with types of complements, which are provided by
traditional Italian grammars and can be found in almost every Italian dic-
tionary. In (Zingarelli, 2008), we found 33 different types of prepositional
phrases (PPs), which we grouped into 21 classes (for instance, all of the
48
location-related PPs were grouped under a single LOC class). This infor-
mation was included in the annotation scheme (Celli, 2008), although is not
used in the current relation identification model.
Following (Langacker, 1987), the nouns within each CNs were also re-
visited within a trajector (Tr) and landmark (Lm) approach mirroring the
two-pole interpretation of the semantic relations.
The set of five semantic relations we arrived at is given in Table 2. These
five relations are the target of our classification experiments (Section 4).
Table 2: Relations for Italian prepositions.
Relation(Tag) Description Examples
cause-effect (CE) tr. causes lm. death
Lm
by privations
Tr
located-location (LL) lm. localizes tr. window
Lm
passage
Tr
owner-property (OP) tr. possess lm. stone
Lm
statue
Tr
included-set (IS) lm. includes tr. thousands
Tr
of men
Lm
bound-bounded (RR) lm. undergoes tr. city
Lm
destruction
Tr
In the cause-effect (CE) relation the trajector is the cause or the agent
and the landmark is the product or the effect produced by the agent/causer,
as in ?morte per stenti? (en: death by privations). In located-location (LL),
a trajector is located in space or time with respect to a landmark, as ?casa
in montagna? (en: mountain house). The owner-property (OP) relation as-
sociates a trajector (owner) with its property, part, or characteristic, which
is the landmark. Examples are ?statua di pietra? (en: stone statue) and
?cane da caccia? (en: hunting dog). In included-set (IS) the trajector is the
included object and the landmark is the set: in ?migliaia di uomini? (en:
thousands of men), ?migliaia? (en: thousands) is the subset and ?uomini?
(en: men) is the set. The bound-bounded (RR) relation is a direct rela-
tionship between an event, usually a deverbal (trajector), and its undergoer
(landmark), as ?distruzione della citta`? (en: destruction of the city). Clas-
sic relations such as part-whole, producer-product, and is-a are covered in
this account by the owner-property, cause-effect and included-set relations,
respectively.
Annotation categories Each extracted CN (see Section 3.2) was anno-
tated with the following information:
? the lemma (A, CON, DI, DA, IN, PER, SU, TRA)
2
2
The preposition ?tra? can also be written as ?fra?. They are semantically equivalent.
Occurrences of both variants were extracted, but we refer to them always as ?tra?.
49
? the relation (CE, OP, LL, IS, RR)
? the type of prepositional phrase (21 tags)
? the semantic type of n1/n2 (natural, abstract, artifact, metaphorical usage)
? the position of trajector and landmark in the CN (TL, LT)
? the order of the head and the modifier in the CN (HM, MH)
The following CN types were to be excluded from annotation:
? CNs including proper nouns, such as ?problema di Marco? (en: Mark?s prob-
lem);
? CNs involving complex prepositions, such as ?hotel nel mezzo del deserto?
(en: hotel in the middle of the desert);
? CNs involving n1 and/or n2 of categories other than noun, due to POS-
tagging errors;
? CNs containing bisyllabic prepositions, such as ?macchina senza benzina?
(en: car without fuel);
3
? CNs used as adverbs, e.g. ?accordo di massima? (en: generally agreed with)
3.2 Data
Corpus Selection We used CORISsmall, a reduced version of CORIS,
a 100M-word, balanced corpus of written Italian (Rossini Favretti, 2000).
CORISsmall was sampled by randomly extracting sentences with a length
between 2 and 40 words. We discarded a few domain-specific subcorpora
which were likely to contain prepositions used in ways different from common
usage, as the legal subcorpus. The resulting corpus, henceforth CORISnom-
inals, contains 75,000 words. The corpus was then automatically tagged
with part-of-speech information, using TreeTagger (Schmid, 1994).
CN detection We chose to annotate monosyllabic prepositions only, namely
a (to), con (with), di (of), da (from), in (in),per (for), su (on) and tra
(within), because they are more frequent in CNs, more polysemous and
not occurring as any other grammatical category, differently from bisyllabic
prepositions which can be used adverbially. In any case, bisyllabic preposi-
tions occurr in less than 2% of all the extracted CNs (42 out of 2298).
Exploiting part-of-speech information, we extracted all the N+P+N com-
binations with a context window of 10 words left and right. The frequency
of the CNs found in CORISnominals is reported in Table 3.
3
Prepositions which incorporate the determiner, such as ?della? (di+la, en: of the)
or ?sulla? (su+la, en: on the), although possible bisyllabic, are definite variants of their
corresponding monosyllabic prepositions, and are therefore included in the dataset.
50
Table 3: Frequency of CN types in CORISnominals
CNs extracted #inst example
N+P+N 1125 lampada a olio (oil lamp)
N+Pdet+N 1044 dorso della mano (back of the hand)
N+P+D+N 129 casa per le vacanze (holiday home)
total 2298
Annotation procedure and evaluation The annotation was performed
by a native speaker of Italian, with experience in the semantic analysis of
complex nominals. After discarding some CNs according to the rules defined
in the annotation scheme, the final number of annotated instances is 1700.
In order to assess the difficulty of the relation assignment task, a randomly
extracted portion of the data (186 CNs) was further annotated by a second
native speaker of Italian. The second annotator marked them up following
specific guidelines and some training material composed of about 50 already
annotated CNs as examples. We calculated inter-annotator agreement using
Cohen?s kappa statistics (Cohen, 1960), obtaining a kappa of .695. While
this relatively not so high value can be considered satisfactory in the field
of semantic annotation (this score is also in the same ballpark as the 70.3%
agreement reported for the SemeEval Task 4 annotation (Girju et al, 2007)),
it still indicates that the phenomenon involves a good amount of ambiguity
thus making the classification task far from straightforward. Table 4 reports
the confusion matrix for the annotated subset.
Table 4: Confusion matrix for annotator A and annotator B
A/B CE IS LL OP RR total
CE 2 ? ? ? ? 2
IS 1 22 4 5 1 33
LL ? ? 12 4 1 17
OP 34 4 8 44 6 96
RR 5 ? 1 3 29 38
total 42 26 25 56 37 186
The largest area of disagreement is in the opposition between CE and
OP: annotator B assigned the type CE to a large number of CNs which
annotator A had marked as OP. This might be due to the fact that CE
relations can be triggered by parts of objects (or features of concepts), which
are expressed by the OP relation. A prime example of such overlap is ?fumo
51
di sigaretta? (en: cigarette smoke), which can be seen both as a cause-effect
relation as well as a owner-property relation. Thus, future work will involve
a reassessment of these two categories and a revision of the guidelines.
Corpus Distribution Table 5 illustrates the distribution of semantic re-
lations across each preposition.
Table 5: Distribution of relations across prepositions in CORISnominals
prep/rel CE IS LL OP RR total
a 0 8 29 34 28 99
con 0 5 0 10 14 29
di 62 262 69 646 289 1328
da 2 0 7 18 8 35
in 0 5 50 31 14 100
per 8 2 2 29 7 48
su 3 0 18 12 11 44
tra 0 0 4 3 10 17
total 75 282 179 783 381 1700
The most striking figure is the overwhelming predominance of ?di?,
which features in 78% of all CNs. This is in line with the extremely high
overall frequency of ?di? in Italian, which is ranked as the most frequent
word in CoLFIS (an Italian frequency lexicon based on a 3M word corpus,
Laudanna et al (1995)), and also with Girju?s 2007 observation that 77.7%
of the English noun-noun compounds in her data can be rephrased as ?of?
phrases. We can also observe that some prepositions, namely ?a? and ?con?,
show more than one predominant relation usage in CNs. Overall, OP is by
far the most frequent relation, occurring in nearly half of the CNs.
As an additional observation, for each preposition we compared its fre-
quency of occurrence in CNs and in any other constructions. We found that
while ?di? and ?su? are particularly CN-oriented prepositions, both with
over 55% of their occurrences being in CNs, the others appear in CNs about
10% or less of their total occurrences.
4 Automatic identification of CN relations
We can see the problem of semantic relations in CNs from at least two (con-
verging) points of view. From a more language understanding side, given a
CN (two nouns connected by a preposition), we might want to know what the
52
Table 6: Accuracy for most frequent relation baseline and for basic system
prep #inst most freq rel baseline basic system
a 99 OP (34) 34.34 47.47
con 29 RR (14) 48.28 48.28
da 35 OP (18) 51.43 51.43
di 1328 OP (646) 48.64 56.40
in 100 LL (50) 50.00 52.00
per 48 OP (29) 60.42 60.42
tra 17 RR (10) 58.82 58.82
su 44 LL (18) 40.91 50.00
underlying semantic relation is. From a more language generation perspec-
tive, though, we might want to be able to select the appropriate preposition,
given two nouns and a relation between the concepts they express.
This translates into two different classification tasks. One where the
target categories are relations, the other where they are prepositions. In
the work we describe in this paper we concentrate on the first task. For
each preposition we build a supervised model where the target categories
correspond to the annotation tags for the semantic relations: CE, IS, LL,
OP, RR. As evaluation measures, we report accuracy and coverage. Coverage
amounts to the portion of data for which supersenses could be found for both
n1 and n2, thus providing insights in assessing the contribution of different
supersense assignment methods (see Section 4.2 and Section 4.3).
For assessing the difficulty of the task, beside inter-annotator agreement,
we take a simple baseline where we assign to each CN the semantic relation
which is most frequently associated with the CN?s preposition (Table 6).
In the learning experiments, we use the Weka implementation (Wit-
ten and Frank, 2000) of the sequential minimal optimization algorithm for
training a support vector classifier, within a ten-fold cross-validation setting.
Girju (2007) has shown SVMs to be most efficient for this task.
4.1 Basic system
The basic system uses as features only n1 and n2 as simple strings. Table 6
shows accuracy per preposition for the basic system and for the baseline.
The most evident limitation of this basic approach is data sparseness.
Out of 1700 CNs, 1662 involve a combination of n1 and n2 which occurs only
once, independently of the preposition used. The most frequent n1 (?parte?,
part) occurs 13 times with two different prepositions, and the most frequent
53
n2 (?lavoro?, job/work) 16 across four different prepositions.
One intuitive way to alleviate the data sparseness problem without in-
creasing the corpus size, is to cluster instances. Following Girju (2007), who
uses hypernyms obtained from WordNet (Fellbaum, 1998) in place of strings,
we reduce each noun in our data set to its hypernym. In this supersense
assignment, we experimented with two procedures: a more sophisticated one
involving sequential sense tagging, thus dealing with sense disambiguation,
and a simpler one involving plain assignment of hypernyms.
4.2 Hypernym selection via sense tagging
Two major problems related to finding a hypernym for a word are sense
ambiguity (one term can easily have more than one hypernym if it has
more than one sense) and coverage (even large ontologies/databases might
not include some of the encountered terms). A supersense tagger alleviates
such limitations by tagging words in context, thus tackling the ambiguity
issue, and by using a combination of features rather than just the lexical
entry, thereby being able to classify also words that are not included in the
dictionary. Picca et al (2008) have developed such a tagger for Italian,
building on an existing version for English (Ciaramita and Altun, 2006),
retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-aligned
English-Italian corpus which contains the translation of the English texts
in SemCor. The set of 26 noun supersense labels come from MultiWordNet
(Pianta et al, 2002), a multilingual lexical database in which the Italian
WordNet is strictly aligned with Princeton WordNet 1.6, and which is linked
to MultiSemCor.
The average reported performance of the tagger is about 60% (Picca
et al, 2008). This relatively low accuracy introduces a large portion of errors
in the classification, thus reducing the advantage of dealing with supersenses
rather than words in the identification of semantic relations in CNs. Errors
can be of three types: (i) the assignment of a wrong noun class, (ii) the
assignment of a class of the wrong part-of-speech type (any non-noun tag),
and (iii) the non-assignment of any class (tag ?0?). Whereas errors of type
(i) can only be spotted via manual investigation, mistakes of type (ii) and
(iii) can be detected automatically and a backoff strategy can be deployed.
In 228 CNs out of 1700 both nouns have been assigned a ?0? tag. In a
further 751 CNs, one of the two nouns is tagged as ?0?. Out of these, there
are 33 cases where the other noun is assigned a non-noun tag (adj or verb).
A non-noun tag for n1 or n2 is also found in a further 57 cases.
As a backoff strategy for all cases that fall under (ii) and (iii), we searched
54
Table 7: Results using supersenses obtained via tagging, in combination
with string features, and alone, and with and without backoff.
no backoff backoff
prep #inst cov%
acc%
#inst cov%
acc%
string nostring string nostring
a 32 32.32 68.75 75.00 99 100 45.46 44.44
con 11 37.93 72.73 63.64 29 100 62.07 58.62
da 14 40.00 64.29 57.14 35 100 65.71 65.71
di 526 39.61 58.55 51.71 1328 100 59.71 50.75
in 36 36.00 63.89 61.11 100 100 64.00 62.00
per 16 33.33 68.75 56.25 48 100 56.25 54.17
tra 10 58.82 70.00 70.00 17 100 64.71 64.71
su 20 45.45 45.00 50.00 44 100 54.54 47.73
hypernyms directly in MultiWordNet (MWN). (The set of possible hyper-
nyms is identical to the set of the 26 supersenses used by the tagger.) As a
first step, we lemmatised the string using Morph-it!, an existing lemmatiser
for Italian (Zanchetta and Baroni, 2005), since MWN contains lemmata but
not their morphological variants. Whenever we found more than one synset
associated to a term, a corresponding number of hypernyms was also found.
If one of the hypernyms was recurring more than the others, this was se-
lected. Otherwise, the hypernym associated to the first sense was selected.
4
Whenever the lemmatised noun was not in MWN (106 cases), we assigned
the most frequent supersense in the dataset (?act? for both n1 and n2).
We then ran classification experiments using the obtained supersenses
for n1 and n2 as additional features, as well as on their own (thus ignoring
the original string?this is reported as ?nostring? in Tables 7?8), both with
and without the backoff strategy. In the latter case, we excluded all CNs
where at least one of the two nouns had been tagged as a non-noun or had no
supersense assignment. Under these settings coverage was seriously affected,
but accuracy was generally higher than when deploying the backoff strategy.
Table 7 reports results.
4
Optimally, we would select the hypernym for the most frequent sense (the one ranked
first in Princeton WordNet). However, synsets for a given term are not ordered by fre-
quency in MWN. One option would be to exploit frequencies from MultiSemCor, but the
corpus is rather small and might not be very reliable.
55
Table 8: Results using supersenses obtained via plain assignment, in com-
bination with string features, and alone, and with and without backoff.
no backoff backoff
prep #inst cov%
acc%
#inst cov%
acc%
string nostring string nostring
a 90 90.91 47.78 42.22 99 100 39.39 34.34
con 26 89.65 61.54 57.69 29 100 55.17 55.17
da 30 85.71 60.00 63.33 35 100 62.86 65.71
di 1178 88.70 61.88 52.63 1328 100 60.54 51.13
in 88 88.00 50.00 52.27 100 100 56.00 53.00
per 41 85.42 65.85 60.98 48 100 56.25 47.92
tra 14 82.35 42.86 42.86 17 100 47.06 35.29
su 36 81.82 52.78 52.78 44 100 59.09 40.91
4.3 Hypernym selection via plain assignment
Given the large number of cases where we had to resort to a backoff strategy
on the tagger?s output, we tried to obtain hypernyms from MWN directly,
thus bypassing the tagging stage. Whenever necessary, we employed the
backoff strategies described above: most frequent hypernym found for an
ambiguous term (or first sense?s hypernym in case of equal frequency), and
overall most frequent assigned hypernym in the corpus (?act? in this case as
well) for all those nouns that were not found in MWN. This direct lookup
approach should improve on coverage but suffer more from ambiguity-related
problems. Table 8 summarises the results.
4.4 Discussion
Under the best settings, at full coverage, our average performance is around
59% (using tagger-assigned supersenses, backoff, the string feature), with
wide variation across prepositions. Given the currently limited set of fea-
tures, results are in general promising, especially if compared to the inter-
annotator agreement, and to previous work (see below).
When using supersenses obtained from the tagger, results are steadily
better than when using hypernyms directly looked up in MWN (both with
and without backoff) with the exception of ?di? and ?su?. The low coverage
but higher accuracy yielded when using the tagger?s senses without resorting
to a backoff strategy were both expected, as mentioned above.
Results suggest that the utility of a backoff strategy varies from one
56
preposition to another. For instance, for ?a?, ?con?, and ?per?, backoff
appears to lower performance, independently on how the supersenses were
obtained. These three prepositions had the three lowest coverage scores
when using the tagger, which suggests that if too large a proportion is left
to the approximation of backoff, the benefits of accurate sense tagging are
lost. This is not however true for the MWN lookup, where the coverage for
these three prepositions is rather high.
Additionally, we can observe that in most cases, in the back-off settings,
including the string as a feature helps improve the performance (both in the
tagging and in the plain assignment). This is likely due to the fact that the
approximation given by not having precise information about the supersense
and needing to resort to a backoff strategy is (partially) compensated by
taking into account the original noun. In contrast, using the string without
the backoff strategy on the tagger?s output yields a decrease in performance,
proving supersenses useful.
For a better assessment of the actual contribution of using hypernyms
for detecting the semantic relation without incurring in the noise introduced
by wrong hypernym assignments or the backoff strategy, we manually cor-
rected the tagger?s output in 60% of the data. This allowed us to evaluate
the tagger?s performance on supersense assignment for this 60% portion as
well as to compare on this subset, contaning 1024 CNs, an algorithm us-
ing ?gold? supersenses with that built on the tagger?s output (using the
backoff strategy, and including string features, see Section 4.2). We found
that supersenses were assigned by the tagger with an accuracy of 63.9%,
a result in line with previously reported performance (Picca et al, 2008).
We also observed that using the manually assigned hypernyms yielded an
average improvement of about seven percentage points over using the tag-
ger?s senses, although for some prepositions, instances in this smaller dataset
were just too few to draw any solid conclusion. Although more accurate,
the gold tags do not boost the performance as much as one might expect.
On the one hand, this might suggest that hypernyms can contribute only
to a certain extent to this task, and other more expressive features must be
found. On the other, it is also possible that the chosen set of 26 supersenses
is too large, especially for a dataset like ours which is rather small, thereby
not really overcoming the data sparseness problem.
Comparison to previous work in terms of performance is not straight-
forward, because of the language difference, the relation sets used, and the
evaluation settings. In the SemEval-2007 exercise, for example, for each
of the seven semantic relations used (see Table 1), a system must decide
whether a given instance expresses that relation or not within an ad hoc-
57
built dataset, so that the overall semantic relation identification of the task
is actually split in seven different binary classification tasks, one per relation.
The highest reported average accuracy is 76.3% (Girju et al, 2007).
Girju (2007) classifies noun-noun compounds in 22 different semantic
relations. Best results on English are obtained when using a rich feature
set including cross-linguistic information. Reported figures differ slightly ac-
cording to the dataset used, with an average accuracy of 76.1%. When using
only language-internal supersense features, the average accuracy is 44.15%.
Girju (2007) also trains and tests another state-of-the-art supervised model
for English, namely Semantic Scattering (Moldovan and Badulescu, 2005),
reporting an average accuracy of 59.07%.
5 Conclusions and future work
We have presented a framework for the annotation of Italian complex nom-
inals in a very high data sparseness condition, and supervised models for
the identification of the underlying semantic relation for monosyllabic Ital-
ian prepositions. We exploited both string and supersense features, showing
that the importance of including string information varies from one prepo-
sition to another and from whether we are using backoff strategies or not.
We have also seen that for obtaining the supersenses, a sequential sense tag-
ging approach yields better overall results than a simple lookup in MWN,
although it dramatically cuts on coverage.
Future work will involve further classification experiments with addi-
tional features, including web counts obtained via lexico-syntactic patterns
(Lapata and Keller, 2005; Nakov and Hearst, 2008). We will exploit part of
the annotation which we have not considered in this study (see Section 3),
namely the type of prepositional phrase (see Appendix), a very general con-
ceptual clustering which also marks metaphorical usage, the position of tra-
jector and landmark in the CN, and the order of the head and the modifier.
References
Bentivogli, L. and E. Pianta (2005). Exploiting parallel texts in the cre-
ation of multilingual semantically annotated resources: the MultiSemCor
Corpus. Natural Language Engineering 11 (3), 247?261.
Busa, F. and M. Johnston (1996). Cross-linguistic semantics for complex
nominals in the generative lexicon. In AISB Workshop on Multilinguality
in the Lexicon.
58
Celli, F. (2008). La semantica delle preposizioni italiane nella combinazione
concettuale. Master thesis in Linguistics, Universita` di Bologna.
Ciaramita, M. and Y. Altun (2006). Broad-coverage sense disambiguation
and information extraction with a supersense sequence tagger. In Pro-
ceedings of EMNLP 2006, pp. 594?602.
Cimiano, P. and J. Wenderoth (2005). Automatically learning qualia struc-
tures from the web. In Proceedings of the ACL-SIGLEX Workshop on
Deep Lexical Acquisition, Ann Arbor, Michigan, pp. 28?37.
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational
and Psychological Measurement 20, 37?46.
Downing, P. (1977). On the creation and use of English compound nouns.
Language 53, 810?842.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database. Cam-
bridge: MIT Press.
Girju, R. (2007). Improving the interpretation of noun phrases with cross-
linguistic information. In Proceedings of ACL?07, pp. 568?575.
Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret
(2007, June). SemEval-2007 Task 04: Classification of Semantic Relations
between Nominals. In Proceedings of SemEval-2007, pp. 13?18.
Johnston, M. and F. Busa (1996). Qualia structure and the compositional
interpretation of compounds. In Proceedings of the ACL Workshop on
breadth and depth of semantic lexicons.
Langacker, R.W. (1987). Foundations of cognitive grammar. Univ. Press.
Lapata, M. (2002). The disambiguation of nominalisations. Computational
Linguistics 28 (3), 357?388.
Lapata, M. and F. Keller (2005). Web-based models for natural language
processing. ACM Transactions on Speech and Language Processing 2.
Laudanna, A., A. Thornton, G. Brown, C. Burani, and L. Marconi (1995).
Un corpus dell?italiano scritto contemporaneo dalla parte del ricevente. In
S. Bolasco, L. Lebart, and A. Salem (Eds.), III Giornate internazionali
di Analisi Statistica dei Dati Testuali. Volume I, pp. 103?109. Cisu.
Lauer, M. (1995). Corpus statistics meet the noun compound: some empir-
ical results. In Proceedings of ACL?95.
Levi, J. (1978). The Syntax and Semantics of Complex Nominals. Academic
Press.
Moldovan, D. and A. Badulescu (2005). A semantic scattering model for
the automatic interpretation of genitives. In Proceedings of HLT-EMNLP
2005, pp. 891?898.
59
Nakov, P. and M. A. Hearst (2008). Solving relational similarity problems
using the web as a corpus. In Proceedings of ACL-08: HLT, Columbus,
Ohio, pp. 452?460. Association for Computational Linguistics.
Nastase, V. and S. Szpakowicz (2003). Exploring noun-modifier semantic
relations. In Proceedings of IWCS-5, pp. 285?301.
Pantel, P. and M. Pennacchiotti (2006). Espresso: Leveraging generic pat-
terns for automatically harvesting semantic relations. In Proceedings of
ACL?06, Sydney, Australia, pp. 113?120.
Pianta, E., L. Bentivogli, and C. Girardi (2002). MultiWordNet: developing
an aligned multilingual database. In Proceedings of the First International
Conference on Global WordNet, pp. 293?302.
Picca, D., A. M. Gliozzo, and M. Ciaramita (2008). Supersense Tagger for
Italian. In Proceedings of LREC 2008.
Pustejovsky, J. (1995). The Generative Lexicon. The MIT Press.
Rosario, B. and M. Hearst (2001). Classifying the semantic relations in
noun compounds via a domain-specific lexical hierarchy. In L. Lee and
D. Harman (Eds.), Proceedings of EMNLP 2001, pp. 82?90.
Rossini Favretti, R. (2000). Progettazione e costruzione di un corpus di ital-
iano scritto: CORIS/CODIS. In R. Rossini Favretti (Ed.), Linguistica e
informatica. Multimedialita`, corpora e percorsi di apprendimento. Bulzoni.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees.
In Proc. of the Conference on New Methods in Language Processing, 44-49.
Turney, P. D. (2006). Expressing implicit semantic relations without super-
vision. In Proceedings of ACL?06, Sydney, Australia, pp. 313?320.
Warren, B. (1978). Semantic patterns of noun-noun compounds. Gothenburg
Studies in English 41.
Witten, I. H. and E. Frank (2000). Data Mining: Practical Machine Learning
Tools and Techniques with Java Implementations. Morgan Kaufmann.
Zanchetta, E. and M. Baroni (2005). Morph-it! a free corpus-based morpho-
logical resource for the italian language. Corpus Linguistics 2005 1 (1).
Zingarelli, N. (2008). Lo Zingarelli 2008. Vocabolario della Lingua Italiana.
Zanichelli.
60
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 198?201,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UNITN: Part-Of-Speech Counting in Relation Extraction
Fabio Celli
University of Trento
Italy
fabio.celli@unitn.it
Abstract
This report describes the UNITN system, a
Part-Of-Speech Context Counter, that par-
ticipated at Semeval 2010 Task 8: Multi-
Way Classification of Semantic Relations
Between Pairs of Nominals. Given a text
annotated with Part-of-Speech, the system
outputs a vector representation of a sen-
tence containing 20 features in total. There
are three steps in the system?s pipeline:
first the system produces an estimation of
the entities? position in the relation, then
an estimation of the semantic relation type
by means of decision trees and finally it
gives a predicition of semantic relation
plus entities? position. The system ob-
tained good results in the estimation of en-
tities? position (F1=98.3%) but a critically
poor performance in relation classification
(F1=26.6%), indicating that lexical and se-
mantic information is essential in relation
extraction. The system can be used as an
integration for other systems or for pur-
poses different from relation extraction.
1 Introduction and Background
This technical report describes the UNITN system
(a Part-Of-Speech Context Counter) that partici-
pated to Semeval 2010 Task 8: Multi-Way Clas-
sification of Semantic Relations Between Pairs of
Nominals (see Hendrickx et al, 2009). A different
version of this system based on Part-Of-Speech
counting has been previously used for the auto-
matic annotation of three general and separable se-
mantic relation classes (taxonomy, location, asso-
ciation) obtaining an average F1-measure of 0.789
for english and 0.781 for italian, see Celli 2010
for details. The organizers of Semeval 2010 Task
8 provided ten different semantic relation types in
context, namely:
? Cause-Effect (CE). An event or object leads
to an effect. Example: Smoking causes can-
cer.
? Instrument-Agency (IA). An agent uses an
instrument. Example: Laser printer.
? Product-Producer (PP). A producer causes
a product to exist. Example: The growth hor-
mone produced by the pituitary gland.
? Content-Container (CC). An object is phys-
ically stored in a delineated area of space,
the container. Example: The boxes contained
books.
? Entity-Origin (EO). An entity is coming or
is derived from an origin (e.g., position or
material). Example: Letters from foreign
countries.
? Entity-Destination (ED). An entity is mov-
ing towards a destination. Example: The boy
went to bed.
? Component-Whole (CW). An object is a
component of a larger whole. Example: My
apartment has a large kitchen.
? Member-Collection (MC). A member forms
a nonfunctional part of a collection. Exam-
ple: There are many trees in the forest.
? Message-Topic (CT). An act of communica-
tion, whether written or spoken, is about a
topic. Example: The lecture was about se-
mantics.
? Other. The entities are related in a way that
do not fall under any of the previous men-
tioned classes. Example: Batteries stored in
a discharged state are susceptible to freezing.
198
The task was to predict, given a sentence and two
marked-up entities, which one of the relation la-
bels to apply and the position of the entities in the
relation (except from ?Other?). An example is re-
ported below:
??The <e1>bag</e1>
contained <e2>books</e2>,
a cell phone and notepads,
but no explosives.??
Content-Container(e2,e1)
The task organizers also provided 8000 sentences
for training and 2717 sentences for testing. Part
of the task was to discover whether it is better to
predict entities? position before semantic relation
or viceversa.
In the next section there is a description of the
UNITN system, in section 3 are reported the re-
sults of the system on the dataset provided for Se-
meval Task 8, in section 4 there is the discussion,
then some conclusions follow in section 5.
2 System Description
UNITN is a Part-Of-Speech Context Counter.
Given as input a plain text with Part-Of-Speech
and end-of-sentence markers annotated it outputs
a numerical feature vector that gives a representa-
tion of a sentence. For Part-Of-Speech and end-of-
sentence annotation I used Textpro, a tool for NLP
that showed state-of-the-art performance for POS
tagging (see Pianta et al, 2008). The POS tagset
is the one used in the BNC, described at http:
//pie.usna.edu/POScodes.html.
Features in the vector can be tailored for specific
tasks, in this case 20 features were used in total.
They are:
1. Number of prepositions in sentence.
2. Number of nouns and proper names in sen-
tence.
3. Number of lexical verbs in sentence.
4. Number of ?be? verbs in sentence.
5. Number of ?have? verbs in sentence.
6. Number of ?do? verbs in sentence.
7. Number of modal verbs in sentence.
8. Number of conjunctions in sentence.
9. Number of adjectives in sentence.
10. Number of determiners in sentence.
11. Number of pronouns in sentence.
12. Number of punctuations in sentence.
13. Number of negative particles in sentence.
14. Number of words in the context between the
first and the second entity.
15. Number of verbs in the context between the
first and the second entity.
16. patterns (from, in, on, by, of, to).
17. POS of entity 1 (noun, adjective, other).
18. POS of entity 2 (noun, adjective, other).
19. Estimate of entities? position in the relation
(e1-e2, e2-e1, 00).
20. Estimate of semantic relation (relations de-
scribed in section 1 above).
Prepositional patterns in feature 16 were chosen
for their high cooccurrence frequency with a se-
mantic relation type and their low cooccurrence
with the other ones.
The system works in three steps: in the first one
features 1-18 are used for predicting feature 19,
in the second one features 1-19 are used for pre-
dicting feature 20. In the third step, after the ap-
plication of Hall 1998?s attribute selection filter
(that evaluates the worth of a subset of attributes
by considering the individual predictive ability of
each feature along with the degree of redundancy
between them) features 12, 14, 16, 19 and 20 are
used for the prediction of semantic relation plus
entities? position (19 relations in total).
For all the steps I used C4.5 decision trees (see
Quinlan 1993) and Cohen 1995?s RIPPER algo-
rithm (Repeated Incremental Pruning to Produce
Error Reduction). Evaluation for steps 1, 2 and 3
have been run on the training set, with a 10-fold
cross-validation, since the test set was relased in
a second time. Results of evaluation of step 1, 2
and 3 are reported in table 1 below, chance values
(100/number of classes) are taken as baselines, all
experiments have been run in Weka (see Witten
and Frank, 2005).
I also inverted step 1 and 2 for predicting seman-
199
Prediction Baseline average F1
step 1 33.33% 98.3%
step 2 10% 29.8%
step 3 5.26% 28.1%
Table 1: Evaluation for steps 1, 2 and 3.
tic relation estimate before entities? position esti-
mate and the average F1-measure is even worse
(0.271), demonstrating that entities? position esti-
mate has a positive weight on semantic relation es-
timate. There are instead some problems with step
2, and I will return on this later in the discussion
(section 4).
3 Results
As it was requested by the task, the system has
been run 4 times in the testing phase: the first time
(r1) using 1000 examples from the training set for
building the model, the second time (r2) 2000 ex-
amples, the third (r3) 4000 example and the last
one (r4) using the entire training set.
The results obtained by UNITN in the competi-
tion are not good, overall performance is poor, es-
pecially for some relations, in particular Product-
Producer and Message-Topic. The best perfor-
mance is achieved by the Member-Collection re-
lation (47.30% ), that changed from 0% in the first
run to 42.71% in the second one. Scores are re-
ported, relation by relation, in table 2 below, the
discussion follows in section 4.
Rel F1 (r1) F1 (r2) F1 (r3) F1 (r4)
CE 23.08% 17.24% 22.37% 26.86%
CW 13.64% 0.00% 13.85% 25.23%
CC 26.43% 25.36% 26.72% 28.39%
ED 37.26% 37.25% 46.27% 46.35%
EO 36.60% 36.49% 37.61% 41.79%
IA 10.68% 7.95% 5.59% 17.32%
MC 0.00% 42.71% 43.08% 47.30%
CT 1.48% 0.00% 4.93% 6.81%
PP 0.00% 0.00% 1.67% 0.00%
Other 27.14% 26.15% 25.80% 20.64%
avg* 16.57% 18.56% 22.45% 26.67%
Table 2: Results. *Macro average excuding
?Other?.
4 Discussion
On the one hand the POSCo system showed an
high performance in step 1 (entities? position
detection), indicating that the numerical sentence
representation obtained by means of Part-Of-
Speech can be a good way for extracting syntactic
information.
On the other hand the POSCo system proved
not to be good for the classification of semantic
relations. This clearly indicates that lexical and
semantic information is essential in relation
extraction. This fact is highlighted also by the
attribute selection filter algorithm that choosed,
among others, feature 16 (prepositional patterns),
which was the only attribute providing lexical
information in the system.
It is interesting to note that it chose feature
12 (punctuation) and 14 (number of words in
the context between the first and the second
entity). Punctuation can be used to provide, to
a certain level, information about how much
the sentence is complex (the higher the number
of the punctuation, the higher the subordinated
phrases), while feature 14 provides information
about the distance between the related entities and
this could be useful for the classification between
presence or absence of a semantic relation (the
longer the distance, the lower the probability to
have a relation between entities) but it is useless
for a multi-way classification with many semantic
relations, like in this case.
5 Conclusions
In this report we have seen that Part-Of-Speech
Counting does not yield good performances in re-
lation extraction. Despite this it provides some
information about the complexity of the sentence
and this can be useful for predicting the position
of the entities in the relation. The results confirm
the fact that lexical and semantic information is
essential in relation extraction, but also that there
are some useful non-lexical features, like the com-
plexity of the sentence and the distance between
the first and the second related entities, that can be
used as a complement for systems based on lexical
and semantic resources.
200
References
Fabio Celli. 2010. Automatic Semantic Relation
Annotation for Italian and English. (technical report
available at http://clic.cimec.unitn.
it/fabio).
William W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the 12th International Conference
on Machine Learning. Lake Tahoe, CA.
Mark A. Hall. 1998. Correlation-based Feature
Selection for Discrete and Numeric Class Ma-
chine Learning. Technical report available
at http://citeseerx.ist.psu.edu/
viewdoc/download?doi=10.1.1.148.
6025&rep=rep1&type=pdf.
Iris Hendrickx and Su Nam Kim and Zornitsa Kozareva
and Preslav Nakov and Diarmuid
?
O S?eaghdha
and Sebastian Pad?o and Marco Pennacchiotti and
Lorenza Romano and Stan Szpakowicz. 2010.
SemEval-2010 Task 8: Multi-Way Classification of
Semantic Relations Between Pairs of Nominals. In
Proceedings of the 5th SIGLEX Workshop on Se-
mantic Evaluation, Uppsala, Sweden.
Emanuele Pianta and Christian Girardi and Roberto
Zanoli. 2008. The TextPro tool suite. In Proceedings
of LREC, Marrakech, Morocco.
John Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Ian H. Witten and Eibe Frank. 2005. Data Mining.
Practical Machine Learning Tools and Techniques
with Java implementations. Morgan and Kaufman,
San Francisco, CA.
201
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?17,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
The Role of Emotional Stability in Twitter Conversations
Fabio Celli
CLIC-CIMeC
University of Trento
fabio.celli@unitn.it
Luca Rossi
LaRiCA
University of Urbino
luca.rossi@uniurb.it
Abstract
In this paper, we address the issue of how
different personalities interact in Twitter. In
particular we study users? interactions using
one trait of the standard model known as the
?Big Five?: emotional stability. We collected
a corpus of about 200000 Twitter posts and
we annotated it with an unsupervised person-
ality recognition system. This system exploits
linguistic features, such as punctuation and
emoticons, and statistical features, such as fol-
lowers count and retweeted posts. We tested
the system on a dataset annotated with per-
sonality models produced from human judge-
ments. Network analysis shows that neurotic
users post more than secure ones and have the
tendency to build longer chains of interacting
users. Secure users instead have more mutual
connections and simpler networks.
1 Introduction and Background
Twitter is one of the most popular micro-blogging
web services. It was founded in 2006, and allows
users to post short messages up to 140 characters of
text, called ?tweets?.
Following the definition in Boyd and Ellison
(2007), Twitter is a social network site, but is shares
some features with blogs. Zhao and Rosson (2009)
highlights the fact that people use twitter for a va-
riety of social purposes like keeping in touch with
friends and colleagues, raising the visibility of their
interests, gathering useful information, seeking for
help and relaxing. They also report that the way
people use Twitter can be grouped in three broad
classes: people updating personal life activities,
people doing real-time information and people fol-
lowing other people?s RSS feeds, which is a way to
keep informed about personal intersts.
According to Boyd et al (2010), there are many
features that affect practices and conversations in
Twitter. First of all, connections in Twitter are di-
rected rather than mutual: users follow other users?
feeds and are followed by other users. Public mes-
sages can be addressed to specific users with the
symbol @. According to Honeycutt and Herring
(2009) this is used to reply to, to cite or to include
someone in a conversation. Messages can be marked
and categorized using the ?hashtag? symbol #, that
works as an aggregator of posts having something
in common. Another important feature is that posts
can be shared and propagated using the ?retweet?
option. Boyd et al (2010) emphasize the fact that
retweeting a post is a means of participating in a dif-
fuse conversation. Moreover, posts can be marked as
favorites and users can be included into lists. Those
practices enhance the visibility of the posts or the
users.
In recent years the interest towards Twitter raised
in the scientific community, especially in Informa-
tion Retrieval. For example Pak and Paroubek
(2010) developed a sentiment analysis classifier
from Twitter data, Finin et al (2010) performed
Named Entity Recognition on Twitter using crowd-
sourcing services such as Mechanical Turk1 and
CrowdFlower2, and Zhao et al (2011) proposed a
ranking algorithm for extracting topic keyphrases
from tweets. Of course also in the personality recog-
1https://www.mturk.com/mturk/welcome
2http://crowdflower.com
10
nition field there is a great interest towards the anal-
ysis of Twitter. For example Quercia et al (2011)
analyzed the correlations between personality traits
and the behaviour of four types of users: listeners,
popular, hi-read and influential.
In this paper, we describe a personality recog-
nition tool we developed in order to annotate data
from Twitter and we analyze how emotional stabil-
ity affects interactions in Twitter. In the next sec-
tion, given an overview of personality recognition
and emotional stability, we will describe our person-
ality recognition system in detail and we present the
dataset we collected from Twitter. In the last two
sections we report and discuss the results of the ex-
periment and we provide some provisional conclu-
sions.
2 Personality Recognition
2.1 Definition of Personality and Emotional
Stability
Personality is a complex of attributes that charac-
terise a unique individual. Psychologists, see for ex-
ample Goldberg (1992), formalize personality along
five traits known as the ?Big Five?, a model intro-
duced by Norman (1963) that has become a stan-
dard over the years. The five traits are the following:
Extraversion (sociable vs shy); Emotional stabil-
ity (calm vs insecure); Agreeableness (friendly vs
uncooperative); Conscientiousness (organized vs
careless); Openness (insightful vs unimaginative).
Among all the 5 traits, emotional stability plays a
crucial role in social networks. Studying offline so-
cial networks, Kanfer and Tanaka (1993) report that
secure (high emotional stability) subjects had more
people interacting with them. Moreover, Van Zalk et
al. (2011) reports that youths who are socially anx-
ious (low emotional stability) have fewer friends in
their network and tend to choose friends who are so-
cially anxious too. We will test if it is true also in
online social networks.
2.2 Previous Work and State of the Art
Computational linguistics community started to pay
attention to personality recognition only recently.
A pioneering work by Argamon et al (2005) clas-
sified neuroticism and extraversion using linguistic
features such as function words, deictics, appraisal
expressions and modal verbs. Oberlander and Now-
son (2006) classified extraversion, emotional sta-
bility, agreeableness and conscientiousness of blog
authors? using n-grams as features. Mairesse et al
(2007) reported a long list of correlations between
big5 personality traits and 2 feature sets, one from
linguistics (LIWC, see Pennebaker et al (2001) for
details) and one from psychology (RMC, see Colt-
heart (1981)). Those sets included features such
as punctuation, length and frequency of words used.
They obtained those correlations from psychologi-
cal factor analysis on a corpus of Essays (see Pen-
nebaker and King (1999) for details) annotated with
personality, and developed a supervisd system for
personality recognition available online as a demo3.
In a recent work, Iacobelli et al (2011) tested dif-
ferent feature sets, extracted from a corpus of blogs,
and found that bigrams and stop words treated as
boolean features yield very good results. As is stated
by the authors themselves, their model may overfit
the data, since the n-grams extracted are very few in
a very large corpus. Quercia et al (2011) predicted
personality scores of Twitter users by means of
network statistics like following count and retweet
count, but they report root mean squared error, not
accuracy. Finally Golbeck et al (2011) predicted the
personality of 279 users from Facebook using either
linguistic. such as word and long-word count, and
extralinguistic features, such as friend count and the
like. The State-of-the-art in personality recognition
E.Stab. Arg05 Ob06 Mai07 Ia11 Gol11
acc 0.581 0.558 0.573 0.705 0.531
Table 1: State-of-the-Art in Personality Recognition from
language for the emotional stability trait.
is reported in table 1. Argamon (Arg05) and Ober-
lander (Ob06) use naive bayes, Mairesse (Mai07)
and Iacobelli (Ia11) use support vector machines and
Golbeck (Gol11) uses M5 rules with a mix of lin-
guistic and extralinguistic features.
2.3 Description of the Unsupervised
Personality Recognition Tool
Given a set of correlations between personality traits
and some linguistic or extralinguistic features, we
3http://people.csail.mit.edu/francois/research/personality/
demo.html
11
are able to develop a system that builds models of
personality for each user in a social network site
whose data are publicly available. In our system per-
sonality models can take 3 possible values: secure
(s), neurotic (n) and omitted/balanced (o), indicat-
ing that a user do not show any feature or shows both
the features of a neurotic and a secure user in equal
measure. Many scholars provide sets of correlations
between some cues and the traits of personality for-
malized in the big5. In our system we used a fea-
ture set taken partly from Mairesse et al (2007) and
partly from Quercia et al (2011). The former pro-
vides a long list of linguistic cues that correlate with
personality traits in English. The latter provides the
correlations between personality traits and the count
of following, followers, listed and retweeted.
We selected the features reported in table 2, since
they are the most frequent in the dataset for which
we have correlation coefficients with emotional sta-
bility.
Features Corr. to Em. Stab. from
exclam. marks -.05* Mai07
neg. emot. -.18** Mai07
numbers .05* Mai07
pos. emot. .07** Mai07
quest. marks -.05* Mai07
long words .06** Mai07
w/t freq. .10** Mai07
following -.17** Qu11
followers -.19** Qu11
retweeted -.03* Qu11
Table 2: Features used in the system and their Pearson?s
correlation coefficients with personality traits as reported
in Mairesse et al (2007) and Quercia et al (2011). * = p
smaller than .05 (weak correlation), ** = p smaller than
.01 (strong correlation)
Exclamation marks: the count of ! in a post;
negative emoticons: the count of emoticons ex-
pressing negative feelings in a post; numbers: the
count of numbers in the post; positive emoticons:
the count of emoticons expressing positive feelings
in a post; question marks: the count of ? in a post;
long words: count of words longer than 6 charac-
ters in the post; word/token frequency: frequency
of repeated words in a post, defined as
wt =
repeated words
post word count
following count: the count of users followed; fol-
lowers count: the count of followers; retweeted
count: the amount of user?s posts retweeted.
The processing pipeline, as shown in figure 1, is
divided in three steps: preprocess, process and eval-
uation.
Figure 1: Unsupervised Personality Recognition System
pipeline.
In the preprocessing phase the system randomly
samples a predefined number of posts (in this case
2000) in order to capture the average occurrence of
each feature. In the processing phase the system
generates one personality model per post matching
features and applying correlations. If the system
finds feature values above the average, it increments
or decrements the score associated to emotional sta-
bility, depending on a positive or negative correla-
tion. The list of all features used and their correla-
tions with personality traits provided by Mairesse et
al. (2007) (Mai07) and Quercia et al (2011) (Qu11),
is reported in table 2.
In order to evaluate the personality models gen-
erated, the system compares all the models gener-
ated for each post of a single user and retrieves one
model per user. This is based on the assumption that
12
one user has one and only one complex personality,
and that this personality emerges at a various levels
from written text, as well as from other extralinguis-
tic cues. The system provides confidence and vari-
ability as evaluation measures. Confidence gives a
measure of the consistency of the personality model.
It is defined as
c =
tp
M
where tp is the amount of personality models (for
example ?s? and?s?, ?n? and ?n?), matching while
comparing all posts of a user and M is the amount
of the models generated for that user. Variability
gives information about how much one user tends
to write expressing the same personality traits in all
the posts. It is defined as
v =
c
P
where c is confidence score and P is the count of
all user?s posts. The system can evaluate personal-
ity only for users that have more than one post, the
other users are discarded.
Our personality recognition system is unsuper-
vised. This means that it exploits correlations in or-
der to build models and does not require previously
annotated data to modelize personality. Since the
evaluation is performed directly on the dataset we
need to test the system before using it. In the fol-
lowing section we describe how we tested system?s
performance.
2.4 Testing the Unsupervised Personality
Recognition Tool
We run two tests, the first one to evaluate the accu-
racy in predicting human judges on personality, and
the second one to evaluate the performance of the
system on Twitter data. In the first one, we com-
pared the results of our system on a dataset, called
Personage (see Mairesse and Walker (2007)), an-
notated with personality ratings from human judges.
Raters expressed their judgements on a scale from 1
(low) to 7 (high) for each of the Big Five personal-
ity traits on English sentences. In order to obtain a
gold standard, we converted this scale into our three-
values scheme applying the following rules: if value
is greater or equal to 5 then we have ?s?, if value is
4 we have ?o? and if value is smaller or equal to 3
we have ?n?. We used a balanced set of 8 users (20
sentences per user), we generated personality mod-
els automatically and we compared them to the gold
standard. We obtained an accuracy of 0.625 over a
majority baseline of 0.5, which is in line with the
state of the art.
In the second test we compared the output of our
system to the score of Analyzewords4, an online tool
for Twitter analysis based on LIWC features (see
Pennebaker et al (2001)). This tool does not provide
big5 traits but, among others, it returns scores for
?worried? and ?upbeat?, and we used those classes
to evaluate ?n? and ?s? respectively. We randomly
extracted 18 users from our dataset (see section 3 for
details), 10 neurotics and 8 secure, and we manually
checked whether the classes assigned by our system
matched the scores of Analyzewords. Results, re-
p r f1
n 0.8 0.615 0.695
s 0.375 0.6 0.462
avg 0.587 0.607 0.578
Table 3: Results of test 2.
ported in table 3, reveal that our system has a good
precision in detecting worried/neurotic users. The
bad results for upbeat/secure users could be due to
the fact that the class ?upbeat? do not correspond
perfectly to the ?secure? class. Overall the perfor-
mance of our system is in line with the state of the
art.
3 Collection of the Dataset
The corpus, called ?Personalitwit2?, was collected
starting from Twitter?s public timeline5. The sam-
pling procedure is depicted in figure 2.
We sampled data from December 25th to 28th,
2011 but most of the posts have a previous post-
ing date since we also collected data from user
pages, where 20 recent tweets are displayed in re-
verse chronological order. For each public user,
sampled from the public timeline, we collected the
nicknames of the related users, who had a conver-
sation with the public users, using the @ symbol.
We did this in order to capture users that are in-
cluded in social relationships with the public users.
4http://www.analyzewords.com/index.php
5http://twitter.com/public timeline
13
Figure 2: Data sampling pipeline.
We excluded from sampling all the retweeted posts
because they are not written by the user themselves
and could affect linguistic-based personality recog-
nition. The dataset contains all the following in-
formation for each post: username; text; post date;
user type (public user or related user); user retweet
count; user following count; user followers count;
user listed count; user favorites count; total tweet
count; user page creation year; time zone; related
users (users who replied to the sampled user); reply
score (rp), defined as
rp =
page reply count
page post count
and retweet score (rt), defined as
rt =
page retweet count
page post count
min median mean max
tweets 3 5284 12246 582057
following 0 197 838 320849
followers 0 240 34502 17286123
listed 0 1 385 539019
favorites 0 7 157 62689
Table 4: Summary of Personalitwit2.
Figure 3: Frequency distribution of users per language.
From the top: Arabic, Bahasa, Chinese, Czech, Dutch,
English, French, German, Greek, Hebrew, Hindi, Italian,
Japanese, Korean, Malay, Norwegian, Portuguese, Rus-
sian, Slovene, Spanish, Swedish, Thai, Turkish, Uniden-
tified.
In the corpus there are 200000 posts, more than
13000 different users and about 7800 ego-networks,
where public users are the central nodes and re-
lated users are the edges. We annotated the corpus
with our personality recognition system. The aver-
age confidence is 0.601 and the average variability
is 0.049. A statistical summary of the data we col-
lected is reported in table 4, the distribution of users
per language is reported in figure 3. We kept only
English users (5392 egonetworks), discarding all the
other users.
4 Experiments and Discussion
Frequency distribution of emotional stability trait in
the corpus is as follows: 56.1% calm users, 39.2%
neurotic users and 4.7% balanced users.
We run a first experiment to check whether neu-
rotic or calm users tend to have conversations with
other users with the same personality trait. To this
purpose we extracted all the ego-networks anno-
tated with personality. We automatically extracted
14
Figure 4: Relationships between users with the same per-
sonality traits.
the trait of the personality of the ?public-user? (the
center of the network) and we counted how many
edges of the ego-network have the same personal-
ity trait. The users in the ego-network are weighted:
this means that if a ?public-user? had x conversa-
tions with the same ?related-user?, it is counted x
times. The frequency is defined as
freq =
trait count
egonetwork nodes count
where the same trait is between the public-user and
the related users. The experiment, whose results are
reported in figure 4, shows that there is a general
tendency to have conversations between users that
share the same traits.
We run a second experiment to find which person-
ality type is most incline to tweet, to retweet and to
reply. Results, reported in figure 5, show that neu-
rotic users tend to post and to retweet more than sta-
ble users. Stable users are slightly more inclined to
reply with respect to neurotic ones.
In order to study if conversational practices
among users with similar personality traits might
generate different social structure, we applied a so-
cial network analysis to the collected data through
the use of the Gephi software6. We analysed sepa-
rately the network of interactions between neurotic
users (n) and calm users (s) to point out any person-
ality related aspect of the emerging social structure.
Visualisations are shown in figure 6.
Due to the way in which data have been acquired
6http://www.gephi.org
Figure 5: Relationships between emotional stability and
Twitter activity.
- starting from the users randomly displayed on the
Twitter public timeline - there is a large number of
scattered networks made of few interactions. Never-
theless the extraction of the ego networks allowed
us to detect a rather interesting phenomena: neu-
rotic users seem to have the tendency to build longer
chains of interacting users while calm users have the
tendency to build mutual connections.
The average path length value of neurotic users
is 1.551, versus the average path length measured
on the calm users of 1.334. This difference results
in a network diameter of 6 for the network made of
only neurotic users and of 5 for the network made
15
Figure 6: Social structures of stable (s) and neurotic (n)
users.
of secure users. A single point of difference in the
network diameter produces a neurotic network much
more complex than the calm network. While this
difference might be overlooked in large visualisa-
tions due to the presence of many minor clusters of
nodes it becomes evident when we focus only on the
giant component of the two networks in figure 7.
The giant components are those counting the ma-
jor part of nodes and can be used as an exam-
ple of the most complex structure existing within
a network. As it should appear clear neurotic net-
work contains more complex interconnected struc-
tures than calm network even if, as we claimed be-
fore, have on average smaller social networks.
5 Conclusions and Future Work
In this paper, we presented an unsupervised system
for personality recognition and we applied it suc-
Figure 7: Giant components of stable (s) and neurotic (n)
users.
cessfully on a quite large and richly annotated Twit-
ter dataset. Results confirm some offline psycholog-
ical findings in the social networks online, for ex-
ample the fact that neurotic people tend to choose
friends who are also neurotic.
We also confirm the fact that neurotic users have
smaller social networks at the level of a single user,
but they tend to build longer chains. This means
that a tweet propagated in ?neurotic networks? has
higher visibility. We also found that neurotic users
have the highest posting rate and retweet score.
In the future we should change the sampling set-
tings in order to capture larger networks. It would be
also very interesting to explore how other person-
ality traits affect user?s behaviour. To this purpose
we need to improve the personality recognition sys-
tem and we would benefit from topic identification,
which is another growing field of research.
16
References
Amichai-Hamburger, Y. and Vinitzky, G. 2010. Social
network use and personality. In Computers in Human
Behavior. 26(6). pp. 1289?1295.
Argamon, S., Dhawle S., Koppel, M., Pennebaker J. W.
2005. Lexical Predictors of Personality Type. In Pro-
ceedings of Joint Annual Meeting of the Interface and
the Classification Society of North America. pp. 1?16.
Bastian M., Heymann S., Jacomy M. 2009. Gephi: an
open source software for exploring and manipulating
networks. In Proceedings of International AAAI Con-
ference on Weblogs and Social Media. pp. 1?2.
Boyd, D. Golder, S. and Lotan, G. 2010. Tweet, Tweet,
Retweet: Conversational Aspects of Retweeting on
Twitter. In Proceedings of HICSS-43. pp. 1?10.
Boyd, D. and Ellison, N. 2007. Social Network Sites:
Definition, history, and scholarship. In Journal of
Computer-Mediated Communication 13(1). pp. 210?
230.
Celli, F., Di Lascio F.M.L., Magnani, M., Pacelli, B., and
Rossi, L. 2010. Social Network Data and Practices:
the case of Friendfeed. Advances in Social Comput-
ing, pp. 346?353. Series: Lecture Notes in Computer
Science, Springer, Berlin.
Coltheart, M. 1981. The MRC psycholinguistic database.
In Quarterly Journal of Experimental Psychology,
33A, pp. 497?505.
Finin, T., Murnane, W., Karandikar, A., Keller, N., Mar-
tineau, J., Dredze, M. 2010. Annotating named entities
in Twitter data with crowdsourcing. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk
(CSLDAMT ?10). pp. 80?88.
Golbeck, J. and Robles, C., and Turner, K. 2011. Predict-
ing Personality with Social Media. In Proceedings of
the 2011 annual conference extended abstracts on Hu-
man factors in computing systems, pp. 253?262.
Golbeck, J. and Hansen, D.,L. 2011. Computing political
preference among twitter followers. In Proceedings of
CHI 2011: pp. 1105?1108.
Goldberg, L., R. The Development of Markers for the Big
Five factor Structure. 1992. In Psychological Assess-
ment, 4(1). pp. 26?42.
Honeycutt, C., and Herring, S. C. 2009. Beyond mi-
croblogging: Conversation and collaboration via Twit-
ter. In Proceedings of the Forty-Second Hawaii Inter-
national Conference on System Sciences. pp 1?10.
Kanfer, A., Tanaka, J.S. 1993. Unraveling the Web of
Personality Judgments: The Inuence of Social Net-
works on Personality Assessment. Journal of Person-
ality, 61(4) pp. 711?738.
Iacobelli, F., Gill, A.J., Nowson, S. Oberlander, J. Large
scale personality classification of bloggers. 2011. In
Lecture Notes in Computer Science (6975), pp. 568?
577.
Mairesse, F., and Walker, M.. PERSONAGE: Personality
Generation for Dialogue. 2007. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pp.496?503.
Mairesse, F. and Walker, M. A. and Mehl, M. R., and
Moore, R, K. 2007. Using Linguistic Cues for the
Automatic Recognition of Personality in Conversation
and Text. In Journal of Artificial intelligence Research,
30. pp. 457?500.
Norman, W., T. 1963. Toward an adequate taxonomy of
personality attributes: Replicated factor structure in
peer nomination personality rating. In Journal of Ab-
normal and Social Psychology, 66. pp. 574?583.
Oberlander, J., and Nowson, S. 2006. Whose thumb is
it anyway? classifying author personality from we-
blog text. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics ACL.
pp. 627?634.
Pak, A., Paroubek P. 2010. Twitter as a corpus for senti-
ment analysis and opinion mining. In Proceedings of
LREC 2010. pp. 1320?1326.
Pennebaker, J. W., King, L. A. 1999. Linguistic styles:
Language use as an individual difference. In Journal
of Personality and Social Psychology, 77, pp. 1296?
1312.
Pennebaker, J. W., Francis, M. E., Booth, R. J. 2001.
Inquiry and Word Count: LIWC 2001. Lawrence Erl-
baum, Mahwah, NJ.
Platt, J. 1998. Machines using Sequential Minimal Op-
timization. In Schoelkopf, B., Burges, C., Smola, A.
(ed), Advances in Kernel Methods, Support Vector
Learning. pp. 37?49.
Quercia, D. and Kosinski, M. and Stillwell, D., and
Crowcroft, J. 2011. Our Twitter Profiles, Our Selves:
Predicting Personality with Twitter. In Proceedings of
SocialCom2011. pp. 180?185.
Van Zalk, N., Van Zalk, M., Kerr, M. and Stattin, H. 2011.
Social Anxiety as a Basis for Friendship Selection and
Socialization in Adolescents? Social Networks. Jour-
nal of Personality, 79: pp. 499?526.
Zhao, D., Rosson, M.B. 2009. How and why people Twit-
ter: The role that micro-blogging plays in informal
communication at work. In Proceedings of GROUP
2009 pp. 243?252.
Zhao, W.X., Jiang, J., He, J., Song, Y., Achananuparp, P.,
Lim, E.P., Li, X. 2011. Topical keyphrase extraction
from Twitter. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1 (HLT ?11).
pp. 379?388.
17
