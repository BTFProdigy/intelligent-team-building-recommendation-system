Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917?927,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Polynomial-Time Dynamic Oracle
for Non-Projective Dependency Parsing
Carlos G
?
omez-Rodr??guez
Departamento de
Computaci?on
Universidade da Coru?na, Spain
cgomezr@udc.es
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
The introduction of dynamic oracles has
considerably improved the accuracy of
greedy transition-based dependency pars-
ers, without sacrificing parsing efficiency.
However, this enhancement is limited to
projective parsing, and dynamic oracles
have not yet been implemented for pars-
ers supporting non-projectivity. In this
paper we introduce the first such oracle,
for a non-projective parser based on At-
tardi?s parser. We show that training with
this oracle improves parsing accuracy over
a conventional (static) oracle on a wide
range of datasets.
1 Introduction
Greedy transition-based parsers for dependency
grammars have been pioneered by Yamada and
Matsumoto (2003) and Nivre (2003). These meth-
ods incrementally process the input sentence from
left to right, predicting the next parsing action,
called transition, on the basis of a compact rep-
resentation of the derivation history.
Greedy transition-based parsers can be very
efficient, allowing web-scale parsing with high
throughput. However, the accuracy of these meth-
ods still falls behind that of transition-based pars-
ers using beam-search, where the accuracy im-
provement is obtained at the cost of a decrease
in parsing efficiency; see for instance Zhang and
Nivre (2011), Huang and Sagae (2010), Choi and
McCallum (2013). As an alternative to beam-
search, recent research on transition-based parsing
has therefore explored possible ways of improving
accuracy at no extra cost in parsing efficiency.
The training of transition-based parsers relies
on a component called the parsing oracle, which
maps parser configurations to optimal transitions
with respect to a gold tree. A discriminative model
is then trained to simulate the oracle?s behavior,
and is later used for decoding. Traditionally, so-
called static oracles have been exploited in train-
ing, where a static oracle is defined only for con-
figurations that have been reached by computa-
tions with no mistake, and it returns a single ca-
nonical transition among those that are optimal.
Very recently, Goldberg and Nivre (2012),
Goldberg and Nivre (2013) and Goldberg et al.
(2014) showed that the accuracy of transition-
based parsers can be substantially improved using
dynamic oracles. A dynamic oracle returns the
set of all transitions that are optimal for a given
configuration, with respect to the gold tree, and
is well-defined and correct for every configuration
that is reachable by the parser.
Na??ve implementations of dynamic oracles run
in exponential time, since they need to simulate
all possible computations of the parser for the in-
put configuration. Polynomial-time implementa-
tions of dynamic oracles have been proposed by
the above mentioned authors for several project-
ive dependency parsers. To our knowledge, no
polynomial-time algorithm has been published for
transition-based parsers based on non-projective
dependency grammars.
In this paper we consider a restriction of a
transition-based, non-projective parser originally
presented by Attardi (2006). This restriction
was further investigated by Kuhlmann and Nivre
(2010) and Cohen et al. (2011). We provide an im-
plementation for a dynamic oracle for this parser
running in polynomial time.
We experimentally compare the parser trained
with the dynamic oracle to a baseline obtained
by training with a static oracle. Significant ac-
curacy improvements are achieved on many lan-
guages when using our dynamic oracle. To our
knowledge, these are the first experimental results
on non-projective parsing based on a dynamic or-
acle.
917
2 Preliminary Definitions
Transition-based dependency parsing was origin-
ally introduced by Yamada and Matsumoto (2003)
and Nivre (2003). In this section we briefly sum-
marize the notation we use for this framework and
introduce the notion of dynamic oracle.
2.1 Transition-Based Dependency Parsing
We represent an input sentence as a string w =
w
0
? ? ?w
n
, n ? 1, where each w
i
with i 6= 0 is a
lexical symbol and w
0
is a special symbol called
root. Set V
w
= {i | 0 ? i ? n} denotes the sym-
bol occurrences in w. For i, j ? V
w
with i 6= j,
we write i ? j to denote a grammatical depend-
ency of some unspecified type betweenw
i
andw
j
,
where w
i
is the head and w
j
is the dependent.
A dependency tree t for w is a directed tree
with node set V
w
and with root node 0. An arc of t
is a pair (i, j), encoding a dependency i ? j; we
will often use the latter notation to denote arcs.
A transition-based dependency parser typically
uses a stack data structure to process the input
string from left to right, in a way very similar
to the classical push-down automaton for context-
free languages (Hopcroft et al., 2006). Each stack
element is a node from V
w
, representing the root
of a dependency tree spanning some portion of the
input w, and no internal state is used. At each step
the parser applies some transition that updates the
stack and/or consumes one symbol from the input.
Transitions may also construct new dependencies,
which are added to the current configuration of the
parser.
We represent the stack as an ordered sequence
? = [h
d
, . . . , h
1
], d ? 0, of nodes h
i
? V
w
, with
the topmost element placed at the right. When
d = 0, we have the empty stack ? = []. We use
the vertical bar to denote the append operator for
?, and write ? = ?
?
|h
1
to indicate that h
1
is the
topmost element of ?.
The portion of the input string still to be pro-
cessed by the parser is called the buffer. We
represent the buffer as an ordered sequence ? =
[i, . . . , n] of nodes from V
w
, with i the first ele-
ment of the buffer. We denote the empty buffer
as ? = []. Again, we use the vertical bar to de-
note the append operator, and write ? = i|?
?
to
indicate that i is the first symbol occurrence of ?;
consequently, we have ?
?
= [i+ 1, . . . , n].
In a transition-based parser, the parsing pro-
cess is defined through the technical notions of
configuration and transition. A configuration of
the parser relative to w is a triple c = (?, ?,A),
where ? and ? are a stack and a buffer, respect-
ively, and A is the set of arcs that have been built
so far. A transition is a partial function map-
ping the set of parser configurations into itself.
Each transition-based parser is defined by means
of some finite inventory of transitions. We will
later introduce the specific inventory of transitions
for the parser that we investigate in this paper.
We use the symbol ` to denote the binary relation
formed by the union of all transitions of a parser.
With the notions of configuration and transition
in place, we can define a computation of the
parser on w as a sequence c
0
, c
1
, . . . , c
m
, m ? 0,
of configurations relative tow, under the condition
that c
i?1
` c
i
for each i with 1 ? i ? m. We use
the reflexive and transitive closure of `, written
`
?
, to represent computations.
2.2 Configuration Loss and Dynamic Oracles
A transition-based dependency parser is a non-
deterministic device, meaning that a given con-
figuration can be mapped into several configur-
ations by the available transitions. However, in
several implementations the parser is associated
with a discriminative model that, on the basis of
some features of the current configuration, always
chooses a single transition. In other words, the
model is used to run the parser as a pseudo-de-
terministic device. The training of the discriminat-
ive model relies on a component called the parsing
oracle, which maps parser configurations to ?op-
timal? transitions with respect to some reference
dependency tree, which we call the gold tree.
Traditionally, so-called static oracles have been
used which return a single, canonical transition
and they do so only for configurations that can
reach the gold tree, that is, configurations repres-
enting parsing histories with no mistake. In re-
cent work, Goldberg and Nivre (2012), Goldberg
and Nivre (2013) and Goldberg et al. (2014) have
introduced dynamic oracles, which return the set
of all transitions that are optimal with respect to
a gold tree, and are well-defined and correct for
every configuration that is reachable by the parser.
These authors have shown that the accuracy of
transition-based dependency parsers can be sub-
stantially improved if dynamic oracles are used in
place of static ones. In what follows, we provide
a mathematical definition of dynamic oracles, fol-
lowing Goldberg et al. (2014).
918
(?, k|?,A) `
sh
(?|k, ?,A)
(?|i|j, ?,A) `
la
(?|j, ?,A ? {j ? i})
(?|i|j, ?,A) `
ra
(?|i, ?, A ? {i? j})
(?|i|j|k, ?,A) `
la
2
(?|j|k, ?,A ? {k ? i})
(?|i|j|k, ?,A) `
ra
2
(?|i|j, ?,A ? {i? k})
Figure 1: Transitions of the non-projective parser.
Let t
1
and t
2
be dependency trees for w, with
arc sets A
1
and A
2
, respectively. The loss of t
1
with respect to t
2
is defined as
L(t
1
, t
2
) = |A
1
\A
2
| . (1)
Note that L(t
1
, t
2
) = L(t
2
, t
1
), since |A
1
| = |A
2
|.
Furthermore L(t
1
, t
2
) = 0 if and only if t
1
and t
2
are the same tree.
Let c be a configuration of a transition-based
parser relative to w. Let also D(c) be the set of all
dependency trees that can be obtained in a com-
putation of the form c `
?
c
f
, where c
f
is a final
configuration, that is, a configuration that has con-
structed a dependency tree for w. We extend the
loss function in (1) to configurations by letting
L(c, t
2
) = min
t
1
?D(c)
L(t
1
, t
2
) . (2)
Let t
G
be the gold tree for w. Quantity L(c, t
G
)
can be used to define a dynamic oracle as follows.
For any transition `
?
in the finite inventory of our
parser, we use the functional notation ?(c) = c
?
in
place of c `
?
c
?
. We then let
oracle(c, t
G
) =
{? | L(?(c), t
G
)? L(c, t
G
) = 0} . (3)
In words, (3) provides the set of transitions that do
not increase the loss of c; we call these transitions
optimal for c.
A na??ve way of implementing (3) would be
to explicitly compute the set D(c) in (2), which
has exponential size. More interestingly, the im-
plementation of dynamic oracles proposed by the
above cited authors all run in polynomial time.
These oracles are all defined for projective pars-
ing. In this paper, we present a polynomial-time
oracle for a non-projective parser.
3 Non-Projective Dependency Parsing
In this section we introduce a parser for non-
projective dependency grammars that is derived
from the transition-based parser originally presen-
ted by Attardi (2006), and was further investigated
by Kuhlmann and Nivre (2010) and Cohen et al.
(2011). Our definitions follow the framework in-
troduced in Section 2.1.
We start with some additional notation. Let t be
a dependency tree for w and let k be a node of t.
Consider the complete subtree t
?
of t rooted at k,
that is, the subtree of t induced by k and all of the
descendants of k in t. The span of t
?
is the sub-
sequence of tokens in w represented by the nodes
of t
?
. Node k has gap-degree 0 if the span of t
?
forms a (contiguous) substring of w. A depend-
ency tree is called projective if all of its nodes
have gap-degree 0; a dependency tree which is not
projective is called non-projective.
Given w as input, the parser starts with the ini-
tial configuration ([], [0, . . . , n], ?), consisting of
an empty stack, a buffer with all the nodes repres-
enting the symbol occurrences in w, and an empty
set of constructed dependencies (arcs). The parser
stops when it reaches a final configuration of the
form ([0], [], A), consisting of a stack with only the
root node and of an empty buffer; in any such con-
figuration, set A always implicitly defines a valid
dependency tree (rooted in node 0).
The core of the parser consists of an invent-
ory of five transitions, defined in Figure 1. Each
transition is specified using the free variables ?,
?, A, i, j and k. As an example, the schema
(?|i|j, ?,A) `
la
(?|j, ?,A? {j ? i}) means that
if a configuration c matches the antecedent, then a
new configuration is obtained by instantiating the
variables in the consequent accordingly.
The transition `
sh
, called shift, reads a new
token from the input sentence by removing it from
the buffer and pushing it into the stack. Each
of the other transitions, collectively called reduce
transitions, has the effect of building a dependency
between two nodes in the stack, and then removing
the dependent node from the stack. The removal
of the dependent ensures that the output depend-
ency tree is built in a bottom-up order, collecting
all of the dependents of each node i before linking
i to its head.
The transition `
la
, called left-arc, creates a left-
ward arc where the topmost stack node is the
head and the second topmost node is the depend-
ent, and removes the latter from the stack. The
transition `
ra
, called right-arc, is defined sym-
metrically, so that the topmost stack node is at-
919
? ? ?
h h h
h
1 1 2
3minimum
stack length
at c     c1
c c c0 1
stack length
...
...
m
m
Figure 2: General form of the computations asso-
ciated with an item [h
1
, h
2
, h
3
].
tached as a dependent of the second topmost node.
The combination of the shift, left-arc and right-
arc transitions provides complete coverage of pro-
jective dependency trees, but no support for non-
projectivity, and corresponds to the so-called arc-
standard parser introduced by Nivre (2004).
Support for non-projective dependencies is
achieved by adding the transitions `
la
2
and `
ra
2
,
which are variants of the left-arc and right-arc
transitions, respectively. These new transitions
create dependencies involving the first and the
third topmost nodes in the stack. The creation of
dependencies between non-adjacent stack nodes
might produce crossing arcs and is the key to the
construction of non-projective trees.
Recall that transitions are partial functions,
meaning that they might be undefined for some
configurations. Specifically, the shift transition is
only defined for configurations with a non-empty
buffer. Similarly, the left-arc and right-arc trans-
itions can only be applied if the length of the stack
is at least 2, while the transitions `
la
2
and `
ra
2
re-
quire at least 3 nodes in the stack.
Transitions `
la
2
and `
ra
2
were originally intro-
duced by Attardi (2006) together with other, more
complex transitions. The parser we define here
is therefore more restrictive than Attardi (2006),
meaning that it does not cover all the non-pro-
jective trees that can be processed by the ori-
ginal parser. However, the restricted parser has re-
cently attracted some research interest, as it covers
the vast majority of non-projective constructions
appearing in standard treebanks (Attardi, 2006;
Kuhlmann and Nivre, 2010), while keeping sim-
plicity and interesting properties like being com-
patible with polynomial-time dynamic program-
ming (Cohen et al., 2011).
4 Representation of Computations
Our oracle algorithm exploits a dynamic program-
ming technique which, given an input string, com-
bines certain pieces of a computation of the parser
from Section 3 to obtain larger pieces. In order
to efficiently encode pieces of computations, we
borrow a representation proposed by Cohen et al.
(2011), which is introduced in this section.
Let w = a
0
? ? ? a
n
and V
w
be specified as in
Section 2, and let w
?
be some substring of w. (The
specification of w
?
is not of our concern in this
section.) Let also h
1
, h
2
, h
3
? V
w
. We are inter-
ested in computations of the parser processing the
substring w
?
and having the form c
0
, c
1
, . . . , c
m
,
m ? 1, that satisfy both of the following condi-
tions, exemplified in Figure 2.
? For some sequence of nodes ? with |?| ? 0,
the stack associated with c
0
has the form ?|h
1
and the stack associated with c
m
has the form
?|h
2
|h
3
.
? For each intermediate configuration c
i
, 1 ?
i ? m ? 1, the stack associated with c
i
has
the form ??
i
, where ?
i
is a sequence of nodes
with |?
i
| ? 2.
An important property of the above definition
needs to be discussed here, which is at the heart of
the polynomial-time algorithm in the next section.
If in c
0
, c
1
, . . . , c
m
we replace ? with a different
sequence ?
?
, we obtain a valid computation for w
?
constructing exactly the same dependencies as the
original computation. To see this, let c
i?1
`
?
i
c
i
for each i with 1 ? i ? m. Then `
?
1
must be a
shift, otherwise |?
1
| ? 2 would be violated. Con-
sider now a transition `
?
i
with 2 ? i ? m that
builds some dependency. From |?
i
| ? 2 we derive
|?
i?1
| ? 3. We can easily check from Figure 1
that none of the nodes in ? can be involved in the
constructed dependency.
Intuitively, the above property asserts that the
sequence of transitions `
?
1
,`
?
2
, . . . ,`
?
m
can be
applied to parse substring w
?
independently of the
context ?. This suggests that we can group into
an equivalence class all the computations satisfy-
ing the conditions above, for different values of
?. We indicate such class by means of the tuple
[h
1
, h
2
h
3
], called item. It is easy to see that each
item represents an exponential number of compu-
tations. In the next section we will show how we
can process items with the purpose of obtaining an
efficient computation for dynamic oracles.
920
5 Dynamic Oracle Algorithm
Our algorithm takes as input a gold tree t
G
for
string w and a parser configuration c = (?, ?,A)
relative to w, specified as in Section 2. We assume
that t
G
can be parsed by the non-projective parser
of Section 3 starting from the initial configuration.
5.1 Basic Idea
The algorithm consists of two separate stages, in-
formally discussed in what follows. In the first
stage we identify some tree fragments of t
G
that
can be constructed by the parser after reaching
configuration c, in a way that does not depend on
the content of ?. This means that these fragments
can be precomputed by looking only into ?. Fur-
thermore, since these fragments are subtrees of t
G
,
their computation has no effect on the overall loss
of a computation on w.
For each fragment t with the above properties,
we replace all the nodes in ? that are also nodes
of t with the root node of t itself. The result of the
first stage is therefore a new node sequence shorter
than ?, which we call the reduced buffer ?
R
.
In the second stage of the algorithm we use a
variant of the tabular method developed by Co-
hen et al. (2011), which was originally designed
to simulate all computations of the parser in Sec-
tion 3 on an input string w. We run the above
method on the concatenation of the stack and the
reduced buffer, with some additional constraints
that restrict the search space in two respects. First,
we visit only those computations of the parser
that step through configuration c. Second, we
reach only those dependency trees that contain all
the tree fragments precomputed in the first stage.
We can show that such search space always con-
tains at least one dependency tree with the desired
loss, which we then retrieve performing a Viterbi
search.
5.2 Preprocessing of the Buffer
Let t be a complete subtree of t
G
, having root
node k in ?. Consider the following two condi-
tions, defined on t.
? Bottom-up completeness: No arc i ? j in t
is such that i is a node in ?, i 6= k, and j is a
node in ?.
? Zero gap-degree: The nodes of t that are in ?
form a (contiguous) substring of w.
We claim that if t satisfies the above conditions,
then we can safely reduce the nodes of t appearing
in ?, replacing them with node k. We only report
here an informal discussion of this claim, and omit
a formal proof.
As a first remark, recall that our parser imple-
ments a purely bottom-up strategy. This means
that after a tree has been constructed, all of its
nodes but the root are removed from the parser
configuration. Then the Bottom-up completeness
condition guarantees that if we remove from ? all
nodes of t but k, the nodes of t that are in ? can still
be processed in a way that does not affect the loss,
since their parent must be either k or a node that is
neither in ? nor in ?. Note that the nodes of t that
are neither in ? nor in ? are irrelevant to the pre-
computation of t from ?, since these nodes have
already been attached and are no longer available
to the parser.
As a second remark, the Zero gap-degree con-
dition guarantees that the span of t over the nodes
of ? is not interleaved by nodes that do not belong
to t. This is also an important requirement for the
precomputation of t from ?, since a tree fragment
having a discontinuous span over ? might not be
constructable independently of ?. More specific-
ally, parsing such fragment implies dealing with
the nodes in the discontinuities, and this might re-
quire transitions involving nodes from ?.
We can now use the sufficient condition above
to compute ?
R
. We process ? from left to right.
For each node k, we can easily test the Bottom-up
completeness condition and the Zero gap-degree
condition for the complete subtree t of t
G
rooted
at k, and perform the reduction if both conditions
are satisfied. Note that in this process a node k
resulting from the reduction of t might in turn be
removed from ? if, at some later point, we reduce
a supertree of t.
5.3 Computation of the Loss
We describe here our dynamic programming al-
gorithm for the computation of the loss of an in-
put configuration c. We start with some additional
notation. Let ? = ??
R
be the concatenation of ?
and ?
R
, which we treat as a string of nodes. For
integers i with 0 ? i ? |?| ? 1, we write ?[i] to
denote the (i + 1)-th node of ?. Let also ` = |?|.
Symbol ` is used to mark the boundary between
the stack and the reduced buffer in ?, thus ?[i] with
i < ` is a node of ?, while ?[i] with i ? ` is a node
of ?
R
.
Algorithm 1 computes the loss of c by pro-
cessing the sequence ? in a way quite similar to the
921
standard nested loop implementation of the CKY
parser for context-free grammars (Hopcroft et al.,
2006). The algorithm uses a two-dimensional ar-
ray T whose indexes range from 0 to |?| = ` +
|?
R
|, and only the cells T [i, j] with i < j are
filled.
We view each T [i, j] as an association list
whose keys are items [h
1
, h
2
h
3
], defined in the
context of the substring ?[i] ? ? ? ?[j ? 1] of ?; see
Section 4. The value stored at T [i, j]([h
1
, h
2
h
3
])
is the minimum loss contribution due to the com-
putations represented by [h
1
, h
2
h
3
]. For technical
reasons, we assume that our parser starts with a
symbol $ 6? V
w
in the stack, denoting the bottom
of the stack.
We initialize the table by populating the cells
of the form T [i, i + 1] with information about
the trivial computations consisting of a single `
sh
transition that shifts the node ?[i] into the stack.
These computations are known to have zero loss
contribution, because a `
sh
transition does not cre-
ate any arcs. In the case where the node ?[i] be-
longs to ?, i.e., i < `, we assign loss contribution
0 to the entry T [i, i + 1]([?[i? 1], ?[i? 1]?[i]])
(line 3 of Algorithm 1), because ?[i] is shifted with
?[i? 1] at the top of the stack. On the other hand,
if ?[i] is in ?, i.e., i ? `, we assign loss contri-
bution 0 to several entries in T [i, i + 1] (line 6)
because, at the time ?[i] is shifted, the content of
the stack depends on the transitions executed be-
fore that point.
After the above initialization, we consider
pairs of contiguous substrings ?[i] ? ? ? ?[k ? 1] and
?[k] ? ? ? ?[j ? 1] of ?. At each inner iteration
of the nested loops of lines 7-11 we update cell
T [i, j] based on the content of the cells T [i, k] and
T [k, j]. We do this through the procedure PRO-
CESSCELL(T , i, k, j), which considers all pairs
of keys [h
1
, h
2
h
3
] in T [i, k] and [h
3
, h
4
h
5
] in
T [k, j]. Note that we require the index h
3
to match
between both items, meaning that their computa-
tions can be concatenated. In this way, for each
reduce transition ? in our parser, we compute the
loss contribution for a new piece of computation
defined by concatenating a computation with min-
imum loss contribution in the first item and a com-
putation with minimum loss contribution in the
second item, followed by the transition ? . The fact
that the new piece of computation can be repres-
ented by an item is exemplified in Figure 3 for the
case ? = `
ra
2
.
?
h1
c0
?
h
h
2
3
c
?
h
h
2
3
c
?
h
h
2
c
h5
4
?
h
h
4
5
c +1
la  : create arc
h     h  and remove
h   from stack
2
5 2
2
[h , h h ]1 2 3 la 2?
?
h1
c0
?
h
h
4
5
c +1
... ...
+ +[h , h h ]3 4 5
[h , h h ]1 4 5
...
?
m m r r
r
Figure 3: Concatenation of two computa-
tions/items and transition `
ra
2
, resulting in a new
computation/item.
The computed loss contribution is used to up-
date the entry in T [i, j] corresponding to the item
associated with the new computation. Observe
how the loss contribution provided by the arc cre-
ated by ? is computed by the ?
G
function at lines
17, 20, 23 and 26, which is defined as:
?
G
(i? j) =
{
0, if i? j is in t
G
;
1, otherwise.
(4)
We remark that the nature of our problem al-
lows us to apply several shortcuts and optimiza-
tions that would not be possible in a setting where
we actually needed to parse the string ?. First, the
range of variable i in the loop in line 8 starts at
max{0, `?d}, rather than at 0, because we do not
need to combine pairs of items originating from
nodes in ? below the topmost node, as the items
resulting from such combinations correspond to
computations that do not contain our input config-
uration c. Second, when we have set values for i
such that i+2 < `, we can omit calling PROCESS-
CELL for values of the parameter k ranging from
i+2 to `?1, as those calls would use as their input
one of the items described above, which are not of
interest. Finally, when processing substrings that
are entirely in ?
R
(i ? `) we can restrict the trans-
itions that we explore to those that generate arcs
that either are in the gold tree t
G
, or have a parent
node which is not present in ? (see conditions in
922
Algorithm 1 Computation of the loss function
1: T [0, 1]([$, $0])? 0 . shift node 0 on top of empty stack symbol $
2: for i? 1 to `? 1 do
3: T [i, i+ 1]([?[i? 1], ?[i? 1]?[i]])? 0 . shift node ?[i] with ?[i? 1] on top of the stack
4: for i? ` to |?| do
5: for h? 0 to i? 1 do
6: T [i, i+ 1]([?[h], ?[h]?[i]])? 0 . shift node ?[i] with ?[h] on top of the stack
7: for d? 2 to |?| do . consider substrings of length d
8: for i? max{0, `? d} to |?| ? d do . i = beginning of substring
9: j ? i+ d . j ? 1 = end of substring
10: PROCESSCELL(T , i, i+ 1, j) . We omit the range k = i+ 2 to max{i+ 2, `} ? 1
11: for k ? max{i+ 2, `} to j do . factorization of substring at k
12: PROCESSCELL(T , i, k, j)
13: return T [0, |?|]([$, $0]) +
?
i?[0,`?1]
L
c
(?[i], t
G
)
14: procedure PROCESSCELL(T , i, k, j)
15: for each key [h
1
, h
2
h
3
]) defined in T [i, k] do
16: for each key [h
3
, h
4
h
5
]) defined in T [k, j] do . h
3
must match between the two entries
17: loss
la
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
5
? h
4
)
18: if (i < `) ? ?
G
(h
5
? h
4
) = 0 ? (h
5
6? ?) then
19: T [i, j]([h
1
, h
2
h
5
])? min{loss
la
, T [i, j]([h
1
, h
2
h
5
])} . cell update `
la
20: loss
ra
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
4
? h
5
)
21: if (i < `) ? ?
G
(h
4
? h
5
) = 0 ? (h
4
6? ?) then
22: T [i, j]([h
1
, h
2
h
4
])? min{loss
ra
, T [i, j]([h
1
, h
2
h
4
])} . cell update `
ra
23: loss
la
2
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
5
? h
2
)
24: if (i < `) ? ?
G
(h
5
? h
2
) = 0 ? (h
5
6? ?) then
25: T [i, j]([h
1
, h
4
h
5
])? min{loss
la
2
, T [i, j]([h
1
, h
4
h
5
])} . cell update `
la
2
26: loss
ra
2
? T [i, k]([h
1
, h
2
h
3
]) + T [k, j]([h
3
, h
4
h
5
]) + ?
G
(h
2
? h
5
)
27: if (i < `) ? ?
G
(h
2
? h
5
) = 0 ? (h
2
6? ?) then
28: T [i, j]([h
1
, h
2
h
4
])? min{loss
ra
2
, T [i, j]([h
1
, h
2
h
4
])} . cell update `
ra
2
lines 18, 21, 24, 27), because we know that incor-
rectly attaching a buffer node as a dependent of an-
other buffer node, when the correct head is avail-
able, can never be an optimal decision in terms of
loss.
Once we have filled the table T , the loss for
the input configuration c can be obtained from the
value of the entry T [0, |?|]([$, $0]), representing
the minimum loss contribution among computa-
tions that reach the input configuration c and parse
the whole input string. To obtain the total loss,
we add to this value the loss contribution accu-
mulated by the dependency trees with root in the
stack ? of c. This is represented in Algorithm 1 as
?
i?[0,`?1]
L
c
(?[i], t
G
), where L
c
(?[i], t
G
) is the
count of the descendants of ?[i] (the (i+1)-th ele-
ment of ?) that had been assigned the wrong head
by the parser with respect to t
G
.
5.4 Sample Run
Consider the Czech sentence and the gold depend-
ency tree t
G
shown in Figure 4(a). Given the con-
figuration c = (?, ?,A) where ? = [0, 1, 3, 4],
? = [5, . . . , 13] and A = {3 ? 2}, we trace the
two stages of the algorithm.
Preprocessing of the buffer The complete sub-
tree rooted at node 7 satisfies the Bottom-up com-
pleteness and the Zero gap-degree conditions in
Section 5.2, so the nodes 5, . . . , 12 in ? can be
replaced with the root 7. Note that all the nodes in
the span 5, . . . , 12 have all their (gold) dependents
in that span, with the exception of the root 7, with
its dependent node 1 still in the stack. No other
reduction is possible, and we have ?
R
= [7, 13].
The corresponding fragment of t
G
is represented
in Figure 4(b).
Computation of the loss Let ? = ??
R
. Al-
gorithm 1 builds the two-dimensional array T in
Figure 4(c). Each cell T [i, j] contains an asso-
ciation list, whose (key:value) pairs map items to
their loss contribution. Figure 4(c) only shows the
pairs involved in the minimum-loss computation.
Lines 1-6 of Algorithm 1 initialize the cells in
the diagonal, T [0, 1], . . . , T [5, 6]. The boundary
between stack and buffer is ` = 4, thus cells
T [0, 1], T [1, 2], and T [2, 3] contain only one ele-
ment, while T [3, 4], T [4, 5] and T [5, 6] contain as
many as the previous elements in ?, although not
all of them are shown in the figure.
Lines 7-12 fill the superdiagonals until T [0, 6]
is reached. The cells T [0, 2], T [0, 3] and T [1, 3]
923
-Root- V be?z?ne?m provozu vs?ak telefonn?? linky nermaj?? takivou kvalitu jako v laborator?i .
0 1 2 3 4 5 6 7 8 9 10 11 12 13
(a) Non-projective dependency tree from the Prague Dependency Treebank.
-Root- V provozu vs?ak nermaj?? .
0 1 3 4 7 13
? ?R
(b) Fragment of dependency tree in (a) after buffer
reduction.
i j 1 2 3 4 5 6
0 [$,$ 0]:0 ? ? . . . [$,$ 0]:1 [$,$ 0]:1
1 [0,0 1]:0 ? . . . [0,0 4]:1 . . .
2 [1,1 3]:0 [1,1 4]:1 [1,4 7]:1 . . .
3 [3,3 4]:0 [3,4 7]:1 . . .
4 [4,4 7]:0 . . .
5 [0,0 13]:0
(c) Relevant portion of T computed by Algorithm 1, with the
loss of c in the yellow entry.
Figure 4: Example of loss computation given the sentence in (a) and considering a configuration c with
? = [0, 1, 3, 4] and ? = [5, . . . , 13].
are left empty because ` = 4. Once T [0, 6]
is calculated, it contains only the entry with key
[$, $, 0], with the associated value 1 representing
the minimum number of wrong arcs that the pars-
ing algorithm has to build to reach a final con-
figuration from c. Then, Line 13 retrieves the
loss of the configuration, computed as the sum of
T [0, 6]([$, $, 0]) with the termL
c
, representing the
erroneous arcs made before reaching c.
Note that in our example the loss of c is 1, even
though L
c
= 0, meaning that there are no wrong
arcs in A. Indeed, given c, there is no single com-
putation that builds all the remaining arcs in t
G
.
This is reflected in T , where the path to reach the
item with minimum loss has to go through either
T [3, 5] or T [2, 4], which implies building the erro-
neous arc (w
7
? w
3
) or (w
4
? w
3
), respectively.
6 Computational Analysis
The first stage of our algorithm can be easily im-
plemented in time O(|?| |t
G
|), where |t
G
| is the
number of nodes in t
G
, which is equal to the length
n of the input string.
For the worst-case complexity of the second
stage (Algorithm 1), note that the number
of cell updates made by calling PROCESS-
CELL(T , i, k, j) with k < ` is O(|?|
3
|?|
2
|?
R
|).
This is because these updates can only be caused
by procedure calls on line 10 (as those on line 12
always set k ? `) and therefore the index k always
equals i + 1, while h
2
must equal h
1
because the
item [h
1
, h
2
h
3
] is one of the initial items created
on line 3. The variables i, h
1
and h
3
must index
nodes on the stack ? as they are bounded by k,
while j ranges over ?
R
and h
4
and h
5
can refer to
nodes either on ? or on ?
R
.
On the other hand, the number of cell updates
triggered by calls to PROCESSCELL such that k ?
` is O(|?|
4
|?
R
|
4
), as they happen for four indices
referring to nodes of ?
R
(k, j, h
4
, h
5
) and four
indices that can range over ? or ?
R
(i, h
1
, h
2
, h
3
).
Putting everything together, we conclude that
the overall complexity of our algorithm is
O(|?| |t
G
|+ |?|
3
|?|
2
|?
R
|+ |?|
4
|?
R
|
4
).
In practice, quantities |?|, |?
R
| and |?| are signi-
ficantly smaller than n, providing reasonable train-
ing times as we will see in Section 7. For instance,
when measured on the Czech treebank, the aver-
age value of |?| is 7.2, with a maximum of 87.
Even more interesting, the average value of |?
R
|
is 2.6, with a maximum of 23. Comparing this to
the average and maximum values of |?|, 11 and
192, respectively, we see that the buffer reduction
is crucial in reducing training time.
Note that, when expressed as a function of n,
our dynamic oracle has a worst-case time com-
plexity of O(n
8
). This is also the time complexity
of the dynamic programming algorithm of Cohen
et al. (2011) we started with, simulating all com-
putations of our parser. In contrast, the dynamic
oracle of Goldberg et al. (2014) for the projective
case achieves a time complexity ofO(n
3
) from the
dynamic programming parser by Kuhlmann et al.
(2011) running in time O(n
5
).
924
The reason why we do not achieve any asymp-
totic improvement is that some helpful properties
that hold with projective trees are no longer satis-
fied in the non-projective case. In the projective
(arc-standard) case, subtrees that are in the buf-
fer can be completely reduced. As a consequence,
each oracle step always combines an inferred entry
in the table with either a node from the stack or a
node from the reduced buffer, asymptotically re-
ducing the time complexity. However, in the non-
projective (Attardi) case, subtrees in the buffer can
not always be completely reduced, for the reasons
mentioned in the second-to-last paragraph of Sec-
tion 5.2. As a consequence, the oracle needs to
make cell updates in a more general way, which
includes linking pairs of elements in the reduced
buffer or pairs of inferred entries in the table.
-Root- John was not as good for the job as Kate .
0 1 2 3 4 5 6 7 8 9 10 11
Figure 5: Non-projective dependency tree adapted
from the Penn Treebank.
An example of why this is needed is provided
by the gold tree in Figure 5. Assume a config-
uration c = (?, ?,A) where ? = [0, 1, 2, 3, 4],
? = [5, . . . , 11], and A = ?. It is easy to see that
the loss of c is greater than zero, since the gold tree
is not reachable from c: parsing the subtree rooted
at node 5 requires shifting 6 into the stack, and
this makes it impossible to build the arcs 2 ? 5
and 2? 6. However, if we reduced the subtree in
the buffer with root 5, we would incorrectly obtain
a loss of 0, as the resulting tree is parsable if we
start with `
sh
followed by `
la
and `
ra
2
. Note that
there is no way of knowing whether it is safe to
reduce the subtree rooted at 5 without using non-
local information. For example, the arc 2 ? 6 is
crucial here: if 6 depended on 5 or 4 instead, the
loss would be zero. These complications are not
found in the projective case, allowing for the men-
tioned asymptotic improvement.
7 Experimental Evaluation
For comparability with previous work on dynamic
oracles, we follow the experimental settings repor-
ted by Goldberg et al. (2014) for their arc-standard
dynamic oracle. In particular, we use the same
training algorithm, features, and root node posi-
tion. However, we train the model for 20 itera-
static dynamic
UAS LAS UAS LAS
Arabic 80.90 71.56 82.23 72.63
Basque 75.96 66.74 74.32 65.59
Catalan 90.55 85.20 89.94 84.96
Chinese 84.72 79.93 85.34 81.00
Czech 79.83 72.69 82.08 74.44
English 85.52 84.46 87.38 86.40
Greek 79.84 72.26 81.55 74.14
Hungarian 78.13 68.90 76.27 68.14
Italian 83.08 78.94 84.43 80.45
Turkish 79.57 69.44 79.41 70.32
Bulgarian 89.46 85.99 89.32 85.92
Danish 85.58 81.25 86.03 81.59
Dutch 79.05 75.69 80.13 77.22
German 88.34 86.48 88.86 86.94
Japanese 93.06 91.64 93.56 92.18
Portuguese 84.80 81.38 85.36 82.10
Slovene 76.33 68.43 78.20 70.22
Spanish 79.88 76.84 80.25 77.45
Swedish 87.26 82.77 87.24 82.49
PTB 89.55 87.18 90.47 88.18
Table 1: Unlabelled Attachment Score (UAS) and
Labelled Attachment Score (LAS) using a static
and a dynamic oracle. Evaluation on CoNLL 2007
(first block) and CoNLL 2006 (second block) data-
sets is carried out including punctuation, evalu-
ation on the Penn Treebank excludes it.
tions rather than 15, as the increased search space
and spurious ambiguity of Attardi?s non-project-
ive parser implies that more iterations are required
to converge to a stable model. A more detailed
description of the experimental settings follows.
7.1 Experimental Setup
Training We train a global linear model using
the averaged perceptron algorithm and a labelled
version of the parser described in Section 3. We
perform on-line training using the oracle defined
in Section 5: at each parsing step, the model?s
weights are updated if the predicted transition res-
ults into an increase in configuration loss, but
the process continues by following the predicted
transition independently of the loss increase.
As our baseline we train the model using the
static oracle defined by (Cohen et al., 2012). This
oracle follows a canonical computation that cre-
ates arcs as soon as possible, and prioritizes the
`
la
transition over the `
la
2
transition in situations
925
where both create a gold arc. The static oracle
is not able to deal with configurations that can-
not reach the gold dependency tree, so we con-
strain the training algorithm to follow the zero-loss
transition provided by the oracle.
While this version of Attardi?s parser has been
shown to cover the vast majority of non-projective
sentences in several treebanks (Attardi, 2006; Co-
hen et al., 2012), there still are some sentences
which are not parsable. These sentences are
skipped during training, but not during test and
evaluation of the model.
Datasets We evaluate the parser performance
over CoNLL 2006 and CoNLL 2007 datasets.
If a language is present in both datasets, we
use the latest version. We also include res-
ults over the Penn Treebank (PTB) (Marcus et
al., 1993) converted to Stanford basic dependen-
cies (De Marneffe et al., 2006). For the CoNLL
datasets we use the provided part-of-speech tags
and the standard training/test partition; for the
PTB we use automatically assigned tags, we train
on sections 2-21 and test on section 23.
7.2 Results and Analysis
In Table 1 we report the unlabelled (UAS) and la-
belled (LAS) attachment scores for the static and
the dynamic oracles. Each figure is an average
over the accuracy provided by 5 models trained
with the same setup but using a different random
seed. The seed is only used to shuffle the sentences
in random order during each iteration of training.
Our results are consistent with the results re-
ported by Goldberg and Nivre (2013) and Gold-
berg et al. (2014). For most of the datasets, we
obtain a relevant improvement in both UAS and
LAS. For Dutch, Czech and German, we achieve
an error reduction of 5.2%, 11.2% and 4.5%, re-
spectively. Exceptions to this general trend are
Swedish and Bulgarian, where the accuracy differ-
ences are negligible, and the Basque, Catalan and
Hungarian datasets, where the performance actu-
ally decreases.
If instead of testing on the standard test sets we
use 10-fold cross-validation and average the res-
ulting accuracies, we obtain improvements for all
languages in Table 1 but Basque and Hungarian.
More specifically, measured (UAS, LAS) pairs for
Swedish are (86.85, 82.17) with dynamic oracle
against (86.6, 81.93) with static oracle; for Bul-
garian (88.42, 83.91) against (88.20, 83.55); and
for Catalan (88.33, 83.64) against (88.06, 83.13).
This suggests that the negligible or unfavourable
results in Table 1 for these languages are due to
statistical variability given the small size of the test
sets.
As for Basque, we measure (75.54, 67.58)
against (76.77, 68.20); similarly, for Hungarian
we measure (75.66, 67.66) against (77.22, 68.42).
Unfortunately, we have no explanation for these
performance decreases, in terms of the typology
of the non-projective patterns found in these two
datasets. Note that Goldberg et al. (2014) also
observed a performance decrease on the Basque
dataset in the projective case, although not on
Hungarian.
The parsing times measured in our experiments
for the static and the dynamic oracles are the same,
since the oracle algorithm is only used during the
training stage. Thus the reported improvements in
parsing accuracy come at no extra cost for parsing
time. In the training stage, the extra processing
needed to compute the loss and to explore paths
that do not lead to a gold tree made training about
4 times slower, on average, for the dynamic oracle
model. This confirms that our oracle algorithm is
fast enough to be of practical interest, in spite of its
relatively high worst-case asymptotic complexity.
8 Conclusions
We have presented what, to our knowledge, are
the first experimental results for a transition-based
non-projective parser trained with a dynamic or-
acle. We have also shown significant accuracy im-
provements on many languages over a static oracle
baseline.
The general picture that emerges from our ap-
proach is that dynamic programming algorithms
originally conceived for the simulation of trans-
ition-based parsers can effectively be used in the
development of polynomial-time algorithms for
dynamic oracles.
Acknowledgments
The first author has been partially funded by Min-
isterio de Econom??a y Competitividad/FEDER
(Grant TIN2010-18552-C03-02) and by Xunta de
Galicia (Grant CN2012/008). The third author has
been partially supported by MIUR under project
PRIN No. 2010LYA9RH 006.
926
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166?
170, New York, USA.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1052?
1062, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Shay B. Cohen, Carlos G?omez-Rodr??guez, and Giorgio
Satta. 2011. Exact inference for generative prob-
abilistic non-projective dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1234?
1245, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Shay B. Cohen, Carlos G?omez-Rodr??guez, and Gior-
gio Satta. 2012. Elimination of spurious ambigu-
ity in transition-based dependency parsing. CoRR,
abs/1206.6735.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 449?454.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc. of
the 24
th
COLING, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the Associ-
ation for Computational Linguistics, 2(April):119?
130.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2006. Introduction to Automata Theory, Lan-
guages, and Computation (3rd Edition). Addison-
Wesley Longman Publishing Co., Inc., Boston, MA,
USA.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, July.
Marco Kuhlmann and Joakim Nivre. 2010. Transition-
based techniques for non-projective dependency
parsing. Northern European Journal of Language
Technology, 2(1):1?19.
Marco Kuhlmann, Carlos G?omez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673?682, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Techno-
logies (IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57, Barcelona, Spain.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Stat-
istical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
927
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 411?415,
Dublin, Ireland, August 23-24, 2014.
LyS: Porting a Twitter Sentiment Analysis Approach
from Spanish to English
David Vilares, Miguel Hermo, Miguel A. Alonso, Carlos Go?mez-Rodr??guez, Yerai Doval
Grupo LyS, Departamento de Computacio?n, Facultade de Informa?tica
Universidade da Corun?a, Campus de A Corun?a
15071 A Corun?a, Spain
{david.vilares, miguel.hermo, miguel.alonso, carlos.gomez, yerai.doval}@udc.es
Abstract
This paper proposes an approach to solve
message- and phrase-level polarity classi-
fication in Twitter, derived from an exist-
ing system designed for Spanish. As a
first step, an ad-hoc preprocessing is per-
formed. We then identify lexical, psycho-
logical and semantic features in order to
capture different dimensions of the human
language which are helpful to detect sen-
timent. These features are used to feed a
supervised classifier after applying an in-
formation gain filter, to discriminate irrel-
evant features. The system is evaluated on
the SemEval 2014 task 9: Sentiment Anal-
ysis in Twitter. Our approach worked com-
petitively both in message- and phrase-
level tasks. The results confirm the robust-
ness of the approach, which performed
well on different domains involving short
informal texts.
1 Introduction
Millions of opinions, conversations or just trivia
are published each day in Twitter by users of dif-
ferent cultures, countries and ages. This provides
an effective way to poll how people praise, com-
plain or discuss about virtually any topic. Compre-
hending and analysing all this information has be-
come a new challenge for organisations and com-
panies, which aim to find out a way to make quick
and more effective decisions for their business. In
particular, identifying the perception of the public
with respect to an event, a service or an entity are
some of their main goals in a short term. In this
respect, sentiment analysis, and more specifically
polarity classification, is playing an important role
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
in order to automatically analyse subjective infor-
mation in texts.
This paper describes our participation at Sem-
Eval 2014 task 9: Sentiment Analysis in Twit-
ter. Specifically, two subtasks were presented:
(A) contextual polarity disambiguation and (B)
message polarity classification. The first sub-
task consists on determining the polarity of words
or phrases extracted from short informal texts,
the scope of extracts being provided by the Se-
mEval organisation. Subtask B focusses on clas-
sifying the content of the whole message. In
both cases, three possible sentiments are consid-
ered: positive, negative and neutral (which in-
volves mixed and non-opinionated instances). Al-
though the training set only contains tweets, the
test set also includes short informal texts from
other domains, in order to measure cross-domain
portability. You can test the model for subtask B
at miopia.grupolys.org.
2 SemEval 2014-Task 9: Sentiment
Analysis in Twitter
Our contribution is a reduced version of a Span-
ish sentiment classification system (Vilares et al.,
2013a; Vilares et al., 2013b) that participated in
TASS 2013 (Villena-Roma?n et al., 2014), achiev-
ing the 5th place on the global sentiment classifi-
cation task and the 1st place on topic classification
on tweets. In this section we describe how we have
ported to English this system originally designed
for Spanish. Tasks A and B are addressed from
the same perspective, which is described below.
2.1 Preprocessing
We implement a naive preprocessing algorithm
which seeks to normalise some of the most com-
mon ungrammatical elements. It is intended for
Twitter, but many of the issues addressed would
also be valid in other domains:
411
? Replacement of frequent abbreviations The
list of the most frequent ones was extracted
from the training set, taking the Penn Tree-
bank (Marcus et al., 1993) as our dictionary.
A term is considered ungrammatical if it does
not appear in our dictionary. We then carry
out a manual review to distinguish between
unknown words and abbreviations, providing
a correction in the latter case. For example,
?c?mon? becomes ?come on? and ?Sat? is re-
placed by ?Saturday?.
? Emoticon normalisation: We employ the
emoticon collection published in (Agarwal et
al., 2011). Each emoticon is replaced with
one of these five labels: strong positive (ESP),
positive (EP), neutral (ENEU), negative (EN)
or strong negative (ESN).
? Laughs : Multiple forms used in social media
to reflect laughs (e.g. ?hhahahha?, ?HHEHE-
HEH?) are preprocessed in a homogeneous
way to obtain a pattern of the form ?hxhx?
where x ? {a, e, i, o, u}.
? URL normalisation: External links are re-
placed by the string ?url?.
? Hashtags (?#?) and usernames (?@?): If the
hashtag appears at the end or beginning of
the tweet, we remove the hashtag. Based
on other participant approaches at SemEval
2013 (Nakov et al., 2013), we realized maybe
this is not the best option, although we be-
lieve hashtags will not be useful in most of
cases, since they refer to very specific events.
Otherwise, only the ?#? is removed, hypothe-
sising the hashtag is used to emphasise a term
(e.g. ?Matthew #Mcconaughey has won the
Oscar?).
2.2 Feature Extraction
Our approach only takes into account information
extracted from the text, without considering any
kind of meta-data. Extracted features combine
lexical, psychological and semantic knowledge in
order to build a linguistic model able to analyse
tweets, but also other kinds of messages. These
features can be divided into two types: corpus-
extracted features and lexicon-extracted features.
All of them take the total number of occurrences
of the respective feature as the weighting factor to
then feed the supervised classifier.
2.2.1 Corpus-extracted features
Given a corpus, we use it to extract the following
set of features:
? Word forms: A model based on this type of
features is our baseline. Each single word is
considered as a feature in order to feed the
supervised classifier. This often becomes a
simple and acceptable start point which ob-
tains a decent performance.
? Part-of-speech (PoS) information: some
coarse-grained PoS-tags such as adjective or
adverb are usually good indicators of subjec-
tive texts while some fine-grained PoS tags
such as third person personal pronoun pro-
vide evidence of non-opinionated messages
(Pak and Paroubek, 2010).
2.2.2 Lexicon-extracted features
We also consider information obtained from exter-
nal lexicons in order to capture linguistic informa-
tion that can not be extracted from a training cor-
pus by means of bag-of-words and PoS-tag mod-
els. We rely on two manually-build lexicons:
? Pennebaker et al. (2001) psychometric dictio-
naries. Linguistic Inquiry and Word Count1
(LIWC) is a software which includes a seman-
tic dictionary to measure how people use dif-
ferent kinds of words over a wide number of
texts. It categorises terms into psychometric
properties, which correspond to different di-
mensions of the human language. The dictio-
nary relates terms with psychological prop-
erties (e.g. anger or anxiety), but also with
topics (e.g. family, friends, religion) or even
morphological features (e.g. future time, past
time or exclamations).
? Hu and Liu (2004) opinion lexicon. It is a col-
lection of positive and negative words. Many
of the occurrences are misspelled, since they
often come from web environments.
2.2.3 Syntactic features
We also parsed the tweets using MaltParser (Nivre
et al., 2007) in order to obtain dependency triplets
of the form (w
i
, arc
ij
, w
j
), where w
i
is the head
word w
j
, the dependent one and arc
ij
the exist-
ing syntactic relation between them. We tried to
incorporate generalised dependency triplets (Joshi
1http://www.liwc.net/
412
and Penstein-Rose?, 2009), following an enriched
perspective presented in Vilares et al. (2014). A
generalisation consists on backing off the words
to more abstracted terms. For example, a valid de-
pendency triplet for the phrase ?awesome villain?
is (villain, modifier, awesome), which could be
generalised into (anger, modifier, assent) by means
of psychometric properties. However, experimen-
tal results over the development corpus using these
features decreased performance with respect to
our best model, probably due to the small size of
the training corpus, since dependency triplets tend
to suffer from sparsity, so a larger training corpus
is needed to exploit them in a proper way (Vilares
et al., 2014).
2.3 Feature Selection
For a machine learning approach, sparsity could
be an issue. In particular, due to the size of the cor-
pus, many of the terms extracted from the training
set only appear a few times in it. This makes it
impossible to properly learn the polarity of many
tokens. Thus, we carry out a filtering step before
feeding our classifier. In particular, we rely on
the information gain (IG) method to then rank the
most relevant features. Information gain measures
the relevance of an attribute with respect to a class.
It takes values between 0 and 1, where a higher
value implies a higher relevance. Table 1 shows
the top five relevant features based on their infor-
mation gain for our best model. The top features
for task A were very similar. Our official runs only
consider features with an IG greater than zero.
IG Feature Category
0.140 positive emotion Pennebaker et al. (2001)
0.137 #positive-words Hu and Liu (2004)
0.126 affect Pennebaker et al. (2001)
0.089 #negative-words Hu and Liu (2004)
0.083 negative emotion Pennebaker et al. (2001)
Table 1: Most relevant features for task B. ?#? must
be read this table as ?the number of?and not as a
hashtag.
2.4 Classifier
We have trained our runs with a SVMLibLINEAR
classifier (Fan et al., 2008) taking the implementa-
tion provided in WEKA (Hall et al., 2009). The
selection was motivated by the acceptable results
that some of the participants in SemEval 2013, e.g.
Becker et al. (2013), obtained using this imple-
mentation. We configured the multi-class support
vector machine by Crammer and Singer (2002) as
the SVMtype. Since the corpus was unbalanced,
we tuned the weights for the classes using the de-
velopment corpus: 1 for the positive class, 2 for
negative and 0.5 for neutral. The rest of parame-
ters were set to default values.
3 Experimental Results
The SemEval 2014 organisation provides a stan-
dard training corpus for both tasks A an B. For task
A, each tweet is marked with a list of the words
and phrases to analyse, and for each one its senti-
ment label is provided. In addition, a development
corpus was released for tuning the system parame-
ters. The training and the development corpus can
be used jointly (constrained runs) to train mod-
els that are then evaluated over the test corpus.2
Some participants used external annotated corpora
(unconstrained runs) to build their models. With
respect to the test corpus, it contains texts from
tweets but also from LiveJournal texts, which we
are abbreviating as LJ, and SMS messages.
Table 2 contains the statistics of the corpora we
used. Sharing data is a violation of Twitter?s terms
of service, so we had to download them. Unfortu-
nately, some of the tweets were no longer available
for several reasons, e.g., user or a tweet does not
exist anymore or the privacy settings of a user have
changed. As a result, the size of our training and
development corpora may be different from those
of other participant?s corpora.
Task Set Positive Negative Neutral
Train 4,917 2,591 385
A Dev 555 365 45
Test 6,354 3,771 556
Train 3,063 1,202 3,935
B Dev 493 290 633
Test 3,506 1,541 3,940
Table 2: SemEval 2014 corpus statistics.
3.1 Evaluation Metrics
F-measure is the official score to measure how sys-
tems behave on each class. In order to rank partic-
ipants, the SemEval 2014 organisation proposed
the averaged F-measure of positive and negative
tweets.
2We followed this angle.
413
3.2 Performance on Sets
Tables 3 and 4 show performance on the test set
of different combinations of the proposed features.
Table 5 shows the performance of our run on task
A. The results over the corresponding sets for task
B are illustrated in Table 6. They are significant
lower than in task A. This suggests that when a
message involves more than one of two tokens, a
lexical approach is not enough. Improving perfor-
mance should involve taking into account context
and linguistic phenomena that appear in sentences
to build a model based on the composition of lin-
guistic information.
Model LJ SMS Twitter Twitter Twitter2013 2014 Sarcasm
WPLT 82.21 82.32 84.82 81.69 71.19(no IG)
WPL 83.55 81.04 84.85 80.64 68.79
WPLT* 83.96 81.46 85.63 79.93 71.98
WP 78.53 80.97 80.34 73.35 74.18
P 75.70 78.74 73.58 65.75 71.82
W 61.58 65.45 64.56 59.16 62.93
L 66.04 64.11 62.96 53.81 61.26
T 47.07 51.37 71.82 43.64 49.37
Table 3: Performance on the test set for task A.
The model marked with a * was our official run. W
stands for features obtained from a bag-of-words
approach, L from Hu and Liu (2004), P from Pen-
nebaker et al. (2001) and T for fine-grained PoS-
tags. They can be combined, e.g., a model named
WP use both words and psychometric properties.
Model LJ SMS Twitter Twitter Twitter2013 2014 Sarcasm
WPLT* 69.79 60.45 66.92 64.92 42.40
WPL 70.19 61.41 66.71 64.51 45.72
WP 66.84 60.22 65.29 63.90 45.90
WPLT 66.38 57.01 61.96 62.84 43.71(no IG)
W 65.12 56.00 62.87 62.64 48.75
P 63.42 54.80 60.05 57.66 54.20
T 45.99 35.85 46.53 45.99 48.58
L 57.53 45.14 48.80 44.48 49.14
Table 4: Performance on the test set for task B.
4 Conclusions
This papers describes the participation of the LyS
Research Group (http://www.grupolys.
org) at the SemEval 2014 task 9: Sentiment Anal-
ysis in Twitter, with a system that attained com-
petitive performance both in message and phrase-
Test set Positive Negative Neutral
DEV 86.30 81.60 4.30
TWITTER 2013 88.70 81.90 17.60(full)
TWITTER 2013 88.81 82.57 20.75(progress subset)
LJ 84.34 83.56 13.84
SMS 80.31 82.56 7.10
TWITTER 2014 89.02 70.82 4.44
TWITTER SARCASM 85.71 57.63 28.57
Table 5: Performance on different sets for our
model on task A. The model evaluated on the de-
velopment set was only built using the training set.
Test set Positive Negative Neutral
DEV 69.80 60.40 66.70
TWITTER 2013 72.50 64.30 72.30(full)
TWITTER 2013 71.92 61.92 71.22(progress subset)
LJ 71.94 67.65 66.23
SMS 63.83 57.06 73.76
TWITTER 2014 74.26 55.58 66.76
TWITTER SARCASM 55.17 29.63 51.61
Table 6: Performance on different sets for our
model on task B.
Test set Task A Task B
LiveJournal 2014 4 / 27 13 / 50
SMS 2013 12 / 27 19 / 50
Twitter 2013 9 / 27 10 / 50
Twitter 2014 11 / 27 18 / 50
Twitter 2014 Sarcasm 10 / 27 33 / 50
Table 7: Position of our submission on each cor-
pus and task, according to results provided by the
organization on April 22, 2014.
level tasks, as can be observed in Table 7. This
system is a reduced version of a sentiment classifi-
cation model for Spanish texts that performed well
in the TASS 2013 (Villena et al., 2013). The offi-
cial results show how our approach works com-
petitively both on tasks A and B without needing
large and automatically-built resources. The ap-
proach is based on a bag-of-words that includes
word-forms and PoS-tags. We also extract psy-
chometric and sentiment information from exter-
nal lexicons. In order to reduce sparsity problems,
we firstly apply an information gain filter to select
only the most relevant features. Experiments on
the development set showed a significant improve-
ment on the same model with respect to skipping
it on subtask B.
414
Acknowledgements
Research reported in this paper has been partially
funded by Ministerio de Econom??a y Competitivi-
dad and FEDER (Grant TIN2010-18552-C03-02)
and by Xunta de Galicia (Grant CN2012/008).
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of Twitter data. In Proceedings of the Work-
shop on Languages in SocialMedia, LSM ?11, pages
30?38, Stroudsburg, PA, USA. ACL.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. AVAYA: Sentiment Analysis on
Twitter with Self-Training and Polarity Lexicon Ex-
pansion. Atlanta, Georgia, USA, page 333.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. The Journal
of Machine Learning Research, 9:1871?1874.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18, November.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Manesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, ACLShort ?09, pages 313?316,
Suntec, Singapore.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. pages 312?320, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
A. Pak and P. Paroubek. 2010. Twitter as a Corpus for
Sentiment Analysis and Opinion Mining. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10),
pages 1320?1326, Valletta, Malta, May. European
Language Resources Association (ELRA).
J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: LIWC 2001.
Mahway: Lawrence Erlbaum Associates, 71.
David Vilares, Miguel A. Alonso, and Carlos Go?mez-
Rodr??guez. 2013a. LyS at TASS 2013: Analysing
Spanish tweets by means of dependency pars-
ing, semantic-oriented lexicons and psychometric
word-properties. In Alberto D??az Esteban, In?aki
Alegr??a Loinaz, and Julio Villena Roma?n, editors,
XXIX Congreso de la Sociedad Espan?ola de Proce-
samiento de Lenguaje Natural (SEPLN 2013). TASS
2013 - Workshop on Sentiment Analysis at SEPLN
2013, pages 179?186, Madrid, Spain, September.
David Vilares, Miguel A. Alonso, and Carlos Go?mez-
Rodr??guez. 2013b. Supervised polarity classifica-
tion of Spanish tweets based on linguistic knowl-
edge. In DocEng?13. Proceedings of the 13th ACM
Symposium on Document Engineering, pages 169?
172, Florence, Italy, September. ACM.
David Vilares, Miguel A. Alonso, and Carlos Go?mez-
Rodr??guez. 2014. On the usefulness of lexical
and syntactic processing in polarity classification of
Twitter messages. Journal of the Association for In-
formation Science Science and Technology, to ap-
pear.
Julio Villena-Roma?n, Janine Garc??a-Morera, Cristina
Moreno-Garc??a, Sara Lana-Serrano, and Jose? Carlos
Gonza?lez-Cristo?bal. 2014. TASS 2013 ? a sec-
ond step in reputation analysis in Spanish. Proce-
samiento del Lenguaje Natural, 52:37?44, March.
415
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 25?35,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Language variety identification in Spanish tweets
Wolfgang Maier
Institute for Language and Information
University of D?usseldorf
D?usseldorf, Germany
maierw@hhu.de
Carlos G
?
omez-Rodr??guez
Depto. de Computaci?on
Universidade da Coru?na
A Coru?na, Spain
cgomezr@udc.es
Abstract
We study the problem of language vari-
ant identification, approximated by the
problem of labeling tweets from Spanish
speaking countries by the country from
which they were posted. While this task
is closely related to ?pure? language iden-
tification, it comes with additional com-
plications. We build a balanced collec-
tion of tweets and apply techniques from
language modeling. A simplified version
of the task is also solved by human test
subjects, who are outperformed by the
automatic classification. Our best auto-
matic system achieves an overall F-score
of 67.7% on 5-class classification.
1 Introduction
Spanish (or castellano), a descendant of Latin,
is currently the language with the second largest
number of native speakers after Mandarin Chi-
nese, namely around 414 million people (Lewis
et al., 2014). Spanish has a large number of re-
gional varieties across Spain and the Americas
(Lipski, 1994).
1
They diverge in spoken language
and vocabulary and also, albeit to a lesser extent,
in syntax. Between different American varieties
of Spanish, there are important differences; how-
ever, the largest differences can be found between
American and European (?Peninsular?) Spanish.
Language identification, the task of automati-
cally identifying the natural language used in a
given text segment, is a relatively well understood
problem (see Section 2). To our knowledge, how-
ever, there is little previous work on the identifica-
tion of the varieties of a single language, such as
the regional varieties of Spanish. This task is espe-
cially challenging because the differences between
1
We are aware that there are natively Spanish-speaking
communities elsewhere, such as on the Philippines, but we
do not consider them in this study.
variants are subtle, making it difficult to discern
between them. This is evidenced by the fact that
humans that are native speakers of the varieties
are often unable to solve the problem, particularly
when given short, noisy text segments (which are
the focus of this work) where the amount of avail-
able information is limited.
In this paper, we approximate the problem of
language variety identification by the problem
of classifying status messages from the micro-
blogging service Twitter (?tweets?) from Span-
ish speaking countries by the country from which
they were sent. With the tweet, the location of
the device from which the tweet was sent can be
recorded (depending on the Twitter users? permis-
sion) and can then be retrieved from the metadata
of the tweet. The tweet location information does
not always correlate with the actual language va-
riety used in the tweet: it is conceivable, e.g., that
migrants do not use the prevalent language vari-
ety of the country in which they live, but rather
their native variety. Nevertheless, Twitter can give
a realistic picture of actual language use in a cer-
tain region, which, additionally, is closer to spoken
than to standard written language. Eventually and
more importantly, Twitter data is available from
almost all Spanish speaking countries.
We proceed as follows. We build a balanced
collection of tweets sent by Twitter users from
five countries, namely Argentina, Chile, Colom-
bia, Mexico, and Spain. Applying different meth-
ods, we perform an automatic classification be-
tween all countries. In order to obtain a more de-
tailed view of the difficulty of our task, we also
investigate human performance. For this purpose,
we build a smaller sample of tweets from Ar-
gentina, Chile and Spain and have them classified
by both our system and three native human evalua-
tors. The results show that automatic classification
outperforms human annotators. The best variant
of our system, using a meta-classifier with voting,
25
reaches an overall F-score of 67.72 on the five-
class problem. On the two-class problem, human
classification is outperformed by a large margin.
The remainder of this paper is structured as fol-
lows. In the following section, we present related
work. Section 3 presents our data collection. Sec-
tions 4 and 5 present our classification methodol-
ogy and the experiments. Section 7 discusses the
results, and Section 8 concludes the article.
2 Related Work
Research on language identification has seen a va-
riety of methods. A well established technique is
the use of character n-gram models. Cavnar and
Trenkle (1994) build n-gram frequency ?profiles?
for several languages and classify text by match-
ing it to the profiles. Dunning (1994) uses lan-
guage modeling. This technique is general and
not limited to language identification; it has also
been successfully employed in other areas, e.g., in
authorship attribution (Ke?selj et al., 2003) and au-
thor native language identification (Gyawali et al.,
2013). Other language identification systems use
non-textual methods, exploiting optical properties
of text such as stroke geometry (Muir and Thomas,
2000), or using compression methods which rely
on the assumption that natural languages differ
by their entropy, and consequently by the rate
to which they can be compressed (Teahan, 2000;
Benedetto et al., 2002). Two newer approaches
are Brown (2013), who uses character n-grams,
and
?
Reh?u?rek and Kolkus (2009), who treat ?noisy?
web text and therefore consider the particular in-
fluence of single words in discriminating between
languages.
Language identification is harder the shorter the
text segments whose language is to be identified
(Baldwin and Lui, 2010). Especially due to the
rise of Twitter, this particular problem has recently
received attention. Several solutions have been
proposed. Vatanen et al. (2010) compare character
n-gram language models with elaborate smooth-
ing techniques to the approach of Cavnar and
Trenkle and the Google Language ID API, on the
basis of different versions of the Universal Decla-
ration of Human Rights. Other researchers work
on Twitter. Bergsma et al. (2012) use language
identification to create language specific tweet col-
lections, thereby facilitating more high-quality re-
sults with supervised techniques. Lui and Baldwin
(2014) review a wide range of off-the-shelf tools
for Twitter language identification, and achieve
their best results with a voting over three individ-
ual systems, one of them being langid.py (Lui
and Baldwin, 2012). Carter et al. (2013) exploit
particular characteristics of Twitter (such as user
profile data and relations between Twitter users)
to improve language identification on this genre.
Bush (2014) successfully uses LZW compression
for Twitter language identification.
Within the field of natural language processing,
the problem of language variant identification has
only begun to be studied very recently. Zampieri
et al. (2013) have addressed the task for Spanish
newspaper texts, using character and word n-gram
models as well as POS and morphological infor-
mation. Very recently, the Discriminating between
Similar Languages (DSL) Shared Task (Zampieri
et al., 2014) proposed the problem of identify-
ing between pairs of similar languages and lan-
guage variants on sentences from newspaper cor-
pora, one of the pairs being Peninsular vs. Argen-
tine Spanish. However, all these approaches are
tailored to the standard language found in news
sources, very different from the colloquial, noisy
language of tweets, which presents distinct chal-
lenges for NLP (Derczynski et al., 2013; Vilares et
al., 2013). Lui and Cook (2013) evaluate various
approaches to classify documents into Australian,
British and Canadian English, including a corpus
of tweets, but we are not aware of any previous
work on variant identification in Spanish tweets.
A review of research on Spanish varieties from
a linguistics point of view is beyond the scope of
this article. Recommended further literature in this
area is Lipski (1994), Quesada Pacheco (2002)
and Alvar (1996b; 1996a).
3 Data Collection
We first built a collection of tweets using the
Twitter streaming API,
2
requesting all tweets sent
within the geographic areas given by the coordi-
nates -120
?
, -55
?
and -29
?
, 30
?
(roughly delimit-
ing Latin America), as well as -10
?
, 35
?
and 3
?
,
46
?
(roughly delimiting Spain). The download ran
from July 2 to July 4, 2014. In a second step, we
sorted the tweets according to the respective coun-
tries.
Twitter is not used to the same extent in all
countries where Spanish is spoken. In the time
2
https://dev.twitter.com/docs/api/
streaming
26
it took to collect 2,400 tweets from Bolivia,
we could collect over 700,000 tweets from Ar-
gentina.
3
To ensure homogeneous conditions for
our experiments, our final tweet collection com-
prises exactly 100,000 tweets from each of the five
countries from which most tweets were collected,
that is, Argentina, Chile, Colombia, Mexico, and
Spain.
At this stage, we do not perform any cleanup
or normalization operations such as, e.g., deleting
forwarded tweets (?re-tweets?), deleting tweets
which are sent by robots, or tweets not written in
Spanish (some tweets use code switching, or are
entirely written in a different language, mostly in
English or in regional and minority languages that
coexist with Spanish in the focus countries). Our
reasoning behind this is that the tweet production
in a certain country captures the variant of Spanish
that is spoken.
We mark the start and end of single tweets by
<s> and </s>, respectively. We use 80% of the
tweets of each language for training, and 10% for
development and testing, respectively. The data
is split in a round-robin fashion, i.e., every ninth
tweet is put into the development set and every
tenth tweet is put in the test set, all other tweets
are put in the training set.
In order to help with the interpretation of clas-
sification results, we investigate the distribution of
tweet lengths on the development set, as shown in
Figure 1. We see that in all countries, tweets tend
to be either short, or take advantage of all available
characters. Lengths around 100 to 110 characters
are the rarest. The clearest further trend is that the
tweets from Colombia and, especially, Argentina
tend to be shorter than the tweets from the other
countries.
4 Automatic Tweet Classification
The classification task we envisage is similar to
the task of language identification in short text
segments. We explore three methods that have
been used before for that task, namely character
n-gram frequency profiles (Cavnar and Trenkle,
1994; Vatanen et al., 2010), character n-gram lan-
guage models (Vatanen et al., 2010), as well as
LZW compression (Bush, 2014). Furthermore, we
explore the usability of syllable-based language
3
We are aware that the Twitter API does not make all sent
tweets available. However, we still assume that this huge dif-
ference reflects a variance in the number of Twitter users.
 0
 50
 100
 150
 200
 0  20  40  60  80  100  120  140
#
Tweet length
ESCLCOARMX
Figure 1: Tweet length distribution
50 100 500 1k 10k
AR 31.68 29.72 43.93 31.77 18.42
CO 24.29 21.36 26.14 19.68 19.03
MX 31.86 28.97 32.58 30.28 22.27
ES 20.19 25.22 22.08 21.25 16.15
CL 22.95 29.74 35.67 26.01 16.69
Table 1: Results (F
1
): n-gram frequency profiles
(classes/profile sizes)
models. For all four approaches, we train mod-
els for binary classification for each class, i.e., five
models that decide for each tweet if it belongs to a
single class. As final label, we take the output of
the one of the five classifiers that has the highest
score.
We finally use a meta-classifier on the basis of
voting. All methods are tested on the development
set. For evaluation, we compute precision, recall
and F
1
overall as well as for single classes.
Note that we decided to rely on the tweet text
only. An exploration of the benefit of, e.g., directly
exploiting Twitter-specific information (such as
user mentions or hash tags) is out of the scope of
this paper.
4.1 Character n-gram frequency profiles
We first investigate the n-gram frequency ap-
proach of Cavnar and Trenkle (1994). We use the
well-known implementation TextCat.
4
The re-
sults for all classes with different profile sizes are
shown in Table 1. Table 2 shows precision and re-
call for the best setting, a profile with a maximal
size of 500 entries.
The results obtained with a profile size of 500
4
As available from http://odur.let.rug.nl/
?
vannoord/TextCat/.
27
class precision recall F
1
AR 32.60 67.33 43.93
CO 31.66 22.26 26.14
MX 51.52 23.82 32.58
ES 32.83 16.63 22.08
CL 31.96 40.36 35.67
overall 34.08 34.08 34.08
Table 2: Results: n-gram frequency profile with
500 n-grams
AR CO MX ES CL
AR 6,733 949 384 610 1,324
CO 4,207 2,226 720 803 2,044
MX 2,547 1,342 2,382 1,051 2,678
ES 3,781 1,361 649 1,663 2,546
CL 3,384 1,153 488 939 4,036
Table 3: Confusion matrix (n-gram freq. profiles,
500 n-grams)
entries for Colombia align with the results for
Spain and Mexico in that the precision is higher
than the recall. The results for Chile align with
those for Argentina with the recall being higher
than the precision. For Mexico and Argentina the
differences between recall and precision are par-
ticularly large (28 and 35 points, respectively).
The confusion matrix in Table 3 reveals that tweets
from all classes are likely to be mislabeled as
coming from Argentina, while, on the other hand,
Mexican tweets are mislabeled most frequently as
coming from other countries.
Overall, the n-gram frequency profiles are not
very good at our task, achieving an maximal over-
all F-score of only 34.08 with a profile size of 500
entries. However, this performance is still well
above the 20.00 F-score we would obtain with
a random baseline. Larger profile sizes deterio-
rate results: with 10,000 entries, we only have
an overall F-score of 18.23. As observed before
(Vatanen et al., 2010), the weak performance can
most likely be attributed to the shortness of the
tweets and the resulting lack of frequent n-grams
that hinders a successful profile matching. While
Vatanen et al. alleviate this problem to some ex-
tent, they have more success with character-level
n-gram language models, the approach which we
explore next.
 40
 45
 50
 55
 60
 65
 70
 2  3  4  5  6
F-sc
ore
n-gram order
no pruning0.01 pruning0.1 pruning1 pruning
Figure 2: Character n-gram lm: Pruning vs. n-
gram order
4.2 Character n-gram language models
We recur to n-gram language models as avail-
able in variKN (Siivola et al., 2007).
5
We run
variKN with absolute discounting and the cross-
product of four different pruning settings (no prun-
ing, and thresholds 0.01, 0.1 and 1) and five differ-
ent n-gram lengths (2 to 6).
Figure 2 contrasts the effect of different pruning
settings with different n-gram lengths. While ex-
cessive pruning is detrimental to the result, slight
pruning has barely any effect on the results, while
reducing look-up time immensely. The order of
the n-grams, however, does have an important in-
fluence. We confirm that also for this problem, we
do not benefit from increasing it beyond n = 6,
like Vatanen et al. (2010).
We now check if some countries are more dif-
ficult to identify than others and how they bene-
fit from different n-gram orders. Figure 3 visual-
izes the corresponding results. Not all countries
profit equally from longer n-grams. When com-
paring the 3- and 6-gram models without pruning,
we see that the F
1
for Argentina is just 8 points
higher, while the difference is more than 14 points
for Mexico.
Table 4 shows all results including precision and
recall for all classes, in the setting with 6-grams
and no pruning. We can see that this approach
works noticeably better than the frequency pro-
files, achieving an overall F-score of 66.96. The
behavior of the classes is not uniform: Argentina
shows the largest difference between precision and
recall, and is furthermore the only class in which
precision is higher than recall. Note also that in
5
https://github.com/vsiivola/variKN
28
 35
 40
 45
 50
 55
 60
 65
 70
 2  3  4  5  6
F-sc
ore
n-gram order
ESCLCOARMX
Figure 3: Character n-gram lm: Classes vs. n-
gram order (no pruning)
class precision recall F
1
AR 70.67 66.22 68.37
CO 62.56 62.77 62.66
MX 65.23 65.74 65.48
ES 68.75 69.36 69.06
CL 67.81 70.73 69.24
overall 66.96 66.96 66.96
Table 4: Results: 6-grams without pruning
general, the differences between precision and re-
call are lower than for the n-gram frequency pro-
file approach. The confusion matrix shown in Ta-
ble 5 reveals that the Colombia class is the one
with the highest confusion, particularly in com-
bination with the Mexican class. This could in-
dicate that those classes are more heterogeneous
than the others, possibly showing more Twitter-
specific noise, such as tweets consisting only of
URLs, etc.
We finally investigate how tweet length influ-
ences classification performance in the 6-gram
model. Figure 4 shows the F-scores for intervals
of length 20 for all classes. The graph confirms
that longer tweets are easier to classify. This cor-
relates with findings from previous work. Over
82 points F
1
are achieved for tweets from Chile
AR CO MX ES CL
AR 6,622 1,036 702 740 900
CO 800 6,277 1,151 875 897
MX 509 1,237 6,574 847 833
ES 630 850 857 6,936 727
CL 809 634 794 690 7,073
Table 5: Confusion matrix (6-grams, no pruning)
 40
 50
 60
 70
 80
 90
 20  40  60  80  100  120  140
F-sc
ore
length
ESCLCOARMX
Figure 4: Character n-grams: Results (F
1
) for
tweet length intervals
 40
 50
 60
 70
 80
 90
 20  40  60  80  100  120  140
prec
isio
n/re
call
length
CO precisionCO recallCL precisionCL recall
Figure 5: Character n-grams: Precision/recall for
AR and CL
longer than 120 characters, while for those con-
taining up to 20 characters, F
1
is almost 30 points
lower. We investigate precision and recall sepa-
rately. Figure 5 shows the corresponding curves
for the best and worst performing classes, namely,
CL and CO. For Chile, both precision and recall
develop in parallel to the F-score (i.e., the longer
the tweets, the higher the scores). For Colombia,
the curves confirm that the low F
1
is rather due to
a low precision than a low recall, particularly for
tweets longer than 40 characters. This correlates
with the counts in the confusion table (Tab. 5).
4.3 Syllable n-gram language models
Since varieties of Spanish exhibit differences in
vocabulary, we may think that models based on
word n-grams can be more useful than character
n-grams to discriminate between varieties. How-
ever, the larger diversity of word n-grams means
that such models run into sparsity problems. An
intermediate family of models can be built by us-
29
 45
 50
 55
 60
 65
 2  3  4
F-sc
ore
n-gram order
ESCLCOARMX
Figure 6: Syllable n-gram lm: pruning vs. n-gram
order
ing syllable n-grams, taking advantage of the fact
that Spanish variants do not differ in the criteria
for syllabification of written words. Note that this
property does not hold in general for the language
identification problem, as different languages typ-
ically have different syllabification rules, which is
a likely reason why syllable n-gram models have
not been used for this problem.
To perform the splitting of Spanish words into
syllables, we use the TIP syllabifier (Hern?andez-
Figeroa et al., 2012), which applies an algorithm
implementing the general syllabification rules de-
scribed by the Royal Spanish Academy of Lan-
guage and outlined in standard Spanish dictionar-
ies and grammars. These rules are enough to cor-
rectly split the vast majority of Spanish words, ex-
cluding only a few corner cases related with word
prefixes (Hern?andez-Figueroa et al., 2013). While
accurate syllabification requires texts to be written
correctly with accented characters, and this is of-
ten not the case in informal online environments
(Vilares et al., 2014); we assume that this need not
cause problems because the errors originated by
unaccented words will follow a uniform pattern,
producing a viable model for the purposes of clas-
sification.
We train n-gram language models with
variKN as described in the last section, using
absolute discounting. Due to the larger vocabulary
size, we limit ourselves to 0.01 pruning, and to
n-gram orders 2 to 4. Figure 6 shows the results
(F
1
) of all classes for the different n-gram orders,
and Table 6 shows the results for all classes for
the 4-gram language model.
As expected, shorter n-grams are more effective
for syllable than for character language models.
class precision recall F
1
AR 55.94 61.11 58.41
CO 53.23 53.03 53.13
MX 59.10 56.17 57.60
ES 62.35 56.96 59.53
CL 59.31 62.12 60.68
overall 57.88 57.88 57.88
Table 6: Results (F
1
): Syllable 4-gram lm
For the Chilean tweets, e.g., the F-score for the 2-
gram language model is around 11 points higher
than for the character 2-gram language model.
Furthermore, the performance seems to converge
earlier, given that the results change only slightly
when raising the n-gram order from 3 to 4. The
overall F-score for the 4-gram language model is
around 6 points lower than for character 4-grams.
However, the behavior of the classes is similar:
again, Mexico and Colombia have slightly lower
results than the other classes.
4.4 Compression
We eventually test the applicability of
compression-based classification using the
approach of Bush (2014). As mentioned ear-
lier, the assumption behind compression-based
strategies for text categorization is that different
text categories have a different entropy. Clas-
sification is possible because the effectivity of
compression algorithms depends on the entropy
of the data to be compressed (less entropy ? more
compression).
A simple classification algorithm is Lempel-
Ziv-Welch (LZW) (Welch, 1984). It is based on
a dictionary which maps sequences of symbols to
unique indices. Compression is achieved by re-
placing sequences of input symbols with the re-
spective dictionary indices. More precisely, com-
pression works as follows. First, the dictionary
is initialized with the inventory of symbols (i.e.,
with all possible 1-grams). Then, until the input is
fully consumed, we repeat the following steps. We
search the dictionary for the longest sequence of
symbols s that matches the current input, we out-
put the dictionary entry for s, remove s from the
input and add s followed by the next input symbol
to the dictionary.
For our experiments, we use our own imple-
mentation of LZW. We first build LZW dictionar-
ies by compressing our training sets as described
30
1k 8k 25k 50k
AR 28.42 38.78 46.92 51.89
CO 19.81 28.27 32.81 36.05
MX 22.07 33.90 43.10 45.06
ES 22.08 29.48 35.15 38.61
CL 27.08 28.22 33.59 36.68
Table 7: Results (F
1
): LZW without ties
above, using different limits on dictionary lengths.
As symbol inventory, we use bytes, not unicode
symbols. Then we use these dictionaries to com-
press all tweets from all test sets, skipping the ini-
tialization stage. The country assigned to each
tweet is the one whose dictionary yields the high-
est compression. We run LZW with different max-
imal dictionary sizes.
The problem with the evaluation of the results
is that the compression produced many ties, i.e.,
the compression of a single tweet with dictionaries
from different languages resulted in identical com-
pression rates. On the concatenated dev sets (50k
tweets, i.e., 10k per country) with a maximal dic-
tionary size of 1k, 8k, 25k and 50k entries, we got
14.867, 20,166, 22,031, and 23,652 ties, respec-
tively. In 3,515 (7%), 4,839 (10%), 5,455 (11%)
and 6,102 (12%) cases, respectively, the correct re-
sult was hidden in a tie. If we replace the labels
of all tied instances with a new label TIE, we ob-
tain the F-scores shown in Table 7. While they are
higher than the scores for n-gram frequency pro-
files, they still lie well below the results for both
syllable and character language models.
While previous literature mentions an ideal size
limit on the dictionary of 8k entries (Bush, 2014),
we obtain better results the larger the dictionaries.
Note that already with a dictionary of size 1000,
even without including the ties, we are above the
20.00 F-score of a random baseline. The high
rate of ties constitutes a major problem of this ap-
proach, and remains even if we would find im-
provements to the approach (one possibility could
be to use unicode characters instead of bytes for
dictionary initialization). It cannot easily be alle-
viated, because if the compression rate is taken as
the score, particularly the scores for short tweets
are likely to coincide.
4.5 Voting
Voting is a simple meta-classifying technique
which takes the output of different classifiers and
class precision recall F
1
AR 70.96 68.36 69.64
CO 62.44 64.22 63.32
MX 66.37 65.67 66.02
ES 70.10 69.64 69.87
CL 68.97 70.72 69.83
overall 67.72 67.72 67.72
Table 8: Results: Voting
decides based on a predefined method on one of
them, thereby combining their strengths and level-
ing out their weaknesses. It has been successfully
used to improve language identification on Twitter
data by Lui and Baldwin (2014).
We utilize the character 5-gram and 6-gram lan-
guage models without pruning, as well as the syl-
lable 3-gram and 4-gram models. We decide as
follows. All instances for which the output of the
5-gram model coincides with the output of at least
one of the syllable models are labeled with the out-
put of the 5-gram model. For all other instances,
the output of the 6-gram model is used. The corre-
sponding results for all classes are shown in Table
8.
We obtain a slightly higher F-score than for
the 6-gram character language model (0.8 points).
In other words, even though the 6-gram language
model leads to the highest overall results among
individual models, in some instances it is out-
performed by the lower-order character language
model and by the syllable language models, which
have a lower overall score.
5 Human Tweet Classification
In order to get a better idea of the difficulty of the
task of classifying tweets by the country of their
authors, we have tweets classified by humans.
Generally, speakers of Spanish have limited
contact with speakers of other varieties, simply
due to geographical separation of varieties. We
therefore recur to a simplified version of our task,
in which the test subjects only have to distinguish
their own variety from one other variety, i.e., per-
form a binary classification. We randomly draw
two times 150 tweets from the Argentinian test and
150 tweets from the Chilean and Spanish test sets,
respectively. We then build shuffled concatena-
tions of the first 150 Argentinian and the Chilean
tweets, as well as of the remaining 150 Argen-
tinian and the Spanish tweets. Then we let three
31
data subject class prec. rec. F
1
AR-ES AR AR 68.5 76.7 72.3
ES 73.5 64.7 68.8
ES AR 71.5 62.0 66.4
ES 66.5 75.3 70.6
n-gram AR 92.3 87.3 89.7
ES 88.0 92.7 90.3
AR-CL AR AR 61.0 77.3 68.2
CL 69.1 50.7 58.5
CL AR 70.0 70.0 70.0
CL 70.0 70.0 70.0
n-gram AR 93.4 84.7 88.8
CL 86.0 94.0 89.8
Table 9: Results: Human vs. automatic classifica-
tion
natives classify them. The test subjects are not
given any other training data samples or similar re-
sources before the task, and they are instructed not
to look up on the Internet any information within
the tweet that might reveal the country of its author
(such as hyperlinks, user mentions or hash tags).
Table 9 shows the results, together with the re-
sults on the same task of the character 6-gram
model without pruning. Note that with 300 test
instances out of 20,000, there is a sampling er-
ror of ? 4.7% (confidence interval 95%). The re-
sults confirm our intuition in the light of the good
performance achieved by the n-gram approach in
the 5-class case: when reducing the classification
problem from five classes to two, human classi-
fication performance is much below the perfor-
mance of automatic classification, by between 17
and 31 F-score points. In terms of error rate, the
human annotators made between 3 and 4 times
more classification errors than the automatic sys-
tem. One can observe a tendency among the hu-
man test subjects that more errors come from la-
beling too many tweets as coming from their na-
tive country than vice versa (cf. the recall values).
In order to better understand the large result dif-
ference, we ask the test subjects for the strategies
they used to label tweets. They stated that the eas-
iest tweets where those specifying a location (?Es-
toy en Madrid?), or referencing local named en-
tities (TV programs, public figures, etc.). In case
of absence of such information, other clues were
used that tend to occur in only one variety. They
include the use of different words (such as en-
fadado (Spain) vs. enojado (America) (?angry?)),
data subject class prec. rec. F
1
AR-ES AR AR 71.8 80.0 75.7
ES 74.8 65.4 69.7
ES AR 74.6 62.9 68.2
ES 65.1 76.3 70.2
n-gram AR 93.2 88.6 90.8
ES 88.1 92.9 90.4
AR-CL AR AR 61.1 78.6 68.8
CL 68.8 48.5 56.9
CL AR 73.0 71.4 72.2
CL 71.2 72.8 72.0
n-gram AR 95.3 87.1 91.0
CL 87.8 95.6 91.5
Table 10: Results: Human vs. automatic classifi-
cation (filtered)
a different distribution of the same word (such as
the filler pues), and different inflection, such as the
second person plural verb forms, which in Amer-
ican Spanish, albeit sometimes not in Chile, is re-
placed by the identical third person plural forms
(for the verb hacer (?do?), the peninsular form
would be hac?eis instead of hacen), and the per-
sonal pronoun vos (?you?), which is rarely used
in Chile, and not used in Spain. To sum up, the
test subjects generally relied on lexical cues on
the surface, and were therefore bound to miss non-
obvious information captured by the character n-
gram model.
Since the test subjects also stated that some
tweets were impossible to assign to a country be-
cause they contained only URLs, emoticons, or
similar, in Table 10 we show a reevaluation of a
second version of the two shuffled concatenated
samples in which we remove all tweets which con-
tain only emoticons, URLs, or numbers; tweets
which are entirely written in a language other than
Spanish; and tweets which are only two or one
words long (i.e., tweets with zero or one spaces).
For the AR-ES data, we remove 23 Spanish and
10 Argentinian tweets, while for the AR-CL data,
we remove 10 Argentinian and 14 Chilean tweets.
As for the human classification on the AR/ES
data, the results for Spain do not change much. For
Argentina, there is an increase in performance (2
to 3 points). On the AR/CL data, there is a slight
improvement on all sets except for the Chilean
data classified.
As for the automatic classification, the filter-
ing gives better result on all data sets. However,
32
training dev test
AR 57,546 (71.9%) 7,174 7,196
CO 58,068 (72.6%) 7,249 7,289
MX 48,527 (60.7%) 6,117 6,061
ES 53,199 (66.5%) 6,699 6,657
CL 56,865 (71.1%) 6,998 7,071
Table 11: Data sizes (filtered by langid.py)
the difference between the F
1
of the filtered and
unfiltered data is larger on the AR/CL data set.
This can be explained with the fact that among
the tweets removed from the AR/ES data set, there
were more longer tweets (not written in Spanish)
than among the tweets removed from the CL/AR
data set, the longer tweets being easier to iden-
tify. Note that the filtering of tweets does not cause
much change in the difference between human and
automatic classification.
6 Language Filtering
As mentioned before, our data has not been
cleaned up or normalized. In particular, the data
set contains tweets written in languages other than
Spanish. We have reasoned that those can be seen
as belonging to the ?natural? language production
of a country. However, in order to see what im-
pact they have on our classification results, we
perform an additional experiment on a version of
the data were we only include the tweets that the
state-of-the-art language identifier langid.py
labels Spanish (Lui and Baldwin, 2012).
6
Table
11 shows the sizes of all data sets after filtering.
Note that many of the excluded tweets are in fact
written in Spanish, but are very noisy, due to or-
thography, Twitter hash tags, etc. The next most
frequent labels across all tweets is English (9%).
Note that in the data from Spain, 2% of the tweets
are labeled as Catalan, 1.2% as Galician, and only
0.3% as Basque.
Table 12 finally shows the classification re-
sults for character 6-gram language models with-
out pruning.
The changes in F
1
are minor, i.e., below one
point, except for the Mexican tweets, which lose
around 4 points. The previous experiments have
already indicated that the Mexican data set is the
most heterogeneous one which also resulted in the
largest number of tweets being filtered out. In
general, we see that the character n-gram method
6
https://github.com/saffsd/langid.py.
class precision recall F
1
AR 70.32 66.09 68.14
CO 63.76 62.22 62.98
MX 61.52 61.11 61.31
ES 69.13 69.20 69.17
CL 67.12 73.29 70.07
overall 66.45 66.45 66.45
Table 12: Results: Filtered by langid.py
seems to be relatively stable with respect to a dif-
ferent number of non-Spanish tweets in the data.
More insight could be obtained by performing ex-
periments with advanced methods of tweet nor-
malization, such as those of Han and Baldwin
(2011). We leave this for future work.
7 Discussion
Human classification of language varieties was
judged by our test subjects to be considerably
more difficult that differentiating between lan-
guages. Additionally, the test subjects were only
able to differentiate between two classes. While
the automatic classification results lie below the
results which one would expect for language iden-
tification, n-gram classification still achieves good
performance.
Our experiments touch on the more general
question of how a language variety is defined. In
order to take advantage of the metadata provided
by Twitter, we had to restrict the classification
problem to identifying varieties associated with
countries were tweets were sent. In reality, the
boundaries between variants are often blurred, and
there can also be variance within the same country
(e.g., the Spanish spoken in the southern Spanish
region of Andalusia is different from that of As-
turias, even if they both share features common
to Peninsular Spanish and larger differences with
American Spanish). However, it would be diffi-
cult to obtain a reliable corpus with this kind of
fine-grained distinctions.
It is also worth noting that not all the classifica-
tion criteria used by the human test subjects were
purely linguistic ? for example, a subject could
guess a tweet as being from Chile by recogniz-
ing a mention to a Chilean city, public figure or
TV show. Note that this factor intuitively seems to
benefit humans ? who have a wealth of knowledge
about entities, events and trending topics from
their country ? over the automatic system. In spite
33
of this, automatic classification still vastly outper-
formed human classification, suggesting that the
language models are capturing linguistic patterns
that are not obvious to humans.
8 Conclusion
We have studied different approaches to the task
of classifying tweets from Spanish-speaking coun-
tries according to the country from which they
were sent. To the best of our knowledge, these are
the first results for this problem. On the problem
of assigning one of five classes (Argentina, Mex-
ico, Chile, Colombia, Spain) to 10,000 tweets, the
best performance, an overall F-score of 67.72, was
obtained with a voting meta-classifier approach
that recombines the results for four single clas-
sifiers, the 6-gram (66.96 F
1
) and 5-gram (66.75
F
1
) character-based language models, and the 4-
gram (57.87 F
1
) and 3-gram (57.24 F
1
) syllable-
based language models. For a simplified version
of the problem that only required a decision be-
tween two classes (Argentina vs. Chile and Spain
vs. Argentina), given a sample of 150 tweets from
each class, human classification was outperformed
by automatic classification by up to 31 points.
In future work, we want to investigate the ef-
fect of tweet normalization on our problem, and
furthermore, how the techniques we have used can
be applied to classify text from other social media
sources, such as Facebook.
Acknowledgements
The first author has been funded by Deutsche
Forschungsgemeinschaft (DFG). The second au-
thor has been partially funded by Ministerio
de Econom??a y Competitividad/FEDER (Grant
TIN2010-18552-C03-02) and by Xunta de Galicia
(Grant CN2012/008).
References
Manuel Alvar, editor. 1996a. Manual de dialectolog??a
hisp?anica. El espa?nol de Am?erica. Ariel, Barcelona.
Manuel Alvar, editor. 1996b. Manual de dialectolog??a
hisp?anica. El espa?nol de Espa?na. Ariel, Barcelona.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237, Los Angeles, CA.
Dario Benedetto, Emanuele Caglioti, and Vittorio
Loreto. 2002. Language trees and zipping. Phys-
ical Review Letters, 88(4).
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, LSM ?12, pages 65?
74, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ralph D. Brown. 2013. Selecting and weighting n-
grams to identify 1100 languages. In Springer, ed-
itor, Proceedings of the 16th International Confer-
ence on Text, Speech, and Dialogue, volume 8082 of
LNCS, pages 475?483, Pilsen, Czech Republic.
Brian O. Bush. 2014. Language identication of tweets
using LZW compression. In 3rd Pacific Northwest
Regional NLP Workshop: NW-NLP 2014, Redmond,
WA.
Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
overcoming the limitations of short, unedited and id-
iomatic text. Language Resources and Evaluation,
47(1):195?215.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175,
Las Vegas, NV.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing, pages
198?206. Association for Computational Linguis-
tics.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab, New Mexico State University.
Binod Gyawali, Gabriela Ramirez, and Thamar
Solorio. 2013. Native language identification: a
simple n-gram based approach. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 224?231,
Atlanta, Georgia, June. Association for Computa-
tional Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 368?378, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Zen?on Hern?andez-Figeroa, Gustavo Rodr??guez-
Rodr??guez, and Francisco J. Carreras-
Riudavets. 2012. Separador de s??labas
34
del espa?nol - silabeador TIP. Available at
http://tip.dis.ulpgc.es.
Zen?on Hern?andez-Figueroa, Francisco J. Carreras-
Riudavets, and Gustavo Rodr??guez-Rodr??guez.
2013. Automatic syllabification for Spanish
using lemmatization and derivation to solve the
prefix?s prominence issue. Expert Syst. Appl.,
40(17):7122?7131.
Vlado Ke?selj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for
authorship attribution. In Proceedings of PACLING,
pages 255?264.
M. Paul Lewis, Gary F. Simons, and Charles D. Fen-
nig, editors. 2014. Ethnologue: Languages of
the World. SIL International, Dallas, Texas, sev-
enteenth edition edition. Online version: http:
//www.ethnologue.com.
John M. Lipski. 1994. Latin American Spanish. Long-
man, London.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 25?30, Jeju Island, Korea, July. Association
for Computational Linguistics.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden.
Marco Lui and Paul Cook. 2013. Classifying english
documents by national dialect. In Proceedings of
the Australasian Language Technology Association
Workshop 2013 (ALTA 2013), pages 5?15, Brisbane,
Australia, December.
Douglas W. Muir and Timothy R. Thomas. 2000. Au-
tomatic language identification by stroke geometry
analysis, May 16. US Patent 6,064,767.
Miguel
?
Angel Quesada Pacheco. 2002. El Espa?nol
de Am?erica. Editorial Tecnol
?ogica de Costa Rica,
Cartago, 2a edition.
Vesa Siivola, Teemu Hirsim?aki, and Sami Virpi-
oja. 2007. On growing and pruning kneser-
ney smoothed n-gram models. IEEE Transac-
tions on Speech, Audio and Language Processing,
15(5):1617?1624.
William J. Teahan. 2000. Text classification and seg-
mentation using minimum cross-entropy. In Pro-
ceedings of RIAO?00, pages 943?961.
Tommi Vatanen, Jaakko J. Vyrynen, and Sami Virpi-
oja. 2010. Language identification of short text
segments with n-gram models. In Proceedings
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
David Vilares, Miguel A. Alonso, and Carlos G?omez-
Rodr??guez. 2013. Supervised polarity classification
of spanish tweets based on linguistic knowledge. In
Proceedings of 13th ACM Symposium on Document
Engineering (DocEng 2013), pages 169?172, Flo-
rence, Italy.
David Vilares, Miguel A. Alonso, and Carlos G?omez-
Rodr??guez. 2014. A syntactic approach for opinion
mining on Spanish reviews. Natural Language En-
gineering, FirstView:1?25, 6.
Radim
?
Reh?u?rek and Milan Kolkus. 2009. Language
identification on the web: Extending the dictionary
method. In Proceedings of CICLing, pages 357?
368.
Terry A. Welch. 1984. A technique for high-
performance data compression. Computer, 17(6):8?
19, June.
Marcos Zampieri, Binyam Gebrekidan Gebre, and
Sascha Diwersy. 2013. N-gram language models
and pos distribution for the identification of spanish
varieties. In Proceedings of TALN2013, pages 580?
587.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and
J?org Tiedemann. 2014. A report on the dsl shared
task 2014. In Proceedings of the First Workshop
on Applying NLP Tools to Similar Languages, Va-
rieties and Dialects, pages 58?67, Dublin, Ireland,
August. Association for Computational Linguistics
and Dublin City University.
35
