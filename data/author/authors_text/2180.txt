SpokenDialogueforVirtualAdvisersinasemi-imme rsiveCommand
andControlenvironment
DominiqueEstival,MichaelBroughton,AndrewZschor n,ElizabethPronger
HumanSystemsIntegrationGroup,CommandandContro lDivision
DefenceScienceandTechnologyOrganisation
POBox1500,EdinburghSA5111
AUSTRALIA
{Dominique.Estival,Michael.Broughton,AndrewZscho rn}@dsto.defence.gov.au


Abstract
We present the spoken dialogue system
designed and implemented for Virtual
Advisers in the FOCAL environment. Its
architectureisbasedon:DialogueAgents
using propositional attitudes, a Natural
Language Understanding component
using typed unification grammar, and a
commercial speaker-independent speech
recognition system. The current
application aims to facilitate the multi-
media presentation of military planning
information in a semi-immersive
environment.
1 Introduction
In this paper, we present the spoken dialogue
system implemented for communicating with the
virtual advisers (VAs) in the Future Operations
Centre Analysis Laboratory (FOCAL) at the
Australian Defence Science and Technology
Organisation(DSTO).Weareexperimentingwith
the use of spoken dialogue with virtual
conversational characters to access multi-media
information during the conduct of military
operations and in particular to facilitate the
planningofsuchoperations.
Unlike telephone-based dialogue systems
(Estival, 2002),whicharemainlycreated fornew
commercial applications, dialogue systems for
Command and Control applications (Moore et al
1997) generally seek to simulate the military
domain and therefore require an understanding of
thatdomain.
2 UsingVirtualAdvisersinFOCAL
FOCAL was established to "pioneer a paradigm
shiftincommandenvironmentsthroughasuperior
useof capability andgreater situationawareness".
The facility was designed to experiment with
innovativetechnologiestosupportthisgoal,andi t
hasnowbeenrunningfortwoyears.
FOCAL contains a large-screen, semi-
immersive virtual reality environment as its
primary display, allowing vast quantities of
informationtobedisplayed. OurcurrentVAscan
be described as 3-dimensional "Talking Heads",
i.e. only the head and upper portions of the body
are represented. Theycandisplayexpression, lip-
synchronisation and head movement, along with
certain autonomous behaviours such as blinking
and gaze (Taplin et al, 2001). These factors all
combinetoaddlife-likenesstotheVAsandcreate
moreengaginginteractionwithusers.
Presenting information via aTalkingHeadhas
been commercially demonstrated by the virtual
newscaster ?Ananova? (Ananova, 2002).
Embodiedcharactersarealsobeingdevelopedand
include the PPP (Andre, Rist and Muller, 1998)
and Rea (Cassell, 2000).  PPP is a cartoon style
Personalized Plan-based Presenter that combines
pointing, head movements and facial expressions
to draw the viewer?s attention to the information
beingpresented. Rea isa virtual real-estateagen t
that takesanactive role inconversation, shenods
herheadtoindicateunderstandingofspokeninput,
orcanraiseherhandtoindicateadesiretospeak .
Several VAs have been implemented for
FOCAL, each having a particular role or
knowledge expertise.  For example, one adviser
may have specialist knowledge relating to legal
issues, another may have information relating to
the geography of a region.  Each VA has a
differentfacialappearance,voiceandmannerisms.
To demonstrate and evaluate the performance
of VAs (and of the other FOCAL projects), a
fictitious scenario has been developed that
incorporates key elements of military planning at
the operational level (see section 8).  The VAs
provide information rich briefs through the
combineduseofspokenoutputviaText-to-Speech
(TTS)andmultimedia.  Relevantquestionscanbe
asked at the end of the briefs through the use of
spokendialogue.
3 Previousimplementation:Franco
Asdescribedin(Taplinetal.,2001)thefirstVA in
FOCAL, named Franco, was also an animated 3-
dimensional "Talking Head" model, intended to
either deliver prepared information, such as a
briefing or slide show, or to interact
conversationally with users. To demonstrate the
conversational functionality (Broughton et al,
2002), it was implemented with a commercial
speaker-dependent automated speech recogniser
(ASR),DragonNaturallySpeaking?.TheNatural
Language understanding component was
implemented in NatLink (Gould, 2001) and a
simpleuser-drivendialoguemanagement,basedon
key-word recognition and nesting of dialogue
statestoprovidecontext,wasalsoimplementedin
Python.
Franco has been successful in demonstrating
the proof-of-concept of a VA in the FOCAL
environment.  Answering spoken questions about
specific military assets and platforms, it also
permits the display of other types of information
such as pictures, animated video clips, tabular
information from a database, and location details
ondigitalmaps.
4 Improvements
Although Francowas successful in demonstrating
the potential usefulness of a VA in a Command
andControlenvironment foroperationalplanning,
it suffers from certain limitations which we are
nowaddressinginafollow-upproject.
The first limitation, and the easiest to remedy,
was the unnaturalness of the synthetic voice we
had given Franco.  For greater effectiveness, we
had to provide ourVAwith amore natural voice
andwithanAustralianaccent. Wechosethenew
Australian TTS voice from Rhetorical, developed
byAppen (rVoice,2002).   This requiredmaking
somechanges, some of them relatively important,
to the interface with the talking head model to
achieve lip-synchronisation, but that aspect of the
workwillnotbeaddressedinthispaper.
The second limitation was the relative rigidity
of the dialogue management strategy we were
using.  The alternative approach we have
developed is to create Dialogue Agents
implemented in A TTITUDE.  This is described in
section6.
The third limitation was due to the speaker-
dependent nature of the ASR.  While a speaker-
dependent ASR allows greater flexibility in the
input towhich theVAcanrespond,wewantedto
develop a system which could not only be
demonstratedby the fewpeoplewhohave trained
the speech recogniser, but where visitors
themselvescouldbeparticipantsandcouldinteract
with theVA. Switching toa speaker-independent
ASR led us to radically modify our Spoken
Language Understanding component, and this is
describedinsection7.
The new implementationwe describe here has
allowed us not only to address those three
limitations, but also to alter fundamentally the
architectureofthesystem,openingupthedialogue
managementcomponentstocontrolandinteraction
by other tools and agents in the FOCAL
environment.  The resulting system is now fully
modular and provides scalability as well as
flexibility.
Thisnewimplementationallowsustofocusour
research into dialogue management issue, to
investigate the use of A TTITUDE  for dialogue
management and to experimentwithmore natural
languageinput.
5 Integration
Communication between the various components
ofthesystem(speechrecogniser,dialoguecontrol,
virtual adviser control andmultimedia display) is
nowachievedwith theCoABS (ControlofAgent
Based Systems) Grid infrastructure (Global
InfoTek,2002).TheCoABSGridwasdesignedto
allowalargenumberofheterogeneousprocedural,
object-oriented and agent-based systems to
communicate.  Using the CoABS Grid as our
infrastructure has allowed us to integrate all the
components of the dialogue system and it will
provideaneasywaytointegrateotheragentsanda
variety of input and output devices.
Communication between CoABS agents is
accomplishedviastringmessages.
6 DialogueManagementwithA TTITUDE
ATTITUDE  is a multi-agent architecture developed
at DSTO, capable of representing and reasoning
both with uncertainty and about multiple
alternative scenarios (Lambert, 1999).  It is a
multi-agent extension of the MetaCon reactive
planner developed for control of phased array
radars on the Swedish Airborne Early Warning
aircraft(LambertandRelbe,1998). A TTITUDE has
some similarities with Prolog and other logic
programming languages as well as with AI
research on blackboard and multi-agent
architectures.  Because A TTITUDE  was designed
specificallytosupporttheprogrammingofreactive
systems, it possesses powerful facilities for
handling interactions of the internal system
entities,bothwitheachotherandwiththeexterna l
world.
ATTITUDE isveryhigh-level,weakly-typed,and
thanks to the agent paradigm, it produces loosely
coupled and modularised systems. For these
reasons, and because A TTITUDE  implements
reasoningabout propositionalattitudes ,itprovides
a very attractive framework in which to develop
and express dialogue management control
strategies.  It is worth emphasizing here that
ATTITUDE is not merely a notation to represent
speech acts or  communicative acts between
agents, but that it is actually the programming
language and environment in which both the
agents themselves and the control structure for
interaction between the agents are implemented
andexecuted.
BecauseA TTITUDE hasneverbeenusedforthis
purpose before, this is an interesting area of
research in itself, and one of the goals of the
projecthasbeentoseehowA TTITUDE needstobe
extended to implement dialogue management.
Further, this allows us to investigate how far
attitude programming  (see section 6.2) can go
towardsexpressingspeechactsandcommunicative
acttype. However,wedonotclaimtoemploythe
full power of propositional attitudes in our
implementation yet. This is another area of
researchwhichwearenowexploring. Neitherare
we yet at the stage where we could perform
automatic detection of utterance type (Wright,
1998) or of dialogue act (Carberry and Lambert,
1999;PrasadandWalker,2002).
6.1 Propositionalattitudes
The A TTITUDE  programming environment is so
named because it utilises propositional attitude
instructions  as programming instructions (this has
beendubbed attitudeprogramming ).  Propositional
attitudesareallegedmentalstatescharacterisedb y
propositional attitude expressions, which are the
means by which individuals relate their own
mentalbehaviourtoothers'.
Propositional attitude instructions are of the form
shownin(1).

(1)[subject][attitude][propositionalexpression]

In(1):
-[subject]denotesthe individualwhosemental
stateisbeingcharacterised;
- [propositional expression] describes some
propositionalclaimabouttheworld;and
- [attitude]expresses the subject'sdispositional
attitudetowardthatclaimabouttheworld.
6.2 ATTITUDEprogramming
When software agent Mary  encounters the
propositionalattitudeinstruction" Fred  desire [the
door is closed]", Mary  will issue a message to
softwareagent Fred instructing Fred  todesirethat
the door be closed. Similarly, when encountering
thepropositionalattitudeinstruction" I believe [the
sky isblue]", Mary  herselfwillattempt to believe
thattheskyisblue.
An important characteristic of A TTITUDE
programming is that each propositional attitude
instruction either succeeds or fails, possibly with
sideeffects,dependinguponwhetherthe recipient
agentisabletosatisfytheinstructionalrequest. As
each propositional attitude instruction either
succeeds or fails, the execution path selected
through a network of propositional attitude
instructions (routine) is determined by the
successesandfailuresof thepropositionalattitud e
instructionsattemptedalong theway. Thecontrol
structure is therefore governed by a semantics of
success.
Computational routines for a software agent
arise by linking together particular choices of
propositional attitude instructions.Thesenetworks
ofpropositionalattitudeinstructionsthenprescri be
recipesdefiningthepossiblementalbehaviourofa
softwareagent.
6.3 The  ATTITUDEDialogueAgents
We have implemented a number of A TTITUDE
DialogueAgents. ThemainagentinourDialogue
Management architecture (shown in Figure 1) is
the Conductor.  It is the agent responsible for the
flowofinformationbetweentheotheragentsandit
manages multi-modal interactions. The other
agents, also described further in this section, are
the Speaker, the NLG (Natural Language
Generator), the MMP (Multimedia Presenter) and
several IS(InformationSource)agents.Inaddition
to theseagents,eachdialoguestate (seesection8 )
isalsoimplementedasanA TTITUDE agent,withits
ownsetofroutines.
As explained in section 6.2, each A TTITUDE
agent?s behaviour is programmed as a set of
routines

Figure1.DialoguewithA TTITUDE

The interaction between the A TTITUDE
DialogueagentsisshowninFigure1,inwhichthe
frame around the A TTITUDE  agents can be
interpretedasrepresentingtheCoABSgrid.
SpeakerAgent
Whenspeechfromtheuserhasbeendetectedand
recognised, the attribute-value pairs for that
utterance (see section 7) are sent to Speaker.
Speaker takes that information and produces a
correspondingA TTITUDE expression,whichisthen
forwardedto Conductor.
The linguistic coverage of the system is
determinedbythegrammarswhichareavailableat
each dialogue state.  For now, the coverage is
limited to a set of utterances appropriate for the
briefing scenario described in section 8.  These
were used to define the Regulus1 grammars from
whichtheNuancegrammarsarecompiled.Weare
nowplanningtomovefromRegulus1toRegulus2,
which will allow us to derive dialogue state
grammarsfromalargeEnglishgrammarusingthe
EBLstrategydescribedin(Rayneretal.,2002b)








Conductor
Thisagentisresponsiblefordialogueflowcontrol andallothe  rdialogue
agentsmustregisterwithit.

Conductor

receivescommunicativeactsfrom
 Speaker

.Forexample:

( whquestion  (property mig- 29flying -range?value?units))
Thisqueryisforwardedontoallregisteredagents .
 Conductor

choosesthe

mostappropriateresponsereceivedandsendsthist o
 MMP

topresentthe

answer.
Speaker
Speakerreceivesspeechrecognitionresultsinthe form

ofattribute

-

valuepairs,andtranslatestheseinto

Attitudeexpressionstosendto
 Conductor

.

InformationSource
(IS)Thiscategoryofagentseachregisterwith  Conductor  and interfacewithabackgrounddatasource,forexampl e,a
databaseofaircraftproperties.

Eachusestheirdatasourcetorespondtoqueriesf rom

Conductor

.

MultimediaPresenter
(MMP)Thisagentreceivesalistofexpressionsfrom  Conductor anddirectstheappropriateservicestopresent multimediadatatotheuser.Forexample:

((whanswer (property mig -29flying - range810nautical - miles))(image mig- 29))
Inthiscase,
 MMP

requestsanEnglishformofthe
 whanswer expressionandsendstheresulttotheTTS
application.Similarly,anappropriateapplication isdirectedt

odisplaytherequestedimage.

NaturalLanguageGenerator
(NLG)Receivesexpressionsfrom  MMP andusestemplatestoreturn correspondingEnglishsentences.

Englishquestionfromuser
Nuance/Regulus

Attitudeexpression

Query

Response

Presentationdirectives

NLGdirective
TTS
Englishstring

VirtualAdvisorspeaking

Englishstring

Attribute/Valuepairs
Multimediadisplayed
ConductorAgent
Conductor  takes an A TTITUDE  expression from
Speakerandforwardsitontoallthe IS agentsthat
have registered with it.  It then waits for all the
responses to come back from those agents, in the
formoflistsofexpressions.
Every response Conductor  receives is put into
its knowledge base, along with some extra
information:
-Sender:whichISagentsenttheresponse.
- In-Reply-To: which previous communicative
actthisisaresponseto.
- Strength: whether every expression of the
response is 'strong' (the sender believes it is
either absolute truthor absolutenegation)or if
oneormore is 'weak' (the senderbelieves it is
neitherabsolutetruthnorabsolutenegation).
-Bound-State: if thereareanyfreevariablesin
theresponse,orifitisfullyground.
- Unifiability: whether one or more of the
expressionsintheresponseisofthesameform
as Speaker?s initial expression.
The final expression in Conductor?s knowledge
baseisasshownin(2).

(2)(response?in-reply-to?sender?strength
   ?bound_state?unifiability?content)

Given the initial expression from Speaker and the
replies it receives from the IS agents, Conductor
chooses the 'best' response. For example, a
response that is strong, fully ground and unifies
with Speaker?s expression is deemed to be more
relevant and informative than a response that is
weak and contains free variables.  Conductor
forwardsthisresponseto MMP.
MultimediaPresenter(MMP)
MMP  iterates through the list of expressions sent
by Conductor  andpresents eachexpression to the
user.  MMP recognises classes of expressionsand
chooses topresent themusingcertainmedia.  For
example, some expressions are instructions to
changetheVAheadmodel,whileothersaretobe
translatedintoEnglishsentencesandspokenbythe
VA. For the latter function MMP  uses NLG (see
below).
Othermediathroughwhich MMP canchooseto
present the information contained in the
expressionsinclude:imageryfromadatabase(e.g.
pictures of military platforms, or of strategic
locations), video clips, images from weather or
radar information sources, virtual video, 3-
dimensional virtual battle space maps, textual
informationandaudio.
NaturalLanguageGenerator(NLG)
For now, NLG  uses templates to transform
ATTITUDE expressionsintoEnglish.  Forexample,
the instruction in (3) provides two possible
responsesfortheA TTITUDE expressionspecified: 1

(3)(property?assetoverview?valuetext)
  whanswerpriority10
 ((response1("The"?asset"isa"?value".") )
 ((response2("Iunderstandthatthe"?asset "isa
   "?value"."))))

When NLG  is first requested to generate the
Englishoutputfortheexpressionin(4.a),intende d
to be a communicative act of type whanswer, it
uses the template given in (4.b), corresponding to
"response1"in(3), toproducetheEnglishanswer
givenin(4.c).

(4.a)(propertymig-29overview"Russianmulti-role
   fighter"text)
b.("The"?asset"isa"?value".")
c.TheMig-29isaRussianmulti-rolefighter.

When NLG  isrequestedasecond time togenerate
the output for (3), it uses the template in (5.a),
corresponding to "response 2" in (3), to produce
theEnglishanswergivenin(5.b).

(5.a)("Iunderstandthatthe"?asset"isa"?valu e".")
b.IunderstandthattheMig-29isaRussianm ulti-
rolefighter.

Thus NLG  cycles through the list of templates for
appropriateresponses. Prioritiescanalsobegive n
to templates, enabling NLG  to use general
templates togetherwithmore specificand tailored
ones.
It is clear that template-based language
generation is too rigid for fully natural dialogues ,
andweintendtoexploremoreflexible techniques
after we implement a wider coverage English
grammar;however,ithassofarbeensufficientfor
                                                         
1Variablesaredenotedwith"?",whiletextstrings (tobesent
tospeechsynthesis,ordisplayedonaslide)areb etween
doublequotes,"".
our purposes, namely to demonstrate and
investigateagent-baseddialoguemanagement.
InformationSourceAgent(IS)
The ISagents,e.g.aWeatherAgentor aPlatform
Capabilities Agent, can answer users' questions,
eitherby using their own internalknowledgebase
orbyaccessingexternalInformationSources,such
as a weather information server, or a database of
military assets.  All IS agents register with
Conductor, and when an expression is sent by
Speaker,all IS agentstrytorespondtoit.

ByusingtheCoABSGridastheinfrastructureand
implementing the agentwithA TTITUDE, we leave
the architecture extremely flexible and scalable
(Kahn and Della Torre Cicalese, 2001). For
instance, it is possible to increase the amount of
information at the system?s disposal during run-
time by launching a new IS agent and by adding
sometemplatesto NLG.
6.4 Dialoguedesign
Fornow, thedialogue is specifiedasa finite stat e
machineandisstillverymuchsystemdirected. In
thebriefingapplication (seesection8.1), theVAs
first "push" the information that needs to be
presented, as briefing officers do in a normal
briefing.Someoftheinformationisalsopresente d
using visual aids, such as power point slides and
maps for specifying location  information.  The
information to be presented and the media to be
usedaredeterminedbytheagentforthatparticula r
dialoguestate.
The VA then allows users to ask questions to
repeat or clarify particular points, or to gain
additionalinformation.
7 SpokenLanguageProcessing
7.1 Speaker-independentspeechrecognition
Asstatedinsection4,oneofthemainmotivations
formovingfromaspeaker-dependenttoaspeaker-
independentASRwastoallowvisitorsinFOCAL
the possibility of using the system themselves,
rather than relying on a small set of trained
individuals to run demonstrations.  We chose to
usetheNuanceToolkit(Nuance,2002)forseveral
reasons:  besides its reliability as a speaker-
independent ASR for both telephone and
microphone speech, Nuance 8.0 provides
Australian-New Zealand English, as well as US
andUKEnglish,acousticlanguagemodels.  Even
more importantly for our purposes, Nuance
grammarscanbecompiledfromRegulus,ahigher-
level language processing component which has
already been used to develop several spoken
dialogue systems in different domains (Rayner et
al.,2001,RaynerandBouillon,2002).
7.2 SpokenLanguageUnderstanding
Following our decision to move from a speaker-
dependent to a speaker-independent ASR, we
decided to use Regulus to implement our Natural
Language Understanding component.  Regulus is
an Open Source environment which compiles
typed unification grammars into context-free
grammar language models compatible with the
Nuance Toolkit.  It is "written in a Prolog-based
feature-value notation and compiles into Nuance
GSLgrammars."(Rayneretal.,2002a).  Regulus
isalsodescribedindetailin(Rayneretal.,2001 ).
The main motivation for using Regulus is the
usual one of greater efficiency due to the more
compact nature of a unification grammar
representation compared with a context-free
grammar.  In addition, using Regulus to define a
higherlevelgrammar,weareabletoobtainasour
semantic representation a list of attribute-value
pairs, and this permits a more sophisticated
processingoftheinformationbytheotheragents.
Regulus also allows the development of bi-
directional grammars, and we intend tomake use
ofthisfunctionalityinlaterimplementationsoft he
NLG  agent. However, fornow, thegrammarswe
have developed have been limited to recognition
andunderstanding.
8 Currentapplicationimplementation
8.1 Dialoguescenario
The scenario for the current application was
developed by members of the Human Systems
Integration (HSI) group and is grounded on their
experience with, and observations of, military
operational planning.  It is based on a fictitious
scenario developed for training (the examples
givenherehaveallbeenmodified)andexemplifies
theJointMilitaryAppreciationProcess(JMAP)for
militaryplanningacross the three services (Army,
Navy and Air Force). A sub-scenario was chosen
for the development of the spoken dialogue with
theVAs. 2
8.2 Dialogueflow
The structured nature of a military planning task
such as this onemakes it very easy to partition it
intodifferentstages,whichcanthenbemappedto
different dialogue states. In our dialogue script,
each top-level dialogue state corresponds to a
sectionoftheplanningexercise,givenin(6).

(6)Commander'sInitialGuidance
-CDF(ChiefofDefenceForces)Intent
-PlanningGuidance
-Constraints
-Restrictions
-LegalIssues
-CommandandControl

These6topleveldialoguestatesarethenfollowed
byanOverallQuestionTime.
The mixed-initiative nature of the system can
bemodelledinafinitestatediagram,allowingfor
a) briefing-like system ?pushes?, b) confirmation
queriesfromthesystemandc)questionsfromthe
user.   However, because the system is primarily
agent-based, the dialogue can also evolve
dynamically. Forinstance,oncethesystemisina
?question? state,  the dialogue flow then allows
users to ask anumberofquestions,until they are
satisfied,and thedialoguecanmove toadifferent
state.
Each of the top level dialogue states also
corresponds to an IS agent with its own set of
ATTITUDE  routines.  These agents register with
Conductor  and act as experts in their particular
fields (e.g., the Legal Issues adviser).  Theagent s
contain knowledge which they use to answer
questionsposedtothemby Conductor. Allagents
have the ability to keep track of which state (or
topic)theyarein.Thisallowsnotonly Conductor,
but also the other dialogue agents, to distinguish
between providing the user with new information
orinformationthathasalreadybeenpresented.
                                                         
2
ThisistheCommander?sinitialguidancetotheTh eatre
PlanningGroup(TPG),whichispartoftheMission Analysis
sectionofJMAP.
8.3 KnowledgeRepresentation
Thecurrentontologydevelopedforthisapplication
is only a small part of the larger Knowledge
Representationontologytobeusedthroughout the
whole FOCAL system.  For now, we only
representtheconceptsneededinoursmalldomain,
and their relationships are translated into
ATTITUDE  statements, allowing agents to draw
inferences.    For example, if a user can ask the
question given  in (7.a), it will be translated int o
the listof  attributevaluepairsgiven in (7.b)a nd
sent to Speaker. Speaker then translates these
attributevaluepairsinto theA TTITUDE expression
in(7.c)andforwardsitonto Conductor.

(7.a) Whatdepartmentoverseesnegotiations
withunionsandindustry?
b. [questionwhatquestion,concept
negotiation,attributeoversee,obj1department]
c. conductordesire(comm_act(negotiation
oversee?department)fromspeakertype
whatquestionin-response-tonull)

As described in section 6, when Conductor  poses
thequestiontotheappropriateagents,theyrespon d
with the information in their knowledge base or
information they can extract from a database.
Agentsstoreknowledgeas believe statementssuch
astheoneshownin(8):

(8) Ibelieve(negotiationoversee?department
ofworkplacerelations?)

These believe  statementsarethenunifiedwith the
propositions translated by Speaker, and if
unification is successful, a reply is sent back to
Conductor.  Finally, Conductor  passes the answer
on to NLG  to match a template and produce an
Englishanswer,forinstance(9).

(9) TheDepartmentofWorkplaceRelations
overseesnegotiationswithunionsandindustry.

An agentwhich has access to a databasecan also
translate a user's question into the relevant
databasequerytoobtaintheanswer. Animportant
issue under research concerns the automatic
derivation of A TTITUDE  statements from a pre-
existingdatabase.
8.4 SeveraldifferentVAs
As explained above, each stage of the planning
processispresentedtotheuserbyaparticularVA
withitsassociated IS agentandtheVAthenallows
users to ask further questions. Besides their
specialised knowledge, theVAs are differentiated
through different head models, different TTS
voices (maleorfemale,differentregionalaccents)
anddifferentpersonalities.
Onceadialoguestateiscompletedandtheuser
has no further questions, the VA for that state
sendsamessage to Conductor  tomovetothenext
state.  Conductor can then initiate the change in
recognition grammar, voice for the next VA and
modelforthenextVAhead.
Having several VAs coming on at different
stagestopresentdifferent informationallowsfor a
VA tobe specialised in a particular domain,  just
as real briefing officers are during a realmilitar y
planningexercise.
Fornow,weonlydisplayoneVAatatime,but
weintendtoexperimentwithhavingmultipleVAs
at the same time. The final state of the dialogue
flowallowsuserstoaskquestionsaboutanyaspect
of the planning process, and questions can be
posedtoalltheVAs,soitwouldbenaturalforth e
userstoseealltheVAsatthatstage.
8.5 RapidPrototypingandEvaluation
The key word version developed previously (see
Broughton et al, 2002) has been maintained as a
rapid prototyping environment for evaluating new
scripts and dialogues.  It allows newdialogues to
be quickly tested by entering suitable key words,
sufficient to discriminate one question from
another. Thissystemprovesfasterfortestingtha n
the more precise method of grammar building.
Multiple response strings can be generated,
providing more naturalness for those interacting
with the VAs on a regular basis.  By rapidly
prototyping questions and responses, we can test
the intuitiveness of expected questions and the
smoothness and timeliness of responses,
particularly when presented combined with
multimedia.
The implemented system described here has
so faronlybeen testedwithothermembersof the
group,butdemonstrations tovisitorsandpotential
users will provide a more rigorous  form of
evaluation on an on-going basis.  An evaluation
phase for the project is scheduled for 2003-2004,
during which time we will have access to more
users andwill be able to conductmore structured
experiments.
9 NaturalInteractionwithVAs
In addition to the ASR and TTS systems
previously discussed, other technologies can be
combined into the overall system to increase
naturalnessofinteraction,andweareinvestigatin g
speaker recognitionaswellasa rangeofpointing
technologies.
Theneed for a speaker recognition systemhas
emerged with the move to a speaker independent
ASR.WithaspeakerdependentASR,userswould
load their individual profile before use, thus
enabling the system to know who was using it.
With a speaker-independent ASR, a speaker
recognition system would allow the VAs to
recognisewho is talking to themandenable them
to address known users by name.  We plan to
integrate within FOCAL the speaker recognition
system which has been developed at DSTO
(Roberts, 1998).  This system uses statistical
modelling techniques and is capable of both
speaker identification (recognising users from a
database of stored speech profiles) and speaker
verification (verifying the identity of a particula r
user).
We are also proposing to use pointing
techniques in combination with the speech and
language technologies to build a multimodal
system.  Multimodal systems were originally
demonstrated by Bolts (1980) and research is
continuing across varied applications (e.g., Oviatt
et al, 2000 and Gibbon et al, 2000).  However,
unlike systems such as MATCH (Johnston et al,
2002), where the issue is allowing multimodal
interaction on portable devices with very small
screens, in FOCAL we are concerned with
ensuring thatusersget the full benefit of the ver y
large screen and with allowing several users to
interact at a distance from the screen.  It is also
worth mentioning that, unlike the interactive
systemdescribedin (Rickeletal.,2002),whichi s
concernedwithtraining inamilitaryenvironment,
we are not trying to simulate a complete virtual
worldwithembodiedagents.
However, we propose to include traditional
pointingtechnologies,suchasthestandarddesktop
mouse, through to3-dimensional trackingsystems
for gaze, gesture and user tracking.  This will
involve integrating more complex language
understanding, as information will need to be
derived from both the user's utterance and from
whatisbeingpointedto.Forexample,tointerpre t
an utterance such as (10) uttered while the user
points toa locationonamap,weneed toperform
reference resolution on "this region", and match
thatreferenttotheitembeingpointedat.

(10)Whatdoweknowaboutthisregion?

10 Conclusion
We have now implemented in FOCAL the
infrastructure needed to perform spoken and
multimodaldialoguewithseveralVAs. This isof
interestinitself,asitwillallowustocontinue our
research on spoken language understanding and
spokendialoguesystemsandalsotoaddressissues
of language generation which have for now been
left aside.  Already we have been able to move
from a rigid dialogue control structure, with very
constrained input, to a more flexible and scalable
control structure allowing real connectivity
betweenagents.
Havingmoved to a speaker-independent ASR,
and takingadvantageof theopensourcenatureof
Regulus, we intend to pursue research issues
regarding robustprocessing of spoken input, such
asusinggrammarspecialisationfromacorpusand
devisingtechniquesforignoringpartsoftheinput .
We have implementeda dialoguemanagement
architecture based on A TTITUDE  agents which
communicate with each other using propositional
attitude expressions.  Other agents can now be
developed to  perform additional functions, in
particular to launch the display of other types of
informationandtointerpretothertypesofinput.
This will allow us to explore how spoken
dialogue with VAs can be combined with other
virtual interaction technologies (e.g., gesture,
pointing, gaze tracking). In this respect, the next
step in our project is the development of a full
fledge MMP  agent based on the framework
describedin(ColineauandParis,2003).

However,theworkwehavereportedheremust
also be seen as part of the larger research
programmeundertakenwithinFOCAL. Fromthis
perspective, this work is of interest because it
allows othermembers of theHSI group topursue
research in the usability of new technologies to
perform the paradigm shift in command
environments.  In particular, this project is
providing the support for further research into
whether this way of presenting information is
helpful in an operational command environment.
It allows us to devise experiments to explore the
crucial issue of trust in the information being
presented,andhowtheway theinformationbeing
presentedcanaffectthattrust.
Integratingspokendialoguewithplanningtools
willalsoallowustoexplorewhetherVAscanhelp
inmilitaryoperationplanning,andhowbesttouse
thesetools.

Acknowledgements
We wish to thank the Chief of C2D, and the
Director of Information Sciences Laboratory, for
sponsoring and funding this work. We wish to
acknowledge the work of Paul Taplin in
integrating speech synthesis and lip-
synchronisation, and the work of Benjamin Fry
from the University of South Australia in
developing the Regulus/Nuance grammars.
Finallywewishtothanktheothermembersofthe
HSIgroupinC2Dfortheirconstantandinvaluable
helpwiththeFOCALproject.

References
Ananova.2002. http://www.ananova.com.
E. Andre, T. Rist, and J. Muller. 1998. Integrating
Reactive and Scripted Behaviours in a Life-Like
Presentation Agent, Proceedings of the Second
International Conference on Autonomous Agents ,
261-268.
Appen.2002. http://www.appen.com.au.
R.A.Bolt.1980."Put-that-there":voiceandgestu reat
the graphics interface . Proceedings of the
SIGGRAPH, July,262-270.
Michael Broughton, Oliver Carr, Dominique Estival,
Paul Taplin, Steven Wark, Dale Lambert.  2002.
"Conversing with Franco, FOCAL?s Virtual
Adviser". Conversation Characters Workshop,
HumanFactors2002 ,Melbourne,Australia.
Sandra Carberry and Lynn Lambert. 1999. "A Process
Model for Recognizing Communicative Acts and
ModelingNegotiationSubdialogues". Computational
Linguistics.25,1,pp.1-53
Justine Cassell. 2000. Embodied Conversational
InterfaceAgents, Communicationsof theACM ,Vol.
43,No.4,70-78.
Nathalie Colineau and C?cile Paris. 2003. Framework
fortheDesignofIntelligentMultimediaPresentati on
Systems: An architecture proposal for FOCAL.
CMISTechnicalReport03/92,CSIRO,May2003.
Dominique Estival. 2002. "The Syrinx Spoken
Language System". International Journal of  Speech
Technology. vol.5.no.1.pp.85-96.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, Steve Whittaker, Preetam Maloor. 2002.
"MATCH: anArchitecture forMultimodalDialogue
Systems". Proceedingsofthe40thAnnualMeetingof
the Association for Computational Linguistics
(ACL'02).pp.376-383.Philadelphia..
DafyddGibbon, IngeMertins,RogerK.Moore (Eds.).
2000.  Handbook of Multimodal and Spoken
Dialogue Systems: Resources, Terminology and
ProductEvaluation. KluwerAcademicPublishers.
Global InfoTek Inc. 2002. Control of Agent Based
Systems.  http://coabs.globalinfotek.com.
JoelGould. 2001. "Implementation and Acceptance of
NatLink, aPython-BasedMacroSystem forDragon
NaturallySpeaking", The Ninth International Python
Conference,March5-8,California
Martha L. Kahn and Cynthia Della Torre Cicalese.
2001. "CoABS Grid Scalability Experiments".
Proceedings of the Second International Workshop
on Infrastructure for Agents, MAS, and Scalable
MAS, AutonomousAgents2001Conference.
Dale A. Lambert and Mikael G. Relbe.  1998.
"Reasoning with Tolerance".  2nd  International
Conference on Knowledge-Based Intelligent
ElectronicSystems .IEEE.pp.418-427.
DaleA.Lambert.1999."AdvisersWithA TTITUDE for
Situation Awareness". Proceedings of the 1999
Workshop on Defence Applications of Signal
Processing. pp.113-118, Edited A. Lindsey, B.
Moran, J. Schroeder, M. Smith and L. White.
LaSalle,Illinois.
Dale A. Lambert. 2003. "Automating Cognitive
Routines", accepted for publication in the 6th
InternationalConferenceonInformationFusion.
R.Moore, J.Dowding,H.Bratt, J. Gawron,Y.Gorfu ,
A. Cheyer. 1997. "CommandTalk: A spoken-
language interface for battlefield simulations". In
Proceedings of the Fifth Conference on Applied
NaturalLanguageProcessing,pp1-7.
Nuance.2002.http://www.nuance.com/ .
Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L .,
Suhm, B., Bers, J., Holzman, T., Winograd, T.,
Landay,J.,Larson,J.,Ferro,D.2000."Designing the
user interface formultimodal speech and pen-based
gesture applications: state-of-the-art systems and
future research directions". Human Computer
Interaction.
RashmiPrasad andMarilynWalker. 2002. "Training a
Dialogue Act Tagger for Human-Human and
Human-ComputerTravelDialogues". Proceedingsof
3rdSIGDIALWorkshop .Philadelphia.pp.162-173.
Manny Rayner, John Dowding, Beth Ann Hockey.
2001. "A Baseline method for compiling typed
unification grammars into context free language
models". In Proceedings of Eurospeech 2001, pp
729-732.Aalborg,Denmark.
Manny Rayner, John Dowding, Beth Ann Hockey.
2002a."RegulusDocumentation".
Manny Rayner, Beth Ann Hockey, John Dowding.
2002b.  "Grammar Specialisation meets Language
Modelling". ICSLP2002. Denver.
Manny Rayner and Pierrette Bouillon.  2002. "A
Flexible Speech to Speech Phrasebook Translator".
Proceedings of the ACL-02 Speech-Speech
TranslationWorkshop ,pp69-76.
Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randa ll
Hill,DavidTraum,WilliamSwartout.2002.Toward
aNewGenerationofVirtualHumans for Interactive
Experiences. IEEE Intelligent Systems, 1094-7167,
pp.32-38.
William Roberts. 1998. "Automatic Speaker
Recognition Using Statistical Models". DSTO
ResearchReport,DSTO-RR-0131 ,DSTOElectronics
andSurveillanceResearchLaboratory.
rVoice. 2002. Rhetorical Systems,
http://www.rhetoricalsystems.com/rvoice.html.
Paul Taplin, Geoffrey Fox, Michael Coleman, Steven
Wark, Dale Lambert. 2001.  "Situation Awareness
Using a Virtual Adviser", Talking HeadWorkshop,
OzCHI2001 ,Fremantle,Australia.
Helen Wright. 1998. "Automatic utterance type
detection using suprasegmental features".
Proceedings of the 5th International Conference on
SpokenLanguageProcessing(ICSLP'98).Sydney.
Towards Ontology-Based Natural Language Processing 
Dominique Estival, Chris Nowak and Andrew Zschorn 
Human Systems Integration Group 
Defence Science and Technology Organisation 
PO Box 1500, Edinburgh SA 5111 
AUSTRALIA 
{Dominique.Estival,Chris.Nowak,Andrew.Zschorn}@dsto.defence.gov.au 
 
 
 
Abstract 
Conceptualising a domain has long been 
recognised as a prerequisite for 
understanding that  domain and processing 
information about it. Ontologies are 
explicit specifications of conceptualisations 
which are now recognised as important 
components of information systems and 
information processing. In this paper, we 
describe a project in which ontologies are 
part of the reasoning process used for 
information management and for the 
presentation of information.  Both 
accessing and  presenting information  are 
mediated via natural language and the 
ontologies are coupled with the lexicon 
used in the natural language component.  
1 Introduction 
Ontologies are now being recognised as important 
components of information systems and 
information processing.  It is commonly accepted 
that an ontology is an explicit specification of a 
conceptualisation (Gruber, 1995).  In the areas of 
knowledge representation and reasoning (KR) and 
of conceptual modelling, it has long been 
recognised that conceptualising a domain is a 
prerequisite for understanding the domain and 
processing information about the domain, 
especially in the case of large, non-trivial domains.  
Nowadays, there is no clear-cut border between 
large and small domains, simply because 
information systems are no longer isolated but are 
parts of the global information system and need to 
be interoperable. Hence, conceptualisations and 
ontologies are required for all kinds of information 
systems and information processing.  In some 
cases it is not clear yet what functions and 
advantages ontologies can offer, but there is no 
doubt that in every case ontologies do offer 
something: at the very least they offer a way to 
address meaning of terms (concepts, relations) 
required for information processing.  
This paper attempts to provide some suggestions 
on how natural language processing can benefit 
from using ontologies.  We present a large-scale 
research project in which ontologies are part of the 
reasoning process used for information 
management and for the presentation of 
information.  Users' access to information and the 
presentation of information to users are both 
mediated via natural language, and the ontologies 
used in the reasoning component are coupled with 
the lexicon used in the natural language 
component. 
In Section 2,  we describe the FOCAL (Future 
Operations Centre Analysis Laboratory) project: 
both the ontological processing and the natural 
language processing work presented here are based 
on the relevant aspects of FOCAL.  In Section 3, 
we present ontology-related work for FOCAL and 
in Section 4, the NLP-related aspects of FOCAL.  
In Section 5, we show how ontologies and NLP are 
combined. Section 6 summarises the current state 
of this work and indicates directions for future 
research. 
2 Future Operations Centre Analysis 
Laboratory (FOCAL) 
The Future Operations Centre Analysis Laboratory 
(FOCAL) is a research project whose goal is to 
"pioneer a paradigm shift in command 
environments through a superior use of capability 
and greater situation awareness" (FOCAL Task 
Plan).  In part, this involves building a high-level 
information fusion system for the military domain 
(Lambert, 2003; FOCAL, 2002).  
To support this goal, the FOCAL facility was 
designed to experiment with innovative 
technologies. FOCAL contains a large-screen 
(150?) semi-immersive virtual reality environment 
as its primary display, allowing vast quantities of 
information (real or virtual) to be displayed.   
Spoken dialogue with virtual characters known as 
VAs (Virtual Advisers) is one of the means of 
delivering information (Estival et al, 2003).  
Within the FOCAL project, the Natural 
Language Processing (NLP) and the Knowledge 
Representation and Reasoning (KR) work 
packages are tasked with providing appropriate 
NLP and KR functionalities, including processing 
natural language queries and providing a 
formalisation of the domain and reasoning 
capabilities.  These two work packages are closely 
related in that a natural language query is to be 
processed, mapped to its formal representation and 
answered by a reasoning subsystem, and then a 
natural language answer is returned to the user. 
Current FOCAL work is focused on 
implementing a scenario, which is located within a 
particular military situation and describes a 
military domain, a limited (in space and time) 
region of the world, and other relevant elements of 
that situation.  Among other things, the domain 
description requires dealing with geography, 
logistics and planning.   
The FOCAL architecture is agent-based and 
uses the CoABS (Control of Agent Based Systems) 
Grid as its infrastructure (Global InfoTek, 2002).  
The CoABS Grid was designed to allow a large 
number of heterogeneous procedural, object-
oriented and agent-based systems to communicate. 
FOCAL agents process information, communicate 
and collaborate.  Most agents are implemented in 
ATTITUDE and communication between agents is 
accomplished via string messages (Wark et al, 
2004). 
Humans are also involved in FOCAL, as the end 
users who interact with the system to perform their 
work and achieve their goal: successfully planning 
and conducting an operation.  The current scenario 
provides a testbed for the system.  Extensions of 
the scenario and new scenarios for different 
domains will ensure that FOCAL functions as 
expected outside of the limited domain of the 
current scenario. 
There are many aspects of FOCAL which are 
not directly related to NLP and KR activities, and 
which are therefore excluded from this discussion.  
In the rest of this paper, only aspects relevant to 
NLP and KR are considered.  
3 Ontological Reasoning for FOCAL 
The main task of the KR work package within the 
FOCAL project is to provide the FOCAL users 
with automated knowledge management and with 
automated reasoning capabilities about a complex 
domain.  Ontologies have been chosen as the type 
of representation most suited for this task, and the 
provision of  ontological reasoning capabilities has 
been one of the main thrusts.   An ontology for 
FOCAL has been built and a number of reasoning 
activities are now ontology-based. 
3.1 Conceptualisation  
Lambert (2001) advocated Dennett's Intentional 
Stance framework (Dennett, 1987).  Dennett 
identified three means by which people predict and 
explain outcomes.  
1. The first is the Physical Stance, where one 
engages principles of Physics to predict 
outcomes. People employ this when playing 
snooker or assessing the trajectories of 
projectile weapons.  
2. The second is the Design Stance, where one 
engages principles of design to predict and 
explain outcomes. People employ this when 
troubleshooting an automobile fault or coding 
and maintaining computer programs.  
3. The third is the Intentional Stance, where one 
engages principles of rationality to predict 
outcomes. People employ this when 
forecasting the actions of a fighter pilot or 
when competing with an advanced computer 
game.  
 
The Design Stance is used whenever the physics of 
the situation is too difficult or laborious. The 
Intentional Stance is used whenever the design 
underpinning the situation is too difficult or 
laborious.  
Lambert (2001, 2003) adopts Dennett's 
framework for representing knowledge about the 
world, but adds two other layers: a metaphysical 
layer below the physical layer, and a social layer 
above the intentional layer. Therefore, formal 
theories that allow one to represent and reason 
about the world, would be assigned to the 
following levels: 
1. Metaphysical theories, for what there is, where 
and when. 
2. Physical theories, for the operation of aspects 
of the environment. 
3. Functional theories, for the operation of 
designed artefacts. 
4. Intentional  theories, for the operation of 
individual minds. 
5. Social theories, for the operation of groups of 
individuals. 
 
This five level framework proposed by Lambert 
suggests a way to conceptualise the domain in 
terms of processes, namely metaphysical, physical, 
functional, intentional and social processes (M, P, 
F, I, S processes). The resulting conceptualisation 
is referred to as a Mephisto conceptualisation 
(Nowak, 2003) and is the basis for the ontologies 
we are constructing for FOCAL.    
3.2 Ontological languages 
Ontologies are concerned with the meaning of 
terms.  It is therefore appropriate when selecting an 
ontological   language to choose a language which 
is equipped with a formal semantics. This 
requirement excludes XML from the list of 
possible candidates, as XML does not offer 
semantics, but only syntax.  RDF provides some 
semantics, but proper, formal semantics requires 
languages based on logics.  Description logics 
(DL) provide some frameworks, and several 
languages used for building and processing 
ontologies are DL-based, e.g. DAML and 
DAML+OIL languages, including such languages 
as SHF and SHIQ, and the OWL language 
(Horrocks et al, 2003).   
A commonly  used view of an architecture for 
the Semantic Web is a layered architecture, with 
XML as the bottom layer, RDF as the middle 
layer, and logic (e.g. DL) as the top layer 
(sometimes the top layer distinguishes ontological 
vocabulary, logic, proof;  on top of the logic layer 
a trust layer is sometimes placed).  The logic layer 
is a necessary component if the Semantic Web is to 
be equipped with a formal semantics; this logic 
layer can be based on a description logic (such as 
SHIQ or OWL), on first-order logics, KIF or 
CycL, and whichever logic is used determines the 
expressibility and tractability of the framework, 
but in every case a formal semantics is added.  
Frameworks based on DL (description logics) are 
most successful, because they provide expressive 
languages with practical tractability.  SHIQ is one 
such language, another is the closely related 
language OWL  
The ontological language chosen for FOCAL is 
SHIQ, a DL language of the DAML+OIL project 
(http://www.daml.org/), a successor of   the OIL 
project (http://www.ontoknowledge,org/oil/).  
FaCT (http://www.cs.man.ac.uk/~horrocks/FaCT/) 
is a reasoner for the SHIQ logic employed in the 
OilEd ontology editor (http://oiled.man.ac.uk/). 
The logic SHIQ has also been implemented in the 
(www.cs.concordia.ca/~faculty/haarslev/racer/) 
RACER project.   
SHIQ is closely related to OWL (Horrocks et 
al., 2003).  In fact, there are a few variants of 
OWL, namely OWL Lite, OWL DL and OWL 
Full.  OWL Lite is similar to a description logic 
SHIF(D), while OWL DL is similar to a 
description logic SHOIN(D).  The language 
implemented in the RACER framework is a 
version of SHIQ, which provides some 
functionalities for dealing with individuals, and 
dealing with concrete domains; this makes the 
RACER?s version of SHIQ very close to OWL 
Lite.  A proper discussion on these languages is 
beyond the scope of the paper, but clearly the 
RACER language is an implemented  language and 
reasoner for a logic very close to OWL DL. 
References related to OWL, SHIQ and OIL include 
(Horrocks et al, 2003), (Bechhofer and Horrocks, 
2003) and  (Horrocks, Sattler and Tobies, 2000). 
3.3 Ontological frameworks 
Ontology frameworks provide formalisms for 
building ontologies, but do not provide the 
contents.  Therefore, they should do at least two 
things: 
? provide a formal language in which the 
ontologies can be   expressed or specified, and  
? provide some reasoning capabilities, so that an 
ontology can be  demonstrated to be consistent 
(i.e. free of contradictions, assuming that   
contradictions indicate modelling mistakes or 
errors). 
Given this standpoint, frameworks that do not 
provide reasoning capabilities are unsatisfactory.  
Note also that a formal language is usually a 
logical language, with clearly specified syntax and 
semantics, and the logic should be sound, 
complete, decidable, and hopefully tractable (or 
tractable in practice).  These properties of the 
logical framework are necessary to obtain 
reasoning facilities. The most attractive ontology 
frameworks seem to be the following (see Table 1 
for a more detailed comparison of the different 
frameworks): 
1. the OIL framework based on description 
logics, 
2. the OntoEdit/OntoBroker framework (F-logic), 
3. the Ontolingua framework based on the KIF 
logic. 
 
 
 
For FOCAL, we have chosen to employ the OIL 
and RACER frameworks.  Ontologies are built 
using the OilEd ontology editor and verified using 
FaCT.  At run-time, a RACER agent is initialised 
with the ontology (see section 3.4). 
Higher order relations and Description Logic 
Although description logics on which OIL and 
RACER are based allow only binary relations, we 
use OIL and Racer in a way that also allows us to 
employ arbitrary n-ary relations and higher-order 
relations. Given that a ternary relation can be 
represented as a binary relation that takes another 
binary relation as one of its argument, any n-ary 
relations can be represented via higher-order 
relations, i.e. relations which take other relations as 
arguments.  Suppose that we want to implement a 
second-order relation that takes as its first 
argument a binary relation- more precisely, the 
second order relation takes as its first argument 
instances of that binary relation- rather than 
instances of a concept.  The instances of the binary 
relation can be mapped to instances of a newly 
created concept, i.e. the concept of individuals 
which are single entities but correspond to (and are 
linked to) the instances of the binary relation.  
There is an exact correspondence between the 
second-order relation taking a binary relation 
instance as its first argument and its 
implementation in terms of a binary relation that 
takes as its first argument an instance of the 
concept which has instances of the other binary 
relation as its individuals. The approach we 
described here has now been used to implement in 
the FOCAL ontology information which extends 
beyond the binary relation based language. 
Multiple facts involving n-ary relation and higher-
order relation are present in the current version of 
the FOCAL ontology. ATTITUDE agents are 
currently being built to allow automated reasoning 
with this extended language. 
Implemented Ontology 
As mentioned in section 1 the FOCAL scenario, 
which is based on real material for training 
exercise, provides background information in a 
number of domains, including geography, political 
situation, logistics, weather.  For now, the scenario 
also specifies what kinds of questions can be asked 
by FOCAL users, to be answered by the FOCAL 
agents. The ontology serves as a formal, clearly 
specified knowledge base containing the 
background information and allowing the agents to 
query that knowledge base and to get replies 
helping them to answer the queries. 
An initial version of the FOCAL ontology has 
been created manually using OilEd and verified 
using FaCT.1 There are in fact several ontologies, 
for the different domains covered in the scenario, 
and an important research issue is that of the 
combining (or merging) of the ontologies in the 
larger FOCAL one.  Another issue is that the 
manual creation of the ontologies is a time 
consuming and tedious process, but the existence 
of tools such as FaCT ensures that the result is 
consistent and free of mistakes due to user input 
errors.   
3.4 Ontological reasoning 
                                                            
1
 The FOCAL ontology currently contains over 300 
concepts, about 80 relations and over 100 individuals 
(plus a large number of facts connecting all of these).  
Both the FaCT and RACER reasoning agents 
provide reasoning facilities, FaCT during the 
building of the ontologies to ensure coherence and 
consistency, and RACER at run-time. When 
integrated within the FOCAL system, the RACER 
server can be initialised with a given ontology and 
there is a RACER client wrapped as a CoABS 
agent on the grid, which  can connect to the server.  
Other FOCAL agents, e.g. the Dialogue Manager 
(see section 4.1), can then communicate with the 
RACER server (via the RACER client agent) and 
receive answers using the ontology. 
The ontology can be also be accessed and 
queried outside of the FOCAL system, still using a 
client-server connection. 
? Using OilEd, the ontology "focal.daml" can be 
saved in the DIG format as a file named 
"focal.dig".2 
? The RACER server can be started and 
initialised to the "focal.dig" ontology. 
? A java package called jracer includes a test 
client (http://www.lsi.upc.es/~jalvarez/) which 
can be used to connect to the RACER server. 
 
At the ">" prompt, queries can be entered.  The 
queries are received and replied to by the server.  
For instance, we show in (1) an example of a query 
as to whether (the individual) AUSTRALIA is an 
instance of (the concept) nation, and give the 
server's answer to that query, i.e. T (for true). 
 
(1)  >  (individual-instance? AUSTRALIA nation) 
      T 
3.5 Hierarchies of concepts and relations 
A DL-based ontology, such as our OilEd "Focal" 
ontology,  is a knowledge base (KB) expressed in a 
DL language.  Every DL language provides 
facilities for defining concepts, with the relation of 
subsumption between the concepts being the core 
relation and the basis for building the definitions.  
The set of concepts can be seen as an ordered set, 
the subsumption relation being the ordering 
relation; hence, we have a hierarchy of concepts.  
There is also a hierarchy of relations ordered by 
the subsumption relation.  These two  hierarchies, 
together with the concepts' definitions, can be 
taken to form a lexicon, i.e. a list of words (for 
                                                            
2
 OilEd can export to SHIQ, OWL and other formats. 
concepts and relations) with well-defined 
meanings for those words. 
These two hierarchies of concepts and relations 
thus provide a basis for a domain specific lexicon 
and one of the advantages which ontologies can 
offer NLP systems is that a properly built 
knowledge base (as on ontology) will allow the 
semi-automatic creation of a lexicon. 
4 NLP in FOCAL 
The underlying architecture for dialogue 
management has been developed using ATTITUDE 
agents (Estival et al, 2003).  Input from FOCAL 
users can be either spoken or typed and is 
processed by the same NLP component.  We use 
Nuance for speaker-independent speech 
recognition (Nuance, 2002) and the open source 
Regulus NLP package  (Rayner et al, 2001) for 
grammar development.3 We are in the process of 
integrating language input with input from other 
devices, e.g. pointing devices such as mouse or 
wand, gesture tracking device and, in the future, 
gaze tracking. 
4.1 Dialogue Agents 
The FOCAL Dialogue Agents can be divided into 
3 categories: Input Agents, Internal Reasoning 
Agents and Output Agents.  The Input Agents 
comprise: 
? Speech Input 
The Speech Input agent is a straightforward 
wrapper around a Nuance Client implementation. 
It forwards on to the Input Fuser the interpretations 
of speech recognition results (in the form of lists of 
Attribute-Value pairs), notifications of failed 
recognition events and the interpretations of typed 
input. It also passes on instructions to activate and 
de-activate the recogniser.   
? Input Fuser 
The Input Fuser (IF) is responsible for receiving 
and combining user input.  This input can be via 
speech (Nuance), keyboard (typed input), gesture, 
gaze etc. The IF turns streams of input events into 
a Bayesian network of discrete communicative acts 
                                                            
3
 The existing grammar was developed using Regulus 1, 
but we are currently developing a larger, more flexible 
grammar with Regulus 2 (Rayner et al, 2002) which 
will provide a broader coverage, allowing the more 
na?ve users to be recognised more easily.   
which are then interpreted by the Dialogue 
Manager.  
 
The Internal Reasoning Agents comprise: 
? Reference Resolver 
This is currently a stub, but the Reference Resolver 
is meant to assist other agents (particularly the 
Input Fuser and the Dialogue Manager) resolve 
anaphoric references found in user communicative 
acts by maintaining context and linking dialogue 
variables to referents. 
? Dialogue Manager 
The Dialogue Manager (DM) is activated by a 
message that includes an activation context 
symbol. The DM receives the Bayesian network of 
interpretations of user(s) communicative acts from 
the IF and it finds the interpretation with the 
highest probability that unifies with the current 
dialogue context. The DM then informs the IF of 
which interpretation of the communicative act was 
chosen, so the IF can forward the full information 
on to the Transcriber. At the same time, the DM 
requests that the Presentation Planner present the 
response to this communicative act; this request is 
termed a communicative goal.  
? Presentation Planner 
The Presentation Planner (PP) receives requests 
from the DM to achieve communicative goals.  For 
now a communicative goal will succeed if there is 
a presentation clip which is marked-up with the 
conjunction of the DM's activation context and the 
meaning representation for the query, but current 
work is extending the PP agent along the lines 
given in (Colineau and Paris, 2003). 
 
The Output Agents comprise: 
? Transcriber 
The Transcriber agent receives notification of 
user's communicative acts from IF and of the 
system's communicative acts from DM. It produces 
an HTML listing of these communicative acts, 
which includes speech recognition results and a 
link pointing to the audio recording. 
? Text-to-Speech 
If the output is to be presented verbally by the 
Virtual Advisers, it is sent to the Text-to-Speech 
(TTS) component.  We use the rVoice TTS system, 
which gives us a choice of voices for the different 
VAs (rVoice, 2002). 
4.2 Lexicon for NLP 
As described above, language processing is 
performed by the Nuance/Regulus grammar.  
Regulus is an Open Source environment which 
compiles typed unification grammars into context-
free grammar language models compatible with the 
Nuance Toolkit.4   
The lexicon for Regulus 2 is of the form shown 
in (2) and (3), where the macro in (2) defines the 
properties of a noun class, and the instances in (3) 
specify the lexical items belonging to that class, in 
this case result, results, outcome, outcomes. 
 
(2) macro defining noun class 
macro(noun_like_result(Words,Sem),    
       @noun(Words, [sem= @noun_sem(abstract, Sem), 
        sem_n_type=abstract, takes_det_type=def\/null,  
        n_of_mod_type=_])). 
 
(3) examples of nouns for that class: 
@noun_like_result([result, results], result). 
@noun_like_result([outcome, outcomes], result). 
4.3 Meaning representation  
The Meaning Representation produced by the NLP 
component, and passed on by the Speech Input 
agent, is translated into an ATTITUDE expression. 
For example, if a user can ask the question given  
in (4.a), it will first be translated into the 
(simplified)  list of  attribute value pairs given in 
(4.b) and sent to the Speech Input agent.  Speech 
Input then translates these attribute value pairs into 
the (simplified) ATTITUDE expression given in 
(4.c) and forwards it on to the Input Fuser agent. 
 
(4) a.  What is our relationship with PNG? 
     b. (question whquestion concept relationship obj1 
Australia obj2 Papua_New_Guinea) 
     c. (comm_act (?relationship Australia 
Papua_New_Guinea) from speaker type whquestion ) 
5 Natural Language & Ontological Processing 
for FOCAL 
There are at least two ways that ontologies can 
facilitate language processing.  Firstly, an ontology 
can be used directly when building the lexicon, 
defining the terms (concepts and relations) for 
content words.  Secondly, an ontology is a 
knowledge base  (KB), expressed in a formal 
language, and therefore it provides (formally 
                                                            
4
 Regulus is described in detail in (Rayner et al, 2001). 
expressed) knowledge for more complex language 
processing. 
5.1 Ontology and the lexicon 
We view an ontology as a knowledge base, 
consisting of a structured list of concepts, relations 
and individuals. The ontology provides partial 
definitions for these, through the taxonomy 
relation between the terms and the properties 
specified for them.  An example of how a fragment 
of a lexicon, for the content words in the domain, 
can be obtained from an ontology is presented 
below.   
We give in (6) an ontology fragment, where 
every concept is listed in the format shown in (5). 
 
(5)  (  concept_n 
        list-of-parents_of_concept_n 
        list-of-children_of_concept_n  ) 
(6)   ( ( |ship|  
        (|platform|)  
        (|frigate|) ) 
   ( |platform|  
        (|asset|)  
        (|aircraft| |ship|) ) 
   ( |frigate|  
(|ship|)  
   (|ffg|) ) ) 
 
For completeness, we give in Figure 1 the actual 
OWL format for this fragment.  
 
<?xml version="1.0" encoding="ISO-8859-1"?> 
<owls:Ontology xmlns:owls=http://www.w3.org/2002/OWL-
XMLSchema         
xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
xsi:schemaLocation="http://www.w3.org/2002/OWL-
XMLSchema 
http://potato.cs.man.ac.uk/owl/schemas/owl1-dl.xsd"> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#platform"> 
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#asset"/> 
    </owls:Class> <owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#ffg">
  
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#frigate"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#ship"> 
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#platform"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#frigate"> 
<owls:Class owls:name="file:/D:/ontology/focal.daml#ship"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#aircraft"> 
<owls:Class 
owls:name="file:/D:/ontology/focal.daml#platform"/> 
    </owls:Class> 
<owls:Class owls:complete="false" 
owls:name="file:/D:/ontology/focal.daml#asset"/> 
</owls:Ontology> 
Fig. 1:  FOCAL ontology fragment in OWL format 
 
Simplified lexical entries for the words aircraft, 
airplane, airplanes, plane, planes, ship, ships, 
frigate, frigates and FFG are shown in (7) and (8). 
 
(7) macro for noun class "platform": 
macro(noun_like_platform(Words,Sem),    
       @noun(Words, [sem= @noun_sem(platform, Sem), 
        sem_n_type=platform, takes_det_type=def\/null,  
        n_of_mod_type=_])). 
 
(8) examples of nouns for class "platform": 
@noun_like_platform([frigate, frigates], ship). 
@noun_like_platform([ffg], ship). 
@noun_like_platform([ship, ships], ship). 
@noun_like_platform([airplane,airplanes,plane,planes], 
aircraft). 
@noun_like_platform([aircraft], aircraft). 
 
This example shows how synonyms are handled 
in our system, with the same semantic 
interpretation, and the same parent class, given to a 
number of lexical items. 
5.2 Ontology as knowledge 
Since an ontology is a knowledge base expressed 
in a formal language, it provides formally 
expressed  knowledge for language processing.  
Although at this point not all this knowledge can 
be used directly by the speech recognition system 
which processes the speech input, nor by the 
grammar which builds the meaning 
representations, some of this knowledge can 
already be used by the other Dialogue agents, in 
particular the Dialogue Manager, and later by the 
Reference Resolver.   
The best example is the resolution of ambiguity, 
such as the polysemy of some terms.  For instance 
the name Adelaide can refer to a city (Adelaide in 
South Australia), a ship ("HMAS Adelaide"), a 
river (the Adelaide River in the Northern Territory 
of Australia), or even a person, (e.g. "Queen 
Adelaide").  While, as shown in Section 5.1,  
synonymy is handled by the lexicon, polysemy is 
resolved by drawing on a variety of sources, 
including the ontology.   
When the Dialogue Manager receives from the 
Input Fuser a set of communicative acts, if one of 
these communicative acts correspond to distinct 
plausible interpretation results, e.g. 
"Adelaide:{city, ship}", it can try to resolve the 
ambiguity by using the context information and by 
sending a request to the KR agent. 
6 Conclusion 
This paper has described our current work within 
the FOCAL project to combine ontologies built 
with the OIL/RACER framework with our spoken 
dialogue system.  It provides some suggestions on 
how ontologies can help a natural language 
processing component build semantic 
representations which are directly used in a 
complex information management system. 
This is work in progress and a formal evaluation 
has not yet been put in place. However, the 
reviewers for this paper have rightly asked how 
this would be conducted.  In the agent-based 
architecture we use, each agent can be tested in 
isolation and we have already conducted tests to 
ensure that the answers returned by the KR agent  
for specific questions in our scenario are correct 
and consistent.  A more interesting evaluation will 
be possible when the scenario is expanded, to see 
whether unplanned answers returned when the 
system is asked new unscripted questions are in 
fact useful to the users.  This will take place in the 
next phase of the project. 
For now, we conclude that an ontology is a 
knowledge base which can serve as the basis for 
creating the part of the lexicon for domain content 
words.  This is achieved by producing a list of 
terms with their meanings, i.e. partial definitions 
given the two hierarchies in the ontology, and we 
are exploring methods to automate this process. 
References 
S. Bechhofer and Ian Horrocks. 2003. The Wonder Web 
Ontology Language.  Report and Tutorial. 
S. Blackburn. 1996. The Oxford Dictionary of 
Philosophy. Oxford University Press. 
Nathalie Colineau and C?cile Paris. 2003. Framework 
for the Design of Intelligent Multimedia Presentation 
Systems: An architecture proposal for FOCAL. 
CMIS Technical Report 03/92, CSIRO, May 2003. 
Daniel C. Dennet. 1987. The Intentional Stance. 
Cambridge: MIT Press. 
Dominique Estival, Michael Broughton, Andrew 
Zschorn, Elizabeth Pronger. 2003. "Spoken Dialogue 
for Virtual Advisers in a semi-immersive Command 
and Control environment". In Proceedings of the 4th 
SIGdial Workshop on Discourse and Dialogue, 
Sapporo, Japan. pp.125-134. 
FOCAL. 2002. DSTO and Virtual Reality. 
http://www.dsto.defence.gov.au/isl/focal.pdf.  
Global InfoTek Inc. 2002. Control of Agent Based 
Systems.  http://coabs.globalinfotek.com. 
T. R. Gruber. 1995.  "Toward Principles for the Design 
of Ontologies Used for Knowledge Sharing". Human 
and Computer Studies,  vol. 43, no. 5-6. 
Ian Horrocks, Peter F. Patel-Schneider and Frank van 
Harmelen. 2003. "From SHIQ and RDF to OWL: 
The Making of a Web Ontology Language". Journal 
of Web Semantics,vol.1, no,1, pp.7-26.  
Ian Horrocks, U. Sattler and S. Tobies. 2000.  "Practical 
reasoning for very expressive description logics". 
Logic Journal of the IGPL, 8(3):239-263. 
Dale A. Lambert. 2001. "An Exegesis of Data Fusion". 
In Soft Computing in Measurement and Information 
Acquisition, eds. L. Reznik and V. Kreinovich.  
Physica-Verlag. 
Dale A. Lambert. 2003. "Grand Challenges of 
Information Fusion". In Proceedings of the Sixth 
International Conference on Information Fusion. 
Cairns, Australia. 
Chris Nowak. 2003. "On ontologies for high-level 
information fusion". In Proceedings of the Sixth 
International Conference on Information Fusion. 
Cairns, Australia. 
Nuance.  2002.  http://www.nuance.com/.   
Manny Rayner, John Dowding, Beth Ann Hockey.  
2001. "A Baseline method for compiling typed 
unification grammars into context free language 
models". In Proceedings of Eurospeech 2001, pp 
729-732. Aalborg, Denmark.  
Manny Rayner, Beth Ann Hockey, John Dowding.  
2002.  "Grammar Specialisation meets Language 
Modelling".  ICSLP 2002. Denver, USA.  
rVoice. 2002. Rhetorical Systems, 
http://www.rhetoricalsystems.com/rvoice.html. 
Steven Wark, Andrew Zschorn, Michael Broughton, 
Dale Lambert. 2004. "FOCAL: A Collaborative 
Multimodal Multimedia Display Environment".  In 
Proceedings of  SimTecT.  Canberra, Australia. 
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 35?41,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning from OzCLO, the Australian Computational and Linguistics 
Olympiad 
 
Dominique Estival 
U. of Western Sydney 
d.estival@uws.edu.au 
John Henderson 
U. of Western Australia 
john.henderson@uwa.edu.au 
Mary Laughren 
U. of Queensland 
m.laughren@uq.edu.au 
 
Diego Moll? 
Macquarie U. 
diego.molla-
aliod@mq.edu.au 
Cathy Bow 
Charles Darwin U. 
cathy.bow@cdu.edu.au 
Rachel Nordlinger 
U. of Melbourne 
racheln@unimelb.edu.au 
Verna Rieschild 
Macquarie U. 
verna.rieschild@mq.edu.au 
Andrea C. Schalley 
Griffith U. 
a.schalley@griffith.edu.au 
 Alexander W. Stanley 
Macquarie U. 
alexander.stanley@students.mq.edu.au  
Colette Mrowa-Hopkins 
Flinders U.  
colette.mrowa-hopkins@flinders.edu.au 
 
 
Abstract 
The Australian Computational and Linguistics 
Olympiad (OzCLO) started in 2008 in only 
two locations and has since grown to a na-
tionwide competition with almost 1500 high 
school students participating in 2013. An Aus-
tralian team has participated in the Interna-
tional Linguistics Olympiad (ILO) every year 
since 2009. This paper describes how the 
competition is run (with a regional First 
Round and a final National Round) and the or-
ganisation of the competition (a National 
Steering Committee and Local Organising 
Committees for each region) and discusses the 
particular challenges faced by Australia (tim-
ing of the competition and distance between 
the major population centres). One major fac-
tor in the growth and success of OzCLO has 
been the introduction of the online competi-
tion, allowing participation of students from 
rural and remote country areas. The organisa-
tion relies on the good-will and volunteer 
work of university and school staff but the 
strong interest among students and teachers 
shows that OzCLO is responding to a demand 
for linguistic challenges. 
1 Introduction 
The Australian Computational and Linguistic 
Olympiad (OzCLO, www.ozclo.org.au) began as 
an idea in late 2007, largely prompted by a par-
ent in Ballarat, a small town in Victoria, who 
came across the North American competition 
(NACLO, Radev et al 2008) on the internet and 
thought it was something that her daughter 
would be interested in doing.  Her emails to the 
organisers of NACLO, asking about the likeli-
hood of such an event being run in Australia, led 
to initiating contact with the Australasian Lan-
guage Technology Association (ALTA) with the 
suggestion that a computational linguistic olym-
piad be established in Australia. Dominique Es-
tival (then at Appen Pty Ltd, and a member of 
the ALTA Steering Committee) took on the pro-
ject and, jointly with Jane Simpson (then from 
the University of Sydney), Rachel Nordlinger 
and Jean Mulder (from the University of Mel-
bourne), ran the first ever Australian Computa-
tional and Linguistic Olympiad in 2008, with 
financial support from HCSNet (the Human 
Communication Science Network), and help 
from ALTA (the Australasian Language Tech-
nology Association), ALS (the Australian Lin-
guistic Society) and CSIRO (the Commonwealth 
Scientific and Industrial Research Organisation).  
The first competition was held in two locations ? 
the University of Melbourne (Victoria) and the 
University of Sydney (New South Wales) ? with 
a total of 119 students participating from 22 
schools.  Given the success of this first competi-
tion, 2009 saw the addition of four new locations 
around Australia (Adelaide, South Australia; 
Brisbane, Queensland; Canberra, ACT; Perth, 
Western Australia) and the sending of the na-
tional winning team to the International Linguis-
tic Olympiad in Wroclaw, Poland. Since then 
OzCLO has run every year, with the recent addi-
tion of two regions (NSW-North in 2010 and 
Northern Territory in 2013) and the participation 
of an Australian team in every ILO.  
35
2 Philosophy, Aims and Principles 
The immediate aim of OzCLO (Simpson and 
Henderson, 2010) is to introduce high school 
students to language puzzles from which they 
can learn about the richness, diversity and sys-
tematic nature of language, and develop their 
reasoning skills. The general value of this type of 
knowledge and skills in high school education 
has not been specifically articulated to potential 
participants or their teachers, schools or parents, 
as it has in the UK (UKLO, 2011; Hudson and 
Sheldon, 2013). However, informal feedback and 
the participation rate both indicate a widespread 
perception in the school sector that this type of 
activity has educational value, albeit with differ-
ent focuses in different schools. For many of the 
schools that participate, OzCLO provides a 
means to meet their institutional responsibility to 
provide extra-curricular activities that are intel-
lectually stimulating and broadening for aca-
demically high-achieving students (under rubrics 
such as ?gifted and talented?). Some schools offer 
OzCLO to a wider range of students.  
The broader aim of OzCLO is to promote 
awareness of, and interest in linguistics and 
computational linguistics in high schools and in 
the wider community, and more specifically to 
increase enrolments in these disciplines at uni-
versity level. A further goal is that this will ulti-
mately attract people to careers in these areas. 
Linguistics has traditionally had little recognition 
at high school level in Australia, even within 
language education, although more recently there 
is linguistics content at upper high school level in 
the English Language course in Victoria and in 
the new national English curriculum. OzCLO has 
been running in most regions long enough to see 
participants reaching university, and although 
there has been no proper research on the impact 
of OzCLO on enrolments, there is anecdotal evi-
dence that some former participants have chosen 
to study at least some linguistics. 
Consistent with the key aim of promoting in-
terest, OzCLO operates on the principles that 
participation should be fun and should offer 
achievable if challenging tasks to a wide range of 
students across science and humanities interests, 
especially in the First Round. Schools are pro-
vided with a training package of problems which 
starts with a simple morphological analysis that 
is suitable to do as a whole-class exercise even if 
they do not proceed to the competition itself. In 
both rounds participation takes place in school-
based teams, rather than individual competition. 
This is partly to encourage students to learn to 
communicate their analytical ideas, to collabo-
rate effectively, and to provide mutual support 
and social interaction. It also offers some organ-
isational advantages in terms of registration and 
marking. Because team members may have dif-
ferent levels of ability, the competition process 
does not necessarily identify the highest achiev-
ing individuals, but this risk is out-weighed by 
the benefits of teams. The organisation of the 
First Round as separate competitions in each re-
gion provides each team with a smaller pool to 
compete in initially and a distinct level of local 
achievement. However, since there are consider-
able differences in the number of teams in each 
region, and the top teams from each region are 
invited into the National Round, the national 
competition does not necessarily consist of the 
highest achieving teams nationally and there is 
currently discussion of methods to minimise this 
effect. Finally, the results are structured to rec-
ognise participation as well as high achievement: 
in addition to recognising the top teams, all 
teams receive certificates in the categories Gold 
(top ?25%), Silver (next ?25%) and Bronze (re-
mainder). 
3 Organising the Annual Competition 
3.1 University level 
All Australian states and territories (with the ex-
ception of Tasmania) now participate in OzCLO 
and there is typically one Local Organising 
Committee (LOC) for each geographical region. 
There are currently eight LOCs (soon to be nine 
with the addition of a third New South Wales 
region).  Each LOC has the responsibility for 
student and school liaison, university space 
booking, recruiting volunteer academic and stu-
dent helpers, running the competitions, publicis-
ing the event locally, and finding cash or in-kind 
sponsorship (e.g. for rooms, venues, printing and 
prizes). 
The National Steering Committee (NSC) 
comprises the Chair of each LOC, the Problems 
Coordinator, the Treasurer, the OZCLO Web-
master and the Online Competition Coordinator.  
The NSC?s role is to coordinate between LOCs, 
make and implement OzCLO decisions, and co-
ordinate national sponsorships and publicity. A 
training package is developed by the NSC and 
provided online each year, on the OzCLO web-
site and within the online competition site. The 
NSC Chair has the responsibility of ensuring the 
coordination and execution of tasks for OzCLO, 
36
both nationally and internationally. The NSC 
Chair and the Problems Coordinator liaise with 
ELCLO (English Linguistics and Computational 
Linguistics Olympiads) with regard to develop-
ing annual problem sets, and with the Interna-
tional Linguistics Olympiad (ILO/IOL) with re-
gard to the international competition. NSC mem-
bers may have dual responsibilities. 
 Because of the distances between regional 
centres, the NSC meetings are all conducted via 
teleconferences, and committee members share 
documents and records using Airset, a cloud-
based collaboration site. 
3.2 School level 
OzCLO operates on a democratic basis, with the 
devolution of decision making passing from NSC 
to LOC to school teacher to students. Teacher 
and student feedback often contributes to NSC 
discussions. Information is disseminated to 
school teachers through the website as well as 
through emails from the region?s LOC. This in-
formation is also shared via Facebook and Twit-
ter accounts. Training sessions are provided 
online, at universities and, in some cases, within 
schools.  Teachers register teams of 4 members 
at the Junior (Years 9 and 10) or Senior level 
(Years 11-12) online. There is no limit to regis-
trations for the online competition, but registra-
tions for the offline competition (in which stu-
dents typically attend the organising University 
campus) may be constrained by University venue 
availability issues. Some schools have Linguis-
tics Clubs, and OzCLO is a strong focus for their 
activities.  In some regions, schools with over 80 
participating students request in-house training 
and invigilation for an offline First Round. 
3.3 The public face of OzCLO 
OzCLO has a website (www.ozclo.org.au) and a 
social media presence with Twitter and Facebook 
accounts for communications and promotion. 
Most LOCs have been successful in gaining pub-
licity for OzCLO through their University media 
departments. Many schools publish pictures and 
items about OzCLO achievements in their school 
newsletters. Some individual schools have fea-
tured in the local press after results of competi-
tions have been published. OzCLO has also fea-
tured in national radio segments. 
4 The OzCLO Competition  
4.1 Competitions Rounds  
The OzCLO competition consists of two rounds, 
a regional or state-wide First Round and a Na-
tional Round. In both, school-based teams of up 
to four students attempt to solve five or six lin-
guistic problems in two hours. The teams are 
divided into Senior and Junior sections, with the 
Senior teams drawn from the last two years of 
high school (Tears 11 and 12) while the Junior 
teams are drawn from the two preceding years 
(Years 9 and 10). The same problems sets and 
competition conditions hold for both Senior and 
Junior teams. The top three teams from each 
LOC are invited to go on to the National Round 
which is held under the same conditions. If the 
top Junior team is not in the overall top three 
teams, then it is also invited. The Senior team 
which wins the National Round is invited to rep-
resent Australia at the ILO. 
4.2 Problem sets 
In its first two years, OzCLO greatly benefited 
from NACLO, which allowed use of their prob-
lem sets. Some additional problems were com-
posed by linguists engaged in the running of the 
competition, or their colleagues. Since 2009, 
OzCLO has been part of ELCLO, the English 
Language Computational Linguistics Olympiad, 
in which participating countries (Australia, Ire-
land, North America and the United Kingdom) 
contribute to a shared set of problems. Because 
of the OzCLO rationale described above, an at-
tempt is made to try to have a mix of problems 
based on data from a wide range of languages, 
and also a wide range of data types. Different 
levels of difficulty are included so that students 
have the satisfaction of being able to solve most 
of the problems. The aim is to show students that 
analysing language phenomena can be fun as 
well as challenging, and also that linguistic skills 
can be applied to some very practical tasks. The 
problems include: deciphering non-Roman 
scripts; translation tasks involving typical mor-
phological and syntactic analysis; computational 
linguistic tasks; search for phonological rules, or 
linguistic reconstruction. 
4.3 Training for ILO 
Since 2009, an Australian team has participated 
in every ILO. While the main goal of OzCLO 
has always been the promotion of language stud-
ies, linguistic knowledge and analysis skills in 
37
Australian high schools, the appeal of potentially 
participating in an international competition has 
proved an additional incentive for many of the 
students and their teachers. However, because of 
the rationale for OzCLO discussed above, the 
problems used in the First Round and even the 
National Round are not nearly as difficult as the 
actual ILO problems. Therefore the Australian 
team needs to be given additional training before 
competing at the international level. This training 
was first provided by a coach accompanying the 
team at the ILO but we have found that this was 
insufficient and too late to be helpful. We now 
provide training sessions aimed at solving ILO-
level problems to the winning team prior to trav-
elling to the ILO. This has resulted in higher re-
sults, including an individual silver medal in 
2011 and honourable mentions in 2010 and 2012. 
5 Participation 2008-2013 
OzCLO has evolved from 22 schools and 119 
competing students in 2008 to 87 schools and 
1,451 competing students in 2013. Some schools 
have participated each year, and there has been a 
steady increase in new schools. Private and se-
lective government schools have so far been the 
majority in most regions, but the numbers of 
government schools participating are growing. 
All participating schools are highly enthusiastic 
about the OzCLO competitions. 
OzCLO naturally attracts schools keen on of-
fering a new kind of challenge to students in 
their GATS (gifted and talented students) pro-
grammes. However, teachers (not only language 
teachers, but also mathematics and computer sci-
ences teachers) also comment that OzCLO is a 
rare kind of competition because it provides fun, 
challenge, stimulation and team work for any 
student. 
A challenge for Australia compared with 
Europe or North America is the enormous dis-
tance between rural and metropolitan areas, mak-
ing it difficult for many schools in rural areas to 
participate in an offline University-based compe-
tition. The advent of the online option gives ur-
ban, rural and country remote students equity in 
access. Thanks to this plus a strong marketing 
drive in that state, numbers have increased dra-
matically in Queensland. In other regions, some 
schools prefer the university campus experience 
offered by the offline option. 
As Table 1 shows, numbers have increased 
steadily over the six years since inception. In 
2013, Australia?s population of 23 million has 
provided nearly as many Linguistics Olympiads 
competitors as has the United States and Canada 
combined, whose population figures are fifteen 
times more than Australia?s. The OzCLO par-
ticipation rate is 6.4 per 100,000 population. For 
UKLO it is 4.55, and for NACLO 0.49. 
6 Going on-line 
In the first four years of OzCLO?s existence, the 
competition was offered on campus by academic 
staff volunteers from a number of mainly metro-
politan Universities. Participating teams travelled 
from their schools to the respective Universities? 
campuses to take part in the Training Session and 
the First Round, except for NSW, where several 
OzCLO representatives also travelled to schools 
with a large participation base, in order to run the 
competition at the school. Teachers often re-
ported that these visits to the University campus 
were a highlight for the participating students 
who very much enjoyed the experience. 
Nonetheless, a number of drawbacks to this 
approach became apparent quite early. These 
included: 
? The difficulty of organising suitable venues 
on campus for running the competition due to 
the timing of the First Round (usually coin-
ciding with Universities? Orientation Week or 
their first weeks of teaching in the first semes-
ter). 
? The distance factor with the result that only 
schools within travel distance could partici-
pate in the competition (in the case of Queen-
sland, for instance, no school beyond a dis-
tance of about 100kms from campus partici-
pated in the offline competition). Given the 
size of Australia, most regional and rural 
schools were thus virtually excluded from 
competing. 
? Constraints on availability of venues and 
markers put a cap on the overall number of 
students who could compete in each region. 
Thus, the number of schools and the number 
of students per school had to be limited by the 
local committees from the outset (e.g. in 
Queensland, only two teams per school were 
able to compete, although some schools 
wished to enrol many more).  
 
 
38
LOC 
 
2008 
Schools/ 
students 
2009 
Schools/ 
Students 
2010 
Schools/ 
students 
2011 
Schools/ 
students 
2012 
Schools/ 
students 
2013 
Schools/ 
students 
Region  
population 
000s 
Participants 
per 100,000 
population 
NSW-S 10 
64 
14 
105 
[fn/a] 
92 
15 
279 
12 
289 
9 
312 
7,314 5.24 
NSW-N n/a  n/a  5 
40 
7 
58 
5 
60 
6 
71 
VIC 12 
55 
11 
90 
[fn/a] 
120 
9 
115 
16 
245 
18 
304 
5,649 5.38 
ACT n/a  7 
30 
5 
83 
5 
72 
9 
136 
9 
161 
377 42.76 
QLD n/a  11 
60 
15 
90 
15 
106 
20 
312 
25 
377 
4,585 8.22 
SA n/a  [fn/a]  
29 
5 
33 
3 
19 
4 
27 
3 
34 
1,658 2.05 
NT n/a  n/a  n/a  n/a  n/a  6 
80 
236 33.86 
WA n/a  10 
78 
11 
144 
16 
143 
14 
120 
12 
120 
2,451 4.90 
TAS n/a n/a n/a n/a n/a n/a 512 0 
Overall  119 
students 
392 
students 
602 
students 
792  
students 
1069  
students 
1459  
students 
22,786 6.40 
Table 1:  Participation schools/students 
(n/a = not applicable = LOC was not participating;  [fn/a] =figure not available 
 
In order to address these issues, it was de-
cided to offer an online option in 2012, using 
Griffith University?s Learning Management 
System. This lifted restrictions on numbers 
(both school and students per school), and 
schools were able to compete from anywhere 
in Australia if they so wished. As a result, 
schools located as far as 1,500 kms from the 
metropolitan areas have successfully partici-
pated in the competition, and some schools 
registered more than 20 teams in the latest 
competition. With the online option, the over-
all number of participants has increased dra-
matically (see Table 2). For instance, Victoria 
saw the number of their participants double 
from 2011 to 2012, while numbers in Queen-
sland nearly tripled. Even in those regions that 
shifted to exclusively offering the online op-
tion (such as Queensland in the last two years), 
most schools have remained in the competi-
tion. 
 
 
 2012 2013 
LOC Online students On campus students Online Students On-campus Students 
NSW-S 91 198 120 192 
NSW-N 60 [on/a] 8 63 
VIC 137 108 195 109 
ACT 64 72 115 46 
QLD 312 [on/a] 377 on/a 
SA 0 27 34 on/a 
WA 28 92 120 on/a 
NT n/a n/a 80 on/a 
Table 2: Participation numbers by mode (online/on-campus) 
 (n/a = not applicable (LOC was not participating);  [fn/a] =figure not available;  
[on/a] =option not available) 
39
In terms of students competing online vs. 
on-campus, except for the NSW-N region, 
there is a distinct shift towards participating 
online. Feedback from teachers has shown that 
in many cases it is easier for teams to stay 
within the school grounds for the competition 
rather than to travel to the University campus. 
For some schools, however, travelling to the 
University campus is still one of the major 
benefits they would not want to lose. For this 
reason most LOCs offer both on-campus and 
online options. Some regions choose to only 
offer the online option (with a training session 
at the University). 
Teams participating online have access to 
training materials and all the necessary infor-
mation, which is made available through the 
OzCLO website well before the competition 
day. This site also allows teams to familiarise 
themselves with the online testing system. On 
the day, all teams across Australia compete at 
the same time on the same day and within the 
same two hour period (to compensate for time 
zone differences, teams started at 12:00 in 
WA, 13:30 in the NT, 14:00 in QLD, 14:30pm 
in SA and 15:00 in the ACT, NSW and VIC in 
the 2013 competition). 
In terms of process and technical require-
ments, each participating team needs access to 
an Internet-enabled computer on the day of the 
competition. No special software is required on 
the school?s computers. The problem set is 
made available to teachers shortly before the 
competition commences, in order to allow 
them to print and copy the problems for the 
students. Students usually work on the paper 
copy, and then access the computer to enter 
their responses.  There is also a virtual class-
room set up for live communication during the 
competition, in order to allow students and 
teachers to ask questions but also to show stu-
dents that there are hundreds of competitors 
participating from around the country at the 
same time.  
Overall, the addition of the online alterna-
tive has been a very beneficial development for 
OzCLO. The strong growth in overall partici-
pant numbers over recent years is not simply 
due to the online option, but this has certainly 
played a major role. It remains to be seen if 
there is even more potential for growth ? espe-
cially in areas outside of the major cities. 
7 Challenges 
One of the main challenges OzCLO faces is 
the timing of the competition in relation to the 
schedule of the international linguistics compe-
titions. The Australian school year begins in 
February and ends in December, and the uni-
versity year is roughly March to November, in 
contrast to the September-June academic cal-
endars of the northern hemisphere. In order for 
an Australian team to be selected with enough 
time to prepare for participation in the ILO, the 
National Round needs to be held before the 
Easter break (March/April). For Universities 
and schools, this creates a very rushed timeline 
at the busiest time of the school/academic year. 
As mentioned earlier, another challenge for 
Australia is the vast distances between metro-
politan areas, where most of the universities 
are located. In spite of the success of the online 
competition, so far OzCLO has had mostly a 
metropolitan base and has not yet fully en-
gaged in marketing to regional and rural areas 
across the whole country. Targeting appropri-
ate teachers within schools can also be a chal-
lenge, as experience has shown that often the 
information does not filter through to the rele-
vant teachers (these are usually the coordina-
tors of Languages, Gifted Education, Mathe-
matics, or Computing programmes). Contact-
ing the professional associations for the differ-
ent teaching specialties could ensure that in-
formation is disseminated more efficiently. 
Funding is not guaranteed, and fundraising 
efforts are not rewarded every year. All organ-
isational efforts at University and school level 
depend on good-will and volunteering as well 
as donations. Changes in Heads of Depart-
ments in Universities and principals in schools 
can impact negatively on funds and participa-
tion levels. This means that core issues need to 
be resolved again every year, for example, the 
ongoing maintenance of the OzCLO web-
site/online registration system, which is both a 
challenge and a solution to other issues. The 
OzCLO website hosting is provided by Mac-
quarie University and the site is maintained by 
a student volunteer.  It has served as the central 
hub of information, with other modes (email, 
Facebook and Twitter) leading back to it for 
detailed information.  In addition to ordinary 
information, it also enables self-service regis-
tration, and the automated generation of PDF 
certificates after the competition. These facili-
ties and the volunteer support of the webmaster 
40
have significantly lowered the administrative 
and financial overhead for the organisers.  
An additional problem for OzCLO is the 
division of Australia?s most populated state 
(NSW, with almost a third of Australia?s popu-
lation) into northern and southern regions, 
which leads to one state providing double the 
competitors of other states into the National 
Round. A model is needed whereby all com-
petitors, no matter whether they come from a 
small or a large region, have an equal opportu-
nity to compete in the National Round.  
Finally, while OzCLO has been able to con-
tribute a number of linguistic problems to the 
ELCLO pool, it has proved extremely difficult 
to obtain contributions from Computational 
Linguistics (Estival, 2011). 
8 Conclusions 
In conclusion, running the OzCLO competition 
has been an activity well worth the effort, and 
it is very rewarding that it has become a fixture 
in the academic calendar for many schools. 
Students, teachers and principals have been 
extremely positive about the experience, giv-
ing encouraging feedback and expressing 
strong support for the competition. The recent 
increases in participation rates have come from 
new regions (only one Australian state cur-
rently has no LOC, but possibilities are being 
explored in this area), new schools, and larger 
numbers from individual schools (up to 100 
participants from a single school). Some 
schools have started a linguistics club as after 
school activity, and others are promoting their 
experiences on social media.  
While there is no data currently available 
regarding any effect on enrolments in tertiary 
linguistics programs, increased interest in and 
awareness of linguistics is certainly a positive 
outcome for a discipline which faces chal-
lenges of funding and viability. The coopera-
tion of academics from universities across the 
country in all the LOCs and the NSC, plus the 
support of the Australian Linguistics Society 
(ALS) and of the Australasian Language 
Technology Association (ALTA), make the 
competition a truly national event. This means 
that the competition is not dependent on any 
one single person or institution (although com-
petition within particular regions is), and al-
lows for further growth. Ongoing funding and 
continued support from both universities and 
schools across the country should see contin-
ued growth in the popularity and spread of the 
competition. 
References  
Derzhanski, Ivan, and Thomas Payne. (2010). The 
linguistic olympiads: academic competitions in 
linguistics for secondary school students. Lin-
guistics at school: language awareness in pri-
mary and secondary education, ed. By Kristin 
Denham and Anne Lobeck, 213?26. Cambridge: 
Cambridge University Press. 
Estival, Dominique. (2011). ?OzCLO: The Austra-
lian Computational Linguistic Olympiad?. Pro-
ceedings of the Australasian Language Technol-
ogy Association Workshop 2011. Canberra, Aus-
tralia.  
Hudson, Richard, and Sheldon, Neil. (2013). ?Lin-
guistics at School: The UK Linguistics Olym-
piad.? Language and Linguistics 
pass, Volume 7, Issue 2. pp. 
Radev, Dragomir R., Lori S. Levin, and Thomas E. 
Payne. (2008). ?The North American Computa-
tional Linguistics Olympiad (NACLO)?. In Pro-
ceedings of The Third Workshop on Issues in 
Teaching Computational Linguistics, Columbus, 
OH, USA. 
91?104,  
Simpson, Jane, and Henderson, John. (2010).  Aus-
tralian Computational and Linguistics Olympiad. 
Cross Section, Vol. 20, No. 3, July 2010: 10-14.  
United Kingdom Linguistics Olympiad Committee. 
(n.d.) The Linguistics Olympiads: Lots of fun, 
but are they educa-
tional? http://www.uklo.org/?page_id=35  .
41
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 12?22,
Dublin, Ireland, August 23rd 2014.
Integrating UIMA with Alveo, a human communication science virtual 
laboratory 
 
Dominique Estival 
 U. of Western Sydney 
d.estival@uws.edu.au  
Steve Cassidy 
Macquarie University 
steve.cassidy@mq.edu.au 
Karin Verspoor 
University of Melbourne 
karin.verspoor@unimelb.edu.au 
                           Andrew MacKinlay                            Denis Burnham 
       RMIT                                      U. of Western Sydney  
                 andrew.mackinlay@rmit.edu.au                d.burnham@uws.edu.au 
Abstract 
This paper describes two aspects of Alveo, a new virtual laboratory for human communication science 
(HCS). As a platform for HCS researchers, the integration of the Unstructured Information Management 
Architecture (UIMA) with Alveo was one of the aims during the development phase and we report on 
the choices that were made for the implementation. User acceptance testing (UAT) constituted an inte-
gral part of the development and evolution of Alveo and we present the distributed testing organisation, 
the test development process and the evolution of the tests. We conclude with some lessons learned re-
garding multi-site collaborative work on the development and deployment of HLT research infrastruc-
ture. 
 
1 Introduction 
The Alveo Virtual Laboratory provides a new platform for collaborative research in human communi-
cation science (HCS). 1 Funded by the Australian Government National eResearch Collaboration Tools 
and Resources (NeCTAR) program, it involves partners from 16 institutions in a range of disciplines: 
linguistics, natural language processing, speech science, psychology, as well as music and acoustic 
processing. The goal of the platform is to provide easy access to a variety of databases and a range of 
analysis tools, in order to foster inter-disciplinary research and facilitate the discovery of new methods 
for solving old problems or the application of known methods to new datasets (Estival, Cassidy, 
Sefton, & Burnham, 2013). The platform integrates a number of tools and enables non-technical users 
to process communication resources (including not only text and speech corpora but also music re-
cordings and videos) using these tools in a straightforward manner. In this paper, we report on the re-
cent integration of the Unstructured Information Management Architecture (UIMA) with Alveo. This 
integration is bi-directional, in that existing resources and annotations captured over those resources in 
Alveo can flow to a UIMA process, and new annotations produced by a UIMA process can be con-
sumed and persisted by Alveo. We also introduce the general approach to user acceptance testing 
(UAT) of Alveo, focussing on the organisation and process acceptance adopted to meet the acceptance 
criteria required for the project and to ensure user uptake within the research community. Finally, we 
demonstrate the application of the testing process for acceptance of the UIMA integration. 
Section 2 briefly describes Alveo and its components, in particular the tools and corpora already 
available on the platform and the workflow engine, then Section 3 describes the Alveo-UIMA integra-
tion. Section 4 describes the UAT requirement and the organisation of the testing among the Alveo 
project partners, outlines the actual testing process and gives examples of the tests, among them the 
UIMA tests, which were developed for the project. Section 5 discusses alternative strategies and we 
conclude with some lessons learned regarding multi-site collaborative work on the development and 
deployment of HLT research infrastructure. 
                                                 
1  This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0. 12
2 The Alveo Virtual Laboratory 
Alveo provides easy access to a range of databases relevant to human communication science disci-
plines, including speech, text, audio and video, some of which would previously have been difficult 
for researchers to access or even know about. The system implements a uniform and secure license 
management system for the diverse licensing and user agreement conditions required. Browsing, 
searching and dataset manipulation are also functionalities which are available in a consistent manner 
across the data collections through the web-based Discovery Interface. The first phase of the project 
(December 2012 ? June 2014) saw the inclusion of the collections shown in Table 1. 
 
1. PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures): audio, video, 
text and image resources for Australian and Pacific Island languages (Thieberger, Barwick, Billington, & 
Vaughan, 2011) 
2. AusTalk, audio-visual speech corpus  of Australian English (Burnham et al., 2011) 
3. The Australian National Corpus  (S. Cassidy, Haugh, Peters, & Fallu, 2012) comprising: Australian Cor-
pus of English (ACE); Australian Radio Talkback (ART); AustLit; Braided Channels; Corpus of Oz Early 
English (COOEE); Griffith Corpus of Spoken English (GCSAusE); International Corpus of English (ICE-
AUS); Mitchell & Delbridge corpus; Monash Corpus of Spoken English (Musgrave & Haugh, 2009). 
4. AVOZES, a visual speech corpus (Goecke & Millar, 2004) 
5. UNSW Pixar Emotional Music Excerpts: Pixar movie theme music expressing different emotions 
6. Sydney University Room Impulse Responses: environmental audio samples which, through convolution 
with speech or music, can create the effect of that speech or music in that acoustic environment 
7. Macquarie University Battery of Emotional Prosody: sung sentences with different prosodic patterns 
8. Colloquial Jakartan Indonesian corpus: audio and text, recorded in Jakarta in the early 1990?s (ANU) 
9. The ClueWeb dataset (http://lemurproject.org/clueweb12/). 
  Table 1: Alveo Data Collections 
 
Through the web-based Discovery interface, the user can select items based on the results of faceted 
search across the collections and can organise selected data in Items Lists. Beyond browsing and 
searching, Alveo offers the possibility of analysing and processing the data with a range of tools. In 
the first phase of the project, the tools listed in Table 2 were integrated within Alveo.  
 
1. EOPAS (PARADISEC tool) for text interlinear text and media analysis 
2. NLTK (Natural Language Toolkit) for text analytics with linguistic data (Bird, Klein, & Loper, 2009) 
3. EMU, for search, speech analysis and interactive labelling of spectrograms and waveforms (Steve Cassidy 
& Harrington, 2000) 
4. AusNC Tools: KWIC, Concordance, Word Count, statistical summary and statistical analysis 
5. Johnson-Charniak parser, to generate full parse trees for text sentences (Charniak & Johnson, 2005) 
6. ParseEval, to evaluate the syllabic parse of consonant clusters (Shaw & Gafos, 2010) 
7. HTK-modifications, a patch to HTK (Hidden Markov Model Toolkit, http://htk.eng.cam.ac.uk/) to enable 
missing data recognition 
8. DeMoLib, for video analysis (http://staff.estem-uc.edu.au/roland/research/demolib-home/) 
9. PsySound3, for physical and psycho-acoustical analysis of complex visual and auditory scenes (Cabrera, 
Ferguson, & Schubert, 2007) 
10. ParGram, grammar for Indonesian (Arka, 2012) 
11. INDRI, for information retrieval with large data sets (http://www.lemurproject.org/indri/) 
Table 2: Alveo Tools 
 
Most of these tools require significant expertise to set up and one of the Alveo project goals is to make 
this easier for non-technical researchers. The Alveo Workflow Engine is built around the Galaxy open 
source workflow management system (Goecks, Nekrutenko, Taylor, & Team, 2010), which was origi-
nally designed for use in the life sciences to support researchers in running pipelines of tools to ma-
nipulate data. Workflows in Galaxy can be stored, shared and published, and we hope this will also 
become a way for human communication science researchers to codify and exchange common anal-
yses.  
A number of the tools listed in Table 2 have been packaged as Python scripts, for instance NLTK 
based scripts to carry out part-of-speech tagging, stemming and parsing. Other tools are implemented 13
in R, e.g. EMU/R and ParseEval. An API is provided to mediate access to data, ensuring that permis-
sions are respected, and providing a way to access individual items, and 'mount' datasets for fast ac-
cess (Steve Cassidy, Estival, Jones, Burnham, & Burghold, 2014). An instance of the Galaxy Work-
flow engine is run on a virtual machine in the NeCTAR Research Cloud, a secure platform for Aus-
tralian research, funded by the same government program (https://www.nectar.org.au/research-cloud).  
Finally, a UIMA interface has been developed to enable the conversion of Alveo items, as well as their 
associated annotations, into UIMA CAS documents, for analysis in a conventional UIMA pipeline. 
Conversely annotations from a UIMA pipeline can be associated with a document in Alveo. Figure 1 
gives an overview of the architecture. 
 
 
Figure 1: The architecture of the Alveo Virtual Laboratory 
 
2.1 Annotations in Alveo 
Annotations in Alveo are stored in a standoff format based on the model described in the ISO Linguis-
tic Annotation Framework (ISO-LAF).  Internally, annotations are represented as RDF using the DA-
DA (Steve Cassidy, 2010). Each annotation is identified by a distinct URI and references into the 
source documents are stored as offsets either using character positions, times or frame counts for au-
dio/video data.  Annotations have an associated type attribute that denotes the kind of annotation 
(speaker turn, part of speech, phonetic segment) and a label that defines a simple string value associat-
ed with the annotation.  Annotations may also have other properties defined as standard RDF proper-
ties and values.   
The API exposes a direct interface to the annotation store in RDF via a SPARQL endpoint, but the 
normal mode of access is via the REST API where each item (document) has a corresponding URI 
that returns the collection of annotations on that item in a JSON-LD format.  JSON-LD allows us to 
represent the full RDF namespaces of properties and values in a concise format that is easily processed 
using standard JSON tools.  An example annotation delivered in this format is shown in Figure 2.  The 
same JSON-LD format can be used to upload new annotations to be stored in Alveo. 
 
{ 
 "@context": "https://app.alveo.edu.au/schema/json-ld", 
 "commonProperties": { 
    "alveo:annotates":"https://app?AusE08/document/GCSAusE07.mp3" 
 }, 
 "alveo:annotations": [ 
 { 
  "@id": "http://ns.ausnc.org.au/corpora/gcsause/annotation/535958", 
  "type": "http://ns.ausnc.org.au/schemas/annotation/conversation/micropause", 
  "@type": "dada:TextAnnotation", 
  "end": "34", 
  "start": "33" 
 }, 
?} 
Figure 2: An example of the Alveo JSON-LD annotation format 
 
 
 14
 3 UIMA 
3.1 Background 
The Unstructured Information Management Architecture (UIMA) is an Apache open source project 
(http://uima.apache.org) (D. Ferrucci & Lally, 2004) that provides an architecture for developing ap-
plications involving analysis of large volumes of unstructured information. This framework allows 
development of modular pipelines for analysing the sorts of data available in Alveo, including speech, 
text and video. A number of groups around the world have adopted UIMA to enable easier interopera-
bility and sharing of language technology components. This is true particularly in the biomedical natu-
ral language processing community; several groups have made tools available as UIMA modules. 
Most OpenNLP modules have been wrapped for use within UIMA, and the UIMA community more 
broadly has a range of language technology tools available in UIMA-compliant modules (David 
Ferrucci et al., 2010). 
Each component in a UIMA application implements interfaces defined by the framework and pro-
vides self-describing metadata via XML descriptor files. The framework manages these components 
and the data flow between them. Since UIMA applications are defined in terms of descriptors that 
clearly specify both the component modules of the application and the configuration parameter set-
tings for executing the application, they are ?re-runnable, re-usable procedures? of the kind that Alveo 
aims to capture.  
Given the objective of Alveo to facilitate access to analysis tools, and the UIMA objective of mak-
ing such analysis tools interoperable, bringing the two frameworks together made sense. Thus the ob-
jective of the integration was to build a bidirectional translation layer between Alveo and any standard 
UIMA pipeline. In other words, the translation component was required to: 1) read corpus data includ-
ing associated annotations stored in Alveo into a UIMA pipeline and 2) store annotations produced by 
a UIMA pipeline in Alveo. 
3.2 Overview of the Conversion Layer Architecture 
We opted for the most straightforward connection between the two frameworks, i.e. communicating 
directly with the Alveo REST API. The approach involves allowing annotations and documents to 
flow from Alveo, be processed externally to Alveo in a specially-configured UIMA pipeline, and then 
providing a mechanism for new annotations over the documents to be returned to Alveo for storage. 
The Alveo REST API provides access to item metadata, documents and annotations using a JSON-LD 
based interchange format.  The API supports most actions that are available via the web interface in-
cluding meta-data queries and retrieval of documents either individually or in batches.  
We built the UIMA-Alveo conversion layer, denoted Alveo-UIMA. It allows reading a group of 
documents from Alveo, and converting the documents along with their associated annotations into 
UIMA Common Annotation Structure (CAS) instances. It also allows annotations produced by a UI-
MA Collection Processing Engine (CPE) pipeline on a set of CASes to be uploaded to an Alveo serv-
er. Alveo-UIMA is built on top of a native Java wrapper for the Alveo REST API. It is implemented in 
Java and is distributed as an open-source package.2 The conversion layer exposes the Alveo data as 
native Java data structures and is also available as a standalone package,3 providing, as a side effect, a 
method to access the Alveo REST API without needing to invoke the UIMA machinery. Similar pack-
ages are also available for Python and R in the Alveo repository. 
The first component of the UIMA interface, for reading existing items, is implemented as UIMA 
Collection Reader. This takes as parameters an Alveo item list ID, corresponding to a user-created list 
of documents, and server credentials. It converts the Alveo items from that item list, as well as their 
associated annotations, into UIMA CAS documents, which can then be used as part of a conventional 
UIMA pipeline. The UIMA processing pipeline can then take advantage of the annotations download-
ed from Alveo (e.g. by using a speaker turn annotation to demarcate a sentence). 
                                                 
2 https://github.com/Alveo; https://github.com/Alveo/alveo-uima 
3 https://github.com/Alveo/alveo-java-rest-api 
15
The second component is for the opposite direction, i.e. taking annotations from a UIMA pipeline 
and associating them with the document in Alveo. There is no capability yet to add new documents 
that derive from outside Alveo, as this is not currently possible using Alveo?s REST API. This means 
that documents for which we are uploading UIMA annotations must have originated in Alveo and 
have come from the Collection Reader, ensuring that each document has appropriate identifying 
metadata for Alveo.  
Annotations need to be converted from Alveo to UIMA and vice versa, since the annotation formats 
are not identical. Some attributes, such as textual character offsets, are directly convertible, while oth-
ers require more work. Every document retrieved from the Alveo server has metadata (e.g. data 
source, recording date and language) and annotations (e.g. POS tags) associated with it. Converting 
metadata from Alveo to UIMA is straightforward, as the expected metadata fields can be directly 
mapped to a customised UIMA type. Converting annotations from between the frameworks requires 
more work, due to the required use of a type system in UIMA. This is discussed further below. 
3.3 Conversion from Alveo to UIMA 
An annotation in Alveo consists of a beginning and ending offset, which can correspond to a character 
span for textual data or a time span for audio-visual data, a human-readable plain-text label indicating 
additional attributes of the data, and a type (a URI indicating the kind of entity to which the annotation 
corresponds, e.g. ?speaker-turn?, ?intonation? or ?part-of-speech?). Since UIMA also encodes text 
annotations as character spans, these can be straightforwardly converted into the UIMA CAS (audio-
visual data can be similarly treated, but we have focused on text in the current work).  
UIMA allows the definition of custom data types with specific fields for storing salient values. We 
add a generic Alveo annotation type (inheriting from the standard UIMA Annotation, which means it 
still has spans attached). The label of the annotation in Alveo is a non-decomposable string, so this 
top-level type has a field to store the label as a string.  
Handling annotation types requires more care. Types in Alveo are encoded as fully-qualified URIs, 
while types in UIMA are more strictly defined. In particular, UIMA has a notion of a fully-specified 
type system associated with each pipeline specification, including a full inheritance hierarchy up to a 
root type and features corresponding to attributes of each type.  There are also minor differences be-
tween the encoding of the type names  (instead of a URI, UIMA uses a ?big-endian? qualified type 
name similar to a Java package, such as com.example.nlp.Sentence). 
In addition, UIMA component specification requires specifying in advance the type system to 
which all annotations represented within UIMA must conform. All these requirements are handled in a 
type system generation phase triggered when the Alveo-UIMA collection reader is created.4 During 
this phase, the reader requests an enumeration of all known type URIs from the Alveo server. Since 
we have no explicit additional information about type inheritance from the URIs, we make as few as-
sumptions as possible by having all types inherit from a generic Alveo annotation parent type. The 
Alveo URIs are automatically converted to UIMA types names, essentially by reversing the compo-
nents of the domain name, and replacing the ?/? character in the path component of the URI with ?.?, 
with some extra handling of non-alphanumeric characters, giving conversions such as the following: 
 http://example.com/nlp/sentence  ? com.example.nlp.Sentence  
In addition, the type URI is stored as an attribute of the UIMA annotation, providing an explicit record 
of the original Alveo type. Because it is far more natural to work with UIMA types than comparing 
string values when manipulating and filtering annotations in a UIMA pipeline, the automatically gen-
erated type system is very beneficial. 
We note that there have been proposals to simplify the internal UIMA type system through the use 
of a generic Referent type which refers to an external source of domain semantics (D. Ferrucci, 
Lally, Verspoor, & Nyberg, 2009) This would be a good strategy to pursue here, so that the UIMA 
annotations could refer explicitly to the Alveo URIs as an external type system. However, it has been 
noted previously that this representational choice has consequences for the indexing and reasoning 
over the semantics of annotations in UIMA analysis engines (Verspoor, Baumgartner Jr, Roeder, & 
                                                 
4 If the framework users are using UIMA canonically, where the type systems are described by pre-existing XML descriptors, 
they can explicitly request generation of the appropriate XML. The wrapper was primarily developed using UIMAFit 
(https://uimafit.apache.org/uimafit), which allows a more dynamic approach. 
16
Hunter, 2009). The current version of UIMA does not provide direct support for this model and hence 
our strategy is in line with current technical practice. These proposals aim to not replicate the full ex-
ternal type structure within the UIMA type system definition, and this is the practice we follow here, 
although since Alveo does not currently have a strong notion of type hierarchy this was not a signifi-
cant consideration. 
3.4 Conversion from UIMA to Alveo 
The annotation upload component is implemented as a UIMA CAS Consumer, i.e. a component which 
accepts CASes and performs some action with them. To upload annotations, as noted above, we ex-
pect the supplied CAS to derive originally from the Alveo server, with annotations added to that CAS 
by UIMA processing components. The original metadata from Alveo is used to determine the URL of 
the original item, and otherwise ignored.  
The first step in annotation upload is to retrieve the original source document and remove from the 
set of annotations to be uploaded those annotations which already appear in the version found on the 
server. In addition, since processing pipelines may produce a wide variety of annotations which may 
not all be appropriate or relevant for uploading to Alveo, the annotation type must occur in a precon-
figured whitelist. Converting each annotation from UIMA to Alveo is in some ways the inverse of the 
operation described in the previous section, although there are some intricacies to the process.  
The character spans can be directly converted as before; again the type and label require more work. 
For type conversion, some sensible default behaviours are used. A configurable list of UIMA features 
are inspected on the UIMA annotations, and the first match found is used as the Alveo annotation 
type. This list of features naturally includes the default feature for storing the type URI, ensuring that 
annotations which derive from Alveo originally can be matched back to the original annotation. If no 
matches are found, a type URI is inferred from the fully-qualified UIMA annotation type name, using 
the inverse operation to that described above.  
Alveo annotations also have labels, as noted in the previous section. As with the annotation types, 
there is a similar list of UIMA feature names which can be used to populate the label attribute on the 
Alveo annotation, defaulting to the empty string if no feature name is found. If these strategies do not 
produce the desired behaviour when uploading, it is possible to customise them by implementing a 
Java interface. Alternatively, it is also possible to insert a custom UIMA component into the pipeline 
to convert the added UIMA annotations so that the Alveo conversion works as desired. 
4 Alveo User Acceptance Testing 
As it was a requirement of the funding agency for the project to provide evidence of User Acceptance 
Testing (UAT) and acceptance of the results of these tests by the project governing body (the Steering 
Committee), the project was organised from the start around these requirements, with all the partners 
bidding for participation in tests of specific components or versions of the system. A testing schedule 
was developed to accompany the system development, with the aim of gathering feedback from the 
project partners during development to provide targeted input for improvement. A sub-committee of 
the Steering Committee was designated to oversee the tests distributed to the testers, examine the re-
ports summarising the results of those tests and recommend acceptance. 
Alveo was designed and implemented in partnership with Intersect, a commercial software devel-
opment company specialised in the support of academic eResearch. This partnership afforded exten-
sive professional support during development, using the Agile process (Beck & al, 2001) as well as 
thorough regression testing and debugging. In other projects of this type, Intersect provided UAT or 
managed the UAT process in-house. For the Alveo project, since user testing was the main way in 
which the academic partners were involved in the project, UAT was organised by the academic part-
ners with technical support from Intersect. The central team at the lead institution oversaw the creation 
of the tests (see section 4.2), distributed the tests and monitored the results.    
4.1 The Alveo UAT process 
During development, Alveo was deployed incrementally on separate servers. While the Production 
Server remained stable between versions, the Staging 1 server was reserved for UAT and only updated 
17
when new functionalities were added; the Staging 2 Server was used for development and frequently 
updated. This rarely caused problems, even with the distributed nature of the testing process. 
 Each partner site engaged High Degree Researchers (HDRs), generally Masters and Doctoral 
students but also Post-Doctoral Fellows or project members, who had an interest in a particular do-
main or tool, or who could provide critical comments about the functionalities. Some Testers were 
Linguistics students with no computing background, some were Computer Science students with lim-
ited linguistic knowledge. At some sites, the Testers were Research Assistants who had worked on the 
tools or corpora contributed by their institutions, while others were the tool developers themselves. 
This variety of backgrounds and skills ensured coverage of the main domains and functionalities ex-
pected of the Alveo Virtual Lab. Some sites had undertaken to conduct large amounts of testing 
throughout the development, while other partners only chose to perform limited or more targeted test-
ing, with commitments varying from 10 to 200 hours. Over 30 Testers participated at various times 
during of the project and a total of more than 300 hours has been spent on testing during Phase I. 
4.2 Evolution of the tests 
For each version of the system during development (Prototype, Version 1, 2, and 3) a series of tests 
were developed and posted on a Google Form. To record the results, the Testers filled out a Google 
form which was monitored by the central team. The first tests developed were very directive, giving 
very specific instructions as to what actions the user was asked to perform and what results were ex-
pected for each action, as shown in Figure 3, one of the tests for Version 1. 
 
 
Figure 3: Test for Alveo Version 1 
 
Gradually the tests became more open-ended, giving less guidance and gathering more informative 
feedback. The latest round of testing asked Testers to log in and to carry out a small research task, as 
shown in Figure 4, the instructions for the open form testing of Version 3. 
 
 
Figure 4: Open form test for Alveo Version 3 
 
Some of the early tests, such as the one shown in Figure 3, have become tutorials provided on the Al-
veo web page and are now available as help from within the Virtual Lab. 
Test 2 - Browsing COOEE 
1. Login to the main website. 
2. In the list of facets on the left, click Corpus, this should show a list of corpus names. 
3. From the list of corpus names click cooee, the page should update to show 1354 results, listing the first 10 
matching items from the COOEE corpus. 
4. In the list of facets on the left click Created, this should show a list of decades. 
5. From the list of decades click 1870-1879, the page should update to show 61 results which are COOEE items 
from the 1870s. 
6. From the list of matching items, click on the first item, the page should update to show the details of this 
item. Verify that the Created date is within the 1870s and that the isPartOf field shows cooee. 
7. Scroll down to the bottom of the page where you should see links to the documents in this item.  Click on 
the document marked Text(Original), you should see the text of the document including some markup at 
the start and end of the file. 
8. Use the Back button in your browser to return to the item display page, click on the document marked 
Text(Plain), you should see the text of the document with no markup. 
9. Use the Back button in your browser to return to the item display page. 
10. When you are finished, click on the HCSvLab logo on the top left of the page to return to the home page and 
reset your search. 
Based on your own research interests and based on what you've seen of the HCS vLab platform, please try to make 
use of the virtual lab to carry out a small research task. Use the form below to tell us about what you tried to do: the 
collections and tools that you used, a description of your task, the outcomes and any problems that you faced. 
18
4.3 UIMA Testing 
In order to test the Alveo-UIMA implementation and provide an example of how it can be used, we 
created a tutorial application5 available from the Alveo github repository. This tutorial shows an ex-
ample of instantiating a UIMA CPE pipeline which reads documents from an Alveo item list, aug-
ments it with part-of-speech annotations and uploads them to the Alveo server. A UIMA pipeline con-
sists of a collection reader, and one or more CAS annotators. The UIMA tutorial pipeline includes the 
standard Collection Reader from Alveo-UIMA, a basic POS-tagging CAS annotator from DKPro-
Core,6 and the annotation uploading CAS annotator from Alveo-UIMA. An advanced version also 
demonstrates implementing an interface which remaps the POS tag types from those automatically-
derived from DKPro.outputs. 
5 Discussion 
5.1 Related Work 
There are several frameworks that have been developed to enable development and evaluation of text 
processing workflows, and UIMA has been used as the backbone for a few such frameworks due to its 
support for processing module interoperability. The Argo web service (Rak, Rowley, & Ananiadou, 
2012; Rak, Rowley, Carter, & Ananiadou, 2013) is a recent web application that enables development 
of UIMA-based text processing workflows through an on-line graphical interface. In contrast to Al-
veo, documents are uploaded to the system within an individual user space, and resulting annotations 
are not persisted outside of the UIMA data structures; although they can be serialised and stored for 
subsequent re-use in processing pipelines, or exported as RDF, they are not directly accessible within 
the framework itself. The repository contains a wide range of NLP components, e.g., modules to per-
form sentence splitting, POS tagging, parsing, and a number of information extraction tasks targeted to 
biomedical text. 
The U-Compare system (Kano et al., 2009; Kano, Dorado, McCrohon, Ananiadou, & Tsujii, 2010) 
also supports evaluation and performance comparison of UIMA-based automated annotation tools. It 
was designed with UIMA in mind from the ground up, enabling UIMA workflow creation and execu-
tion through a GUI. Therefore it assumes that all analysis of collections is performed with a set of 
UIMA components, and indeed provides a substantial number of such components in their repository, 
although other components can be added. The system is launched locally via Java Web Start; given 
recent changes to how browsers interact with Java, this no longer works reliably and off-line use (after 
downloading and installing) is likely necessary, although interaction with web service-based pro-
cessing components is possible (Kontonasios, Korkontzelos, Kolluru, & Ananiadou, 2011). 
A competing framework based on the GATE architecture (Cunningham, Maynard, Bontcheva, & 
Tablan, 2002) is the cloud-based AnnoMarket platform. This framework provides access to natural 
language processing (NLP) components, and a limited number of existing resources (one at the time of 
writing7, with the facility to upload user-specific data) on a fee-for-service basis (passing along costs 
of using the Amazon cloud services). Results of NLP studies of this data can be downloaded, or in-
dexed and made available for search. There are a wide array of annotation services and pre-configured 
pipelines available within the AnnoMarket that can be applied to a user?s document collection, either 
directly through the on-line application or via a web service API. 
5.2 Alternative strategies for UIMA integration with Alveo 
There were several possible places where the UIMA-Alveo translation layer could have been inserted, 
and indeed several possible architectures were considered for integrating UIMA with Alveo.  
Since Alveo was already working with the workflow engine Galaxy, one option was to create a 
compatibility layer to bridge UIMA with Galaxy, for instance to enable a pre-configured UIMA pipe-
line to be instantiated via a Galaxy wrapper. The technical details for accomplishing this were not im-
mediately obvious, and it was decided that this approach would add substantial complexity to the con-
version of annotations in the conversion layer. 
                                                 
5 http://github.com/Alveo/alveo-uima-tutorial 
6 https://code.google.com/p/dkpro-core-asl/ 
7 https://annomarket.com/dataSources, accessed 29 May 2014 
19
Another option that was considered was to allow for dynamic construction of UIMA workflows 
from UIMA components directly through the Alveo web interface. UIMA is a workflow engine analo-
gous to Galaxy, in that it enables dynamic configuration of pipelines from the available set of UIMA 
components set up in a given environment. In principle, therefore, it would be possible to enable spec-
ification, instantiation, and execution of UIMA pipelines from a set of UIMA components made avail-
able via Alveo. However, this would have required a substantial development effort specifically tar-
geted towards hosting UIMA components and manipulating UIMA pipelines; it was decided that a 
more general approach to integrating a broader range of tools was more appropriate for Alveo. Given 
the recent availability of the Argo web application, an Alveo/Argo integration could be considered that 
would enable users to create UIMA workflows with Argo but execute them from Alveo, and on doc-
uments or corpora stored in Alveo (Rak et al., 2012; Rak et al., 2013). The current web service-based 
architecture of the Alveo-UIMA integration lends itself well to this possibility. This could be explored 
in future work. 
The current implementation assumes that an Alveo user will have the knowledge to create and run 
UIMA pipelines externally to Alveo. A complementary strategy, possible now that the conversion lay-
er is in place, would be to make complete, pre-configured UIMA pipelines available as tools that can 
be applied to Alveo corpora/data. A number of such services, e.g. services aimed at annotation of text 
with one of a set of biomedically-relevant entity types (diseases, genes, chemicals) have been built 
(MacKinlay & Verspoor, 2013). Each such service is run as a separate UIMA instance that is accessed 
via a web service.  Text is passed in via the REST interface, handed over to the UIMA instance, pro-
cessed, and annotations are returned. This basic model could be replicated for a number of UIMA 
pipelines that do standard text-related processing (e.g. split sentences, perform part of speech tagging 
and parsing, etc.) such that text extracted from Alveo could be processed by the UIMA-based service 
and annotations returned. This approach has been criticised for its inability to be extended or adapted 
(Tablan, Bontcheva, Roberts, Cunningham, & Dimitrov, 2013) although it is suitable where pre-
packaged pipelines can be applied to accomplish tasks of broad interest. 
6 Conclusions 
The development of Alveo presented a number of challenges, some technical, such as the integration 
of UIMA with the platform, and others more logistic, such as the distributed nature of testing during 
development. In this paper, we described the solution and the choices we made for the implementation 
of UIMA pipelines, given the constraints regarding the organisation of items, documents and their as-
sociated annotations in Alveo. One of the conditions of success of such a project is that the platform 
be used by researchers for their own projects and on their own data. The organisation of the User Ac-
ceptance Testing, requiring partners to contribute during the development, and providing exposure to 
the tools and the datasets to a large group of diverse researchers is expected to lead to a much wider 
uptake of Alveo as a platform for HCS research in Australia. We plan to open it to users outside the 
original project partners during Phase II (2014-2016). We will also continue to explore further interac-
tions with complementary frameworks, such that the data and annotation storage available in Alveo 
can be enhanced via processing and tools from external services to supplement the functionality that is 
currently directly integrated. 
 
Acknowledgements 
We gratefully acknowledge funding from the Australian Government National eResearch Collabora-
tion Tools and Resources (NeCTAR) and thank all our collaborating partners in the Alveo Virtual La-
boratory project: University of Western Sydney, Macquarie University, RMIT, University of Mel-
bourne, Australian National University, University of Western Australia, University of Sydney, Uni-
versity of New England, University of Canberra, Flinders University, University of New South Wales, 
University of La Trobe, University of Tasmania, ASSTA, AusNC Inc., NICTA, and Intersect. 
 
 
 
20
References 
Arka, I. W. (2012). Developing a Deep Grammar of Indonesian within the ParGram Framework: Theoretical 
and Implementational Challenges Paper presented at the 26th Pacific Asia Conference on 
Language,Information and Computation.  
Beck, K., et al. (2001). Manifesto for Agile Software Development. http://agilemanifesto.org/ 
Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python - Analyzing Text with the 
Natural Language Toolkit: O'Reilly Media. 
Burnham, D., Estival, D., Fazio, S., Cox, F., Dale, R., Viethen, J., . . . Wagner, M. (2011). Building an audio-
visual corpus of Australian English: large corpus collection with an economical portable and 
replicable Black Box. Paper presented at the Interspeech 2011, Florence, Italy.  
Cabrera, D., Ferguson, S., & Schubert, E. (2007). 'Psysound3': Software for Acoustical and Psychoacoustical 
Analysis of Sound Recordings. Paper presented at the International Community on Auditory Display.  
Cassidy, S. (2010). An RDF Realisation of LAF in the DADA Annotation Server. Paper presented at the ISA-5, 
Hong Kong.  
Cassidy, S., Estival, D., Jones, T., Burnham, D., & Burghold, J. (2014). The Alveo Virtual Laboratory: A Web 
Based Repository API. Paper presented at the 9th Language Resources and Evaluation Conference 
(LREC 2014), Reykjavik, Iceland.  
Cassidy, S., & Harrington, J. (2000). Multi-level Annotation in the Emu Speech Database Management System. 
Speech Communication, 33, 61?77.  
Cassidy, S., Haugh, M., Peters, P., & Fallu, M. (2012). The Australian National Corpus : national infrastructure 
for language resources. Paper presented at the LREC.  
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. Paper 
presented at the 43rd Annual Meeting on Association for Computational Linguistics.  
Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V. (2002). GATE: A Framework and Graphical 
Development Environment for Robust NLP Tools and Applications. Paper presented at the 40th 
Anniversary Meeting of the Association for Computational Linguistics (ACL'02), Philadelphia, USA.  
Estival, D., Cassidy, S., Sefton, P., & Burnham, D. (2013). The Human Communication Science Virtual Lab. 
Paper presented at the 7th eResearch Australasia Conference, Brisbane, Australia.  
Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., . . . Welty, C. (2010). Building 
Watson: An Overview of the DeepQA Project. AI Magazine, 31(3), 59-79. doi: 
http://dx.doi.org/10.1609/aimag.v31i3.2303  
Ferrucci, D., & Lally, A. (2004). UIMA: an architectural approach to unstructured information processing in the 
corporate research environme. Natural Language Engineering, 10(3-4), 327-348.  
Ferrucci, D., Lally, A., Verspoor, K., & Nyberg, A. (2009). Unstructured Information Management Architecture 
(UIMA) Version 1.0 Oasis Standard. 
Goecke, R., & Millar, J. B. (2004). The Audio-Video Australian English Speech Data Corpus AVOZES. Paper 
presented at the 8th International Conference on Spoken Language Processing (INTERSPEECH 2004 - 
ICSLP), Jeju, Korea.  
Goecks, J., Nekrutenko, A., Taylor, J., & Team, T. G. (2010). Galaxy: a comprehensive approach for supporting 
accessible, reproducible, and transparent computational research in the life sciences. Genome Biology, 
11(8), R86.  
Kano, Y., Baumgartner, W. A., McCrohon, L., Ananiadou, S., Cohen, K. B., Hunter, L., & Tsujii, J. I. (2009). U-
Compare: share and compare text mining tools with UIMA. Bioinformatics, 25(15), 1997-1998.  
Kano, Y., Dorado, R., McCrohon, L., Ananiadou, S., & Tsujii, J. (2010). U-Compare: An Integrated Language 
Resource Evaluation Platform Including a Comprehensive UIMA Resource Library. Paper presented at 
the LREC. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.8878&rep=rep1&type=pdf 
Kontonasios, G., Korkontzelos, I., Kolluru, B., & Ananiadou, S. (2011). Adding text mining workflows as web 
services to the BioCatalogue. Paper presented at the Proceedings of the 4th International Workshop on 
Semantic Web Applications and Tools for the Life Sciences (SWAT4LS '11 ).  
MacKinlay, A., & Verspoor, K. (2013). A Web Service Annotation Framework for CTD Using the UIMA 
Concept Mapper. Paper presented at the Fourth BioCreative Challenge Evaluation Workshop. 
http://www.biocreative.org/media/store/files/2013/bc4_v1_14.pdf 
Musgrave, S., & Haugh, M. (2009). The AusNC Project: Plans, Progress and Implications for Language 
Technology. Paper presented at the ALTA 2009, Sydney.  
Rak, R., Rowley, A., & Ananiadou, S. (2012). Collaborative Development and Evaluation of Text-processing 
Workflows in a UIMA-supported Web-based Workbench. Paper presented at the LREC. 
http://www.lrec-conf.org/proceedings/lrec2012/pdf/960_Paper.pdf 
Rak, R., Rowley, A., Carter, J., & Ananiadou, S. (2013). Development and Analysis of NLP Pipelines in Argo. 
Paper presented at the ACL. http://aclweb.org/anthology//P/P13/P13-4020.pdf 21
Shaw, J. A., & Gafos, A. I. (2010). Quantitative evaluation of competing syllable parses. Paper presented at the 
11th Meeting of the Association for Computational Linguistics. Special Interest Group on 
Computational Morphology and Phonology, Uppsala, Sweden.  
Tablan, V., Bontcheva, K., Roberts, I., Cunningham, H., & Dimitrov, M. (2013). AnnoMarket: An Open Cloud 
Platform for NLP. Paper presented at the 51st Annual Meeting of the Association for Computational 
Linguistics (ACL 2013), Sofia, Bulgaria.  
Thieberger, N., Barwick, L., Billington, R., & Vaughan, J. (Eds.). (2011). Sustainable data from digital 
research: Humanities perspectives on digital scholarship. A PARDISEC Conference: Custom Book 
Centre. http://ses.library.usyd.edu.au/handle/2123/7890. 
Verspoor, K., Baumgartner Jr, W., Roeder, C., & Hunter, L. (2009). Abstracting the types away from a UIMA 
type system From Form to Meaning: Processing Texts Automatically (pp. 249-256). 
 
22
