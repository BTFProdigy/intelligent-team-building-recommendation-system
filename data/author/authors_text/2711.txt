A System for Searching and Browsing Spoken Communications
Lee Begeja
Bernard Renger
Murat Saraclar
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
{lee, renger, murat}
@research.att.com
David Gibbon
Zhu Liu
Behzad Shahraray
AT&T Labs ? Research
200 Laurel Ave S
Middletown, NJ 07748
{dcg, zliu, behzad}
@research.att.com
Abstract
As the amount of spoken communications ac-
cessible by computers increases, searching and
browsing is becoming crucial for utilizing such
material for gathering information. It is desir-
able for multimedia content analysis systems
to handle various formats of data and to serve
varying user needs while presenting a simple
and consistent user interface. In this paper,
we present a research system for searching and
browsing spoken communications. The system
uses core technologies such as speaker segmen-
tation, automatic speech recognition, transcrip-
tion alignment, keyword extraction and speech
indexing and retrieval to make spoken commu-
nications easy to navigate. The main focus is
on telephone conversations and teleconferences
with comparisons to broadcast news.
1 Introduction
Archiving and organizing multimedia communications
for easy user access is becoming more important as such
information sources are becoming available in amounts
that can easily overwhelm a user. As storage and ac-
cess become cheaper, the types of multimedia communi-
cations are also becoming more diverse. Therefore, it is
necessary for multimedia content analysis and navigation
systems to handle various forms of data.
In this paper we present SpeechLogger, a research sys-
tem for searching and browsing spoken communications,
or the spoken component of multimedia communications.
In general, the information contained in a spoken com-
munication consists of more than just words. Our goal is
to make use of all the information within a spoken com-
munication. Our system uses automatic speech recogni-
tion (ASR) to convert speech into a format which makes
word and phonetic searching of the material possible. It
also uses speaker segmentation to aid navigation.
We are interested in a wide range of spoken communi-
cations with different characteristics, including broadcast
material, lectures, meetings, interviews, telephone con-
versations, call center recordings, and teleconferences.
Each of these communication types presents interesting
opportunities, requirements and challenges. For example,
lectures might have accompanying material that can aid
ASR and navigation. Prior knowledge about the speakers
and the topic may be available for meetings. Call center
recordings may be analyzed to create aggregate reports.
Spoken document retrieval (SDR) for Broadcast News
type of content has been well studied and there are many
research and commercial systems. There has also been
some interest in the Voicemail domain (Hirschberg et al,
2001) which consists of typically short duration human-
to-machine messages. Our focus here is on telephone
conversations and teleconferences with comparisons to
broadcast news.
The paper is organized as follows. In Section 2, we
motivate our approach by describing the user needs un-
der various conditions. Then we describe our system in
Section 3, giving the details of various components. Ex-
perimental results for some components are given in Sec-
tion 4. Finally, in Section 5 we present a summary.
2 User Needs
We are primarily interested in situations in which a per-
son needs to gather information from audio data but the
quality of that data is not always sufficient to produce
good ASR results. In the case of telephone conversations,
the information gatherer needs to know who was on the
call, how long the call was, what was said, a summary of
the call, the ability to listen to any part of the call based
on search parameters that s/he specifies, etc. Our users
want to be able to scan a database of many calls, across a
long period of time to look for specific phrases, speakers,
or patterns of speech.
In many cases, it is difficult to gather this type of infor-
mation from teleconference calls since the audio quality
is poor because of speaker phones, cell phones and line
noise. All of these combine to lower ASR results to a
point where the text of the call is not fully representative
of the conversation. Thus, using standard information re-
trieval techniques may not provide sufficient information
to the user. We focus on the navigation aspect of infor-
mation gathering with the goal of compensating for lower
ASR accuracy by presenting user interface elements rel-
evant to the specific task at hand (Stark et al, 2000).
Rather than looking at the recorded conversation as
merely audio information, we view it as a source of lin-
guistic information to which we can apply information
retrieval and data mining techniques. We use all avail-
able metadata to enhance the search and the presentation.
We wanted to have a set of interface elements that
would be useful no matter what the ASR accuracy was.
The main interface elements are:
? Timeline with tick marks indicates search hits within
the spoken document which allows for many search
results to be displayed without overwhelming the
user. This is particularly useful for cases where there
are many false positives.
? Keyword extraction summarizes a given communi-
cation, enables differentiation among a collection of
many spoken documents, and detects subtopics in a
large spoken document.
? Speaker segmentation and speaker identification
separate a long spoken document into inherently
useful pieces.
? Lattice search and phoneme search expand the pos-
sible search space.
In this paper we examine three classes of spoken doc-
uments and consider what tasks a user might want to per-
form on them.
? Broadcast News - excellent ASR conditions, one
speaker at a time, good audio quality and gener-
ally a good speaker. Task involves primarily inter-
document navigation. User needs to search text for
information with metadata possibly used to enhance
the search.
? Telephone Conversations - fair ASR conditions, two
speakers, decent quality audio. User needs to search
text but also wants speaker identification and some
classification (call type, urgency, importance).
? Teleconferences - poor ASR conditions, multiple
speakers, mixed to poor audio quality. Most time
is spent in intra-document navigation. User needs to
navigate through the calls and find relevant informa-
tion in the audio.
3 System Description
The system overview is shown in Figure 1. Our sys-
tem is flexible enough to support various forms of live
(via a VoiceXML Gateway) or prerecorded spoken com-
munications including the three classes of spoken docu-
ments discussed above. It can record the audio via tele-
phone for two-party or multi-party calls. Alternatively,
the system can support prerecorded audio input from var-
ious sources including telephone conversations or video
content in which case the audio is extracted from the
video. Once various speech processing techniques are ap-
plied and the speech is indexed, it is possible to search
and browse the audio content. Our system is scalable
and supports open source/industry standard components
(J2EE, VXML, XML, Microsoft SAMI, Microsoft Media
Player). It is also flexible enough to support other forms
of audio as input or to support new speech processing
techniques as they become available. The system was de-
signed with modularity in mind. For instance, it should
be possible to add a speaker identification module to the
processing.
Figure 1: System Overview
Once a new audio recording is available on the
File Server, the following processing steps can begin:
speaker segmentation, speech recognition, transcription
alignment, keyword extraction, audio compression, and
speech indexing. Each step will be described in more de-
tail below. We attempt to distinguish the different speak-
ers from each other in the speaker segmentation compo-
nent. The speech recognition component converts the au-
dio into a word or phone based representation including
alternative hypotheses in the form of a lattice. If a tran-
script is available, the transcript can be synchronized (or
aligned) in time with the speech recognition output. The
keyword extraction component generates the most salient
words found in the speech recognition output (one-best
word) or transcript (if available) and can be used to de-
termine the nature of the spoken communications. The
audio compression component compresses the audio file
and creates an MP3 audio file which is copied to the Me-
dia Server. The final step in the processing is text and
lattice indexing. This includes creating indices based on
one-best word and one-best phone strings or word and
phone lattices.
After processing, the user can search and browse the
audio using either the text index or the lattice index. The
audio is played back via media streaming. Alternatively,
the user can playback the audio file over the phone using
the VoiceGenie VoiceXML Gateway.
3.1 Speaker Segmentation
Speaker-based segmentation of multi-speaker audio data
has received considerable attention in recent years. Ap-
plications that have been considered include: indexing
archived recorded spoken documents by speaker to facil-
itate browsing and retrieval of desired portions; tagging
speaker specific portions of data to be used for adapt-
ing speech models in order to improve the quality of
automatic speech recognition transcriptions, and track-
ing speaker specific segments in audio streams to aid in
surveillance applications. In our system, speaker segmen-
tation is used for more effective visualization of the audio
document and speaker-based audio playback.
Figure 2 gives an overview of the speaker segmenta-
tion algorithm we developed. It consists of two steps:
preprocessing and iterative speaker segmentation. Dur-
ing the preprocessing step, the input audio stream is seg-
mented into frames and acoustic features are computed
for each frame. The features we extracted are energy, 12
Mel-frequency cepstral coefficients (MFCC), pitch, and
the first and second order temporal derivatives. Then,
all speaker boundary candidates are located, which in-
clude silent frames and frames with minimum energy in
a window of neighboring frames. The preprocessing step
generates a set of over-segmented audio segments, whose
durations may be as short as a fraction of a second to as
long as a couple of seconds.
The iterative speaker segmentation step, as depicted in
the bigger dotted rectangle in Figure 2, detects all seg-
ments of each speaker in an iterative way and then marks
the boundaries where speakers change. At the beginning,
all segments produced by the preprocessing step are un-
labeled. Assuming that the features within each segment
follow a Gaussian distribution, we compute the distances
between each pair of segments using the Kullback Leibler
distance (KLD) (Cover and Thomas, 1991). Here, we just
consider features extracted from voiced frames since only
voiced frames have pitch information. Based on the seg-
ment distance matrix, a hierarchical agglomerative clus-
tering (HAC) (Jain and Dubes, 1988) algorithm is applied
Figure 2: Overview of the Speaker Segmentation Algo-
rithm.
to all unlabeled segments. The biggest cluster will be hy-
pothesized as the set of segments for a new speaker and
the rest of the segments will be considered as background
audio. Accordingly, each unlabeled segment is labeled as
either the target speaker or background. Then an embed-
ded speaker segment refinement substep is activated to
iteratively refine the segments of the target speaker.
The refinement substep is depicted in the smaller dot-
ted rectangle in Figure 2. For each iteration, two Gaus-
sian mixture models (GMM) are built based on current
segment labels, one for the target speaker, one for back-
ground audio. Then all segments are relabeled as either
the target speaker or background audio using the maxi-
mum likelihood method based on the two GMM models.
If the set of segments for the target speaker converges
or the refinement iteration number reaches its maximum,
the refinement iteration stops. Otherwise, a new itera-
tion starts. Before the refinement substep terminates, it
assigns a new speaker label for all segments of the tar-
get speaker, and sets the background audio as unlabeled.
Then the iterative speaker segmentation step needs to test
for more speakers or needs to stop. The termination cri-
teria could be the given number of speakers (or major
speakers) in an audio document, the percentage of unla-
beled segments to the number of all segments, or the max-
imum distance among all pairs of unlabeled segments. If
any of the criteria are met, the speaker segmentation algo-
rithm merges all adjacent segments if their speaker labels
are the same, and then outputs a list of audio segments
with corresponding speaker labels.
Obviously, one advantage of our speaker segmentation
method is that the speaker labels are also extracted. Al-
though the real speaker identities are not available, the
Figure 3: Presentation of Speaker Segmentation Results.
labels are very useful for presenting, indexing, and re-
trieving audio documents. For more detailed description
of the speaker segmentation algorithm, please refer to
Rosenberg et al (2002).
Figure 3 illustrates a graphic interface for presenting
the speaker segmentation results. The audio stream is
shown in colored blocks along a timeline which goes
from top to bottom, and from left to right. Color is used to
differentiate the speaker labels. There are two layers for
each line: the bottom layer shows the manually labeled
speaker segments and the top layer displays the automat-
ically generated segments. This allows the segmentation
performance to be clearly observed.
3.2 Automatic Speech Recognition
We use two different state-of-the-art HMM based large
vocabulary continuous speech recognition (LVCSR) sys-
tems for telephone and microphone recordings. In both
cases the front-end uses 9 frames of 12 MFCC compo-
nents and energy summarized into a feature vector via
linear discriminant analysis. The acoustic models consist
of decision tree state clustered triphones and the output
distributions are mixtures of Gaussians. The models are
discriminatively trained using maximum mutual informa-
tion estimation. The language models are pruned backoff
trigram models.
For narrow-band telephone recordings we use the first
pass of the Switchboard evaluation system developed
by Ljolje et al (2002). The calls are automatically seg-
mented prior to ASR. The acoustic models are trained on
265 hours of speech. The recognition vocabulary of the
system has 45K words.
For wide-band recordings, we use the real-time
Broadcast News transcription system developed by
Saraclar et al (2002). The acoustic models are trained on
140 hours of speech. The language models are estimated
on a mixture of newspaper text, closed captions and high-
accuracy transcriptions from LDC. Since the system was
designed for SDR, the recognition vocabulary of the sys-
tem has over 200K words.
Both systems use the same Finite State Machine (FSM)
based LVCSR decoder (Allauzen et al, 2003). The out-
put of the ASR system is represented as a FSM and may
be in the form of a one-best hypothesis string or a lattice
of alternate hypotheses. The lattices are normalized so
that the probability of the set of all paths leading from
any state to the final state is 1. The labels on the arcs of
the FSM may be words or phones and the conversion be-
tween the two can easily be done using FSM composition
using the AT&T FSM Library (Mohri et al, 1997). The
costs on the arcs of the FSM are negative log likelihoods.
Additionally, timing information can also be present in
the output.
3.3 Alignment with Transcripts
Manual transcriptions of spoken communications are
available for certain application domains such as medical
diagnosis, legal depositions, television and radio broad-
casts. Most audio and video teleconferencing providers
offer transcription as an optional service. In these
cases, we can take advantage of this additional informa-
tion to create high quality multimedia representations of
the archived spoken communications using parallel text
alignment techniques (Gibbon, 1998). The obvious ad-
vantage is increased retrieval accuracy due to the lower
word error rate (manual transcriptions are seldom com-
pletely error free.) What is more compelling, however, is
that we can construct much more evolved user interfaces
for browsing speech by leveraging the fact that the tran-
scription is by its nature readable whereas the one-best
hypothesis from ASR is typically useful only in small
segments to establish context for a search term occur-
rence.
There are several methods for aligning text with
speech. We use dynamic programming techniques to
maximize the number or word correspondences between
the manual transcription and the one-best ASR word hy-
pothesis. For most applications, finding the start and end
times of the transcript sentences is sufficient; but we do
alignment at the word level and then derive the sentence
alignment from that. In cases where the first or last word
of a sentence is not recognized, we expand to the near-
est recognized word to avoid cropping even though we
may include small segments from neighboring sentences
during playback. The accuracy of the resulting align-
ment is directly related to the ASR word error rate; more
precisely it can be thought of as a sentence error rate
where we impose a minimum percentage of correspond-
ing words per sentence (typically 20%) before declar-
ing a sentence a match to avoid noise words triggering
false matches. For sentences without correspondences,
we must fall back to deriving the timings from the near-
est neighboring sentences with correspondences.
Figure 4: Illustration of Keyword Extraction.
3.4 Keyword Extraction
Playing back a spoken document or linearly skimming
the corresponding text transcript, either from automatic
speech recognition or manual transcription, is not an ef-
ficient way for a user to grasp the central topics of the
document within a short period of time. A list of repre-
sentative keywords, which serve as a dense summary for a
document, can effectively convey the essence of the docu-
ment to the user. The keywords have been widely used for
indexing and retrieval of documents in large databases. In
our system, we extract a list of keywords for each audio
document based on its transcript (ASR or manual tran-
script).
There are different ways to automatically extract key-
words for a text document within a corpus. A popular
approach is to select keywords that frequently occur in
one document but do not frequently occur in other doc-
uments based on the term frequency - inverse document
frequency (TF-IDF) feature. Our task is slightly differ-
ent. We are interested in choosing keywords for a sin-
gle document, independent of the remaining documents
in the database. Accordingly, we adopt a different fea-
ture, which is term frequency - inverse term probability
(TF-ITP) to serve our purpose. The term probability mea-
sures the probability that a term may appear in a general
document and it is a language dependent characteristic.
Assuming that a term Tk occurs tfk times in a docu-
ment, and its term probability is tpk, the TF-ITP of Tk is
defined as wk = tfk/tpk.
Figure 4 illustrates the keyword extraction method that
we have developed. For the transcript of a given doc-
ument, we first apply the Porter stemming algorithm
(Porter, 1980) to remove word variations. Then, the stop
words, which are common words that have no impact on
the document content (also called noise words), are re-
moved. Here we use two lists of noise words, one for gen-
eral purposes, which apply to all varieties of documents,
and one for specific domains, which can be customized
by the user when prior knowledge about the document is
available. For each remaining term in the document, a
value of TF-ITP is calculated. A vocabulary is created
based on the transcripts of 600 hours of broadcast news
data and corresponding term probabilities are estimated
using the same corpus. If a term in the document is not
in the vocabulary, and its term frequency is more than
2, then a default term probability value tpd will be used.
The tpd we use is the minimum term probability in the
vocabulary. After we get a list of terms and their TF-ITP
values, we sort the terms based on their TF-ITP values,
such that the most representative terms (highest TF-ITP
values) are on the top of the list. Depending on certain
criteria, for example, the number of keywords desired or
the minimum TF-ITP value required, a list of keywords
can be chosen from the top of the term list. In our sys-
tem, we choose the top ten terms as the keywords for a
document.
3.5 Speech Indexing and Retrieval
Two different indexing and retrieval modules are uti-
lized depending on the type of ASR output. In
the case of one-best word or phone strings, we use
an off-the-shelf text-based index server called Lucene
(http://jakarta.apache.org/lucene). In the case of word
and phone lattices, we use the method described in
Saraclar and Sproat (2004). Here we give a brief descrip-
tion of the latter.
The lattice output is a compact representation of likely
alternative hypotheses of an ASR system. Each path in
the lattice corresponds to a word (or phone) string and
has a probability attached to it. The expected count for
a substring can be defined as the sum of the probabilities
of all paths which contain that substring. Lattice based
retrieval makes the system more robust to recognition er-
rors, whereas phonetic search allows for retrieving words
that are not in the vocabulary of the recognizer.
The lattice index is similar to a standard inverted index
but contains enough information to compute the expected
count of an arbitrary substring for each lattice. This can
be achieved by storing a set of index files, one for each
label (word or phone) l. For each arc labeled with l in a
lattice, the index file for l records the lattice number, the
previous and next states of the arc, along with the prob-
ability mass leading to the previous state of the arc and
the probability of the arc itself. For a lattice, which is
normalized so that the probability of the set of all paths
leading from any state to the final state is 1, the poste-
rior probability of an arc is given by the multiplication of
the probability mass leading to the previous state and the
probability of the arc itself. The expected count of a label
given a lattice is equal to the sum of the posterior proba-
bilities of all arcs in the index for that label with the same
lattice number.
To search for a multi-label expression (e.g., a multi-
word phrase) w1w2 . . . wn we seek on each label in the
expression and then for each (wi, wi+1) join the next
states of wi with the matching previous states of wi+1.
In this way, we retrieve just those path segments in each
lattice that match the entire multi-label expression. The
probability of each match is defined as the multiplication
of the probability mass leading to the previous state of
the first arc and the probabilities of all the arcs in the path
segment. The expected count of a multi-label expression
for the lattice is computed as above.
The answer to a query contains an audio segment only
if the expected count of the query for the lattice corre-
sponding to that audio segment is higher than a threshold.
3.6 User Interface
The user interface description will apply for the three
types of spoken communications (Telephone Conversa-
tions, Teleconferences, Broadcast News) although the au-
dio and speaker quality do vary for each of these types
of spoken communications. Once the user has found the
desired call (or spoken communication) using one of the
retrieval modules (one-best word, one-best phone string,
word lattice, phone lattice, or both word and phone lat-
tice), the user can navigate the call using the user inter-
face elements described below.
For the one-best word index, the Web page in Fig-
ure 5 shows the user interface for searching, browsing,
and playing back this call. The user can browse the call
at any time by clicking on the timeline to start playing at
that location on the timeline. The compressed audio file
(MP3) that was created during the processing would be
streamed to the user. The user can at any time either enter
a word (or word phrase) in the Search box or use one of
the common keywords generated during the keyword ex-
traction process. The text index would be queried and the
results of the search would be shown. The timeline plot
at the top would show all the hits or occurrences of the
word as thin tick marks. The list of hits would be found
under the keyword list. In this case, the word ?chap-
ter? was found 4 times and the time stamps are shown.
The time stamps come from the results of the automatic
speech recognition process when the one-best words and
time stamps were generated. The search term ?chapter?
is shown in bold with 5 context words on either side. The
user can click on any of these 4 hits to start playing where
the hit occurred. The solid band in the timeline indicates
the current position of the audio being played back. The
entire call, in this case, is 9:59 minutes long and the au-
dio is playing at the beginning of the fourth hit at 5:20
minutes. As part of the processing, caption data is gener-
ated in Microsoft?s SAMI (Synchronized Accessible Me-
dia Interchange) format from the one-best word output in
order to show caption text during the playback. The cap-
tion text under the timeline will be updated as the audio
is played. At this point in the call, the caption text is ?but
i did any chapter in a?. This caption option can be dis-
Figure 5: User Interface for ASR One-Best Word Search.
Figure 6: User Interface for Lattice Search.
abled by clicking on the CC icon and can be enabled by
clicking on the CC icon again. The user can also speed
up or slow down the playback at any time by using the
?Speed? button. The speed will toggle from 50% (slow)
to 100% to 150% (fast) to 200% (faster) and then start
over at 50%. The speed, which is currently ?fast?, will be
shown next to the current time above the ?Stop? button.
This allows the user to more quickly peruse the audio file.
A similar Web page in Figure 6 shows the user inter-
face for searching a lattice index. Note that for the same
audio file (or call) and the same search term ?chapter?,
the results of the query show 6 hits compared to the 4
hits in the text index in Figure 5. In this particular case,
the manual transcript does indeed contain these 6 occur-
rences of the word ?chapter?. The search terms were
found in audio segments, which is why the time of the
hit is a time range. The information in brackets is the ex-
pected count and can exceed 1.0 if the search term occurs
more than once in the audio segment. The time range is
reflected in the timeline since the thin tick marks have
been replaced with colored segments. The colors of the
segments correspond to the colors of the hits in the list.
The darker the color, the higher the count and the lighter
the color, the lower the count. Finally, the search can be
refined by altering the threshold using the ?Better Hits?
and ?More Hits? buttons. In this example, the threshold
is set to 0.2 as can be seen under the timeline. If the
user clicks on the ?Better Hits? button, the threshold is
increased so that only better hits are shown. If the ?More
Hits? button is used, the threshold is decreased so more
hits are shown although the hits may not be as good. The
lattice index only returns hits where each hit has a count
above the threshold.
The lattice search user interface allows the user to more
easily find what the user wants and has additional controls
(threshold adjustments) and visual feedback (colored seg-
ments/hits) that are not possible for the text search user
interface.
4 Experimental Results
We used three different corpora to assess the effectiveness
of different techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types.
The second corpus is the Switchboard corpus consist-
ing of two-party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types.
The third corpus is named Teleconference since it con-
sists of multi-party teleconferences on various topics. A
test set of six teleconferences (about 3.5 hours) was tran-
scribed. It contains 31106 word tokens and 2779 word
types. Calls are automatically segmented into a total of
1157 segments prior to ASR.
4.1 Speaker Segmentation
The performance of the speaker segmentation is evalu-
ated as follows. For an audio document, assume there
are N true boundaries, and the algorithm generates M
speaker boundaries. If a detected boundary is within
1 second of a true boundary, it is a correctly detected
boundary, otherwise it is a falsely detected boundary. Let
C denote the number of correctly detected boundaries,
the recall and precision of the boundary detection can be
computed as R = C/N and P = C/M , respectively.
We can combine these two values using the F-measure
F = 2 ? P ? R/(P + R) to measure the speaker seg-
mentation performance.
We evaluated the developed method on three different
types of audio documents: Broadcast News recordings
(16KHz sampling rate, 16 bits/sample), two-party tele-
phone conversations (8KHz, 16bps), and multi-party tele-
conference recordings (8KHz, 16bps). Due to the high
audio quality and well controlled structure of the broad-
cast news program, the achieved F-measure for broadcast
news data is 91%. Teleconference data has the worst au-
dio quality given the various devices (headset, speaker-
phone, etc.) used and different channels (wired and wire-
less) involved. There are also a lot of spontaneous speech
segments less than 1 second long, for example, ?Yes?,
?No?, ?Uh?, etc. These characteristics make the telecon-
ference data the most challenging one to segment. The
F-measure we achieved for this type of data is 70%. The
F-measure for two-party telephone conversations is in the
middle at 82%.
4.2 Automatic Speech Recognition
For evaluating ASR performance, we use the standard
word error rate (WER) as our metric. Since we are in-
terested in retrieval, we use OOV (Out Of Vocabulary)
rate by type to measure the OOV word characteristics.
In Table 1, we present the ASR performance on these
three tasks as well as the OOV Rate by type of the cor-
pora. It is important to note that the recognition vocabu-
lary for the Switchboard and Teleconference tasks are the
same and no data from the Teleconference task was used
while building the ASR systems.
Task WER OOV Rate by Type
Broadcast News ?20% 0.6%
Switchboard ?40% 6%
Teleconference ?50% 12%
Table 1: Word Error Rate and OOV Rate Comparison.
4.3 Retrieval
Our task is to retrieve the audio segments in which the
user query appears. For evaluating retrieval performance,
we use precision and recall with respect to manual tran-
scriptions. Let C(q) be the number of times the query
q is found correctly, M(q) be the number of answers
to the query q, and N(q) be the number of times q is
found in the reference. We compute precision and re-
call rates for each query as P (q) = C(q)/M(q) and
R(q) = C(q)/N(q). We report the average of these
quantities over a set of queries Q, P =
?
q?Q P (q)/|Q|
and R =
?
q?Q R(q)/|Q|. The set of queries Q includes
all the words seen in the reference except for a stop list of
the 100 most common words.
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve. In addition to individual precision-recall
values we also compute the F-measure defined above and
report the maximum F-measure (maxF) to summarize the
information in a precision-recall curve.
In Table 2, a comparison of the maximum F-measure
(maxF) is given for various corpora. Using word lattices
yields a relative gain of 3-5% in maxF over using one-
best word hypotheses. Using both word and phone lat-
tices, the relative gain over the baseline increases to 8-
12%. In this approach, we first search the word index;
if no matches are found then we search the phone index.
This allows the system to return matches even if the user
query is not in the ASR vocabulary.
Task System
1-best W Lats W+P Lats
Broadcast News 84.0 84.8 86.0
Switchboard 57.1 58.4 60.5
Teleconference 47.4 50.3 52.8
Table 2: Maximum F-measure Comparison.
In Figure 7, we present the precision-recall curves.
The gain from using better techniques utilizing word
and phone lattices increases as retrieval performance gets
worse.
0 20 40 60 80 1000
20
40
60
80
100
Precision
Reca
ll
Teleconferences
SwitchboardBroadcast News
1?best Word HypothesesWord LatticesWord and Phone Lattices
Figure 7: Precision vs Recall Comparison.
5 Summary
We presented a system for searching and browsing spo-
ken communications. The system is flexible enough to
support various forms of spoken communications. In this
paper, our focus was on telephone conversations and tele-
conferences. We also presented experimental results for
the speaker segmentation, ASR and retrieval components
of the system.
Acknowledgments
We would like to thank Richard Sproat for useful dis-
cussions and for making his lattice indexing software
(lctools) available for our system.
References
C. Allauzen, M. Mohri, and M. Riley.
2003. DCD Library ? Decoder Library.
http://www.research.att.com/sw/tools/dcd.
T. M. Cover and J. A. Thomas. 1991. Elements of Infor-
mation Theory. John Wiley & Sons.
D. Gibbon. 1998. Generating hypermedia documents
from transcriptions of television programs using paral-
lel text alignment. In B. Furht, editor, Handbook of In-
ternet and Multimedia Systems and Applications. CR-
CPress.
J. Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,
A. Rosenberg, L. Stark, L. Stead, S. Whittaker, and
G. Zamchick. 2001. Scanmail: Browsing and search-
ing speech data by content. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), Aalborg, Denmark.
A. K. Jain and R. C. Dubes. 1988. Algorithms for Clus-
tering Data. Prentice-Hall.
A. Ljolje, M. Saraclar, M. Bacchiani, M. Collins, and
B. Roark. 2002. The AT&T RT-02 STT system. In
Proc. RT02 Workshop, Vienna, Virginia.
M. Mohri, F. C. N. Pereira, and M. Riley. 1997.
AT&T FSM Library ? Finite-State Machine Library.
http://www.research.att.com/sw/tools/fsm.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
A. Rosenberg, A. Gorin, Z. Liu, and S. Parthasarathy.
2002. Unsupervised speaker segmentation of tele-
phone conversations. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), Denver, Colorado, USA.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
M. Saraclar, M. Riley, E. Bocchieri, and V. Goffin. 2002.
Towards automatic closed captioning: Low latency
real time broadcast news transcription. In Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), Denver, Colorado, USA.
L. Stark, S. Whittaker, and J. Hirschberg. 2000. ASR
satisficing: the effects of ASR accuracy on speech re-
trieval. In Proceedings of the International Conference
on Spoken Language Processing (ICSLP).
Interactive Machine Learning Techniques for Improving SLU Models 
 
Lee Begeja  
Bernard Renger 
AT&T Labs-Research 
180 Park Ave 
Florham Park, NJ 07932 
{lee,renger} 
@research.att.com 
David Gibbon  
Zhu Liu 
Behzad Shahraray 
AT&T Labs-Research 
200 Laurel Ave S 
 Middletown, NJ  07748 
{dcg, zliu, behzad} 
@research.att.com 
 
 
 
Abstract 
Spoken language understanding is a critical 
component of automated customer service ap-
plications.  Creating effective SLU models is 
inherently a data driven process and requires 
considerable human intervention.  We de-
scribe an interactive system for speech data 
mining.  Using data visualization and interac-
tive speech analysis, our system allows a User 
Experience (UE) expert to browse and under-
stand data variability quickly.  Supervised 
machine learning techniques are used to cap-
ture knowledge from the UE expert.  This cap-
tured knowledge is used to build an initial 
SLU model, an annotation guide, and a train-
ing and testing system for the labelers.  Our 
goal is to shorten the time to market by in-
creasing the efficiency of the process and to 
improve the quality of the call types, the call 
routing, and the overall application. 
1 Introduction 
The use of spoken dialogue systems to automate ser-
vices in call centers is continually expanding.  In one 
such system, unconstrained speech recognition is used 
in a limited domain to direct call traffic in customer call 
centers (Gorin et al 1997).  The challenge in this envi-
ronment is not only the accuracy of the speech recogni-
tion but more importantly, the knowledge and 
understanding of how the customer request is mapped to 
the business requirement.   
The first step of the process is to collect utterances 
from customers, which are transcribed.  This gives us a 
baseline for the types of requests (namely, the user in-
tents) that customers make when they call a client.  A 
UE expert working with the business customer uses 
either a spreadsheet or a text document to classify these 
calls into call types.  For example, 
? ?I want a refund?   REFUND 
? ?May I speak with an operator?  
GET_CUSTOMER_REP 
The end result of this process is a document, the an-
notation guide, that describes the types of calls that may 
be received and how to classify them.  This guide is 
then given to a group of ?labelers? who are trained and 
given thousands of utterances to label.  The utterances 
and labels are then used to create the SLU model for the 
application.  The call flow which maps the call types to 
routing destinations (dialog trajectory) is finalized and 
the development of the dialogue application begins.  
After the field tests, the results are given to the UE ex-
pert, who then will refine the call types, create a new 
annotation guide, retrain the labelers, redo the labels and 
create new ones from new data and rebuild the SLU 
model.   
Previously, this knowledge was only captured in a 
document and was not formalized until the SLU model 
was generated.  Our goal in creating our system is not 
only to give the UE expert tools to classify the calls, but 
to capture and formalize the knowledge that is gained in 
the process and to pass it on to the labelers.  We can 
thus automatically generate training instances and test-
ing scenarios for the labelers, thereby creating more 
consistent results.  Additionally, we can use the SLU 
model generated by our system to ?pre-label? the utter-
ances.  The labelers can then view these ?pre-labeled? 
utterances and either agree or disagree with the gener-
ated labels.  This should speed up the overall labeling 
process. 
More importantly, this knowledge capture will en-
able the UE expert to generate and test a SLU model as 
part of the process of creating the call types for the 
speech data.  The feedback from this initial SLU test 
allows the UE expert to refine the call types and to im-
prove them without having to train a group of labelers 
and to run a live test with customers.  This results in an 
improved SLU model and makes it easier to find prob-
lems before deployment, thus saving time and money. 
 Figure 1. System Diagram 
 
At the same time, the process is more efficient due to 
the increased uniformity in the way different UE experts 
classify calls into call type labels. 
We will describe Annomate, an interactive system 
for speech data mining.  In this system, we employ sev-
eral machine learning techniques such as clustering and 
relevance feedback in concert with standard text search-
ing methods.  We focus on interactive dynamic tech-
niques and visualization of the data in the context of the 
application. 
 The paper is organized as follows.  The overview of 
the system is presented in Section 2.  Section 3 briefly 
discusses the different components of the system.  Some 
results are given in Section 4.  Finally, in Sections 5 and 
6, we give conclusions and point to some future direc-
tions. 
2 System Overview 
In this section, we will give a system overview and 
show how automation has sped up and improved the 
existing process.  The UE expert no longer needs to 
keep track of utterances or call type labels in spread-
sheets. Our system allows the UE expert to more easily 
and efficiently label collected utterances in order to 
automatically build a SLU model and an electronic an-
notation guide (see System Diagram in Figure 1). The 
box in Figure 1 contains the new components used in 
the improved and more automated process of creating 
SLU models and annotation guides.  
After data collection, the Preprocessing steps (the 
data reduction and clustering steps are described in 
more detail below) reduce the data that the UE expert 
needs to work with thus saving time and money.  The 
Processed Data, which initially only contains the tran-
scribed utterances but later will also contain call types, 
is stored in an XML database which is used by the Web 
Interface.  At this point, various components of the Web 
Interface are applied to create call types from utterances 
and the Processed Data (utterances and call types) con-
tinue to get updated as these changes are applied.  These 
include the Clustering Tool to fine-tune the optimal 
clustering performance by adjusting the clustering 
threshold. Using this tool, the UE expert can easily 
browse the utterances within each cluster and compare 
the members of one cluster with those of its neighboring 
clusters.  The Relevance Feedback component is im-
plemented by the Call Type Editor Tool. This tool pro-
vides an efficient way to move utterances between two 
call types and to search relevant utterances for a specific 
call type. The Search Engine is used to search text in the 
utterances in order to facilitate the use of relevance 
feedback. It is also used to get a handle on utterance and 
La-
belers Transcribed 
Utterances 
 
Annotation 
Guide 
Preprocessing 
?Data Reduction 
?Clustering 
 
Processed 
Data 
Web-Enabled User Interface 
Clustering 
Relevance 
Feedback 
Search Engine 
 
Report 
Generation 
 
Initial 
SLU 
User  
Experience 
Expert 
SLU  
Toolset 
V 
I 
S 
U 
A 
L 
I 
Z 
A 
T 
I 
O 
N 
 
call type proximity using utterance and cluster dis-
tances.  
After a reasonable percentage of the utterances are 
populated or labeled into call types, an initial SLU 
model can be built and tested using the SLU Toolset. 
Although a reduced dataset is used for labeling (see 
discussion on clone families and reduced dataset be-
low), all the utterances are used when building the SLU 
model in order to take advantage of more data and 
variations in the utterances. The UE expert can itera-
tively refine the SLU model.  If certain test utterances 
are being incorrectly classified or are not providing suf-
ficient differentiability among certain call types (the 
SLU metric described below is used to improve call 
type differentiation), then the UE expert can go back 
and modify the problem call types (by adding utterances 
from other call types or by removing utterances using 
the Web Interface). The updated Processed Data can 
then be used to rebuild the SLU model and it can be 
retested to ensure the desired result.  This initial SLU 
model can also be used as a guide in determining the 
call flow for the application. 
The Reporting component of the Web Interface can 
automatically create the annotation guide from the 
Processed Data in the XML database at any time using 
the Annotation Guide Generation Tool. If changes are 
made to utterances or call types, then the annotation 
guide can be regenerated almost instantly.  Thus, this 
Web Interface allows the UE expert to easily and more 
efficiently create the annotation guide in an automated 
fashion unlike the manual process that was used before. 
3 Components 
Many SLU systems require data collection and some 
form of utterance preprocessing and possibly utterance 
clustering.  Our system uses relevance feedback and 
SLU tools to improve the SLU process. 
3.1 Data Collection 
Natural language data exists in a variety of forms such 
as documents, e-mails, and text chat logs. We will focus 
here on transcriptions of telephone conversations, and in 
particular, on data collected in response to the first 
prompt from an open dialogue system. The utterances 
collected are typically short phrases or single sentences, 
although in some cases, the caller may make several 
statements. It is assumed that there may be multiple 
intents for each utterance. We have also found that the 
methods presented here work well when used with the 
one-best transcription from a large vocabulary auto-
matic speech recognition system instead of manual tran-
scription.  
3.2 Preprocessing 
Our tools add structure to the raw collected data through 
a series of preprocessing steps. Utterance redundancy 
(and even repetition) is inherent in the collection proc-
ess and this is tedious for UE experts to deal with as 
they examine and work with the dataset. This section 
describes taking the original utterance set and reducing 
the redundancy (using text normalization, named entity 
extraction, and feature extraction) and thereby the vol-
ume of data to be examined. The end product of this 
processing is a subset of the original utterances that 
represents the diversity of the input data in a concise 
way. Sets of identical or similar utterances are formed 
and one utterance is selected at random to represent 
each set (alternative selection methods are also possible, 
see the Future Work section). UE experts may choose to 
expand these clone families to view individual mem-
bers, but the bulk of the interaction needs to only in-
volve a single representative utterance from each set.  
Text Normalization 
There is a near continuous degree of similarity between 
utterances. At one extreme are exact text duplicates 
(data samples in which two different callers say the ex-
act same thing). At the next level, utterances may differ 
only by transcription variants like ?100? vs. ?one hun-
dred? or ?$50? vs. ?fifty dollars.? Text normalization is 
used to remove this variation. Moving further, utter-
ances may differ only by the inclusion of verbal pauses 
or of transcription markup such as: ?uh, eh, background 
noise.? Beyond this, for many applications it is insig-
nificant if the utterances differ only by contraction: ?I?d 
vs. I would? or ?I wanna? vs. ?I want to.? Acronym 
expansions can be included here: ?I forgot my personal 
identification number? vs. ?I forgot my P I N.? Up to 
this point it is clear that these variations are not relevant 
for the purposes of intent determination (but of course 
they are useful for training a SLU classifier). We could 
go further and include synonyms or synonymous 
phrases: ?I want? vs. ?I need.?  Synonyms however, 
quickly become too powerful at data reduction, collaps-
ing semantically distinct utterances or producing other 
undesirable effects (?I am in want of a doctor.?)  Also, 
synonyms may be application specific.  
Text normalization is handled by string replacement 
mappings using regular expressions. Note that these 
may be represented as context free grammars and com-
posed with named entity extraction (see below) to per-
form both operations in a single step.  In addition to 
one-to-one replacements, the normalization includes 
many-to-one mappings (you   	
	-
to-null mappings (to remove noise words). 
Named Entity Extraction 
Utterances that differ only by an entity value should 
also be collapsed. For example ?give me extension 
12345? and ?give me extension 54321? should be repre-
sented by ?give me extension extension_value.? Named 
entity extraction is implemented through rules encoded 
using context free grammars in Backus-Naur form.  A 
library of generic grammars is available for such things 
as phone numbers and the library may be augmented 
with application-specific grammars to deal with account 
number formats, for example. The grammars are view-
able and editable, through an interactive web interface. 
Note that any grammars developed or selected at this 
point may also be used later in the deployed application 
but that the named entity extraction process may also be 
data driven in addition to or instead of being rule based.  
Feature Extraction 
To perform processing such as clustering, relevance 
feedback, or building prototype classifiers, the utter-
ances are represented by feature vectors. At the simplest 
level, individual words can be used as features (i.e., a 
unigram language model). In this case, a lexis or vo-
cabulary for the corpus of utterances is formed and each 
word is assigned an integer index. Each utterance is then 
converted to a vector of indices and the subsequent 
processing operates on these feature vectors. Other 
methods for deriving features include using bi-grams or 
tri-grams as features, weighting features based upon the 
number of times a word appears in an utterance or how 
unusual the word is in the corpus (TF, TF-IDF), and 
performing word stemming (Porter, 1980). When the 
dataset available for training is very small (as is the case 
for relevance feedback) it is best to use less restrictive 
features to effectively amplify the training data. In this 
case, we have chosen to use features that are invariant to 
word position, word count and word morphology and 
we ignore noise words. With this, the following two 
utterances have identical feature vector representations: 
? I need to check medical claim status 
? I need check status of a medical claim 
Note that while these features are very useful for the 
process of initially analyzing the data and defining call 
types, it is appropriate to use a different set of features 
when training classifiers with large amounts of data 
when building the SLU model to be fielded. In that case, 
tri-grams may be used, and stemming is not necessary 
since the training data will contain all of the relevant 
morphological variations. 
Clustering 
After the data reductions steps above, we use clustering 
as a good starting point to partition the dataset into clus-
ters that roughly map to call types. 
Clustering is grouping data based on their intrinsic 
similarities. After the data reduction steps described 
above, clustering is used as a bootstrapping process to 
create a reasonable set of call types. 
In any clustering algorithm, we need to define the 
similarity (or dissimilarity, which is also called distance) 
between two samples, and the similarity between two 
clusters of samples. Specifically, the data samples in our 
task are call utterances. Each utterance is converted into 
a feature vector, which is an array of terms and their 
weights. The distance of two utterances is defined as the 
cosine distance between corresponding feature vectors. 
Assume x and y are two feature vectors, the distance 
d(x,y) between them is given by 
yx
yxyx
?
?
?= 1),(d  
As indicated in the previous section, there are differ-
ent ways to extract a feature vector from an utterance. 
The options include named entity extraction, stop word 
removal, word stemming, N-gram on terms, and binary 
or TF-IDF (Term frequency ? inverse document fre-
quency) based weights. Depending on the characteris-
tics of the applications in hand, certain combinations of 
these options are appropriate. For all the results pre-
sented in this paper, we applied named entity extraction, 
stop word removal, word stemming, and 1-gram term 
with binary weights to extract the feature vectors. 
The cluster distance is defined as the maximum dis-
tance between any pairs of two utterances, one from 
each cluster. Figure 2 illustrates the definition of the 
cluster distance.  
 
 
 
Figure 2. Illustration of Cluster Distance. 
 
The range of utterance distance is from 0 to 1, and 
the range of the cluster distance is the same. When the 
cluster distance is 1, it means that there exists at least 
one pair of utterances, one from each cluster, that are 
totally different (sharing no common term).  
The clustering algorithm we adopted is the Hierar-
chical Agglomerative Clustering (HAC) method. The 
details of agglomerative hierarchical clustering algo-
rithm can be found in (Jan and Dubes, 1988).  The fol-
lowing is a brief description of the HAC procedure. 
Initially, each utterance is a cluster on its own. Then, for 
each iteration, two clusters with a minimum distance 
value are merged. This procedure continues until the 
minimum cluster distance exceeds a preset threshold. 
The principle of HAC is straightforward, yet the compu-
tational complexity and memory requirements may be 
high for large size datasets. We developed an efficient 
implementation of HAC by on-the-fly cluster/utterance 
distance computation and by keeping track of the cluster 
distances from neighboring clusters, such that the mem-
ory usage is effectively reduced and the speed is signifi-
cantly increased. 
Our goal is to partition the dataset into call types 
recognized by the SLU model and the clustering results 
provide a good starting point. It is easier to transform a 
set of clusters into call types than to create call types 
directly from a large set of flat data. Depending on the 
distance threshold chosen in the clustering algorithm, 
the clustering results may either be conservative (with 
small threshold) or aggressive (with large threshold). If 
the clustering is conservative, the utterances of one call 
type may be scattered into several clusters, and the UE 
expert has to merge these clusters to create the call type. 
On the other hand, if the cluster is aggressive, there may 
be multiple call types in one cluster, and the UE expert 
needs to manually split the mixture cluster into different 
call types. In real applications, we tend to set a rela-
tively low threshold since it is easier to merge small 
homogeneous clusters than to split one big heterogene-
ous cluster.  
3.3 Relevance Feedback 
Although clustering provides a good starting point, find-
ing all representative utterances belonging to one call 
type is not a trivial task. Effective data mining tools are 
desirable to help the UE expert speed up this manual 
procedure. Our solution is to provide a relevance feed-
back mechanism based on support vector machine 
(SVM) techniques for the UE expert to perform this 
tedious task.  
Relevance feedback is a form of query-free retrieval 
where documents are retrieved according to a measure 
of relevance to given documents. In essence, a UE ex-
pert indicates to the retrieval system that it should re-
trieve ?more documents like the ones desired, not the 
ones ignored.? Selecting relevant documents based on 
UE expert?s inputs is basically a classification (rele-
vant/irrelevant) problem. We adopted support vector 
machine as the classifier for to two reasons: First, SVM 
efficiently handles high dimensional data, especially a 
text document with a large vocabulary. Second, SVM 
provides reliable performance with small amount of 
training data. Both advantages perfectly match the task 
at hand. For more details about SVM, please refer to 
(Vapnik, 1998; Drucker et al 2002).  
Relevance feedback is an iterative procedure. The 
UE expert starts with a cluster or a query result by cer-
tain keywords, and marks each utterance as either a 
positive or negative utterance for the working call type. 
The UE expert?s inputs are collected by the relevance 
feedback engine, and they are used to build a SVM clas-
sifier that attempts to capture the essence of the call type. 
The SVM classifier is then applied to the rest of the 
utterances in the dataset, and it assigns a relevance score 
for each utterance. A new set of the most relevant utter-
ances are generated and presented to the UE expert, and 
the second loop of relevance feedback begins. During 
each loop, the UE expert does not need to mark all the 
given utterances since the SVM is capable of building a 
reasonable classifier based on very few, e.g., 10, train-
ing samples. The superiority of relevance feedback is 
that instead of going through all the utterances one by 
one to create a specific call type, the UE expert only 
needs to check a small percentage of utterances to create 
a satisfactory call type.  
 
 
 
Figure 3. The Interface for Relevance Feedback. 
 
The relevance feedback engine is implemented by 
the Call Type Editor Tool. This tool provides an inte-
grated environment for the UE expert to create a variety 
of call types and assign relevant utterances to them.  
The tool provides an efficient way to move utterances 
between two call types and to search relevant utterances 
for a specific call type. The basic search function is to 
search a keyword or a set of keywords within the dataset 
and retrieve all utterances containing these search terms. 
The UE expert can then assign these utterances into the 
appropriate call types. Relevance feedback serves as an 
advanced searching option. Relevance feedback can be 
applied to the positive and negative utterances of a clus- 
 Table 1. Data Reduction Results 
 
ter or call type or can be applied to utterances, from a 
search query, which are marked as positive or negative. 
The interface for the relevance feedback is shown in 
Figure 3. In the interface, the UE expert can mark the 
utterances as positive or negative samples. The UE ex-
pert can also control the threshold of the relevance value 
such that the relevance feedback engine only returns 
utterances with high enough relevance values. In the 
tool, we are using an internally developed package for 
learning large margin classifiers to implement the SVM 
classifier (Haffner et al 2003). 
   
3.4 SLU Toolset 
The SLU toolset is based on an internally developed 
NLU Toolset. The underlying boosting algorithm for 
text classification used, BoosTexter, is described else-
where (Freund and Schapire, 1999; Schapire and Singer, 
2000; Rochery et al 2002). We added interactive input 
and display capabilities via a Web interface allowing the 
UE expert to easily build and test SLU models. 
Named entity grammars are constructed as described 
above. About 20% of the labeled data is set aside for 
testing. The remaining data is used to build the initial 
SLU model which is used to test the utterances set aside 
for testing. The UE expert can interactively test utter-
ances typed into a Web page or can evaluate the test 
results of the test data.  For each of the tested utterances 
in the test data, test logs show the classification confi-
dence scores for each call type. The confidence scores 
are replaced by probability thresholds that have been 
computed using a logistic function.  These scores are 
then used to calculate a simple metric which is a meas-
ure of call type differentiability. If the test utterance 
labeled by the UE expert is correctly classified, then the 
call type is the truth call type. The SLU metric is calcu-
lated as follows and it is averaged over the utterances: 
? if the call type is the truth, the score is the dif-
ference (positive) between the truth probability 
and the next highest probability 
? if the call type is not the truth, the score is the 
difference (negative) between the truth prob-
ability and the highest probability 
This metric allows the UE expert to easily spot prob-
lem call types or those that might give potential prob-
lems in the field.  It is critical that call types are easily 
differentiable in order to properly route the call.  The 
UE expert can iteratively build and test the initial SLU 
models until the UE expert has a set of self-consistent 
call types before creating the final annotation guide.  
The final annotation guide would then be used by the 
labelers to label all the utterance data needed to build 
the final SLU model.  Thus, the SLU Toolset is critical 
for creating the call types defined in the annotation 
guide which in turn is needed to label the data for creat-
ing the final SLU. 
Alternatively, the labeled utterances can easily be 
exported in a format compatible with the internally de-
veloped NLU Toolset if further SLU model tuning is to 
be performed by the NLU expert using just the com-
mand line interface. 
3.5 Reporting 
One of the reporting components is the Annotation 
Guide Generation Tool.  The UE expert can use this at 
any time to automatically generate the annotation guide 
from the Processed Data.  Other reporting components 
include summary statistics and spreadsheets containing 
utterance and call type information. 
4 Results 
The performance of the preprocessing techniques has 
been evaluated on several datasets from various industry 
sectors. Approximately 10,000 utterances were col-
lected for each application and the results of the data 
reduction at each processing stage are shown in Table 1. 
The Redundancy R is given by 
N
UR ?= 1  
where U is the number of unique utterances after feature 
extraction and N is the number of original utterances.  
Industry 
Sector 
Original 
Utterances 
Unique 
Utterances 
Unique  
Utterances after 
Text  
Normalization 
Unique 
Utterances  
after Entity 
Extraction 
Unique 
Utterances  
after Feature 
Extraction 
Redundancy 
Financial 11,623 10,021 9,670 9,165 7,929 31.8% 
Healthcare 12,080 10,255 9,452 9,382 7,946 34.2% 
Insurance 12,109 8,865 8,103 7,963 6,530 46.1% 
Retail 10,240 4,956 4,392 4,318 3,566 65.2% 
Initial UE experts of the tools have been successful 
in producing annotation guides more quickly and with 
very good initial F-measures.  
recallprecision
recallprecisionF
+
??
=
2
 
They have also reported that the task is much less 
tedious and that they have done a better job of covering 
all of the significant utterance clusters. Further studies 
are required to generate quantitative measures of the 
performance of the toolset. 
5 Future Work 
In the future, the system could be improved using other 
representative utterance selection algorithms (e.g., se-
lecting the utterance with the minimum string edit dis-
tance to all others).  
The grammars for entity extraction were not tuned 
for these applications and it is expected that further data 
reduction will be obtained with improved grammars. 
6 Conclusions 
We presented an interactive speech data analysis system 
for creating and testing spoken language understanding 
systems.  Spoken language understanding is a critical 
component of automated customer service applications.  
Creating effective SLU models is inherently a data 
driven process and requires considerable human inter-
vention.  The fact that this process relies heavily on hu-
man expertise prevents a total automation of the 
process. Our experience indicates that augmenting the 
human expertise with interactive data analysis tech-
niques made possible by machine learning techniques 
can go a long way towards increasing the efficiency of 
the process and the quality of the final results.  The 
automatic preprocessing of the utterance data prior to its 
use by the UE expert results in a considerable reduction 
in the number of utterances that needs to be manually 
examined. Clustering uncovers certain structures in the 
data that can then be refined by the UE expert. Super-
vised machine learning capabilities provided by interac-
tive relevance feedback tend to capture the knowledge 
of the UE expert to create the guidelines for labeling the 
data.  The ability to test the generated call types during 
the design process helps detect and remove problematic 
call types prior to their inclusion in the SLU model.  
This tool has been used to create the labeling guide for 
several applications by different UE experts.  Aside 
from the increased efficiency and improved quality of 
the generated SLU systems, the tool has resulted in in-
creased uniformity in the way different UE experts clas-
sify calls into call type labels.  
Acknowledgements 
We would like to thank Harris Drucker, Patrick Haffner, 
Steve Lewis, Maria Alvarez-Ryan, Barbara Hollister, 
Harry Blanchard, Liz Alba, Elliot Familant, Greg Pulz, 
David Neeves, Uyi Stewart, and Lan Zhang for their 
contributions to this work. 
 
 
References 
 
Harris Drucker, Behzad Shahraray, and David C. Gib-
bon, 2002. Support Vector Machines: Relevance 
Feedback and Information Retrieval, Information 
Processing and Management, 38(3):305-323. 
Yaov Freund and Robert Schapire, 1999. A Short Intro-
duction to Boosting, Journal of Japanese Society for 
Artificial Intelligence, 14(5):771-780. 
Patrick Haffner, Gokhan Tur, and Jerry Wright, 2003. 
Optimizing SVMs for complex Call Classification, 
ICASSP 2003. 
A. L. Gorin, G. Riccardi, and J. H. Wright. 1997. How 
May I Help You? Speech Communication, 23:113-
127. 
A. K. Jan and R. C. Dubes, 1988. Algorithms for Clus-
tering Data, Prentice Hall. 
M. F. Porter, 1980. An Algorithm For Suffix Stripping, 
Program, 14(3):130-137. 
M. Rochery, R. Schapire, M. Rahim, N. Gupta, G. Ric-
cardi, S. Bangalore, H. Alshawi and S. Douglas, 
2002. Combining prior knowledge and boosting for 
call classification in spoken language dialogue, 
ICASSP 2002. 
SAS Institute Press Release, 2002. New SAS? Text Min-
ing Software Surfaces Intelligence beyond the Num-
bers, 1/21/02. 
Robert Schapire and Yoram Singer, 2000. BoosTex-
ter: A Boosting-based System for Text Categorization, 
Machine Learning, 39(2/3):135-168. 
Gokhan Tur, Robert E. Schapire, and Dilek Hakkani-
T?r, 2003. Active Learning for Spoken Language 
Understanding, Proceedings of International Confer-
ence on Acoustics, Speech and Signal Processing, 
ICASSP 2003. 
V. N. Vapnik, 1998. Statistical Learning Theory, John 
Wiley & Sons, Inc. 
 
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 17?24,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Multimodal Presentation Dashboard 
 
Michael Johnston 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
johnston 
@research. 
att.com 
Patrick Ehlen 
CSLI 
 Stanford University 
Palo Alto, CA  
ehlen@csli. 
stanford.edu 
David Gibbon 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
dcg@research. 
att.com 
Zhu Liu 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
zliu@research. 
att.com 
 
Abstract 
The multimodal presentation dashboard al-
lows users to control and browse presenta-
tion content such as slides and diagrams 
through a multimodal interface that sup-
ports speech and pen input. In addition to 
control commands (e.g. ?take me to slide 
10?), the system allows multimodal search 
over content collections. For example, if 
the user says ?get me a slide about internet 
telephony,? the system will present a 
ranked series of candidate slides that they 
can then select among using voice, pen, or 
a wireless remote. As presentations are 
loaded, their content is analyzed and lan-
guage and understanding models are built 
dynamically. This approach frees the user 
from the constraints of linear order allow-
ing for a more dynamic and responsive 
presentation style. 
1 Introduction 
Anthropologists have long informed us that the 
way we work?whether reading, writing, or giving 
a presentation?is tightly bound to the tools we 
use. Web browsers and word processors changed 
the way we read and write from linear to nonlinear 
activities, though the linear approach to giving a 
presentation to a roomful of people has evolved 
little since the days of Mylar sheets and notecards, 
thanks to presentation software that reinforces?or 
even further entrenches?a linear bias in our no-
tion of what ?giving a presentation? means to us. 
While today?s presentations may be prettier and 
flashier, the spontaneity once afforded by holding a 
stack of easily re-arrangeable sheets has been lost. 
 
 
 
Figure 1 Presentation dashboard in action 
Instead, a question from the audience or a change 
in plan at the podium results in a whizzing-by of 
all the wrong slides as the presenter sweats through 
an awkward silence while hammering an arrow 
key to track down the right one. In theory there are 
?search? functions that presenters could use to find 
another slide in the same presentation, or even in 
another presentation on the same machine, though 
none of the authors of this paper has ever seen a 
presenter do this. A likely reason is that these 
search functions are designed for desktop ergo-
17
nomics rather than for standing at a podium or 
walking around the room, making them even more 
disruptive to the flow of a presentation than frantic 
arrow key hammering. 
In some utopian future, we envision presenters 
who are unhindered by limitations imposed by 
their presentation tools, and who again possess, as 
Aristotle counseled, ?all available means of per-
suasion? at the tips of their fingers?or their 
tongues. They enjoy freeform interactions with 
their audiences, and benefit from random access to 
their own content with no arrow hammering and no 
disruption in flow. Their tools help to expand their 
possible actions rather than limiting them. We are 
hardly alone in this vision. 
In that spirit, many tools have been developed of 
late?both within and outside of research labs?
with the aim of helping people work more effec-
tively when they are involved in those assemblies 
of minds of mutual interest we often call ?meet-
ings.? Tools that capture the content of meetings, 
perform semantic understanding, and provide a 
browsable summary promise to free meeting par-
ticipants from the cognitive constraints of worrying 
about trying to record and recall what happened 
when a meeting takes place (e.g., Ehlen, Purver & 
Niekrasz, 2007; Tucker & Whittaker, 2005).  
Presentations are a kind of meeting, and several 
presentation tools have also sought to free present-
ers from similar constraints. For example, many 
off-the-shelf products provide speech interfaces to 
presentation software. These often replace the lin-
ear arrow key with the voice, offering command-
based navigation along a one-dimensional vector 
of slides by allowing a presenter to say ?next slide 
please? or ?go to the last slide.?  
A notable exception is the Jabberwocky inter-
face to PowerPoint (Franklin, Bradshaw & 
Hammond, 1999; 2000), which aims to follow 
along with a presenter?s talk?like a human assis-
tant might do?and switch to the appropriate slide 
when the presenter seems to be talking about it. 
Using a method similar to topic modeling, words 
spoken by the presenter are compared to a prob-
ability distribution of words across slides. Jabber-
wocky changes to a different slide when a 
sufficient probability mass has been reached to 
justify the assumption that the speaker is now talk-
ing about a different slide from the one that?s al-
ready showing. 
A similar effort (Rogina & Schaaf, 2002) uses 
words extracted from a presentation to augment a 
class-based language model and attempt automatic 
tracking of a presentation as it takes place. This 
intelligent meeting room system then aligns the 
presenter?s spoken words with parts of a presenta-
tion, hoping to determine when a presenter has 
moved on to a new slide. 
A major drawback of this ?machine-initiative? 
approach to presentation assistance is that a pre-
senter must speak enough words associated with a 
new slide for a sufficient probability mass to be 
reached before the slide is changed. The resulting 
delay is likely to make an audience feel like the 
presentation assistant is rather dim-witted. And any 
errors that change slides before the presenter is 
ready can be embarrassing and disruptive in front 
of potentially important audiences. 
So, in fashioning our own presentation control 
interface, we chose to allow the presenter to retain 
full initiative in changing slides, while offering a 
smarter and more flexible way to navigate through 
a presentation than the single degree of freedom 
afforded by arrow keys that simply traverse a pre-
determined order. The result is the Multimodal 
Presentation Dashboard, a presentation interface 
that integrates command-based control with prob-
abilistic, content-based search. Our method starts 
with a context-free grammar of speech commands, 
but embeds a stochastic language model generated 
from the presenter?s slide deck content so a pre-
senter can request any slide from the deck?or 
even a large set of decks?just by asking for its 
contents. Potentially ambiguous results are re-
solved multimodally, as we will explain. 
2 Multimodal interface for interactive 
presentations  
The presentation dashboard provides presenters 
with the ability to control and adapt their presenta-
tions on the fly in the meeting room. In addition to 
the traditional next/previous approach to navigat-
ing a deck of slides, they can access slides by posi-
tion in the active deck (e.g., ?show slide 10? or 
?last slide please?) or they can multimodally com-
bine voice commands with pen or remote control 
to browse for slides by content, saying, for in-
stance, ?show the slide on internet telephony,? and 
then using the pen to select among a ranked list of 
alternatives. 
18
2.1 Setup configuration 
Though the dashboard offers many setup configu-
rations, the preferred arrangement uses a single PC 
with two displays (Figure 1). Here, the dashboard 
is running on a tablet PC with a large monitor as a 
second external display. On the tablet, the 
dashboard UI is visible only to the presenter. On 
the external display, the audience sees the current 
slide, as they would with a normal presentation. 
The presenter can interact with the dashboard 
using either the microphone onboard the tablet PC, 
or, preferably, a wireless microphone. A wireless 
remote functions as a presentation control, which 
can be used to manually change slides in the tradi-
tional manner, and also provides a ?push to talk? 
button to tell the dashboard when to listen. A wire-
less microphone combined with the wireless pres-
entation control and voice selection mode (see 
Section 2.3) allows a presenter to stroll around the 
room or stage completely untethered.  
2.2 Presenter UI 
The presenter?s primary control of the system is 
through the presenter UI, a graphical user interface 
augmented with speech and pen input. The inter-
face has three main screens: a presentation panel 
for controlling an ongoing presentation (Figure 2), 
a loader panel for selecting a set of presentations to 
load (Figure 4), and a control panel for adjusting 
system settings and bundling shareable index and 
grammar models. The user can select among the 
panels using the tabs at the top left.  
 
  
Figure 2 The presentation panel 
 
The presentation panel has three distinct functional 
areas from top to bottom. The first row shows the 
current slide, along with thumbnails of the previ-
ous and next slides to provide context. The user 
can navigate to the next or previous slide by click-
ing on these thumbnails. The next row shows a 
scrolling list of search results from content-based 
queries. The last row contains interaction informa-
tion. There is a click & speak button for activating 
the speech recognizer and a feedback window that 
displays recognized speech.  
Some user commands are independent of the 
content of slide decks, as with basic commands for 
slide navigation: 
- ?next slide please? 
- ?go back? 
- ?last slide? 
In practice, however, navigation to next and previ-
ous slides is much easier using buttons on the wire-
less control. The presenter can also ask for slides 
by position number, allowing random access: 
- ?take me to slide 10? 
- ?slide 4 please? 
But not many presenters can remember the posi-
tion numbers of some 40 or 50 slides, we?d guess, 
so we added content-based search, a better method 
of random access slide retrieval by simply saying 
key words or phrases from the desired slide, e.g.:  
- ?slides about internet telephony? 
- ?get me the slide with the 
  system architecture? 
- ?2006 highlights? 
- ?budget plan, please? 
When the presenter gives this kind of request, the 
system identifies any slides that match the query 
and displays them in a rank ordered list in the mid-
dle row of the presenter?s panel. The presenter can 
then scroll through the list of thumbnails and click 
one to display it to the audience. 
This method of ambiguity resolution offers the 
presenter some discretion in selecting the correct 
slide to display from multiple search results, since 
search results appear first on the presenter?s private 
interface rather than being displayed to the audi-
ence. However, it requires the presenter to return to 
the podium (or wherever the tablet is located) to 
select the correct slide.  
 
19
2.3 Voice selection mode 
Alternatively, the presenter may sacrifice discre-
tion for mobility and use a ?voice selection mode,? 
which lets the presenter roam freely throughout the 
auditorium while making and resolving content-
based queries in plain view of the audience. In this 
mode, if a presenter issues a content-based query 
(e.g., ?shows slides about multimodal access?), 
thumbnails of the slides returned by the query ap-
pear as a dynamically-generated interactive 
?chooser? slide (Figure 3) in the main presentation 
viewed by the audience. The presenter can then 
select the desired slide by voice (e.g., ?slide three?) 
or by using the previous, next, and select controls 
on the wireless remote. If more than six slides are 
returned by the query, multiple chooser slides are 
generated with six thumbnails to each slide, which 
can be navigated with the remote.  
While voice selection mode allows the presenter 
greater mobility, it has the drawback of allowing 
the audience to see thumbnails of every slide re-
turned by a content-based query, regardless of 
whether the presenter intended for them to be seen. 
Hence this mode is more risky, but also more im-
pressive! 
 
 
Figure 3 Chooser slide for voice selection mode 
2.4 Compiling deck sets 
Sometimes a presenter wishes to have access to 
more than one presentation deck at a time, in order 
to respond to unexpected questions or comments, 
or to indulge in a whimsical tangent. We respond 
to this wish by allowing the presenter to compile a 
deck set, which is, quite simply, a user-defined 
bundle of multiple presentations that can all be 
searched at once, with their slides available for 
display when the user issues a query. In fact, this 
option makes it easy for a presenter to follow spon-
taneous tangents by switching from one presenta-
tion to another, navigating through the alternate 
deck for a while, and then returning to the original 
presentation, all without ever walking to the po-
dium or disrupting the flow of a presentation by 
stopping and searching through files. 
Deck sets are compiled in the loader panel (Fig-
ure 4), which provides a graphical browser for se-
lecting a set of active decks from the file system. 
When a deck set is chosen, the system builds ASR 
and language understanding models and a retrieval 
index for all the slides in the deck set. A compiled 
deck set is also portable, with all of the grammar 
and understanding model files stored in a single 
archive that can be transferred via e-mail or thumb 
drive and speedily loaded on another machine.  
A common use of deck sets is to combine a 
main presentation with a series of other slide decks 
that provide background information and detail for 
answering questions and expanding points, so the 
presenter can adapt to the interests of the audience. 
 
Figure 4 The loader panel 
3 Multimodal architecture  
The Multimodal Presentation Dashboard uses an 
underlying multimodal architecture that inherits 
core components from the MATCH architecture 
(Johnston et al2002). The components communi-
cate through a central messaging facilitator and 
include a speech recognition client, speech recog-
nition server (Goffin et al2005), a natural lan-
guage understanding component (Johnston & 
Bangalore 2005), an information retrieval engine, 
20
and a graphical user interface client.  The graphical 
UI runs in a web browser and controls PowerPoint 
via its COM interface.  
We first describe the compilation architecture, 
which builds models and performs indexing when 
the user selects a series of decks to activate. We 
then describe the runtime architecture that operates 
when the user gives a presentation using the sys-
tem. In Section 3.3, we provide more detail on the 
slide indexing mechanism and in Section 3.4 we 
describe a mechanism used to determine key-
phrases from the slide deck that are used on a drop 
down menu and for determining relevancy.  
3.1 Compilation architecture 
In a sense, the presentation dashboard uses neither 
static nor dynamic grammars; the grammars com-
piled with each deck set lie somewhere in-between 
those two concepts. Command-based speech inter-
faces often fare best when they rely on the predict-
ability of a fixed, context-free grammar, while 
interfaces that require broader vocabulary coverage 
and a wider range of syntax are better off leverag-
ing the flexibility of stochastic language models. 
To get the best of both worlds for our ASR model, 
we use a context-free command ?wrapper? to a 
stochastic language model (c.f. Wang & Acero 
2003). This is coupled to the understanding 
mechanism using a transducer with a loop over the 
content words extracted from the slides.  
This combined grammar is best thought of as a 
fixed, context-free template which contains an em-
bedded SLM of dynamic slide contents. Our 
method allows a static background grammar and 
understanding model to happily co-exist with a 
dynamic grammar component which is compiled 
on the fly when presentations are loaded, enabling 
custom, content-based queries.  
When a user designates a presentation deck set 
and compiles it, the slides in the set are processed 
to create the combined grammar by composing an 
SLM training corpus based on the slide content.  
First, a slide preprocessor extracts sentences, ti-
tles, and captions from each slide of each deck, and 
normalizes the text by converting numerals and 
symbols to strings, Unicode to ASCII, etc. These 
content phrases are then used to compose (1) a 
combined corpus to use for training an SLM for 
speech recognition, and (2) a finite-state transducer 
to use for multimodal natural language understand-
ing (Johnston & Bangalore 2005). 
Combined Corpus
Presentations
Slide 
index
Keyphrases
Slide PreprocessorSlide Preprocessor
Sentences
Index 
Server
Index 
Server
SLM 
for ASR
SLM 
for ASR NLU
MODEL
NLU
MODEL
GUI 
Menu
GUI 
Menu
Grammar
Template
Class-tagged
Corpus
Grammar
Words
 
Figure 5 Compilation architecture 
To create a combined corpus for the SLM, the con-
tent phrases extracted from slides are iterated over 
and folded into a static template of corpus classes. 
For instance, the template entry, 
<POLITE> <SHOWCON> <CONTENT_PHRASE> 
could generate the phrase ?please show the slide 
about <CONTENT_PHRASE>? for each content 
phrase?as well as many others.  These templates 
are currently manually written but could poten-
tially be induced from data as it becomes available. 
The content corpus is appended to a command 
corpus of static command classes that generate 
phrases like ?next slide please? or ?go back to the 
last one.? Since the number of these command 
phrases remains constant for every grammar while 
the number of content phrases depends on how 
many phrases are extracted from the deck set, a 
weighting factor is needed to ensure the number of 
examples of both content and command phrases is 
balanced in the SLM training data. The resulting 
combined corpus is used to build a stochastic lan-
guage model that can handle variations on com-
mands and slide content.  
In parallel to the combined corpus, a stack of 
slide content words is compiled for the finite state 
understanding machine. Phrases extracted for the 
combined corpus are represented as a terminal 
_CWORD class. (Terminals for tapes in each gram-
mar class are separated by colons, in the format 
speech:meaning, with empty transitions repre-
21
sented as ?) For example, the phrase ?internet 
telephony? on a slide would appear in the under-
standing grammar like so: 
_CWORD internet:internet 
_CWORD telephony:telephony 
These content word classes are then ?looped? in 
the FSM (Figure 6) into a flexible understanding 
model of potential slide content results using only 
a few grammar rules, like: 
_CONTENT _CWORD _CONTENT 
_CONTENT _CWORD 
The SLM and the finite-state understanding ma-
chine now work together to extract plausible mean-
ings from dynamic and inexact speech queries. 
 
 
Figure 6 Understanding FSM 
To provide an example of how this combined ap-
proach to understanding comes together in the run-
ning system, let?s say a presenter?s slide contains 
the title ?Report for Third Quarter? and she asks 
for it by saying, ?put up the third quarter report 
slide.? Though she asks for the slide with language 
that doesn?t match the phrase on the slide, our for-
giving stochastic model might return a speech re-
sult like, ?put up third quarter report mine.? The 
speech result is then mapped to the finite-state 
grammar, which catches ?third quarter report 
mine? as a possible content phrase, and returns, 
?third,quarter,report,mine? as a con-
tent-based meaning result. That result is then used 
for information retrieval and ranking to determine 
which slides best match the query (Section 3.3). 
3.2 Runtime architecture  
A primary goal of the presentation dashboard was 
that it should run standalone on a single laptop. A 
tablet PC works best for selecting slides with a 
pen, though a mouse or touch screen can also be 
used for input. We also developed a networked 
version of the dashboard system where indexing, 
compilation, speech recognition, and understand-
ing are all network services accessed over HTTP 
and SIP, so any web browser-based client can log 
in, upload a presentation, and present without in-
stalling software aside from PowerPoint and a SIP 
plug-in. However, our focus in this paper is on the 
tablet PC standalone version. 
ASR SERVERASR SERVER
Multimodal Dashboard 
UI (Browser)
Multimodal Dashboard 
UI (Browser)
NLUNLU
Powerpoint
Application
Powerpoint
Application
Index Server (http)Index Server (http)
Language
Model
Slide index
HTTP
Commands
Images
FACILITATORFACILITATOR
SPEECH
CLIENT
SPEECH
CLIENT
Understanding
Model
 
Figure 7 Multimodal architecture 
The multimodal user interface client is browser-
based, using dynamic HTML and Javascript. Inter-
net Explorer provides COM access to the Power-
Point object model, which reveals slide content and 
controls the presentation. Speech recognition, un-
derstanding, and compilation components are ac-
cessed through a java-based facilitator via a socket 
connection provided by an ActiveX control on the 
client page (Figure 7). When the user presses or 
taps the click & speak button, a message is sent to 
the Speech client, which sends audio to the ASR 
Server. The recognizer?s speech result is processed 
by the NLU component using a finite-state trans-
ducer to translate from the input string to an XML 
meaning representation. When the multimodal UI 
receives XML for simple commands like ?first 
slide? or ?take me to slide ten,? it calls the appro-
priate function through the PowerPoint API. For 
content-based search commands, an SQL query is 
constructed and issued to the index server as an 
HTTP query. When the results are returned, mul-
timodal thumbnail images of each slide appear in 
the middle row of the UI presenter panel. The user 
can then review the choices and switch to the ap-
propriate slide by clicking on it?or, in voice se-
lection mode, by announcing or selecting a slide 
shown in the dynamically-generated chooser slide.  
The system uses a three stage strategy in search-
ing for slides. First it attempts an exact match by 
looking for slides which have the words of the 
query in the same order on the same slide in a sin-
gle phrase. If no exact matches are found, the sys-
tem backs off to an AND query and shows slides 
which contain all of the words, in any order. If that 
22
fails, the system resorts to an OR query and shows 
slides which have any of the query terms.  
3.3 Information retrieval 
When the slide preprocessor extracts text from a 
presentation, it retains the document structure as 
much as possible and stores this in a set of hier-
archal XML documents. The structure includes 
global document metadata such as creation date 
and title, as well as more detailed data such as slide 
titles. It also includes information about whether 
the text was part of a bullet list or text box. With 
this structure, queries can be executed against the 
entire text or against specified textual attributes 
(e.g. ?show me the chart titled ?project budget??). 
For small document collections, XPath queries 
can search the entire collection with good response 
time, providing a stateless search method. But as 
the collection of presentation decks to be searched 
grows, a traditional inverted index information re-
trieval system achieves better response times. We 
use a full text retrieval system that employs stem-
ming, proximity search, and term weighting, and 
supports either a simplified query syntax or SQL. 
Global metadata can also constrain queries. Incre-
mental indexing ensures that new presentation 
decks cause the index to update automatically 
without being rebuilt from scratch. 
3.4 Key phrase extraction 
Key phrases and keywords are widely used for in-
dexing and retrieving documents in large data-
bases. For presentation slides, they can also help 
rank a slide?s relevance to a query. We extract a 
list of key phrases with importance scores for each 
slide deck, and phrases from a set of decks are 
merged and ranked based on their scores. 
A popular approach to selecting keywords from 
a document within a corpus is to find keywords 
that frequently occur in one document but seldom 
occur in others, based on term frequency-inverse 
document frequency (TF-IDF). Our task is slightly 
different, since we wish to choose key phrases for 
a single document (the slide deck), independent of 
other documents. So our approach uses term fre-
quency-inverse term probability (TF-ITP), which 
expresses the probability of a term calculated over 
a general language rather than a set of documents. 
Assuming a term Tk occurs tfk times in a docu-
ment, and its term probability is tpk, the TF-ITP of 
Tk is defined as, wTk = tfk / tpk. This method can be 
extended to assign an importance score to each 
phrase. For a phrase Fk = {T1 T2 T3 ? TN}, which 
contains a sequence of N terms, assuming it ap-
pears ffk times in a document, its importance score, 
ISk, is defined as, 
?
=
=
N
i i
k
k T
ff
IS
1
. 
To extract a set of key phrases, we first segment 
the document into sentences based on punctuation 
and some heuristics. A Porter stemming algorithm 
(Porter 1980) eliminates word variations, and 
phrases up to N=4 terms long are extracted, remov-
ing any that start or end with noise words. An im-
portance score ranks each phrase, where term 
probabilities are estimated from transcripts of 600 
hours of broadcast news data. A term that is out of 
the vocabulary with a term frequency of more than 
2 is given a default term probability value, defined 
as the minimum term probability in the vocabulary. 
Phrases with high scores are chosen as key 
phrases, eliminating any phrases that are contained 
in other phrases with higher scores. For an overall 
list of key phrases in a set of documents, we merge 
individual key phrase lists and sum the importance 
scores for key phrases that recur in different lists, 
keeping the top 10 phrases. 
4 Performance and future work 
The dashboard is fully implemented, and has been 
used by staff and management in our lab for inter-
nal presentations and talks. It can handle large 
decks and collections (100s to 1000s of slides). A 
tablet PC with a Pentium M 1.6Ghz processor and 
1GB of RAM will compile a presentation of 50 
slides?with ASR, understanding models, and 
slide index?in under 30 seconds.  
In ongoing work, we are conducting a usability 
test of the system with users in the lab. Effective 
evaluation of a tool of this kind is difficult without 
fielding the system to a large number of users. An 
ideal evaluation would measure how users fare 
when giving their own presentations, responding to 
natural changes in narrative flow and audience 
questions. Such interaction is difficult to simulate 
in a lab, and remains an active area of research. 
23
We also hope to extend current retrieval meth-
ods to operate at the level of concepts, rather than 
words and phrases, so a request to show ?slides 
about mortgages? might return a slide titled ?home 
loans.? Thesauri, gazetteers, and lexicons like 
WordNet will help achieve this. Analyzing non-
textual elements like tables and charts could also 
allow a user to say, ?get the slide with the network 
architecture diagram.? And, while we now use a 
fixed lexicon of common abbreviations, an auto-
mated analysis based on web search and other 
techniques could identify likely expansions. 
5 Conclusion 
Our goal with the multimodal presentation 
dashboard was to create a meeting/presentation 
assistance tool that would change how people be-
have, inspiring presenters to expand the methods 
they use to interact with audiences and with their 
own material. To this end, our dashboard runs on a 
single laptop, leaves the initiative in the hands of 
the presenter, and allows slides from multiple pres-
entations to be dynamically retrieved from any-
where in the room. Our assistant requires no 
?intelligent room?; only an intelligent presenter, 
who may now offer the audience a presentation 
that is as dynamic or as dull as imagination allows. 
As Tufte (2006) reminds us in his analysis of 
how PowerPoint presentations may have precipi-
tated the Columbia shuttle tragedy, the way infor-
mation is presented can have a profound?even 
life-threatening?impact on the decisions we 
make. With the multimodal presentation 
dashboard, we hope to free future presenters from 
that single, arrow-key dimension, offering access 
to presentation slides and diagrams in any order, 
using a diverse combination of modes. Presenters 
can now pay more attention to the needs of their 
audiences than to the rigid determinism of a fixed 
presentation. Whether they will break free of the 
linear presentation style imposed by current tech-
nology if given a chance remains to be seen. 
References  
Patrick Ehlen, Matthew Purver, and John Niekrasz. 
2007. A meeting browser that learns. In Proceedings 
of the AAAI Spring Symposium on Interaction Chal-
lenges for Intelligent Assistants. 
David Franklin, Shannon Bradshaw, and Kristian 
Hammond. 1999. Beyond ?Next slide, please?: The 
use of content and speech in multi-modal control. In 
Working Notes of the AAAI-99 Workshop on Intelli-
gent Information Systems. 
David Franklin, Shannon Bradshaw, and Kristian 
Hammond. 2000. Jabberwocky: You don't have to be 
a rocket scientist to change slides for a hydrogen 
combustion lecture. In Proceedings of Intelligent 
User Interfaces 2000 (IUI-2000). 
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, Dilek 
Hakkani-T?r, Andrej Ljolje, Sarangarajan Partha-
sarathy, Mazin Rahim, Giuseppe Riccardi, and Murat 
Saraclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP.   
Michael Johnston, Srinivas Bangalore, Guna Vasireddy, 
Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve 
Whittaker, Preetam Maloor. 2002. MATCH: An Ar-
chitecture for Multimodal Dialogue Systems. In Pro-
ceedings of the 40th ACL. 376-383. 
Michael Johnston  and Srinivas Bangalore. 2005. Finite-
state multimodal integration and understanding. 
Journal of Natural Language Engineering. 11.2, pp. 
159-187, Cambridge University Press. 
Martin F. Porter. 1980. An algorithm for suffix strip-
ping, Program, 14, 130-137. 
Ivica Rogina and Thomas Schaaf. 2002. Lecture and 
presentation tracking in an intelligent meeting room. 
In Proceedings of the 4th IEEE International Confer-
ence on Multimodal Interfaces. 47-52. 
Simon Tucker and Steve Whittaker. 2005. Accessing 
multimodal meeting data: Systems, problems and 
possibilities. In Samy Bengio and Herv? Bourlard 
(Eds.) Lecture Notes in Computer Science, 3361, 1-
11 
Edward Tufte. 2006. The Cognitive Style of PowerPoint. 
Graphics Press, Cheshire, CT. 
Ye-Yi Wang and Alex Acero. 2003. Combination of 
CFG and N-gram Modeling in Semantic Grammar 
Learning. Proceedings of Eurospeech conference, 
Geneva, Switzerland. 
Acknowledgements We would like to thank Srinivas Banga-
lore, Rich Cox, Mazin Gilbert, Vincent Goffin, and Behzad 
Shahraray for their help and support.  
24
