Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2314?2325, Dublin, Ireland, August 23-29 2014.
Solving Substitution Ciphers with Combined Language Models
Bradley Hauer Ryan Hayward Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, Canada
{bmhauer,hayward,gkondrak}@ualberta.ca
Abstract
We propose a novel approach to deciphering short monoalphabetic ciphers that combines both
character-level and word-level language models. We formulate decipherment as tree search, and
use Monte Carlo Tree Search (MCTS) as a fast alternative to beam search. Our experiments
show a significant improvement over the state of the art on a benchmark suite of short ciphers.
Our approach can also handle ciphers without spaces and ciphers with noise, which allows us to
explore its applications to unsupervised transliteration and deniable encryption.
1 Introduction
Monoalphabetic substitution is a well-known method of enciphering a plaintext by converting it into a
ciphertext of the same length using a key, which is equivalent to a permutation of the alphabet (Figure 1).
The method is elegant and easy to use, requiring only the knowledge of a key whose length is no longer
than the size of the alphabet. There are over 10
26
possible 26-letter keys, so brute-force decryption is in-
feasible. Manual decipherment of substitution ciphers typically starts with frequency analysis, provided
that the ciphertext is sufficiently long, followed by various heuristics (Singh, 1999).
In this paper, we investigate the task of automatically solving substitution ciphers. Complete automa-
tion of the key discovery process remains an active area of research (Ravi and Knight, 2008; Corlett
and Penn, 2010; Nuhn et al., 2013). The task is to recover the plaintext from the ciphertext without the
key, given only a corpus representing the language of the plaintext. The key is a 1-1 mapping between
plaintext and ciphertext alphabets, which are assumed to be of equal length. Without loss of generality,
we assume that both alphabets are composed of the same set of symbols, so that the key is equiva-
lent to a permutation of the alphabet. Accurate and efficient automated decipherment can be applied
to other problems, such as optical character recognition (Nagy et al., 1987), decoding web pages that
utilize an unknown encoding scheme (Corlett and Penn, 2010), cognate identification (Berg-Kirkpatrick
and Klein, 2011), bilingual lexicon induction (Nuhn et al., 2012), machine translation without parallel
training data (Ravi and Knight, 2011), and archaeological decipherment of lost languages (Snyder et al.,
2010).
Our contribution is a novel approach to the problem that combines both character-level and word-level
language models. We formulate decipherment as a tree search problem, and find solutions with beam
search, which has previously been applied to decipherment by Nuhn et al. (2013), or Monte Carlo Tree
Search (MCTS), an algorithm originally designed for games, which can provide accurate solutions in less
time. We compare the speed and accuracy of both approaches. On a benchmark set of variable-length
ciphers, we achieve significant improvement in terms of accuracy over the state of the art. Additional
experiments demonstrate that our approach is robust with respect to the lack of word boundaries and the
presence of noise. In particular, we use it to recover transliteration mappings between different scripts
without parallel data, and to solve the Gold Bug riddle, a classic example of a substitution cipher. Finally,
we investigate the feasibility of deniable encryption with monoalphabetic substitution ciphers.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/
2314
advise the mayor
abcdefghijklmnopqrstuvwxyzOTNPQDUIEBSHJWXCGKRFMYAZLV
OPYERQ FIQ JOLXK
plaintext
key
ciphertext
Figure 1: An example of encryption with a substitution cipher.
The paper is organized as follows. After reviewing previous work on automated decipherment in Sec-
tion 2, we describe our approach to combining character-level and word-level language models with
respect to key scoring (Section 3), and key generation (Section 4). In Section 5, we introduce Monte
Carlo Tree Search and its adaptation to decipherment. In Section 6, we discuss several evaluation exper-
iments and their results. Section 7 is devoted to experiments in deniable encryption.
2 Related Work
Kevin Knight has been the leading proponent of attacking decipherment problems with NLP techniques,
as well as framing NLP problems as decipherment. Knight and Yamada (1999) introduce the topic to
the NLP community by demonstrating how to decode unfamiliar writing scripts using phonetic mod-
els of known languages. Knight et al. (2006) explore unsupervised learning methods, including the
expectation-maximization (EM) algorithm, for a variety of decipherment problems. Ravi and Knight
(2009) formulate the problem of unsupervised transliteration as decipherment in order to reconstruct
cross-lingual phoneme mapping tables, achieving approximately 50% character accuracy on U.S. names
written in the Japanese Katakana script. Reddy and Knight (2011) apply various computational tech-
niques to analyze an undeciphered medieval document. Knight et al. (2011) relate a successful decipher-
ment of a nineteenth-century cipher, which was achieved by combining both manual and computational
techniques.
In the remainder of this section, we focus on the work specifically aimed at solving monoalphabetic
substitution ciphers. Olson (2007) presents a method that improves on previous dictionary-based ap-
proaches by employing an array of selection heuristics. The solver attempts to match ciphertext words
against a word list, producing candidate solutions which are then ranked by ?trigram probabilities?. It
is unclear how these probabilities are computed, but the resulting language model seems deficient. For
example, given a ciphertext for plaintext ?it was a bright cold day in april? (the opening of George Or-
well?s novel Nineteen Eighty-Four), the solver
1
produces ?us far a youngs with had up about?. Our new
approach, which employs word-level language models, correctly solves this cipher.
Ravi and Knight (2008) formulate decipherment as an integer programming problem in which the
objective function is defined by a low-order character language model; an integer program solver then
finds the solution that is optimal with respect to the objective function. This method is slow, precluding
the use of higher order language models. Our reimplementation of their 2-gram solver deciphers ?it was
a bright cold day in april? as ?ae cor o blathe wind dof as oulan?. By contrast, our approach incorporates
word-level information and so tends to avoid out-of-vocabulary words.
Norvig (2009) describes a hill-climbing method that involves both word and character language mod-
els, but the models are only loosely combined; specifically, the word model is used to select the best
solution from a small number of candidates identified by the character model. When applied to the ci-
pher that corresponds to our example sentence from Orwell, the solver
2
returns ?ache red tab scoville
magenta i?.
1
http://www.blisstonia.com/software/Decrypto (accessed August 1, 2013)
2
http://norvig.com/ngrams (accessed June 2, 2014)
2315
Corlett and Penn (2010) use fast heuristic A* search, which can handle much longer ciphers than the
method of Ravi and Knight (2008), while still finding the optimal solution. The authors report results
only on ciphers of at least 6000 characters, which are much easier to break than short ciphers. The
ability to break shorter ciphers implies the ability to break longer ones, but the converse is not true. Our
approach achieves a near-zero error rate for ciphers as short as 128 characters.
Nuhn et al. (2013) set the state of the art by employing beam search to solve substitution ciphers. Their
method is inexact but fast, allowing them to incorporate higher-order (up to 6-gram) character language
models. Our work differs in incorporating word-level information for the generation and scoring of
candidate keys, which improves decipherment accuracy.
3 Key Scoring
Previous work tend to employ either character-level language models or dictionary-type word lists. How-
ever, word-level language models have a potential of improving the accuracy and speed of decipherment.
The information gained from word n-gram frequency is often implicitly used in manual decipherment.
For example, a 150-year old cipher of Edgar Allan Poe was solved only after three-letter ciphertext
words were replaced with high-frequency unigrams the, and, and not.
3
Similarly, a skilled cryptographer
might guess that a repeated ?XQ YWZ? sequence deciphers as the high-frequency bigram ?of the?. We
incorporate this insight into our candidate key scoring function.
On the other hand, our character-level language model helps guide the initial stages of the search
process, when few or no words are discernible, towards English-like letter sequences. In addition, if
the plaintext contains out-of-vocabulary (OOV) words, which do not occur in the training corpus, the
character model will favor pronounceable letter sequences. For example, having identified most of the
words in plaintext ?village of XeYoviY and burned it?, our solver selects pecovic as the highest scoring
word that fits the pattern, which in fact is the correct solution.
In order to assign a score to a candidate key, we apply the key to the ciphertext, and compute the
probability of the resulting letter sequence using a combined language model that incorporates both
character-level and word-level information. With unigram, bigram, and trigram language models over
both words and characters trained on a large corpus, n-gram models of different orders are combined by
deleted interpolation (Jelinek and Mercer, 1980). The smoothed word trigram probability
?
P is:
?
P (w
k
|w
k?2
w
k?1
) = ?
1
P (w
k
) + ?
2
P (w
k
|w
k?1
) + ?
3
P (w
k
|w
k?2
w
k?1
),
such that the ?s sum to 1. The linear coefficients are determined by successively deleting each trigram
from the training corpus and maximizing the likelihood of the rest of the corpus (Brants, 2000). The
probability of text s = w
1
, w
2
, . . . , w
n
according to the smoothed word language model is:
P
W
(s) = P (w
n
1
) =
n
?
k=1
?
P (w
k
|w
k?2
w
k?1
).
The unigram, bigram, and trigram character language models are combined in a similar manner to yield
P
C
(s). The final score is then computed as a linear combination of the log probabilities returned by both
character and word components:
score(s) = ? logP
C
(s) + (1? ?) logP
W
(s),
with the value of ? optimized on a development set. The score of a key is taken to be the score of the
decipherment that it produces.
The handling of the OOV words is an important feature of the key scoring algorithm. An incomplete
decipherment typically contains many OOV words, which according to the above equations would result
in probability P
W
(s) being zero. In order to avoid this problem, we replace all OOV words in a decipher-
ment with a special UNKNOWN token for the computation of P
W
(s). Prior to deriving the word language
models, a sentence consisting of a single UNKNOWN token is appended to the training corpus. As a result,
word n-grams that include an UNKNOWN token are assigned very low probability, encouraging the solver
to favor decipherments containing fewer OOV words.
3
http://www.newswise.com/articles/edgar-allen-poe-cipher-solved
2316
4 Key Mutation
The process of generating candidate keys can be viewed as constructing a search tree, where a modified
key is represented as a child of an earlier key. The root of the tree contains the initial key, which is
generated according to simple frequency analysis (i.e., by mapping the nth most common ciphertext
character to the nth most common character in the training corpus). We repeatedly spawn new tree
leaves by modifying the keys of current leaves, while ensuring that each node in the tree has a unique
key. The fitness of each new key is evaluated by scoring the resulting decipherment, as described in
Section 3. At the end of computation, we return the key with the highest score as the solution.
There are an exponential number of possible keys, so it is important to generate new keys that are
likely to achieve a higher score than the current key. We exploit this observation: any word n-gram can
be represented as a pattern, or sequence, of repeated letters (Table 1). We identify the pattern represented
by each word n-gram in the ciphertext, and find a set of pattern-equivalent n-grams from the training
corpus. For each such n-gram, we generate a corresponding new key from the current key by performing
a sequence of transpositions.
Pattern p-equivalent n-grams
ABCD said, from, have
ABCC will, jazz, tree
ABCA that, says, high
ABCD EFG from you, said the
ABCA ABD that the, says sam
ABC DEEFGBCHICG the bookshelves
Table 1: Examples of pattern-equivalent n-grams.
Pattern-equivalence (abbreviated as p-equivalence) induces an equivalence relation between n-grams
(Moore et al., 1999). Formally, two n-grams u and v are p-equivalent (u
p
? v) if and only if they satisfy
the following three conditions, where stands for the space character:
1. |u| = |v|
2. ?i: u
i
= ? v
i
=
3. ?i, j: u
i
= u
j
? v
i
= v
j
For example, consider ciphertext ?ZXCZ ZXV?. Adopting ?that?, which is p-equivalent to ?ZXCZ?, as a
temporary decipherment of the first word, we generate a new key in which Z maps to t, X to h, and C to
a. This is accomplished by three letter-pair transpositions in the parent key, producing a child key where
?ZXCZ? deciphers to ?that?. Further keys are generated by matching ?ZXCZ? to other p-equivalent words,
such as ?says? and ?high?. The process is repeated for the second word ?ZXV?, and then for the entire
bigram ?ZXCZ ZXV?. Each such match induces a series of transpositions resulting in a new key. Leaf
expansion is summarized in Figure 3.
In order to avoid spending too much time expanding a single node, we limit the number of replace-
ments for each n-gram in the current decipherment to the k most promising candidates, where k is a
parameter optimized on a development set. Note that n-grams excluded in this way may still be included
as part of a higher-order n-gram. For example, if the word birddog is omitted in favor of more promising
candidates, it might be considered as a part of the bigram struggling birddog.
Two distinct modes of ranking the candidate n-grams are used throughout the solving process. In the
initial stage, n-grams are ranked according to the score computed using the method described in Sec-
tion 3. Thus, the potential replacements for a given ciphertext n-gram are the highest scoring p-equivalent
n-grams from the training corpus regardless of the form of the decipherment implied by the current key.
Afterwards, candidates are ranked according to their Hamming distance to the current decipherment,
with score used only to break ties. This two-stage approach is designed to exploit the fact that the solver
typically gets closer to the correct decipherment as the search progresses.
2317
1: Root contains InitialKey
2: for m iterations do
3: recursively select optimal Path from Root
4: Leaf = last node of Path
5: BestLeaf = EXPAND(Leaf, CipherText)
6: append BestLeaf to Path
7: Max = Path node with the highest score
8: assign score of Max to all nodes in Path
Figure 2: MCTS for decipherment.
5 Tree Search
Nuhn and Ney (2013) show that finding the optimal decipherment with respect to a character bigram
model is NP-hard. Since our scoring function incorporates a language model score, choosing an appro-
priate tree search technique is crucial in order to minimize the number of search errors, where the score
of the returned solution is lower than the score of the actual plaintext. In this section we describe two
search algorithms: an adaptation of Monte Carlo Tree Search (MCTS), and a version of beam search.
5.1 Monte Carlo Tree Search
MCTS is a search algorithm for heuristic decision making. Starting from an initial state that acts as the
root node, MCTS repeats these four steps: (1) selection ? starting from the root, recursively pick a child
until a leaf is reached; (2) expansion ? add a set of child nodes to the leaf; (3) simulation ? simulate
the evaluation of the leaf node state; (4) backpropagation ? recursively ascend to the root, updating the
simulation result at all nodes on this path. This process continues until a state is found which passes a
success threshold, or time runs out.
Previous work with MCTS has focused on board games, including Hex (Arneson et al., 2010) and
Go (Enzenberger et al., 2010), but it has also been employed for problems unrelated to game play-
ing (Previti et al., 2011). Although originally designed for two-player games, MCTS has also been ap-
plied to single-agent search (Browne et al., 2012). Inspired by such single-agent MCTS methods (Schadd
et al., 2008; Matsumoto et al., 2010; M?hat and Cazenave, 2010), we frame decipherment as a single-
player game with a large branching factor, in which the simulation step is replaced with a heuristic
scoring function. Since we have no way of verifying that the current decipherment is correct, we stop
after performing m iterations. The value of m is determined on a development set.
The function commonly used for comparing nodes in the tree is the upper-confidence bound (UCB)
formula for single-player MCTS (Kocsis and Szepesv?ri, 2006). The formula augments our scoring
function from Section 3 with an additional term:
UCB(n) = score(n) + C
?
ln(v(p(n)))
v(n)
where p(n) is the parent of node n, and v(n) is the number of times that n has been visited. The second
term favors nodes that have been visited relatively infrequently in comparison with their parents. The
value of C is set on a development set.
Figure 2 summarizes our implementation. Each iteration begins by finding a path through the tree that
is currently optimal according to the UCB. The path begins at the root, includes a locally optimal child
at each level, and ends with a leaf. The leaf is expanded using the function EXPAND shown in Figure 3.
The highest-scoring of the generated children is then appended to the optimal path. If the score of the
new leaf (not the UCB) is higher than the score of its parent, we backpropagate that score to all nodes
along the path leading from the root. This encourages further exploration along all or part of this path.
5.2 Beam Search
Beam search is a tree search algorithm that uses a size-limited list of nodes currently under consideration,
which is referred to as the beam. If the beam is full, a new node can be added to it only if it has a higher
2318
1: function EXPAND(Leaf, CipherText)
2: for all word n-grams w in CipherText do
3: for k best w
?
s.t. w
?
p
? w do
4: NewLeaf = Modify(Leaf, w 7? w
?
)
5: if NewLeaf not in the tree then
6: add NewLeaf as a child of Leaf
7: if score(NewLeaf) > score(BestLeaf) then
8: BestLeaf = NewLeaf
9: return BestLeaf
Figure 3: Leaf expansion.
score than at least one node currently in the beam. In such a case, the lowest-scoring node is removed
from the beam and any further consideration.
Nuhn et al. (2013) use beam search for decipherment in their character-based approach. Starting from
an empty root node, a partial key is extended by one character in each iteration, so that each level of the
search tree corresponds to a unique ciphertext symbol. The search ends when the key covers the entire
ciphertext.
By contrast, we apply beam search at the word n-gram level. The EXPAND subroutine defined in Fig-
ure 3 is repeatedly invoked for a specified number of iterations (a tunable parameter). In each iteration,
the algorithm analyzes a set of word n-gram substitutions, which may involve multiple characters, as de-
scribed in Section 4. The search stops early if the beam becomes empty. On short ciphers (32 characters
or less), the best solution is typically found within the first five iterations, but this can only be confirmed
after the search process is completed.
6 Experiments
In order to evaluate our approach and compare it to previous work, we conducted several experiments.
We created three test sets of variable-length ciphers: (1) with spaces, (2) without spaces, and (3) with
spaces and added encipherment noise. In addition, we tested our system on Serbian Cyrillic, and the
Gold Bug cipher.
We derive our English language models from a subset of the New York Times corpus (LDC2003T05)
containing 17M words. From the same subset, we obtain letter-frequency statistics, as well as the lists
of p-equivalent n-grams. For comparison, Ravi and Knight (2008) use 50M words, while Nuhn et al.
(2013) state that they train on a subset of the Gigaword corpus without specifying its size.
6.1 Substitution Ciphers
Following Ravi and Knight (2008) and Nuhn et al. (2013), we test our approach on a benchmark set
of ciphers of lengths, 2, 4, 8, . . . , 256, where each length is represented by 50 ciphers. The plaintexts
are randomly extracted from the Wikipedia article on History, which is quite different from our NYT
training corpus. Spaces are preserved, and the boundaries of the ciphers match word boundaries.
Figure 4 shows the decipherment error rate of the beam-search version of our algorithm vs. the pub-
lished results of the best-performing variants of Ravi and Knight (2008) and Nuhn et al. (2013): letter
3-gram and 6-gram, respectively. The decipherment error rate is defined as the ratio of the number of
incorrectly deciphered characters to the length of the plaintext. Our approach achieves a statistically sig-
nificant improvement on ciphers of length 8 and 16. Shorter ciphers are inherently hard to solve, while
the error rates on longer ciphers are close to zero. Unfortunately, Nuhn et al. (2013) only provide a graph
of their error rates, which in some cases prevents us from confirming the statistical significance of the
improvements (c.f. Table 2).
Examples of decipherment errors are shown in Table 3. As can be seen, the proposed plaintexts
are often perfectly reasonable given the cipher letter pattern. The solutions proposed for very short
ciphers are usually high-frequency words; for example, the 2-letter ciphers matching the pattern ?AB?
2319
Figure 4: Average decipherment error rate as a function of cipher length on the Wikipedia test set.
Figure 5: Average decipherment error rate as a function of cipher length on the NYT test set.
Wikipedia NYT
with spaces with spaces no spaces noisy
Beam MCTS Greedy Beam MCTS MCTS Beam MCTS
2 58.00 58.00 58.00 81.00 81.00 75.00 83.00 83.00
4 83.00 83.00 83.00 66.00 66.00 77.50 83.50 83.50
8 52.50 52.50 52.50 49.00 49.00 55.71 73.50 73.50
16 10.50 12.62 18.50 13.50 14.50 55.00 69.75 69.38
32 2.12 6.12 10.88 0.88 0.94 28.57 46.81 50.44
64 0.56 0.72 2.50 0.03 0.03 7.85 16.66 25.47
128 0.14 0.16 0.16 0.00 1.61 0.87 5.20 5.41
256 0.00 0.00 0.10 0.02 0.02 0.00 2.73 2.75
Table 2: Average decipherment error rate of our solver as a function of cipher length on the Wikipedia
and the NYT test sets.
2320
Cipher length Cipher pattern Actual plaintext Decipherment
2 AB to of
4 ABCD from said
4 ABBC look been
8 ABCDCEFG slobodan original
8 ABCDE FG filed by would be
16 ABCCDEE BFG HBCI jarrett and mark carroll and part
16 ABCDE FGCHA IJKL group along with drugs would make
Table 3: Examples of decipherment errors.
are invariably deciphered as ?of ?. The errors in ciphers of length 32 or more tend to be confined to
individual words, which are often OOV names.
6.2 Beam Search vs. MCTS
The error rates of the two versions of our algorithm are very close, with a few exceptions (Table 2). Out
of 400 ciphers with spaces in the Wikipedia test set, the MCTS variant correctly solves 260 out of 400
ciphers, compared to 262 when beam search is used. In 9 MCTS solutions and 3 beam search solutions,
the score of the proposed decipherment is lower than the score of the actual plaintext, which indicates a
search error.
By setting the beam size to one, or the value of C in MCTS to zero, the two search techniques are
reduced to greedy search. As shown in Table 2, in terms of accuracy, greedy search is worse than MCTS
on the lengths of 16, 32, and 64, and roughly equal on other lengths. This suggests that an intelligent
search strategy is important for obtaining the best results.
In terms of speed, the MCTS version outperforms beam search, thanks to a smaller number of ex-
panded nodes in the search tree. For example, it takes on average 9 minutes to solve a cipher of length
256, compared to 41 minutes for the beam search version. Direct comparison of the execution times with
the previous work is difficult because of variable computing configurations, as well as the unavailability
of the implementations. However, on ciphers of the length of 128, our MCTS version takes on average
197 seconds, which is comparable to 152 seconds reported by Nuhn et al. (2013), and faster than our
reimplementation of the bigram solver of Ravi and Knight (2008) which takes on average 563 seconds.
The trigram solver of Ravi and Knight (2008) is even slower, as evidenced by the fact that they report no
corresponding results on ciphers longer than 64 letters.
6.3 Noisy Ciphers
Previous work has generally focused on noise-free ciphers. However, in real-life applications, we may
encounter cases of imperfect encipherment, in which some characters are incorrectly mapped. Corlett
and Penn (2010) identify the issue of noisy ciphers as a worthwhile future direction. Adding noise also
increases a cipher?s security, as it alters the pattern of letter repetitions in words. In this section, we
evaluate the robustness of our approach in the presence of noise.
In order to quantify the effect of adding noise to ciphers, we randomly corrupt log
2
(n) of the ciphertext
letters, where n is the length of the cipher. Our results on such ciphers are shown in Table 2. As
expected, adding noise to the ciphertexts increases the error rate in comparison with ciphers without
noise. However, our algorithm is still able to break most of the ciphers of length 64 and longer, and
makes only occasional mistakes on ciphers of length 256. Beam search is substantially better than MCTS
only on lengths of 32 and 64. These results indicate that our word-oriented approach is reasonably robust
with respect to the presence of noise.
6.4 Croatian and Serbian
We further test the robustness of our approach by performing an experiment on decipherment of an
unknown script. For this experiment, we selected Croatian and Serbian, two closely related languages
2321
Figure 6: The decipherment error rate on a Serbian sample text as a function of the ciphertext length.
that are written in different scripts (Latin and Cyrillic). The correspondence between the two script
alphabets is not exactly one-to-one: Serbian Cyrillic uses 30 symbols, while Croatian Latin uses 27. In
particular, the Cyrillic characters ?, ?, and 
 are represented in the Latin script as digraphs lj, nj, and
d?. In addition, there are differences in lexicon and grammar between the two languages, which make
this task a challenging case of noisy encipherment.
In the experiment, we treat a short text in Serbian as enciphered Croatian and attempt to recover the
key, which in this case is the mapping between the characters in the two writing scripts. Each letter
with a diacritic is considered as different from the same letter with no diacritic. We derive the word
and character language models from the Croatian part of the ECI Multilingual Corpus, which contains
approximately 720K word tokens. For testing, we use a 250-word, 1583-character sample from the
Serbian version of the Universal Declaration of Human Rights.
sva ?udska bic?a raaju se slobodna i jednaka u dostojanstvu i pravima ona su obdarena razumom i svexc?u i treba jedni prema drugima
sva ? udska b i ha r a l aju se s ?obodna i jednaka u dos t ojans t vu i p r av i ma ona su obda r ena r a ?cumom i sve c hu i t r eba jedn i p r ema d r uz i ma
Table 4: Serbian Cyrillic deciphered as Croatian. The decipherment errors are shown in boldface.
The decipherment error rate on the Serbian ciphertext drops quickly, leveling at about 3% at the length
of 50 words (Figure 6). The residual error rate reflects the lack of correct mapping for the three Serbian
letters mentioned above. As can be seen in Table 4, the actual decipherment of a 30-word ciphertext
contains only a handful of isolated errors. On the other hand, a pure frequency-based approach fails on
this task with a mapping error rate close to 90%.
6.5 Ciphers Without Spaces
Removing spaces that separate words is another way of increasing the security of a cipher. The assump-
tion is that the intended recipient, after applying the key, will still be able to guess the location of word
boundaries, and recover the meaning of the message. We are interested in testing our approach on such
ciphers, but since it is dependent on word language models, we need to first modify it to identify word
boundaries. In particular, the two components that require word boundaries are the scoring function
(Section 3), and the search tree node expansion (Section 5).
In order to compute the scoring function, we try to infer word boundaries in the current decipherment
using the following simple greedy algorithm. The current decipherment is scanned repeatedly from left
to right in search for words of length L, where L gradually decreases from the length of the longest
word in the training corpus, down to the minimal value of 2. If a word is found, the process is applied
recursively to both remaining parts of the ciphertext. We use a fast greedy search instead of a slower
but more accurate dynamic programming approach as this search must be executed each time a key is
evaluated.
In the search tree node expansion step, for each substring of length at least 2 in the current decipher-
ment, we attempt to replace it with all pattern-equivalent n-grams (with spaces removed). As a result,
2322
53???305))6
*
;4826)4?.)4?);806
*
;48?8?60))85;1?(;:?
*
8?83(88)5
*
?;46(;88
*
96
*
?;8)
*
?(;485);5
*
?2:
*
?(;4956
*
2(5
*
-4)8?8
*
;4069285);)6?8)4
agoodglassinthebishopshostelinthedevilsseatfortyonedegreesandthirteenminutesnortheastandbynorthmainbranchseventhlimbeastsidesh
Table 5: The beginning of the Gold Bug cipher and its decipherment.
each key spawns a large number of children, increasing both time and memory usage. Overall, the mod-
ified algorithm is as much as a hundred times slower than the original algorithm. However, when MCTS
is used as search method, we are still able to perform the decipherment in reasonable time.
For testing, we remove spaces from both the plaintexts and ciphertexts, and reduce the number of
ciphers to 10 for each cipher length. Our results, shown in Figure 5, compare favorably to the solver
of (Norvig, 2009), which is designed to work on ciphers without spaces.
The final test of our decipherment algorithm is the cipher from The Gold Bug by Edgar Alan Poe.
In that story, the 204-character cipher gives the location of hidden treasure. Our implementation finds
a completely correct solution, the beginning of which is shown in Table 5. Both experiments reported
in this section confirm that our word-based approach works well even when spaces are removed from
ciphers.
7 Deniable Encryption
In one of Stanis?aw Lem?s novels, military cryptographers encipher messages in such a way that the
ciphertext appears to be plain text (Lem, 1973). Canetti et al. (1997) investigate a related idea, in which
the ciphertext ?looks like? an encryption of a plaintext that is different from the real message. In the
context of monoalphabetic substitution ciphers, we define the task as follows: given a message, find an
encipherment key yielding a ciphertext that resembles natural language text. For example, ?game with
planes? is a deniable encryption of the message ?take your places? (the two texts are p-equivalent).
We applied our solver to a set of sentences from the text of Nineteen Eighty-Four, treating each sen-
tence as a ciphertext. In order to ensure that the alternative plaintexts are distinct from the original
sentences, we modified our solver to disregard candidate keys that yield a solution containing a content
word from the input. For example, ?fine hours? was not deemed an acceptable deniable encryption of
?five hours?. With this condition added, alternative plaintexts were produced for all 6531 sentences.
Of these, 1464 (22.4%) were determined to be composed entirely of words seen in training. However,
most of these deniable encryptions were either non-grammatical or differed only slightly from the actual
plaintexts. It appears that substitution ciphers that preserve spaces fail to offer sufficient flexibility for
finding deniable encryptions.
In the second experiment, we applied our solver to a subset of 757 original sentences of length 32 or
less, with spaces removed. The lack of spaces allows for more flexibility in finding deniable encryptions.
For example, the program finds ?draft a compromise? as a deniable encryption of ?zeal was not enough?.
None of the produced texts contained out-of-vocabulary words, but most were still ungrammatical or
nonsensical. Allowing for some noise to be introduced into the one-to-one letter mapping would likely
result in more acceptable deniable encryptions, but our current implementation can handle noise only on
the input side.
8 Conclusion
We have presented a novel approach to the decipherment of monoalphabetic substitution ciphers that
combines character and word-level language models. We have proposed Monte Carlo Tree Search as
a fast alternative to beam search on the decipherment task. Our experiments demonstrate significant
improvement over the current state of the art. Additional experiments show that our approach is robust in
handling ciphers without spaces, and ciphers with noise, including the practical application of recovering
transliteration mappings between Serbian and Croatian.
In the future, we would like to extend our approach to handle homophonic ciphers, in which the one-
to-one mapping restriction is relaxed. Another interesting direction is developing algorithms to generate
syntactically correct and meaningful deniable encryptions.
2323
Acknowledgements
This research was supported by the Natural Sciences and Engineering Research Council of Canada and
the Alberta Innovates Technology Futures.
References
Broderick Arneson, Ryan B Hayward, and Philip Henderson. 2010. Monte Carlo Tree Search in Hex. IEEE
Transactions on Computational Intelligence and AI in Games, 2(4):251?258.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In
Empirical Methods in Natural Language Processing, pages 313?321.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tagger. In Proceedings of the Sixth Conference on
Applied Natural Language Processing, pages 224?231.
Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen,
Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. 2012. A survey of Monte Carlo tree
search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1?43.
Ran Canetti, Cynthia Dwork, Moni Naor, and Rafi Ostrovsky. 1997. Deniable encryption. In Advances in
Cryptology?CRYPTO?97, pages 90?104.
Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering letter-substitution ciphers. In the 48th
Annual Meeting of the Association for Computational Linguistics, pages 1040?1047.
Markus Enzenberger, Martin Muller, Broderick Arneson, and Richard Segal. 2010. Fuego ? an open-source frame-
work for board games and go engine based on Monte Carlo tree search. IEEE Transactions on Computational
Intelligence and AI in Games, 2(4):259?270.
F. Jelinek and R.L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. Pattern
recognition in practice.
Kevin Knight and Kenji Yamada. 1999. A computational approach to deciphering unknown scripts. In ACL
Workshop on Unsupervised Learning in Natural Language Processing, pages 37?44.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment
problems. In the COLING/ACL 2006 Main Conference Poster Sessions, pages 499?506.
Kevin Knight, Be?ta Megyesi, and Christiane Schaefer. 2011. The Copiale cipher. In the 4th Workshop on
Building and Using Comparable Corpora: Comparable Corpora and the Web, pages 2?9.
Levente Kocsis and Csaba Szepesv?ri. 2006. Bandit based Monte-Carlo Planning. In Johannes F?rnkranz, Tobias
Scheffer, and Myra Spiliopoulou, editors, Euro. Conf. Mach. Learn., pages 282?293, Berlin, Germany. Springer.
Stanis?aw Lem. 1973. Memoirs found in a bathtub. The Seabury Press.
Shimpei Matsumoto, Noriaki Hirosue, Kyohei Itonaga, Kazuma Yokoo, and Hisatomo Futahashi. 2010. Evalua-
tion of simulation strategy on single-player Monte-Carlo tree search and its discussion for a practical scheduling
problem. In the International MultiConference of Engineers and Computer Scientists, volume 3, pages 2086?
2091.
Jean M?hat and Tristan Cazenave. 2010. Combining UCT and nested Monte Carlo search for single-player general
game playing. IEEE Transactions on Computational Intelligence and AI in Games, 2(4):271?277.
Dennis Moore, W.F. Smyth, and Dianne Miller. 1999. Counting distinct strings. Algorithmica, 23(1):1?13.
George Nagy, Sharad Seth, and Kent Einspahr. 1987. Decoding substitution ciphers by means of word matching
with application to ocr. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9(5):710?715.
Peter Norvig. 2009. Natural language corpus data. In Toby Segaran and Jeff Hammerbacher, editors, Beautiful
data: the stories behind elegant data solutions. O?Reilly.
Malte Nuhn and Hermann Ney. 2013. Decipherment complexity in 1:1 substitution ciphers. In the 51st Annual
Meeting of the Association for Computational Linguistics, pages 615?621.
2324
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language
models and context vectors. In the 50th Annual Meeting of the Association for Computational Linguistics,
pages 156?164.
Malte Nuhn, Julian Schamper, and Hermann Ney. 2013. Beam search for solving substitution ciphers. In the 51st
Annual Meeting of the Association for Computational Linguistics, pages 1568?1576.
Edwin Olson. 2007. Robust dictionary attack of short simple substitution ciphers. Cryptologia, 31(4):332?342.
Alessandro Previti, Raghuram Ramanujan, Marco Schaerf, and Bart Selman. 2011. Applying UCT to Boolean
satisfiability. In Theory and Applications of Satisfiability Testing-SAT 2011, pages 373?374. Springer.
Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models.
In Empirical Methods in Natural Language Processing, pages 812?819.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme mappings for transliteration without parallel data. In
NAACL, pages 37?45.
Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies, pages 12?21.
Sravana Reddy and Kevin Knight. 2011. What we know about the Voynich manuscript. In the 5th ACL-HLT
Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 78?86.
Maarten PD Schadd, Mark HM Winands, H Jaap Van Den Herik, Guillaume MJ-B Chaslot, and Jos WHM Uiter-
wijk. 2008. Single-player Monte-Carlo tree search. In Computers and Games, pages 1?12. Springer.
Simon Singh. 1999. The Code Book: The Science of Secrecy from Ancient Egypt to Quantum Cryptography.
Random House.
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment.
In the 48th Annual Meeting of the Association for Computational Linguistics, pages 1048?1057.
2325
Proceedings of NAACL-HLT 2013, pages 634?643,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Automatic Generation of English Respellings
Bradley Hauer and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{bmhauer,gkondrak}@ualberta.ca
Abstract
A respelling is an alternative spelling of a
word in the same writing system, intended to
clarify pronunciation. We introduce the task
of automatic generation of a respelling from
the word?s phonemic representation. Our ap-
proach combines machine learning with lin-
guistic constraints and electronic resources.
We evaluate our system both intrinsically
through a human judgment experiment, and
extrinsically by passing its output to a letter-
to-phoneme converter. The results show that
the respellings generated by our system are
better on average than those found on the Web,
and approach the quality of respellings de-
signed by an expert.
1 Introduction
Respellings are a widely employed method of con-
veying the pronunciation of English and foreign
words, both in print and on the Web. For example,
Huatulco, the name of a Mexican resort, is respelled
as ?wah-tool-koh? in a travel guide (Noble, 2012).
The advantage of using respellings lies in removing
the need for a separately defined phonetic transcrip-
tion system. Since they contain only the letters of
the Latin alphabet, their phonetic interpretation re-
lies exclusively on orthographic intuitions of read-
ers. For this reason, respellings are widely used in
travel phrase books, medical compendia, and drug
name pronunciation guides, among others.
Despite their utility, good respellings are not easy
to create. Respellings found on the Web often con-
tain errors or ambiguities. For example, Henoch-
Schoenlein purpura, a skin disease, is respelled both
as ?heh-nok shoon-line purr-puh-ruh? and ?hen-awk
sher-line purr-purr-ah?. Does ?heh? rhyme with eh
[e] or with Nineveh [@], or is it the same vowel as
in hen [E]? Clearly, if both respellings refer to the
same pronunciation, at least one of them must be
wrong. In addition, converting the pronunciation of
a foreign name to English phonemes is in itself a
non-trivial task.
In this paper, we focus on the task of generating
respellings from the intended pronunciation given as
a sequence of phonemes. We develop a stand-alone
system that combines linguistic knowledge and re-
sources with machine learning models trained on
data mined from the Web and electronic dictionar-
ies. One of our ultimate objectives is to aid writ-
ers by evaluating their respellings, improving them,
or generating new candidates. Accordingly, we en-
deavour to maintain the generation and the evalua-
tion stages as separate modules in our system.
The evaluation of respellings is a challenging
problem. Since English spelling conventions are no-
toriously inconsistent, there is no algorithm for ac-
curately predicting the pronunciation of an out-of-
vocabulary word. The current state-of-the-art letter-
to-phoneme (L2P) converters are typically reported
with 10-30% error rates on dictionary words (Bisani
and Ney, 2008). On the other hand, human read-
ers often disagree on the details of the pronunciation
implied by a respelling. In this paper, we conduct
two kinds of evaluations: an automated verification
with an independent L2P system, and an experiment
with human participants that pass judgments on dif-
ferent respellings of the same word. We interpret
the results as evidence that the output of our system
compares favourably with typical respellings found
on the Web.
634
2 Definitions and Conventions
Although Chomsky and Halle (1968) characterize
English orthography as close to optimal, Kominek
and Black (2006) estimate that it is about 3 times
more complex than German, and 40 times more
complex than Spanish. This is confirmed by
lower accuracy of letter-to-phoneme systems on En-
glish (Bisani and Ney, 2008). A survey of English
spelling (Carney, 1994) devotes 120 pages to de-
scribe phoneme-to-letter correspondences, and lists
226 letter-to-phoneme rules, almost all of which ad-
mit exceptions.
There is no consensus on how to best convey the
pronunciation of an uncommon word in English.
Most dictionaries employ either the International
Phonetic Alphabet (IPA), or their own transcription
schemes that incorporate special symbols and dia-
critics. Unfortunately, many readers are unfamiliar
with phonetic transcription. Instead, respellings are
often preferred by writers in the news and on the
Web. In this section, we define the respelling task in
detail.
2.1 Form of Respellings
A respelling is a non-standard spelling of a word,
that is intended to better convey its pronunciation.
We assume that the pronunciation is defined as a se-
quence of English phonemes, and that the respelling
contains only the 26 letters of the alphabet, with
optional hyphenation. Some transcription schemes
combine respellings with special symbols for repre-
senting certain phonemes. For example, an other-
wise purely alphabetic Wikipedia scheme employs
the symbol @ for the vowel schwa. In our opin-
ion, such devices destroy the main advantage of re-
spellings, which is their universality, without attain-
ing the precision of a true phonetic transcription. In
fact, Fraser (1997) identifies the schwa symbol as
the cause of many pronunciation errors.
In our system, we consistently use hyphens to
segment multi-syllable respellings. Each syllable-
size segment contains the representation of exactly
one vowel phoneme, so that the number of segments
matches the number of syllables.1 However, the hy-
phenation need not correspond exactly to the actual
1Henceforth, we refer to ?syllable-size segments? simply as
?syllables?.
syllable breaks. This approach has several advan-
tages. First, individual syllables are easier to pro-
nounce than an entire unfamiliar word. Second, hy-
phens limit the context that affects the pronuncia-
tion of a given letter (e.g. th in Beethoven ?bayt-
hoe-ven?). Finally, hyphens indicate whether adja-
cent vowel letters, such as oe in ?hoe?, represent one
vowel phoneme or two.
Some respellings explicitly indicate the stressed
syllable by expressing it in a different font. This is
potentially helpful because unstressed vowels tend
to be reduced, which changes their pronunciation.
However, since the vowel reduction phenomenon is
by no means universal, the readers may be unsure
whether to apply it to, e.g. the final o in ?KWAT-
ro?. In this paper, we make no distinction between
stressed and unstressed syllables; instead, we follow
the principle that each syllable is to be pronounced
as if it was a separate word. Nonetheless, it would be
straightforward to project the stress indicators onto
the appropriate syllables in the respellings generated
by our system.
2.2 Quality of Respellings
There is no clear-cut distinction between good and
bad respellings. The quality of a respelling is more
of a subjective opinion rather than a verifiable fact.
We propose to evaluate it according to the follow-
ing three criteria: ambiguity, correctness, and pref-
erence.
A respelling is ambiguous if it is perceived as
compatible with more than one pronunciation. Be-
cause most of the rules of English spelling have ex-
ceptions, it is rarely possible to demonstrate that
a respelling is completely unambiguous. How-
ever, some respellings are clearly more ambiguous
than others. For example, the digraph ee almost
always represents the vowel [i], whereas the let-
ter sequence ough can represent several different
phonemes.2 Respellings that contain highly ambigu-
ous letter-phoneme mappings can be expected to be
ambiguous themselves. Ambiguity is a property of a
respelling itself, regardless of the intended pronun-
ciation.
A respelling is correct if it accurately conveys the
intended pronunciation to the reader. Unlike the am-
2Compare bough, cough, dough, tough, lough, through.
635
biguity, correctness can be verified objectively for
a particular reader, by comparing the intended pro-
nunciation with the pronunciation inferred by the
reader. A respelling that is judged correct with re-
spect to one pronunciation cannot be judged correct
with respect to a different pronunciation. Never-
theless, it is entirely possible that different readers
will derive different pronunciations from the same
respelling.
A respelling can be classified as unambiguous
and yet incorrect by a given reader, but it cannot
be judged as simultaneously ambiguous and cor-
rect. Indeed, an ambiguous respelling is compatible
with at least two pronunciations, only one of which
can be the intended pronunciation. Therefore, for
a given reader, unambiguity is a necessary but not
sufficient condition for correctness.
Given two unambiguous and correct respellings,
a reader may prefer one over the other, perhaps be-
cause of the ease of inferring the intended pronun-
ciation. For example, ?rode-ease-yew? may be pre-
ferred to ?roh-dee-zyoo? because the former is en-
tirely composed of actual English words with unique
pronunciation, whereas the latter contains an un-
usual consonant cluster zy. Preference is also ex-
pressed implicitly if only one of the alternative re-
spellings is judged as unambiguous (or correct),
3 Related Work
Fraser (1997) describes an experiment in which 15
human subjects were asked to pronounce uncom-
mon words after being shown a representation of
their pronunciation. The respellings designed by the
author were much more effective for that purpose
than either the IPA phonetic transcription or phone-
mic respelling (Section 4.3). However, the creation
of respellings was described as labour-intensive, and
at least one of them was found to be sub-optimal dur-
ing the experiment.
Williams and Jones (2008) propose respellings as
a way of extending pronunciation lexicons by infor-
mants who lack linguistic training. Galescu (2009)
reports that the addition of respellings of medical
terms from an on-line dictionary improves the ac-
curacy of an L2P system. The author identifies an
automatic pronunciation-to-respelling system as fu-
ture work.
Ghoshal et al (2009) extract a large number of re-
spellings from the Web, and show that they can be
exploited to improve the accuracy of the L2P con-
version by supplementing the data in pronunciation
dictionaries. Can et al (2009) further analyze the ef-
fect of using respellings on the accuracy of spoken-
term detection (STD) systems.
4 Direct Methods
In this section, we discuss three direct methods of
generating respellings: manual design, dictionary
lookup, and phonemic respelling.
4.1 Manual Design
Respellings found on the Web and in news articles
are usually ad-hoc creations of the authors of those
texts. Respellings designed by different writers for
the same word are rarely identical.3 The quality of
Web respellings vary.
The respellings found in specialized lexicons are
more likely to be designed by experts, and are of-
ten guided by a set of respelling rules. Nevertheless,
such respelling guides may also be ambiguous.4 Re-
gardless of the source, since respellings are often
used for names and foreign words, no lexicon can
be expected to provide a complete coverage.
4.2 Dictionary Lookup
Pronunciation dictionaries can be helpful in gener-
ating respellings. Assuming that we have a method
of dividing pronunciations into syllables, a complete
respelling of an out-of-dictionary word can in some
cases be automatically derived from the list of syl-
lable pronunciations. For example, hyphy can be re-
spelled as ?high-fee? by following such a procedure.
If each of the syllables has a unique pronunciation,
such respellings are arguably both unambiguous and
correct.
Unfortunately, only a subset of potential phone-
mic syllables actually occur in a lexicon. Consider-
ing only the syllables of the CVC type (consonant-
vowel-consonant), there are over ten thousand dis-
tinct possibilities (e.g., [bEb], [bES], etc.), of which
3For example, the word capoeira is represented by 99 dif-
ferent respellings in the corpus of Ghoshal et al (2009).
4For an example of a confusing respelling guide see http:
//www.ama-assn.org/go/usan.
636
fewer than three thousand can be found in the Com-
bilex pronunciation dictionary (Richmond et al,
2009). While the dictionary lookup may produce
attractive respellings, it is not sufficient for a stand-
alone use.
4.3 Phonemic Respelling
A simple method that can produce a respelling for
any word is to directly map each phoneme to a par-
ticular letter or a letter sequence that is frequently
used to represent that phoneme. Phonemes such as
[m], [d] and [f] are indeed closely associated with in-
dividual letters. This is not surprising since the Ro-
man letters were originally created to represent sin-
gle phonemes in Latin, and some of those phonemes
also exist in English. However, many phonemes, es-
pecially vowels, have no obvious orthographic rep-
resentation. One solution is to use digraphs such as
ee and aw, but a number of phonemes, such as [aU]
as in loud, have no mappings that work in all con-
texts.
The principal weakness of a phonemic respelling
is its inflexibility, which often results in counter-
intuitive respellings. For example, many readers are
baffled by respelling such as ?gee? for ghee or ?john?
for Joan. Phonemic respelling tends to fail in cases
where it generates a sequence of letters that is inher-
ently ambiguous, or which pronunciation changes
because of the context. On the other hand, mappings
such as uu for [U] and ahy for [aI], which never oc-
cur in real English words, are difficult to interpret
for some readers.
In this paper, we adopt a context-free phonemic
respelling scheme as the baseline, with the mappings
from the online dictionary Dictionary.com, which
differs from the system used in Wikipedia only in
a few details.
5 Candidate Generation
In this section, we present our syllabification ap-
proach, as well as two generation modules: a trained
phoneme-to-letter (P2L) model and a rule-based re-
speller.
5.1 Syllabification
Our respelling generation process is for the most
part performed on the level of individual syllables.
VOWEL ONSET LAX CODA
nt *
nd@n *
b? *
d@nm *
b?n
Table 1: Examples of syllables that violate phonotactic
constraints.
Correct syllabification is by itself a non-trivial prob-
lem, but even if it was provided by an oracle, it might
not correspond to the optimal segmentation of a re-
spelling. For example, the word trigonal [trIg@n@l]
is usually syllabified as tri-go-nal, but a better seg-
mentation for the purposes of respelling is trig-on-
al. We adopt an overgenerate-and-rank approach,
whereby instead of committing to a specific word
segmentation at the start of the process, we process
multiple syllabification alternatives in parallel, one
of which is ultimately selected at the respelling eval-
uation stage.
Ideally, syllabification should conform to the
phonotactic constraints of English, so that the result-
ing respellings are easy to pronounce. The conso-
nant sonority should be rising in onsets, and falling
in codas (Kenstowicz, 1994). We verify that sylla-
bles follow the sonority principle by following the
formulation of Bartlett et al (2009). The sonor-
ity constraints are not tested at the boundaries of
the word, which are independent of the syllabifica-
tion choice. We also incorporate another important
principle of English phonotactics that asserts that
lax vowels do not occur in open syllables (Rogers,
2000).
In our implementation, each candidate syllable is
tested with respect to the following sequence of four
violable constraints, ordered from the strongest to
the weakest: (1) the syllable contains exactly one
vowel phoneme; (2) the onset satisfies the sonority
principle; (3) if the nucleus contains a lax vowel (ex-
cept @), the coda is non-empty; (4) the coda satis-
fies the sonority principle. For a syllabification to be
accepted, all its syllables must satisfy the four con-
straints. However, if this results in rejection of all
possible syllabifications, the constraints are gradu-
ally relaxed starting from the weakest.
637
As an example, consider the word abandonment
[@b?nd@nm@nt], which has 18 different syllabifica-
tions satisfying the VOWEL constraint (Table 1). 8
of the 18 satisfy the ONSET constraint as well, but
only two syllabifications satisfy all four constraints:
[@b-?n-d@n-m@nt] and [@-b?n-d@n-m@nt].
5.2 P2L Generator
The respelling problem can be viewed as a string
transduction problem, with the transduction occur-
ring between phonemes and letters. As such, it is di-
rectly related to the well-studied letter-to-phoneme
conversion task. The difference is that the letters
may not conform to the standard orthography of
English. If we had a sufficiently large training set
of pronunciation-respelling pairs, we could train a
machine learning algorithm to directly generate re-
spellings for any strings of English phonemes. How-
ever, such a training set is not readily available. The
respellings in the corpus collected by Ghoshal et al
(2009) are not easily matched to the phonetic tran-
scriptions, and few of them can be found in elec-
tronic pronunciation dictionaries. In addition, the
quality of Web respellings vary greatly.
In place of a direct pronunciation-to-respelling
model, we aim to model the orthographic intuitions
of readers by deriving a phoneme-to-letter (P2L)
transduction model from an English pronunciation
dictionary. A possible criticism of such an approach
is that our model may create ambiguous respellings,
which abound in English orthography. However, we
rely on a separate evaluation module to identify and
filter ambiguous respellings at a later stage.
Our systems utilizes the DIRECTL+ program (Ji-
ampojamarn et al, 2008), which was originally de-
signed for L2P conversion. Since our basic unit is
the syllable, rather than the word, we train our P2L
model on a set of of 4215 pairs of monosyllabic
words and their pronunciations extracted from the
Combilex dictionary. We exclude syllables in multi-
syllabic words from training because their pronunci-
ation is often affected by context. This is consistent
with our expectation that the reader will pronounce
each hyphen-delimited segment of the respelling as
if it was an individual word.
Since the P2L training data consists of a relatively
small set of syllables, we ensure that the phoneme-
letter alignment is highly accurate. As a preprocess-
ing step, we replace the letter x with ks, and we con-
vert digraphs, such as ch and th, to single symbols.
The alignment is performed by M2M-ALIGNER (Ji-
ampojamarn et al, 2007), under the restriction that
each phoneme is matched to either one or two letter
symbols.
5.3 Context-Sensitive Respeller
A hand-crafted context-sensitive respeller is in-
tended to complement the trained P2L model de-
scribed in the previous section. It is similar to to
the phonemic respelling approach described in Sec-
tion 4.3 in that it converts each phoneme to a letter
sequence. However, the mappings depend on adja-
cent phonemes, as well as on the CV pattern of the
current syllable. In addition, more than one map-
ping for a phoneme can be proposed. We designed
the mappings by analyzing their frequency and con-
sistency in pronunciation dictionaries.
The process of candidate generation involves es-
tablishing the pattern of consonants in the input syl-
lable. The consonant mappings are the same as in
the baseline, except for [?] and [T], while the vowels
yield up to three different letter sequences. For ex-
ample, [o] is mapped to oh as a default, but also to o
if both onset and coda are empty, or to o followed by
a consonant and a silent e if the coda is composed of
a single consonant. So, given the syllable [tok] as in-
put, the respeller produces two candidates: tohk and
toke.
We make no claims about the completeness or op-
timality of the mappings, but in our development
experiments we observed that the context-sensitive
respeller contributes to the robustness of our sys-
tem, and in some cases produces more attractive re-
spellings that the P2L model.
6 Candidate Selection
We aim at developing a stand-alone method for the
assessment of respellings that could be applied re-
gardless of their origin. We consider two criteria:
correctness, which is evaluated against the intended
pronunciation, and ambiguity, which is a property of
the respelling itself. As was the case in the genera-
tion stage, the evaluation is performed at the level of
syllables.
638
6.1 L2P Correctness Filter
The principal method of verifying the correctness
of a respelling involves the application of a letter-
to-phoneme (L2P) model trained on the word-
pronunciation pairs extracted from an English dic-
tionary. The generated pronunciation of each sylla-
ble is compared against its intended pronunciation;
if any of the syllables fail the test, the entire re-
spelling is rejected.
The L2P model is derived using the DIRECTL+
system. The main difference between the L2P model
described in this section and the P2L model from
Section 5.2 is that the input and output data are re-
versed. However, the L2P model is not simply a mir-
ror image of the P2L model. Often the phonemic
output of the composition of the two models is dif-
ferent from the initial phonemic input; e.g., [ro] ?
row? [raU]. This is because the intermediate ortho-
graphic string may be ambiguous. Furthermore, the
L2P model is also intended to test the correctness of
respellings that were generated with other methods.
Other differences between the two models per-
tain to the preprocessing of the training data, and
the letter-to-phoneme alignment. As with the P2L
model, the training data consists of a set of mono-
syllabic words from the Combilex dictionary. How-
ever, in order to make our correctness filter more
conservative, we also remove all words that con-
tain diacritics (e.g., cr?pe), non-English phonemes
(e.g., avant), or silent consonants (e.g., limn). The
alignment is restricted to matching each letter sym-
bol to at most one phoneme, and is derived with
the ALINE phonetic aligner (Kondrak, 2000), which
has been shown to outperform other 1-1 alignment
methods (Jiampojamarn and Kondrak, 2010).
6.2 Vowel Counter
Syllables that contain multiple vowel groups may be
confusing to readers even if they correctly represent
the intended pronunciation. For example, readers
might be unsure whether takess represents one or
two syllables. A simple vowel counter is provided
to filter out such syllables. The vowel filter accepts
a syllable only if (a) it contains exactly one vowel
group (e.g., moe), or (b) the second vowel group
consists of a single e at the end of the syllable (e.g.,
zake).
6.3 SVM Ambiguity Classifier
This module is designed to compute a score that re-
flects the ambiguity of an orthographic syllable. The
ambiguity score of a respelling is defined as the av-
erage of scores assigned to each of its syllables. The
score can then be used to select the best respelling
from a number of candidates generated by our sys-
tem, or to rate a respelling from another source.
Since we have no explicit ambiguity annotations
for respellings, we attempt instead to exploit ambi-
guity judgments that are implicitly made when re-
spellings are created by human authors. We ap-
proach ambiguity as a binary classification task. For
any given syllable, we wish to determine whether it
is ambiguous (a negative instance), or unambiguous
(a positive instance). Our assumption is that a sylla-
ble will not be respelled unless it is necessary due
to ambiguity. For each observed word-respelling
pair, we take all syllables from the respelling as pos-
itive instances, and all syllables in the original word
that are not preserved in the respelling as negative
instances. For example, the pair consisting of the
word cec-il-y respelled as ?sehs-il-ee? provides three
positive instances: sehs, il and ee; and two negative
instances: cec and y.
We extracted word-respelling pairs from the Web-
derived corpora of Ghoshal et al (2009). The syl-
lable breaks in the respellings were mapped onto
the original words using ALINE. In order to im-
prove the quality of the data, we applied a letter-
to-phoneme model to both the original words and
their respellings, and removed pairs with divergent
pronunciations (computed as normalized edit dis-
tance ? 0.8). After the filtering, we were left a
set of 25067 word-respelling pairs containing 78411
training syllables, which yielded 47270 positive and
31141 negative instances.
For the classification task we utilize the SVM-
light software package (Joachims, 1999). Each in-
stance is represented by a set of binary indicator fea-
tures. The features correspond to character n-grams
(including syllable boundary markers) with the val-
ues of n ranging from 1 to 5. For example, the syl-
lable -il- turns on the following features: i, l, -i, il,
l-, -il, il-, -il-. The model learns which n-grams are
characteristic of ambiguous or unambiguous sylla-
bles. For example, it classifies both le and li as am-
639
biguous, and lee as unambiguous. Apart from the
binary classification, the classifier also provides a
real-valued score for each syllable.
6.4 Lexical Reviser
Since the use of familiar English letter sequences
makes the respellings easier to interpret (Fraser,
1997), we incorporate dictionary lookup (Section
4.2) into our system. When the pronunciation of a
syllable happens to correspond to the pronunciation
an actual dictionary word, the syllable may be re-
spelled using that word. This is done as the final step
in the generation process because dictionary words
often receive poor scores from the SVM classifier on
the account of their n-gram composition. The lexi-
cal reviser is restricted to optionally improving the
top-ranked word respelling candidate as determined
by the SVM classifier without altering its syllabifi-
cation. For example, the respelling ?surr-sin-uss? of
circinus is modified to ?sir-sin-us?. If more than one
word can be used, we let the SVM classifier select
the least ambiguous one.
7 System Overview
Our respelling generation system is a multi-stage
process. The input is a sequence of phonemes rep-
resenting the pronunciation of the word. We start by
identifying acceptable syllabifications of phonemes
as described in Section 5.1. For each syllable, we
take up to five respelling candidates produced by
the P2L model (Section 5.2), and between one and
three candidates proposed by the context-sensitive
respeller (Section 5.3). The next stage involves fil-
tering the candidate respellings with the L2P model
(Section 6.1), and the vowel counter (Section 6.2).
If all candidates happen to be rejected, we retain the
first output of the context-sensitive respeller as the
default. The candidate respellings are then scored
by the SVM model (Section 6.3). At this point the
syllables are combined into word respellings, which
are ranked according to their syllable score average.
Finally, the lexical reviser described in Section 6.4 is
applied to the top candidate in an attempt to further
improve the result.
8 Evaluation
In this section, after describing our test sets, we
present the results of two evaluation experiments:
direct human judgment, and indirect validation with
an L2P system.
8.1 Test Sets
Our two test sets were defined after the development
of our system had been completed. There is no over-
lap between the test sets and any of our training sets.
The first test set consists of 27 out of 30 words com-
piled by Fraser (1997) ? 3 words from the origi-
nal set were excluded because the corresponding re-
spellings assume a non-rhotic variety of English. We
refer to Fraser?s respellings as expert, and consider
them as the upper bound in terms of quality.
The second test set of 231 words (henceforth re-
ferred to as the Web set) was extracted from the
corpus of Ghoshal et al (2009) after performing
additional data clean-up described in Section 6.3.
We identified a subset of words for which we
could find phonetic transcriptions composed of En-
glish phonemes on Wikipedia. In order to ensure
that the respellings and the corresponding transcrip-
tions reflect the same pronunciation, we adapted the
Soundex algorithm to apply to phonetic transcrip-
tions, and retained only the respelling/transcription
pairs that yielded identical Soundex codes. We re-
moved words that are found in the Combilex dic-
tionary as those could be familiar to human judges.
Since longer words are more challenging to respell,
and more likely to exhibit variation in respellings
from different sources, we retained only words con-
taining at least eight phonemes.
8.2 Human Judgment
We conducted an experiment with human evalua-
tors using a specially developed graphical annota-
tion program with synthesized word pronunciations.
The evaluators were students enrolled in an intro-
ductory linguistic course, who were not involved in
our project. 13 out of 20 evaluators declared them-
selves as native speakers of English.
The evaluation process involves 40 randomly se-
lected words: 10 from Fraser?s set, and 30 from the
Web set. For each word, the program displays in a
random sequence three respellings, which are from
640
Source Web set Fraser?s set
U U&C U U&C
Baseline 43.0 25.5 41.0 20.0
Web 68.0 32.6 ?
Expert ? 72.0 46.0
Our system 70.0 41.3 67.0 38.5
Table 2: Human judgments on respellings in %: U -
unambiguous; U&C - unambiguous & correct.
the following sources: (1) the Baseline approach de-
scribed in Section 4.3, (2) our system, and (3) ei-
ther expert design (for Fraser?s set) or the Web (for
the Web set). In order to reduce bias, the origi-
nal spelling of the word is not shown. Each re-
spelling is judged separately with regards to ambi-
guity, and those that are judged ambiguous are re-
moved from further consideration. Next, an audio
clip synthesized from the phonemic sequence repre-
senting the intended pronunciation is played through
headphones. For each of the remaining respellings,
the evaluators decide whether it is correct with re-
spect to the recorded pronunciation. Finally, if more
than one respelling have been judged both unam-
biguous and correct, the evaluators are asked to iden-
tify the one that they prefer.
The results of the experiment are shown in Ta-
ble 2. Our system significantly outperforms both
Web respellings and the Baseline approach in terms
of unambiguity and correctness. In addition, the re-
spellings produced by our system are more likely to
be preferred over the Web respellings, and more than
twice as likely to be preferred over the baseline re-
spellings than vice versa. The results on the small
Fraser?s set are less conclusive, but suggest that in
terms of overall quality our system is much closer to
the upper bound than to the baseline.
8.3 Automated Appraisal
Human evaluation is expensive and limited in terms
of the number of variant respellings. Moreover, hu-
man judgements may be biased by previously seen
respellings or by the familiarity with the standard
spelling of a word. An automated evaluation is much
less constrained, and facilitates an ablation study to
determine the relative importance of various compo-
nents of our system.
Source Web set Fraser?s set
WA PA WA PA
No respelling 13.0 76.2 14.8 76.3
Baseline 8.2 78.9 7.4 71.0
Web 14.3 77.9 ?
Expert ? 37.0 85.6
Our system 58.0 93.0 70.4 95.6
Table 4: Word accuracy (WA) and phoneme accuracy
(PA) of eSpeak on respellings.
Source Web set
WA PA
Full system 58.0 93.0
w/o lexical reviser 57.6 93.1
w/o context-sensitive respeller 56.7 92.8
w/o P2L generator 51.9 92.1
w/o L2P correctness filter 33.8 88.0
w/o syllable breaks 20.8 83.9
Table 5: Accuracy of eSpeak on respellings produced by
variants of our system.
eSpeak is a publicly available speech synthesizer5
that can also convert text into phonemic sequences.
The letter-to-phoneme component for English uti-
lizes about five thousand rules, and a dictionary of
about three thousand words, names, and abbrevia-
tions. In our evaluation, we treat eSpeak as a ?black
box? which translates a respelling into its most likely
pronunciation. By determining if there is a match
between the output of eSpeak and the intended pro-
nunciation, we directly test the correctness of the re-
spelling, and indirectly also its ambiguity.
The results of the automated evaluation are shown
in Table 4. The accuracy on the original orthogra-
phy is low, which is unsurprising since the test sets
contain mostly rare, unusually spelled words. Nei-
ther the baseline nor the Web respellings are sig-
nificantly easier for eSpeak than the original words.
On the other hand, respellings generated by our sys-
tem make a massive difference, boosting phoneme
accuracy to well over 90% on both sets. They are
also significantly more effective than the expert re-
spellings.
Table 5 shows the results of our system on the
5http://espeak.sourceforge.net
641
No. Spelling IPA Web/HF respelling Score System respelling Score
1 Incirlik [in?irlik] injirlik 1/6 een-jeer-leek 4/6
2 Captopril [k?pt@prIl] kap-toh-pril 1/6 cap-tuh-prill 4/6
3 Coquitlam [kokwItl@m] ko-kwit-lam 1/6 koh-quit-lumb 4/6
4 Karolina [kArOlinA] karo-leena 4/6 car-awl-ee-nah 1/6
5 subluxation [s@bl@kseS@n] sub-luck-say-shun 3/5 suh-bluck-say-shun 1/5
6 swingle [swINg@l] swing-gl 0/5 swing-gull 2/5
7 cockatrice [kAk@traIs] kok-a-trice 0/7 cock-uh-trice 4/7
8 recalesce [rik@lEs] ree-ka-less 1/5 re-cull-ess 3/5
9 jongleur [ZANgl@r] jong-gler 7/9 zhahng-gler 0/9
10 ylang-ylang [il?Nil?N] ee-lang-ee-lang 5/5 eel-ang-eel-ang 1/5
Table 3: Examples of respellings.
Web set with various modules disabled, which pro-
vides an estimate of their importance. Neither
the context-sensitive respeller nor dictionary lookup
seem to contribute much to eSpeak?s performance.
On the other hand, disabling the P2L generator pro-
duces a significant drop in word accuracy, while re-
moving the L2P correctness filter almost doubles the
phoneme error rate. Interestingly, removing syllable
breaks from the output of the full system has an even
greater negative impact.
8.4 Analysis
Each of 20 evaluators judged 3 variant respellings
of 40 different words. The average number of judg-
ments per word was 7.4 for the 27 words in Fraser?s
set, and 2.8 for the 212 words in the Web set (due to
random selection, 19 words from the Web set were
not judged). Table 3 shows examples of respellings
that were judged by at least five evaluators. The
score columns indicate the proportion of the evalu-
ators that judged a particular respellings as unam-
biguous and correct. The baseline respellings are
not included as their scores were rarely higher than
the scores of the other respellings for a given word.
An interesting exception is palimpsest, for which the
baseline respelling is identical to the actual spelling
of the word.
Examples 1-5 in Table 3 come from the Web set,
while examples 6-10 are from Fraser?s set. The
low scores of the first three Web respellings can be
attributed to specific letter-to-phoneme mappings:
[i]?i, [@]?oh, and [@]?a. Each of the examples
3-5 indicate the evaluators? acceptance of a partic-
ular respelling device: silent letters, multi-syllable
units, and dictionary words. In examples 6-8, the
syllables immediately after the first hyphen in Helen
Fraser?s respellings seem to be problematic. The ex-
pert respelling of jongleur is considered correct even
though the initial j suggests [?], not [Z]. Finally,
the last example demonstrates that the hyphenation
choice can result in very different judgments.
9 Conclusion
In this paper, we introduced the task of automati-
cally generating respellings from the given pronun-
ciation. We investigated the characteristics of good
respellings, and discussed three direct methods of
their creation. We proposed a system that combines
supervised and unsupervised learning with phonetic
and orthographic principles. The evaluation experi-
ment involving human participants indicates that the
respellings produced by our system are better on av-
erage than those found on the Web. The automated
verification demonstrates that they are also much
easier to interpret for a rule-based text-to-speech
converter. In the future we plan to address the re-
lated tasks of improving existing respellings, and as-
sisting writers in creating respellings without direct
access to the phonemic representations.
Acknowledgements
We thank Aditya Bhargava and Clarke Chomyc for
their contribution to the creation of data sets, and
Ben Tucker for advice on the complexities of the hu-
man evaluation experiment. This research was par-
tially funded by the Natural Sciences and Engineer-
ing Research Council of Canada.
642
References
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. In Proc.
of HLT-NAACL, pages 308?316.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Dog?an Can, Erica Cooper, Arnab Ghoshal, Martin Jan-
sche, Sanjeev Khudanpur, Bhuvana Ramabhadran,
Michael Riley, Murat Sara?lar, Abhinav Sethy, Mor-
gan Ulinski, and Christopher White. 2009. Web de-
rived pronunciations for spoken term detection. In
Proc. of ACM SIGIR, pages 83?90.
Edward Carney. 1994. A Survey of English Spelling.
Routledge.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. New York: Harper & Row.
Helen Fraser. 1997. Dictionary pronunciation guides
for English. International Journal of Lexicography,
10(3):181?208.
Lucian Galescu. 2009. Extending pronunciation lexi-
cons via non-phonemic respellings. In Proc. of HLT-
NAACL: Short Papers, pages 129?132.
Arnab Ghoshal, Martin Jansche, Sanjeev Khudanpur,
Michael Riley, and Morgan Ulinski. 2009. Web-
derived pronunciations. In Proc. of ICASSP, pages
4289?4292.
Sittichai Jiampojamarn and Grzegorz Kondrak. 2010.
Letter-phoneme alignment: An exploration. In Proc.
of ACL, pages 780?788.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Proc. of HLT-NAACL, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc. of
ACL, pages 905?913.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schalkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Michael Kenstowicz. 1994. Phonology in Generative
Grammar. Blackwell.
John Kominek and Alan W Black. 2006. Learning
pronunciation dictionaries: Language complexity and
word selection strategies. In Proc. of HLT-NAACL,
pages 232?239.
Grzegorz Kondrak. 2000. A new algorithm for the align-
ment of phonetic sequences. In Proc. of NAACL, pages
288?295.
John Noble. 2012. Mexico. Lonely Planet, 13th edition.
Korin Richmond, Robert Clark, and Sue Fitt. 2009. Ro-
bust LTS rules with the Combilex speech technology
lexicon. In Proc. of Interspeech, pages 1295?1298.
Henry Rogers. 2000. The Sounds of Language. Pearson.
Briony Williams and Rhys James Jones. 2008. Acquir-
ing pronunciation data for a placenames lexicon in a
less-resourced language. In Proc. of LREC.
643
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 140?145,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Cognate and Misspelling Features for Natural Language Identification
Garrett Nicolai, Bradley Hauer, Mohammad Salameh, Lei Yao, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, Canada
{nicolai,bmhauer,msalameh,lyao1,gkondrak}@ualberta.ca
Abstract
We apply Support Vector Machines to differ-
entiate between 11 native languages in the
2013 Native Language Identification Shared
Task. We expand a set of common language
identification features to include cognate inter-
ference and spelling mistakes. Our best results
are obtained with a classifier which includes
both the cognate and the misspelling features,
as well as word unigrams, word bigrams, char-
acter bigrams, and syntax production rules.
1 Introduction
As the world becomes more inter-connected, an in-
creasing number of people devote effort to learn-
ing one of the languages that are dominant in the
global community. English, in particular, is stud-
ied in many countries across the globe. The goal is
often related to increasing one?s chances to obtain
employment and succeed professionally. The lan-
guage of work-place communication is often not a
speaker?s native language (L1) but their second lan-
guage (L2). Speakers and writers of the same L1
can sometimes be identified by similar L2 errors.
The weak Contrastive Analysis Hypothesis (Jarvis
and Crossley, 2012) suggests that these errors may
be a result of L1 causing linguistic interference; that
is, common tendencies of a speaker?s L1 are super-
imposed onto their L2. Native Language Identifi-
cation, or NLI, is an attempt to exploit these errors
in order to identify the L1 of the speaker from texts
written in L2.
Our group at the University of Alberta was unfa-
miliar with the NLI research prior to the announce-
ment of a shared task (Tetreault et al, 2013). How-
ever, we saw it as an opportunity to apply our exper-
tise in character-level NLP to a new task. Our goal
was to propose novel features, and to combine them
with other features that have been previously shown
to work well for language identification.
In the end, we managed to define two feature sets
that are based on spelling errors made by L2 writers.
Cognate features relate a spelling mistake to cognate
interference with the writer?s L1. Misspelling fea-
tures identify common mistakes that may be indica-
tive of the writer?s L1. Both feature sets are meant
to exploit the Contrastive Analysis Hypothesis, and
benefit from the writer?s L1 influence on their L2
writing.
2 Related Work
Koppel et al (2005b) approach the NLI task using
Support Vector Machines (SVMs). They experi-
ment with features such as function-word unigrams,
rare part-of-speech bigrams, character bigrams, and
spelling and syntax errors. They report 80% accu-
racy across 5 languages. We further investigate the
role of word unigrams and spelling errors in native
language identification. We consider not only func-
tion words, but also content words, as well as word
bigrams. We also process spell-checking errors with
a text aligner to find common spelling errors among
writers with the same L1.
Tsur and Rappoport (2007) also use SVMs on the
NLI task, but limit their feature set to character bi-
grams. They report 65% accuracy on 5 languages,
and hypothesize that the choice of words when writ-
ing in L2 is strongly affected by the phonology of
140
their L1. We also consider character bigrams in our
feature set, but combine them with a number of other
features.
Wong and Dras (2011) opt for a maximum en-
tropy classifier, and focus more on syntax errors than
lexical errors. They find that syntax tree production
rules help their classifier in a seven language clas-
sification task. They only consider non-lexicalized
rules, and rules with function words. In contrast, we
consider both lexicalized and non-lexicalized pro-
duction rules, and we include content words.
Bergsma et al (2012) consider the NLI task as a
sub-task of the authorship attribution task. They fo-
cus on the following three questions: (1) whether the
native language of the writer of a paper is English,
(2) what is the gender of the writer, and (3) whether
a paper is a conference or workshop paper. The au-
thors conclude that syntax aids the native language
classification task, further motivating our decision to
use part-of-speech n-grams and production rules as
features for our classifier. Furthermore, the authors
suggest normalizing text to reduce sparsity, and im-
plement several meta-features that they claim aid the
classification.
3 Classifier
Following Koppel et al (2005b) and others, we
perform classification with SVMs. We chose the
SVM-Multiclass package, a version of the SVM-
light package(Joachims, 1999) specifically modified
for multi-class classification problems. We use a lin-
ear kernel, and two hyperparameters that were tuned
on the development set: the c soft-margin regular-
ization parameter, which measures the tradeoff be-
tween training error and the size of the margin, and
, which is used as a stopping criterion for the SVM.
C was tuned to a value of 5000, and epsilon to a
value of 0.1.
4 Features
As features for our SVM, we used a combination of
features common in the literature and new features
developed specifically for this task. The features are
listed in the following section.
4.1 Word n-grams
Following previous work, we use word n-grams as
the primary feature set. We normalize the text before
selecting n-grams using the method of Bergsma et
al. (2012). In particular, all digits are replaced with
a representative ?0? character; for example, ?22? and
?97? are both represented as ?00?. However, unlike
Koppel et al (2005b), we incorporate word bigrams
in addition to word unigrams, and utilize both func-
tion words and content words.
4.1.1 Function Words
Using a list of 295 common function words, we
reduce each document to a vector of values repre-
senting their presence or absence in a document. All
other tokens in the document are ignored. When
constructing vectors of bigrams, any word that is not
on the list of function words is converted to a place-
holder token. Thus, most of our function-word bi-
grams consist of a single function word preceded or
followed by a placeholder token.
4.1.2 Content Words
Other than the normalization mentioned in Sec-
tion 4.1, all tokens in the documents are allowed as
possible word unigrams. No spelling correction is
used for reducing the number of word n-grams. Fur-
thermore, we consider all token unigrams that occur
in the training data, regardless of their frequency.
An early concern with token bigrams was that
they were both large in number, and sparse. In an
attempt to reduce the number of bigrams, we con-
ducted experiments on the development set with dif-
ferent numbers of bigrams that exhibited the highest
information gain. It was found that using all combi-
nations of word bigrams improved predictive accu-
racy the most, and did not lead to a significant cost
to the SVM. Thus, for experiments on the test set, all
token bigrams that were encountered in the training
set were used as features.
4.2 Character n-grams
Following Tetreault et al (2012), we utilize all char-
acter bigrams that occur in the training data, rather
than only the most frequent ones. However, where
the literature uses either binary indicators or relative
frequency of bigrams as features, we use a modi-
fied form of the relative frequency in our classifier.
141
In a pre-processing step, we calculate the average
frequency of each character bigram across all train-
ing documents. Then, during feature extraction, we
again determine the relative frequency of each char-
acter bigram across documents. We then use bi-
nary features to indicate if the frequency of a bigram
is higher than the average frequency. Experiments
conducted on the development set showed that al-
though this modified frequency was out-performed
by the original relative frequency on its own, our
method performed better when further features were
incorporated into the classifier.
4.3 Part-of-speech n-grams
All documents are tagged with POS tags using the
Stanford parser (Klein and Manning, 2003), From
the documents in the training data, a list of all POS
bigrams was generated, and documents were repre-
sented by binary indicators of the presence or ab-
sence of a bigram in the document. As with char-
acter bigrams, we did not simply use the most com-
mon bigrams, but rather considered all bigrams that
appeared in the training data.
4.4 Syntax Production Rules
After generating syntactic parse trees with the Stan-
ford Parser. we extract all possible production rules
from each document, including lexicalized rules.
The features are binary; if a production rule occurs
in an essay, its value is set to 1, and 0 otherwise. For
each language, we use information gain for feature
selection to select the most informative production
rules as suggested by Wong and Dras (2011). Ex-
periments on the development set indicated that the
information gain is superior to raw frequency for the
purpose of syntax feature selection. Since the accu-
racy increased as we added more production rules,
the feature set for final testing includes all produc-
tion rules encountered in the training set. The ma-
jority of the rules are of the form POS? terminal.
We hypothesized that most of the information con-
tained in these rules may be already captured by the
word unigram features. However, experiments on
the development set suggested that the lexicalized
rules contain information that is not captured by the
unigrams, as they led to an increase in predictive ac-
curacy.
4.5 Spelling Errors
Koppel et al (2005a) suggested spelling errors
could be helpful as writers might be affected by
the spelling convention in their native languages.
Moreover, spelling errors also reflect the pronun-
ciation characteristics of the writers? native lan-
guages. They identified 8 types of spelling errors
and collected the statistics of each error type as
their features. Unlike their approach, we focus on
the specific spelling errors made by the writers be-
cause 8 types may be insufficient to distinguish the
spelling characteristics of writers from 11 differ-
ent languages. We extract the spelling error fea-
tures from character-level alignments between the
misspelled word and the intended word. For ex-
ample, if the word abstract is identified as the in-
tended spelling of a misspelling abustruct, the char-
acter alignments are as follows:
a bu s t ru ct
| | | | | |
a b s t ra ct
Only the alignments of the misspelled parts, i.e.
(bu,b) and (ru,ra) in this case, are used as fea-
tures. The spell-checker we use is aspell1, and the
character-level alignments are generated by m2m-
aligner (Jiampojamarn et al, 2007).
4.6 Cognate Interference
Cognates are words that share their linguistic origin.
For example, English become and German bekom-
men have evolved from the same word in a com-
mon ancestor language. Other cognates are words
that have been transfered between languages; for ex-
ample, English system comes from the Greek word
??????? via Latin and French. On average, pairs
of cognates exhibit higher orthographic similarity
than unrelated translation pairs (Kondrak, 2013).
Cognate interference may cause an L1-speaker
to use a cognate word instead of a correct English
translation (for example, become instead of get).
Another instance of cognate interference is mis-
spelling of an English word under the influence of
the L1 spelling (Table 1).
We aim to detect cognate interference by identi-
fying the cases where the cognate word is closer to
1http://aspell.net
142
Misspelling Intended Cognate
developped developed developpe? (Fre)
exemple example exemple (Fre)
organisation organization organisation (Ger)
conzentrated concentrated konzentrierte (Ger)
comercial commercial comercial (Spa)
sistem system sistema (Spa)
Table 1: Examples of cognate interference in the data.
the misspelling than to the intended word (Figure 1).
We define one feature to represent each language L,
for which we could find a downloadable bilingual
English-L dictionary. We use the following algo-
rithm:
1. For each misspelled English word m found in
a document, identify the most likely intended
word e using a spell-checking program.
2. For each language L:
(a) Look up the translation f of the intended
word e in language L.
(b) Compute the orthographic edit distance D
between the words.
(c) If D(e, f) < t then f is assumed to be a
cognate of e.
(d) If f is a cognate and D(m, f) < D(e, f)
then we consider it as a clue that L = L1.
We use a simple method of computing ortho-
graphic distance with threshold t = 0.58 defined
as the baseline method by Bergsma and Kondrak
(2007). However, more accurate methods of cog-
nate identification discussed in that paper could also
be used.
Misspellings can betray cognate interference even
if the misspelled word has no direct cognate in
language L1. For example, a Spanish speaker
might spell the word quick as cuick because of
the existence of numerous cognates such as ques-
tion/cuestio?n. Our misspelling features can detect
such phenomena at the character level; in this case,
qu:cu corresponds to an individual misspelling fea-
ture.
4.7 Meta-features
We included a number of document-specific meta-
features as suggested by Bergsma et al (2012): the
conzentratedconcentrated konzentrierte0.30.4
Figure 1: A cognate word influencing the spelling.
average number of words per sentence, the average
word length, as well as the total number of char-
acters, words, and sentences in a document. We
reasoned that writers from certain linguistic back-
grounds may prefer many short sentences, while
other writers may prefer fewer but longer sentences.
Similarly, a particular linguistic background may in-
fluence the preference for shorter or longer words.
5 Results
The dataset used for experiments was the TOEFL11
Non-Native English Corpus (Blanchard et al, 2013).
The dataset was split into three smaller datasets: the
Training set, consisting of 9900 essays evenly dis-
tributed across 9 languages, the Development set,
which contained a further 1100 essays, and the Test
set, which also contained 1100 essays. As the data
had a staggered release, we used the data differently.
We further split the Training set, with a split of 80%
for training, and 10% for development and testing.
We then used the Development set as a held-out test
set. For held-out testing, the classifier was trained on
all data in the Training set, and for final testing, the
classifier was trained on all data in both the Training
and Development sets.
We used four different combinations of features
for our task submissions. The results are shown in
Table 2. We include the following accuracy values:
(1) the results that we obtained on the Development
set before the Test data release, (2) the official Test
set results provided by the organizers (Tetreault et
al., 2013), (3) the actual Test set results, and (4) the
mean cross-validation results (for submissions 1 and
3). The difference between the official and the ac-
tual Test set results is attributed to two mistakes in
our submissions. In submission 1, the feature lists
used for training and testing did not match. In sub-
missions 3 and 4, only non-lexicalized syntax pro-
duction rules were used, whereas our intention was
to use all of them.
143
No. Features Dev Org Test CV
1 Base 82.0 61.2 80.4 58.2
2 ? cont. words 67.4 68.7 68.7 ?
3 + char 81.4 80.3 81.7 58.5
4 + char + meta 81.2 80.0 80.8 ?
Table 2: Accuracy of our submissions.
All four submissions used the following base
combination of features:
? word unigrams
? word bigrams
? error alignments
? syntax production rules
? word-level cognate interference features
In addition, submission 3 includes character bi-
grams, while submission 4 includes both character
bigrams and meta-features. In submission 2, only
function words are used, with the exclusion of con-
tent words.
Our best submission, which achieves 81.73% ac-
curacy on the Test set, includes all features discussed
in Section 4 except POS bigrams. Early tests in-
dicated that any gains obtained with POS bigrams
were absorbed by the production rules, so they were
excluded form the final experiments. Character bi-
grams help on the Test set but not on the Devel-
opment set. The meta-features decrease accuracy
on both sets. Finally, the content words dramati-
cally improve accuracy. The reason we included a
submission which did not use content words is that
it is a common practice in previous work. In our
analysis of the data, we found content words that
were highly indicative of the language of the writer.
Particularly, words and phrases which contained the
speaker?s home country were useful in predicting the
language. It should be noted that this correspon-
dence may be dependent upon the prompt given to
the writer. Furthermore, it may lead to false posi-
tives for L1 speakers who live in multi-lingual coun-
tries.
5.1 Confusion Matrix
We present the confusion matrix for our best submis-
sion in Table 5.1. The highest number of incorrect
A C F G H I J K S T Tu
ARA 83 0 0 0 2 2 2 1 4 5 1
CHI 1 81 2 0 1 0 8 6 1 0 0
FRE 6 0 82 2 1 3 0 0 1 0 5
GER 1 0 0 90 1 1 1 0 2 0 4
HIN 1 2 2 0 76 1 0 0 0 16 2
ITA 1 1 0 1 0 89 1 0 5 1 1
JPN 2 1 1 1 0 1 86 6 0 0 2
KOR 1 8 0 0 0 0 11 78 0 1 1
SPA 2 2 7 0 3 5 0 2 75 0 4
TEL 2 0 0 2 15 0 0 0 1 80 0
TUR 4 3 2 1 0 1 1 5 2 2 79
Table 3: Confusion Matrix for our best classifier.
Features Test
Full system 81.7
w/o error alignments 81.3
w/o word unigrams 81.1
w/o cognate features 81.0
w/o production rules 80.6
w/o character bigrams 80.4
w/o word bigrams 76.7
Table 4: Accuracy of various feature combinations.
classifications are between languages that are either
linguistically or culturally related (Jarvis and Cross-
ley, 2012). For example, Korean is often misclassi-
fied as Japanese or Chinese. The two languages are
not linguistically related to Korean, but both have
historically had cultural ties with Korean. Likewise,
while Hindi and Telugu are not related linguistically,
they are both spoken in the same geographic area,
and speakers are likely to have contact with each
other.
5.2 Ablation Study
Table 4 shows the results of an ablation experiment
on our best-performing submission. The word bi-
grams contribute the most to the classification; their
removal increases the relative error rate by 27%. The
word unigrams contribute much less., This is un-
surprising, as much of the information contained in
the word unigrams is also contained in the bigrams.
The remaining features are also useful. In particu-
lar, our cognate interference features, despite apply-
ing to only 4 of 11 languages, reduce errors by about
4%.
144
6 Conclusions and Future Work
We have described the system that we have devel-
oped for the NLI 2013 Shared Task. The system
combines features that are prevalent in the litera-
ture with our own novel character-level spelling fea-
tures and word cognate interference features. Most
of the features that we experimented with appear
to increase the overall accuracy, which contradicts
the view that simple bag-of-words usually perform
better than more complex feature sets (Sebastiani,
2002).
Our cognate features can be expanded by includ-
ing languages that do not use the Latin script, such
as Russian and Greek, as demonstrated by Bergsma
and Kondrak (2007). We utilized bilingual dictio-
naries representing only four of the eleven languages
in this task2; yet our cognate interference features
still improved classifier accuracy. With more re-
sources and with better methods of cognate identi-
fication, the cognate features have the potential to
further contribute to native language identification.
Our error-alignment features can likewise be fur-
ther investigated in the future. Currently, after ana-
lyzing texts with a spell-checker, we automatically
accept the first suggestion as the correct one. In
many cases, this leads to faulty corrections, and mis-
leading alignments. By using context sensitive spell-
checking, we can choose better corrections, and ob-
tain information which improves classification.
This shared task was a wonderful introduction
to Native Language Identification, and an excellent
learning experience for members of our group,
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 656?663.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 327?337,
Montre?al, Canada.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
2French, Spanish, German, and Italian.
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Scott Jarvis and Scott Crossley, editors. 2012. Approach-
ing Language Transfer Through Text Classification:
Explorations in the Detection-based Approach, vol-
ume 64. Multilingual Matters Limited, Bristol, UK.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and HMMs to letter-to-phoneme conversion. In Pro-
ceedings of NAACL-HLT, pages 372?379.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in ker-
nel methods, pages 169?184. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430.
Grzegorz Kondrak. 2013. Word similarity, cognation,
and translational equivalence. To appear.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1?47.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, pages 9?16, Prague,
Czech Republic.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK.
145
