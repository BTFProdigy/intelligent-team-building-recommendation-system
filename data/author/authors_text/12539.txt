Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371?379,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Global Models of Document Structure Using Latent Permutations
Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{harr, branavan, regina, karger}@csail.mit.edu
Abstract
We present a novel Bayesian topic model for
learning discourse-level document structure.
Our model leverages insights from discourse
theory to constrain latent topic assignments in
a way that reflects the underlying organiza-
tion of document topics. We propose a global
model in which both topic selection and order-
ing are biased to be similar across a collection
of related documents. We show that this space
of orderings can be elegantly represented us-
ing a distribution over permutations called the
generalized Mallows model. Our structure-
aware approach substantially outperforms al-
ternative approaches for cross-document com-
parison and single-document segmentation.1
1 Introduction
In this paper, we introduce a novel latent topic model
for the unsupervised learning of document structure.
Traditional topic models assume that topics are ran-
domly spread throughout a document, or that the
succession of topics in a document is Markovian.
In contrast, our approach takes advantage of two
important discourse-level properties of text in de-
termining topic assignments: first, that each docu-
ment follows a progression of nonrecurring coher-
ent topics (Halliday and Hasan, 1976); and sec-
ond, that documents from the same domain tend
to present similar topics, in similar orders (Wray,
2002). We show that a topic model incorporat-
ing these long-range dependencies outperforms al-
1Code, data, and annotations used in this work are available
at http://groups.csail.mit.edu/rbg/code/mallows/
ternative approaches for segmentation and cross-
document comparison.
For example, consider a collection of encyclope-
dia articles about cities. The first constraint captures
the notion that a single topic, such as Architecture,
is expressed in a contiguous block within the docu-
ment, rather than spread over disconnected sections.
The second constraint reflects our intuition that all
of these related articles will generally mention some
major topics associated with cities, such as History
and Culture, and will often exhibit similar topic or-
derings, such as placing History before Culture.
We present a Bayesian latent topic model over re-
lated documents that encodes these discourse con-
straints by positing a single distribution over a doc-
ument?s entire topic structure. This global view on
ordering is able to elegantly encode discourse-level
properties that would be difficult to represent using
local dependencies, such as those induced by hid-
den Markov models. Our model enforces that the
same topic does not appear in disconnected portions
of the topic sequence. Furthermore, our approach
biases toward selecting sequences with similar topic
ordering, by modeling a distribution over the space
of topic permutations.
Learning this ordering distribution is a key tech-
nical challenge in our proposed approach. For this
purpose, we employ the generalized Mallows model,
a permutation distribution that concentrates proba-
bility mass on a small set of similar permutations.
It directly captures the intuition of the second con-
straint, and uses a small parameter set to control how
likely individual topics are to be reordered.
We evaluate our model on two challenging
371
document-level tasks. In the alignment task, we aim
to discover paragraphs across different documents
that share the same topic. We also consider the seg-
mentation task, where the goal is to partition each
document into a sequence of topically coherent seg-
ments. We find that our structure modeling approach
substantially outperforms state-of-the-art baselines
for both tasks. Furthermore, we demonstrate the im-
portance of explicitly modeling a distribution over
topic permutations; our model yields significantly
better results than variants that either use a fixed or-
dering, or are order-agnostic.
2 Related Work
Topic and ContentModels Our work is grounded
in topic modeling approaches, which posit that la-
tent state variables control the generation of words.
In earlier topic modeling work such as latent Dirich-
let alocation (LDA) (Blei et al, 2003; Griffiths and
Steyvers, 2004), documents are treated as bags of
words, where each word receives a separate topic
assignment; the topic assignments are auxiliary vari-
ables to the main task of language modeling.
More recent work has attempted to adapt the con-
cepts of topic modeling to more sophisticated repre-
sentations than a bag of words; they use these rep-
resentations to impose stronger constraints on topic
assignments (Griffiths et al, 2005; Wallach, 2006;
Purver et al, 2006; Gruber et al, 2007). These
approaches, however, generally model Markovian
topic or state transitions, which only capture lo-
cal dependencies between adjacent words or blocks
within a document. For instance, content mod-
els (Barzilay and Lee, 2004; Elsner et al, 2007)
are implemented as HMMs, where the states cor-
respond to topics of domain-specific information,
and transitions reflect pairwise ordering prefer-
ences. Even approaches that break text into con-
tiguous chunks (Titov and McDonald, 2008) as-
sign topics based on local context. While these
locally constrained models can implicitly reflect
some discourse-level constraints, they cannot cap-
ture long-range dependencies without an explosion
of the parameter space. In contrast, our model cap-
tures the entire sequence of topics using a compact
representation. As a result, we can explicitly and
tractably model global discourse-level constraints.
Modeling Ordering Constraints Sentence order-
ing has been extensively studied in the context of
probabilistic text modeling for summarization and
generation (Barzilay et al, 2002; Lapata, 2003;
Karamanis et al, 2004). The emphasis of that body
of work is on learning ordering constraints from
data, with the goal of reordering new text from the
same domain. Our emphasis, however, is on ap-
plications where ordering is already observed, and
how that ordering can improve text analysis. From
the methodological side, that body of prior work is
largely driven by local pairwise constraints, while
we aim to encode global constraints.
3 Problem Formulation
Our document structure learning problem can be for-
malized as follows. We are given a corpus of D
related documents. Each document expresses some
subset of a common set of K topics. We assign a
single topic to each paragraph,2 incorporating the
notion that paragraphs are internally topically con-
sistent (Halliday and Hasan, 1976). To capture the
discourse constraint on topic progression described
in Section 1, we require that topic assignments be
contiguous within each document.3 Furthermore,
we assume that the underlying topic sequences ex-
hibit similarity across documents. Our goal is to re-
cover a topic assignment for each paragraph in the
corpus, subject to these constraints.
Our formulation shares some similarity with the
standard LDA setup, in that a common set of topics
is assigned across a collection of documents. How-
ever, in LDA each word?s topic assignment is con-
ditionally independent, following the bag of words
view of documents. In contrast, our constraints on
how topics are assigned let us connect word distri-
butional patterns to document-level topic structure.
4 Model
We propose a generative Bayesian model that ex-
plains how a corpus of D documents, given as se-
quences of paragraphs, can be produced from a set
of hidden topic variables. Topic assignments to each
2Note that our analysis applies equally to other levels of tex-
tual granularity, such as sentences.
3That is, if paragraphs i and j are assigned the same topic,
every paragraph between them must have that topic.
372
paragraph, ranging from 1 to K, are the model?s
final output, implicitly grouping topically similar
paragraphs. At a high level, the process first selects
the bag of topics to be expressed in the document,
and how they are ordered; these topics then deter-
mine the selection of words for each paragraph.
For each document dwithNd paragraphs, we sep-
arately generate a bag of topics td and a topic order-
ing pid. The unordered bag of topics, which contains
Nd elements, expresses how many paragraphs of the
document are assigned to each of theK topics. Note
that some topics may not appear at all. Variable td
is constructed by taking Nd samples from a distri-
bution over topics ? , a multinomial representing the
probability of each topic being expressed. Sharing
? between documents captures the intuition that cer-
tain topics are more likely across the entire corpus.
The topic ordering variable pid is a permutation
over the numbers 1 through K that defines the order
in which topics appear in the document. We draw pid
from the generalized Mallows model, a distribution
over permutations that we explain in Section 4.1. As
we will see, this particular distribution biases the
permutation selection to be close to a single cen-
troid, reflecting the discourse constraint of prefer-
ring similar topic structures across documents.
Together, a document?s bag of topics td and or-
dering pid determine the topic assignment zd,p for
each of its paragraphs. For example, in a corpus
with K = 4, a seven-paragraph document d with
td = {1, 1, 1, 1, 2, 4, 4} and pid = (2 4 3 1) would
induce the topic sequence zd = (2 4 4 1 1 1 1). The
induced topic sequence zd can never assign the same
topic to two unconnected portions of a document,
thus satisfying the constraint of topic contiguity.
As with LDA, we assume that each topic k is as-
sociated with a language model ?k. The words of a
paragraph assigned to topic k are then drawn from
that topic?s language model ?k.
Before turning to a more formal discussion of the
generative process, we first provide background on
the permutation model for topic ordering.
4.1 The Generalized Mallows Model
A central challenge of the approach we take is mod-
eling the distribution over possible topic permuta-
tions. For this purpose we use the generalized Mal-
lows model (GMM) (Fligner and Verducci, 1986;
Lebanon and Lafferty, 2002; Meila? et al, 2007),
which exhibits two appealing properties in the con-
text of this task. First, the model concentrates proba-
bility mass on some ?canonical? ordering and small
perturbations of that ordering. This characteris-
tic matches our constraint that documents from the
same domain exhibit structural similarity. Second,
its parameter set scales linearly with the permuta-
tion length, making it sufficiently constrained and
tractable for inference. In general, this distribution
could potentially be applied to other NLP applica-
tions where ordering is important.
Permutation Representation Typically, permuta-
tions are represented directly as an ordered sequence
of elements. The GMM utilizes an alternative rep-
resentation defined as a vector (v1, . . . , vK?1) of in-
version counts with respect to the identity permuta-
tion (1, . . . ,K). Term vj counts the number of times
a value greater than j appears before j in the permu-
tation.4 For instance, given the standard-form per-
mutation (3 1 5 2 4), v2 = 2 because 3 and 5 appear
before 2; the entire inversion count vector would be
(1 2 0 1). Every vector of inversion counts uniquely
identifies a single permutation.
The Distribution The GMM assigns proba-
bility mass according to the distance of a
given permutation from the identity permutation
{1, . . . ,K}, based on K ? 1 real-valued parameters
(?1, . . . ?K?1).5 Using the inversion count represen-
tation of a permutation, the GMM?s probability mass
function is expressed as an independent product of
probabilities for each vj :
GMM(v | ?) = e
??j ?jvj
?(?)
=
n?1?
j=1
e??jvj
?j(?j) , (1)
where ?j(?j) is a normalization factor with value:
?j(?j) = 1? e
?(K?j+1)?j
1? e??j .
4The sum of a vector of inversion counts is simply that per-
mutation?s Kendall?s ? distance to the identity permutation.
5In our work we take the identity permutation to be the fixed
centroid, which is a parameter in the full GMM. As we explain
later, our model is not hampered by this apparent restriction.
373
Due to the exponential form of the distribution, re-
quiring that ?j > 0 constrains the GMM to assign
highest probability mass to each vj being zero, cor-
responding to the identity permutation. A higher
value for ?j assigns more probability mass to vj be-
ing close to zero, biasing j to have fewer inversions.
The GMM elegantly captures our earlier require-
ment for a probability distribution that concentrates
mass around a global ordering, and uses few param-
eters to do so. Because the topic numbers in our
task are completely symmetric and not linked to any
extrinsic observations, fixing the identity permuta-
tion to be that global ordering does not sacrifice any
representational power. Another major benefit of
the GMM is its membership in the exponential fam-
ily of distributions; this means that it is particularly
amenable to a Bayesian representation, as it admits
a natural conjugate prior:
GMM0(?j | vj,0, ?0) ? e(??jvj,0?log?j(?j))?0 . (2)
Intuitively, this prior states that over ?0 prior trials,
the total number of inversions was ?0vj,0. This dis-
tribution can be easily updated with the observed vj
to derive a posterior distribution.6
4.2 Formal Generative Process
We now fully specify the details of our model. We
observe a corpus of D documents, each an ordered
sequence of paragraphs, and a specification of a
number of topics K. Each paragraph is represented
as a bag of words. The model induces a set of hid-
den variables that probabilistically explain how the
words of the corpus were produced. Our final de-
sired output is the distributions over the paragraphs?
hidden topic assignment variables. In the following,
variables subscripted with 0 are fixed prior hyperpa-
rameters.
1. For each topic k, draw a language model ?k ?
Dirichlet(?0). As with LDA, these are topic-
specific word distributions.
2. Draw a topic distribution ? ? Dirichlet(?0),
which expresses how likely each topic is to ap-
pear regardless of position.
6Because each vj has a different range, it is inconvenient
to set the prior hyperparameters vj,0 directly. In our work, we
instead fix the mode of the prior distribution to a value ?0, which
works out to setting vj,0 = 1exp(?0)?1 ? K?j+1exp((K?j+1)?0)?1 .
3. Draw the topic ordering distribution parame-
ters ?j ? GMM0(?0, ?0) for j = 1 to K ? 1.
These parameters control how rapidly probabil-
ity mass decays for having more inversions for
each topic. A separate ?j for every topic allows
us to learn that some topics are more likely to
be reordered than others.
4. For each document d with Nd paragraphs:
(a) Draw a bag of topics td by sampling Nd
times from Multinomial(?).
(b) Draw a topic ordering pid by sampling a
vector of inversion counts vd ? GMM(?).
(c) Compute the vector of topic assignments
zd for document d?s paragraphs, by sorting
td according to pid.7
(d) For each paragraph p in document d:
i. Sample each word wd,p,j according to
the language model of p: wd,p,j ?
Multinomial(?zd,p).
5 Inference
The variables that we aim to infer are the topic as-
signments z of each paragraph, which are deter-
mined by the bag of topics t and ordering pi for each
document. Thus, our goal is to estimate the marginal
distributions of t and pi given the document text.
We accomplish this inference task through Gibbs
sampling (Bishop, 2006). A Gibbs sampler builds
a Markov chain over the hidden variable state space
whose stationary distribution is the actual posterior
of the joint distribution. Each new sample is drawn
from the distribution of a single variable conditioned
on previous samples of the other variables. We can
?collapse? the sampler by integrating over some of
the hidden variables in the model, in effect reducing
the state space of the Markov chain. Collapsed sam-
pling has been previously demonstrated to be effec-
tive for LDA and its variants (Griffiths and Steyvers,
2004; Porteous et al, 2008; Titov and McDonald,
2008). Our sampler integrates over all but three sets
7Multiple permutations can contribute to the probability of a
single document?s topic assignments zd, if there are topics that
do not appear in td. As a result, our current formulation is bi-
ased toward assignments with fewer topics per document. In
practice, we do not find this to negatively impact model perfor-
mance.
374
of hidden variables: bags of topics t, orderings pi,
and permutation inversion parameters ?. After a
burn-in period, we treat the last samples of t and
pi as a draw from the true posterior.
Document Probability As a preliminary step,
consider how to calculate the probability of a single
document?s words wd given the document?s para-
graph topic assignments zd, and other documents
and their topic assignments. Note that this proba-
bility is decomposable into a product of probabil-
ities over individual paragraphs, where paragraphs
with different topics have conditionally independent
word probabilities. Let w?d and z?d indicate the
words and topic assignments to documents other
than d, and W be the vocabulary size. The proba-
bility of the words in d is then:
P (wd | z,w?d, ?0)
=
K?
k=1
?
?k
P (wd | zd, ?k)P (?k | z,w?d, ?0)d?k
=
K?
k=1
DCM({wd,i : zd,i = k}
| {w?d,i : z?d,i = k}, ?0), (3)
where DCM(?) refers to the Dirichlet compound
multinomial distribution, the result of integrat-
ing over multinomial parameters with a Dirichlet
prior (Bernardo and Smith, 2000). For a Dirichlet
prior with parameters ? = (?1, . . . , ?W ), the DCM
assigns the following probability to a series of ob-
servations x = {x1, . . . , xn}:
DCM(x | ?) = ?(
?
j ?j)?
j ?(?j)
W?
i=1
?(N(x, i) + ?i)
?(|x|+?j ?j)
,
where N(x, i) refers to the number of times word
i appears in x. Here, ?(?) is the Gamma function,
a generalization of the factorial for real numbers.
Some algebra shows that the DCM?s posterior prob-
ability density function conditioned on a series of
observations y = {y1, . . . , yn} can be computed by
updating each ?i with counts of how often word i
appears in y:
DCM(x | y, ?)
= DCM(x | ?1 +N(y, 1), . . . , ?W +N(y,W )).
(4)
Equation 3 and 4 will be used again to compute the
conditional distributions of the hidden variables.
We now turn to a discussion of how each individ-
ual random variable is resampled.
Bag of Topics First we consider how to resample
td,i, the ith topic draw for document d conditioned
on all other parameters being fixed (note this is not
the topic of the ith paragraph, as we reorder topics
using pid):
P (td,i = t | . . .)
? P (td,i = t | t?(d,i), ?0)P (wd | td, pid,w?d, z?d, ?0)
?
N(t?(d,i), t) + ?0
|t?(d,i)|+K?0 P (wd | z,w?d, ?0),
where td is updated to reflect td,i = t, and zd is de-
terministically computed by mapping td and pid to
actual paragraph topic assignments. The first step
reflects an application of Bayes rule to factor out the
term for wd. In the second step, the first term arises
out of the DCM, by updating the parameters ?0 with
observations t?(d,i) as in equation 4 and dropping
constants. The document probability term is com-
puted using equation 3. The new td,i is selected
by sampling from this probability computed over all
possible topic assignments.
Ordering The parameterization of a permutation
pi as a series of inversion values vj reveals a natural
way to decompose the search space for Gibbs sam-
pling. For a single ordering, each vj can be sampled
independently, according to:
P (vj = v | . . .)
? P (vj = v | ?j)P (wd | td, pid,w?d, z?d, ?0)
= GMMj(v | ?j)P (wd | zd,w?d, z?d, ?0),
where pid is updated to reflect vj = v, and zd is com-
puted according to td and pid. The first term refers
to the jth multiplicand of equation 1; the second is
computed using equation 3. Term vj is sampled ac-
cording to the resulting probabilities.
GMM Parameters For each j = 1 to K ? 1, we
resample ?j from its posterior distribution:
P (?j | . . .)
= GMM0
(
?j
????
?
i vj,i + vj,0?0
N + ?0 , N + ?0
)
,
375
where GMM0 is evaluated according to equation 2.
The normalization constant of this distribution is un-
known, meaning that we cannot directly compute
and invert the cumulative distribution function to
sample from this distribution. However, the distri-
bution itself is univariate and unimodal, so we can
expect that an MCMC technique such as slice sam-
pling (Neal, 2003) should perform well. In practice,
the MATLAB black-box slice sampler provides a ro-
bust draw from this distribution.
6 Experimental Setup
Data Sets We evaluate our model on two data sets
drawn from the English Wikipedia. The first set
is 100 articles about large cities, with topics such
as History, Culture, and Demographics. The sec-
ond is 118 articles about chemical elements in the
periodic table, including topics such as Biological
Role, Occurrence, and Isotopes. Within each cor-
pus, articles often exhibit similar section orderings,
but many have idiosyncratic inversions. This struc-
tural variability arises out of the collaborative nature
of Wikipedia, which allows articles to evolve inde-
pendently. Corpus statistics are summarized below.
Corpus Docs Paragraphs Vocab Words
Cities 100 6,670 41,978 492,402
Elements 118 2,810 18,008 191,762
In each data set, the articles? noisy section head-
ings induce a reference structure to compare against.
This reference structure assumes that two para-
graphs are aligned if and only if their section head-
ings are identical, and that section boundaries pro-
vide the correct segmentation of each document.
These headings are only used for evaluation, and are
not provided to any of the systems.
Using the section headings to build the reference
structure can be problematic, as the same topic may
be referred to using different titles across different
documents, and sections may be divided at differing
levels of granularity. Thus, for the Cities data set, we
manually annotated each article?s paragraphs with a
consistent set of section headings, providing us an
additional reference structure to evaluate against. In
this clean section headings set, we found approxi-
mately 18 topics that were expressed in more than
one document.
Tasks and Metrics We study performance on the
tasks of alignment and segmentation. In the former
task, we measure whether paragraphs identified to
be the same topic by our model have the same sec-
tion headings, and vice versa. First, we identify the
?closest? topic to each section heading, by finding
the topic that is most commonly assigned to para-
graphs under that section heading. We compute the
proportion of paragraphs where the model?s topic as-
signment matches the section heading?s topic, giv-
ing us a recall score. High recall indicates that
paragraphs of the same section headings are always
being assigned to the same topic. Conversely, we
can find the closest section heading to each topic,
by finding the section heading that is most com-
mon for the paragraphs assigned to a single topic.
We then compute the proportion of paragraphs from
that topic whose section heading is the same as the
reference heading for that topic, yielding a preci-
sion score. High precision means that paragraphs
assigned to a single topic usually correspond to the
same section heading. The harmonic mean of recall
and precision is the summary F-score.
Statistical significance in this setup is measured
with approximate randomization (Noreen, 1989), a
nonparametric test that can be directly applied to
nonlinear metrics such as F-score. This test has been
used in prior evaluations for information extraction
and machine translation (Chinchor, 1995; Riezler
and Maxwell, 2005).
For the second task, we take the boundaries at
which topics change within a document to be a
segmentation of that document. We evaluate us-
ing the standard penalty metrics Pk and WindowD-
iff (Beeferman et al, 1999; Pevzner and Hearst,
2002). Both pass a sliding window over the doc-
uments and compute the probability of the words
at the ends of the windows being improperly seg-
mented with respect to each other. WindowDiff re-
quires that the number of segmentation boundaries
between the endpoints be correct as well.8
Our model takes a parameter K which controls
the upper bound on the number of latent topics. Note
that our algorithm can select fewer thanK topics for
each document, soK does not determine the number
8Statistical significance testing is not standardized and usu-
ally not reported for the segmentation task, so we omit these
tests in our results.
376
of segments in each document. We report results
using both K = 10 and 20 (recall that the cleanly
annotated Cities data set had 18 topics).
Baselines andModel Variants We consider base-
lines from the literature that perform either align-
ment or segmentation. For the first task, we
compare against the hidden topic Markov model
(HTMM) (Gruber et al, 2007), which represents
topic transitions between adjacent paragraphs in a
Markovian fashion, similar to the approach taken in
content modeling work. Note that HTMM can only
capture local constraints, so it would allow topics to
recur noncontiguously throughout a document.
We also compare against the structure-agnostic
approach of clustering the paragraphs using the
CLUTO toolkit,9 which uses repeated bisection to
maximize a cosine similarity-based objective.
For the segmentation task, we compare to
BayesSeg (Eisenstein and Barzilay, 2008),10
a Bayesian topic-based segmentation model
that outperforms previous segmentation ap-
proaches (Utiyama and Isahara, 2001; Galley et al,
2003; Purver et al, 2006; Malioutov and Barzilay,
2006). BayesSeg enforces the topic contiguity
constraint that motivated our model. We provide
this baseline with the benefit of knowing the correct
number of segments for each document, which is
not provided to our system. Note that BayesSeg
processes each document individually, so it cannot
capture structural relatedness across documents.
To investigate the importance of our ordering
model, we consider two variants of our model that
alternately relax and tighten ordering constraints. In
the constrained model, we require all documents to
follow the same canonical ordering of topics. This
is equivalent to forcing the topic permutation distri-
bution to give all its probability to one ordering, and
can be implemented by fixing all inversion counts v
to zero during inference. At the other extreme, we
consider the uniform model, which assumes a uni-
form distribution over all topic permutations instead
of biasing toward a small related set. In our im-
plementation, this can be simulated by forcing the
9http://glaros.dtc.umn.edu/gkhome/views/cluto/
10We do not evaluate on the corpora used in their work, since
our model relies on content similarity across documents in the
corpus.
GMM parameters ? to always be zero. Both variants
still enforce topic contiguity, and allow segments
across documents to be aligned by topic assignment.
Evaluation Procedures For each evaluation of
our model and its variants, we run the Gibbs sampler
from five random seed states, and take the 10,000th
iteration of each chain as a sample. Results shown
are the average over these five samples. All Dirich-
let prior hyperparameters are set to 0.1, encouraging
sparse distributions. For the GMM, we set the prior
decay parameter ?0 to 1, and the sample size prior
?0 to be 0.1 times the number of documents.
For the baselines, we use implementations pub-
licly released by their authors. We set HTMM?s pri-
ors according to values recommended in the authors?
original work. For BayesSeg, we use its built-in hy-
perparameter re-estimation mechanism.
7 Results
Alignment Table 1 presents the results of the
alignment evaluation. In every case, the best per-
formance is achieved using our full model, by a sta-
tistically significant and usually substantial margin.
In both domains, the baseline clustering method
performs competitively, indicating that word cues
alone are a good indicator of topic. While the sim-
pler variations of our model achieve reasonable per-
formance, adding the richer GMM distribution con-
sistently yields superior results.
Across each of our evaluations, HTMM greatly
underperforms the other approaches. Manual ex-
amination of the actual topic assignments reveals
that HTMM often selects the same topic for discon-
nected paragraphs of the same document, violating
the topic contiguity constraint, and demonstrating
the importance of modeling global constraints for
document structure tasks.
We also compare performance measured on the
manually annotated section headings against the ac-
tual noisy headings. The ranking of methods by per-
formance remains mostly unchanged between these
two evaluations, indicating that the noisy headings
are sufficient for gaining insight into the compara-
tive performance of the different approaches.
Segmentation Table 2 presents the segmentation
experiment results. On both data sets, our model
377
Cities: clean headings Cities: noisy headings Elements: noisy headings
Recall Prec F-score Recall Prec F-score Recall Prec F-score
K
=
10
Clustering 0.578 0.439 ? 0.499 0.611 0.331 ? 0.429 0.524 0.361 ? 0.428
HTMM 0.446 0.232 ? 0.305 0.480 0.183 ? 0.265 0.430 0.190 ? 0.264
Constrained 0.579 0.471 ? 0.520 0.667 0.382 ? 0.485 0.603 0.408 ? 0.487
Uniform 0.520 0.440 ? 0.477 0.599 0.343 ? 0.436 0.591 0.403 ? 0.479
Our model 0.639 0.509 0.566 0.705 0.399 0.510 0.685 0.460 0.551
K
=
20
Clustering 0.486 0.541 ? 0.512 0.527 0.414 ? 0.464 0.477 0.402 ? 0.436
HTMM 0.260 0.217 ? 0.237 0.304 0.187 ? 0.232 0.248 0.243 ? 0.246
Constrained 0.458 0.519 ? 0.486 0.553 0.415 ? 0.474 0.510 0.421 ? 0.461
Uniform 0.499 0.551 ? 0.524 0.571 0.423 ? 0.486 0.550 0.479  0.512
Our model 0.578 0.636 0.606 0.648 0.489 0.557 0.569 0.498 0.531
Table 1: Comparison of the alignments produced by our model and a series of baselines and model variations, for both
10 and 20 topics, evaluated against clean and noisy sets of section headings. Higher scores are better. Within the same
K, the methods which our model significantly outperforms are indicated with ? for p < 0.001 and  for p < 0.01.
Cities: clean headings Cities: noisy headings Elements: noisy headings
Pk WD # Segs Pk WD # Segs Pk WD # Segs
BayesSeg 0.321 0.376 ? 12.3 0.317 0.376 ? 13.2 0.279 0.316 ? 7.7
K
=
10 Constrained 0.260 0.281 7.7 0.267 0.288 7.7 0.227 0.244 5.4Uniform 0.268 0.300 8.8 0.273 0.304 8.8 0.226 0.250 6.6
Our model 0.253 0.283 9.0 0.257 0.286 9.0 0.201 0.226 6.7
K
=
20 Constrained 0.274 0.314 10.9 0.274 0.313 10.9 0.231 0.257 6.6Uniform 0.234 0.294 14.0 0.234 0.290 14.0 0.209 0.248 8.7
Our model 0.221 0.278 14.2 0.222 0.278 14.2 0.203 0.243 8.6
Table 2: Comparison of the segmentations produced by our model and a series of baselines and model variations, for
both 10 and 20 topics, evaluated against clean and noisy sets of section headings. Lower scores are better. ?BayesSeg
is given the true number of segments, so its segments count reflects the reference structure?s segmentation.
outperforms the BayesSeg baseline by a substantial
margin regardless of K. This result provides strong
evidence that learning connected topic models over
related documents leads to improved segmentation
performance. In effect, our model can take advan-
tage of shared structure across related documents.
In all but one case, the best performance is ob-
tained by the full version of our model. This result
indicates that enforcing discourse-motivated struc-
tural constraints allows for better segmentation in-
duction. Encoding global discourse-level constraints
leads to better language models, resulting in more
accurate predictions of segment boundaries.
8 Conclusions
In this paper, we have shown how an unsupervised
topic-based approach can capture document struc-
ture. Our resulting model constrains topic assign-
ments in a way that requires global modeling of en-
tire topic sequences. We showed that the generalized
Mallows model is a theoretically and empirically ap-
pealing way of capturing the ordering component
of this topic sequence. Our results demonstrate the
importance of augmenting statistical models of text
analysis with structural constraints motivated by dis-
course theory.
Acknowledgments
The authors acknowledge the funding support of
NSF CAREER grant IIS-0448168, the NSF Grad-
uate Fellowship, the Office of Naval Research,
Quanta, Nokia, and the Microsoft Faculty Fellow-
ship. We thank the members of the NLP group at
MIT and numerous others who offered suggestions
and comments on this work. We are especially grate-
ful to Marina Meila? for introducing us to the Mal-
lows model. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
378
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL/HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Ar-
tificial Intelligence Research, 17:35?55.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34:177?210.
Jose? M. Bernardo and Adrian F.M. Smith. 2000.
Bayesian Theory. Wiley Series in Probability and
Statistics.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Nancy Chinchor. 1995. Statistical significance of MUC-
6 results. In Proceedings of the 6th Conference on
Message Understanding.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
M.A. Fligner and J.S. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical Soci-
ety, Series B, 48(3):359?369.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse segmen-
tation of multi-party conversation. In Proceedings of
ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in NIPS.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Proceedings of AIS-
TATS.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence for text structuring using
a reliably annotated corpus. In Proceedings of ACL.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL.
Guy Lebanon and John Lafferty. 2002. Cranking: com-
bining rankings using conditional probability models
on permutations. In Proceedings of ICML.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL.
Marina Meila?, Kapil Phadnis, Arthur Patterson, and Jeff
Bilmes. 2007. Consensus ranking under the exponen-
tial model. In Proceedings of UAI.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Eric W. Noreen. 1989. Computer Intensive Methods for
Testing Hypotheses. An Introduction. Wiley.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent dirichlet alo-
cation. In Proceedings of SIGKDD.
Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of ACL/COLING.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL.
Hanna M. Wallach. 2006. Topic modeling: beyond bag
of words. In Proceedings of ICML.
Alison Wray. 2002. Formulaic Language and the Lexi-
con. Cambridge University Press, Cambridge.
379
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 544?551,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating a Table-of-Contents
S.R.K. Branavan, Pawan Deshpande and Regina Barzilay
Massachusetts Institute of Technology
{branavan, pawand, regina}@csail.mit.edu
Abstract
This paper presents a method for the auto-
matic generation of a table-of-contents. This
type of summary could serve as an effec-
tive navigation tool for accessing informa-
tion in long texts, such as books. To gen-
erate a coherent table-of-contents, we need
to capture both global dependencies across
different titles in the table and local con-
straints within sections. Our algorithm ef-
fectively handles these complex dependen-
cies by factoring the model into local and
global components, and incrementally con-
structing the model?s output. The results of
automatic evaluation and manual assessment
confirm the benefits of this design: our sys-
tem is consistently ranked higher than non-
hierarchical baselines.
1 Introduction
Current research in summarization focuses on pro-
cessing short articles, primarily in the news domain.
While in practice the existing summarization meth-
ods are not limited to this material, they are not
universal: texts in many domains and genres can-
not be summarized using these techniques. A par-
ticularly significant challenge is the summarization
of longer texts, such as books. The requirement
for high compression rates and the increased need
for the preservation of contextual dependencies be-
tween summary sentences places summarization of
such texts beyond the scope of current methods.
In this paper, we investigate the automatic gener-
ation of tables-of-contents, a type of indicative sum-
mary particularly suited for accessing information in
long texts. A typical table-of-contents lists topics
described in the source text and provides informa-
tion about their location in the text. The hierarchical
organization of information in the table further re-
fines information access by specifying the relations
between different topics and providing rich contex-
tual information during browsing. Commonly found
in books, tables-of-contents can also facilitate access
to other types of texts. For instance, this type of
summary could serve as an effective navigation tool
for understanding a long, unstructured transcript for
an academic lecture or a meeting.
Given a text, our goal is to generate a tree wherein
a node represents a segment of text and a title that
summarizes its content. This process involves two
tasks: the hierarchical segmentation of the text, and
the generation of informative titles for each segment.
The first task can be addressed by using the hier-
archical structure readily available in the text (e.g.,
chapters, sections and subsections) or by employ-
ing existing topic segmentation algorithms (Hearst,
1994). In this paper, we take the former approach.
As for the second task, a naive approach would be to
employ existing methods of title generation to each
segment, and combine the results into a tree struc-
ture.
However, the latter approach cannot guarantee
that the generated table-of-contents forms a coher-
ent representation of the entire text. Since titles of
different segments are generated in isolation, some
of the generated titles may be repetitive. Even non-
repetitive titles may not provide sufficient informa-
tion to discriminate between the content of one seg-
544
Scientific computing
Remarkable recursive algorithm for multiplying matrices
Divide and conquer algorithm design
Making a recursive algorithm
Solving systems of linear equations
Computing an LUP decomposition
Forward and back substitution
Symmetric positive definite matrices and least squares approximation
Figure 1: A fragment of a table-of-contents generated by our method.
ment and another. Therefore, it is essential to gen-
erate an entire table-of-contents tree in a concerted
fashion.
This paper presents a hierarchical discriminative
approach for table-of-contents generation. Figure 1
shows a fragment of a table-of-contents automat-
ically generated by this algorithm. Our method
has two important points of departure from exist-
ing techniques. First, we introduce a structured dis-
criminative model for table-of-contents generation
that accounts for a wide range of phrase-based and
collocational features. The flexibility of this model
results in improved summary quality. Second, our
model captures both global dependencies across dif-
ferent titles in the tree and local dependencies within
sections. We decompose the model into local and
global components that handle different classes of
dependencies. We further reduce the search space
through incremental construction of the model?s out-
put by considering only the promising parts of the
decision space.
We apply our method to process a 1,180 page al-
gorithms textbook. To assess the contribution of our
hierarchical model, we compare our method with
state-of-the-art methods that generate each segment
title independently.1 The results of automatic eval-
uation and manual assessment of title quality show
that the output of our system is consistently ranked
higher than that of non-hierarchical baselines.
2 Related Work
Although most current research in summarization
focuses on newspaper articles, a number of ap-
proaches have been developed for processing longer
texts. Most of these approaches are tailored to a par-
1The code and feature vector data for
our model and the baselines are available at
http://people.csail.mit.edu/branavan/code/toc.
ticular domain, such as medical literature or scien-
tific articles. By making strong assumptions about
the input structure and the desired format of the out-
put, these methods achieve a high compression rate
while preserving summary coherence. For instance,
Teufel and Moens (2002) summarize scientific arti-
cles by selecting rhetorical elements that are com-
monly present in scientific abstracts. Elhadad and
McKeown (2001) generate summaries of medical ar-
ticles by following a certain structural template in
content selection and realization.
Our work, however, is closer to domain-
independent methods for summarizing long texts.
Typically, these approaches employ topic segmen-
tation to identify a list of topics described in a
document, and then produce a summary for each
part (Boguraev and Neff, 2000; Angheluta et al,
2002). In contrast to our method, these approaches
perform either sentence or phrase extraction, rather
than summary generation. Moreover, extraction for
each segment is performed in isolation, and global
constraints on the summary are not enforced.
Finally, our work is also related to research on ti-
tle generation (Banko et al, 2000; Jin and Haupt-
mann, 2001; Dorr et al, 2003). Since work in this
area focuses on generating titles for one article at a
time (e.g., newspaper reports), the issue of hierarchi-
cal generation, which is unique to our task, does not
arise. However, this is not the only novel aspect of
the proposed approach. Our model learns title gener-
ation in a fully discriminative framework, in contrast
to the commonly used noisy-channel model. Thus,
instead of independently modeling the selection and
grammaticality constraints, we learn both types of
features in a single framework. This joint training
regime supports greater flexibility in modeling fea-
ture interaction.
545
3 Problem Formulation
We formalize the problem of table-of-contents gen-
eration as a supervised learning task where the goal
is to map a tree of text segments S to a tree of titles
T . A segment may correspond to a chapter, section
or subsection.
Since the focus of our work is on the generation
aspect of table-of-contents construction, we assume
that the hierarchical segmentation of a text is pro-
vided in the input. This division can either be au-
tomatically computed using one of the many avail-
able text segmentation algorithms (Hearst, 1994), or
it can be based on demarcations already present in
the input (e.g., paragraph markers).
During training, the algorithm is provided with a
set of pairs (Si, T i) for i = 1, . . . , p, where Si is
the ith tree of text segments, and T i is the table-of-
contents for that tree. During testing, the algorithm
generates tables-of-contents for unseen trees of text
segments.
We also assume that during testing the desired
title length is provided as a parameter to the algo-
rithm.
4 Algorithm
To generate a coherent table-of-contents, we need
to take into account multiple constraints: the titles
should be grammatical, they should adequately rep-
resent the content of their segments, and the table-
of-contents as a whole should clearly convey the re-
lations between the segments. Taking a discrimina-
tive approach for modeling this task would allow us
to achieve this goal: we can easily integrate a range
of constraints in a flexible manner. Since the num-
ber of possible labels (i.e., tables-of-contents) is pro-
hibitively large and the labels themselves exhibit a
rich internal structure, we employ a structured dis-
criminative model that can easily handle complex
dependencies. Our solution relies on two orthogo-
nal strategies to balance the tractability and the rich-
ness of the model. First, we factor the model into
local and global components. Second, we incremen-
tally construct the output of each component using
a search-based discriminative algorithm. Both of
these strategies have the effect of intelligently prun-
ing the decision space.
Our model factorization is driven by the different
types of dependencies which are captured by the two
components. The first model is local: for each seg-
ment, it generates a list of candidate titles ranked by
their individual likelihoods. This model focuses on
grammaticality and word selection constraints, but it
does not consider relations among different titles in
the table-of-contents. These latter dependencies are
captured in the global model that constructs a table-
of-contents by selecting titles for each segment from
the available candidates. Even after this factoriza-
tion, the decision space for each model is large: for
the local model, it is exponential in the length of the
segment title, and for the global model it is exponen-
tial in the size of the tree.
Therefore, we construct the output for each of
these models incrementally using beam search. The
algorithm maintains the most promising partial out-
put structures, which are extended at every itera-
tion. The model incorporates this decoding pro-
cedure into the training process, thereby learning
model parameters best suited for the specific decod-
ing algorithm. Similar models have been success-
fully applied in the past to other tasks including pars-
ing (Collins and Roark, 2004), chunking (Daume?
and Marcu, 2005), and machine translation (Cowan
et al, 2006).
4.1 Model Structure
The model takes as input a tree of text segments S.
Each segment s ? S and its title z are represented
as a local feature vector ?loc(s, z). Each compo-
nent of this vector stores a numerical value. This
feature vector can track any feature of the segment s
together with its title z. For instance, the ith compo-
nent of this vector may indicate whether the bigram
(z[j]z[j+ 1]) occurs in s, where z[j] is the jth word
in z:
(?loc(s, z))i =
{
1 if (z[j]z[j + 1]) ? s
0 otherwise
In addition, our model captures dependencies
among multiple titles that appear in the same table-
of-contents. We represent a tree of segments S
paired with titles T with the global feature vector
?glob(S, T ). The components here are also numer-
ical features. For example, the ith component of the
vector may indicate whether a title is repeated in the
table-of-contents T :
546
(?glob(S, T ))i =
{
1 repeated title
0 otherwise
Our model constructs a table-of-contents in two
basic steps:
Step One The goal of this step is to generate a
list of k candidate titles for each segment s ? S.
To do so, for each possible title z, the model maps
the feature vector ?loc(s, z) to a real number. This
mapping can take the form of a linear model,
?loc(s, z) ? ?loc
where ?loc is the local parameter vector.
Since the number of possible titles is exponen-
tial, we cannot consider all of them. Instead, we
prune the decision space by incrementally construct-
ing promising titles. At each iteration j, the algo-
rithm maintains a beam Q of the top k partially gen-
erated titles of length j. During iteration j + 1, a
new set of candidates is grown by appending a word
from s to the right of each member of the beam Q.
We then sort the entries in Q: z1, z2, . . . such that
?loc(s, zi) ??loc ? ?loc(s, zi+1) ??loc, ?i. Only the
top k candidates are retained, forming the beam for
the next iteration. This process continues until a title
of the desired length is generated. Finally, the list of
k candidates is returned.
Step Two Given a set of candidate titles
z1, z2, . . . , zk for each segment s ? S, our goal is
to construct a table-of-contents T by selecting the
most appropriate title from each segment?s candi-
date list. To do so, our model computes a score for
the pair (S, T ) based on the global feature vector
?glob(S, T ):
?glob(S, T ) ? ?glob
where ?glob is the global parameter vector.
As with the local model (step one), the num-
ber of possible tables-of-contents is too large to be
considered exhaustively. Therefore, we incremen-
tally construct a table-of-contents by traversing the
tree of segments in a pre-order walk (i.e., the or-
der in which segments appear in the text). In this
case, the beam contains partially generated tables-
of-contents, which are expanded by one segment ti-
tle at a time. To further reduce the search space,
during decoding only the top five candidate titles for
a segment are given to the global model.
4.2 Training the Model
Training for Step One We now describe how the
local parameter vector ?loc is estimated from train-
ing data. We are given a set of training examples
(si, yi) for i = 1, . . . , l, where si is the ith text seg-
ment, and yi is the title of this segment.
This linear model is learned using a variant of
the incremental perceptron algorithm (Collins and
Roark, 2004; Daume? and Marcu, 2005). This on-
line algorithm traverses the training set multiple
times, updating the parameter vector ?loc after each
training example in case of mis-predictions. The al-
gorithm encourages a setting of the parameter vector
?loc that assigns the highest score to the feature vec-
tor associated with the correct title.
The pseudo-code of the algorithm is shown in Fig-
ure 2. Given a text segment s and the corresponding
title y, the training algorithm maintains a beam Q
containing the top k partial titles of length j. The
beam is updated on each iteration using the func-
tions GROW and PRUNE. For every word in seg-
ment s and for every partial title in Q, GROW cre-
ates a new title by appending this word to the title.
PRUNE retains only the top ranked candidates based
on the scoring function ?loc(s, z) ??loc. If y[1 . . . j]
(i.e., the prefix of y of length j) is not in the modi-
fied beam Q, then ?loc is updated2 as shown in line
4 of the pseudo-code in Figure 2. In addition, Q is
replaced with a beam containing only y[1 . . . j] (line
5). This process is performed |y| times. We repeat
this process for all training examples over 50 train-
ing iterations. 3
Training for Step Two To train the global param-
eter vector ?glob, we are given training examples
(Si, T i) for i = 1, . . . , p, where Si is the ith tree of
text segments, and T i is the table-of-contents for that
tree. However, we cannot directly use these tables-
of-contents for training our global model: since this
model selects one of the candidate titles zi1, . . . , z
i
k
returned by the local model, the true title of the seg-
ment may not be among these candidates. There-
fore, to determine a new target title for the segment,
we need to identify the title in the set of candidates
2If the word in the jth position of y does not occur in s, then
the parameter update is not performed.
3For decoding, ?loc is averaged over the training iterations
as in Collins and Roark (2004).
547
s ? segment text.
y ? segment title.
y[1 . . . j] ? prefix of y of length j.
Q ? beam containing partial titles.
1. for j = 1 . . . |y|
2. Q = PRUNE(GROW(s,Q))
3. if y[1 . . . j] /? Q
4. ?loc = ?loc + ?loc(s, y[1 . . . j])
?
?
z?Q
?loc(s,z)
|Q|
5. Q = {y[1 . . . j]}
Figure 2: The training algorithm for the local model.
that is closest to the true title.
We employ the L1 distance measure to compare
the content word overlap between two titles.4 For
each input (S, T ), and each segment s ? S, we iden-
tify the segment title closest in the L1 measure to the
true title y5:
z? = arg min
i
L1(zi, y)
Once all the training targets in the corpus have
been identified through this procedure, the global
linear model ?glob(S, T ) ??glob is learned using the
same perceptron algorithm as in step one. Rather
than maintaining the beam of partially generated ti-
tles, the beam Q holds partially generated tables-of-
contents. Also, the loop in line 1 of Figure 2 iterates
over segment titles rather than words. The global
model is trained over 200 iterations.
5 Features
Local Features Our local model aims to generate
titles which adequately represent the meaning of the
segment and are grammatical. Selection and contex-
tual preferences are encoded in the local features.
The features that capture selection constraints are
specified at the word level, and contextual features
are expressed at the word sequence level.
The selection features capture the position of the
word, its TF*IDF, and part-of-speech information.
In addition, they also record whether the word oc-
curs in the body of neighboring segments. We also
4This measure is close to ROUGE-1 which in addition con-
siders the overlap in auxiliary words.
5In the case of ties, one of the titles is picked arbitrarily.
Segment has the same title as its sibling
Segment has the same title as its parent
Two adjacent sibling titles have the same head
Two adjacent sibling titles start with the same word
Rank given to the title by the local model
Table 1: Examples of global features.
generate conjunctive features by combining features
of different types.
The contextual features record the bigram and tri-
gram language model scores, both for words and for
part-of-speech tags. The trigram scores are aver-
aged over the title. The language models are trained
using the SRILM toolkit. Another type of contex-
tual feature models the collocational properties of
noun phrases in the title. This feature aims to elim-
inate generic phrases, such as ?the following sec-
tion? from the generated titles.6 To achieve this ef-
fect, for each noun phrase in the title, we measure
the ratio of their frequency in the segment to their
frequency in the corpus.
Global Features Our global model describes the
interaction between different titles in the tree (See
Table 1). These interactions are encoded in three
types of global features. The first type of global
feature indicates whether titles in the tree are re-
dundant at various levels of the tree structure. The
second type of feature encourages parallel construc-
tions within the same tree. For instance, titles of ad-
joining segments may be verbalized as noun phrases
with the same head (e.g., ?Bubble sort algorithm?,
?Merge sort algorithm?). We capture this property
by comparing words that appear in certain positions
in adjacent sibling titles. Finally, our global model
also uses the rank of the title provided by the local
model. This feature enables the global model to ac-
count for the preferences of the local model in the
title selection process.
6 Evaluation Set-Up
Data We apply our method to an undergraduate al-
gorithms textbook. For detailed statistics on the data
see Table 2. We split its table-of-contents into a set
6Unfortunately, we could not use more sophisticated syntac-
tic features due to the low accuracy of statistical parsers on our
corpus.
548
Number of Titles 540
Number of Trees 39
Tree Depth 4
Number of Words 269,650
Avg. Title Length 3.64
Avg. Branching 3.29
Avg. Title Duplicates 21
Table 2: Statistics on the corpus used in the experi-
ments.
of independent subtrees. Given a table-of-contents
of depth n with a root branching factor of r, we gen-
erate r subtrees, with a depth of at most n ? 1. We
randomly select 80% of these trees for training, and
the rest are used for testing. In our experiments, we
use ten different randomizations to compensate for
the small number of available trees.
Admittedly, this method of generating training
and testing data omits some dependencies at the
level of the table-of-contents as a whole. However,
the subtrees used in our experiments still exhibit
a sufficiently deep hierarchical structure, rich with
contextual dependencies.
Baselines As an alternative to our hierarchical dis-
criminative method, we consider three baselines that
build a table-of-contents by generating a title for
each segment individually, without taking into ac-
count the tree structure, and one hierarchical gener-
ative baseline. The first method generates a title for a
segment by selecting the noun phrase from that seg-
ment with the highest TF*IDF. This simple method
is commonly used to generate keywords for brows-
ing applications in information retrieval, and has
been shown to be effective for summarizing techni-
cal content (Wacholder et al, 2001).
The second baseline is based on the noisy-channel
generative (flat generative, FG) model proposed by
Banko et al, (2000). Similar to our local model,
this method captures both selection and grammati-
cal constraints. However, these constraints are mod-
eled separately, and then combined in a generative
framework.
We use our local model (Flat Discriminative
model, FD) as the third baseline. Like the second
baseline, this model omits global dependencies, and
only focuses on features that capture relations within
individual segments.
In the hierarchical generative (HG) baseline we
run our global model on the ranked list of titles pro-
duced for each section by the noisy-channel genera-
tive model.
The last three baselines and our algorithm are pro-
vided with the title length as a parameter. In our
experiments, the algorithms use the reference title
length.
Experimental Design: Comparison with refer-
ence tables-of-contents Reference based evalu-
ation is commonly used to assess the quality of
machine-generated headlines (Wang et al, 2005).
We compare our system?s output with the table-of-
contents from the textbook using ROUGE metrics.
We employ a publicly available software package,7
with all the parameters set to default values.
Experimental Design: Human assessment The
judges were each given 30 segments randomly se-
lected from a set of 359 test segments. For each test
segment, the judges were presented with its text, and
3 alternative titles consisting of the reference and
the titles produced by the hierarchical discriminative
model, and the best performing baseline. In addi-
tion, the judges had access to all of the segments in
the book. A total of 498 titles for 166 unique seg-
ments were ranked. The system identities were hid-
den from the judges, and the titles were presented in
random order. The judges ranked the titles based on
how well they represent the content of the segment.
Titles were ranked equal if they were judged to be
equally representative of the segment.
Six people participated in this experiment. All the
participants were graduate students in computer sci-
ence who had taken the algorithms class in the past
and were reasonably familiar with the material.
7 Results
Figure 3 shows fragments of the tables-of-contents
generated by our method and the four baselines
along with the reference counterpart. These extracts
illustrate three general phenomena that we observed
in the test corpus. First, the titles produced by key-
word extraction exhibit a high degree of redundancy.
In fact, 40% of the titles produced by this method are
repeated more than once in the table-of-contents. In
7http://www.isi.edu/licensed-sw/see/rouge/
549
Reference:
hash tables
direct address tables
hash tables
collision resolution by chaining
analysis of hashing with chaining
open addressing
linear probing
quadratic probing
double hashing
Flat Generative:
linked list
worst case time
wasted space
worst case running time
to show that there are
dynamic set
occupied slot
quadratic function
double hashing
Flat Discriminative:
dictionary operations
universe of keys
computer memory
element in the list
hash table with load factor
hash table
hash function
hash function
double hashing
Keyword Extraction:
hash table
dynamic set
hash function
worst case
expected number
hash table
hash function
hash table
double hashing
Hierarchical Generative:
dictionary operations
worst case time
wasted space
worst case running time
to show that there are
collision resolution
linear time
quadratic function
double hashing
Hierarchical Discriminative:
dictionary operations
direct address table
computer memory
worst case running time
hash table with load factor
address table
hash function
quadratic probing
double hashing
Figure 3: Fragments of tables-of-contents generated by our method and the four baselines along with the
corresponding reference.
Rouge-1 Rouge-L Rouge-W Full Match
HD 0.256 0.249 0.216 13.5
FD 0.241 0.234 0.203 13.1
HG 0.139 0.133 0.117 5.8
FG 0.094 0.090 0.079 4.1
Keyword 0.168 0.168 0.157 6.3
Table 3: Title quality as compared to the reference
for the hierarchical discriminative (HD), flat dis-
criminative (FD), hierarchical generative (HG), flat
generative (FG) and Keyword models. The improve-
ment given by HD over FD in all three Rouge mea-
sures is significant at p ? 0.03 based on the Sign
test.
better worse equal
HD vs. FD 68 32 49
Reference vs. HD 115 13 22
Reference vs. FD 123 7 20
Table 4: Overall pairwise comparisons of the rank-
ings given by the judges. The improvement in ti-
tle quality given by HD over FD is significant at
p ? 0.0002 based on the Sign test.
contrast, our method yields 5.5% of the titles as du-
plicates, as compared to 9% in the reference table-
of-contents.8
Second, the fragments show that the two discrim-
inative models ? Flat and Hierarchical ? have a
number of common titles. However, adding global
dependencies to rerank titles generated by the local
model changes 30% of the titles in the test set.
Comparison with reference tables-of-contents
Table 3 shows the average ROUGE scores over
the ten randomizations for the five automatic meth-
ods. The hierarchical discriminative method consis-
tently outperforms the four baselines according to
all ROUGE metrics.
At the same time, these results also show that only
a small ratio of the automatically generated titles
are identical to the reference ones. In some cases,
the machine-generated titles are very close in mean-
ing to the reference, but are verbalized differently.
Examples include pairs such as (?Minimum Span-
ning Trees?, ?Spanning Tree Problem?) and (?Wal-
lace Tree?, ?Multiplication Circuit?).9 While mea-
sures like ROUGE can capture the similarity in the
first pair, they cannot identify semantic proximity
8Titles such as ?Analysis? and ?Chapter Outline? are re-
peated multiple times in the text.
9A Wallace Tree is a circuit that multiplies two integers.
550
between the titles in the second pair. Therefore,
we supplement the results of this experiment with
a manual assessment of title quality as described be-
low.
Human assessment We analyze the human rat-
ings by considering pairwise comparisons between
the models. Given two models, A and B, three out-
comes are possible: A is better than B, B is bet-
ter than A, or they are of equal quality. The re-
sults of the comparison are summarized in Table 4.
These results indicate that using hierarchical infor-
mation yields statistically significant improvement
(at p ? 0.0002 based on the Sign test) over a flat
counterpart.
8 Conclusion and Future Work
This paper presents a method for the automatic gen-
eration of a table-of-contents. The key strength of
our method lies in its ability to track dependencies
between generation decisions across different levels
of the tree structure. The results of automatic evalu-
ation and manual assessment confirm the benefits of
joint tree learning: our system is consistently ranked
higher than non-hierarchical baselines.
We also plan to expand our method for the task
of slide generation. Like tables-of-contents, slide
bullets are organized in a hierarchical fashion and
are written in relatively short phrases. From the
language viewpoint, however, slides exhibit more
variability and complexity than a typical table-of-
contents. To address this challenge, we will explore
more powerful generation methods that take into ac-
count syntactic information.
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168 and grant IIS-0415865). We would also
like to acknowledge the many people who took part
in human evaluations. Thanks to Michael Collins,
Benjamin Snyder, Igor Malioutov, Jacob Eisenstein,
Luke Zettlemoyer, Terry Koo, Erdong Chen, Zo-
ran Dzunic and the anonymous reviewers for helpful
comments and suggestions. Any opinions, findings,
conclusions or recommendations expressed above
are those of the authors and do not necessarily re-
flect the views of the NSF.
References
Roxana Angheluta, Rik De Busser, and Marie-Francine
Moens. 2002. The use of topic segmentation for auto-
matic summarization. In Proceedings of the ACL-2002
Workshop on Automatic Summarization.
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statistical
translation. In Proceedings of the ACL, pages 318?
325.
Branimir Boguraev and Mary S. Neff. 2000. Discourse
segmentation in aid of document summarization. In
Proceedings of the 33rd Hawaii International Confer-
ence on System Sciences, pages 3004?3014.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the ACL, pages 111?118.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the EMNLP, pages 232?241.
Hal Daume? and Daniel Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of the ICML,
pages 169?176.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop, pages 1?8.
Noemie Elhadad and Kathleen R. McKeown. 2001. To-
wards generating patient specific summaries of med-
ical articles. In Proceedings of NAACL Workshop on
Automatic Summarization, pages 31?39.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of the ACL, pages 9?
16.
Rong Jin and Alexander G. Hauptmann. 2001. Auto-
matic title generation for spoken broadcast news. In
Proceedings of the HLT, pages 1?3.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Nina Wacholder, David K. Evans, and Judith Klavans.
2001. Automatic identification and organization of in-
dex terms for interactive browsing. In JCDL, pages
126?134.
R. Wang, J. Dunnion, and J. Carthy. 2005. Machine
learning approach to augmenting news headline gen-
eration. In Proceedings of the IJCNLP.
551
Proceedings of ACL-08: HLT, pages 263?271,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Document-Level Semantic Properties from Free-text Annotations
S.R.K. Branavan Harr Chen Jacob Eisenstein Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, harr, jacobe, regina}@csail.mit.edu
Abstract
This paper demonstrates a new method for
leveraging free-text annotations to infer se-
mantic properties of documents. Free-text an-
notations are becoming increasingly abundant,
due to the recent dramatic growth in semi-
structured, user-generated online content. An
example of such content is product reviews,
which are often annotated by their authors
with pros/cons keyphrases such as ?a real bar-
gain? or ?good value.? To exploit such noisy
annotations, we simultaneously find a hid-
den paraphrase structure of the keyphrases, a
model of the document texts, and the underly-
ing semantic properties that link the two. This
allows us to predict properties of unannotated
documents. Our approach is implemented as
a hierarchical Bayesian model with joint in-
ference, which increases the robustness of the
keyphrase clustering and encourages the doc-
ument model to correlate with semantically
meaningful properties. We perform several
evaluations of our model, and find that it sub-
stantially outperforms alternative approaches.
1 Introduction
A central problem in language understanding is
transforming raw text into structured representa-
tions. Learning-based approaches have dramatically
increased the scope and robustness of this type of
automatic language processing, but they are typi-
cally dependent on large expert-annotated datasets,
which are costly to produce. In this paper, we show
how novice-generated free-text annotations avail-
able online can be leveraged to automatically infer
document-level semantic properties.
With the rapid increase of online content cre-
ated by end users, noisy free-text annotations have
pros/cons: great nutritional value
... combines it all: an amazing product, quick and
friendly service, cleanliness, great nutrition ...
pros/cons: a bit pricey, healthy
... is an awesome place to go if you are health con-
scious. They have some really great low calorie dishes
and they publish the calories and fat grams per serving.
Figure 1: Excerpts from online restaurant reviews with
pros/cons phrase lists. Both reviews discuss healthiness,
but use different keyphrases.
become widely available (Vickery and Wunsch-
Vincent, 2007; Sterling, 2005). For example, con-
sider reviews of consumer products and services.
Often, such reviews are annotated with keyphrase
lists of pros and cons. We would like to use these
keyphrase lists as training labels, so that the proper-
ties of unannotated reviews can be predicted. Hav-
ing such a system would facilitate structured access
and summarization of this data. However, novice-
generated keyphrase annotations are incomplete de-
scriptions of their corresponding review texts. Fur-
thermore, they lack consistency: the same under-
lying property may be expressed in many ways,
e.g., ?healthy? and ?great nutritional value? (see Fig-
ure 1). To take advantage of such noisy labels, a sys-
tem must both uncover their hidden clustering into
properties, and learn to predict these properties from
review text.
This paper presents a model that addresses both
problems simultaneously. We assume that both the
document text and the selection of keyphrases are
governed by the underlying hidden properties of the
document. Each property indexes a language model,
thus allowing documents that incorporate the same
263
property to share similar features. In addition, each
keyphrase is associated with a property; keyphrases
that are associated with the same property should
have similar distributional and surface features.
We link these two ideas in a joint hierarchical
Bayesian model. Keyphrases are clustered based
on their distributional and lexical properties, and a
hidden topic model is applied to the document text.
Crucially, the keyphrase clusters and document top-
ics are linked, and inference is performed jointly.
This increases the robustness of the keyphrase clus-
tering, and ensures that the inferred hidden topics
are indicative of salient semantic properties.
Our model is broadly applicable to many scenar-
ios where documents are annotated in a noisy man-
ner. In this work, we apply our method to a col-
lection of reviews in two categories: restaurants and
cell phones. The training data consists of review text
and the associated pros/cons lists. We then evaluate
the ability of our model to predict review properties
when the pros/cons list is hidden. Across a variety
of evaluation scenarios, our algorithm consistently
outperforms alternative strategies by a wide margin.
2 Related Work
Review Analysis Our approach relates to previous
work on property extraction from reviews (Popescu
et al, 2005; Hu and Liu, 2004; Kim and Hovy,
2006). These methods extract lists of phrases, which
are analogous to the keyphrases we use as input
to our algorithm. However, our approach is dis-
tinguished in two ways: first, we are able to pre-
dict keyphrases beyond those that appear verbatim
in the text. Second, our approach learns the rela-
tionships between keyphrases, allowing us to draw
direct comparisons between reviews.
Bayesian Topic Modeling One aspect of our
model views properties as distributions over words
in the document. This approach is inspired by meth-
ods in the topic modeling literature, such as Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), where
topics are treated as hidden variables that govern the
distribution of words in a text. Our algorithm ex-
tends this notion by biasing the induced hidden top-
ics toward a clustering of known keyphrases. Tying
these two information sources together enhances the
robustness of the hidden topics, thereby increasing
the chance that the induced structure corresponds to
semantically meaningful properties.
Recent work has examined coupling topic mod-
els with explicit supervision (Blei and McAuliffe,
2007; Titov and McDonald, 2008). However, such
approaches assume that the documents are labeled
within a predefined annotation structure, e.g., the
properties of food, ambiance, and service for restau-
rants. In contrast, we address free-text annotations
created by end users, without known semantic prop-
erties. Rather than requiring a predefined annotation
structure, our model infers one from the data.
3 Problem Formulation
We formulate our problem as follows. We assume
a dataset composed of documents with associated
keyphrases. Each document may be marked with
multiple keyphrases that express unseen semantic
properties. Across the entire collection, several
keyphrases may express the same property. The
keyphrases are also incomplete ? review texts of-
ten express properties that are not mentioned in their
keyphrases. At training time, our model has access
to both text and keyphrases; at test time, the goal is
to predict the properties supported by a previously
unseen document. We can then use this property list
to generate an appropriate set of keyphrases.
4 Model Description
Our approach leverages both keyphrase clustering
and distributional analysis of the text in a joint, hi-
erarchical Bayesian model. Keyphrases are drawn
from a set of clusters; words in the documents are
drawn from language models indexed by a set of
topics, where the topics correspond to the keyphrase
clusters. Crucially, we bias the assignment of hid-
den topics in the text to be similar to the topics rep-
resented by the keyphrases of the document, but we
permit some words to be drawn from other topics
not represented by the keyphrases. This flexibility in
the coupling allows the model to learn effectively in
the presence of incomplete keyphrase annotations,
while still encouraging the keyphrase clustering to
cohere with the topics supported by the text.
We train the model on documents annotated with
keyphrases. During training, we learn a hidden
topic model from the text; each topic is also asso-
264
? ? keyphrase cluster model
x ? keyphrase cluster assignment
s ? keyphrase similarity values
h ? document keyphrases
? ? document keyphrase topics
? ? probability of selecting ? instead of ?
c ? selects between ? and ? for word topics
? ? document topic model
z ? word topic assignment
? ? language models of each topic
w ? document words
? ? Dirichlet(?0)
x? ? Multinomial(?)
s?,?? ?
{
Beta(?=) if x? = x??
Beta(?6=) otherwise
?d = [?d,1 . . . ?d,K ]T
where
?d,k ?
{
1 if x? = k for any l ? hd
0 otherwise
? ? Beta(?0)
cd,n ? Bernoulli(?)
?d ? Dirichlet(?0)
zd,n ?
{
Multinomial(?d) if cd,n = 1
Multinomial(?d) otherwise
?k ? Dirichlet(?0)
wd,n ? Multinomial(?zd,n)
Figure 2: The plate diagram for our model. Shaded circles denote observed variables, and squares denote hyper
parameters. The dotted arrows indicate that ? is constructed deterministically from x and h.
ciated with a cluster of keyphrases. At test time,
we are presented with documents that do not con-
tain keyphrase annotations. The hidden topic model
of the review text is used to determine the proper-
ties that a document as a whole supports. For each
property, we compute the proportion of the docu-
ment?s words assigned to it. Properties with propor-
tions above a set threshold (tuned on a development
set) are predicted as being supported.
4.1 Keyphrase Clustering
One of our goals is to cluster the keyphrases, such
that each cluster corresponds to a well-defined prop-
erty. We represent each distinct keyphrase as a vec-
tor of similarity scores computed over the set of
observed keyphrases; these scores are represented
by s in Figure 2, the plate diagram of our model.1
Modeling the similarity matrix rather than the sur-
1We assume that similarity scores are conditionally inde-
pendent given the keyphrase clustering, though the scores are
in fact related. Such simplifying assumptions have been previ-
ously used with success in NLP (e.g., Toutanova and Johnson,
2007), though a more theoretically sound treatment of the sim-
ilarity matrix is an area for future research.
face forms allows arbitrary comparisons between
keyphrases, e.g., permitting the use of both lexical
and distributional information. The lexical com-
parison is based on the cosine similarity between
the keyphrase words. The distributional similar-
ity is quantified in terms of the co-occurrence of
keyphrases across review texts. Our model is inher-
ently capable of using any arbitrary source of simi-
larity information; for a discussion of similarity met-
rics, see Lin (1998).
4.2 Document-level Distributional Analysis
Our analysis of the document text is based on proba-
bilistic topic models such as LDA (Blei et al, 2003).
In the LDA framework, each word is generated from
a language model that is indexed by the word?s topic
assignment. Thus, rather than identifying a single
topic for a document, LDA identifies a distribution
over topics.
Our word model operates similarly, identifying a
topic for each word, written as z in Figure 2. To
tie these topics to the keyphrases, we deterministi-
cally construct a document-specific topic distribu-
265
tion from the clusters represented by the document?s
keyphrases ? this is ? in the figure. ? assigns equal
probability to all topics that are represented in the
keyphrases, and a small smoothing probability to
other topics.
As noted above, properties may be expressed in
the text even when no related keyphrase appears. For
this reason, we also construct a document-specific
topic distribution ?. The auxiliary variable c indi-
cates whether a given word?s topic is drawn from
the set of keyphrase clusters, or from this topic dis-
tribution.
4.3 Generative Process
In this section, we describe the underlying genera-
tive process more formally.
First we consider the set of all keyphrases ob-
served across the entire corpus, of which there are
L. We draw a multinomial distribution ? over the K
keyphrase clusters from a symmetric Dirichlet prior
?0. Then for the ?th keyphrase, a cluster assign-
ment x? is drawn from the multinomial ?. Finally,
the similarity matrix s ? [0, 1]L?L is constructed.
Each entry s?,?? is drawn independently, depending
on the cluster assignments x? and x?? . Specifically,
s?,?? is drawn from a Beta distribution with parame-
ters ?= if x? = x?? and ?6= otherwise. The parame-
ters ?= linearly bias s?,?? towards one (Beta(?=) ?
Beta(2, 1)), and the parameters ?6= linearly bias s?,??
towards zero (Beta(?6=) ? Beta(1, 2)).
Next, the words in each of the D documents
are generated. Document d has Nd words; zd,n is
the topic for word wd,n. These latent topics are
drawn either from the set of clusters represented by
the document?s keyphrases, or from the document?s
topic model ?d. We deterministically construct a
document-specific keyphrase topic model ?d, based
on the keyphrase cluster assignments x and the ob-
served keyphrases hd. The multinomial ?d assigns
equal probability to each topic that is represented by
a phrase in hd, and a small probability to other top-
ics.
As noted earlier, a document?s text may support
properties that are not mentioned in its observed
keyphrases. For that reason, we draw a document
topic multinomial ?d from a symmetric Dirichlet
prior ?0. The binary auxiliary variable cd,n deter-
mines whether the word?s topic is drawn from the
keyphrase model ?d or the document topic model
?d. cd,n is drawn from a weighted coin flip, with
probability ?; ? is drawn from a Beta distribution
with prior ?0. We have zd,n ? ?d if cd,n = 1,
and zd,n ? ?d otherwise. Finally, the word wd,n
is drawn from the multinomial ?zd,n , where zd,n in-
dexes a topic-specific language model. Each of the
K language models ?k is drawn from a symmetric
Dirichlet prior ?0.
5 Posterior Sampling
Ultimately, we need to compute the model?s poste-
rior distribution given the training data. Doing so
analytically is intractable due to the complexity of
the model, but sampling-based techniques can be
used to estimate the posterior. We employ Gibbs
sampling, previously used in NLP by Finkel et al
(2005) and Goldwater et al (2006), among others.
This technique repeatedly samples from the condi-
tional distributions of each hidden variable, eventu-
ally converging on a Markov chain whose stationary
distribution is the posterior distribution of the hid-
den variables in the model (Gelman et al, 2004).
We now present sampling equations for each of the
hidden variables in Figure 2.
The prior over keyphrase clusters ? is sampled
based on hyperprior ?0 and keyphrase cluster as-
signments x. We write p(? | . . .) to mean the prob-
ability conditioned on all the other variables.
p(? | . . .) ? p(? | ?0)p(x | ?),
= p(? | ?0)
L
?
?
p(x? | ?)
= Dir(?;?0)
L
?
?
Mul(x?;?)
= Dir(?;??),
where ??i = ?0 + count(x? = i). This update rule
is due to the conjugacy of the multinomial to the
Dirichlet distribution. The first line follows from
Bayes? rule, and the second line from the conditional
independence of each keyphrase assignment x? from
the others, given ?.
?d and ?k are resampled in a similar manner:
p(?d | . . .) ? Dir(?d;??d),
p(?k | . . .) ? Dir(?k; ??k),
266
p(x? | . . .) ? p(x? | ?)p(s | x?,x??, ?)p(z | ?, ?, c)
? p(x? | ?)
?
?
?
?? 6=?
p(s?,?? | x?, x?? , ?)
?
?
?
?
D
?
d
?
cd,n=1
p(zd,n | ?d)
?
?
= Mul(x?;?)
?
?
?
?? 6=?
Beta(s?,?? ;?x?,x?? )
?
?
?
?
D
?
d
?
cd,n=1
Mul(zd,n; ?d)
?
?
Figure 3: The resampling equation for the keyphrase cluster assignments.
where ??d,i = ?0 + count(zd,n = i ? cd,n = 0)
and ??k,i = ?0 +
?
d count(wd,n = i ? zd,n = k). In
building the counts for ??d,i, we consider only cases
in which cd,n = 0, indicating that the topic zd,n is
indeed drawn from the document topic model ?d.
Similarly, when building the counts for ??k, we con-
sider only cases in which the word wd,n is drawn
from topic k.
To resample ?, we employ the conjugacy of the
Beta prior to the Bernoulli observation likelihoods,
adding counts of c to the prior ?0.
p(? | . . .) ? Beta(?;??),
where ?? = ?0 +
[ ?
d count(cd,n = 1)
?
d count(cd,n = 0)
]
.
The keyphrase cluster assignments are repre-
sented by x, whose sampling distribution depends
on ?, s, and z, via ?. The equation is shown in Fig-
ure 3. The first term is the prior on x?. The second
term encodes the dependence of the similarity ma-
trix s on the cluster assignments; with slight abuse of
notation, we write ?x?,x?? to denote ?= if x? = x?? ,
and ?6= otherwise. The third term is the dependence
of the word topics zd,n on the topic distribution ?d.
We compute the final result of Figure 3 for each pos-
sible setting of x?, and then sample from the normal-
ized multinomial.
The word topics z are sampled according to
keyphrase topic distribution ?d, document topic dis-
tribution ?d, words w, and auxiliary variables c:
p(zd,n | . . .)
? p(zd,n | ?d, ?d, cd,n)p(wd,n | zd,n, ?)
=
{
Mul(zd,n; ?d)Mul(wd,n; ?zd,n) if cd,n = 1,
Mul(zd,n;?d)Mul(wd,n; ?zd,n) otherwise.
As with x?, each zd,n is sampled by computing
the conditional likelihood of each possible setting
within a constant of proportionality, and then sam-
pling from the normalized multinomial.
Finally, we sample each auxiliary variable cd,n,
which indicates whether the hidden topic zd,n is
drawn from ?d or ?d. The conditional probability
for cd,n depends on its prior ? and the hidden topic
assignments zd,n:
p(cd,n | . . .)
? p(cd,n | ?)p(zd,n | ?d, ?d, cd,n)
=
{
Bern(cd,n;?)Mul(zd,n; ?d) if cd,n = 1,
Bern(cd,n;?)Mul(zd,n;?d) otherwise.
We compute the likelihood of cd,n = 0 and cd,n = 1
within a constant of proportionality, and then sample
from the normalized Bernoulli distribution.
6 Experimental Setup
Data Sets We evaluate our system on reviews from
two categories, restaurants and cell phones. These
reviews were downloaded from the popular Epin-
ions2 website. Users of this website evaluate prod-
ucts by providing both a textual description of their
opinion, as well as concise lists of keyphrases (pros
and cons) summarizing the review. The statistics of
this dataset are provided in Table 1. For each of
the categories, we randomly selected 50%, 15%, and
35% of the documents as training, development, and
test sets, respectively.
Manual analysis of this data reveals that authors
often omit properties mentioned in the text from
the list of keyphrases. To obtain a complete gold
2http://www.epinions.com/
267
Restaurants Cell Phones
# of reviews 3883 1112
Avg. review length 916.9 1056.9
Avg. keyphrases / review 3.42 4.91
Table 1: Statistics of the reviews dataset by category.
standard, we hand-annotated a subset of the reviews
from the restaurant category. The annotation effort
focused on eight commonly mentioned properties,
such as those underlying the keyphrases ?pleasant
atmosphere? and ?attentive staff.? Two raters anno-
tated 160 reviews, 30 of which were annotated by
both. Cohen?s kappa, a measure of interrater agree-
ment ranging from zero to one, was 0.78 for this sub-
set, indicating high agreement (Cohen, 1960).
Each review was annotated with 2.56 properties
on average. Each manually-annotated property cor-
responded to an average of 19.1 keyphrases in the
restaurant data, and 6.7 keyphrases in the cell phone
data. This supports our intuition that a single se-
mantic property may be expressed using a variety of
different keyphrases.
Training Our model needs to be provided with the
number of clusters K . We setK large enough for the
model to learn effectively on the development set.
For the restaurant data ? where the gold standard
identified eight semantic properties ? we set K to
20, allowing the model to account for keyphrases not
included in the eight most common properties. For
the cell phones category, we set K to 30.
To improve the model?s convergence rate, we per-
form two initialization steps for the Gibbs sampler.
First, sampling is done only on the keyphrase clus-
tering component of the model, ignoring document
text. Second, we fix this clustering and sample the
remaining model parameters. These two steps are
run for 5,000 iterations each. The full joint model
is then sampled for 100,000 iterations. Inspection
of the parameter estimates confirms model conver-
gence. On a 2GHz dual-core desktop machine, a
multi-threaded C++ implementation of model train-
ing takes about two hours for each dataset.
Inference The final point estimate used for test-
ing is an average (for continuous variables) or a
mode (for discrete variables) over the last 1,000
Gibbs sampling iterations. Averaging is a heuris-
tic that is applicable in our case because our sam-
ple histograms are unimodal and exhibit low skew.
The model usually works equally well using single-
sample estimates, but is more prone to estimation
noise.
As previously mentioned, we convert word topic
assignments to document properties by examining
the proportion of words supporting each property. A
threshold for this proportion is set for each property
via the development set.
Evaluation Our first evaluation examines the ac-
curacy of our model and the baselines by compar-
ing their output against the keyphrases provided by
the review authors. More specifically, the model
first predicts the properties supported by a given re-
view. We then test whether the original authors?
keyphrases are contained in the clusters associated
with these properties.
As noted above, the authors? keyphrases are of-
ten incomplete. To perform a noise-free compari-
son, we based our second evaluation on the man-
ually constructed gold standard for the restaurant
category. We took the most commonly observed
keyphrase from each of the eight annotated proper-
ties, and tested whether they are supported by the
model based on the document text.
In both types of evaluation, we measure the
model?s performance using precision, recall, and F-
score. These are computed in the standard manner,
based on the model?s keyphrase predictions com-
pared against the corresponding references. The
sign test was used for statistical significance test-
ing (De Groot and Schervish, 2001).
Baselines To the best of our knowledge, this task
not been previously addressed in the literature. We
therefore consider five baselines that allow us to ex-
plore the properties of this task and our model.
Random: Each keyphrase is supported by a doc-
ument with probability of one half. This baseline?s
results are computed (in expectation) rather than ac-
tually run. This method is expected to have a recall
of 0.5, because in expectation it will select half of
the correct keyphrases. Its precision is the propor-
tion of supported keyphrases in the test set.
Phrase in text: A keyphrase is supported by a doc-
ument if it appears verbatim in the text. Because of
this narrow requirement, precision should be high
whereas recall will be low.
268
Restaurants Restaurants Cell Phones
gold standard annotation free-text annotation free-text annotation
Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score
Random 0.500 0.300 ? 0.375 0.500 0.500 ? 0.500 0.500 0.489 ? 0.494
Phrase in text 0.048 0.500 ? 0.087 0.078 0.909 ? 0.144 0.171 0.529 ? 0.259
Cluster in text 0.223 0.534 0.314 0.517 0.640 ? 0.572 0.829 0.547 0.659
Phrase classifier 0.028 0.636 ? 0.053 0.068 0.963 ? 0.126 0.029 0.600 ? 0.055
Cluster classifier 0.113 0.622 ? 0.192 0.255 0.907 ? 0.398 0.210 0.759 0.328
Our model 0.625 0.416 0.500 0.901 0.652 0.757 0.886 0.585 0.705
Our model + gold clusters 0.582 0.398 0.472 0.795 0.627 ? 0.701 0.886 0.520 ? 0.655
Table 2: Comparison of the property predictions made by our model and the baselines in the two categories as evaluated
against the gold and free-text annotations. Results for our model using the fixed, manually-created gold clusterings are
also shown. The methods against which our model has significantly better results on the sign test are indicated with a
? for p <= 0.05, and ? for p <= 0.1.
Cluster in text: A keyphrase is supported by a
document if it or any of its paraphrases appears in
the text. Paraphrasing is based on our model?s clus-
tering of the keyphrases. The use of paraphrasing
information enhances recall at the potential cost of
precision, depending on the quality of the clustering.
Phrase classifier: Discriminative classifiers are
trained for each keyphrase. Positive examples are
documents that are labeled with the keyphrase;
all other documents are negative examples. A
keyphrase is supported by a document if that
keyphrase?s classifier returns positive.
Cluster classifier: Discriminative classifiers are
trained for each cluster of keyphrases, using our
model?s clustering. Positive examples are docu-
ments that are labeled with any keyphrase from the
cluster; all other documents are negative examples.
All keyphrases of a cluster are supported by a docu-
ment if that cluster?s classifier returns positive.
Phrase classifier and cluster classifier employ
maximum entropy classifiers, trained on the same
features as our model, i.e., word counts. The former
is high-precision/low-recall, because for any partic-
ular keyphrase, its synonymous keyphrases would
be considered negative examples. The latter broad-
ens the positive examples, which should improve re-
call. We used Zhang Le?s MaxEnt toolkit3 to build
these classifiers.
3http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
7 Results
Comparative performance Table 2 presents the
results of the evaluation scenarios described above.
Our model outperforms every baseline by a wide
margin in all evaluations.
The absolute performance of the automatic meth-
ods indicates the difficulty of the task. For instance,
evaluation against gold standard annotations shows
that the random baseline outperforms all of the other
baselines. We observe similar disappointing results
for the non-random baselines against the free-text
annotations. The precision and recall characteristics
of the baselines match our previously described ex-
pectations.
The poor performance of the discriminative mod-
els seems surprising at first. However, these re-
sults can be explained by the degree of noise in
the training data, specifically, the aforementioned
sparsity of free-text annotations. As previously de-
scribed, our technique allows document text topics
to stochastically derive from either the keyphrases or
a background distribution ? this allows our model
to learn effectively from incomplete annotations. In
fact, when we force all text topics to derive from
keyphrase clusters in our model, its performance de-
grades to the level of the classifiers or worse, with
an F-score of 0.390 in the restaurant category and
0.171 in the cell phone category.
Impact of paraphrasing As previously ob-
served in entailment research (Dagan et al, 2006),
paraphrasing information contributes greatly to im-
proved performance on semantic inference. This is
269
Figure 4: Sample keyphrase clusters that our model infers
in the cell phone category.
confirmed by the dramatic difference in results be-
tween the cluster in text and phrase in text baselines.
Therefore it is important to quantify the quality of
automatically computed paraphrases, such as those
illustrated in Figure 4.
Restaurants Cell Phones
Keyphrase similarity only 0.931 0.759
Joint training 0.966 0.876
Table 3: Rand Index scores of our model?s clusters, using
only keyphrase similarity vs. using keyphrases and text
jointly. Comparison of cluster quality is against the gold
standard.
One way to assess clustering quality is to com-
pare it against a ?gold standard? clustering, as con-
structed in Section 6. For this purpose, we use the
Rand Index (Rand, 1971), a measure of cluster sim-
ilarity. This measure varies from zero to one; higher
scores are better. Table 3 shows the Rand Indices
for our model?s clustering, as well as the clustering
obtained by using only keyphrase similarity. These
scores confirm that joint inference produces better
clusters than using only keyphrases.
Another way of assessing cluster quality is to con-
sider the impact of using the gold standard clustering
instead of our model?s clustering. As shown in the
last two lines of Table 2, using the gold clustering
yields results worse than using the model clustering.
This indicates that for the purposes of our task, the
model clustering is of sufficient quality.
8 Conclusions and Future Work
In this paper, we have shown how free-text anno-
tations provided by novice users can be leveraged
as a training set for document-level semantic infer-
ence. The resulting hierarchical Bayesian model
overcomes the lack of consistency in such anno-
tations by inducing a hidden structure of seman-
tic properties, which correspond both to clusters of
keyphrases and hidden topic models in the text. Our
system successfully extracts semantic properties of
unannotated restaurant and cell phone reviews, em-
pirically validating our approach.
Our present model makes strong assumptions
about the independence of similarity scores. We be-
lieve this could be avoided by modeling the genera-
tion of the entire similarity matrix jointly. We have
also assumed that the properties themselves are un-
structured, but they are in fact related in interest-
ing ways. For example, it would be desirable to
model antonyms explicitly, e.g., no restaurant review
should be simultaneously labeled as having good
and bad food. The correlated topic model (Blei and
Lafferty, 2006) is one way to account for relation-
ships between hidden topics; more structured repre-
sentations, such as hierarchies, may also be consid-
ered.
Finally, the core idea of using free-text as a
source of training labels has wide applicability, and
has the potential to enable sophisticated content
search and analysis. For example, online blog en-
tries are often tagged with short keyphrases. Our
technique could be used to standardize these tags,
and assign keyphrases to untagged blogs. The no-
tion of free-text annotations is also very broad ?
we are currently exploring the applicability of this
model to Wikipedia articles, using section titles as
keyphrases, to build standard article schemas.
Acknowledgments
The authors acknowledge the support of the NSF,
Quanta Computer, the U.S. Office of Naval Re-
search, and DARPA. Thanks to Michael Collins,
Dina Katabi, Kristian Kersting, Terry Koo, Brian
Milch, Tahira Naseem, Dan Roy, Benjamin Snyder,
Luke Zettlemoyer, and the anonymous reviewers for
helpful comments and suggestions. Any opinions,
findings, and conclusions or recommendations ex-
pressed above are those of the authors and do not
necessarily reflect the views of the NSF.
270
References
David M. Blei and John D. Lafferty. 2006. Correlated
topic models. In Advances in NIPS, pages 147?154.
David M. Blei and Jon McAuliffe. 2007. Supervised
topic models. In Advances in NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. Lecture Notes in Computer Science,
3944:177?190.
Morris H. De Groot and Mark J. Schervish. 2001. Prob-
ability and Statistics. Addison Wesley.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by Gibbs sampling. In
Proceedings of the ACL, pages 363?370.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, 2nd edi-
tion.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of ACL, pages
673?680.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of SIGKDD,
pages 168?177.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING/ACL, pages 483?490.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML, pages 296?304.
Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.
2005. OPINE: Extracting product features and opin-
ions from reviews. In Proceedings of HLT/EMNLP,
pages 339?346.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850, December.
Bruce Sterling. 2005. Order out of chaos: What is the
best way to tag, bag, and sort data? Give it to the
unorganized masses. http://www.wired.com/
wired/archive/13.04/view.html?pg=4.
Accessed April 21, 2008.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the ACL.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Advances in NIPS.
Graham Vickery and Sacha Wunsch-Vincent. 2007. Par-
ticipative Web and User-Created Content: Web 2.0,
Wikis and Social Networking. OECD Publishing.
271
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82?90,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Reinforcement Learning for Mapping Instructions to Actions
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, harr, lsz, regina}@csail.mit.edu
Abstract
In this paper, we present a reinforce-
ment learning approach for mapping nat-
ural language instructions to sequences of
executable actions. We assume access to
a reward function that defines the qual-
ity of the executed actions. During train-
ing, the learner repeatedly constructs ac-
tion sequences for a set of documents, ex-
ecutes those actions, and observes the re-
sulting reward. We use a policy gradient
algorithm to estimate the parameters of a
log-linear model for action selection. We
apply our method to interpret instructions
in two domains ? Windows troubleshoot-
ing guides and game tutorials. Our results
demonstrate that this method can rival su-
pervised learning techniques while requir-
ing few or no annotated training exam-
ples.1
1 Introduction
The problem of interpreting instructions written
in natural language has been widely studied since
the early days of artificial intelligence (Winograd,
1972; Di Eugenio, 1992). Mapping instructions to
a sequence of executable actions would enable the
automation of tasks that currently require human
participation. Examples include configuring soft-
ware based on how-to guides and operating simu-
lators using instruction manuals. In this paper, we
present a reinforcement learning framework for in-
ducing mappings from text to actions without the
need for annotated training examples.
For concreteness, consider instructions from a
Windows troubleshooting guide on deleting tem-
porary folders, shown in Figure 1. We aim to map
1Code, data, and annotations used in this work are avail-
able at http://groups.csail.mit.edu/rbg/code/rl/
Figure 1: A Windows troubleshooting article de-
scribing how to remove the ?msdownld.tmp? tem-
porary folder.
this text to the corresponding low-level commands
and parameters. For example, properly interpret-
ing the third instruction requires clicking on a tab,
finding the appropriate option in a tree control, and
clearing its associated checkbox.
In this and many other applications, the valid-
ity of a mapping can be verified by executing the
induced actions in the corresponding environment
and observing their effects. For instance, in the
example above we can assess whether the goal
described in the instructions is achieved, i.e., the
folder is deleted. The key idea of our approach
is to leverage the validation process as the main
source of supervision to guide learning. This form
of supervision allows us to learn interpretations
of natural language instructions when standard su-
pervised techniques are not applicable, due to the
lack of human-created annotations.
Reinforcement learning is a natural framework
for building models using validation from an envi-
ronment (Sutton and Barto, 1998). We assume that
supervision is provided in the form of a reward
function that defines the quality of executed ac-
tions. During training, the learner repeatedly con-
structs action sequences for a set of given docu-
ments, executes those actions, and observes the re-
sulting reward. The learner?s goal is to estimate a
82
policy ? a distribution over actions given instruc-
tion text and environment state ? that maximizes
future expected reward. Our policy is modeled in a
log-linear fashion, allowing us to incorporate fea-
tures of both the instruction text and the environ-
ment. We employ a policy gradient algorithm to
estimate the parameters of this model.
We evaluate our method on two distinct applica-
tions: Windows troubleshooting guides and puz-
zle game tutorials. The key findings of our ex-
periments are twofold. First, models trained only
with simple reward signals achieve surprisingly
high results, coming within 11% of a fully su-
pervised method in the Windows domain. Sec-
ond, augmenting unlabeled documents with even
a small fraction of annotated examples greatly re-
duces this performance gap, to within 4% in that
domain. These results indicate the power of learn-
ing from this new form of automated supervision.
2 Related Work
Grounded Language Acquisition Our work
fits into a broader class of approaches that aim to
learn language from a situated context (Mooney,
2008a; Mooney, 2008b; Fleischman and Roy,
2005; Yu and Ballard, 2004; Siskind, 2001; Oates,
2001). Instances of such approaches include
work on inferring the meaning of words from
video data (Roy and Pentland, 2002; Barnard and
Forsyth, 2001), and interpreting the commentary
of a simulated soccer game (Chen and Mooney,
2008). Most of these approaches assume some
form of parallel data, and learn perceptual co-
occurrence patterns. In contrast, our emphasis
is on learning language by proactively interacting
with an external environment.
Reinforcement Learning for Language Pro-
cessing Reinforcement learning has been previ-
ously applied to the problem of dialogue manage-
ment (Scheffler and Young, 2002; Roy et al, 2000;
Litman et al, 2000; Singh et al, 1999). These
systems converse with a human user by taking ac-
tions that emit natural language utterances. The
reinforcement learning state space encodes infor-
mation about the goals of the user and what they
say at each time step. The learning problem is to
find an optimal policy that maps states to actions,
through a trial-and-error process of repeated inter-
action with the user.
Reinforcement learning is applied very differ-
ently in dialogue systems compared to our setup.
In some respects, our task is more easily amenable
to reinforcement learning. For instance, we are not
interacting with a human user, so the cost of inter-
action is lower. However, while the state space can
be designed to be relatively small in the dialogue
management task, our state space is determined by
the underlying environment and is typically quite
large. We address this complexity by developing
a policy gradient algorithm that learns efficiently
while exploring a small subset of the states.
3 Problem Formulation
Our task is to learn a mapping between documents
and the sequence of actions they express. Figure 2
shows how one example sentence is mapped to
three actions.
Mapping Text to Actions As input, we are
given a document d, comprising a sequence of sen-
tences (u1, . . . , u`), where each ui is a sequence
of words. Our goal is to map d to a sequence of
actions ~a = (a0, . . . , an?1). Actions are predicted
and executed sequentially.2
An action a = (c,R,W ?) encompasses a com-
mand c, the command?s parameters R, and the
words W ? specifying c and R. Elements of R re-
fer to objects available in the environment state, as
described below. Some parameters can also refer
to words in document d. Additionally, to account
for words that do not describe any actions, c can
be a null command.
The Environment The environment state E
specifies the set of objects available for interac-
tion, and their properties. In Figure 2, E is shown
on the right. The environment state E changes
in response to the execution of command c with
parameters R according to a transition distribu-
tion p(E ?|E , c, R). This distribution is a priori un-
known to the learner. As we will see in Section 5,
our approach avoids having to directly estimate
this distribution.
State To predict actions sequentially, we need to
track the state of the document-to-actions map-
ping over time. A mapping state s is a tuple
(E , d, j,W ), where E refers to the current environ-
ment state; j is the index of the sentence currently
being interpreted in document d; and W contains
words that were mapped by previous actions for
2That is, action ai is executed before ai+1 is predicted.
83
Figure 2: A three-step mapping from an instruction sentence to a sequence of actions in Windows 2000.
For each step, the figure shows the words selected by the action, along with the corresponding system
command and its parameters. The words of W ? are underlined, and the words of W are highlighted in
grey.
the same sentence. The mapping state s is ob-
served after each action.
The initial mapping state s0 for document d is
(Ed, d, 0, ?); Ed is the unique starting environment
state for d. Performing action a in state s =
(E , d, j,W ) leads to a new state s? according to
distribution p(s?|s, a), defined as follows: E tran-
sitions according to p(E ?|E , c, R), W is updated
with a?s selected words, and j is incremented if
all words of the sentence have been mapped. For
the applications we consider in this work, environ-
ment state transitions, and consequently mapping
state transitions, are deterministic.
Training During training, we are provided with
a set D of documents, the ability to sample from
the transition distribution, and a reward function
r(h). Here, h = (s0, a0, . . . , sn?1, an?1, sn) is
a history of states and actions visited while in-
terpreting one document. r(h) outputs a real-
valued score that correlates with correct action
selection.3 We consider both immediate reward,
which is available after each action, and delayed
reward, which does not provide feedback until the
last action. For example, task completion is a de-
layed reward that produces a positive value after
the final action only if the task was completed suc-
cessfully. We will also demonstrate how manu-
ally annotated action sequences can be incorpo-
rated into the reward.
3In most reinforcement learning problems, the reward
function is defined over state-action pairs, as r(s, a) ? in this
case, r(h) =
P
t r(st, at), and our formulation becomes a
standard finite-horizon Markov decision process. Policy gra-
dient approaches allow us to learn using the more general
case of history-based reward.
The goal of training is to estimate parameters ?
of the action selection distribution p(a|s, ?), called
the policy. Since the reward correlates with ac-
tion sequence correctness, the ? that maximizes
expected reward will yield the best actions.
4 A Log-Linear Model for Actions
Our goal is to predict a sequence of actions. We
construct this sequence by repeatedly choosing an
action given the current mapping state, and apply-
ing that action to advance to a new state.
Given a state s = (E , d, j,W ), the space of pos-
sible next actions is defined by enumerating sub-
spans of unused words in the current sentence (i.e.,
subspans of the jth sentence of d not in W ), and
the possible commands and parameters in envi-
ronment state E .4 We model the policy distribu-
tion p(a|s; ?) over this action space in a log-linear
fashion (Della Pietra et al, 1997; Lafferty et al,
2001), giving us the flexibility to incorporate a di-
verse range of features. Under this representation,
the policy distribution is:
p(a|s; ?) =
e???(s,a)
?
a?
e???(s,a
?)
, (1)
where ?(s, a) ? Rn is an n-dimensional feature
representation. During test, actions are selected
according to the mode of this distribution.
4For parameters that refer to words, the space of possible
values is defined by the unused words in the current sentence.
84
5 Reinforcement Learning
During training, our goal is to find the optimal pol-
icy p(a|s; ?). Since reward correlates with correct
action selection, a natural objective is to maximize
expected future reward ? that is, the reward we
expect while acting according to that policy from
state s. Formally, we maximize the value function:
V?(s) = Ep(h|?) [r(h)] , (2)
where the history h is the sequence of states and
actions encountered while interpreting a single
document d ? D. This expectation is averaged
over all documents in D. The distribution p(h|?)
returns the probability of seeing history h when
starting from state s and acting according to a pol-
icy with parameters ?. This distribution can be de-
composed into a product over time steps:
p(h|?) =
n?1?
t=0
p(at|st; ?)p(st+1|st, at). (3)
5.1 A Policy Gradient Algorithm
Our reinforcement learning problem is to find the
parameters ? that maximize V? from equation 2.
Although there is no closed form solution, policy
gradient algorithms (Sutton et al, 2000) estimate
the parameters ? by performing stochastic gradi-
ent ascent. The gradient of V? is approximated by
interacting with the environment, and the resulting
reward is used to update the estimate of ?. Policy
gradient algorithms optimize a non-convex objec-
tive and are only guaranteed to find a local opti-
mum. However, as we will see, they scale to large
state spaces and can perform well in practice.
To find the parameters ? that maximize the ob-
jective, we first compute the derivative of V?. Ex-
panding according to the product rule, we have:
?
??
V?(s) = Ep(h|?)
[
r(h)
?
t
?
??
log p(at|st; ?)
]
,
(4)
where the inner sum is over all time steps t in
the current history h. Expanding the inner partial
derivative we observe that:
?
??
log p(a|s; ?) = ?(s, a)?
?
a?
?(s, a?)p(a?|s; ?),
(5)
which is the derivative of a log-linear distribution.
Equation 5 is easy to compute directly. How-
ever, the complete derivative of V? in equation 4
Input: A document set D,
Feature representation ?,
Reward function r(h),
Number of iterations T
Initialization: Set ? to small random values.
for i = 1 . . . T do1
foreach d ? D do2
Sample history h ? p(h|?) where3
h = (s0, a0, . . . , an?1, sn) as follows:
3a for t = 0 . . . n? 1 do
3b Sample action at ? p(a|st; ?)
3c Execute at on state st: st+1 ? p(s|st, at)
end
??
P
t
`
?(st, at)?
P
a? ?(st, a
?)p(a?|st; ?)
?
4
? ? ? + r(h)?5
end
end
Output: Estimate of parameters ?
Algorithm 1: A policy gradient algorithm.
is intractable, because computing the expectation
would require summing over all possible histo-
ries. Instead, policy gradient algorithms employ
stochastic gradient ascent by computing a noisy
estimate of the expectation using just a subset of
the histories. Specifically, we draw samples from
p(h|?) by acting in the target environment, and
use these samples to approximate the expectation
in equation 4. In practice, it is often sufficient to
sample a single history h for this approximation.
Algorithm 1 details the complete policy gradi-
ent algorithm. It performs T iterations over the
set of documents D. Step 3 samples a history that
maps each document to actions. This is done by
repeatedly selecting actions according to the cur-
rent policy, and updating the state by executing the
selected actions. Steps 4 and 5 compute the empir-
ical gradient and update the parameters ?.
In many domains, interacting with the environ-
ment is expensive. Therefore, we use two tech-
niques that allow us to take maximum advantage
of each environment interaction. First, a his-
tory h = (s0, a0, . . . , sn) contains subsequences
(si, ai, . . . sn) for i = 1 to n ? 1, each with its
own reward value given by the environment as a
side effect of executing h. We apply the update
from equation 5 for each subsequence. Second,
for a sampled history h, we can propose alterna-
tive histories h? that result in the same commands
and parameters with different word spans. We can
again apply equation 5 for each h?, weighted by its
probability under the current policy, p(h
?|?)
p(h|?) .
85
The algorithm we have presented belongs to
a family of policy gradient algorithms that have
been successfully used for complex tasks such as
robot control (Ng et al, 2003). Our formulation is
unique in how it represents natural language in the
reinforcement learning framework.
5.2 Reward Functions and ML Estimation
We can design a range of reward functions to guide
learning, depending on the availability of anno-
tated data and environment feedback. Consider the
case when every training document d ? D is an-
notated with its correct sequence of actions, and
state transitions are deterministic. Given these ex-
amples, it is straightforward to construct a reward
function that connects policy gradient to maxi-
mum likelihood. Specifically, define a reward
function r(h) that returns one when h matches the
annotation for the document being analyzed, and
zero otherwise. Policy gradient performs stochas-
tic gradient ascent on the objective from equa-
tion 2, performing one update per document. For
document d, this objective becomes:
Ep(h|?)[r(h)] =
?
h
r(h)p(h|?) = p(hd|?),
where hd is the history corresponding to the an-
notated action sequence. Thus, with this reward
policy gradient is equivalent to stochastic gradient
ascent with a maximum likelihood objective.
At the other extreme, when annotations are
completely unavailable, learning is still possi-
ble given informative feedback from the environ-
ment. Crucially, this feedback only needs to cor-
relate with action sequence quality. We detail
environment-based reward functions in the next
section. As our results will show, reward func-
tions built using this kind of feedback can provide
strong guidance for learning. We will also con-
sider reward functions that combine annotated su-
pervision with environment feedback.
6 Applying the Model
We study two applications of our model: follow-
ing instructions to perform software tasks, and
solving a puzzle game using tutorial guides.
6.1 Microsoft Windows Help and Support
On its Help and Support website,5 Microsoft pub-
lishes a number of articles describing how to per-
5support.microsoft.com
Notation
o Parameter referring to an environment object
L Set of object class names (e.g. ?button?)
V Vocabulary
Features onW and object o
Test if o is visible in s
Test if o has input focus
Test if o is in the foreground
Test if o was previously interacted with
Test if o came into existence since last action
Min. edit distance between w ?W and object labels in s
Features on words inW , command c, and object o
?c? ? C, w ? V : test if c? = c and w ?W
?c? ? C, l ? L: test if c? = c and l is the class of o
Table 1: Example features in the Windows do-
main. All features are binary, except for the nor-
malized edit distance which is real-valued.
form tasks and troubleshoot problems in the Win-
dows operating systems. Examples of such tasks
include installing patches and changing security
settings. Figure 1 shows one such article.
Our goal is to automatically execute these sup-
port articles in the Windows 2000 environment.
Here, the environment state is the set of visi-
ble user interface (UI) objects, and object prop-
erties such as label, location, and parent window.
Possible commands include left-click, right-click,
double-click, and type-into, all of which take a UI
object as a parameter; type-into additionally re-
quires a parameter for the input text.
Table 1 lists some of the features we use for this
domain. These features capture various aspects of
the action under consideration, the current Win-
dows UI state, and the input instructions. For ex-
ample, one lexical feature measures the similar-
ity of a word in the sentence to the UI labels of
objects in the environment. Environment-specific
features, such as whether an object is currently in
focus, are useful when selecting the object to ma-
nipulate. In total, there are 4,438 features.
Reward Function Environment feedback can
be used as a reward function in this domain. An
obvious reward would be task completion (e.g.,
whether the stated computer problem was fixed).
Unfortunately, verifying task completion is a chal-
lenging system issue in its own right.
Instead, we rely on a noisy method of check-
ing whether execution can proceed from one sen-
tence to the next: at least one word in each sen-
tence has to correspond to an object in the envi-
86
Figure 3: Crossblock puzzle with tutorial. For this
level, four squares in a row or column must be re-
moved at once. The first move specified by the
tutorial is greyed in the puzzle.
ronment.6 For instance, in the sentence from Fig-
ure 2 the word ?Run? matches the Run... menu
item. If no words in a sentence match a current
environment object, then one of the previous sen-
tences was analyzed incorrectly. In this case, we
assign the history a reward of -1. This reward is
not guaranteed to penalize all incorrect histories,
because there may be false positive matches be-
tween the sentence and the environment. When
at least one word matches, we assign a positive
reward that linearly increases with the percentage
of words assigned to non-null commands, and lin-
early decreases with the number of output actions.
This reward signal encourages analyses that inter-
pret al of the words without producing spurious
actions.
6.2 Crossblock: A Puzzle Game
Our second application is to a puzzle game called
Crossblock, available online as a Flash game.7
Each of 50 puzzles is played on a grid, where some
grid positions are filled with squares. The object
of the game is to clear the grid by drawing vertical
or horizontal line segments that remove groups of
squares. Each segment must exactly cross a spe-
cific number of squares, ranging from two to seven
depending on the puzzle. Humans players have
found this game challenging and engaging enough
to warrant posting textual tutorials.8 A sample
puzzle and tutorial are shown in Figure 3.
The environment is defined by the state of the
grid. The only command is clear, which takes a
parameter specifying the orientation (row or col-
umn) and grid location of the line segment to be
6We assume that a word maps to an environment object if
the edit distance between the word and the object?s name is
below a threshold value.
7hexaditidom.deviantart.com/art/Crossblock-108669149
8www.jayisgames.com/archives/2009/01/crossblock.php
removed. The challenge in this domain is to seg-
ment the text into the phrases describing each ac-
tion, and then correctly identify the line segments
from references such as ?the bottom four from the
second column from the left.?
For this domain, we use two sets of binary fea-
tures on state-action pairs (s, a). First, for each
vocabulary word w, we define a feature that is one
if w is the last word of a?s consumed words W ?.
These features help identify the proper text seg-
mentation points between actions. Second, we in-
troduce features for pairs of vocabulary word w
and attributes of action a, e.g., the line orientation
and grid locations of the squares that a would re-
move. This set of features enables us to match
words (e.g., ?row?) with objects in the environ-
ment (e.g., a move that removes a horizontal series
of squares). In total, there are 8,094 features.
Reward Function For Crossblock it is easy to
directly verify task completion, which we use as
the basis of our reward function. The reward r(h)
is -1 if h ends in a state where the puzzle cannot
be completed. For solved puzzles, the reward is
a positive value proportional to the percentage of
words assigned to non-null commands.
7 Experimental Setup
Datasets For the Windows domain, our dataset
consists of 128 documents, divided into 70 for
training, 18 for development, and 40 for test. In
the puzzle game domain, we use 50 tutorials,
divided into 40 for training and 10 for test.9
Statistics for the datasets are shown below.
Windows Puzzle
Total # of documents 128 50
Total # of words 5562 994
Vocabulary size 610 46
Avg. words per sentence 9.93 19.88
Avg. sentences per document 4.38 1.00
Avg. actions per document 10.37 5.86
The data exhibits certain qualities that make
for a challenging learning problem. For instance,
there are a surprising variety of linguistic con-
structs ? as Figure 4 shows, in the Windows do-
main even a simple command is expressed in at
least six different ways.
9For Crossblock, because the number of puzzles is lim-
ited, we did not hold out a separate development set, and re-
port averaged results over five training/test splits.
87
Figure 4: Variations of ?click internet options on
the tools menu? present in the Windows corpus.
Experimental Framework To apply our algo-
rithm to the Windows domain, we use the Win32
application programming interface to simulate hu-
man interactions with the user interface, and to
gather environment state information. The operat-
ing system environment is hosted within a virtual
machine,10 allowing us to rapidly save and reset
system state snapshots. For the puzzle game do-
main, we replicated the game with an implemen-
tation that facilitates automatic play.
As is commonly done in reinforcement learn-
ing, we use a softmax temperature parameter to
smooth the policy distribution (Sutton and Barto,
1998), set to 0.1 in our experiments. For Windows,
the development set is used to select the best pa-
rameters. For Crossblock, we choose the parame-
ters that produce the highest reward during train-
ing. During evaluation, we use these parameters
to predict mappings for the test documents.
Evaluation Metrics For evaluation, we com-
pare the results to manually constructed sequences
of actions. We measure the number of correct ac-
tions, sentences, and documents. An action is cor-
rect if it matches the annotations in terms of com-
mand and parameters. A sentence is correct if all
of its actions are correctly identified, and analo-
gously for documents.11 Statistical significance is
measured with the sign test.
Additionally, we compute a word alignment
score to investigate the extent to which the input
text is used to construct correct analyses. This
score measures the percentage of words that are
aligned to the corresponding annotated actions in
correctly analyzed documents.
Baselines We consider the following baselines
to characterize the performance of our approach.
10VMware Workstation, available at www.vmware.com
11In these tasks, each action depends on the correct execu-
tion of all previous actions, so a single error can render the
remainder of that document?s mapping incorrect. In addition,
due to variability in document lengths, overall action accu-
racy is not guaranteed to be higher than document accuracy.
? Full Supervision Sequence prediction prob-
lems like ours are typically addressed us-
ing supervised techniques. We measure how
a standard supervised approach would per-
form on this task by using a reward signal
based on manual annotations of output ac-
tion sequences, as defined in Section 5.2. As
shown there, policy gradient with this re-
ward is equivalent to stochastic gradient as-
cent with a maximum likelihood objective.
? Partial Supervision We consider the case
when only a subset of training documents is
annotated, and environment reward is used
for the remainder. Our method seamlessly
combines these two kinds of rewards.
? Random and Majority (Windows) We con-
sider two na??ve baselines. Both scan through
each sentence from left to right. A com-
mand c is executed on the object whose name
is encountered first in the sentence. This
command c is either selected randomly, or
set to the majority command, which is left-
click. This procedure is repeated until no
more words match environment objects.
? Random (Puzzle) We consider a baseline
that randomly selects among the actions that
are valid in the current game state.12
8 Results
Table 2 presents evaluation results on the test sets.
There are several indicators of the difficulty of this
task. The random and majority baselines? poor
performance in both domains indicates that na??ve
approaches are inadequate for these tasks. The
performance of the fully supervised approach pro-
vides further evidence that the task is challenging.
This difficulty can be attributed in part to the large
branching factor of possible actions at each step ?
on average, there are 27.14 choices per action in
the Windows domain, and 9.78 in the Crossblock
domain.
In both domains, the learners relying only
on environment reward perform well. Although
the fully supervised approach performs the best,
adding just a few annotated training examples
to the environment-based learner significantly re-
duces the performance gap.
12Since action selection is among objects, there is no natu-
ral majority baseline for the puzzle.
88
Windows Puzzle
Action Sent. Doc. Word Action Doc. Word
Random baseline 0.128 0.101 0.000 ?? 0.081 0.111 ??
Majority baseline 0.287 0.197 0.100 ?? ?? ?? ??
Environment reward ? 0.647 ? 0.590 ? 0.375 0.819 ? 0.428 ? 0.453 0.686
Partial supervision  0.723 ? 0.702 0.475 0.989 0.575 ? 0.523 0.850
Full supervision  0.756 0.714 0.525 0.991 0.632 0.630 0.869
Table 2: Performance on the test set with different reward signals and baselines. Our evaluation measures
the proportion of correct actions, sentences, and documents. We also report the percentage of correct
word alignments for the successfully completed documents. Note the puzzle domain has only single-
sentence documents, so its sentence and document scores are identical. The partial supervision line
refers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle. Each
result marked with ? or  is a statistically significant improvement over the result immediately above it;
? indicates p < 0.01 and  indicates p < 0.05.
Figure 5: Comparison of two training scenarios where training is done using a subset of annotated
documents, with and without environment reward for the remaining unannotated documents.
Figure 5 shows the overall tradeoff between an-
notation effort and system performance for the two
domains. The ability to make this tradeoff is one
of the advantages of our approach. The figure also
shows that augmenting annotated documents with
additional environment-reward documents invari-
ably improves performance.
The word alignment results from Table 2 in-
dicate that the learners are mapping the correct
words to actions for documents that are success-
fully completed. For example, the models that per-
form best in the Windows domain achieve nearly
perfect word alignment scores.
To further assess the contribution of the instruc-
tion text, we train a variant of our model without
access to text features. This is possible in the game
domain, where all of the puzzles share a single
goal state that is independent of the instructions.
This variant solves 34% of the puzzles, suggest-
ing that access to the instructions significantly im-
proves performance.
9 Conclusions
In this paper, we presented a reinforcement learn-
ing approach for inducing a mapping between in-
structions and actions. This approach is able to use
environment-based rewards, such as task comple-
tion, to learn to analyze text. We showed that hav-
ing access to a suitable reward function can signif-
icantly reduce the need for annotations.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0835445,
grant IIS-0835652, and a Graduate Research Fel-
lowship) and the ONR. Thanks to Michael Collins,
Amir Globerson, Tommi Jaakkola, Leslie Pack
Kaelbling, Dina Katabi, Martin Rinard, and mem-
bers of the MIT NLP group for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
89
References
Kobus Barnard and David A. Forsyth. 2001. Learning
the semantics of words and pictures. In Proceedings
of ICCV.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language acqui-
sition. In Proceedings of ICML.
Stephen Della Pietra, Vincent J. Della Pietra, and
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Trans. Pattern Anal. Mach. Intell.,
19(4):380?393.
Barbara Di Eugenio. 1992. Understanding natural lan-
guage instructions: the case of purpose clauses. In
Proceedings of ACL.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated language learning. In Proceed-
ings of CoNLL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Diane J. Litman, Michael S. Kearns, Satinder Singh,
and Marilyn A. Walker. 2000. Automatic optimiza-
tion of dialogue management. In Proceedings of
COLING.
Raymond J. Mooney. 2008a. Learning language
from its perceptual context. In Proceedings of
ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings of AAAI.
Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, and
Shankar Sastry. 2003. Autonomous helicopter flight
via reinforcement learning. In Advances in NIPS.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
Deb K. Roy and Alex P. Pentland. 2002. Learn-
ing words from sights and sounds: a computational
model. Cognitive Science 26, pages 113?146.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of ACL.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman,
and Marilyn A. Walker. 1999. Reinforcement learn-
ing for spoken dialogue systems. In Advances in
NIPS.
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. J. Artif. Intell. Res. (JAIR),
15:31?90.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. The MIT
Press.
Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 2000. Policy gradient meth-
ods for reinforcement learning with function approx-
imation. In Advances in NIPS.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press.
Chen Yu and Dana H. Ballard. 2004. On the integra-
tion of grounding language and learning objects. In
Proceedings of AAAI.
90
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1268?1277,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Reading Between the Lines:
Learning to Map High-level Instructions to Commands
S.R.K. Branavan, Luke S. Zettlemoyer, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, lsz, regina}@csail.mit.edu
Abstract
In this paper, we address the task of
mapping high-level instructions to se-
quences of commands in an external en-
vironment. Processing these instructions
is challenging?they posit goals to be
achieved without specifying the steps re-
quired to complete them. We describe
a method that fills in missing informa-
tion using an automatically derived envi-
ronment model that encodes states, tran-
sitions, and commands that cause these
transitions to happen. We present an ef-
ficient approximate approach for learning
this environment model as part of a policy-
gradient reinforcement learning algorithm
for text interpretation. This design enables
learning for mapping high-level instruc-
tions, which previous statistical methods
cannot handle.1
1 Introduction
In this paper, we introduce a novel method for
mapping high-level instructions to commands in
an external environment. These instructions spec-
ify goals to be achieved without explicitly stat-
ing all the required steps. For example, consider
the first instruction in Figure 1 ? ?open control
panel.? The three GUI commands required for its
successful execution are not explicitly described
in the text, and need to be inferred by the user.
This dependence on domain knowledge makes the
automatic interpretation of high-level instructions
particularly challenging.
The standard approach to this task is to start
with both a manually-developed model of the en-
vironment, and rules for interpreting high-level in-
structions in the context of this model (Agre and
1Code, data, and annotations used in this work are avail-
able at http://groups.csail.mit.edu/rbg/code/rl-hli/
Chapman, 1988; Di Eugenio and White, 1992;
Di Eugenio, 1992; Webber et al, 1995). Given
both the model and the rules, logic-based infer-
ence is used to automatically fill in the intermedi-
ate steps missing from the original instructions.
Our approach, in contrast, operates directly on
the textual instructions in the context of the in-
teractive environment, while requiring no addi-
tional information. By interacting with the en-
vironment and observing the resulting feedback,
our method automatically learns both the mapping
between the text and the commands, and the un-
derlying model of the environment. One partic-
ularly noteworthy aspect of our solution is the in-
terplay between the evolving mapping and the pro-
gressively acquired environment model as the sys-
tem learns how to interpret the text. Recording the
state transitions observed during interpretation al-
lows the algorithm to construct a relevant model
of the environment. At the same time, the envi-
ronment model enables the algorithm to consider
the consequences of commands before they are ex-
ecuted, thereby improving the accuracy of inter-
pretation. Our method efficiently achieves both of
these goals as part of a policy-gradient reinforce-
ment learning algorithm.
We apply our method to the task of mapping
software troubleshooting guides to GUI actions in
the Windows environment (Branavan et al, 2009;
Kushman et al, 2009). The key findings of our
experiments are threefold. First, the algorithm
can accurately interpret 61.5% of high-level in-
structions, which cannot be handled by previous
statistical systems. Second, we demonstrate that
explicitly modeling the environment also greatly
improves the accuracy of processing low-level in-
structions, yielding a 14% absolute increase in
performance over a competitive baseline (Brana-
van et al, 2009). Finally, we show the importance
of constructing an environment model relevant to
the language interpretation task ? using textual
1268
"open control panel, double click system, then go to the advanced tab"
Document (input)D
"open control panel"
left-click Advanced
double-click System
left-click Control Panel
left-click Settings
left-click Start
InstructionsD
DocDumenemt
o (ip)Iios 
msrumenemt
o (ip)Iios (
Command Sequence (output)D
: :
:
:
:
:
"double click system"
"go to the advanced tab"
:
:
Figure 1: An example mapping of a document containing high-level instructions into a candidate se-
quence of five commands. The mapping process involves segmenting the document into individual in-
struction word spans Wa, and translating each instruction into the sequence ~c of one or more commands
it describes. During learning, the correct output command sequence is not provided to the algorithm.
instructions enables us to bias exploration toward
transitions relevant for language learning. This ap-
proach yields superior performance compared to a
policy that relies on an environment model con-
structed via random exploration.
2 Related Work
Interpreting Instructions Our approach is most
closely related to the reinforcement learning algo-
rithm for mapping text instructions to commands
developed by Branavan et al (2009) (see Section 4
for more detail). Their method is predicated on the
assumption that each command to be executed is
explicitly specified in the instruction text. This as-
sumption of a direct correspondence between the
text and the environment is not unique to that pa-
per, being inherent in other work on grounded lan-
guage learning (Siskind, 2001; Oates, 2001; Yu
and Ballard, 2004; Fleischman and Roy, 2005;
Mooney, 2008; Liang et al, 2009; Matuszek et
al., 2010). A notable exception is the approach
of Eisenstein et al (2009), which learns how an
environment operates by reading text, rather than
learning an explicit mapping from the text to the
environment. For example, their method can learn
the rules of a card game given instructions for how
to play.
Many instances of work on instruction inter-
pretation are replete with examples where in-
structions are formulated as high-level goals, tar-
geted at users with relevant knowledge (Winograd,
1972; Di Eugenio, 1992; Webber et al, 1995;
MacMahon et al, 2006). Not surprisingly, auto-
matic approaches for processing such instructions
have relied on hand-engineered world knowledge
to reason about the preconditions and effects of
environment commands. The assumption of a
fully specified environment model is also com-
mon in work on semantics in the linguistics lit-
erature (Lascarides and Asher, 2004). While our
approach learns to analyze instructions in a goal-
directed manner, it does not require manual speci-
fication of relevant environment knowledge.
Reinforcement Learning Our work combines
ideas of two traditionally disparate approaches to
reinforcement learning (Sutton and Barto, 1998).
The first approach, model-based learning, con-
structs a model of the environment in which the
learner operates (e.g., modeling location, velocity,
and acceleration in robot navigation). It then com-
putes a policy directly from the rich information
represented in the induced environment model.
In the NLP literature, model-based reinforcement
learning techniques are commonly used for dia-
log management (Singh et al, 2002; Lemon and
Konstas, 2009; Schatzmann and Young, 2009).
However, if the environment cannot be accurately
approximated by a compact representation, these
methods perform poorly (Boyan and Moore, 1995;
Jong and Stone, 2007). Our instruction interpreta-
tion task falls into this latter category,2 rendering
standard model-based learning ineffective.
The second approach ? model-free methods
such as policy learning ? aims to select the opti-
2For example, in the Windows GUI domain, clicking on
the File menu will result in a different submenu depending on
the application. Thus it is impossible to predict the effects of
a previously unseen GUI command.
1269
left-cikdo-uteo
-ft-btoiuu
eioiiiiii
LEFT_CLICK"ooooooopstart
-eoiii
iuProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 268?277,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning to Win by Reading Manuals in a Monte-Carlo Framework
S.R.K. Branavan David Silver * Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, regina}@csail.mit.edu
* Department of Computer Science
University College London
d.silver@cs.ucl.ac.uk
Abstract
This paper presents a novel approach for lever-
aging automatically extracted textual knowl-
edge to improve the performance of control
applications such as games. Our ultimate goal
is to enrich a stochastic player with high-
level guidance expressed in text. Our model
jointly learns to identify text that is relevant
to a given game state in addition to learn-
ing game strategies guided by the selected
text. Our method operates in the Monte-Carlo
search framework, and learns both text anal-
ysis and game strategies based only on envi-
ronment feedback. We apply our approach to
the complex strategy game Civilization II us-
ing the official game manual as the text guide.
Our results show that a linguistically-informed
game-playing agent significantly outperforms
its language-unaware counterpart, yielding a
27% absolute improvement and winning over
78% of games when playing against the built-
in AI of Civilization II. 1
1 Introduction
In this paper, we study the task of grounding lin-
guistic analysis in control applications such as com-
puter games. In these applications, an agent attempts
to optimize a utility function (e.g., game score) by
learning to select situation-appropriate actions. In
complex domains, finding a winning strategy is chal-
lenging even for humans. Therefore, human players
typically rely on manuals and guides that describe
promising tactics and provide general advice about
the underlying task. Surprisingly, such textual infor-
mation has never been utilized in control algorithms
despite its potential to greatly improve performance.
1The code, data and complete experimental setup for this
work are available at http://groups.csail.mit.edu/rbg/code/civ.
The natural resources available where a population
settles affects its ability to produce food and goods.  
Build your city on a plains or grassland square with 
a river running through it if possible.
Figure 1: An excerpt from the user manual of the game
Civilization II.
Consider for instance the text shown in Figure 1.
This is an excerpt from the user manual of the game
Civilization II.2 This text describes game locations
where the action ?build-city? can be effectively ap-
plied. A stochastic player that does not have access
to this text would have to gain this knowledge the
hard way: it would repeatedly attempt this action in
a myriad of states, thereby learning the characteri-
zation of promising state-action pairs based on the
observed game outcomes. In games with large state
spaces, long planning horizons, and high-branching
factors, this approach can be prohibitively slow and
ineffective. An algorithm with access to the text,
however, could learn correlations between words in
the text and game attributes ? e.g., the word ?river?
and places with rivers in the game ? thus leveraging
strategies described in text to better select actions.
The key technical challenge in leveraging textual
knowledge is to automatically extract relevant infor-
mation from text and incorporate it effectively into a
control algorithm. Approaching this task in a super-
vised framework, as is common in traditional infor-
mation extraction, is inherently difficult. Since the
game?s state space is extremely large, and the states
that will be encountered during game play cannot be
known a priori, it is impractical to manually anno-
tate the information that would be relevant to those
states. Instead, we propose to learn text analysis
based on a feedback signal inherent to the control
application, such as game score.
2http://en.wikipedia.org/wiki/Civilization II
268
Our general setup consists of a game in a stochas-
tic environment, where the goal of the player is to
maximize a given utility function R(s) at state s.
We follow a common formulation that has been the
basis of several successful applications of machine
learning to games. The player?s behavior is deter-
mined by an action-value function Q(s, a) that as-
sesses the goodness of an action a in a given state
s based on the features of s and a. This function is
learned based solely on the utilityR(s) collected via
simulated game-play in a Monte-Carlo framework.
An obvious way to enrich the model with textual
information is to augment the action-value function
with word features in addition to state and action
features. However, adding all the words in the docu-
ment is unlikely to help since only a small fraction of
the text is relevant for a given state. Moreover, even
when the relevant sentence is known, the mapping
between raw text and the action-state representation
may not be apparent. This representation gap can
be bridged by inducing a predicate structure on the
sentence?e.g., by identifying words that describe
actions, and those that describe state attributes.
In this paper, we propose a method for learning an
action-value function augmented with linguistic fea-
tures, while simultaneously modeling sentence rele-
vance and predicate structure. We employ a multi-
layer neural network where the hidden layers rep-
resent sentence relevance and predicate parsing de-
cisions. Despite the added complexity, all the pa-
rameters of this non-linear model can be effectively
learned via Monte-Carlo simulations.
We test our method on the strategy game Civiliza-
tion II, a notoriously challenging game with an im-
mense action space.3 As a source of knowledge for
guiding our model, we use the official game man-
ual. As a baseline, we employ a similar Monte-
Carlo search based player which does not have ac-
cess to textual information. We demonstrate that the
linguistically-informed player significantly outper-
forms the baseline in terms of number of games won.
Moreover, we show that modeling the deeper lin-
guistic structure of sentences further improves per-
formance. In full-length games, our algorithm yields
a 27% improvement over a language unaware base-
3Civilization II was #3 in IGN?s 2007 list of top video games
of all time (http://top100.ign.com/2007/ign top game 3.html)
line, and wins over 78% of games against the built-
in, hand-crafted AI of Civilization II.4
2 Related Work
Our work fits into the broad area of grounded lan-
guage acquisition where the goal is to learn linguis-
tic analysis from a situated context (Oates, 2001;
Siskind, 2001; Yu and Ballard, 2004; Fleischman
and Roy, 2005; Mooney, 2008a; Mooney, 2008b;
Branavan et al, 2009; Vogel and Jurafsky, 2010).
Within this line of work, we are most closely related
to reinforcement learning approaches that learn lan-
guage by proactively interacting with an external en-
vironment (Branavan et al, 2009; Branavan et al,
2010; Vogel and Jurafsky, 2010). Like the above
models, we use environment feedback (in the form
of a utility function) as the main source of supervi-
sion. The key difference, however, is in the language
interpretation task itself. Previous work has focused
on the interpretation of instruction text where input
documents specify a set of actions to be executed in
the environment. In contrast, game manuals provide
high-level advice but do not directly describe the
correct actions for every potential game state. More-
over, these documents are long, and use rich vocabu-
laries with complex grammatical constructions. We
do not aim to perform a comprehensive interpreta-
tion of such documents. Rather, our focus is on lan-
guage analysis that is sufficiently detailed to help the
underlying control task.
The area of language analysis situated in a game
domain has been studied in the past (Eisenstein et
al., 2009). Their method, however, is different both
in terms of the target interpretation task, and the su-
pervision signal it learns from. They aim to learn
the rules of a given game, such as which moves are
valid, given documents describing the rules. Our
goal is more open ended, in that we aim to learn
winning game strategies. Furthermore, Eisenstein et
al. (2009) rely on a different source of supervision ?
game traces collected a priori. For complex games,
like the one considered in this paper, collecting such
game traces is prohibitively expensive. Therefore
our approach learns by actively playing the game.
4In this paper, we focus primarily on the linguistic aspects
of our task and algorithm. For a discussion and evaluation of
the non-linguistic aspects please see Branavan et al (2011).
269
3 Monte-Carlo Framework for Computer
Games
Our method operates within the Monte-Carlo search
framework (Tesauro and Galperin, 1996), which
has been successfully applied to complex computer
games such as Go, Poker, Scrabble, multi-player
card games, and real-time strategy games, among
others (Gelly et al, 2006; Tesauro and Galperin,
1996; Billings et al, 1999; Sheppard, 2002; Scha?fer,
2008; Sturtevant, 2008; Balla and Fern, 2009).
Since Monte-Carlo search forms the foundation of
our approach, we briefly describe it in this section.
Game Representation The game is defined by a
large Markov Decision Process ?S,A, T,R?. Here
S is the set of possible states, A is the space of legal
actions, and T (s?|s, a) is a stochastic state transition
function where s, s? ? S and a ? A. Specifically, a
state encodes attributes of the game world, such as
available resources and city locations. At each step
of the game, a player executes an action a which
causes the current state s to change to a new state
s? according to the transition function T (s?|s, a).
While this function is not known a priori, the pro-
gram encoding the game can be viewed as a black
box from which transitions can be sampled. Finally,
a given utility function R(s) ? R captures the like-
lihood of winning the game from state s (e.g., an
intermediate game score).
Monte-Carlo Search Algorithm The goal of the
Monte-Carlo search algorithm is to dynamically se-
lect the best action for the current state st. This se-
lection is based on the results of multiple roll-outs
which measure the outcome of a sequence of ac-
tions in a simulated game ? e.g., simulations played
against the game?s built-in AI. Specifically, starting
at state st, the algorithm repeatedly selects and exe-
cutes actions, sampling state transitions from T . On
game completion at time ? , we measure the final
utility R(s? ).5 The actual game action is then se-
lected as the one corresponding to the roll-out with
the best final utility. See Algorithm 1 for details.
The success of Monte-Carlo search is based on
its ability to make a fast, local estimate of the ac-
5In general, roll-outs are run till game completion. However,
if simulations are expensive as is the case in our domain, roll-
outs can be truncated after a fixed number of steps.
procedure PlayGame ()
Initialize game state to fixed starting state
s1 ? s0
for t = 1 . . . T do
Run N simulated games
for i = 1 . . . N do
(ai, ri)? SimulateGame(s)
end
Compute average observed utility for each action
at ? arg max
a
1
Na
?
i:ai=a
ri
Execute selected action in game
st+1 ? T (s?|st, at)
end
procedure SimulateGame (st)
for u = t . . . ? do
Compute Q function approximation
Q(s, a) = ~w ? ~f(s, a)
Sample action from action-value function in
-greedy fashion:
au ?
{
uniform(a ? A) with probability 
arg max
a
Q(s, a) otherwise
Execute selected action in game:
su+1 ? T (s?|su, au)
if game is won or lost break
end
Update parameters ~w of Q(s, a)
Return action and observed utility:
return at, R(s? )
Algorithm 1: The general Monte-Carlo algorithm.
tion quality at each step of the roll-outs. States
and actions are evaluated by an action-value func-
tion Q(s, a), which is an estimate of the expected
outcome of action a in state s. This action-value
function is used to guide action selection during the
roll-outs. While actions are usually selected to max-
imize the action-value function, sometimes other ac-
tions are also randomly explored in case they are
more valuable than predicted by the current estimate
of Q(s, a). As the accuracy of Q(s, a) improves,
the quality of action selection improves and vice
270
versa, in a cycle of continual improvement (Sutton
and Barto, 1998).
In many games, it is sufficient to maintain a dis-
tinct action-value for each unique state and action
in a large search tree. However, when the branch-
ing factor is large it is usually beneficial to approx-
imate the action-value function, so that the value
of many related states and actions can be learned
from a reasonably small number of simulations (Sil-
ver, 2009). One successful approach is to model
the action-value function as a linear combination of
state and action attributes (Silver et al, 2008):
Q(s, a) = ~w ? ~f(s, a).
Here ~f(s, a) ? Rn is a real-valued feature function,
and ~w is a weight vector. We take a similar approach
here, except that our feature function includes latent
structure which models language.
The parameters ~w of Q(s, a) are learned based on
feedback from the roll-out simulations. Specifically,
the parameters are updated by stochastic gradient
descent by comparing the current predicted Q(s, a)
against the observed utility at the end of each roll-
out. We provide details on parameter estimation in
the context of our model in Section 4.2.
The roll-outs themselves are fully guided by the
action-value function. At every step of the simula-
tion, actions are selected by an -greedy strategy:
with probability  an action is selected uniformly
at random; otherwise the action is selected greed-
ily to maximize the current action-value function,
arg maxaQ(s, a).
4 Adding Linguistic Knowledge to the
Monte-Carlo Framework
In this section we describe how we inform the
simulation-based player with information automat-
ically extracted from text ? in terms of both model
structure and parameter estimation.
4.1 Model Structure
To inform action selection with the advice provided
in game manuals, we modify the action-value func-
tion Q(s, a) to take into account words of the doc-
ument in addition to state and action information.
Conditioning Q(s, a) on all the words in the docu-
ment is unlikely to be effective since only a small
Hidden layer encoding 
sentence relevance
Output layer
Input layer: Deterministic feature
layer:
Hidden layer encoding 
predicate labeling
Figure 2: The structure of our model. Each rectan-
gle represents a collection of units in a layer, and the
shaded trapezoids show the connections between layers.
A fixed, real-valued feature function ~x(s, a, d) transforms
the game state s, action a, and strategy document d into
the input vector ~x. The first hidden layer contains two
disjoint sets of units ~y and ~z corresponding to linguis-
tic analyzes of the strategy document. These are softmax
layers, where only one unit is active at any time. The
units of the second hidden layer ~f(s, a, d, yi, zi) are a set
of fixed real valued feature functions on s, a, d and the
active units yi and zi of ~y and ~z respectively.
fraction of the document provides guidance relevant
to the current state, while the remainder of the text
is likely to be irrelevant. Since this information is
not known a priori, we model the decision about a
sentence?s relevance to the current state as a hid-
den variable. Moreover, to fully utilize the infor-
mation presented in a sentence, the model identifies
the words that describe actions and those that de-
scribe state attributes, discriminating them from the
rest of the sentence. As with the relevance decision,
we model this labeling using hidden variables.
As shown in Figure 2, our model is a four layer
neural network. The input layer ~x represents the
current state s, candidate action a, and document
d. The second layer consists of two disjoint sets of
units ~y and ~z which encode the sentence-relevance
and predicate-labeling decisions respectively. Each
of these sets of units operates as a stochastic 1-of-n
softmax selection layer (Bridle, 1990) where only a
single unit is activated. The activation function for
units in this layer is the standard softmax function:
p(yi = 1|~x) = e
~ui?~x
/ ?
k
e~uk?~x,
where yi is the ith hidden unit of ~y, and ~ui is the
weight vector corresponding to yi. Given this acti-
271
vation function, the second layer effectively models
sentence relevance and predicate labeling decisions
via log-linear distributions, the details of which are
described below.
The third feature layer ~f of the neural network is
deterministically computed given the active units yi
and zj of the softmax layers, and the values of the
input layer. Each unit in this layer corresponds to
a fixed feature function fk(st, at, d, yi, zj) ? R. Fi-
nally the output layer encodes the action-value func-
tion Q(s, a, d), which now also depends on the doc-
ument d, as a weighted linear combination of the
units of the feature layer:
Q(st, at, d) = ~w ? ~f,
where ~w is the weight vector.
Modeling Sentence Relevance Given a strategy
document d, we wish to identify a sentence yi that
is most relevant to the current game state st and ac-
tion at. This relevance decision is modeled as a log-
linear distribution over sentences as follows:
p(yi|st, at, d) ? e
~u??(yi,st,at,d).
Here ?(yi, st, at, d) ? Rn is a feature function, and
~u are the parameters we need to estimate.
Modeling Predicate Structure Our goal here is
to label the words of a sentence as either action-
description, state-description or background. Since
these word label assignments are likely to be mu-
tually dependent, we model predicate labeling as a
sequence prediction task. These dependencies do
not necessarily follow the order of words in a sen-
tence, and are best expressed in terms of a syn-
tactic tree. For example, words corresponding to
state-description tend to be descendants of action-
description words. Therefore, we label words in de-
pendency order ? i.e., starting at the root of a given
dependency tree, and proceeding to the leaves. This
allows a word?s label decision to condition on the
label of the corresponding dependency tree parent.
Given sentence yi and its dependency parse qi, we
model the distribution over predicate labels ~ei as:
p(~ei |yi, qi) =
?
j
p(ej |j, ~e1:j?1, yi, qi),
p(ej |j, ~e1:j?1, yi, qi) ? e
~v??(ej ,j,~e1:j?1,yi,qi).
Here ej is the predicate label of the jth word being
labeled, and ~e1:j?1 is the partial predicate labeling
constructed so far for sentence yi.
In the second layer of the neural network, the
units ~z represent a predicate labeling ~ei of every sen-
tence yi ? d. However, our intention is to incorpo-
rate, into action-value function Q, information from
only the most relevant sentence. Thus, in practice,
we only perform a predicate labeling of the sentence
selected by the relevance component of the model.
Given the sentence selected as relevant and its
predicate labeling, the output layer of the network
can now explicitly learn the correlations between
textual information, and game states and actions ?
for example, between the word ?grassland? in Fig-
ure 1, and the action of building a city. This allows
our method to leverage the automatically extracted
textual information to improve game play.
4.2 Parameter Estimation
Learning in our method is performed in an online
fashion: at each game state st, the algorithm per-
forms a simulated game roll-out, observes the out-
come of the game, and updates the parameters ~u,
~v and ~w of the action-value function Q(st, at, d).
These three steps are repeated a fixed number of
times at each actual game state. The information
from these roll-outs is used to select the actual game
action. The algorithm re-learns Q(st, at, d) for ev-
ery new game state st. This specializes the action-
value function to the subgame starting from st.
Since our model is a non-linear approximation of
the underlying action-value function of the game,
we learn model parameters by applying non-linear
regression to the observed final utilities from the
simulated roll-outs. Specifically, we adjust the pa-
rameters by stochastic gradient descent, to mini-
mize the mean-squared error between the action-
value Q(s, a) and the final utility R(s? ) for each
observed game state s and action a. The resulting
update to model parameters ? is of the form:
?? = ?
?
2
?? [R(s? )?Q(s, a)]
2
= ? [R(s? )?Q(s, a)]??Q(s, a; ?),
where ? is a learning rate parameter.
This minimization is performed via standard error
backpropagation (Bryson and Ho, 1969; Rumelhart
272
et al, 1986), which results in the following online
updates for the output layer parameters ~w:
~w ? ~w + ?w [Q?R(s? )] ~f(s, a, d, yi, zj),
where ?w is the learning rate, and Q = Q(s, a, d).
The corresponding updates for the sentence rele-
vance and predicate labeling parameters ~u and ~v are:
~ui ? ~ui + ?u [Q?R(s? )] Q ~x [1? p(yi|?)],
~vi ? ~vi + ?v [Q?R(s? )] Q ~x [1? p(zi|?)].
5 Applying the Model
We apply our model to playing the turn-based strat-
egy game, Civilization II. We use the official man-
ual 6 of the game as the source of textual strategy
advice for the language aware algorithms.
Civilization II is a multi-player game set on a grid-
based map of the world. Each grid location repre-
sents a tile of either land or sea, and has various
resources and terrain attributes. For example, land
tiles can have hills with rivers running through them.
In addition to multiple cities, each player controls
various units ? e.g., settlers and explorers. Games
are won by gaining control of the entire world map.
In our experiments, we consider a two-player game
of Civilization II on a grid of 1000 squares, where
we play against the built-in AI player.
Game States and Actions We define the game state
of Civilization II to be the map of the world, the at-
tributes of each map tile, and the attributes of each
player?s cities and units. Some examples of the at-
tributes of states and actions are shown in Figure 3.
The space of possible actions for a given city or unit
is known given the current game state. The actions
of a player?s cities and units combine to form the ac-
tion space of that player. In our experiments, on av-
erage a player controls approximately 18 units, and
each unit can take one of 15 actions. This results in
a very large action space for the game ? i.e., 1021.
To effectively deal with this large action space, we
assume that given the state, the actions of a single
unit are independent of the actions of all other units
of the same player.
Utility Function The Monte-Carlo algorithm uses
the utility function to evaluate the outcomes of
6www.civfanatics.com/content/civ2/reference/Civ2manual.zip
Map tile attributes:
City attributes:
Unit attributes:
- Terrain type (e.g. grassland, mountain, etc)
- Tile resources (e.g. wheat, coal, wildlife, etc)
- City population
- Amount of food produced
- Unit type (e.g., worker, explorer, archer, etc)
- Is unit in a city ?
1 if action=build-city
   & tile-has-river=true
   & action-words={build,city}
   & state-words={river,hill}
0 otherwise
1 if action=build-city 
   & tile-has-river=true
   & words={build,city,river}
0 otherwise
1 if label=action 
   & word-type='build'
   & parent-label=action
0 otherwise
Figure 3: Example attributes of the game (box above),
and features computed using the game manual and these
attributes (box below).
simulated game roll-outs. In the typical application
of the algorithm, the final game outcome is used as
the utility function (Tesauro and Galperin, 1996).
Given the complexity of Civilization II, running sim-
ulation roll-outs until game completion is impracti-
cal. The game, however, provides each player with a
game score, which is a noisy indication of how well
they are currently playing. Since we are playing a
two-player game, we use the ratio of the game score
of the two players as our utility function.
Features The sentence relevance features ~? and the
action-value function features ~f consider the at-
tributes of the game state and action, and the words
of the sentence. Some of these features compute text
overlap between the words of the sentence, and text
labels present in the game. The feature function ~?
used for predicate labeling on the other hand oper-
ates only on a given sentence and its dependency
parse. It computes features which are the Carte-
sian product of the candidate predicate label with
word attributes such as type, part-of-speech tag, and
dependency parse information. Overall, ~f , ~? and
~? compute approximately 306,800, 158,500, and
7,900 features respectively. Figure 3 shows some
examples of these features.
273
6 Experimental Setup
Datasets We use the official game manual for Civi-
lization II as our strategy guide. This manual uses a
large vocabulary of 3638 words, and is composed of
2083 sentences, each on average 16.9 words long.
Experimental Framework To apply our method to
the Civilization II game, we use the game?s open
source implementation Freeciv.7 We instrument the
game to allow our method to programmatically mea-
sure the current state of the game and to execute
game actions. The Stanford parser (de Marneffe et
al., 2006) was used to generate the dependency parse
information for sentences in the game manual.
Across all experiments, we start the game at the
same initial state and run it for 100 steps. At each
step, we perform 500 Monte-Carlo roll-outs. Each
roll-out is run for 20 simulated game steps before
halting the simulation and evaluating the outcome.
For our method, and for each of the baselines, we
run 200 independent games in the above manner,
with evaluations averaged across the 200 runs. We
use the same experimental settings across all meth-
ods, and all model parameters are initialized to zero.
The test environment consisted of typical PCs
with single Intel Core i7 CPUs (4 hyper-threaded
cores each), with the algorithms executing 8 simula-
tion roll-outs in parallel. In this setup, a single game
of 100 steps runs in approximately 1.5 hours.
Evaluation Metrics We wish to evaluate two as-
pects of our method: how well it leverages tex-
tual information to improve game play, and the ac-
curacy of the linguistic analysis it produces. We
evaluate the first aspect by comparing our method
against various baselines in terms of the percent-
age of games won against the built-in AI of Freeciv.
This AI is a fixed algorithm designed using exten-
sive knowledge of the game, with the intention of
challenging human players. As such, it provides a
good open-reference baseline. Since full games can
last for multiple days, we compute the percentage of
games won within the first 100 game steps as our pri-
mary evaluation. To confirm that performance under
this evaluation is meaningful, we also compute the
percentage of full games won over 50 independent
runs, where each game is run to completion.
7http://freeciv.wikia.com. Game version 2.2
Method % Win % Loss Std. Err.
Random 0 100 ?
Built-in AI 0 0 ?
Game only 17.3 5.3 ? 2.7
Sentence relevance 46.7 2.8 ? 3.5
Full model 53.7 5.9 ? 3.5
Random text 40.3 4.3 ? 3.4
Latent variable 26.1 3.7 ? 3.1
Table 1: Win rate of our method and several baselines
within the first 100 game steps, while playing against the
built-in game AI. Games that are neither won nor lost are
still ongoing. Our model?s win rate is statistically signif-
icant against all baselines except sentence relevance. All
results are averaged across 200 independent game runs.
The standard errors shown are for percentage wins.
Method % Wins Standard Error
Game only 45.7 ? 7.0
Latent variable 62.2 ? 6.9
Full model 78.8 ? 5.8
Table 2: Win rate of our method and two baselines on 50
full length games played against the built-in AI.
7 Results
Game performance As shown in Table 1, our lan-
guage aware Monte-Carlo algorithm substantially
outperforms several baselines ? on average winning
53.7% of all games within the first 100 steps. The
dismal performance, on the other hand, of both the
random baseline and the game?s own built-in AI
(playing against itself) is an indicator of the diffi-
culty of the task. This evaluation is an underesti-
mate since it assumes that any game not won within
the first 100 steps is a loss. As shown in Table 2, our
method wins over 78% of full length games.
To characterize the contribution of the language
components to our model?s performance, we com-
pare our method against two ablative baselines. The
first of these, game-only, does not take advantage
of any textual information. It attempts to model the
action value function Q(s, a) only in terms of the
attributes of the game state and action. The per-
formance of this baseline ? a win rate of 17.3% ?
effectively confirms the benefit of automatically ex-
tracted textual information in the context of our task.
The second ablative baseline, sentence-relevance, is
274
After the road is built, use the settlers to start improving the terrain.
S S AA AA AS
When the settlers becomes active, chose build road.
A AS SS A
Use settlers or engineers to improve a terrain square within the city radius
A A A SA SSSSS ??
Phalanxes are twice as effective at defending cities as warriors.
You can rename the city if you like, but we'll refer to it as washington.
Build the city on plains or grassland with a river running through it.
There are many different strategies dictating the order in which advances are researched 
Figure 4: Examples of our method?s sentence relevance
and predicate labeling decisions. The box above shows
two sentences (identified by check marks) which were
predicted as relevant, and two which were not. The box
below shows the predicted predicate structure of three
sentences, with ?S? indicating state description,?A? ac-
tion description and background words unmarked. Mis-
takes are identified with crosses.
identical to our model, but lacks the predicate label-
ing component. This method wins 46.7% of games,
showing that while identifying the text relevant to
the current game state is essential, a deeper struc-
tural analysis of the extracted text provides substan-
tial benefits.
One possible explanation for the improved perfor-
mance of our method is that the non-linear approx-
imation simply models game characteristics better,
rather than modeling textual information. We di-
rectly test this possibility with two additional base-
lines. The first, random-text, is identical to our full
model, but is given a document containing random
text. We generate this text by randomly permut-
ing the word locations of the actual game manual,
thereby maintaining the document?s overall statisti-
cal properties. The second baseline, latent variable,
extends the linear action-value function Q(s, a) of
the game only baseline with a set of latent variables
? i.e., it is a four layer neural network, where the sec-
ond layer?s units are activated only based on game
information. As shown in Table 1 both of these base-
lines significantly underperform with respect to our
model, confirming the benefit of automatically ex-
tracted textual information in the context of this task.
Sentence Relevance Figure 4 shows examples of
the sentence relevance decisions produced by our
method. To evaluate the accuracy of these decisions,
we ideally require a ground-truth relevance annota-
tion of the game?s user manual. This however, is
20 40 60 80 100Game step0
0.2
0.4
0.6
0.8
1
Sente
ncer
eleva
ncea
ccura
cy
Sentence relevanceMoving average
Figure 5: Accuracy of our method?s sentence relevance
predictions, averaged over 100 independent runs.
impractical since the relevance decision is depen-
dent on the game context, and is hence specific to
each time step of each game instance. Therefore, for
the purposes of this evaluation, we modify the game
manual by adding to it sentences randomly selected
from the Wall Street Journal corpus (Marcus et al,
1993) ? sentences that are highly unlikely to be rel-
evant to game play. We then evaluate the accuracy
with which sentences from the original manual are
picked as relevant.
In this evaluation, our method achieves an average
accuracy of 71.8%. Given that our model only has to
differentiate between the game manual text and the
Wall Street Journal, this number may seem disap-
pointing. Furthermore, as can be seen from Figure 5,
the sentence relevance accuracy varies widely as the
game progresses, with a high average of 94.2% dur-
ing the initial 25 game steps.
In reality, this pattern of high initial accuracy fol-
lowed by a lower average is not entirely surprising:
the official game manual for Civilization II is writ-
ten for first time players. As such, it focuses on the
initial portion of the game, providing little strategy
advice relevant to subsequence game play.8 If this is
the reason for the observed sentence relevance trend,
we would also expect the final layer of the neural
network to emphasize game features over text fea-
tures after the first 25 steps of the game. This is
indeed the case, as can be seen from Figure 6.
To further test this hypothesis, we perform an ex-
periment where the first 50 steps of the game are
played using our full model, and the subsequent 50
steps are played without using any textual informa-
8This is reminiscent of opening books for games like Chess
or Go, which aim to guide the player to a playable middle game.
275
20 40 60 80Game step0
0.5
1
Text
featu
reim
porta
nce
Text
featu
res
domi
nate
Gam
efea
tures
domi
nate
1.5
Figure 6: Difference between the norms of the text fea-
tures and game features of the output layer of the neural
network. Beyond the initial 25 steps of the game, our
method relies increasingly on game features.
tion. This hybrid method performs as well as our
full model, achieving a 53.3% win rate, confirm-
ing that textual information is most useful during
the initial phase of the game. This shows that our
method is able to accurately identify relevant sen-
tences when the information they contain is most
pertinent to game play.
Predicate Labeling Figure 4 shows examples of the
predicate structure output of our model. We eval-
uate the accuracy of this labeling by comparing it
against a gold-standard annotation of the game man-
ual. Table 3 shows the performance of our method
in terms of how accurately it labels words as state,
action or background, and also how accurately it dif-
ferentiates between state and action words. In ad-
dition to showing a performance improvement over
the random baseline, these results display two clear
trends: first, under both evaluations, labeling accu-
racy is higher during the initial stages of the game.
This is to be expected since the model relies heav-
ily on textual features only during the beginning of
the game (see Figure 6). Second, the model clearly
performs better in differentiating between state and
action words, rather than in the three-way labeling.
To verify the usefulness of our method?s predi-
cate labeling, we perform a final set of experiments
where predicate labels are selected uniformly at ran-
dom within our full model. This random labeling
results in a win rate of 44% ? a performance similar
to the sentence relevance model which uses no pred-
icate information. This confirms that our method
is able identify a predicate structure which, while
noisy, provides information relevant to game play.
Method S/A/B S/A
Random labeling 33.3% 50.0%
Model, first 100 steps 45.1% 78.9%
Model, first 25 steps 48.0% 92.7%
Table 3: Predicate labeling accuracy of our method and a
random baseline. Column ?S/A/B? shows performance
on the three-way labeling of words as state, action or
background, while column ?S/A? shows accuracy on the
task of differentiating between state and action words.
state: grassland "city"
state: grassland "build"
action: settlers_build_city "city"
action: set_research "discovery"
game attribute word
Figure 7: Examples of word to game attribute associa-
tions that are learned via the feature weights of our model.
Figure 7 shows examples of how this textual infor-
mation is grounded in the game, by way of the asso-
ciations learned between words and game attributes
in the final layer of the full model.
8 Conclusions
In this paper we presented a novel approach for
improving the performance of control applications
by automatically leveraging high-level guidance ex-
pressed in text documents. Our model, which op-
erates in the Monte-Carlo framework, jointly learns
to identify text relevant to a given game state in ad-
dition to learning game strategies guided by the se-
lected text. We show that this approach substantially
outperforms language-unaware alternatives while
learning only from environment feedback.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0835652),
DARPA Machine Reading Program (FA8750-09-
C-0172) and the Microsoft Research New Faculty
Fellowship. Thanks to Michael Collins, Tommi
Jaakkola, Leslie Kaelbling, Nate Kushman, Sasha
Rush, Luke Zettlemoyer, the MIT NLP group, and
the ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or rec-
ommendations expressed in this paper are those of
the authors, and do not necessarily reflect the views
of the funding organizations.
276
References
R. Balla and A. Fern. 2009. UCT for tactical assault
planning in real-time strategy games. In 21st Interna-
tional Joint Conference on Artificial Intelligence.
Darse Billings, Lourdes Pen?a Castillo, Jonathan Scha-
effer, and Duane Szafron. 1999. Using probabilis-
tic knowledge and simulation to play poker. In 16th
National Conference on Artificial Intelligence, pages
697?703.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
ACL, pages 82?90.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay.
2010. Reading between the lines: Learning to map
high-level instructions to commands. In Proceedings
of ACL, pages 1268?1277.
S.R.K. Branavan, David Silver, and Regina Barzilay.
2011. Non-linear monte-carlo search in civilization ii.
In Proceedings of IJCAI.
John S. Bridle. 1990. Training stochastic model recog-
nition algorithms as networks can lead to maximum
mutual information estimation of parameters. In Ad-
vances in NIPS, pages 211?217.
Arthur E. Bryson and Yu-Chi Ho. 1969. Applied optimal
control: optimization, estimation, and control. Blais-
dell Publishing Company.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings of
EMNLP, pages 958?967.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated natural language learning. In Pro-
ceedings of CoNLL, pages 104?111.
S. Gelly, Y. Wang, R. Munos, and O. Teytaud. 2006.
Modification of UCT with patterns in Monte-Carlo
Go. Technical Report 6062, INRIA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Raymond J. Mooney. 2008a. Learning language from its
perceptual context. In Proceedings of ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings of AAAI, pages
1598?1601.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323:533?536.
J. Scha?fer. 2008. The UCT algorithm applied to games
with imperfect information. Diploma Thesis. Otto-
von-Guericke-Universita?t Magdeburg.
B. Sheppard. 2002. World-championship-caliber Scrab-
ble. Artificial Intelligence, 134(1-2):241?275.
D. Silver, R. Sutton, and M. Mu?ller. 2008. Sample-
based learning and search with permanent and tran-
sient memories. In 25th International Conference on
Machine Learning, pages 968?975.
D. Silver. 2009. Reinforcement Learning and
Simulation-Based Search in the Game of Go. Ph.D.
thesis, University of Alberta.
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. Journal of Artificial Intelli-
gence Research, 15:31?90.
N. Sturtevant. 2008. An analysis of UCT in multi-player
games. In 6th International Conference on Computers
and Games, pages 37?49.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. The MIT Press.
G. Tesauro and G. Galperin. 1996. On-line policy im-
provement using Monte-Carlo search. In Advances in
Neural Information Processing 9, pages 1068?1074.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of the
ACL, pages 806?814.
Chen Yu and Dana H. Ballard. 2004. On the integration
of grounding language and learning objects. In Pro-
ceedings of AAAI, pages 488?493.
277
