Proceedings of the ACL 2007 Student Research Workshop, pages 79?84,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
 
Semantic Classification of Noun Phrases Using                                        
Web Counts and Learning Algorithms 
 
 
 Paul Nulty 
School of Computer Science and Informatics 
University College Dublin 
Belfield, Dublin 4, Ireland 
paul.nulty@ucd.ie 
 
 
 
Abstract 
This paper investigates the use of machine 
learning algorithms to label modifier-noun 
compounds with a semantic relation. The 
attributes used as input to the learning algo-
rithms are the web frequencies for phrases 
containing the modifier, noun, and a prepo-
sitional joining term. We compare and 
evaluate different algorithms and different 
joining phrases on Nastase and Szpako-
wicz?s (2003) dataset of 600 modifier-noun 
compounds. We find that by using a Sup-
port Vector Machine classifier we can ob-
tain better performance on this dataset than 
a current state-of-the-art system; even with 
a relatively small set of prepositional join-
ing terms. 
1 Introduction 
Noun-modifier word pairs occur frequently in 
many languages, and the problem of semantic dis-
ambiguation of these phrases has many potential 
applications in areas such as question-answering 
and machine translation. One very common ap-
proach to this problem is to define a set of seman-
tic relations which capture the interaction between 
the modifier and the head noun, and then attempt 
to assign one of these semantic relations to each 
noun-modifier pair. For example, the phrase ?flu 
virus? could be assigned the semantic relation 
?causal? (the virus causes the flu); the relation for 
?desert storm? could be ?location? (the storm is 
located in the desert). 
There is no consensus as to which set of seman-
tic relations best captures the differences in mean-
ing of various noun phrases. Work in theoretical 
linguistics has suggested that noun-noun com-
pounds may be formed by the deletion of a predi-
cate verb or preposition (Levi 1978). However, 
whether the set of possible predicates numbers 5 or 
50, there are likely to be some examples of noun 
phrases that fit into none of the categories and 
some that fit in multiple categories. 
Modifier-noun phrases are often used inter-
changeably with paraphrases which contain the 
modifier and the noun joined by a preposition or 
simple verb. For example, the query ?morning ex-
ercise? returns 133,000 results from the Yahoo 
search engine, and a query for the phrase ?exercise 
in the morning? returns 47,500 results. Sometimes 
people choose to use a modifier-noun compound 
phrase to describe a concept, and sometimes they 
choose to use a paraphrase which includes a prepo-
sition or simple verb joining head noun and the 
modifier. One method for deducing semantic rela-
tions between words in compounds involves gath-
ering n-gram frequencies of these paraphrases, 
containing a noun, a modifier and a ?joining term? 
that links them. Some algorithm can then be used 
to map from joining term frequencies to semantic 
relations and so find the correct relation for the 
compound in question. This is the approach we use 
in our experiments. We choose two sets of joining 
terms, based on the frequency with which they oc-
cur in between nouns in the British National Cor-
79
  
pus (BNC). We experiment with three different 
learning algorithms; Nearest Neighbor, Multi-
Layer Perceptron and Support Vector Machines 
(SVM). 
2 Motivation 
The motivation for this paper is to discover which 
joining terms are good predictors of a semantic 
relation, and which learning algorithms perform 
best at the task of mapping from joining terms to 
semantic relations for modifier-noun compounds. 
2.1 Joining Terms 
Choosing a set of joining terms in a principled 
manner in the hope of capturing the semantic rela-
tion between constituents in the noun phrase is dif-
ficult, but there is certainly some correlation be-
tween a prepositional term or short linking verb 
and a semantic relation. For example, the preposi-
tion ?during? indicates a temporal relation, while 
the preposition ?in? indicates a locative relation, 
either temporal or spatial. 
   In this paper, we are interested in whether the 
frequency with which a joining term occurs be-
tween two nouns is related to how it indicates a 
semantic interaction. This is in part motivated by 
Zipf?s theory which states that the more frequently 
a word occurs in a corpus the more meanings or 
senses it is likely to have (Zipf 1929). If this is 
true, we would expect that very frequent preposi-
tions, such as ?of?, would have many possible 
meanings and therefore not reliably predict a se-
mantic relation. However, less frequent preposi-
tions, such as ?while? would have a more limited 
set of senses and therefore accurately predict a se-
mantic relation. 
2.2 Machine Learning Algorithms 
We are also interested in comparing the perform-
ance of machine learning algorithms on the task of 
mapping from n-gram frequencies of joining terms 
to semantic relations. For the experiments we use 
Weka, (Witten and Frank, 1999) a machine learn-
ing toolkit which allows for fast experimentation 
with many standard learning algorithms. In Section 
5 we present the results obtained using the nearest-
neighbor, neural network (i.e. multi-layer percep-
tron) and SVM. The mechanisms of these different 
learning approaches will be discussed briefly in 
Section 4. 
3 Related Work 
3.1   Web Mining 
Much of the recent work conducted on the problem 
of assigning semantic relations to noun phrases has 
used the web as a corpus. The use of hit counts 
from web search engines to obtain lexical 
information was introduced by Turney (2001). The 
idea of searching a large corpus for specific lexico-
syntactic phrases to indicate a semantic relation of 
interest was first described by Hearst (1992). 
 A lexical pattern specific enough to indicate a 
particular semantic relation is usually not very 
frequent, and using the web as a corpus alleviates 
the data sparseness problem. However, it also 
introduces some problems. 
? The query language permitted by the large 
search engines is somewhat limited.  
? Two of the major search engines (Google and 
Yahoo) do not provide exact frequencies, but 
give rounded estimates instead. 
? The number of results returned is unstable as 
new pages are created and deleted all the time. 
  Nakov and Hearst (2005) examined the use of 
web-based n-gram frequencies for an NLP task and 
concluded that these issues do not greatly impact 
the interpretation of the results. Keller and Lapata 
(2003) showed that web frequencies correlate 
reliably with standard corpus frequencies. 
  Lauer (1995) tackles the problem of semantically 
disambiguating noun phrases by trying to find the 
preposition which best describes the relation 
between the modifier and head noun. His method 
involves searching a corpus for occurrences 
paraphrases of the form ?noun preposition 
modifier?. Whichever preposition is most frequent 
in this context is chosen. Lapata and Keller (2005) 
improved on Lauer's results at the same task by 
using the web as a corpus. Nakov and Hearst 
(2006) use queries of the form ?noun that * 
modifier? where '*' is a wildcard operator. By 
retrieving the words that most commonly occurred 
in the place of the wildcard they were able to 
identify very specific predicates that are likely to 
represent the relation between noun and modifier. 
80
  
3.2 Machine Learning Approaches 
There have been two main approaches used when 
applying machine learning algorithms to the se-
mantic disambiguation of modifier-noun phrases. 
  The first approach is to use semantic properties of 
the noun and modifier words as attributes, using a 
lexical hierarchy to extract these properties. This 
approach was used by Rosario and Hearst (2001) 
within a specific domain ? medical texts. Using an 
ontology of medical terms they train a neural net-
work to semantically classify nominal phrases, 
achieving 60% accuracy over 16 classes. 
   Nastase and Szpakowicz (2003) use the position 
of the noun and modifier words within general se-
mantic hierarchies (Roget's Thesaurus and Word-
Net) as attributes for their learning algorithms. 
They experiment with various algorithms and con-
clude that a rule induction system is capable of 
generalizing to characterize the noun phrases. 
Moldovan et al(2004) also use WordNet. They 
experiment with a Bayesian algorithm, decision 
trees, and their own algorithm; semantic scattering.  
There are some drawbacks to the technique of us-
ing semantic properties extracted from a lexical 
hierarchy. Firstly, it has been noted that the distinc-
tions between word senses in WordNet are very 
fine-grained, making the task of word-sense dis-
ambiguation tricky. Secondly, it is usual to use a 
rule-based learning algorithm when the attributes 
are properties of the words rather than n-gram fre-
quency counts. As Nastase and Szpakowicz (2003) 
point out, a large amount of labeled data is re-
quired to allow these rule-based learners to effec-
tively generalize, and manually labeling thousands 
of modifier-noun compounds would be a time-
consuming task. 
Table 1: Examples for each of the five relations 
The second approach is to use statistical informa-
tion about the occurrence of the noun and modifier 
in a corpus to generate attributes for a machine 
learning algorithm. This is the method we will de-
scribe in this paper. Turney and Littman (2005)  
use a set of 64 short prepositional and conjunctive 
phrases they call ?joining terms? to generate exact 
queries for AltaVista of the form ?noun joining 
term modifier?, and ?modifier joining term noun?.  
  These hit counts were used with a nearest 
neighbor algorithm to assign the noun phrases se-
mantic relations. Over the set of 5 semantic rela-
tions defined by Nastase and Szpakowicz (2003), 
they achieve an accuracy of 45.7% for the task of 
assigning one of 5 semantic relations to each of the 
600 modifier-noun phrases. 
4   Method 
  The method described in this paper is similar to 
the work presented in Turney and Littman (2005). 
We collect web frequencies for queries of the form 
?head joining term modifier?. We did not collect 
queries of the form ?modifier joining term head?; 
in the majority of paraphrases of noun phrases the 
head noun occurs before the modifying word. As 
well as trying to achieve reasonable accuracy, we 
were interested in discovering what kinds of join-
ing phrases are most useful when trying to predict 
the semantic relation, and which machine learning 
algorithms perform best at the task of using vectors 
of web-based n-gram frequencies to predict the 
semantic relation. 
  For our experiments we used the set of 600 la-
beled noun-modifier pairs of Nastase and Szpako-
wicz (2003). This data was also used by Turney 
and Littman (2005). Of the 600 modifier-noun 
phrases, three contained hyphenated or two-word 
modifier terms, for example ?test-tube baby?. We 
omitted these three examples from our experi-
ments, leaving a dataset of 597 examples. 
  The data is labeled with two different sets of 
semantic relations: one set of 30 relations with 
fairly specific meanings, and another set of 5 rela-
tions with more abstract meanings. For our ex-
periments we focused on the set of 5 relations. One 
reason for this is that dividing a set of 600 in-
stances into 30 classes results in a fairly sparse and 
uneven dataset. Table 1 is a list of the relations 
used and examples of compounds that are labeled 
with each relation. 
4.1 Collecting Web Frequencies 
In order to collect the n-gram frequencies, we used 
the Yahoo Search API. Collecting frequencies for 
causal flu virus, onion tear 
temporal summer travel, morning class 
spatial west coast, home remedy 
participant mail sorter, blood donor 
quality rice paper, picture book 
81
  
600 noun-modifier pairs, using 28 different joining 
terms required 16,800 calls to the search engine. 
We will discuss our choice of the joining terms in 
the next section.  
  When collecting web frequencies we took advan-
tage of the OR operator provided by the search 
engine. For each joining term, we wanted to sum 
the number of hits for the term on its own, the term 
followed by 'a' and the term followed by 'the'. In-
stead of conducting separate queries for each of 
these forms, we were able to sum the results with 
just one search. For example, if the noun phrase 
was ?student invention? and the joining phrase was 
?by?; one of the queries would be:  
?invention by student? OR ?invention by a student? OR 
?invention by the student? 
This returns the sum of the number of pages 
matched by each of these three exact queries. The 
idea is that these sensible paraphrases will return 
more hits than nonsense ones, such as: 
 ?invention has student? OR ?invention has a student? 
OR ?invention has the student? 
It would be possible to construct a set of hand-
coded rules to map from joining terms to semantic 
relations; for example ?during? maps to temporal, 
?by? maps to causal and so on. However, we hope 
that the classifiers will be able to identify combina-
tions of prepositions that indicate a relation. 
4.2 Choosing a Set of Joining Terms 
Possibly the most difficult problem with this 
method is deciding on a set of joining terms which 
is likely to provide enough information about the 
noun-modifier pairs to allow a learning algorithm 
to predict the semantic relation. Turney and Litt-
man (2005) use a large and varied set of joining 
terms. They include the most common preposi-
tions, conjunctions and simple verbs like ?has?, 
?goes? and ?is?. Also, they include the wildcard 
operator '*' in many of their queries; for example 
?not?, ?* not? and ?but not? are all separate que-
ries. In addition, they include prepositions both 
with and without the definite article as separate 
queries, for example ?for? and ?for the?. 
  The joining terms used for the experiments in this 
paper were chosen by examining which phrases 
most commonly occurred between two nouns in 
the BNC. We counted the frequencies with which 
phrases occurred between two nouns and chose the 
28 most frequent of these phrases as our joining 
terms. We excluded conjunctions and determiners 
from the list of the most frequent joining terms. 
We excluded conjunctions on the basis that in most 
contexts a conjunction merely links the two nouns 
together for syntactic purposes; there is no real 
sense in which one of the nouns modifies another 
semantically in this context. We excluded deter-
miners on the basis that the presence of a deter-
miner does not affect the semantic properties of the 
interaction between the head and modifier. 
4.3 Learning Algorithms 
  There were three conditions experimented with 
using three different algorithms. For the first con-
dition, the attributes used by the learning algo-
rithms consisted of vectors of web hits obtained 
using the 14 most frequent joining terms found in  
the BNC. The next condition used a vector of web 
hits obtained using the joining terms that occurred  
Table 2: Joining terms ordered by the frequency  
with which they occurred between two nouns in 
the BNC. 
from position 14 to 28 in the list of the most fre-
quent terms found in the BNC. The third condition 
used all 28 joining terms. The joining terms are 
listed in Table 2.  We used the log of the web 
counts returned, as recommended in previous work 
(Keller and Lapata, 2003). 
  The first learning algorithm we experimented 
with was the nearest neighbor algorithm ?IB1?, as 
1-14 15-28 
of 
in 
to 
for 
on 
with 
at 
is 
from 
as 
by 
between 
about 
has 
against 
within 
during 
through 
over 
towards 
without 
across 
because 
behind 
after 
before 
while 
under 
82
  
implemented in Weka. This algorithm considers 
the vector of n-gram frequencies as a multi-
dimensional space, and chooses the label of the 
nearest example in this space as the label for each 
new example. Testing for this algorithm was done 
using leave-one-out cross validation.  
   The next learning algorithm we used was the 
multi-layer perceptron, or neural network. The 
network was trained using the backpropagation of 
error technique implemented in Weka. For the first 
two sets of data we used a network with 14 input 
nodes, one hidden layer with 28 nodes, and 5 out-
put nodes. For the final condition, which uses the 
frequencies for all 28 joining terms, we used 28 
input nodes, one hidden layer with 56 nodes, and 
again 5 outputs, one for each class. We used 20-
fold cross validation with this algorithm. 
   The final algorithm we tested was an SVM 
trained with the Sequential Minimal Optimization 
method provided by Weka. A support vector ma-
chine is a method for creating a classification func-
tion which works by trying to find a hypersurface 
in the space of possible inputs that splits the posi-
tive examples from the negative examples for each 
class. For this test we again used 20-fold cross 
validation. 
5. Results  
The accuracy of the algorithms on each of the con-
ditions is illustrated below in Table 3. Since the 
largest class in the dataset accounts for 43% of the 
examples, the baseline accuracy for the task 
(guessing ?participant? all the time) is 43%. 
    The condition containing the counts for the less 
frequent joining terms performed slightly better 
than that containing the more frequent ones, but 
the best accuracy resulted from using all 28 fre-
quencies. The Multi-Layer Perceptron performed 
better than the nearest neighbor algorithm on all 
three conditions. There was almost no difference in 
accuracy between the first two conditions, and 
again using all of the joining terms produced the 
best results. 
  The SVM algorithm produced the best accuracy 
of all, achieving 50.1% accuracy using the com-
bined set of joining terms. The less frequent join-
ing terms achieve slightly better accuracy using the 
Nearest Neighbor and SVM algorithms, and very 
slightly worse accuracy using the neural network. 
Using all of the joining terms resulted in a signifi-
cant improvement in accuracy for all algorithms. 
The SVM consistently outperformed the baseline; 
neither of the other algorithms did so. 
6. Discussion and Future Work 
Our motivation in this paper was twofold. Firstly, 
we wanted to compare the performance of different 
machine learning algorithms on the task of map-
ping from a vector of web frequencies of para-
phrases containing joining terms to semantic rela-
tions. Secondly, we wanted to discover whether the 
frequency of joining terms was related to their ef-
fectiveness at predicting a semantic relation. 
6.1 Learning Algorithms 
The results suggest that the nearest neighbor ap-
proach is not the most effective algorithm for the 
classification task. Turney and Littman (2005) 
achieve an accuracy of 45.7%, where we achieve a 
maximum accuracy of 38.1% on this dataset using 
a nearest neighbor algorithm. However, their tech-
nique uses the cosine of the angle between the vec-
tors of web counts as the similarity metric, while 
the nearest neighbor implementation in Weka uses 
the Euclidean distance.  
Also, they use 64 joining terms and gather 
counts for both the forms ?noun joining term modi-
fier? and ?modifier joining term noun? (128 fre-
quencies in total); while we use only the former 
construction with 28 joining terms. By using the 
SVM classifier, we were able to achieve a higher 
accuracy than Turney and Littman (50.1% versus 
45.7%) with significantly fewer joining terms (28 
versus 128).  However, one issue with the SVM is 
Table 3:  Accuracy for each algorithm using each set of joining terms on the Nastase and Szpako-
wicz test set of modifier-noun compounds. 
 Joining Terms 1-14 Joining terms 15-28 All 28 Joining terms 
Nearest Neighbor 32.6 34.7 38.1 
Multi Layer Perceptron 37.6 37.4 42.2 
Support Vector Machine 44.2 45.9 50.1 
83
  
that it never predicted the class ?causal? for any of 
the examples. The largest class in our dataset is 
?participant?, which is the label for 43% of the 
examples; the smallest is ?temporal?, which labels 
9% of the examples. ?Causal? labels 14% of the 
data. It is difficult to explain why the algorithm 
fails to account for the ?causal? class; a useful task 
for future work would be to conduct a similar ex-
periment with a more balanced dataset. 
6.2 Joining Terms 
The difference in accuracy achieved by the two 
sets of joining terms is quite small, although for 
two of the algorithms the less frequent terms did 
achieve slightly better results. The difficulty is that 
the task of deducing a semantic relation from a 
paraphrase such as ?storm in the desert? requires 
many different types of information. It requires 
knowledge about the preposition ?in?; i.e. that it 
indicates a location. It requires knowledge about 
the noun ?desert?, i.e. that it is a location in space 
rather than time, and it requires the knowledge that 
a ?storm? may refer both to an event in time and an 
entity in space. It may be that a combination of 
semantic information from an ontology and statis-
tical information about paraphrases could be used 
together to achieve better performance on this task. 
  Another interesting avenue for future work in 
this area is investigation into exactly how ?joining 
terms? relate to semantic relations. Given Zipf's 
observation that high frequency words are more 
ambiguous than low frequency words, it is possible 
that there is a relationship between the frequency 
of the preposition in a paraphrase such as ?storm 
in the desert? and the ease of understanding that 
phrase. For example, the preposition 'of' is very 
frequent and could be interpreted in many ways. 
Therefore, the ?of? may be used in phrases where 
the semantic relation can be easily deduced from 
the nominals in the phrase alone. Less common 
(and therefore more informative) prepositions such 
as ?after? or ?because? may be used more often in 
phrases where the nominals alone do not contain 
enough information to deduce the relation, or the 
relation intended is not the most obvious one given 
the two nouns. 
 
 
References 
Marti A. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. COLING 92: (2) pp. 
539-545, Nantes, France, 
Frank Keller and Mirella Lapata. 2003. Using the Web 
to Obtain Frequencies for Unseen Bigrams. Compu-
tational Linguistics, 29: pp 459-484. 
Mirella Lapata and Frank Keller. 2005. Web-Based 
Models for Natural Language Processing. ACM 
Transactions on Speech and Language Processing 
2:1, pp 1-31. 
Mark Lauer. 1995. Designing Statistical Language 
Learners: Experiments on Noun Compounds. PhD 
thesis, Macquarie University, NSW 2109, Australia. 
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals, Academic Press, New York, NY. 
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT/NAACL Workshop on Computational 
Lexical Semantics. pp 60-67  Boston , MA. 
Preslav Nakov and Marti Hearst. 2006. Using Verbs to 
Characterize Noun-Noun Relations. In Proceedings 
of AIMSA 2006,  pp 233-244, Varne, Bulgaria. 
Preslav Nakov and Marti Hearst. 2005. Using the Web 
as an Implicit Training Set: Application to Structural 
Ambiguity Resolution. In Proceedings of 
HLT/EMNLP'05. pp 835-842, Vancouver, Canada. 
 Vivi Nastase and Stan Szpakowicz. 2003. Exploring 
Noun-Modifier Semantic Relations. In Fifth Interna-
tional Workshop on Computational Semantics, pp 
285-301. Tillburg, Netherlands. 
Barbara Rosario and Marti A. Hearst. 2001. Classifying 
the semantic relations in noun compounds via a do-
main-specific lexical hierarchy. In Proceedings of 
EMNLP 2001, pp 82-90, Pittsburgh, PA, USA. 
Peter D. Turney. 2001. Mining the web for synonyms: 
PM-IR vs LSA on TOEFL. Proceedings of 
ECML'01. pp 491-502. Freiburg, Germany. 
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations. 
Machine Learning, 60(1?3):251?278. 
Ian H. Witten and Eibe Frank. 1999. Data Mining: 
Practical Machine Learning Tools and Techniques 
with Java Implementations. Morgan Kaufmann. 
George K. Zipf. 1932. Selected Studies of the Principle 
of Relative Frequency in Language. Cambridge, MA. 
84
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 374?377,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
 
UCD-PN: Classification of Semantic Relations Between Nominals      
using WordNet and Web Counts 
 Paul Nulty 
School of Computer Science and Informatics 
University College Dublin 
Dublin, Ireland 
paul.nulty@ucd.ie 
 
 
 
Abstract 
For our system we use the SMO implemen-
tation of a support vector machine provided 
with the WEKA machine learning toolkit. 
As with all machine learning approaches, 
the most important step is to choose a set of 
features which reliably help to predict the 
label of the example. We used 76 features 
drawn from two very different knowledge 
sources. The first 48 features are boolean 
values indicating whether or not each of the 
nominals in the sentence are linked to cer-
tain other words in the WordNet hypernym 
and meronym networks. The remaining 28 
features are web frequency counts for the 
two nominals joined by certain common 
prepositions and verbs. Our system per-
formed well on all but two of the relations; 
theme-tool and origin entity. 
1 Introduction and Related Work 
This paper describes a system for participating 
in SemEval 2007 task 4; ?Classification of Seman-
tic Relations Between Nominals?. This SemEval 
task required systems to establish whether or not a 
particular semantic relation held between two 
nominals in a sentence. There were 7 semantic re-
lations, with approximately 70 positive and 70 
negative example sentences for each relation. 
There were approximately 70 examples in the test 
sets for each relation.  
This task is similar to the problem of determin-
ing what semantic relation holds between the con-
stituents of a noun-noun compound. Work in this 
area has used both statistical information about the 
frequencies of lexical patterns and hand-built 
knowledge databases such as WordNet and the-
saura. In our system we combine these two knowl-
edge sources and build a set of features to use as 
input to a Support Vector Machine learning algo-
rithm.  
  The use of hit counts from web search engines 
to obtain lexical information was introduced by 
Turney (2001). The idea of searching a large cor-
pus for specific lexico-syntactic phrases to indicate 
a semantic relation of interest was first described 
by Hearst (1992). A lexical pattern specific enough 
to indicate a particular semantic relation is usually 
not very frequent, and using the web as a corpus 
alleviates the data sparseness problem. However, it 
also introduces some problems. The number of 
results returned is unstable as pages are created and 
deleted all the time, and the major search engines 
return only rounded frequency estimates and do 
not allow a very sophisticated query interface. Na-
kov and Hearst (2005) examined the use of web-
based n-gram frequencies for an NLP task and 
concluded that these issues do not greatly impact 
the interpretation of the results. 
  Turney and Littman (2005) use web queries to 
the AltaVista search engine as the basis for their 
system to assign semantic relations to modifier-
noun phrases. They use a set of 64 short preposi-
tional and conjunctive phrases (joining terms) to 
generate exact queries of the form ?noun joining 
term modifier?, and ?modifier joining term noun?. 
Using 64 joining terms and trying the noun and 
modifier in either order resulted in a vector of 128 
374
  
hit counts for each noun-modifier pair. These hit 
counts were used with a supervised (nearest 
neighbor) algorithm to label the modifier-noun 
phrases.  
  Nakov and Hearst (2006) use queries of the form 
?noun that * modifier? where '*' is a wildcard 
operator. By retrieving the words that most 
commonly occurred in the place of the wildcard 
they were able to identify very specific predicates 
that are likely to represent the relation between 
noun and modifier. 
  There have also been several approaches which 
used hand built knowledge sources.  Rosario and 
Hearst (2001) used MeSH, a lexical hierarchy of 
medical terms. They use this hierarchy to assign 
semantic properties to head and modifier words in 
the medical domain. They use a neural network 
trained on these attributes to assign the noun 
phrases a semantic relation. 
   Nastase and Szpakowicz (2003) use the position 
of the noun and modifier words within general se-
mantic hierarchies (Roget's Thesaurus and Word-
Net) as attributes for their learning algorithms. 
They experiment with decision trees, a rule induc-
tion system, a relational learner and memory based 
learning. They conclude that the rule induction 
system is capable of generalizing to characterize 
the noun phrases. 
Moldovan et al(2004) also use WordNet. They 
experiment with a Bayesian algorithm, decision 
trees, and their own algorithm; semantic scattering. 
  As far as we are aware ours is the first system to 
combine features derived from a hand-built lexical 
database with corpus frequencies of lexical 
patterns. 
2 System Description 
 2.1 WordNet Features 
Our system uses both features derived from 
WordNet and features obtained by collecting web 
frequencies for lexical patterns. We did not use any 
information from the sentence in which the two 
nominals appeared, nor did we use the query used 
to retrieve the examples. We did make use of the 
WordNet sense for the features we obtained from 
WordNet. 
 There are 48 features derived from WordNet. 
Most of these are boolean values indicating 
whether or not each of the nominals in the sentence 
appear below certain other high-level concepts in 
the hypernym hierarchy. We chose 22 high level 
concepts we believed may be good predictors of 
whether or not a nominal could be an argument of 
the semantic relations used in this task. These 
concepts are listed below in table 1. 
 
Table 1. Concepts in the WordNet hierarchy used to 
generate features. 
 
For each of these WordNet entries we checked 
whether or not each of the nominals in the example 
sentence appeared below the entry in the WordNet 
hypernym tree. This gave us 44 features. We also 
checked whether the first nominal was a hypernym 
of the second; and vice-versa; and whether the first 
nominal was a meronym of the second; and vice 
versa. This gives us in total 48 boolean features 
derived from WordNet. 
2.2 Web Frequencies 
The remaining features were numerical values 
obtained by retrieving the frequencies of web 
searches for the two nominals joined by certain 
common prepositions and verbs. These joining 
terms are listed below in table 2. 
  Table 2. Joining terms used to generate features. 
 
physical_entity 
grouping 
attribute 
psychological_feature 
quantity 
container 
act 
work 
being 
natural_object 
instrumentation 
 
 
physical_object 
substance 
matter 
process 
causal_agent 
tool 
device 
content 
event 
unit 
state 
 
of 
for 
in 
on 
at 
with 
             about 
produces 
used for 
has 
contains 
from 
causes 
made from 
375
  
To obtain the frequencies we used the API to the 
?MSN Live? search engine. 
 
Choosing a set of joining terms in a principled 
manner is not an easy task, but there is certainly 
some correlation between a prepositional term or 
short linking verb and a semantic relation. For ex-
ample, ?contains? tends to indicate a spatial rela-
tion, while the preposition ?in? indicates a locative 
relation, either temporal or spatial. 
When collecting web frequencies we took ad-
vantage of the OR operator provided by the search 
engine. For each joining term, we wanted to sum 
the number of hits for the term on its own, the term 
followed by 'a', and the term followed by 'the'. In-
stead of conducting separate queries for each of 
these forms, we were able to sum the results with 
just one search. For example, if the two nominals 
in the sentence were ?battery? and ?phone?; one of 
the queries would be:  
?battery in phone? OR ?battery in a phone? OR 
?battery in the phone? 
These features were numeric values; the raw num-
ber of documents returned by the query. 
2.3 Learning Algorithm 
All of the features were used as input to our  
learning algorithm, which was a Support Vector 
Machine (SVM). An SVM is a method for creating 
a classification function which works by trying to 
find a hypersurface in the space of possible inputs 
that splits the positive examples from the negative 
examples for each class. We did not normalize 
these values as normalization is handled by the 
WEKA implementation which we used. 
WEKA is a machine learning toolkit written in 
Java (Witten and Frank, 1999).  The algorithm we 
used was an SVM trained with the Sequential 
Minimal Optimization method provided by Weka. 
 
3. Results 
The average f-value obtained by our system using 
all of the training data was 65.4. There was a sig-
nificant difference in performance across different 
relations. The results for each relation are below. 
 
 
Relation                         Pre   Rec    F     Acc    
cause-effect  61.7  90.2  73.3  66.2   
instrument-agency  59.3  84.2  69.6  64.1   
product-producer  70.9  98.4  82.4  72.0   
origin-entity  51.4  50.0  50.7  56.8   
theme-tool  52.9  31.0  39.1  60.6   
part-whole  66.7  69.2  67.9  76.4   
content-container 71.4  78.9  75.0  73.0  
Average                        62.0  71.7  65.4  67. 
 
The standard deviation of the f-values is 13.9. 
The average of the f-values is brought down by 
two of the relations; origin-entity and theme-tool. 
The poor performance of these relations was noted 
during early experimentation with the training 
data; and the list of WordNet concepts and joining 
terms was amended to try to improve classifica-
tion, but no improvement was achieved. If the re-
sults for these relations are omitted the average f-
score rises to 73.6 
3.1 Information Gain 
In order to evaluate which features were the 
most useful for each relation, we used the Informa-
tion Gain feature ranking tool in WEKA. This tool 
measures the change in entropy attributed to each 
feature and ranks them accordingly. In some cases 
we found that the high ranking features for a rela-
tion were ones which were intuitively relevant to 
predicting that relation; however some features still 
had high Information Gain despite seeming 
unlikely to be predictive of the relation. 
The eight most informative features for the 
Cause-Effect and Content-Container relations are 
shown below. WordNet features are in normal 
Table 3. The features with the highest information gain 
for cause-effect and content-container. 
Cause-Effect Content-Container
quantity 
at 
used for2 
grouping 
object2 
substance 
substance2 
instrumentation2 
 
Instrumentation2 
Container2 
contains 
physical_object2 
physical_entity2 
psychological_feature 
substance2 
device2 
 
376
  
font; the joining terms for web searches in italics. 
The '2' after a feature indicates that the web search 
was of the form "N2 joining term N1"; or that the 
WordNet property holds for N2; where the relation 
is relation(N1,N2). 
Most of these features make sense. For example, 
the search query ?contains? and the Wordnet entry 
?Container? linked to the second noun are the sec-
ond and third most informative for the content con-
tainer class, and the query ?N2 used for N1? ranks 
highly in the cause-effect relation. However, it is 
unclear why being a hyponym of ?quantity? would 
provide information about the cause-effect relation. 
4    Conclusion and Future Work 
  This paper describes a system for participating in 
SemEval 2007 task 4; ?Classification of Semantic 
Relations Between Nominals?. Our system com-
bines features generated by analyzing the WordNet 
hypernym tree with features which indicate the 
frequencies of certain lexical patterns involving the 
nominals and common prepositions, using the web 
as a corpus.  
  The performance of the system was above the 
average score of other systems which used the 
WordNet sense of the training examples but not the 
query used to obtain them. The system was held 
back particularly by two relations, theme-tool and 
origin-entity. 
  There are many potential avenues for future work 
in this area. We chose 48 features based on Word-
Net and 28 lexical patterns to search the web for. 
These were chosen arbitrarily on the basis that they 
looked like they would be informative in general, 
over all seven relations. A more principled ap-
proach would be to begin with a much larger num-
ber of features and use information gain to select 
the most informative features for each relation in-
dividually. This should improve performance by 
ensuring that only the most relevant features for a 
specific relation are used to train the classifier for 
that relation. 
Also, there is room for more investigation into how 
short prepositional joining phrases map onto un-
derlying semantic relations (Girjiu 2006).  
 
 
 
References 
Roxana Girju. 2006. Out-of-context noun phrase seman-
tic interpretation with cross-linguistic evidence. In 
Proceedings of the 15th ACM international confer-
ence on Information and knowledge management 
Marti A. Hearst: 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. COLING:539-545 
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT/NAACL Workshop on Computational 
Lexical Semantics. Boston , MA. 
Preslav Nakov and Marti Hearst. 2006. Using Verbs to 
Characterize Noun-Noun Relations, in the Proceed-
ings of AIMSA 2006,  
Preslav Nakov and Marti Hearst. 2005. Using the Web 
as an Implicit Training Set: Application to Structural 
Ambiguity Resolution, in HLT/EMNLP'05,  
 Vivi Nastase and Stan Szpakowicz. 2003. Exploring 
Noun-Modifier Semantic Relations. International 
Workshop on Computational Semantics, Tillburg, 
Netherlands,  2003 
Barbara Rosario and Marti A. Hearst. 2001. Classifying 
the semantic relations in noun compounds via a do-
main-specific lexical hierarchy. In Proceedings of the 
2001 Conference on Empirical Methods in Natural 
Language Processing. ACL 
Peter D. Turney. 2001. Mining the web for synonyms: 
PM-IR vs LSA on TOEFL, Proceedings of the 
Twelth European Conference on machine learning,  
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations. 
Machine Learning, 60(1?3):251?278 
Ian H. Witten and Eibe Frank. 1999. Data Mining: 
Practical Machine Learning Tools and Techniques 
with Java Implementations, Morgan Kaufmann 
(1999) 
 
377
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 58?63,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Lexical Patterns in the Google Web 1T Corpus to Deduce
Semantic Relations Between Nouns
Paul Nulty Fintan Costello
School of Computer Science and Informatics School of Computer Science and Informatics
University College Dublin, Belfield University College Dublin, Belfield
 Dublin 4, Ireland  Dublin 4, Ireland
paul.nulty@ucd.ie fintan.costello@ucd.ie
Abstract
This paper investigates methods for using lexical pat-
terns in a corpus to deduce the semantic relation that 
holds between two nouns in a noun-noun compound 
phrase such as ?flu virus? or ?morning exercise?. Much 
of the previous work in this area has used automated 
queries to commercial web search engines. In our exper-
iments we use the Google Web 1T corpus. This corpus 
contains every 2,3, 4 and 5 gram occurring more
than 40 times in Google's index of the web, but has the
advantage of being available to researchers directly
rather than through a web interface. This paper evalu-
ates the performance of the Web 1T corpus on the task 
compared to similar systems in the literature, and also 
investigates what kind of lexical patterns are most in-
formative when trying to identify a semantic relation
between two nouns.
1 Introduction
Noun-noun combinations occur frequently in many
languages, and the problem of semantic disambig-
uation of these phrases has many potential applica-
tions  in  natural  language  processing  and  other 
areas. Search engines which can identify the rela-
tions between nouns may be able to return more 
accurate  results.  Hand-built  ontologies  such  as 
WordNet at present only contain a few basic se-
mantic  relations  between  nouns,  such  as  hyper-
nymy and meronymy. 
If the process of discovering semantic relations
from  text  were  automated,  more  links  could 
quickly be built up. Machine translation and ques-
tion-answering  are  other  potential  applications.  
Noun compounds are very common in English, es-
pecially  in  technical  documentation  and  neolo-
gisms. Latin languages tend to favour prepositional 
paraphrases instead of direct compound translation, 
and to select the correct preposition it is often
necessary to know the semantic relation. One very 
common approach to this problem is to define a
set of semantic relations which capture the interac-
tion between the modifier and the head noun, and 
then attempt to assign one of these semantic rela-
tions to each noun-modifier pair. For example, the 
phrase flu virus could be assigned the semantic re-
lation causal (the virus causes the flu); the relation 
for desert wind could be location (the storm is loc-
ated in the desert). 
There is no consensus as to which set of semantic 
relations best captures the differences in meaning 
of various noun phrases. Work in theoretical lin-
guistics has suggested that noun-noun compounds 
may be formed by the deletion of a predicate verb 
or preposition (Levi 1978). However, whether the 
set of possible predicates numbers 5 or 50, there 
are  likely to  be  some  examples  of  noun phrases 
that fit into none of the categories and some that fit 
in multiple categories.
2 Related Work
The idea of searching a large corpus for specific 
lexicosyntactic phrases to indicate a semantic rela-
tion  of  interest  was  first  described  by  Hearst 
(1992).  Lauer  (1995)  tackled the  problem of  se-
mantically disambiguating noun phrases by trying 
to find the preposition which best describes the re-
lation  between  the  modifier  and  head  noun.  His 
method  involves  searching  a  corpus  for  occur-
rences paraphrases of the form ?noun preposition 
modifier?. Whichever preposition is most frequent 
in this context is chosen to represent the predicate 
58
of the nominal, which poses the same problem of 
vagueness  as  Levi's  approach.  Lapata  and Keller 
(2005)  improved  on  Lauer's  results  on  the  same 
task by using the web as a corpus.
Turney  and  Littman  (2005)  used  queries  to  the 
AltaVista search engine as the basis for their learn-
ing  algorithm.  Using  the  dataset  of  Nastase  and 
Szpakowicz (2003), they experimented with a set 
of 64 short prepositional and conjunctive phrases 
they call ?joining terms? to generate exact queries 
for AltaVista of the form ?noun joining term mod-
ifier?, and ?modifier joining term noun?. These hit 
counts  were  used  with  a  nearest  neighbour  al-
gorithm to assign the noun phrases semantic rela-
tions. 
Nakov and Hearst (2006) present a system that dis-
covers verbs that characterize the relation between 
two  nouns in a compound. By writing structured 
queries  to a web search engine and syntactically 
parsing  the  returned  'snippet',  they  were  able  to 
identify verbs that were suitable predicates. For ex-
ample, for the compound neck vein, they retrieved 
verbs  and  verb-preposition  such  as  predicates 
emerge from, pass through, terminate in, and oth-
ers. However, their evaluation is qualitative; they 
do not attempt to use the verbs directly to categor-
ize a compound as a particular semantic relation. 
Turney  (2006)  examines  similarity  measures  for 
semantic relations. He notes that there are at least 
two  kinds  of  similarity:  attributional  similarity, 
which applies between words, and relational simil-
arity, which holds between pairs of words. 
Words that have a high attributional similarity are 
known as synonyms; e.g. chair and stool. When the 
relations in each of two pairs of words are similar, 
it is said that there is an analogy between the two 
pairs of words, e.g. stone:mason, carpenter:wood.
Turney points out that word pairs with high rela-
tional similarity do not necessarily contain words 
with high attributional similarity. For example, al-
though the relations are similar in traffic:street and 
water:riverbed, water is not similar to traffic, nor 
street similar to riverbed. 
Therefore, a measure of similarity of semantic rela-
tions allows a more reliable judgment of analogy 
than the first-order similarity of the nouns
3 Motivation
When  looking  for  lexical  patterns  between  two 
nouns, as is required with vector-space approaches, 
data  sparseness  is  a  common  problem.  To  over-
come this, many of the best-performing systems in 
this area rely on automated queries to web search-
engines  (Lapata  and  Keller  (2005),  Turney  and 
Littman  (2005),  Nakov  and  Hearst  (2006)).  The 
most  apparent  advantage  of  using  search-engine 
queries is simply the greater volume of data avail-
able. 
Keller and Lapata (2003) demonstrated the useful-
ness of this extra data on a type of word-sense dis-
ambiguation test and also found that web frequen-
cies of bigrams correlated well with
frequencies in a standard corpus. 
Kilgarriff  (2007)  argues  against  the  use  of  com-
mercial  search engines  for  research,  and outlines 
some  of  the  major  drawbacks.  Search  engine 
crawlers  do  not  lemmatize  or  part-of-speech  tag 
their text. This means that to obtain frequencies for 
may different inflectional forms, researchers must 
perform a  separate  query for  each possible  form 
and sum the results. 
 If part-of-speech tagging is required, the 'snippet' 
of  text  that  is  returned  with  each  result  may  be 
tagged after the query has been executed, however 
the APIs for the major search engines have limita-
tions on how many snippets may be retrieved for a 
given query (100 -1000).
Another problem is that search engine query syn-
tax is  limited,  and sometimes  mysterious.  In  the 
case of Google, only basic boolean operators are 
supported (AND, OR, NOT), and the function of 
the wildcard symbol (*) is limited, difficult to de-
cipher and may have changed over time. 
Kilgarriff also points out that the search API ser-
vices to the major search engines have constraints 
on the number of searches that are allowed per user 
per day. Because of the multiple searches that are 
needed to cover inflectional  variants and recover 
snippets for  tagging,  a limit  of  1000 queries per 
day, as with the Google API, makes experimenta-
tion slow. This paper will describe the use of the 
Web 1T corpus, made available by Google in 2006 
(Brants and Franz 2006). This corpus consists of n-
grams collected from web data, and is available to 
researchers  in  its  entirety,  rather  than  through  a 
web search interface. This means that there is no 
59
limit  to the amount of searches that may be per-
formed, and an arbitrarily complex query syntax is
possible. 
Despite being available since 2006, few research-
ers have made use of the Web 1T corpus. Hawker 
(2006) provides an example of using the corpus for 
word sense documentation, and describes a method 
for  efficient  searching.  We  will  outline  the  per-
formance of the corpus on the task of identifying 
the semantic relation between two nouns. Another 
motivation behind this paper is to examine the use-
fulness of different lexical patterns for the task of 
deducing semantic relations.
 In this paper, we are interested in whether the fre-
quency with which a joining term occurs between 
two nouns is related to how it indicates a semantic 
interaction. This is in part motivated by Zipf?s the-
ory which states that the more frequently a word 
occurs in a corpus the more meanings or senses it 
is  likely to  have  (Zipf  1929).  If  this  is  true,  we 
would expect that very frequent prepositions, such 
as ?of?, would have many possible meanings and 
therefore not reliably predict a semantic relation.
However, less frequent prepositions, such as ?dur-
ing? would have a more limited set of senses and 
therefore  accurately  predict  a  semantic  relation. 
Zipf also showed that the frequency of a term is re-
lated  to  its  length.  We  will  investigate  whether 
longer lexical patterns are more useful at identify-
ing semantic relations than shorter patterns, and
whether less frequent patterns perform better than 
more frequent ones.
4 Web 1T Corpus
The Web1T corpus consists of n-grams taken from
approximately  one  trillion  words  of  English  text 
taken from web pages in Google's  index of web 
pages. The data includes all 2,3,4 and 5-grams that 
occur more than 40 times in these pages. The data 
comes  in  the  form  of  approximately  110  com-
pressed files for each of the window sizes. Each of 
these files consists of exactly 10 million n-grams, 
with their frequency counts. Below is an example 
of the 3-gram data:
ceramics collection and 43
ceramics collection at 52
ceramics collection is 68
ceramics collection | 59
ceramics collections , 66
ceramics collections . 60
The uncompressed 3-grams,  4-grams 5-grams to-
gether take up 80GB on disk. In order to make it 
possible to index and search this data, we excluded 
n-grams that contained any punctuation or non-al-
phanumeric characters. We also excluded n-grams 
that contained any uppercase letters,  although we 
did allow for the first letter of the first word to be 
uppercase. 
We indexed the data using Ferret, a Ruby port of 
the Java search engine package Lucene. We were 
able to index all of the data in under 48 hours, us-
ing 32GB of hard disk space. The resulting index 
was searchable by first word, last word, and inter-
vening pattern. Only n-grams with a frequency of 
40 or higher are included in the dataset, which ob-
viously means that an average query returns fewer 
results than a web search. However, with the data 
available  on  local  disk  it  is  stable,  reliable,  and 
open to any kind of query syntax or lemmatization.
5 Lexical Patterns for Disambiguation
Modifier-noun phrases are often used interchange-
ably with paraphrases which contain the modifier 
and  the  noun  joined  by  a  preposition  or  simple 
verb. For example, the noun-phrase ?morning exer-
cise? may be paraphrased as ?exercise in the morn-
ing? or ?exercise during the morning?. In a very 
large corpus, it is possible to find many reasonable 
paraphrases  of  noun  phrases.  These  paraphrases 
contain information about the relationship
between the modifier and the head noun that is not 
present in the bare modifier-noun phrase. By ana-
lyzing these paraphrases, we can deduce what se-
mantic  relation  is  most  likely.  For  example,  the 
paraphrases  ?exercise  during  the  morning?  and 
?exercise in the morning? are likely to occur more 
frequently  than  ?exercise  about  the  morning?  or 
?exercise at the morning?. 
One  method  for  deducing  semantic  relations 
between words  in  compounds  involves  gathering 
n-gram frequencies of these paraphrases, contain-
ing a noun,  a modifier  and a lexical  pattern that 
links  them.  Some algorithm can then be used to 
map from lexical patterns to frequencies to semant-
60
ic relations and so find the correct relation for the 
compound in question. This is the approach we use 
in our experiments.
 In order to describe the semantic relation between 
two  nouns  in  a  compound  ?noun1  noun2?  we 
search for ngrams that begin with  noun2  and end 
with noun1, since in English the head of the noun 
compound is the second word. For example, for the 
compound 'flu virus', we look at n-grams that begin 
with 'virus' and end with 'flu'. We extract the words 
that occur between the two nouns (a string of 1-3 
words) and use these lexical patterns as features for 
the machine learning algorithm. 
For  each  compound  we  also  include  n-grams 
which have the plural form of noun1 or noun2. We 
assign a score to each of these lexical patterns, as 
the log of the frequency of the n-gram. We used 
the 400 most frequent lexical patterns extracted as 
the features for the model. Below are examples of 
some of the lexical patterns that were extracted:
and
of the
of
in the
for
and the
for the
to the
with
in
or
on the
from the
the
to
of a
with the
on
that the
from
Figure 1: The 20 most frequent patterns
The simplest way to use this vector space model to 
classify noun-noun combinations is  to use a dis-
tance metric to compare a novel pair of nouns to 
ones previously annotated with semantic relations. 
Nulty  (2007)  compares  these  nearest  neighbor 
models with other machine learning techniques and 
finds that using a support vector machine leads to 
improved classification.
In our experiments we used the support vector ma-
chine and k-nearest-neighbor algorithms from the 
WEKA machine learning toolkit. All experiments 
were conducted using leave-one-out cross valida-
tion: each example in the dataset is in turn tested 
alone, with all the other examples used for training. 
The first dataset used in these experiments was cre-
ated by Nastase and Szpackowicz (2003) and used 
in experiments by Turney and Littmann (2005) and 
Turney  (2006).  The  data  consists  of  600  noun-
modifier  compounds.  Of  the  600 examples,  four 
contained hyphenated modifiers, for example ?test-
tube baby?. These were excluded from our dataset, 
leaving 596 examples. The data is labeled with two 
different sets of semantic relations: one set of 30 
relations with fairly specific meanings and another 
set of 5 relations with more abstract relations. In 
these experiments  we use only the set  of  5 rela-
tions. The reason for this is that splitting a set of 
600 examples into 30 classes results in few training 
examples per class. This problem is compounded 
by the fact that the dataset is uneven, with far more 
examples in some classes than in others. Below are 
the five relations and some examples.
Relation: Example:
causal flu virus, onion tear
temporal summer travel, night class
spatial west coast, home remedy
participant mail sorter, blood donor
quality rice paper, picture book
Figure 2: Example phrases and their semantic relations
For our research we are particularly interested in 
noun-noun combinations. Of the 596 examples in 
the dataset, we found that 325 were clearly noun-
noun  combinations,  e.g.?picture  book?,  rice 
paper?, while in the remainder the modifier was an 
adjective, for example ?warm air?, ?heavy storm?. 
We used only the noun-noun combinations in our
experiments,  as this  is  the focus of  our research. 
We experimented with both lemmatization of the 
data and excluding semantically empty stop words 
(determiners  and  conjunctions)  from  the  lexical 
patterns,  however  neither  of  these  methods  im-
proved  performance.  Below  are  the  results  ob-
tained with the k-nearest neighbor algorithm. The
optimum value of k was 3.
Precision Recall f-score class
.442 .452 .447 Quality
.75 .444 .558 Temporal
.243 .167 .198 Causal
.447 .611 .516 Participant
.571 .138 .222 Spatial
 Figure 3: Results using the K-NN algorithm
The overall accuracy was 44% and the macro-aver-
aged f-value was .39.
61
Below are the results obtained using the support-
vector machine algorithm:
Precision Recall f-score class
.725 .345 .468 Quality
.733 .407 .524 Temporal
.545 .111 .185 Causal
.472 .885 .615 Participant
.462 .207 .268 Spatial
 Figure 4: Results using the Support Vector Machine
 The  overall  accuracy  was  51.7% and  the  mac-
roaveraged  f-value  was  .42.  A  majority  class 
baseline
(always predicting the largest class) would achieve 
an accuracy of 43.7%.
6 Which Lexical Patterns are Most Use-
ful?
In addition to evaluating the Google Web 1T cor-
pus,  a  motivation for this  paper is  to investigate 
what  kind of lexical  patterns are most  useful  for 
deducing semantic relations. In order to investigate 
this, we repeated the experiment one using the 3-
grams,  4-grams  and  5-grams  separately,  which 
gave lexical patterns of length 1, 2 and 3 respect-
ively. Accuracy obtained using the support vector
machine and k-nearest-neighbor algorithms are be-
low:
3-grams 4grams 5-grams All
KNN 36 42.5 42.4 44
SVM 44.3 49.2 43.4 51.7
 Figure 5: Results for different sizes of lexical patterns
Again, in each case the support vector machine
performs  better  than  the  nearest  neighbor  al-
gorithm. The 4- grams (two-word lexical patterns) 
give the best performance. One possible explana-
tion for this is that the single word lexical patterns 
don't convey a very specific relation, while the 3 
word  patterns  are  relatively  rare  in  the  corpus, 
leading  to  many  missing  values  in  the  training 
data. 
We were also interested in how the frequency of 
the lexical patterns related to their ability to predict 
the correct semantic relation. To evaluate this, we 
ordered the 400 lexical patterns retrieved by fre-
quency and then split them into three groups. We 
took  the  64  most  frequent  patterns,  the  patterns 
ranked  100-164  in  frequency,  and  those  ranked 
300-364. We chose to include 64 patterns in each 
group to  allow for  comparison  with  Turney and 
Littman  (2001),  who use  64 hand-generated  pat-
terns. Examples of the most frequent patterns are 
shown in Fig 1.  Below are  examples  of  patterns 
from the other two groups.
as well as
out of the
of one
of fresh
into
for all
was
with your
related to the
in the early
my
on Friday
without
which the
with my
and their
around the
when
whose
during
Figure 6: Frequency Ranks 100-120
to produce
but
that cause
of social
while the
or any other
such as the
are in the
to provide
if a
from one
one
provides
from your
of edible
levels and
comes from
chosen by the
producing
does not
than the
belonging to the
Figure 7: Frequency Ranks 300-320
The accuracies obtained using patterns in the dif-
ferent frequency groups are shown below.
1-64 100-164 300-364
KNN 40.9 43.5 41.9
SVM 47.6 45.2 41.5
 Figure 8: Results for different frequency bands of pat-
terns
Although there is no large effect to the accuracy of 
the KNN algorithm, the Support Vector Machine 
seems to perform better with the most frequent pat-
terns. One possible explanation for this is that al-
though the  less  frequent  patterns  seem more  in-
formative, they more often result in zero matches 
in the corpus, which simply leaves a missing value 
in the training data.
62
7 Conclusion
This paper reports several experiments on the se-
mantic disambiguation of noun-noun phrases using 
the Google Web 1T corpus, and shows that the res-
ults are comparable to previous work which has re-
lied on a web interface to search engines. Having a 
useful corpus based on web data that can be stored 
and  searched  locally  means  that  results  will  be 
stable across time and can be subject to complex 
queries. Experiments designed to evaluate the use-
fulness  of  different  lexical  patterns  did not  yield 
strong results and further work is required in this 
area.
References 
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus  Version  1.1.  Technical  report,  Google  Re-
search
Tobias Hawker. 2006. Using Contexts of One Trillion
 Words for WSD. Proceedings of the 10th Conference  
of the Pacific Association for Computational
Linguistics, pages 85?93.
Marti A. Hearst: 1992. Automatic Acquisition of
Hyponyms  from  Large  Text  Corpora.  COLING:
539-545
Keller, Frank and Mirella Lapata. 2003. Using the 
     Web to Obtain Frequencies for Unseen Bigrams
      Computational Linguistics 29:3, 459-484. 
Adam Kilgarriff, 2007. Googleology is Bad Science.
Comput. Linguist. 33, 1 147-151.
Lapata, Mirella and Frank Keller. 2005. Web Based
    Models for Natural Language Processing. ACM 
    Transactions on Speech and Language
    Processing 2:1, 1-31. 
Mark Lauer. Designing Statistical Language Learners:
     Experiments on Noun Compounds. PhD thesis,
    Macquarie University NSW 2109 Australia.
Judith Levi. (1978) The Syntax and Semantics of 
    Complex Nominals, Academic Press, New York, NY.
Phil Maguire (2007) A cognitive model of conceptual
    combination Unpublished PhD Thesis, UCD Dublin
Preslav Nakov and Marti Hearst. 2006. Using Verbs to
    Characterize Noun-Noun Relations, in the 
    Proceedings of AIMSA 2006,
Preslav Nakov and Marti Hearst. 2005. Using the Web
    as an Implicit Training Set: Application to Structural
    Ambiguity Resolution, in HLT/EMNLP'0
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
    Noun-Modifier Semantic Relations. International
    Workshop on Computational Semantics, Tillburg,
    Netherlands, 2003
Paul Nulty and Fintan Costello, 2007. Semantic 
    Classification of Noun Phrases Using Web Counts
 and Learning Algorithms. Proceedings of
 ACL 2007 Student Reseach Workshop. 
Barbara Rosario and Marti A. Hearst. 2001. Classifying
 the semantic relations in noun compounds via a 
 domain-specific lexical hierarchy. In Proceedings of
 the 2001Conference on Empirical Methods in 
 Natural Language Processing. ACL
Peter D. Turney. 2005. Measuring semantic similarity
     by latent relational analysis. In Proceedings of the
     Nineteenth International Joint Conference on 
     Artificial Intelligence (IJCAI-05), pages 1136-1141.
Peter D. Turney., and Michael L. Littman,. 2006. 
     Corpus based learning of analogies and semantic
     relations. Machine Learning, 60(1?3):251?278
Ian H. Witten and Eibe Frank. 1999. Data Mining:
     Practical Machine Learning Tools and Techniques
     with Java Implementations, Morgan Kaufman (1999)
George K. Zipf. 1932. Selected Studies of the Principle
      of Relative Frequency in Language. Cambridge, MA
63
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 234?237,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UCD-PN: Selecting General Paraphrases Using Conditional Probability
Paul Nulty
University College Dublin
Dublin, Ireland
paul.nulty@ucd.ie
Fintan Costello
University College Dublin
Dublin, Ireland
fintan.costello@ucd.ie
Abstract
We describe a system which ranks human-
provided paraphrases of noun compounds,
where the frequency with which a given
paraphrase was provided by human volun-
teers is the gold standard for ranking. Our
system assigns a score to a paraphrase of
a given compound according to the num-
ber of times it has co-occurred with other
paraphrases in the rest of the dataset. We
use these co-occurrence statistics to com-
pute conditional probabilities to estimate a
sub-typing or Is-A relation between para-
phrases. This method clusters together
paraphrases which have similar meanings
and also favours frequent, general para-
phrases rather than infrequent paraphrases
with more specific meanings.
1 Introduction
SemEval 2010 Task 9, ?Noun Compound Inter-
pretation Using Paraphrasing Verbs?, requires sys-
tems to rank paraphrases of noun compounds
according to which paraphrases were most fre-
quently produced for each compound by human
annotators (Butnariu et al, 2010). This paper de-
scribes a system which ranks a paraphrase for a
given compound by computing the probability of
the paraphrase occurring given that we have previ-
ously observed that paraphrase co-occurring with
other paraphrases in the candidate paraphrase list.
These co-occurrence statistics can be built using
either the compounds from the test set or the train-
ing set, with no significant difference in results.
The model is informed by two observations:
people tend to use general, semantically light para-
phrases more often than detailed, semantically
heavy ones, and most paraphrases provided for a
specific compound indicate the same interpreta-
tion of that compound, varying mainly according
to level of semantic detail.
Given these two properties of the data, the ob-
jective of our system was to test the theory that
conditional probabilities can be used to estimate a
sub-typing or Is-A relation between paraphrases.
No information about the compounds was used,
nor were the frequencies provided in the training
set used.
2 Motivation
Most research on the disambiguation of noun com-
pounds involves automatically categorizing the
compound into one of a pre-defined list of seman-
tic relations. Paraphrasing compounds is an alter-
native approach to the disambiguation task which
has been explored by (Lauer, 1995) and (Nakov,
2008). Paraphrases of semantic relations may be
verbs, prepositions, or ?prepositional verbs? like
found in and caused by. (Lauer, 1995) catego-
rized compounds using only prepositions. (Nakov,
2008) and the current task use only verbs and
prepositional verbs, however, many of the para-
phrases in the task data are effectively just prepo-
sitions with a copula, e.g. be in, be for, be of.
The paraphrasing approach may be easier to
integrate into applications such as translation,
query-expansion and question-answering ? its
output is a set of natural language phrases rather
than an abstract relation category. Also, most
sets of pre-defined semantic relations have only
one or maybe two levels of granularity. This
can often lead to semantically converse relations
falling under the same abstract category, for ex-
ample a headache tablet is a tablet for prevent-
ing headaches, while headache weather is weather
that induces headaches ? but both compounds
would be assigned the same relation (perhaps in-
strumental or causal) in many taxonomies of se-
mantic relations. Paraphrases of compounds using
verbs or verb-preposition combinations can pro-
vide as much or as little detail as is required to
adequately disambiguate the compound.
234
2.1 General paraphrases are frequent
The object of SemEval 2010 Task 9 is to rank para-
phrases for noun compounds given by 50-100 hu-
man annotators. When deciding on a model we
took into account several observations about the
data.
Firstly, the model does not need to produce
plausible paraphrases for noun compounds, it sim-
ply needs to rank paraphrases that have been pro-
vided. Given that all of the paraphrases in the
training and test sets have been produced by peo-
ple, we presume that all of them will have at
least some plausible interpretation, and most para-
phrases for a given compound will indicate gen-
erally the same interpretation of that compound.
This will not always be the case; some compounds
are genuinely ambiguous rather than vague. For
example a stone bowl could be a bowl for hold-
ing stones or a bowl made of stone. However, the
mere fact that a compound has occurred in text is
evidence that the speaker who produced the text
believed that the compound was unambiguous, at
least in the given context.
Given that most of the compounds in the dataset
have one clear plausible meaning to readers, when
asked to paraphrase a compound people tend to
observe the Grician maxim of brevity (Grice,
1975) by using simple, frequent terms rather than
detailed, semantically weighty paraphrases. For
the compound alligator leather in the training
data, the two most popular paraphrases were be
made from and come from. Also provided as
paraphrases for this compound were hide of and
be skinned from. These are more detailed, spe-
cific, and more useful than the most popular para-
phrases, but they were only produced once each,
while be made from and come from were pro-
vided by 28 and 20 annotators respectively. This
trend is noticeable in most of the compounds in
the training data - the most specific and detailed
paraphrases are not the most frequently produced.
According to the lesser-known of Zipf?s laws ?
the law of meaning (Zipf, 1945) ? words that are
more frequent overall in a language tend to have
more sub-senses. Frequent terms have a shorter
lexical access time (Broadbent, 1967), so to min-
imize the effort required to communicate mean-
ing of a compound, speakers should tend to use
the most common words - which tend to be se-
mantically general and have many possible sub-
senses. This seems to hold for paraphrasing verbs
and prepositions; terms that have a high overall
frequency in English such as be in, have and be of
are vague ? there are many more specific para-
phrases which could be considered sub-senses of
these common terms.
2.2 Using conditional probability to detect
subtypes
Our model uses conditional probabilities to detect
this sub-typing structure based on the theory that
observing a specific, detailed paraphrase is good
evidence that a more general parent sense of that
paraphrase would be acceptable in the same con-
text. The reverse is not true - observing a fre-
quently occurring, semantically light paraphrase
is not strong evidence that any sub-sense of that
paraphrase would be acceptable in the same con-
text. For example, consider the spatial and tempo-
ral sub-senses of the paraphrase be in. A possible
spatial sub-sense of this paraphrase is be located
in, while a possible temporal sub-sense would be
occur during. The fact that occur during is pro-
vided as a paraphrase for a compound almost al-
ways means that be in is also a plausible para-
phrase. However, observing be in as a paraphrase
does not provide such strong evidence for occur
during also being plausible, as we do not know
which sub-sense of in is intended.
If this is correct, then we would expect that the
conditional probability of a paraphrase B occur-
ring given that we have observed another para-
phrase A in the same context is a measure of the
extent to which B is a more general type (parent
sense) of A.
3 System Description
The first step in our model is to generate a condi-
tional probability table by going over all the com-
pounds in the data and calculating the probabil-
ity of each paraphrase occurring given that we ob-
served another given paraphrase co-occurring for
the same compound. We compute the conditional
probability of every paraphrase with all other para-
phrases individually. We could use either the train-
ing or the test set to collect these co-occurrence
statistics, as the frequencies with which the para-
phrases are ranked are not used ? we simply note
how many times each paraphrase co-occurred as a
possible paraphrase for the same compound with
each other paraphrase. For the submitted system
we used the test data, but subsequently we con-
235
firmed that using only the training data for this step
is not detrimental to the system?s performance.
For each paraphrase in the data, the conditional
probability of that paraphrase is computed with re-
spect to all other paraphrases in the data. For any
two paraphrases B and A:
P (B|A) =
P (A ?B)
P (A)
As described in the previous section, we antic-
ipate that more general, less specific paraphrases
will be produced more often than their more de-
tailed sub-senses. Therefore, we score each para-
phrase by summing its conditional probability
with each other paraphrase provided for the same
compound.
For a list of paraphrases A provided for a given
compound, we score a paraphrase b in that list by
summing its conditional probability individually
with every other paraphrase in the list.
score(b) =
?
a?A
P (b|a)
This gives the more general, broad coverage,
paraphrases a higher score, and also has a cluster-
ing effect whereby paraphrases that have not co-
occurred with the other paraphrases in the list very
often for other compounds are given a lower score
? they are unusual in the context of this para-
phrase list.
4 Results and Analysis
4.1 Task results
Table 1 shows the results of the top 3 systems in
the task. Our system achieved the second high-
est correlation according to the official evaluation
measure, Spearman?s rank correlation coefficient.
Results were also provided using Pearson?s corre-
lation coefficient and the cosine of the vector of
scores for the gold standard and submitted pre-
dictions. Our system performed best using the
cosine measure, which measures how closely the
predicted scores match the gold standard frequen-
cies, rather than the rank correlation. This could
be helpful as the scores provide a scale of accept-
ability.
As mentioned in the system description, we
collected the co-occurrence statistics for our sub-
mitted prediction from the test set of paraphrases
alone. Since our model does not use the frequen-
cies provided in the training set, we chose to use
System Spearman Pearson Cosine
UVT .450 .411 .635
UCD-PN .441 .361 .669
UCD-GOG .432 .395 .652
baseline .425 .344 .524
Table 1: Results for the top three systems.
the test set as it was larger and had more annota-
tors. This could be perceived as an unfair use of
the test data, as we are using all of the test com-
pounds and their paraphrases to calculate the po-
sition of a given paraphrase relative to other para-
phrases.
This is a kind of clustering which would not be
possible if only a few test cases were provided. To
check that our system did not need to collect co-
occurrence probabilities on exactly the same data
as it made predictions on, we submitted a second
set of predictions for the test based on the proba-
bilities from the training compounds alone.
1
These predictions actually achieved a slightly
better score for the official evaluation measure,
with a Spearman rho of 0.444, and a cosine of
0.631. This suggests that the model does not need
to collect co-occurrence statistics from the same
compounds as it makes predictions on, as long as
sufficient data is available.
4.2 Error Analysis
The most significant drawback of this system is
that it cannot generate paraphrases for noun com-
pounds - it is designed to rank paraphrases that
have already been provided.
Using the conditional probability to rank para-
phrases has two effects. Firstly there is a cluster-
ing effect which favours paraphrases that are more
similar to the other paraphrases in a list for a given
compound. Secondly, paraphrases which are more
frequent overall receive a higher score, as frequent
verbs and prepositions may co-occur with a wide
variety of more specific terms.
These effects lead to two possible drawbacks.
Firstly, the system would not perform well if de-
tailed, specific paraphrases of compounds were
needed. Although less frequent, more specific
paraphrases may be more useful for some appli-
cations, these are not the kind of paraphrases that
people seem to produce spontaneously.
1
Thanks to Diarmuid
?
O S?eaghdha for pointing this out
and scoring the second set of predictions
236
Also, because of the clustering effect, this sys-
tem would not work well for compounds that are
genuinely ambiguous e.g. stone bowl (bowl made
of stone vs bowl contains stones). Most examples
are not this ambiguous, and therefore almost all
of the provided paraphrases for a given compound
are plausible, and indicate the same relation. They
vary mainly in how specific/detailed their explana-
tion of the relation is.
The three compounds which our system pro-
duced the worst rank correlation for were diesel
engine, midnight train, and bathing suit. With-
out access to the gold-standard scores for these
compounds it is difficult to explain the poor per-
formance, but examining the list of possible para-
phrases for the first two of these suggests that the
annotators identified two distinct senses for each:
diesel engine is paraphrased by verbs of contain-
ment (e.g. be in) and verbs of function (e.g. runs
on), while midnight train is paraphrased by verbs
of location (e.g. be found in, be located in) and
verbs of movement (e.g. run in, arrive at). Our
model works by separating paraphrases according
to granularity, and cannot disambiguate these dis-
tinct senses. The list of possible paraphrases for
bathing suit suggests that our model is not robust
if implausible paraphrases are in the candidate list
- the model ranked be in, be found in and emerge
from among the top 8 paraphrases for this com-
pound, even though they are barely comprehensi-
ble as plausible paraphrases. The difficulty here
is that even if only one annotator suggests a para-
phrase, it is deemed to have co-occurred with other
paraphrases in that list, since we do not use the fre-
quencies from the training set.
The compounds for which the highest correla-
tions were achieved were wilderness areas, conso-
nant systems and fiber optics. The candidate para-
phrases for the first two of these seem to be fairly
homogeneous in semantic intent. Fiber optics
is probably a lexicalised compound which hardly
needs paraphrasing. This would lead people to use
short and semantically general paraphrases.
5 Conclusion
We have described a system which uses a simple
statistical method, conditional probability, to es-
timate a sub-typing relationship between possible
paraphrases of noun compounds. From a list of
candidate paraphrases for each noun compound,
those which were judged by this method to be
good ?parent senses? of other paraphrases in the
list were scored highly in the rankings.
The system does require a large dataset of com-
pounds with associated plausible paraphrases, but
it does not require a training set of human pro-
vided rankings and does not use any information
about the noun compound itself, aside from the list
of plausible paraphrases that were provided by the
human annotators.
Given the simplicity of our model and its per-
formance compared to other systems which used
more intensive approaches, we believe that our ini-
tial observations on the data are valid: people tend
to produce general, semantically light paraphrases
more often than specific or detailed paraphrases,
and most of the paraphrases provided for a given
compound indicate a similar interpretation, vary-
ing instead mainly in level of semantic weight or
detail.
We have also shown that conditional probabil-
ity is an effective way to compute the sub-typing
relation between paraphrases.
Acknowledgement
This research was supported by a grant under the
FP6 NEST Programme of the European Commis-
sion (ANALOGY: Humans the Analogy-Making
Species: STREP Contr. No 029088).
References
Donald E. Broadbent 1967. Word-frequency effect
and response bias.. Psychological Review, 74,
Cristina Butnariu and Su Nam Kim and Preslav Nakov
and Diarmuid
?
O S?eaghdha and Stan Szpakowicz and
Tony Veale. 2010. SemEval-2 Task 9: The In-
terpretation of Noun Compounds Using Paraphras-
ing Verbs and Prepositions, Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, Upp-
sala, Sweden
Paul Grice. 1975. Studies in the Way of Words. Har-
vard University Press, Cambridge, Mass.
Mark Lauer 1995. Designing statistical language
learners: experiments on noun compound, PhD The-
sis Macquarie University, Australia
Preslav Nakov and Marti Hearst 2008. Solving Re-
lational Similarity Problems using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-08), Columbus, OH.
George Kingsley Zipf. 1945. The Meaning-Frequency
Relationship of Words. Journal of General Psychol-
ogy, 33,
237
