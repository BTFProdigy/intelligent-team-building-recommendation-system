Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 77?86,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatically Producing Plot Unit Representations for Narrative Text
Amit Goyal
Dept. of Computer Science
University of Maryland
College Park, MD 20742
amit@umiacs.umd.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD 20742
hal@umiacs.umd.edu
Abstract
In the 1980s, plot units were proposed as a
conceptual knowledge structure for represent-
ing and summarizing narrative stories. Our
research explores whether current NLP tech-
nology can be used to automatically produce
plot unit representations for narrative text. We
create a system called AESOP that exploits
a variety of existing resources to identify af-
fect states and applies ?projection rules? to
map the affect states onto the characters in a
story. We also use corpus-based techniques
to generate a new type of affect knowledge
base: verbs that impart positive or negative
states onto their patients (e.g., being eaten is
an undesirable state, but being fed is a desir-
able state). We harvest these ?patient polar-
ity verbs? from a Web corpus using two tech-
niques: co-occurrence with Evil/Kind Agent
patterns, and bootstrapping over conjunctions
of verbs. We evaluate the plot unit representa-
tions produced by our system on a small col-
lection of Aesop?s fables.
1 Introduction
In the 1980s, plot units (Lehnert, 1981) were pro-
posed as a knowledge structure for representing nar-
rative stories and generating summaries. Plot units
are fundamentally different from the story represen-
tations that preceded them because they focus on the
affect states of characters and the tensions between
them as the driving force behind interesting and co-
hesive stories. Plot units were used in narrative sum-
marization studies, both in computer science and
psychology (Lehnert et al, 1981), but previous com-
putational models of plot units relied on tremendous
amounts of manual knowledge engineering.
The last few decades have seen tremendous ad-
vances in NLP and the emergence of many resources
that could be useful for plot unit analysis. So we em-
barked on a project to see whether plot unit repre-
sentations can be generated automatically using cur-
rent NLP technology. We created a system called
AESOP that uses a variety of resources to iden-
tify words that correspond to positive, negative, and
mental affect states. AESOP uses affect projection
rules to map the affect states onto the characters in
the story based on verb argument structure. Addi-
tionally, affect states are inferred based on syntactic
properties, and causal and cross-character links are
created using simple heuristics.
Affect states often arise from actions that produce
good or bad states for the character that is acted
upon. For example, ?the cat ate the mouse? pro-
duces a negative state for the mouse because being
eaten is bad. Similarly, ?the man fed the dog? pro-
duces a positive state for the dog because being fed
is generally good. Knowledge about the effects of
actions (i.e., state changes) on patients is not readily
available in existing semantic resources. We create
a new type of lexicon consisting of patient polarity
verbs (PPVs) that impart positive or negative states
on their patients. These verbs reflect world knowl-
edge about desirable/undesirable states for animate
beings; for example, being fed, paid or adopted are
generally desirable states, while being eaten, chased
or hospitalized are generally undesirable states.
We automatically generate a lexicon of ?patient
polarity verbs? from a Web corpus using two tech-
77
The Father and His Sons
(s1) A father had a family of sons who were perpetually
quarreling among themselves. (s2) When he failed to
heal their disputes by his exhortations, he determined
to give them a practical illustration of the evils of dis-
union; and for this purpose he one day told them to
bring him a bundle of sticks. (s3) When they had done
so, he placed the faggot into the hands of each of them
in succession, and ordered them to break it in pieces.
(s4) They tried with all their strength, and were not
able to do it. (s5) He next opened the faggot, took the
sticks separately, one by one, and again put them into
his sons? hands, upon which they broke them easily.
(s6) He then addressed them in these words: ?My sons,
if you are of one mind, and unite to assist each other,
you will be as this faggot, uninjured by all the attempts
of your enemies; but if you are divided among your-
selves, you will be broken as easily as these sticks.?
(a) ?Father and Sons? Fable
Father Sons
(quarreling)a1
(stop quarreling)a3
(annoyed)a2
(exhortations)a4
(exhortations fail)a5
m
m
a
(teach lesson)a6
m
(get sticks & break)a7m (get sticks & break)a8
(cannot break sticks)a9
a
(cannot break sticks)a10
a
(bundle & break)a11 (bundle & break)a12
(break sticks)a13
a
(break sticks)a14
a
m
a
shared
request
request
mixed
shared
s2
s2
s2
s2
s2
s2
s4
s5
s5
s1
s2
s4
s5
s5
(lesson succeeds)a15s5
(b) Plot Unit Analysis for ?Father and Sons? Fable
Figure 1: Sample Fable and Plot Unit Representation
niques: patterns that identify co-occurrence with
stereotypically evil or kind agents, and a bootstrap-
ping algorithm that learns from conjunctions of
verbs. We evaluate the plot unit representations pro-
duced by our system on a small collection of fables.
2 Overview of Plot Units
Plot unit structures consist of affect states for each
character, and links defining the relationships be-
tween them. Plot units include three types of affect
states: positive (+), negative (-), and mental (M).
Affect states can be connected by causal links and
cross-character links, which explain how the nar-
rative hangs together. Causal links exist between
affect states for the same character and have four
types: motivation (m), actualization (a), termination
(t) and equivalence (e). Cross-character links indi-
cate that a single event affects multiple characters.
For instance, if one character requests something of
another, then each character is assigned an M state
and a cross-character link connects the states.
To see a concrete example of a plot unit represen-
tation, a short fable, ?The Father and His Sons,? is
shown in Figure 1(a) and our annotation of its plot
unit structure is shown in Figure 1(b). In this fable,
there are two characters, the ?Father? and (collec-
tively) the ?Sons?, who go through a series of affect
states depicted chronologically in the two columns.
The first affect state (a1) is produced from sen-
tence #1 (s1) and is a negative state for the sons be-
cause they are quarreling. This state is shared by the
father (via a cross-character link) who has a nega-
tive annoyance state (a2). The father decides that
he wants to stop the sons from quarreling, which
is a mental event (a3). The causal link from a2 to
a3 with an m label indicates that his annoyed state
?motivated? this decision. His first attempt is by ex-
hortations (a4). The first M (a3) is connected to the
second M (a4) with an m (motivation) link, which
represents subgoaling. The father?s overall goal is
to stop the quarreling (a3), and to do so he creates a
subgoal of exhorting the sons to stop (a4). The ex-
hortations fail, which produces a negative state (a5)
for the father. The a causal link indicates an ?actu-
alization?, representing the failure of his plan (a4).
This failure motivates a new subgoal: teach the
sons a lesson (a6). At a high level, this subgoal
has two parts, indicated by the two gray regions
(a7 ? a10 and a11 ? a14). The first gray region
begins with a cross-character link (M to M), which
indicates a request (in this case, to break a bundle
of sticks). The sons fail at this, which upsets them
(a9) but pleases the father (a10). The second gray
region depicts the second part of the father?s sub-
goal; he makes a second request (a11 to a12) to sep-
arate the bundle and break the sticks, which the sons
successfully do, making them happy (a13) and the
father happy (a14) as well. This latter structure (the
second gray region) is an HONORED REQUEST plot
unit structure. At the end, the father?s plan succeeds
(a15) which is an actualization (a link) of his goal
to teach the sons a lesson (a6).
78
3 Where Do Affect States Come From?
We briefly overview the variety of situations that can
be represented by affect states in plot units.
Direct Expressions of Emotion: Affect states can
correspond to positive/negative emotional states, as
have been studied in the realm of sentiment anal-
ysis. For example, ?Max was disappointed? pro-
duces a negative affect state for Max, and ?Max was
pleased? produces a positive affect state for Max.
Situational Affect States: Positive and negative af-
fect states can represent good and bad situational
states that characters find themselves in. These
states do not represent emotion, but indicate whether
a situation (state) is good or bad for a character
based on world knowledge. e.g., ?The wolf had a
bone stuck in his throat.? produces a negative affect
state for the wolf. Similarly, ?The woman recovered
her sight.? produces a positive affect state for the
woman.
Plans and Goals: The existence of a plan or goal is
represented as a mental state (M). Plans and goals
can be difficult to detect automatically and can be
revealed in many ways, such as:
? Direct expressions of plans/goals: a plan/goal
may be explicitly stated (e.g., ?John wants food?).
? Speech acts: a plan or goal may be revealed
through a speech act. For example, ?the wolf asked
an eagle to extract the bone? is a directive speech
act that indicates the wolf?s plan to resolve its
negative state (having a bone stuck). This example
illustrates how a negative state (bone stuck) can
motivate a mental state (plan). When a speech act
involves multiple characters, it produces multiple
mental states.
? Inferred plans/goals: plans and goals are some-
times inferred from actions. e.g., ?the lion hunted
deer? implies that the lion has a plan to obtain food.
Similarly, ?the serpent spat poison at John? implies
that the serpent wants to kill John.
? Plan/Goal completion: Plans and goals produce
+/- affect states when they succeed or fail. For
example, if the eagle successfully extracts the bone
from the wolf?s throat, then both the wolf and the
eagle will have positive affect states because both
were successful in their respective goals.
We observed that situational and plan/goal states
often originate from an action. When a character is
acted upon (the patient of a verb), then the charac-
ter may be in a positive or negative state depend-
ing upon whether the action was good or bad for
them based on world knowledge. For example, be-
ing fed, paid or adopted is generally desirable, but
being chased, eaten, or hospitalized is usually unde-
sirable. Consequently, we decided to create a lex-
icon of patient polarity verbs that produce positive
or negative states for their patients. In Section 4.2,
we present two methods for automatically harvest-
ing these verbs from a Web corpus.
4 AESOP: Automatically Generating Plot
Unit Representations
Our system, AESOP, automatically creates plot unit
representations for narrative text. AESOP has four
main steps: affect state recognition, character iden-
tification, affect state projection, and link creation.
During affect state recognition, AESOP identifies
words that may be associated with positive, nega-
tive, and mental states. AESOP then identifies the
main characters in the story and applies affect pro-
jection rules to map the affect states onto these char-
acters. During this process, some additional affect
states are inferred based on verb argument structure.
Finally, AESOP creates cross-character links and
causal links between affect states. We also present
two corpus-based methods to automatically produce
a new resource for affect state recognition: a patient
polarity verb lexicon.
4.1 Plot Unit Creation
4.1.1 Recognizing Affect States
The basic building blocks of plot units are af-
fect states which come in three flavors: positive,
negative, and mental. In recent years, many pub-
licly available resources have been created for sen-
timent analysis and other types of semantic knowl-
edge. We considered a wide variety of resources and
ultimately decided to experiment with five resources
that most closely matched our needs:
? FrameNet (Baker et al, 1998): We manually
identified 87 frame classes that seem to be associ-
ated with affect: 43 mental classes (e.g., COMMU-
NICATION and NEEDING), 22 positive classes (e.g.,
ACCOMPLISHMENT and SUPPORTING), and 22 neg-
ative classes (e.g., CAUSE HARM and PROHIBIT-
79
ING). We use the verbs listed for these classes to
produce M, +, and - affect states.
?MPQA Lexicon (Wilson et al, 2005b): We used
the words listed as having positive or negative senti-
ment polarity to produce +/- states, when they occur
with the designated part-of-speech.
? OpinionFinder (Wilson et al, 2005a) (Version
1.4) : We used the +/- labels assigned by its con-
textual polarity classifier (Wilson et al, 2005b) to
create +/- states and the MPQASD tags produced
by its Direct Subjective and Speech Event Identifier
(Choi et al, 2006) to produce mental (M) states.
? Semantic Orientation Lexicon (Takamura et al,
2005): We used the words listed as having posi-
tive or negative polarity to produce +/- affect states,
when they occur with the designated part-of-speech.
? Speech Act Verbs: We used 228 speech act
verbs from (Wierzbicka, 1987) to produce M states.
4.1.2 Identifying the Characters
For the purposes of this work, we made two sim-
plifying assumptions: (1) There are only two char-
acters per fable1, and (2) Both characters are men-
tioned in the fable?s title. The problem of corefer-
ence resolution for fables is somewhat different than
for other genres, primarily because the characters
are often animals (e.g., he=owl). So we hand-crafted
a simple rule-based coreference system. First, we
apply heuristics to determine number and gender
based on word lists, WordNet (Miller, 1990) and
part-of-speech tags. If no determination of a char-
acter?s gender or number can be made, we employ a
process of elimination. Given the two character as-
sumption, if one character is known to be male, but
there are female pronouns in the fable, then the other
character is assumed to be female. The same is done
for number agreement. Finally, if there is only one
character between a pronoun and the beginning of
a document, then we resolve the pronoun with that
character and the character assumes the gender and
number of the pronoun. Lastly, WordNet provides
some additional resolutions by exploiting hypernym
relations, for instance, linking peasant with man.
4.1.3 Mapping Affect States onto Characters
Plot unit representations are not just a set of af-
fect states, but they are structures that capture the
1We only selected fables that had two main characters.
chronological ordering of states for each character
as the narrative progresses. Consequently, every af-
fect state needs to be attributed to a character. Since
most plots revolve around events, we use verb argu-
ment structure as the primary means for projecting
affect states onto characters.
We developed four affect projection rules that or-
chestrate how affect states are assigned to the char-
acters. We used the Sundance parser (Riloff and
Phillips, 2004) to produce a shallow parse of each
sentence, which includes syntactic chunking, clause
segmentation, and active/passive voice recognition.
We normalized the verb phrases with respect to ac-
tive/passive voice to simplify the rules. We made the
assumption that the Subject of the VP is its AGENT
and the Direct Object of the VP is its PATIENT.2
The rules only project affect states onto AGENTS
and PATIENTS that refer to a character in the story.
The four projection rules are presented below.
1. AGENT VP : This rule applies when the VP
has no PATIENT or the PATIENT corefers with the
AGENT. All affect tags assigned to the VP are pro-
jected onto the AGENT. Example: ?Mary laughed
(+)? projects a + affect state onto Mary.
2. VP PATIENT : This rule applies when the VP
has no agent, which is common in passive voice con-
structions. All affect tags assigned to the VP are
projected onto the PATIENT. Example: ?John was
rewarded (+), projects a + affect state onto John.
3. AGENT VP PATIENT : This rules applies
when both an AGENT and PATIENT are present, do
not corefer, and at least one of them is a character. If
the PATIENT is a character, then all affect tags asso-
ciated with the VP are projected onto the PATIENT.
If the AGENT is a character and the VP has an M
tag, then we also project an M tag onto the AGENT
(representing a shared, cross-character mental state).
4. AGENT VERB1 to VERB2 PATIENT : This
rule has two cases: (a) If the AGENT and PATIENT
refer to the same character, then we apply Rule #1.
Example: ?Bo decided to teach himself...? (b) If the
AGENT and PATIENT are different, then we apply
Rule #1 to VERB1 and Rule #2 to VERB2.
Finally, if an adverb or adjectival phrase has af-
fect, then that affect is mapped onto the preceding
VP and the rules above are applied. For all of the
2This is not always correct, but worked ok in our fables.
80
rules, if a clause contains a negation word, then we
flip the polarity of all words in that clause.
4.1.4 Inferring Affect States
Recognizing plans and goals depends on world
knowledge and inference, and is beyond the scope
of this paper. However, we identified two cases
where affect states often can be inferred based on
syntactic properties. The first case involves verb
phrases (VPs) that have both an AGENT and PA-
TIENT, which corresponds to projection rule #3. If
the VP has polarity, then rule #3 assigns that po-
larity to the PATIENT, not the AGENT. For exam-
ple, ?John killed Paul? imparts negative polarity on
Paul, but not necessarily on John. Unless we are
told otherwise, one assumes that John intentionally
killed Paul, and so in a sense, John accomplished
his goal. Consequently, this action should produce a
positive affect state for John. We capture this notion
of accomplishment as a side effect of projection rule
#3: if the VP has +/- polarity, then we produce an
inferred positive state for the AGENT.
The second case involves infinitive verb phrases
of the form: ?AGENT VERB1 TO VERB2 PA-
TIENT? (e.g., ?Susan tried to warn Mary?). The
infinitive VP construction suggests that the AGENT
has a goal or plan that is being put into motion (e.g.,
tried to, wanted to, attempted to, hoped to, etc.). To
capture this intuition, in rule #4 if VERB1 does not
already have an affect state assigned to it then we
produce an inferred mental state for the AGENT.
4.1.5 Causal and Cross-Character Links
Our research is focused primarily on creating af-
fect states for characters, but plot unit structures
also include cross-character links to connect states
that are shared across characters and causal links
between states for a single character. As an ini-
tial attempt to create complete plot units, AESOP
produces links using simple heuristics. A cross-
character link is created when two characters in a
clause have affect states that originated from the
same word. A causal link is created between each
pair of (chronologically) consecutive affect states
for the same character. Currently, AESOP only pro-
duces forward causal links (motivation (m), actual-
ization (a)) and does not produce backward causal
links (equivalence (e), termination (t)). For forward
links, the causal syntax only allows for five cases:
M m? M , + m? M , ? m? M , M a? +, M a? ?.
So when AESOP produces a causal link between
two affect states, the order and types of the two states
uniquely determine which label it gets (m or a).
4.2 Generating PPV Lexicons
During the course of this research, we identified a
gap in currently available knowledge: we are not
aware of existing resources that identify verbs which
produce a desirable/undesirable state for their pa-
tients even though the verb itself does not carry po-
larity. For example, the verb eat describes an action
that is generally neutral, but being eaten is clearly
an undesirable state. Similarly, the verb fed does not
have polarity, but being fed is a desirable state for the
patient. In the following sections, we try to fill this
gap by using corpus-based techniques to automati-
cally acquire a Patient Polarity Verb (PPV) Lexicon.
4.2.1 PPV Harvesting with Evil/Kind Agents
The key idea behind our first approach is to iden-
tify verbs that frequently occur with evil or kind
agents. Our intuition was that an ?evil? agent will
typically perform actions that are bad for the patient,
while a ?kind? agent will typically perform actions
that are good for the patient.
We manually identified 40 stereotypically evil
agent words, such as monster, villain, terrorist, and
murderer, and 40 stereotypically kind agent words,
such as hero, angel, benefactor, and rescuer. We
searched the Google Web 1T N-gram corpus to
identify verbs that co-occur with these words as
probable agents. For each agent term, we applied
the pattern ?* by [a,an,the] AGENT? and extracted
the matching N-grams. Then we applied a part-of-
speech tagger to each N-gram and saved the words
that were tagged as verbs (i.e., the words in the *
position).3 This process produced 811 negative (evil
agent) PPVs and 1362 positive (kind agent) PPVs.
4.2.2 PPV Bootstrapping over Conjunctions
Our second approach for acquiring PPVs is based
on an observation from sentiment analysis research
that conjoined adjectives typically have the same po-
larity (e.g. (Hatzivassiloglou and McKeown, 1997)).
3The POS tagging quality is undoubtedly lower than if tag-
ging complete sentences but it seemed reasonable.
81
Our hypothesis was that conjoined verbs often share
the same polarity as well (e.g., ?abducted and
killed? or ?rescued and rehabilitated?). We exploit
this idea inside a bootstrapping algorithm to itera-
tively learn verbs that co-occur in conjunctions.
Bootstrapping begins with 10 negative and 10
positive PPV seeds. First, we extracted triples of
the form ?w1 and w2? from the Google Web 1T
N -gram corpus that had frequency ? 100 and were
lower case. We separated each conjunction into
two parts: a primary VERB (?w1?) and a CONTEXT
(?and w2?), and created a copy of the conjunction
with the roles of w1 and w2 reversed. For example,
?rescued and adopted? produces:
VERB=?rescued? CONTEXT=?and adopted?
VERB=?adopted? CONTEXT=?and rescued?
Next, we applied the Basilisk bootstrapping al-
gorithm (Thelen and Riloff, 2002) to learn PPVs.
Basilisk identifies semantically similar words based
on their co-occurrence with seeds in contextual pat-
terns. Basilisk was originally designed for semantic
class induction using lexico-syntactic patterns, but
has also been used to learn subjective and objective
nouns (Riloff et al, 2003).
Basilisk first identifies the pattern contexts that
are most strongly associated with the seed words.
Words that occur in those contexts are labeled as
candidates and scored based on the strength of their
contexts. The top 5 candidates are selected and the
bootstrapping process repeats. Basilisk produces a
lexicon of learned words as well as a ranked list of
pattern contexts. Since we bootstrapped over verb
conjunctions, we also extracted new PPVs from the
contexts. We ran the bootstrapping process to create
a lexicon of 500 words, and we collected verbs from
the top 500 contexts as well.
5 Evaluation
Plot unit analysis of narrative text is enormously
complex ? the idea of creating gold standard plot
unit annotations seemed like a monumental task.
So we began with relatively simple and constrained
texts that seemed appropriate: fables. Fables have
two desirable attributes: (1) they have a small cast
of characters, and (2) they typically revolve around
a moral, which is exemplified by a short and concise
plot. Even so, fables are challenging for NLP due to
anthropomorphic characters, flowery language, and
sometimes archaic vocabulary.
We collected 34 Aesop?s fables from a web site4,
choosing fables that have a true plot (some only con-
tain quotes) and exactly two characters. We divided
them into a development set of 11 stories, a tuning
set of 8 stories, and a test set of 15 stories.
Creating a gold standard was itself a substantial
undertaking, and training non-experts to produce
them did not seem feasible in the short term. So
the authors discussed and iteratively refined manual
annotations for the development and tuning sets un-
til we produced similar results and had a common
understanding of the task. Then two authors inde-
pendently created annotations for the test set, and a
third author adjudicated the differences.
5.1 Evaluation Procedure
For evaluation, we used recall (R), precision (P),
and F-measure (F). In our gold standard, each af-
fect state is annotated with the set of clauses that
could legitimately produce it. In most cases (75%),
we were able to ascribe the existence of a state to
precisely one clause. During evaluation, the system-
produced affect states must be generated from the
correct clause. However, for affect states that could
be ascribed to multiple clauses in a sentence, the
evaluation was done at the sentence level. In this
case, the system-produced affect state must come
from the sentence that contains one of those clauses.
Coreference resolution is far from perfect, so we
created gold standard coreference annotations for
our fables and used them for most of our experi-
ments. This allowed us to evaluate our approach
without coreference mistakes factoring in. In Sec-
tion 5.5, we re-evaluate our final results using auto-
matic coreference resolution.
5.2 Evaluation of Affect States using External
Resources
Our first set of experiments evaluates the quality of
the affect states produced by AESOP using only the
external resources. The top half of Table 1 shows the
results for each resource independently. FrameNet
produced the best results, yielding much higher re-
call than any other resource. The bottom half of Ta-
4www.pacificnet.net/?johnr/aesop/
82
Affect State M (59) + (47) - (37) All (143)
Resource(s) R P F R P F R P F R P F
FrameNet .49 .51 .50 .17 .57 .26 .14 .42 .21 .29 .51 .37
MPQA Lexicon .07 .50 .12 .21 .24 .22 .22 .38 .28 .15 .31 .20
OpinionFinder .42 .40 .41 .00 .00 .00 .03 .17 .05 .18 .35 .24
Semantic Orientation Lexicon .07 .44 .12 .17 .40 .24 .08 .38 .13 .10 .41 .16
Speech Act Verbs .36 .53 .43 .00 .00 .00 .00 .00 .00 .15 .53 .23
FrameNet+MPQA Lexicon .44 .52 .48 .30 .28 .29 .27 .38 .32 .35 .40 .37
FrameNet+OpinionFinder .53 .39 .45 .17 .38 .23 .16 .33 .22 .31 .38 .34
FrameNet+Semantic Orientation Lexicon .49 .51 .50 .26 .36 .30 .22 .42 .29 .34 .45 .39
FrameNet+Speech Act Verbs .51 .48 .49 .17 .57 .26 .14 .42 .21 .30 .49 .37
Table 1: Evaluation results for AESOP using external resources. The # in parentheses is the # of gold affect states.
Affect State M (59) + (47) - (37) All (143)
Resource(s) R P F R P F R P F R P F
- Evil Agent PPVs .07 .50 .12 .21 .40 .28 .46 .46 .46 .22 .44 .29
- Neg Basilisk PPVs .07 .44 .12 .11 .45 .18 .24 .45 .31 .13 .45 .20
- Evil Agent and Neg Basilisk PPVs .05 .43 .09 .21 .38 .27 .46 .40 .43 .21 .39 .27
+ Kind Agent PPVs (?>1) .03 .33 .06 .28 .17 .21 .00 .00 .00 .10 .19 .13
+ Pos Basilisk PPVs .08 .56 .14 .02 .12 .03 .03 1.00 .06 .05 .39 .09
FrameNet+SOLex+EvilAgentPPVs .49 .54 .51 .30 .38 .34 .46 .42 .44 .42 .46 .44
FrameNet+EvilAgentPPVs .49 .54 .51 .28 .45 .35 .46 .46 .46 .41 .49 .45
FrameNet+EvilAgentPPVs+PosBasiliskPPVs .49 .53 .51 .30 .41 .35 .49 .49 .49 .43 .48 .45
Table 2: Evaluation results for AESOP with PPVs. The # in parentheses is the # of gold affect states.
ble 1 shows the results when combining FrameNet
with other resources. In terms of F score, the only
additive benefit came from the Semantic Orientation
Lexicon, which produced a better balance of recall
and precision and an F score gain of +2.
5.3 Evaluation of Affect States using PPVs
Our second set of experiments evaluates the quality
of the automatically generated PPV lexicons. The
top portion of Table 2 shows the results for the neg-
ative PPVs. The PPVs harvested by the Evil Agent
patterns produced the best results, yielding recall
and precision of .46 for negative states. Note that
M and + states are also generated from the negative
PPVs because they are inferred during affect projec-
tion (Section 4.1.4). The polarity of a negative PPV
can also be flipped by negation to produce a + state.
Basilisk?s negative PPVs achieved similar preci-
sion but lower recall. We see no additional recall
and some precision loss when the Evil Agent and
Basilisk PPV lists are combined. The precision drop
is likely due to redundancy, which creates spurious
affect states. If two different words have negative
polarity but refer to the same event, then only one
negative affect state should be generated. But AE-
SOP will generate two affect states, so one will be
spurious.
The middle section of Table 2 shows the results
for the positive PPVs. Both positive PPV lexicons
were of dubious quality, so we tried to extract a high-
quality subset of each list. For the Kind Agent PPVs,
we computed the ratio of the frequency of the verb
with Evil Agents versus Kind Agents and only saved
verbs with an Evil:Kind ratio (?) > 1, which yielded
1203 PPVs. For the positive Basilisk PPVs, we used
only the top 100 lexicon and top 100 context verbs,
which yielded 164 unique verbs. The positive PPVs
did generate several correct affect states (including
a - state when a positive PPV was negated), but also
many spurious states.
The bottom section of Table 2 shows the impact
of the learned PPVs when combined with FrameNet
and the Semantic Orientation Lexicon (SOLex).
Adding the Evil Agent PPVs improved AESOP?s F
score from 39% to 44%, mainly due to a +8 recall
gain. The recall of the - states increased from 22%
to 46% with no loss of precision. Interestingly, if
we remove SOLex and use only FrameNet with our
PPVs, precision increases from 46% to 49% and re-
call only drops by -1. Finally, the last row of Table
83
2 shows that adding Basilisk?s positive PPVs pro-
duces a small recall boost (+2) with a slight drop in
precision (-1).
Evaluating the impact of PPVs on plot unit struc-
tures is an indirect way of assessing their quality be-
cause creating plot units involves many steps. Also,
our test set is small so many verbs will never appear.
To directly measure the quality of our PPVs, we re-
cruited 3 people to manually review them. We devel-
oped annotation guidelines that instructed each an-
notator to judge whether a verb is generally good or
bad for its patient, assuming the patient is animate.
They assigned each verb to one of 6 categories: ?
(not a verb), 2 (always good), 1 (usually good), 0
(neutral, mixed, or requires inanimate patient), -1
(usually bad), -2 (always bad). Each annotator la-
beled 250 words: 50 words randomly sampled from
each of our 4 PPV lexicons5 (Evil Agent PPVs, Kind
Agent PPVs, Positive Basilisk PPVs, and Negative
Basilisk PPVs) plus 50 verbs labeled as neutral in
the MPQA lexicon.
First, we measured agreement based on three
groupings: negative (-2 and -1), neutral (0), or pos-
itive (1 and 2). We computed ? scores to measure
inter-annotator agreement for each pair of annota-
tors.6, but the ? scores were relatively low because
the annotators had trouble distinguishing the posi-
tive cases from the neutral ones. So we re-computed
agreement using two groupings: negative (-2 and -
1) and not-negative (0 through 2), and obtained ?
scores of .69, .71, and .74. We concluded that peo-
ple largely agree on whether a verb is bad for the
patient, but they do not necessarily agree if a verb is
good for the patient. One possible explanation is that
many ?bad? verbs represent physical harm or dan-
ger: these verbs are both plentiful and easy to rec-
ognize. In contrast, ?good? verbs are often more ab-
stract and open to interpretation (e.g., is being ?en-
vied? or ?feared? a good thing?).
We used the labels produced by the two an-
notators with the highest ? score to measure the
accuracy of our PPVs. Both the Evil Agent and
Negative Basilisk PPVs were judged to be 72.5%
accurate, averaged over the judges. The Kind Agent
5The top-ranked Evil/Kind Agent PPV lists (? > 1) which
yields 1203 kind PPVs, and 477 evil PPVs, the top 164 positive
Basilisk verbs, and the 678 (unique) negative Basilisk verbs.
6We discarded words labeled as not a verb.
PPVs were only about 39% accurate, while the
Positive Basilisk PPVs were nearly 50% accurate.
These results are consistent with our impressions
that the negative PPVs are of relatively high quality,
while the positive PPVs are mixed. Some examples
of learned PPVs that were not present in our other
resources are:
- : censor, chase, fire, orphan, paralyze, scare, sue
+ : accommodate, harbor, nurse, obey, respect, value
5.4 Evaluation of Links
We represented each link as a 5-tuple
?src-clause, src-state, tgt-clause, tgt-state, link-type?,
where source/target denotes the direction of the
link, the source/target-states are the affect state type
(+,-,M) and link-type is one of 3 types: actualization
(a), motivation (m), or cross-character (xchar). A
system-produced link is considered correct if all 5
elements of the tuple match the human annotation.
Gold Aff States System Aff States
Links R P F R P F
xchar (56) .79 .85 .82 .18 .43 .25
a (51) .90 .94 .92 .04 .07 .05
m (26) 1.0 .57 .72 .15 .10 .12
Table 3: Link results; parentheses show # of gold links.
The second column of Table 3 shows the perfor-
mance of AESOP when using gold standard affect
states. Our simple heuristics for creating links work
surprisingly well for xchar and a links when given
perfect affect states. However, these heuristics pro-
duce relatively low precision for m links, albeit with
100% recall. This reveals that m links primarily do
connect adjacent states, but we need to be more dis-
criminating when connecting them. The third col-
umn of Table 3 shows the results when using system-
generated affect states. We see that performance is
much lower. This is not particularly surprising, since
AESOP?s F-score is 45%, so over half of the indi-
vidual states are wrong, which means that less than
a quarter of the pairs are correct. From that perspec-
tive, the xchar link performance is reasonable, but
the causal a and m links need improvement.
5.5 Analysis
We performed additional experiments to evaluate
some assumptions and components. First, we cre-
ated a Baseline system that is identical to AESOP
84
except that it does not use the affect projection rules.
Instead, it naively projects every affect state in a
clause onto every character in that clause. The first
two rows of the table below show that AESOP?s pre-
cision is double the Baseline, with nearly the same
recall. This illustrates the importance of the projec-
tion rules for mapping affect states onto characters.
R P F
Baseline .44 .24 .31
AESOP, gold coref .43 .48 .45
AESOP, gold coref, infstates .39 .48 .43
AESOP, auto coref, infstates .24 .56 .34
Our gold standard includes pure inference affect
states that are critical to the plot unit structure but
come from world knowledge outside the story itself.
Of 157 affect states in our test set, 14 were pure in-
ference states. We ignored these states in our previ-
ous experiments because our system has no way to
generate them. The third row of the table shows that
including them lowers recall by -4. Generating pure
inferences is an interesting challenge, but they seem
to be a relatively small part of the problem.
The last row of the table shows AESOP?s perfor-
mance when we use our automated coreference re-
solver (Section 4.1.2) instead of gold standard coref-
erence annotations. We see a -15 recall drop coupled
with a +8 precision gain. We were initially puz-
zled by the precision gain but believe that it is pri-
marily due to the handling of quotations. Our gold
standard includes annotations for characters men-
tioned in quotations, but our automated coreference
resolver ignores quotations. Most fables end with
a moral, which is often a quote that may not men-
tion the plot. Consequently, AESOP generates more
spurious affect states from the quotations when us-
ing the gold standard annotations.
6 Related Work and Conclusions
Our research is the first effort to fully automate
the creation of plot unit structures. Other prelimi-
nary work has begun to look at plot unit modelling
for single character stories (Appling and Riedl,
2009). More generally, our work is related to re-
search in narrative story understanding (e.g., (El-
son and McKeown, 2009)), automatic affect state
analysis (Alm, 2009), and automated learning of
scripts (Schank and Abelson, 1977) and other con-
ceptual knowledge structures (e.g., (Mooney and
DeJong, 1985; Fujiki et al, 2003; Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Kasch
and Oates, 2010)). Our work benefitted from prior
research in creating semantic resources such as
FrameNet (Baker et al, 1998) and sentiment lex-
icons and classifiers (e.g., (Takamura et al, 2005;
Wilson et al, 2005b; Choi et al, 2006)). We showed
that affect projection rules can effectively assign af-
fect states to characters. This task is similar to, but
not the same as, associating opinion words with their
targets or topics (Kim and Hovy, 2006; Stoyanov
and Cardie, 2008). Some aspects of affect state iden-
tification are closely related to Hopper and Thomp-
son?s (1980) theory of transitivity. In particular, their
notions of aspect (has an action completed?), benefit
and harm (how much does an object gain/lose from
an action?) and volition (did the subject make a con-
scious choice to act?).
AESOP produces affect states with an F score of
45%. Identifying positive states appears to be more
difficult than negative or mental states. Our sys-
tem?s biggest shortcoming currently seems to hinge
around identifying plans and goals. This includes
the M affect states that initiate plans, the +/- com-
pletion states, as well as their corresponding links.
We suspect that the relatively low recall on positive
affect states is due to our inability to accurately iden-
tify successful plan completions. Finally, these re-
sults are based on fables; plot unit analysis of other
types of texts will pose additional challenges.
Acknowledgments
The authors gratefully acknowledge the support of
Department of Homeland Security Grant N0014-07-
1-0152, NSF grant IIS-0712764, and the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the U.S. gov-
ernment. Thanks to Peter Jensen, Emily Schlichter,
and Clay Templeton for PPV annotations, Nathan
Gilbert for help with the coreference resolver, and
the anonymous reviewers for many helpful com-
ments.
85
References
Cecilia Ovesdotter Alm. 2009. Affect in Text and Speech.
VDM Verlag Dr. Mller.
D. Scott Appling and Mark O. Riedl. 2009. Representa-
tions for learning to summarize plots. In Proceedings
of the AAAI Spring Symposium on Intelligent Narra-
tive Technologies II.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In In Proceed-
ings of COLING/ACL, pages 86?90.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of the Association for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Association for Compu-
tational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recogni-
tion. In EMNLP ?06: Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 431?439, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
David Elson and Kathleen McKeown. 2009. Extending
and evaluating a platform for story understanding. In
Proceedings of the AAAI 2009 Spring Symposium on
Intelligent Narrative Technologies II.
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In Proceedings of the Eu-
ropean Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathy McKeown. 1997.
Predicting the semantic orientation of adjectives. In
Proceedings of the 35th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 174?181,
Madrid, Spain.
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
56:251299.
Niels Kasch and Tim Oates. 2010. Mining script-like
structures from the web. In NAACL-10 Workshop on
Formalisms and Methodology for Learning by Read-
ing (FAM-LbR).
S. Kim and E. Hovy. 2006. Extracting Opinions, Opin-
ion Holders, and Topics Expressed in Online News
Media Text. In Proceedings of ACL/COLING Work-
shop on Sentiment and Subjectivity in Text.
W. Lehnert, J. Black, and B. Reiser. 1981. Summariz-
ing Narratives. In Proceedings of the Seventh Interna-
tional Joint Conference on Artificial Intelligence.
W. G. Lehnert. 1981. Plot Units and Narrative Summa-
rization. Cognitive Science, 5(4):293?331.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
Raymond Mooney and Gerald DeJong. 1985. Learning
Schemata for Natural Language Processing. In Pro-
ceedings of the Ninth International Joint Conference
on Artificial Intelligence, pages 681?687.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Sub-
jective Nouns using Extraction Pattern Bootstrapping.
In Proceedings of the Seventh Conference on Natural
Language Learning (CoNLL-2003), pages 25?32.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
V. Stoyanov and C. Cardie. 2008. Topic Identification
for Fine-Grained Opinion Analysis. In Conference on
Computational Linguistics (COLING 2008).
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 Inter-
active Demonstrations.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354. Association for Computational Lin-
guistics.
86
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250?261,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Approximate Scalable Bounded Space Sketch for Large Data NLP
Amit Goyal and Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD 20742
{amit,hal}@umiacs.umd.edu
Abstract
We exploit sketch techniques, especially the
Count-Min sketch, a memory, and time effi-
cient framework which approximates the fre-
quency of a word pair in the corpus without
explicitly storing the word pair itself. These
methods use hashing to deal with massive
amounts of streaming text. We apply Count-
Min sketch to approximate word pair counts
and exhibit their effectiveness on three im-
portant NLP tasks. Our experiments demon-
strate that on all of the three tasks, we get
performance comparable to Exact word pair
counts setting and state-of-the-art system. Our
method scales to 49 GB of unzipped web data
using bounded space of 2 billion counters (8
GB memory).
1 Introduction
There is more data available today on the web than
there has ever been and it keeps increasing. Use
of large data in the Natural Language Processing
(NLP) community is not new. Many NLP problems
(Brants et al, 2007; Turney, 2008; Ravichandran et
al., 2005) have benefited from having large amounts
of data. However, processing large amounts of data
is still challenging.
This has motivated NLP community to use com-
modity clusters. For example, Brants et al (2007)
used 1500 machines for a day to compute the rela-
tive frequencies of n-grams from 1.8TB of web data.
In another work, a corpus of roughly 1.6 Terawords
was used by Agirre et al (2009) to compute pair-
wise similarities of the words in the test sets using
the MapReduce infrastructure on 2, 000 cores. How-
ever, the inaccessibility of clusters to an average user
has attracted the NLP community to use streaming,
randomized, and approximate algorithms to handle
large amounts of data (Goyal et al, 2009; Levenberg
et al, 2010; Van Durme and Lall, 2010).
Streaming approaches (Muthukrishnan, 2005)
provide memory and time-efficient framework to
deal with terabytes of data. However, these ap-
proaches are proposed to solve a singe problem.
For example, our earlier work (Goyal et al, 2009)
and Levenberg and Osborne (2009) build approxi-
mate language models and show their effectiveness
in Statistical Machine Translation (SMT). Stream-
based translation models (Levenberg et al, 2010)
has been shown effective to handle large parallel
streaming data for SMT. In Van Durme and Lall
(2009b), a Talbot Osborne Morris Bloom (TOMB)
Counter (Van Durme and Lall, 2009a) was used to
find the top-K verbs ?y? given verb ?x? using the
highest approximate online Pointwise Mutual Infor-
mation (PMI) values.
In this paper, we explore sketch techniques,
especially the Count-Min sketch (Cormode and
Muthukrishnan, 2004) to build a single model to
show its effectiveness on three important NLP tasks:
? Predicting the Semantic Orientation of words
(Turney and Littman, 2003)
? Distributional Approaches for word similarity
(Agirre et al, 2009)
? Unsupervised Dependency Parsing (Cohen and
Smith, 2010) with a little linguistics knowl-
edge.
In all these tasks, we need to compute association
measures like Pointwise Mutual Information (PMI),
250
and Log Likelihood ratio (LLR) between words. To
compute association scores (AS), we need to count
the number of times pair of words appear together
within a certain window size. However, explicitly
storing the counts of all word pairs is both computa-
tionally expensive and memory intensive (Agirre et
al., 2009; Pantel et al, 2009). Moreover, the mem-
ory usage keeps increasing with increase in corpus
size.
We explore Count-Min (CM) sketch to address
the issue of efficient storage of such data. The
CM sketch stores counts of all word pairs within a
bounded space. Storage space saving is achieved
by approximating the frequency of word pairs in
the corpus without explicitly storing the word pairs
themselves. Both updating (adding a new word pair
or increasing the frequency of existing word pair)
and querying (finding the frequency of a given word
pair) are constant time operations making it efficient
online storage data structure for large data. Sketches
are scalable and can easily be implemented in dis-
tributed setting.
We use CM sketch to store counts of word pairs
(except word pairs involving stop words) within a
window of size1 7 over different size corpora. We
store exact counts of words (except stop words) in
hash table (since the number of unique words is
not large that is quadratically less than the num-
ber of unique word pairs). The approximate PMI
and LLR scores are computed using these approxi-
mate counts and are applied to solve our three NLP
tasks. Our experiments demonstrate that on all of
the three tasks, we get performance comparable to
Exact word pair counts setting and state-of-the-art
system. Our method scales to 49 GB of unzipped
web data using bounded space of 2 billion counters
(8 GB memory). This work expands upon our ear-
lier workshop papers (Goyal et al, 2010a; Goyal et
al., 2010b).
2 Sketch Techniques
A sketch is a compact summary data structure to
store the frequencies of all items in the input stream.
Sketching techniques use hashing to map items in
streaming data onto a small sketch vector that can
be updated and queried in constant time. These tech-
17 is chosen from intuition and not tuned.
niques generally process the input stream in one di-
rection, say from left to right, without re-processing
previous input. The main advantage of using these
techniques is that they require a storage which is
sub-linear in size of the input stream. The following
surveys comprehensively review the streaming liter-
ature: (Rusu and Dobra, 2007; Cormode and Had-
jieleftheriou, 2008).
There exists an extensive literature on sketch tech-
niques (Charikar et al, 2004; Li et al, 2008; Cor-
mode and Muthukrishnan, 2004; Rusu and Dobra,
2007) in algorithms community for solving many
large scale problems. However, in practice, re-
searchers have preferred Count-Min (CM) sketch
over other sketch techniques in many application ar-
eas, such as Security (Schechter et al, 2010), Ma-
chine Learning (Shi et al, 2009; Aggarwal and Yu,
2010), and Privacy (Dwork et al, 2010). This moti-
vated us to explore CM sketch to solve three impor-
tant NLP problems.2
2.1 Count-Min Sketch
The Count-Min sketch (Cormode and Muthukrish-
nan, 2004) is a compact summary data structure
used to store the frequencies of all items in the in-
put stream. The sketch allows fundamental queries
on the data stream such as point, range and inner
product queries to be approximately answered very
quickly. It can also be applied to solve the finding
frequent items problem (Manku and Motwani, 2002)
in a data stream. In this paper, we are only interested
in point queries. The aim of a point query is to es-
timate the count of an item in the input stream. For
other details, the reader is referred to (Cormode and
Muthukrishnan, 2004).
Given an input stream of word pairs of length N
and user chosen parameters ? and , the algorithm
stores the frequencies of all the word pairs with the
following guarantees:
? All reported frequencies are within the true fre-
quencies by at most N with a probability of at
least 1-?.
? The space used by the algorithm is O(1 log 1? ).
2In future, in another line of research, we will explore com-
paring different sketch techniques for NLP problems.
251
? Constant time of O(log(1? )) per each update andquery operation.
2.1.1 CM Data Structure
A Count-Min sketch (CM) with parameters (,?)
is represented by a two-dimensional array with
width w and depth d :
?
??
sketch[1, 1] ? ? ? sketch[1, w]
... . . . ...
sketch[d, 1] ? ? ? sketch[d,w]
?
??
Among the user chosen parameters,  controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within the accepted error. These val-
ues of  and ? determine the width and depth of the
two-dimensional array respectively. To achieve the
guarantees mentioned in the previous section, we
set w=2 and d=log(1? ). The depth d denotes thenumber of pairwise-independent hash functions em-
ployed by the algorithm and there exists a one-to-
one correspondence between the rows and the set
of hash functions. Each of these hash functions
hk:{x1 . . . xN} ? {1 . . . w}, 1 ? k ? d, takes a
word pair from the input stream and maps it into a
counter indexed by the corresponding hash function.
For example, h2(x) = 10 indicates that the word
pair ?x? is mapped to the 10th position in the second
row of the sketch array.
Initialize the entire sketch array with zeros.
Update Procedure: When a new word pair ?x?
with count c arrives, one counter in each row (as de-
cided by its corresponding hash function) is updated
by c.
sketch[k, hk(x)]? sketch[k, hk(x)] + c, ?1 ? k ? d
Query Procedure: Since multiple word pairs can
get hashed to the same position, the frequency stored
by each position is guaranteed to overestimate the
true count. Thus, to answer the point query for a
given word pair, we return minimum over all the po-
sitions indexed by the k hash functions. The answer
to Query(x): c? = mink sketch[k, hk(x)]
Both update and query procedures involve evalu-
ating d hash functions and reading of all the values
in those indices and hence both these procedures are
linear in the number of hash functions. Hence both
these steps require O(log(1? )) time. In our experi-ments (see Section 3.1), we found that a small num-
ber of hash functions are sufficient and we use d=5.
Hence, the update and query operations take only a
constant time. The space used by the algorithm is
the size of the array i.e. wd counters, where w is the
width of each row.
2.1.2 Properties
Apart from the advantages of being space ef-
ficient, and having constant update and constant
querying time, the Count-Min sketch has also other
advantages that makes it an attractive choice for
NLP applications.
? Linearity: Given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches in O(1 log 1? )time which is independent of the stream size.
? The linearity is especially attractive because it
allows the individual sketches to be computed
independent of each other, which means that it
is easy to implement it in distributed setting,
where each machine computes the sketch over
a sub set of corpus.
2.2 Conservative Update
Estan and Varghese introduced the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of computer networking. This can easily be used
with CM sketch to further improve the estimate of a
point query. To update a word pair ?x? with fre-
quency c, we first compute the frequency c? of this
word pair from the existing data structure and the
counts are updated according to:
c? = mink sketch[k, hk(x)], ?1 ? k ? d
sketch[k, hk(x)]? max{sketch[k, hk(x)], c?+ c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. Though this is a heuristic, it avoids
the unnecessary updates of counter values and thus
reduces the error.
In our experiments, we found that employing the
conservative update reduces the Average Relative
252
Error (ARE) of these counts approximately by a fac-
tor of 1.5. (see Section 3.1). But unfortunately,
this update can only be maintained over individual
sketches in distributed setting.
3 Intrinsic Evaluations
To show the effectiveness of the CM sketch and CM
sketch with conservative update (CU) in the context
of NLP, we perform intrinsic evaluations. First, the
intrinsic evaluations are designed to measure the er-
ror in the approximate counts returned by CM sketch
compared to their true counts. Second, we compare
the word pairs association rankings obtained using
PMI and LLR with sketch and exact counts.
It is memory and time intensive to perform many
intrinsic evaluations on large data (Ravichandran et
al., 2005; Brants et al, 2007; Goyal et al, 2009).
Hence, we use a subset of corpus of 2 million sen-
tences (Subset) from Gigaword (Graff, 2003) for it.
We generate words and word pairs over a window
of size 7. We store exact counts of words (except
stop words) in a hash table and store approximate
counts of word pairs (except word pairs involving
stop words) in the sketch.
3.1 Evaluating approximate sketch counts
To evaluate the amount of over-estimation error (see
Section 2.1) in CM and CU counts compared to the
true counts, we first group all word pairs with the
same true frequency into a single bucket. We then
compute the average relative error in each of these
buckets. Since low-frequency word pairs are more
prone to errors, making this distinction based on fre-
quency lets us understand the regions in which the
algorithm is over-estimating. Moreover, to focus on
errors on low frequency counts, we have only plot-
ted word pairs with count at most 100. Average Rel-
ative error (ARE) is defined as the average of abso-
lute difference between the predicted and the exact
value divided by the exact value over all the word
pairs in each bucket.
ARE = 1N
N?
i=1
|Exacti ? Predictedi|
Exacti
Where Exact and Predicted denotes values of ex-
act and CM/CU counts respectively; N denotes the
number of word pairs with same counts in a bucket.
In Fig. 1(a), we fixed the number of counters to 20
million (20M ) with four bytes of memory per each
counter (thus it only requires 80 MB of main mem-
ory). Keeping the total number of counters fixed,
we try different values of depth (2, 3, 5 and 7) of the
sketch array and in each case the width is set to 20Md .The ARE curves in each case are shown in Fig. 1(a).
We can make three main observations from Figure
1(a): First it shows that most of the errors occur on
low frequency word pairs. For frequent word pairs,
in almost all the different runs the ARE is close to
zero. Secondly, it shows that ARE is significantly
lower (by a factor of 1.5) for the runs which use
conservative update (CUx run) compared to the runs
that use direct CM sketch (CMx run). The encourag-
ing observation is that, this holds true for almost all
different (width,depth) settings. Thirdly, in our ex-
periments, it shows that using depth of 3 gets com-
paratively less ARE compared to other settings.
To be more certain about this behavior with re-
spect to different settings of width and depth, we
tried another setting by increasing the number of
counters to 50 million. The curves in 1(b) follow a
pattern which is similar to the previous setting. Low
frequency word pairs are more prone to error com-
pared to the frequent ones and employing conserva-
tive update reduces the ARE by a factor of 1.5. In
this setting, depth 5 does slightly better than depth 3
and gets lowest ARE.
We use CU counts and depth of 5 for the rest of
the paper. As 3 and 5 have lowest ARE in different
settings and using 5 hash functions, we get ? = 0.01
(d = log(1? ) refer Section 2.1) that is probability offailure is 1 in 100, making the algorithm more robust
to false positives compared with 3 hash functions,
? = 0.1 with probability of failure 1 in 10.
Fig. 1(c) studies the effect of the number of coun-
ters in the sketch (the size of the two-dimensional
sketch array) on the ARE with fixed depth 5. As ex-
pected, using more number of counters decreases the
ARE in the counts. This is intuitive because, as the
length of each row in the sketch increases, the prob-
ability of collision decreases and hence the array is
more likely to contain true counts. By using 100
million counters, which is comparable to the length
of the stream 88 million, we are able to achieve al-
most zero ARE over all the counts including the rare
253
100 101 102
0
0.5
1
1.5
2
2.5
3
True frequency counts of word pairs (log scale)
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM?7
CM?5
CM?3
CM?2
CU?7
CU?5
CU?3
CU?2
(a) 20M counters
100 101 1020
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
True frequency counts of word pairs (log scale)
Ave
rag
e R
ela
tive
 Er
ror
 
 CM?7CM?5CM?3CM?2CU?7CU?5CU?3CU?2
(b) 50M counters
100 101 102
0
1
2
3
4
5
True frequency counts of word pairs (log scale)
Av
era
ge
 R
ela
tiv
e E
rro
r
 
 
10M
20M
50M
100M
(c) Different size models with depth 5
Figure 1: Compare 20 and 50 million counter models with different (width,depth) settings. The notation CMx represents the
Count Min sketch with a depth of ?x? and CUx represents the CM sketch along with conservative update and depth ?x?.
ones3. Note that the space we save by not storing the
exact counts is almost four times the memory that
we use here because on an average each word pair
is twelve characters long and requires twelve bytes
(thrice the size of an integer) and 4 bytes for storing
the integer count. Note, we get even bigger space
savings if we work with longer phrases (phrase clus-
tering), phrase pairs (paraphrasing/translation), and
varying length n-grams (Information Extraction).
3.2 Evaluating word pairs association ranking
In this experiment, we compare the word pairs asso-
ciation rankings obtained using PMI and LLR with
CU and exact word pair counts. We use two kinds of
measures, namely recall and Spearman?s correlation
to measure the overlap in the rankings obtained by
exact and CU counts. Intuitively, recall captures the
number of word pairs that are found in both the sets
and then Spearman?s correlation captures if the rela-
tive order of these common word pairs is preserved
in both the rankings. In our experimental setup, if
the rankings match exactly, then we get a recall (R)
of 100% and a correlation (?) of 1.
The results with respect to different sized counter
(20 million (20M ), 50 million (50M )) models are
shown in Table 1. If we compare the second and
third column of the table using PMI and LLR for
20M counters, we get exact rankings for LLR com-
pared to PMI while comparing TopK word pairs.
The explanation for such a behavior is: since we are
3Even with other datasets we found that using counters lin-
ear in the size of the stream leads to ARE close to zero ? counts.
# Cs 20M 50M
AS PMI LLR PMI LLR
TopK R ? R ? R ? R ?
50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
100 .98 .94 1.0 1.0 1.0 1.0 1.0 1.0
500 .80 .98 1.0 1.0 .98 1.0 1.0 1.0
1000 .56 .99 1.0 1.0 .96 .99 1.0 1.0
5000 .35 .90 1.0 1.0 .85 .99 1.0 1.0
10000 .38 .55 1.0 1.0 .81 .95 1.0 1.0
Table 1: Evaluating the PMI and LLR rankings obtained using
CM sketch with conservative update (CU) and Exact counts
not throwing away any infrequent word pairs, PMI
will rank pairs with low frequency counts higher
(Church and Hanks, 1989). Hence, we are evaluat-
ing the PMI values for rare word pairs and we need
counters linear in size of stream to get alost perfect
ranking. This is also evident from the fourth column
for 50M of the Table 1, where CU PMI ranking gets
close to the optimal as the number of counters ap-
proaches stream size.
However, in some NLP problems, we are not in-
terested in low-frequency items. In such cases, even
using space less than linear in number of counters
would suffice. In our extrinsic evaluations, we show
that using space less than the length of the stream
does not degrade the performance.
4 Extrinsic Evaluations
4.1 Data
Gigaword corpus (Graff, 2003) and a 50% portion
of a copy of web crawled by (Ravichandran et al,
254
2005) are used to compute counts of words and word
pairs. For both the corpora, we split the text into
sentences, tokenize and convert into lower-case. We
generate words and word pairs over a window of size
7. We use four different sized corpora: SubSet (used
for intrinsic evaluations in Section 3), Gigaword
(GW), GigaWord + 20% of web data (GWB20), and
GigaWord + 50% of web data (GWB50). Corpus
Statistics are shown below. We store exact counts of
words in a hash table and store approximate counts
of word pairs in the sketch. Hence, the stream size
in our case is the total number of word pairs in a
corpus.
Corpus Subset GW GWB20 GWB50
Unzipped .32 9.8 22.8 49Size (GB)
# of sentences 2.00 56.78 191.28 462.60(Million)
Stream Size .088 2.67 6.05 13.20(Billion)
4.2 Semantic Orientation
Given a word, the task of finding the Semantic Ori-
entation (SO) (Turney and Littman, 2003) of the
word is to identify if the word is more likely to be
used in positive or negative sense. We use a similar
framework as used by the authors to infer the SO.
We take the seven positive words (good, nice, excel-
lent, positive, fortunate, correct, and superior) and
the seven negative words (bad, nasty, poor, negative,
unfortunate, wrong, and inferior) used in (Turney
and Littman, 2003) work. The SO of a given word
is calculated based on the strength of its association
with the seven positive words, and the strength of
its association with the seven negative words. We
compute the SO of a word ?w? as follows:
SO-AS(W) =
?
p?Pwords
AS(p, w)?
?
n?Nwords
AS(n,w)
Where, Pwords and Nwords denote the seven pos-
itive and negative prototype words respectively. We
use PMI and LLR to compute association scores
(AS). If this score is positive, we predict the word
as positive. Otherwise, we predict it as negative.
We use the General Inquirer lexicon4 (Stone et
al., 1966) as a benchmark to evaluate the semantic
4The General Inquirer lexicon is freely available at http:
//www.wjh.harvard.edu/?inquirer/
orientation scores similar to (Turney and Littman,
2003) work. Words with multiple senses have multi-
ple entries in the lexicon, we merge these entries for
our experiment. Our test set consists of 1597 posi-
tive and 1980 negative words. Accuracy is used as
an evaluation metric and is defined as the percentage
of number of correctly identified SO words.
0 500M 1B 1.5B 2B60
65
70
75
Model Size
Accu
racy
 
 
CUExact
(a) SO PMI
0 500M 1B 1.5B 2B55
60
65
70
Model Size
Acc
urac
y
 
 
CUExact
(b) SO LLR
Figure 2: Evaluating Semantic Orientation using PMI and LLR
with different number of counters of CU sketch built using Gi-
gaword.
4.2.1 Varying sketch size
We evaluate SO of words using PMI and LLR
on Gigaword (9.8GB). We compare approximate
SO computed using varying sizes of CU sketches:
50 million (50M ), 100M , 200M , 500M , 1 billion
(1B) and 2 billion (2B) counters with Exact SO. To
compute these scores, we count the number of indi-
vidual words w1 and w2 and the pair (w1,w2) within
a window of size 7. Note that computing the exact
counts of all word pairs on these corpora is com-
putationally expensive and memory intensive, so we
consider only those pairs in which one word appears
in the prototype list and the other word appears in
the test set.
First, if we look at the Exact SO using PMI and
LLR in Figure 2(a) and 2(b) respectively, it shows
that using PMI, we get about 6 points higher ac-
curacy than LLR on this task (The 95% statistical
significance boundary for accuracy is about ? 1.5.).
Second, for both PMI and LLR, having more num-
ber of counters improve performance.5 Using 2B
counters, we get the same accuracy as Exact.
5We use maximum of 2B counters (8GB main memory), as
most of the current desktop machines have at most 8GB RAM.
255
4.2.2 Effect of Increasing Corpus Size
We evaluate SO of words on three different sized
corpora (see Section 4.1): GW (9.8GB), GWB20
(22.8GB), and GWB50 (49GB). First, since for this
task using PMI performs better than LLR, so we will
use PMI for this experiment. Second, we will fix
number of counters to 2B (CU-2B) as it performs
the best in Section 4.2.1. Third, we will compare the
CU-2B counter model with the Exact over increas-
ing corpus size.
We can make several observations from the Fig-
ure 3: ? It shows that increasing the amount of data
improves the accuracy of identifying the SO of a
word. We get an absolute increase of 5.5 points in
accuracy, when we add 20% Web data to GigaWord
(GW). Adding 30% more Web data (GWB50), gives
a small increase of 1.3 points in accuracy which is
not even statistically significant. ? Second, CU-2B
performs as good as exact for all corpus sizes. ?
Third, the number of 2B counters (bounded space)
is less than the length of stream for GWB20 (6.05B
), and GWB50 (13.2B). Hence, it shows that using
counters less than the stream length does not degrade
the performance. ? These results are also compara-
ble to Turney?s (2003) state-of-the-art work where
they report an accuracy of 82.84%. Note, they use a
billion word corpus which is larger than GWB50.
0 10GB 20GB 30GB 40GB 50GB72
74
76
78
80
82
Corpus Size
Accu
racy
 
 
CU?2BExact
Figure 3: Evaluating Semantic Orientation of words with Ex-
act and CU counts with increase in corpus size
4.3 Distributional Similarity
Distributional similarity is based on the distribu-
tional hypothesis that similar terms appear in simi-
lar contexts (Firth, 1968; Harris, 1954). The context
vector for each term is represented by the strength
of association between the term and each of the lex-
ical, semantic, syntactic, and/or dependency units
that co-occur with it6. We use PMI and LLR to com-
pute association score (AS) between the term and
each of the context to generate the context vector.
Once, we have context vectors for each of the terms,
cosine similarity measure returns distributional sim-
ilarity between terms.
4.3.1 Efficient Distributional Similarity
We propose an efficient approach for computing
distributional similarity between word pairs using
CU sketch. In the first step, we traverse the corpus
and store counts of all words (except stop words) in
hash table and all word pairs (except word pairs in-
volving stop words) in sketch. In the second step,
for a target word ?x?, we consider all words (except
infrequent contexts which appear less than or equal
to 10.) as plausible context (since it is faster than
traversing the whole corpus.), and query the sketch
for vocabulary number of word pairs, and compute
approximate AS between word-context pairs. We
maintain only top K AS scores7 contexts using pri-
ority queue for every target word ?x? and save them
onto the disk. In the third step, we use cosine simi-
larity using these approximate topK context vectors
to compute efficient distributional similarity.
The efficient distributional similarity using
sketches has following advantages:
? It can return semantic similarity between any
word pairs that are stored in the sketch.
? It can return the similarity between word pairs
in time O(K).
? We do not store word pairs explicitly, and use
fixed number of counters, hence the overall
space required is bounded.
? The additive property of sketch (Sec. 2.1.2) en-
ables us to parallelize most of the steps in the
algorithm. Thus it can be easily extended to
very large amounts of text data.
We use two test sets which consist of word pairs,
and their corresponding human rankings. We gen-
erate the word pair rankings using efficient distri-
butional similarity. We report the spearman?s rank
6Here, the context for a target word ?x? is defined as words
appear within a window of size 7.
7For this work, we use K = 1000 which is not tuned.
256
correlation8 coefficient (?) between the human and
distributional similarity rankings. The two test sets
are:
1. WS-353 (Finkelstein et al, 2002) is a set of 353
word pairs.
2. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
0 500M 1B 1.5B 2B0.1
0.15
0.2
0.25
Model Size
Acc
urac
y
 
 
CUExact
(a) Word Similarity PMI
0 500M 1B 1.5B 2B0.4
0.45
0.5
0.55
Model Size
Acc
urac
y
 
 
CUExact
(b) Word Similarity LLR
Figure 4: Evaluating Distributional Similarity between word
pairs on WS-353 test set using PMI and LLR with different
number of counters of CU sketch built using Gigaword data-set.
4.3.2 Varying sketch size
We evaluate efficient distributional similarity be-
tween between word pairs on WS-353 test set us-
ing PMI and LLR association scores on Giga-
word (9.8GB). We compare different sizes of CU
sketch (similar to SO evaluation): 50 million (50M ),
100M , 200M , 500M , 1 billion (1B) and 2 bil-
lion (2B) counters with the Exact word pair counts.
Here again, computing the exact counts of all word-
context pairs on these corpora is time, and memory
intensive, we generate context vectors for only those
words which are present in the test set.
First, if we look at word pair ranking using exact
PMI and LLR across Figures 4(a) and 4(b) respec-
tively, it shows that using LLR, we get better ? of
.55 compared to ? of .25 using PMI on this task (The
95% statistical significance boundary on ? for WS-
353 is about ? .08). The explanation for such a be-
havior is: PMI rank context pairs with low frequency
counts higher (Church and Hanks, 1989) compared
to frequent ones which are favored by LLR. Second,
8To calculate the Spearman correlations values are trans-
formed into ranks (if tied ranks exist, average of ranks is taken),
and we calculate the Pearson correlation on them.
Test Set WS-353 RG-65
Model GW GWB20 GWB50 GW GWB20 GWB50
Agirre .64 .75
Exact .55 .55 .62 .65 .72 .74
CU-2B .53 .58 .62 .66 .72 .74
Table 2: Evaluating word pairs ranking with Exact and
CU counts. Scores are evaluated using ? metric.
for PMI in Fig. 4(a), having more counters does not
improve ?. Third, for LLR in Fig. 4(b), having more
number of counters improve performance and using
2B counters, we get ? close to the Exact.
4.3.3 Effect of Increasing Corpus Size
We evaluate efficient distributional similarity be-
tween word pairs using three different sized cor-
pora: GW (9.8GB), GWB20 (22.8GB), and GWB50
(49GB) on two test sets: WS-353, and RG-65. First,
since for this task using LLR performs better than
PMI, so we will use LLR for this experiment. Sec-
ond, we will fix number of counters to 2B (CU-
2B) as it performs the best in Section 4.2.1. Third,
we will compare the CU-2B counter model with the
Exact over increasing corpus size. We also com-
pare our results against the state-of-the-art results
(Agirre) for distributional similarity (Agirre et al,
2009). We report their results of context window of
size 7.
We can make several observations from the Ta-
ble 2: ? It shows that increasing the amount of
data is not substantially improving the accuracy of
word pair rankings over both the test sets. ? Here
again, CU-2B performs as good as exact for all cor-
pus sizes. ? CU-2B and Exact performs same as the
state-of-the-art system. ? The number of 2B coun-
ters (bounded space) is less than the length of stream
for GWB20 (6.05B ), and GWB50 (13.2B). Hence,
here again it shows that using counters less than the
stream length does not degrade the performance.
5 Dependency Parsing
Recently, maximum spanning tree (MST) algo-
rithms for dependency parsing (McDonald et al,
2005) have shown great promise, primarily in su-
pervised settings. In the MST framework, words in
a sentence form nodes in a graph, and connections
between nodes indicate how ?related? they are. A
maximum spanning tree algorithm constructs a de-
257
pendency parse by linking together ?most similar?
words. Typically the weights on edges in the graph
are parameterized as a linear function of features,
with weight learned by some supervised learning al-
gorithm. In this section, we ask the question: can
word association scores be used to derive syntactic
structures in an unsupervised manner?
A first pass answer is: clearly not. Metrics like
PMI would assign high association scores to rare
word pairs (mostly content words) leading to incor-
rect parses. Metrics like LLR would assign high
association scores to frequent words, also leading
to incorrect parses. However, with a small amount
of linguistic side information (Druck et al, 2009;
Naseem et al, 2010), we see that these issues can
be overcome. In particular, we see that large data
+ a little linguistics > fancy unsupervised learning
algorithms.
5.1 Graph Definition
Our approach is conceptually simple. We construct
a graph over nodes in the sentence with a unique
?root? node. The graph is directed and fully con-
nected, and for any two words in positions i and j,
the weight from word i to word j is defined as:
wij = ?ascasc(wi, wj)??distdist(i?j)+?lingling(ti, tj)
Here, asc(wi, wj) is a association score such as
PMI or LLR computed using approximate counts
from the sketch. Similarly, dist(i ? j) is a simple
parameterized model of distances that favors short
dependencies. We use a simple unnormalized (log)
Laplacian prior of the form dist(i?j) = ?|i?j?1|,
centered around 1 (encouraging short links to the
right). It is negated because we need to convert dis-
tances to similarities.
The final term, ling(ti, tj) asks: according to
some simple linguistic knowledge, how likely is if
that the (gold standard) part of speech tag associated
with word i points at that associated with word j?
For this, we use the same linguistic information
used by (Naseem et al, 2010), which does not
encode direction information. These rules are:
root? { aux, verb }; verb? { noun,
pronoun, adverb, verb }; aux ? {
verb }; noun ? { adj, art, noun,
num }; prep? { noun }; adj ? { adv
len ? 10 len ? 20 all
COHEN-DIRICHLET 45.9 39.4 34.9
COHEN-BEST 59.4 45.9 40.5
ORACLE 75.1 66.6 63.0
BASELINE+LING 42.4 33.8 29.7
BASELINE 33.5 30.4 28.9
CU-2B LLR OPTIMAL 62.4 ? 7.7 51.1 ? 3.2 41.1 ? 1.9
CU-2B PMI OPTIMAL 63.3 ? 7.8 52.0 ? 3.2 41.1 ? 2.0
CU-2B LLR BALANCED 49.1 ? 7.6 43.6 ? 3.3 37.2 ? 1.9
CU-2B PMI BALANCED 49.5 ? 8.0 45.0 ? 3.2 38.3 ? 2.0
CU-2B LLR SEMISUP 55.7 ? 0.0 44.1 ? 0.0 39.4 ? 0.0
CU-2B PMI SEMISUP 56.5 ? 0.0 45.8 ? 0.0 39.9 ? 0.0
Table 3: Comparing CU-2B build on GWB50 + a little lin-
guistics v/s fancy unsupervised learning algorithms.
}. We simply give an additional weight of 1 to any
edge that agrees with one of these linguistic rules.
5.2 Parameter Setting
The remaining issue is setting the interpolation pa-
rameters ? associated with each of these scores.
This is a difficult problem in purely unsupervised
learning. We report results on three settings. First,
the OPTIMAL setting is based on grid search for op-
timal parameters. This is an oracle result based on
grid search over two of the three parameters (hold-
ing the third fixed at 1). In our second approach,
BALANCED, we normalize the three components to
?compete? equally. In particular, we scale and trans-
late all three components to have zero mean and unit
variance, and set the ?s to all be equal to one. Fi-
nally, our third approach, SEMISUP, is based on us-
ing a small amount of labeled data to set the param-
eters. In particular, we use 10 labeled sentences to
select parameters based on the same grid search as
the OPTIMAL setting. Since this relies heavily on
which 10 sentences are used, we repeat this experi-
ment 20 times and report averages.
5.3 Experiments
Our experiments are on a dependency-converted ver-
sion of section 23 of the Penn Treebank using mod-
ified Collins? head finding rules. We measure accu-
racies as directed, unlabeled dependency accuracy.
We separately report results of sentences of length
at most 10, at most 20 and finally of all length. Note
that there is no training or cross-validation: we sim-
ply run our MST parser on test data directly.
The results of the parsing experiments are shown
258
in Table 3. We compare against the following al-
ternative systems. The first, Cohen-Dirichlet and
Cohen-Best, are previously reported state-of-the-art
results for unsupervised Bayesian dependency pars-
ing (Cohen and Smith, 2010). The first is results
using a simple Dirichlet prior; the second is the best
reported results for any system from that paper.
Next, we compare against an ?oracle? system that
uses LLR extracted from the training data for the
Penn Treebank, where the LLR is based on the prob-
ability of observing an edge given two words. This
is not a true oracle in the sense that we might be
able to do better, but it is unlikely. The next two
baseline system are simple right branching base-
line trees. The Baseline system is a purely right-
branching tree. The Baseline+Ling system is one
that is right branching except that it can only create
edges that are compatible with the linguistic rules,
provided a relevant rule exists. For short sentences,
this is competitive with the Dirichlet prior results.
Finally we report variants of our approach using
association scores computed on the GWB50 using
CU sketch with 2 billion counters. We experiment
with two association scores: LLR and PMI. For each
measure, we report results based on the three ap-
proaches described earlier for setting the ? hyper-
parameters. Error bars for our approaches are 95%
confidence intervals based on bootstrap resampling.
The results show that, for this task, PMI seems
slightly better than LLR, across the board. The OP-
TIMAL performance (based on tuning two hyperpa-
rameters) is amazingly strong: clearly beating out
all the baselines, and only about 15 points behind
the ORACLE system. Using the BALANCED ap-
proach causes a degradation of only 3 points from
the OPTIMAL on sentences of all lengths. In general,
the balancing approach seems to be slightly worse
than the semi-supervised approach, except on very
short sentences: for those, it is substantially better.
Overall, though, the results for both Balanced and
Semisup are competitive with state-of-the-art unsu-
pervised learning algorithms.
6 Discussion and Conclusion
The advantage of using sketch in addition to being
memory and time efficient is that it contains counts
for all word pairs and hence can be used to com-
pute association scores like PMI and LLR between
any word pairs. We show that using sketch counts in
our experiments, on the three tasks, we get perfor-
mance comparable to Exact word pair counts setting
and state-of-the-art system. Our method scales to 49
GB of unzipped web data using bounded space of 2
billion counters (8 GB memory). Moreover, the lin-
earity property of the sketch makes it scalable and
usable in distributed setting. Association scores and
counts from sketch can be used for more NLP tasks
like small-space randomized language models, word
sense disambiguation, spelling correction, relation
learning, paraphrasing, and machine translation.
Acknowledgments
The authors gratefully acknowledge the support of
NSF grant IIS-0712764 and Google Research Grant
for Large-Data NLP. Thanks to Suresh Venkatasub-
ramanian and Jagadeesh Jagarlamudi for useful dis-
cussions and the anonymous reviewers for many
helpful comments.
References
Charu C. Aggarwal and Philip S. Yu. 2010. On classi-
fication of high-cardinality data streams. In SDM?10,
pages 802?813.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ?09: Pro-
ceedings of HLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3?15, January.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
259
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ?09, pages 360?
368, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.
Rothblum, and Sergey Yekhanin. 2010. Pan-private
streaming algorithms. In In Proceedings of ICS.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In ACM
Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010a. Sketch tech-
niques for scaling distributional similarity to the web.
In GEMS workshop at ACL, Uppsala, Sweden.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010b. Sketching tech-
niques for Large Scale NLP. In 6th WAC Workshop at
NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Z. Harris. 1954. Distributional structure. Word 10 (23),
pages 146?162.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
EMNLP, August.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 394?402. As-
sociation for Computational Linguistics.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953?960.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In VLDB.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1234?1244.
Association for Computational Linguistics.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ?07. ACM.
Stuart Schechter, Cormac Herley, and Michael Mitzen-
macher. 2010. Popularity is everything: a new
approach to protecting passwords from statistical-
guessing attacks. In Proceedings of the 5th USENIX
conference on Hot topics in security, HotSec?10, pages
1?8, Berkeley, CA, USA. USENIX Association.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. J. Mach. Learn. Res.,
10:2615?2637, December.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orienta-
tion from association. ACM Trans. Inf. Syst., 21:315?
346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
260
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231?235, July.
261
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444?454,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Corpus-Guided Sentence Generation of Natural Images
Yezhou Yang ? and Ching Lik Teo ? and Hal Daume? III and Yiannis Aloimonos
University of Maryland Institute for Advanced Computer Studies
College Park, Maryland 20742, USA
{yzyang, cteo, hal, yiannis}@umiacs.umd.edu
Abstract
We propose a sentence generation strategy
that describes images by predicting the most
likely nouns, verbs, scenes and prepositions
that make up the core sentence structure. The
input are initial noisy estimates of the objects
and scenes detected in the image using state of
the art trained detectors. As predicting actions
from still images directly is unreliable, we use
a language model trained from the English Gi-
gaword corpus to obtain their estimates; to-
gether with probabilities of co-located nouns,
scenes and prepositions. We use these esti-
mates as parameters on a HMM that models
the sentence generation process, with hidden
nodes as sentence components and image de-
tections as the emissions. Experimental re-
sults show that our strategy of combining vi-
sion and language produces readable and de-
scriptive sentences compared to naive strate-
gies that use vision alone.
1 Introduction
What happens when you see a picture? The most
natural thing would be to describe it using words:
using speech or text. This description of an image is
the output of an extremely complex process that in-
volves: 1) perception in the Visual space, 2) ground-
ing to World Knowledge in the Language Space and
3) speech/text production (see Fig. 1). Each of these
components are challenging in their own right and
are still considered open problems in the vision and
linguistics fields. In this paper, we introduce a com-
putational framework that attempts to integrate these
?indicates equal contribution.
Figure 1: The processes involved for describing a scene.
components together. Our hypothesis is based on
the assumption that natural images accurately reflect
common everyday scenarios which are captured in
language. For example, knowing that boats usually
occur over water will enable us to constrain the
possible scenes a boat can occur and exclude highly
unlikely ones ? street, highway. It also en-
ables us to predict likely actions (Verbs) given the
current object detections in the image: detecting a
dog with a person will likely induce walk rather
than swim, jump, fly. Key to our approach is
the use of a large generic corpus such as the English
Gigaword [Graff, 2003] as the semantic grounding
to predict and correct the initial and often noisy vi-
sual detections of an image to produce a reasonable
sentence that succinctly describes the image.
In order to get an idea of the difficulty of this
task, it is important to first define what makes up
444
Figure 2: Illustration of various perceptual challenges for
sentence generation for images. (a) Different images with
semantically the same content. (b) Pose relates ambigu-
ously to actions in real images. See text for details.
a description of an image. Based on our observa-
tions of annotated image data (see Fig. 4), a de-
scriptive sentence for an image must contain at min-
imum: 1) the important objects (Nouns) that partic-
ipate in the image, 2) Some description of the ac-
tions (Verbs) associated with these objects, 3) the
scene where this image was taken and 4) the prepo-
sition that relates the objects to the scene. That is, a
quadruplet of T = {n, v, s, p} (Noun-Verb-Scene-
Preposition) that represents the core sentence struc-
ture. Generating a sentence from this quadruplet is
obviously a simplification from state of the art gen-
eration work, but as we will show in the experimen-
tal results (sec. 4), it is sufficient to describe im-
ages. The key challenge is that detecting objects, ac-
tions and scenes directly from images is often noisy
and unreliable. We illustrate this using example im-
ages from the Pascal-Visual Object Classes (VOC)
2008 challenge [Everingham et al, 2008]. First,
Fig. 2(a) shows the variability of images in their raw
image representations: pixels, edges and local fea-
tures. This makes it difficult for state of the art ob-
ject detectors [Felzenszwalb et al, 2010; Schwartz
et al, 2009] to reliably detect important objects in
the scene: boat, humans and water ? average preci-
sion scores reported in [Felzenszwalb et al, 2010]
manages around 42% for humans and only 11% for
boat over a dataset of almost 5000 images in 20 ob-
ject categories. Yet, these images are semantically
similar in terms of their high level description. Sec-
ond, cognitive studies [Urgesi et al, 2006; Kourtzi,
2004] have proposed that inferring the action from
static images (known as an ?implied action?) is of-
ten achieved by detecting the pose of humans in the
image: the position of the limbs with respect to one
another, under the assumption that a unique pose oc-
curs for a unique action. Clearly, this assumption
is weak as 1) similar actions may be represented by
different poses due to the inherent dynamic nature of
the action itself: e.g. walking a dog and 2) different
actions may have the same pose: e.g. walking a dog
versus running (Fig. 2(b)). The missing component
here is whether the key object (dog) under interac-
tion is considered. Recent works [Yao and Fei-Fei,
2010; Yang et al, 2010] that used poses for recog-
nition of actions achieved 70% and 61% accuracy
respectively under extremely limited testing condi-
tions with only 5-6 action classes each. Finally, state
of the art scene detectors [Oliva and Torralba, 2001;
Torralba et al, 2003] need to have enough represen-
tative training examples of scenes from pre-defined
scene classes for a classification to be successful ?
with a reported average precision of 83.7% tested
over a dataset of 2600 images.
Addressing all these visual challenges is clearly
a formidable task which is beyond the scope of this
paper. Our focus instead is to show that with the
addition of language to ground the noisy initial vi-
sual detections, we are able to improve the qual-
ity of the generated sentence as a faithful descrip-
tion of the image. In particular, we show that it
is possible to avoid predicting actions directly from
images ? which is still unreliable ? and to use the
corpus instead to guide our predictions. Our pro-
posed strategy is also generic, that is, we make no
prior assumptions on the image domain considered.
While other works (sec. 2) depend on strong anno-
tations between images and text to ground their pre-
dictions (and to remove wrong sentences), we show
that a large generic corpus is also able to provide
the same grounding over larger domains of images.
It represents a relatively new style of learning: dis-
tant supervision [Liang et al, 2009; Mann and Mc-
callum, 2007]. Here, we do not require ?labeled?
data containing images and captions but only sep-
arate data from each side. Another contribution is
a computationally feasible way via dynamic pro-
gramming to determine the most likely quadruplet
T ? = {n?, v?, s?, p?} that describes the image for
generating possible sentences.
445
2 Related Work
Recently, several works from the Computer Vision
domain have attempted to use language to aid im-
age scene understanding. [Kojima et al, 2000] used
predefined production rules to describe actions in
videos. [Berg et al, 2004] processed news captions
to discover names associated with faces in the im-
ages, and [Jie et al, 2009] extended this work to as-
sociate poses detected from images with the verbs
in the captions. Both approaches use annotated ex-
amples from a limited news caption corpus to learn
a joint image-text model so that one can annotate
new unknown images with textual information eas-
ily. Neither of these works have been tested on com-
plex everyday images where the large variations of
objects and poses makes it nearly impossible to learn
a more general model. In addition, no attempt was
made to generate a descriptive sentence from the
learned model. The work of [Farhadi et al, 2010] at-
tempts to ?generate? sentences by first learning from
a set of human annotated examples, and produc-
ing the same sentence if both images and sentence
share common properties in terms of their triplets:
(Nouns-Verbs-Scenes). No attempt was made to
generate novel sentences from images beyond what
has been annotated by humans. [Yao et al, 2010]
has recently introduced a framework for parsing im-
ages/videos to textual description that requires sig-
nificant annotated data, a requirement that our pro-
posed approach avoids.
Natural language generation (NLG) is a long-
standing problem. Classic approaches [Traum et al,
2003] are based on three steps: selection, planning
and realization. A common challenge in generation
problems is the question of: what is the input? Re-
cently, approaches for generation have focused on
formal specification inputs, such as the output of the-
orem provers [McKeown, 2009] or databases [Gol-
land et al, 2010]. Most of the effort in those ap-
proaches has focused on selection and realization.
We address a tangential problem that has not re-
ceived much attention in the generation literature:
how to deal with noisy inputs. In our case, the inputs
themselves are often uncertain (due to misrecogni-
tions by object/scene detectors) and the content se-
lection and realization needs to take this uncertainty
into account.
3 Our Approach
Our approach is summarized in Fig. 3. The input is a
test image where we detect objects and scenes using
trained detection algorithms [Felzenszwalb et al,
2010; Torralba et al, 2003]. To keep the framework
computationally tractable, we limit the elements of
the quadruplet (Nouns-Verbs-Scenes-Prepositions)
to come from a finite set of objects N , actions V ,
scenes S and prepositions P classes that are com-
monly encountered. They are summarized in Ta-
ble. 1. In addition, the sentence that is generated
for each image is limited to at most two objects oc-
curring in a unique scene.
Figure 3: Overview of our approach. (a) Detect objects
and scenes from input image. (b) Estimate optimal sen-
tence structure quadruplet T ?. (c) Generating a sentence
from T ?.
Denoting the current test image as I , the initial
visual processing first detects objects n ? N and
scenes s ? S using these detectors to compute
Pr(n|I) and Pr(s|I), the probabilities that object
n and scene s exist under I . From the observation
that an action can often be predicted by its key ob-
jects, Nk = {n1, n2, ? ? ? , ni}, ni ? N that partici-
pate in the action, we use a trained Language model
Lm to estimate Pr(v|Nk). Lm is also used to com-
pute Pr(s|n, v), the predicted scene using the cor-
pus given the object and verb; and Pr(p|s), the pre-
dicted preposition given the scene. This process is
repeated over all n, v, s, p where we used a modi-
fied HMM inference scheme to determine the most
likely quadruplet: T ? = {n?, v?, s?, p?} that makes
up the core sentence structure. Using the contents
and structure of T ?, an appropriate sentence is then
generated that describes the image. In the following
sections, we first introduce the image dataset used
for testing followed by details of how these compo-
nents are derived.
446
Objects n ? N Actions v ? V Scenes s ? S Preps p ? P
?aeroplane? ?bicycle? ?bird?
?boat? ?bottle? ?bus? ?car?
?cat? ?chair? ?cow? ?table?
?dog? ?horse?, ?motorbike?
?person? ?pottedplant?
?sheep? ?sofa? ?train?
?tvmonitor?
?sit? ?stand? ?park?
?ride? ?hold? ?wear?
?pose? ?fly? ?lie? ?lay?
?smile? ?live? ?walk?
?graze? ?drive? ?play?
?eat? ?cover? ?train?
?close? ...
?airport?
?field?
?highway?
?lake? ?room?
?sky? ?street?
?track?
?in? ?at? ?above?
?around? ?behind?
?below? ?beside?
?between?
?before? ?to?
?under? ?on?
Table 1: The set of objects, actions (first 20), scenes and preposition classes considered
Figure 4: Samples of images with corresponding annota-
tions from the UIUC scene description dataset.
3.1 Image Dataset
We use the UIUC Pascal Sentence dataset, first in-
troduced in [Farhadi et al, 2010] and available on-
line1. It contains 1000 images taken from a sub-
set of the Pascal-VOC 2008 challenge image dataset
and are hand annotated with sentences that describe
the image by paid human annotators using Ama-
zon Mechanical Turk. Fig. 4 shows some sample
images with their annotations. There are 5 anno-
tations per image, and each annotation is usually
short ? around 10 words long. We randomly selected
900 images (4500 sentences) as the learning corpus
to construct the verb and scene sets, {V,S} as de-
scribed in sec. 3.3, and kept the remaining 100 im-
ages for testing and evaluation.
3.2 Object and Scene Detections from Images
We use the Pascal-VOC 2008 trained object detec-
tors [Felzenszwalb et al, 2008] of 20 common ev-
eryday object classes that are defined in N . Each of
the detectors are essentially SVM classifiers trained
on a large number of the objects? image represen-
tations from a large variety of sources. Although
20 classes may seem small, their existence in many
1http://vision.cs.uiuc.edu/pascal-sentences/
(a) (b)
Figure 5: (a) [Top] The part based object detector from
[Felzenszwalb et al, 2010]. [Bottom] The graphical
model representation of an object, for e.g. a bike. (b)
Examples of GIST gradients: (left) an outdoor scene vs
(right) an indoor scene [Torralba et al, 2003].
natural images (e.g. humans, cars and plants) makes
them particularly important for our task, since hu-
mans tend to describe these common objects as well.
As object representations, the part-based descriptor
of [Felzenszwalb et al, 2010] is used. This repre-
sentation decomposes any object, e.g. a cow, into
its constituent parts: head, torso, legs, which are
shared by other objects in a hierarchical manner.
At each level, image gradient orientations are com-
puted. The relationship between each parts is mod-
eled probabilistically using graphical models where
parts are the nodes and the edges are the conditional
probabilities that relate their spatial compatibility
(Fig. 5(a)). For example, in a cow, the probability
of finding the torso near the head is higher than find-
ing the legs near the head. This model?s intuition lies
in the assumption that objects can be deformed but
the relative position of each constituent parts should
remain the same. We convert the object detec-
tion scores to probabilities using Platt?s method [Lin
et al, 2007] which is numerically more stable to ob-
tain Pr(n|I). The parameters of Platt?s method are
obtained by estimating the number of positives and
negatives from the UIUC annotated dataset, from
447
which we determine the appropriate probabilistic
threshold, which gives us approximately 50% recall
and precision.
For detecting scenes defined in S , we use the
GIST-based scene descriptor of [Torralba et al,
2003]. GIST computes the windowed 2D Gabor fil-
ter responses of an input image. The responses of
Gabor filters (4 scales and 6 orientations) encode the
texture gradients that describe the local properties
of the image. Averaging out these responses over
larger spatial regions gives us a set of global im-
age properties. These high dimensional responses
are then reprojected to a low dimensional space via
PCA, where the number of principal components are
obtained empirically from training scenes. This rep-
resentation forms the GIST descriptor of an image
(Fig. 5(b)) which is used to train a set of SVM clas-
sifiers for each scene class in S. Again, Pr(s|I) is
computed from the SVM scores using [Lin et al,
2007]. The set of common scenes defined in S is
learned from the UIUC annotated data (sec. 3.3).
3.3 Corpus-Guided Predictions
Figure 6: (a) Selecting the ROOT verb from the depen-
dency parse ride reveals its subject woman and direct
object bicycle. (b) Selecting the head noun (PMOD)
as the scene street reveals ADV as the preposition on
Predicting Verbs: The key component of our ap-
proach is the trained language model Lm that pre-
dicts the most likely verb v, associated with the ob-
jects Nk detected in the image. Since it is possi-
ble that different verbs may be associated with vary-
ing number of object arguments, we limit ourselves
to verbs that take on at most two objects (or more
specifically two noun phrase arguments) as a sim-
plifying assumption: Nk = {n1, n2} where n2 can
be NULL. That is, n1 and n2 are the subject and
direct objects associated with v ? V . Using this as-
sumption, we can construct the set of verbs, V . To
do this, we use human labeled descriptions of the
training images from the UIUC Pascal-VOC dataset
(sec. 3.1) as a learning corpus that allows us to deter-
mine the appropriate target verb set that is amenable
to our problem. We first apply the CLEAR parser
[Choi and Palmer, 2010] to obtain a dependency
parse of these annotations, which also performs
stemming of all the verbs and nouns in the sentence.
Next, we process all the parses to select verbs which
are marked as ROOT and check the existence of a
subject (DEP) and direct object (PMOD, OBJ) that
are linked to the ROOT verb (see Fig. 6(a)). Finally,
after removing common ?stop? verbs such as {is,
are, be} we rank these verbs in terms of their oc-
currences and select the top 50 verbs which accounts
for 87.5% of the sentences in the UIUC dataset to be
in V .
Object class n ? N Synonyms, ?n?
bus autobus charabanc
double-decker jitney
motorbus motorcoach omnibus
passenger-vehicle schoolbus
trolleybus streetcar ...
chair highchair chaise daybed
throne rocker armchair
wheelchair seat ladder-back
lawn-chair fauteuil ...
bicycle bike wheel cycle velocipede
tandem mountain-bike ...
Table 2: Samples of synonyms for 3 object classes.
Next, we need to explain how n1 and n2 are
selected from the 20 object classes defined previ-
ously in N . Just as the 20 object classes are de-
fined visually over several different kinds of spe-
cific objects, we expand n1 and n2 in their tex-
tual descriptions using synonyms. For example,
the object class n1=aeroplane should include
the synonyms {plane, jet, fighter jet,
aircraft}, denoted as ?n1?. To do this, we ex-
pand each object class using their corresponding
WordNet synsets up to at most three hyponymns lev-
els. Example synonyms for some of the classes are
summarized in Table 2.
We can now compute from the Gigaword cor-
pus [Graff, 2003] the probability that a verb ex-
ists given the detected nouns, Pr(v|n1, n2). We do
this by computing the log-likelihood ratio [Dunning,
1993] , ?nvn, of trigrams (?n1? , v, ?n2?), computed
from each sentence in the English Gigaword corpus
[Graff, 2003]. This is done by extracting only the
words in the corpus that are defined inN and V (in-
448
cluding their synonyms). This forms a reduced cor-
pus sequence from which we obtain our target tri-
grams. For example, the sentence:
the large brown dog chases a small young cat
around the messy room, forcing the cat to run
away towards its owner.
will be reduced to the stemmed sequence dog
chase cat cat run owner2 from which we ob-
tain the target trigram relationships: {dog chase
cat}, {cat run owner} as these trigrams re-
spect the (n1, v, n2) ordering. The log-likelihood ra-
tios, ?nvn, computed for all possible (?n1? , v, ?n2?)
are then normalized to obtain Pr(v|n1, n2). An ex-
ample of ranked ?nvn in Fig. 7(a) shows that ?nvn
predicts v that makes sense: with the most likely
predictions near the top of the list.
Predicting Scenes: Just as an action is strongly
related to the objects that participate in it, a
scene can be predicted from the objects and verbs
that occur in the image. For example, detect-
ing Nk={boat, person} with v={row} would
have predicted the scene s={coast}, since boats
usually occur in water regions. To learn this rela-
tionship from the corpus, we use the UIUC dataset
to discover what are the common scenes that should
be included in S. We applied the CLEAR depen-
dency parse [Choi and Palmer, 2010] on the UIUC
data and extracted all the head nouns (PMOD) in
the PP phrases for this purpose and excluded those
nouns with prepositions (marked as ADV) such as
{with, of} which do not co-occur with scenes in
general (see Fig. 6(b)). We then ranked the remain-
ing scenes in terms of their frequency to select the
top 8 scenes used in S.
To improve recall and generalization, we expand
each of the 8 scene classes using their WordNet
synsets ?s? (up to a max of three hyponymns levels).
Similar to the procedure of predicting the verbs de-
scribed above, we compute the log-likelihood ratio
of ordered bigrams, {n, ?s?} and {v, ?s?}: ?ns and
?vs, by reducing the corpus sentence to the target
nouns, verbs and scenes defined inN ,V and S. The
probabilities Pr(s|n) and Pr(v|n) are then obtained
by normalizing ?ns and ?vs. Under the assumption
that the priors Pr(n) and Pr(v) are independent and
applying Bayes rule, we can compute the probabil-
2stemming is done using [Choi and Palmer, 2010]
ity that a scene co-occurs with the object and action,
Pr(s|n, v) by:
Pr(s|n, v) =
Pr(n, v|s)Pr(s)
Pr(n, v)
= Pr(n|s)Pr(v|s)Pr(s)Pr(n)Pr(v)
? Pr(s|n)? Pr(s|v) (1)
where the constant of proportionality is justified un-
der the assumption that Pr(s) is equiprobable for all
s. (1) is computed for all nouns in Nk. As shown
in Fig. 7(b), we are able to predict scenes that co-
locate with reasonable correctness given the nouns
and verbs.
Predicting Prepositions: It is straightforward to
predict the appropriate prepositions associated with
a given scene. When we construct S from the UIUC
annotated data, we simply collect and rank all the as-
sociated prepositions (ADV) in the PP phrase of the
dependency parses. We then select the top 12 prepo-
sitions used to define P . Using P , we then compute
the log-likelihood ratio of ordered bigrams, {p, ?s?}
for prepositions that co-locate with the scene syn-
onyms over the corpus. Normalizing ?ps yields
Pr(p|s), the probability that a preposition co-locates
with a scene. Examples of ranked ?ps are shown in
Fig. 7(c). Again, we see that reasonable predictions
of p can be found.
Figure 7: Example of how ranked log-likelihood values
(in descending order) suggest a possible T : (a) ?nvn for
n1 = person, n2 = bus predicts v = ride. (b) ?ns
and ?vs for n = bus, v = ride then jointly predicts
s = street and finally (c) ?ps with s = street pre-
dicts p = on.
3.4 Determining T ? using HMM inference
Given the computed conditional probabilities:
Pr(n|I) and Pr(s|I) which are observations
from an input test image with the param-
eters of the trained language model, Lm:
449
Pr(v|n1, n2), Pr(s|n, v), Pr(p|s), we seek to
find the most likely sentence structure T ? by:
T ? = argmax
n,v,s,p
Pr(T |n, v, s, p)
= argmax
n,v,s,p
{Pr(n1|I)Pr(n2|I)Pr(s|I)?
Pr(v|n1, n2)Pr(s|n, v)Pr(p|s)} (2)
where the last equality holds by assuming indepen-
dence between the visual detections and corpus pre-
dictions. Obviously a brute force approach to try all
possible combinations to maximize eq. (2) will not
be feasible due to the large number of possible com-
binations: (20?21?8)?(50?20?20)?(8?20?50)?
(12 ? 8) ? 5? 1013. A better solution is needed.
Figure 8: The HMM used for optimizing T . The relevant
transition and emission probabilities are also shown. See
text for more details.
Our proposed strategy is to pose the optimiza-
tion of T as a dynamic programming problem, akin
to a Hidden Markov Model (HMM) where the hid-
den states are related to the (simplified) sentence
structure we seek: T = {n1, n2, s, v, p}, and the
emissions are related to the observed detections:
{n1, n2, s} in the image if they exist. To sim-
plify our notations, as we are concerned with ob-
ject pairs we will write NN as the hidden states for
all n1, n2 pairs and nn as the corresponding emis-
sions (detections); and all object+verb pairs as hid-
den states NV. The hidden states are therefore de-
noted as: {NN,NV,S,P} with values taken from
their respective word classes from Table 1. The
emission states are {nn,s} with binary values: 1
if the detections occur or 0 otherwise. The full
HMM is summarized in Fig. 8. The rationale for
using a HMM is that we can reuse all previous com-
putation of the probabilities at each level to com-
pute the required probabilities at the current level.
From START, we assume all object pair detections
are equiprobable: Pr(NN|START) = 1|N |?(|N |+1)where we have added an additional NULL value for
objects (at most 1). At each NN, the HMM emits
a detection from the image and by independence
we have: Pr(nn|NN) = Pr(n1|I)Pr(n2|I). Af-
ter NN, the HMM transits to the corresponding verb
at state NV with Pr(NV|NN) = Pr(v|n1, n2) ob-
tained from the corpus statistic3. As no action detec-
tions are performed on the image, NV has no emis-
sions. The HMM then transits from NV to S with
Pr(S|NV) = Pr(s|n, v) computed from the corpus
which emits the scene detection score from the im-
age: Pr(s|S) = Pr(s|I). From S, the HMM transits
to P with Pr(P|S) = Pr(p|s) before reaching the
END state.
Comparing the HMM with eq. (2), one can see
that all the corpus and detection probabilities are
accounted for in the transition and emission prob-
abilities respectively. Optimizing T is then equiv-
alent to finding the best (most likely) path through
the HMM given the image observations using the
Viterbi algorithm which can be done in O(105) time
which is significantly faster than the naive approach.
We show in Fig. 9 (right-upper) examples of the top
viterbi paths that produce T ? for four test images.
Note that the proposed HMM is suitable for gen-
erating sentences that contain the core components
defined in T which produces a sentence of the form
NP-VP-PP, which we will show in sec. 4 is suf-
ficient for the task of generating sentences for de-
scribing images. For more complex sentences with
more components: such as adjectives or adverbs, the
HMM can be easily extended with similar computa-
tions derived from the corpus.
3.5 Sentence Generation
Given the selected sentence structure T =
{n1, n2, v, s, p}, we generate sentences using the
3each verb, v, in NV will have 2 entries with the same value,
one for each noun.
450
Figure 9: Four test images (left) and results. (Right-
upper): Sentence structure T ? predicted using Viterbi
and (Right-lower): Generated sentences. Words marked
in red are considered to be incorrect predictions. Com-
plete results are available at http://www.umiacs.umd.
edu/?yzyang/sentence_generateOut.html.
following strategy for each component:
1) We add in appropriate determiners and cardi-
nals: the, an, a, CARD, based on the content
of n1,n2 and s. For e.g., if n1 = n2, we will use
CARD=two, and modify the nouns to be in the plu-
ral form. When several possible choices are avail-
able, a random choice is made that depends on the
object detection scores: the is preferred when we
are confident of the detections while an, a is pre-
ferred otherwise.
2) We predict the most likely preposition inserted
between the verbs and nouns learned from the Giga-
word corpus via Pr(p|v, n) during sentence genera-
tion. For example, our method will pick the prepo-
sition at between verb sit and noun table.
3) The verb v is converted to a form that agrees
with in number with the nouns detected. The
present gerund form is preferred such as eating,
drinking, walking as it conveys that an ac-
tion is being performed in the image.
4) The sentence structure is therefore of the form:
NP-VP-PP with variations when only one object
or multiple detections of the same objects are de-
tected. A special case is when no objects are de-
tected (below the predefined threshold). No verbs
can be predicted as well. In this case, we sim-
ply generate a sentence that describes the scene
only: for e.g. This is a coast, This is
a field. Such sentences account for 20% of the
entire UIUC testing dataset which are scored lower
in our evaluation metrics (sec. 4.1) since they do not
fully describe the image content in terms of the ob-
jects and actions.
Some examples of sentences generated using this
strategy are shown in Fig. 9(right-lower).
4 Experiments
We performed several experiments to evaluate our
proposed approach. The different metrics used for
evaluation and comparison are also presented, fol-
lowed by a discussion of the experimental results.
4.1 Sentence Generation Results
Three experiments are performed to evaluate the ef-
fectiveness of our approach. As a baseline, we sim-
ply generated T ? directly from images without using
the corpus. There are two variants of this baseline
where we seek to determine if listing all objects in
the image is crucial for scene description. Tb1 is a
baseline that uses all possible objects and scene de-
tected: Tb1 = {n1, n2, ? ? ? , nm, s} and our sentence
will be of the form: {Object 1, object 2 and
object 3 are IN the scene.} and we simply
selected IN as the only admissible preposition. For
the second baseline, Tb2, we limit the number of ob-
jects to just any two: Tb2 = {n1, n2, s} and the
sentence generated will be of the form {Object
1 and object 2 are IN the scene}. In the
second experiment, we applied the HMM strategy
described above but made all transition probabilities
equiprobable, removing the effects of the corpus,
and producing a sentence structure which we denote
as T ?eq. The third experiment produces the full T ?
with transition probabilities learned from the corpus.
All experiments were performed on the 100 unseen
testing images from the UIUC dataset and we used
only the most likely (top) sentence generated for all
evaluation.
We use two evaluation metrics as a measure of the
accuracy of the generated sentences: 1) ROUGE-1
[Lin and Hovy, 2003] precision scores and 2) Rel-
evance and Readability of the generated sentences.
ROUGE-1 is a recall based metric that is commonly
used to measure the effectiveness of text summariza-
tion. In this work, the short descriptive sentence of
an image can be viewed as summarizing the image
451
content and ROUGE-1 is able to capture how well
this sentence can describe the image by comparing it
with the human annotated ground truth of the UIUC
dataset. Due to the short sentences generated, we
did not consider other ROUGE metrics (ROUGE-2,
ROUGE-SU4) which captures fluency and is not an
issue here.
Experiment R1,(length) Relevance Readability
Baseline 1, T ?b1 0.35,(8.2) 2.84? 1.40 3.64? 1.20
Baseline 2, T ?b2 0.39,(6.8) 2.14? 1.13 3.94? 0.91
HMM no cor-
pus, T ?eq
0.42,(6.5) 2.44? 1.25 3.88? 1.18
Full HMM, T ? 0.44,(6.9) 2.51? 1.30 4.10? 1.03
Human Anno-
tation
0.68,(10.1) 4.91? 0.29 4.77? 0.42
Table 3: Sentence generation evaluation results with hu-
man gold standard. Human R1 scores are averaged over
the 5 sentences using a leave one out procedure. Values
in bold are the top scores.
A main shortcoming of using ROUGE-1 is that
the generated sentences are compared only to a fi-
nite set of human labeled ground truth which ob-
viously does not capture all possible sentences that
one can generate. In other words, ROUGE-1 does
not take into account the fact that sentence genera-
tion is innately a creative process, and a better re-
call metric will be to ask humans to judge these
sentences. The second evaluation metric: Rele-
vance and Readability is therefore proposed as an
empirical measure of how much the sentence: 1)
conveys the image content (relevance) in terms of
the objects, actions and scene predicted and 2) is
grammatically correct (readability). We engaged the
services of Amazon Mechanical Turks (AMT) to
judge the generated sentences based on a discrete
scale ranging from 1?5 (low relevance/readability
to high relevance/readability). The averaged results
of ROUGE-1, R1 and mean length of the sentences
with the Relevance+Readability scores for all exper-
iments are summarized in Table 3. For comparison,
we also asked the AMTs to judge the ground truth
sentences as well.
4.2 Discussion
The results reported in Table 3 reveals both the
strengths and some shortcomings of the approach
which we will briefly discuss here. Firstly, the R1
scores indicate that based on a purely summariza-
tion (unigram-overlap) point of view, the proposed
approach of using the HMM to predict T ? achieves
the best results compared to all other approaches
with R1 = 0.44. This means that our sentences are
the closest in agreement with the human annotated
ground truth, correctly predicting the sentence struc-
ture components. In addition sentences generated by
T ? are also succinct: with an average length of 6.9
words per sentence. However, we are still some way
off the human gold standard since we do not predict
other parts-of-speech such as adjectives and adverbs.
Given this fact, our proposed approach performance
is comparable to other state of the art summarization
work in the literature [Bonnie and Dorr, 2004].
Next, we consider the Relevance+Readability
metrics based on human judges. Interestingly, the
first baseline, T ?b1 is considered the most relevant de-
scription of the image and the least readable at the
same time. This is most likely due to the fact that
this recall oriented strategy will almost certainly de-
scribe some objects but the lack of any verb descrip-
tion; and longer sentences that average 8.2 words per
sentence, makes it less readable. It is also possible
that humans tend to penalize less irrelevant objects
compared to missing objects, and further evaluations
are necessary to confirm this. Since T ?b2 is limited
to two objects just like the proposed HMM, it is a
more suitable baseline for comparison. Clearly, the
results show that adding the HMM to predict the op-
timal sentence structure increases the relevance of
the produced sentence. Finally, in terms of read-
ability, T ? generates the most readable sentences,
and this is achieved by leveraging on the corpus to
guide our predictions of the most reasonable nouns,
verbs, scenes and prepositions that agree with the
detections in the image.
5 Future Work
In this work, we have introduced a computationally
feasible framework that integrates visual perception
together with semantic grounding obtained from a
large textual corpus for the purpose of generating a
descriptive sentence of an image. Experimental re-
sults show that our approach produces sentences that
are both relevant and readable. There are, however,
instances where our strategy fails to predict the ap-
452
propriate verbs or nouns (see Fig. 9). This is due
to the fact that object/scene detections can be wrong
and noise from the corpus itself remains a problem.
Compared to human gold standards, therefore, much
work still remains in terms of detecting these objects
and scenes with high precision. Currently, at most
two object classes are used to generate simple sen-
tences which was shown in the results to have penal-
ized the relevance score of our approach. This can
be addressed by designing more complex HMMs to
handle larger numbers of object and verb classes.
Another interesting direction of future work would
be to detect salient objects, learned from training
image+corpus or eye-movement data, and to verify
if these objects aid in improving the descriptive sen-
tences we generate. Another potential application
Figure 10: Images retrieved from 3 verbal search terms:
ride,sit,fly.
of representing images using T ? is that we can eas-
ily sort and retrieve images that are similar in terms
of their semantic content. This would enable us to
retrieve, for example, more relevant images given a
verbal search query such as {ride,sit,fly}, re-
turning images where these verbs are found in T ?.
Some results of retrieved images based on their ver-
bal components are shown in Fig. 10: many images
with dissimilar visual content are correctly classified
based on their semantic meaning.
6 Acknowledgement
This material is based upon work supported by
the National Science Foundation under Grant No.
1035542. In addition, the support of the Eu-
ropean Union under the Cognitive Systems pro-
gram (project POETICON) and the National Sci-
ence Foundation under the Cyberphysical Systems
Program, is gratefully acknowledged.
453
References
Berg, T. L., Berg, A. C., Edwards, J., and Forsyth, D. A.
(2004). Who?s in the picture? In NIPS.
Bonnie, D. Z. and Dorr, B. (2004). Bbn/umd at duc-2004:
Topiary. In In Proceedings of the 2004 Document Un-
derstanding Conference (DUC 2004) at NLT/NAACL
2004, pages 112?119.
Choi, J. D. and Palmer, M. (2010). Robust constituent-
to-dependency conversion for english. In Proceedings
of the 9th International Workshop on Treebanks and
Linguistic Theories, pages 55?66, Tartu, Estonia.
Dunning, T. (1993). Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn,
J., and Zisserman, A. (2008). The PASCAL Visual
Object Classes Challenge 2008 (VOC2008) Results.
Farhadi, A., Hejrati, S. M. M., Sadeghi, M. A., Young, P.,
Rashtchian, C., Hockenmaier, J., and Forsyth, D. A.
(2010). Every picture tells a story: Generating sen-
tences from images. In Daniilidis, K., Maragos, P.,
and Paragios, N., editors, ECCV (4), volume 6314
of Lecture Notes in Computer Science, pages 15?29.
Springer.
Felzenszwalb, P. F., Girshick, R. B., and McAllester, D.
(2008). Discriminatively trained deformable part mod-
els, release 4. http://people.cs.uchicago.edu/ pff/latent-
release4/.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D. A.,
and Ramanan, D. (2010). Object detection with dis-
criminatively trained part-based models. IEEE Trans.
Pattern Anal. Mach. Intell., 32(9):1627?1645.
Golland, D., Liang, P., and Klein, D. (2010). A game-
theoretic approach to generating spatial descriptions.
In Proceedings of EMNLP.
Graff, D. (2003). English gigaword. In Linguistic Data
Consortium, Philadelphia, PA.
Jie, L., Caputo, B., and Ferrari, V. (2009). Who?s do-
ing what: Joint modeling of names and verbs for si-
multaneous face and pose annotation. In NIPS, editor,
Advances in Neural Information Processing Systems,
NIPS. NIPS.
Kojima, A., Izumi, M., Tamura, T., and Fukunaga, K.
(2000). Generating natural language description of hu-
man behavior from video images. In Pattern Recog-
nition, 2000. Proceedings. 15th International Confer-
ence on, volume 4, pages 728 ?731 vol.4.
Kourtzi, Z. (2004). But still, it moves. Trends in Cogni-
tive Sciences, 8(2):47 ? 49.
Liang, P., Jordan, M. I., and Klein, D. (2009). Learning
from measurements in exponential families. In Inter-
national Conference on Machine Learning (ICML).
Lin, C. and Hovy, E. (2003). Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
NAACLHLT.
Lin, H.-T., Lin, C.-J., and Weng, R. C. (2007). A note
on platt?s probabilistic outputs for support vector ma-
chines. Mach. Learn., 68:267?276.
Mann, G. S. and Mccallum, A. (2007). Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In The 24th International Conference on
Machine Learning.
McKeown, K. (2009). Query-focused summarization us-
ing text-to-text generation: When information comes
from multilingual sources. In Proceedings of the 2009
Workshop on Language Generation and Summarisa-
tion (UCNLG+Sum 2009), page 3, Suntec, Singapore.
Association for Computational Linguistics.
Oliva, A. and Torralba, A. (2001). Modeling the shape
of the scene: A holistic representation of the spatial
envelope. International Journal of Computer Vision,
42(3):145?175.
Schwartz, W., Kembhavi, A., Harwood, D., and Davis,
L. (2009). Human detection using partial least squares
analysis. In International Conference on Computer Vi-
sion.
Torralba, A., Murphy, K. P., Freeman, W. T., and Rubin,
M. A. (2003). Context-based vision system for place
and object recognition. In ICCV, pages 273?280. IEEE
Computer Society.
Traum, D., Fleischman, M., and Hovy, E. (2003). Nl gen-
eration for virtual humans in a complex social environ-
ment. In In Proceedings of he AAAI Spring Symposium
on Natural Language Generation in Spoken and Writ-
ten Dialogue, pages 151?158.
Urgesi, C., Moro, V., Candidi, M., and Aglioti, S. M.
(2006). Mapping implied body actions in the human
motor system. J Neurosci, 26(30):7942?9.
Yang, W., Wang, Y., and Mori, G. (2010). Recognizing
human actions from still images with latent poses. In
CVPR.
Yao, B. and Fei-Fei, L. (2010). Grouplet: a structured
image representation for recognizing human and ob-
ject interactions. In The Twenty-Third IEEE Confer-
ence on Computer Vision and Pattern Recognition, San
Francisco, CA.
Yao, B., Yang, X., Lin, L., Lee, M. W., and Zhu, S.-C.
(2010). I2t: Image parsing to text description. Pro-
ceedings of the IEEE, 98(8):1485 ?1508.
454
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930?940,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Improving Bilingual Projections via Sparse Covariance Matrices
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research
Bangalore, India
raghavu@microsoft.com
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Abhijit Bhole
Microsoft Research
Bangalore, India
v-abbhol@microsoft.com
Abstract
Mapping documents into an interlingual rep-
resentation can help bridge the language bar-
rier of cross-lingual corpora. Many existing
approaches are based on word co-occurrences
extracted from aligned training data, repre-
sented as a covariance matrix. In theory, such
a covariance matrix should represent seman-
tic equivalence, and should be highly sparse.
Unfortunately, the presence of noise leads to
dense covariance matrices which in turn leads
to suboptimal document representations. In
this paper, we explore techniques to recover
the desired sparsity in covariance matrices in
two ways. First, we explore word association
measures and bilingual dictionaries to weigh
the word pairs. Later, we explore different
selection strategies to remove the noisy pairs
based on the association scores. Our experi-
mental results on the task of aligning compa-
rable documents shows the efficacy of sparse
covariance matrices on two data sets from two
different language pairs.
1 Introduction
Aligning documents from different languages arises
in a range of tasks such as parallel phrase extrac-
tion (Gale and Church, 1991; Rapp, 1999), mining
translations for out-of-vocabulary words for statis-
tical machine translation (Daume III and Jagarla-
mudi, 2011) and document retrieval (Ballesteros and
Croft, 1996; Munteanu and Marcu, 2005). In this
task, we are given a comparable corpora and some
documents in one language are assumed to have a
comparable document in the other language and the
goal is to recover this hidden alignment. In this pa-
per, we address this problem by mapping the docu-
ments into a common subspace (interlingual repre-
sentation). This common subspace generalizes the
notion of vector space model for cross-lingual ap-
plications (Turney and Pantel, 2010).
Most of the existing approaches use manually
aligned document pairs to find a common subspace
in which the aligned document pairs are maximally
correlated. The sub-space can be found using ei-
ther generative approaches based on topic modeling
(Mimno et al, 2009; Jagarlamudi and Daume? III,
2010; Zhang et al, 2010; Vu et al, 2009) or dis-
criminative approaches based on variants of Princi-
pal Component Analysis (PCA) and Canonical Cor-
relation Analysis (CCA) (Susan T. Dumais, 1996;
Vinokourov et al, 2003; Platt et al, 2010; Haghighi
et al, 2008). Both styles rely on document level
term co-occurrences to find the latent representation.
The discriminative approaches capture essential
word co-occurrences in terms of two monolingual
covariance matrices and a cross-covariance matrix.
Subsequently, they use these covariance matrices to
find projection directions in each language such that
aligned documents lie close to each other (Sec. 2).
The strong reliance of these approaches on the co-
variance matrices leads to problems, especially with
the noisy data caused either by the noisy words
in a document or the noisy document alignments.
Noisy data is not uncommon and is usually the case
with data collected from community based resources
such as Wikipedia. This degrades performance of a
930
variety of tasks, such as transliteration Mining (Kle-
mentiev and Roth, 2006; Hermjakob et al, 2008;
Ravi and Knight, 2009) and multilingual web search
(Gao et al, 2009).
In this paper, we address the problem of identi-
fying and removing noisy entries in the covariance
matrices. We address this problem in two stages.
In the first stage, we explore the use of word asso-
ciation measures such as Mutual Information (MI)
and Yule?s ? (Reis and Judd, 2000) in computing
the strength of a word pair (Sec. 3.1). We also
explore the use of bilingual dictionaries developed
from cleaner resources such as parallel data. In the
second stage, we use the association strengths in fil-
tering out the noisy word pairs from the covariance
matrices. We pose this as a word pair selection prob-
lem and explore multiple strategies (Sec. 3.2).
We evaluate the utility of sparse covariance ma-
trices in improving the bilingual projections incre-
mentally (Sec. 4). We first report results on syn-
thetic multi-view data where the true correspon-
dences between features of different views are avail-
able. Moreover, this also lets us systematically ex-
plore the effect of noise level on the accuracy. Our
experimental results show a significant improvement
when the true correspondences are available. Later,
we report our experimental results on the document
alignment task on Europarl and Wikipedia data sets
and on two language pairs. We found that sparsify-
ing the covariance matrices helps in general, but us-
ing cleaner resource such bilingual dictionaries per-
formed best.
2 Canonical Correlation Analysis (CCA)
In this section, we describe how Canonical Correla-
tion Analysis is used to solve the problem of align-
ing bilingual documents. We mainly focus on repre-
senting the solution of CCA in terms of covariance
matrices. Since most of the existing discriminative
approaches are variants of CCA, showing the advan-
tage of recovering sparseness in CCA makes it appli-
cable to the other variants as well.
Given a training data of n aligned document pairs,
CCA finds projection directions for each language,
so that the documents when projected along these di-
rections are maximally correlated (Hotelling, 1936).
Let X (d1?n) and Y (d2?n) be the representation
of data in both the languages and further assume that
the data is centered (subtract the mean vector from
each document i.e. xi?xi??x and yi ? yi??y).
Then CCA finds projection directions a and b which
maximize:
aTXY Tb?
aTXXTa
?
bTY Y Tb
s.t. aTXXTa = 1 & bTY Y Tb = 1
The projection directions are obtained by solving the
generalized eigen system:
[
0 Cxy
Cyx 0
] [
a
b
]
=
[
(1-?)Cxx+?I 0
0 (1-?)Cyy+?I
] [
a
b
]
(1)
where Cxx = XXT , Cyy = Y Y T are the monolin-
gual covariance matrices, Cxy = XY T is the cross-
covariance matrix and ? is the regularization param-
eter. Using these eigenvectors as columns, we form
the projection matrices A and B. These projection
matrices are used to map documents in both the lan-
guages into interlingual representation.
Given any new pair of documents, their similarity
is computed by first mapping them into the lower di-
mensions space and computing the cosine similarity
between their projections. In general, using all the
eigenvectors is sub optimal and thus retaining top
eigenvectors leads to better generalizability.
3 Covariance Selection
As shown above, the underlying objective function
in most of the discriminative approaches is of the
form aTXY Tb. This can be rewritten as :
aTXY Tb =
n?
k=1
?xk,a??yk,b?
=
n?
k=1
( d1?
i=1
Xi,kai ?
d2?
j=1
Yj,kbj
)
=
d1?
i=1
d2?
j=1
aibj
( n?
k=1
Xi,kYj,k
)
=
d1,d2?
i,j=1
aibjCxyij (2)
Similarly, the constraints can also be rewritten as?d1
i,j=1 aiajCxxij = 1 and
?d2
i,j=1 bibjC
yy
ij = 1.
931
Maximizing this objective function, under the
constraints, involves a careful selection of the vec-
tors a and b such that aibj is high whenever Cxyij
is high. So, every non-zero entry of the cross-
covariance matrix restricts the choice of the pro-
jection directions. While this may not be a severe
problem when the training data is clean, but this is
very uncommon especially in the case of high di-
mensional data like text documents. Moreover, the
inherent ambiguity of natural languages increases
the chances of seeing a noisy word in any docu-
ment. Every occurrence of a noisy word will have a
non-zero contribution towards the covariance matrix
making it dense, which in turn prevents the selection
of appropriate projection directions.
In this section, we describe some techniques to
recover the sparsity by removing the noisy entries
from the covariance matrices. We break this task
into two sub problems: computing an association
score for every word pair and then using an appro-
priate strategy to identify the noisy pairs based on
their weights. We explore multiple ways to address
both the steps in the following two sections. For
the sake of convenience and clarity, we describe our
techniques in the context of cross-covariance ma-
trix between English and Spanish language pair. But
these techniques extend directly to monolingual co-
variance matrices, and to different language pairs as
well.
3.1 Computing Word Pair Association
The first step in filtering out the noisy word co-
occurrences is to use an appropriate measure to com-
pute the strength of word pairs (English and Span-
ish words). This is a well studied problem and sev-
eral association measures have been proposed in the
NLP literature (Dunning, 1993; Inkpen and Hirst,
2002; Moore, 2004). These association measures
can be divided into groups based on the statistics
they use (Hoang et al, 2009). Here we explore a few
of them for sparsifying the cross-covariance matrix.
3.1.1 Covariance
The first option is to use the cross-covariance
matrix itself. As noted above, when the data ma-
trix is centered, the cross-covariance of an English
word (ei) with a Spanish word (fj) is given by?n
k=1 XikYjk. It measures the strength with which
two words co-occur together. This measure uses in-
formation about the occurrence of a word pair in
aligned documents and doesn?t use other statistics
such as ?how often this pair doesn?t co-occur to-
gether? and so on.
3.1.2 Mutual Information
Association measures like covariance and Point-
wise Mutual Information, which only use the fre-
quency with which a word pair co-occurs, often
overestimate the strength of low frequent words
(Moore, 2004). On the other hand, measures
like Log-likelihood ratio (Dunning, 1993) and Mu-
tual Information (MI) use other statistics like the
marginal probabilities of each of the words.
For any two words, ei and fj , let n11, n10, n01
and n00 denote the number of documents in which
both the words co-occur, only English word occurs,
only Spanish word occurs and none of the words oc-
cur. Then the Mutual Information of this word pair
is given by:
MI(ei, fj) =
1
n
?
i,j?{0,1}
nij log
nij ? n
ninj
(3)
where ni and nj denote the number of documents
in which the English and the Spanish word occurs
and n is the total number of documents. We treat
the occurrence of a word in a document slightly dif-
ferent from others, we treat a word as occurring in
a document if it has occurred more than its average
frequency in the corpus. Log-likelihood ratio and
the MI differ only in terms of the constant they use,
so we use only MI in our experiments.
3.1.3 Yule?s ?
Yule?s ? is another popular association measure
used in psychology (Reis and Judd, 2000). It uses
same statistics used by Mutual Information but dif-
fers in the way in which they are combined. MI con-
verts the frequencies into probabilities before com-
puting the association measure where as Yule?s ?
uses the observed frequencies directly, and doesn?t
make any assumptions about the underlying proba-
bility distributions. Given the same interpretation of
the variables as introduced in the previous section,
the Yule?s ? is estimated as:
? =
?n00n11 ?
?n01n10?n00n11 +
?n01n10
(4)
932
This way of combining the frequencies bears simi-
larity with the log-odds ratio.
3.1.4 Bilingual Dictionary
The above three association measures use the
same training data that is available to compute the
covariance matrices in CCA. Thus, their utility in
bringing additional information, which is not cap-
tured by the covariance matrices, is arguable (our
experiments show that they are indeed helpful).
Moreover, they use document level co-occurrence
information which is coarse compared to the co-
occurrence at sentence level or the translational in-
formation provided by a bilingual dictionary. So,
we use bilingual dictionaries as our final resource to
weigh the word co-occurrences. Notice that, using
bilingual information brings in information gleaned
from an external corpus.
We use translation tables learnt using Giza++
(Och and Ney, 2003) on Europarl data set. Since the
translation tables are asymmetric, we combine trans-
lation tables from both the directions. We first use a
threshold on the conditional probability to filter out
the low probability ones and then convert them into
joint probabilities before combining. For each word
pair (ei, fj), we compute the score as:
1
2
(
P (ei|fj)P (fj) + P (fj|ei)P (ei)
)
While the first three association measures can also
be applied to monolingual data, bilingual dictionary
can?t be used for weighting monolingual word pairs.
So in this case, we use either of the above mentioned
techniques for weighting monolingual word pairs.
3.2 Selection Strategies
The next step after computing association measure
for all word pairs is to use them in selecting the pairs
that need to be retained. In this section, we describe
some approaches such as thresholding and matching
for the word pair selection.
3.2.1 Thresholding
A straight forward way to remove the noisy word
co-occurrences is to zero out the entries of the
cross-covariance matrix that are lower than a thresh-
old. To understand the motivation, consider the
rewritten objective function of CCA, aTXY Tb =
?
ij C
xy
ij aibj . This is linear in terms of the individ-
ual components of the cross-covariance matrix. So,
if we want to remove some of the entries of the co-
variance matrix with minimal change in the value of
the objective function, then the optimal choice is to
sort the entries of the covariance matrix and filter out
the less confident word pairs.
3.2.2 Relative Thresholding
While the thresholding strategy described in the
above section is very simple, it is often biased by
the frequent words. Since a frequent word co-occurs
with other words often, it naturally tends to have
high association with most of the other words. As
a result, absolute thresholding tends to remove all
the less frequent word pairs while leaving the co-
occurrences of the frequent words untouched. Even-
tually, this may lead to zeroing out some of the rows
or the columns of the cross-covariance matrix.
To circumvent this, we try thresholding at word
level. For every English word, we choose a few
Spanish words that have high association and vice
versa. Since the nearest neighbour property is asym-
metric, we take the union of all the selected word
pairs. That is, we retain a word pair, if either the
Spanish word is in the top ranked list of the English
word or vice versa.
3.2.3 Maximal Matching
Though relative thresholding overcomes the prob-
lem of zeroing out entire rows or columns posed by
direct thresholding, it is still biased by the frequent
words. The high association measure of a frequent
English word with many Spanish words, makes it a
nearest neighbour for lot of Spanish words. One way
to prevent this is to discourage an already selected
English word from associating with a new Spanish
word. This requires a global knowledge of all the
selected pairs and can not be done by looking at the
individual words, as is the case with the greedy strat-
egy employed by the relative thresholding.
We use matching to solve this problem. We for-
mulate the selection of the word pairs as a network
flow problem (Jagarlamudi et al, 2011). The objec-
tive is to select word pairs that have high association
measure while constraining each word to be asso-
ciated with only a few words from other language.
Let Iij denote an indicator variable taking a value of
933
0 or 1 depending on if the word pair (ei, fj) is se-
lected or not. We want each word to be associated
with k words from other language, i.e.?j Iij = k
and
?
i Iij = k. Moreover, we want word pairs
with high association score to be selected. We can
encode this objective and the constraints as the fol-
lowing optimization problem:
argmax
I
d1,d2?
i,j=1
Cxyij Iij (5)
?i
?
j
Iij = k; ?j
?
i
Iij = k; ?i, j Iij ? {0, 1}
If k = 1, then this problem reduces to a linear as-
signment problem and can be solved optimally us-
ing the Hungarian algorithm (Jonker and Volgenant,
1987). For other values of k, this can be solved by
relaxing the constraint Iij ? {0, 1} to 0 ? Iij ? 1.
The optimal solution of the relaxed problem can be
found efficiently using linear programming (Ravin-
dra et al, 1993). The uni-modular nature of the
constraints guarantees an integral solution (Schri-
jver, 2003), so relaxing the original integer problem
doesn?t introduce any error in the optimal solution.
3.2.4 Monolingual Augmentation
The above three selection strategies operate on the
covariance matrices independently. In this section
we propose to combine them. Specifically, we pro-
pose to augment the set of selected bilingual word
pairs using the monolingual word pairs. We first use
any of the above mentioned strategies to select bilin-
gual and monolingual word pairs. Let Ixy, Ixx and
Iyy be the binary matrices that indicate the selected
word pairs based on the bilingual and monolingual
association scores. Then the monolingual augmen-
tation strategy updates Ixy in the following way:
Ixy ? Binarize(IxxIxyIyy)
i.e., we multiply Ixy with the monolingual selection
matrices and then binarize the resulting matrix. Our
monolingual augmentation is motivated by the fol-
lowing probabilistic interpretation:
P (x, y) =
?
x?,y?
P (x|x?)P (y|y?)P (x?, y?)
which can be rewritten as P ? T xP (T y)T where
T x and T y are monolingual state transition matrices.
3.3 Our Approach
In this section we summarize our approach for the
task of finding aligned documents from a cross-
lingual comparable corpora. The training phase in-
volves finding projection directions for documents
of both the languages. We compute the covariance
matrices using the training data. Then we use any
of the word association measures (Sec. 3.1) along
with a selection criteria (Sec. 3.2) to recover the
sparseness in either only the cross-covariance or all
of the covariance matrices. Let Ixy, Ixx and Iyy
be the binary matrices which represent the word
pairs that are selected based on the chosen sparsi-
fication technique. Now, we replace the covariance
matrices in Eq. 1 as follows: Cxx ? Cxx ? Ixx,
Cyy ? Cyy ? Iyy and Cxy ? Cxy ? Ixy where
? denotes the element-wise matrix product. Subse-
quently, we solve the generalized eigenvalue prob-
lem shown in Eq. 1 to obtain the projection direc-
tions. Let A and B be the matrices formed with top
eigenvectors of Eq. 1 as the columns. These pro-
jection matrices are used to map documents into the
interlingual representation. Such an interlingual rep-
resentation is useful in many tasks like cross-lingual
text categorization (Bel et al, 2003) multilingual
web search (Gao et al, 2009) and so on.
During the testing, given an English document x,
finding an aligned Spanish document involves solv-
ing:
argmax
y
xT
(
(ABT )? Ixy
)
y
?
xT
(
(AAT )? Ixx
)
x
?
yT
(
(BBT )? Iyy
)
y
If the documents are normalized before hand, then
the above equation reduces to computing only the
numerator.
4 Experiments
4.1 Experimental Setup
We experiment with the task of finding aligned doc-
uments from a cross-lingual comparable corpora. In
this task, we are given comparable corpora consist-
ing of two document collections, each in a differ-
ent language. As the corpora are comparable, some
documents in one collection have a comparable doc-
ument in the other collection. The task is to recover
934
this hidden alignment. The recovered alignment is
compared against the ground truth.
We evaluate our idea of sparsifying the covari-
ance matrices incrementally. We first evaluate the
effectiveness of our approach on synthetic data, as
it enables us to systematically study the effect of
noise. Subsequently, we evaluate each of the above
discussed sparsification strategies on real world data
sets. We have discussed four possible ways for
computing word association measure and three ap-
proaches for word pair selection. That leaves us 12
different ways for sparsifying the covariance matri-
ces, with each method having parameters to control
the amount of sparseness. We use a small amount of
development data for model selection and parameter
tuning and choose a few promising models. Finally,
we compare these selected models with state-of-the-
art baselines on two language pairs and on two dif-
ferent data sets.
In each case, we use the training data to learn
the projection directions. And then, for each of the
test documents, we find the aligned document from
other language. We report average accuracy of the
top ranked document and also the Mean Reciprocal
Rank (MRR) of the true aligned document.
4.2 Synthetic Data
We follow the generative story introduced in Bach
and Jordan (2005) to generate synthetic multi-view
data. Their method does not assume any correspon-
dence between the feature dimensions of both the
views. We modify their approach slightly so that
we know the actual correspondence between the fea-
tures. We use these true feature correspondences for
sparsification of the cross-covariance matrix.
We first generate a d dimensional vector in the
common latent space and then use the projection
matrices to map it into the individual feature spaces
as follows:
z ? N (0, Id)
x|z ? (W1z + ?1) + ? N (0, Id1)
y|z ? (W1z + ?2) + ? N (0, Id2)
Notice that we use the same projection matrix W1
for both the views, this ensures a one-to-one corre-
spondence between the features of both the spaces.
Moreover, we also introduce a parameter ? which
controls the amount of noise in the data.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  1.5  2  2.5  3  3.5  4
Sparse MRR
Sparse Accuracy
CCA MRR
CCA Accuracy
Figure 1: Accuracy of CCA and our sparsified version
with the noise parameter.
We generate a total of 3000 pairs of points and use
2000 of them for training the models and the rest
for evaluation. We use the true feature correspon-
dences to form the cross-covariance selection ma-
trix Ixy (Sec. 3.3). For this experiment, we use the
full monolingual covariance matrices. We train both
CCA and our sparse version on the training data and
evaluate them on the test data. We repeat this mul-
tiple times and report the average accuracies. Fig. 1
shows the performance of CCA and our sparse CCA,
as we vary the noise parameter ? from 1 to 4. It
is very clear that the sparse version performs sig-
nificantly better than CCA. As the noise increases,
the performance of CCA drops quickly. This exper-
iment demonstrates a significant performance gain
when the true correspondences are available. But
this information is not available in the case of real
world data sets, so we try to approximate it.
4.3 Model Selection
As we have discussed, there are several choices for
computing the association measure and for selecting
the word pairs to be retained. And each of them have
sparsity parameters, giving raise to many possible
models. For model selection, we use approximately
5000 document pairs collected from the Wikipedia
between English and Spanish. We use the cross-
language links provided as the ground truth. We to-
kenize the documents, retain only the most frequent
2000 words in each language and convert the docu-
935
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 2000  4000  6000  8000  10000  12000  14000  16000  18000  20000
MI+Match
Yule+Match
Cov+Match
MI+RelThreshold
Yule+RelThreshold
Cov+RelThreshold
MI+Threshold
Yule+Threshold
Cov+Threshold
CCA
Figure 2: Comparison of the word association measures
along with different selection criteria. The x-axis plots
the number of non-zero entries in the covariance matrices
and the y-axis plots the accuracy of top-ranked document.
ments into TFIDF vectors. We use 60% of the data
for training different models and the rest for evaluat-
ing the models. We choose a few promising models
based on this development set results and evaluate
them on bigger data sets.
4.3.1 Selection Strategies
In the first experiment, we combine the three
association measures, Covariance (Cov), MI and
Yule?s ?, with the three selection criteria, Thresh-
old, Relative Threshold (RelThreshold) and Match-
ing (Match). Fig. 2 shows the performance of these
different combinations with varying levels of spar-
sity in the covariance matrices. The horizontal line
represents the performance of CCA on this data set.
We start with 2000 non-zero entries in the covari-
ance matrices and experiment up to 20,000 non-zero
entries. Since our data set has 2000 words in each
language, 2000 non-zero entries in a covariance ma-
trix implies that, on an average, every word is as-
sociated with only one word. This results in highly
sparse covariance matrices.
Overall, we observe that reducing the level of
sparsity , i.e. selecting more number of elements in
the covariance matrices, increases the performance
slightly and then decreases again. From the figure, it
seems that sparsifying the covariance matrices might
help in improving the performance of the task. But
it is interesting to note that not all the models per-
form better than CCA. In fact, both the models that
achieve better scores use Matching as the selection
criteria. This suggests that, apart from the weighting
of the word pairs, appropriate selection of the word
pairs is also equally important. In the rest of the ex-
periments we mainly report results with Matching as
the selection criterion. From this figure, we observe
that Mutual Information and Yule?s ? perform com-
petitively but they consistently outperform models
that use covariance as the association measure. So
in the rest of the experiments we report results with
MI or Yule?s ?.
4.3.2 Amount of Sparsity
In the previous experiment, we used same level
of sparsity for all the covariance matrices, i.e. same
number of associations were selected for each word
in all the three covariance matrices. In the following
experiment, we use different levels of sparsity for
the individual covariance matrices. Fig. 3 shows the
performance of Yule+Match and Dictionary+Match
combinations with different levels of sparsity. In
the Yule+Match combination, we use Yule?s ? as-
sociation measure for weighting the word pairs and
use matching for selection. In the Dictionary+Match
combination, we use bilingual dictionary for sparsi-
fying cross-covariance matrix, i.e. we keep all the
word pairs whose conditional translation probabil-
ity is above a threshold. And for monolingual word
pairs, we use MI for weighting and matching for
word pair selection.
For each level of sparsity of the cross-covariance
matrix, we experiment with different levels of spar-
sity on the monolingual covariance matrices. ?Only
XY? indicates we use the full monolingual covari-
ance matrices. In ?Match(k)? runs, we allow each
word to be associated with a total of k words (Eq. 5).
?Aug? indicates that we use monolingual augmen-
tation to refine the sparsity of the cross-covariance
matrix (Sec. 3.2.4).
From both the figures 3(a) and 3(b), we observe
that ?Only XY? run (dark blue) performs poorly
compared to the other runs, indicating that sparsify-
ing all the covariance matrices is better than spar-
sifying only the cross-covariance matrix. In the
936
(a) Performance of Yule+Match combination. The x-axis plots
the number of Spanish words selected per each English word
and vice versa. This determines the sparsity of Cxy. Matching
is used as selection criteria for all the covariance matrices.
(b) Performance of Dictionary+Match combination. The x-axis
plots the threshold on bilingual translation probability and it deter-
mines the sparsity of Cxy. Matching is used to select only the mono-
lingual sparsity.
Figure 3: Comparison of Yule+Match and Dictionary+Match combination with different levels of sparsity for the
covariance matrices. In both the figures, the x-axis plots the sparsity of the cross-covariance matrix and for each
value we try different levels of sparsity on the monolingual covariance matrices (which are grouped together). The
description of these individual runs is provided in the relevant parts of the text. The y-axis plots the accuracy of the
top-ranked document. CCA achieves 61% accuracy on this data set.
Yule+Match combination, Fig. 3(a), all the runs
seem to be performing better when each English
word is allowed to associate with 2 or 3 Spanish
words and vice versa. Among different ways of se-
lecting the monolingual word pairs, Match(2)+Aug
performs better than the remaining runs. So we use
Match(2)+Aug combination for the Yule?s ? mea-
sure.
Unlike the Yule+Match combinations, there is no
clear winner for Dictionary+Match combinations.
First of all, the performance increase as we increase
the translation probability threshold and then de-
creases again (indicated by the ?Average? perfor-
mance in Fig. 3(b)). On an average, all the sys-
tems perform better with a threshold of 0.01, which
we use in our final experiments. In this case, both
Match(1) and Match(2)+Aug runs (orange and green
bars respectively) perform competitively so we use
both of these models in our final experiments.
In both the above experiments, the performance
bars are very similar when we use MI instead of
Yule and vice versa for weighting monolingual word
pairs. Thus, to illustrate the main ideas we chose
Yule?s ? for the former combination and MI for the
latter combination.
4.3.3 Promising Models
Based on the above experiments, we choose the
following combinations for our final experiments.
Yule(l)+Match(k), where l ? {2, 3} is the number
of Spanish words allowed for each English word
and vice versa and k=2 is the number of monolin-
gual word associations for each word. We also run
both these combinations with monolingual augmen-
tation, indicated by Yule(l)+Match(k)+Aug. For
dictionary based weighting, Dictionary+Match(k),
we choose a translation probability threshold of 0.01
and try k ? {1, 2}. Again, we run these combina-
tions with monolingual augmentation identified by
Dictionary+Match(k)+Aug.
4.4 Results
For our final results, we choose data in two language
pairs (English-Spanish and English-German) from
two different resources, Europarl (Koehn, 2005) and
Wikipedia. For Europarl data sets, we artificially
make them comparable by considering the first half
937
Wikipedia Europarl
English-Spanish English-German English-Spanish English-German
Acc. MRR Acc. MRR Acc. MRR Acc. MRR
CCA 0.776 0.852 0.570 0.699 0.872 0.920 0.748 0.831
OPCA 0.781 0.856 0.570 0.700 0.870 0.920 0.748 0.831
Yule(2)+Match(2) 0.798? 0.866? 0.576 0.703 0.901? 0.939? 0.780? 0.853?
Yule(2)+Match(2)+Aug 0.811? 0.876? 0.602? 0.723? 0.883 0.927 0.771? 0.847?
Yule(3)+Match(2) 0.803? 0.870? 0.572 0.700 0.856 0.907 0.747 0.830
Yule(3)+Match(2)+Aug 0.793? 0.861? 0.610? 0.726? 0.878+ 0.925+ 0.763+ 0.843?
Dictionary+Match(1) 0.811? 0.875? 0.656? 0.762? 0.928? 0.957? 0.874? 0.922?
Dictionary+Match(2) 0.811? 0.876? 0.623? 0.736? 0.923? 0.955? 0.853? 0.907?
Dictionary+Match(2)+Aug 0.825? 0.885? 0.630? 0.735? 0.897? 0.935? 0.866? 0.917?
Table 1: Performance of our models in comparison with CCA and OPCA on English-Spanish and English-German
language pairs. ? and + indicate statistical significance measured by paired t-test at p=0.01 and 0.05 levels respectively.
When an improvement is significant at p=0.01 it is automatically significant at p=0.05 and hence is not shown.
of English document and the second half of its
aligned foreign language document (Mimno et al,
2009). For Wikipedia data set, we use the cross-
language link as the ground truth. For each of these
data sets, we choose approximately 5000 aligned
document pairs. We remove the stop words and keep
all the words that occur in at least five documents.
After the preprocessing, on an average, we are left
with 4700 words in each language. Subsequently we
convert the documents into their TFIDF representa-
tion.
In Platt et al (2010), the authors compare differ-
ent systems on the comparable document retrieval
task and show that discriminative approaches work
better compared to their generative counter parts.
So, here we compare only with the state-of-the-
art discriminative systems such as CCA and OPCA
(Platt et al, 2010). For each of the systems, we re-
port the average results of five-fold cross validation.
We divide the data into 3:1:1 ratio for training, vali-
dation and test sets. The validation data set is used to
select the best number of dimensions of the common
sub space. For both CCA and our models, we set the
regularization parameter ? to 0.3 which we found
works well in a relevant but different experiments.
For OPCA, we manually tried different regulariza-
tion parameters ranging from 0.0001 to 1 and found
that a value of 0.001 worked best.
The results are shown in Table 1. On these data
sets, both CCA and OPCA performed competitively.
OPCA takes advantage of the common vocabulary
in both the languages. But in our data sets, vocab-
ulary of both the languages is treated differently, so
it is not surprising that they give almost the same
results. From the results, it is clear that sparsify-
ing the covariance matrices helps improving the ac-
curacies significantly. In all the four data sets, the
best performing method always used dictionary for
cross-lingual sparsity selection. This indicates that
using fine granular information such as a bilingual
dictionary gleaned from an external source is very
helpful in improving the accuracies. Among the
models that rely solely on the training data, models
that use monolingual augmentation performed bet-
ter on Wikipedia data set, while models that do not
use augmentation performed better on Europarl data
sets. This suggests that, when the aligned documents
are clean (closer to being parallel) the statistics com-
puted from cross-lingual corpora are trustworthy. As
the documents become comparable, we need to use
monolingual statistics to refine the bilingual statis-
tics. Moreover, these models achieve higher gains in
the case of Wikipedia data set compared to the gains
in Europarl. This conforms with our initial hunch
that, when the training data is clean the covariance
matrices tend to be less noisy.
5 Discussion
In this paper, we have proposed the idea of sparsi-
fyng covariance matrices to improve bilingual pro-
938
jection directions. We are not aware of any NLP
research that attempts to recover the sparseness of
the covariance matrices to improve the projection
directions. Our work is different from the sparse
CCA (Hardoon and Shawe-Taylor, 2011; Rai and
Daume? III, 2009) proposed in the Machine Learning
literature. Their objective is to find projection di-
rections such that the original documents are repre-
sented as a sparse vectors in the common sub-space.
Another seemingly relevant but different direction
is the sparse covariance matrix selection research
(Banerjee et al, 2005). The objective in this work
is to find matrices such that the inverse of the co-
variance matrix is sparse which has applications in
Gaussian processes.
In this paper, we tried sparsification in the con-
text of CCA only but our technique is general and
can be applied to its variants like OPCA. Our ex-
perimental results show that using external informa-
tion such as bilingual dictionaries which is gleaned
from cleaner resources brings significant improve-
ments. Moreover, we also observe that computing
word pair association measures from the same train-
ing data along with an appropriate selection criteria
can also yield significant improvements. This is cer-
tainly encouraging and in future we would like to
explore more sophisticated techniques to recover the
sparsity based on the training data itself.
6 Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This material is partially supported
by the National Science Foundation under Grant No.
1139909.
References
Francis R. Bach and Michael I. Jordan. 2005. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, Dept Statist Univ California Berke-
ley CA Tech.
Lisa Ballesteros and W. Bruce Croft. 1996. Dictio-
nary methods for cross-lingual information retrieval.
In Proceedings of the 7th International Conference
on Database and Expert Systems Applications, DEXA
?96, pages 791?801, London, UK. Springer-Verlag.
Onureena Banerjee, Alexandre d?Aspremont, and Lau-
rent El Ghaoui. 2005. Sparse covariance selection
via robust maximum likelihood estimation. CoRR,
abs/cs/0506023.
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 407?412, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting on Associ-
ation for Computational Linguistics, pages 177?184,
Morristown, NJ, USA. Association for Computational
Linguistics.
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.
2009. Exploiting bilingual information to improve
web search. In Proceedings of Human Language Tech-
nologies: The 2009 Conference of the Association for
Computational Linguistics, ACL-IJCNLP ?09, pages
1075?1083, Morristown, NJ, USA. ACL.
Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-08:
HLT, pages 771?779, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
David R. Hardoon and John Shawe-Taylor. 2011. Sparse
canonical correlation analysis. Journal of Machine
Learning, 83(3):331?353.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Hung Huu Hoang, Su Nam Kim, and Min-Yen Kan.
2009. A Re-examination of Lexical Association Mea-
sures. In Proceedings of ACL-IJCNLP 2009 Workshop
on Multiword Expressions: Identification, Interpre-
tation, Disambiguation and Applications, Singapore,
August. Association for Computational Linguistics.
H. Hotelling. 1936. Relation between two sets of vari-
ables. Biometrica, 28:322?377.
Diana Zaiu Inkpen and Graeme Hirst. 2002. Ac-
quiring collocations for lexical choice between near-
synonyms. In Proceedings of the ACL-02 workshop
on Unsupervised lexical acquisition - Volume 9, ULA
?02, pages 67?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
939
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Advances in Information Retrieval,
32nd European Conference on IR Research, ECIR,
volume 5993, pages 444?456, Milton Keynes, UK.
Springer.
Jagadeesh Jagarlamudi, Hal Daume III, and Raghavendra
Udupa. 2011. From bilingual dictionaries to interlin-
gual document representations. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 147?152, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 333?340, Barcelona, Spain, July. Association
for Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Comput. Linguist., 31:477?
504, December.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 251?261,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Piyush Rai and Hal Daume? III. 2009. Multi-label pre-
diction via sparse infinite cca. In Advances in Neural
Information Processing Systems, Vancouver, Canada.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 519?526,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37?45, Boulder, Colorado, June. Association for
Computational Linguistics.
K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin
James. 1993. Network Flows: Theory, Algorithms,
and Applications. Prentice-Hall, Inc.
Harry T Reis and Charles M Judd. 2000. Handbook of
Research Methods in Social and Personality Psychol-
ogy. Cambridge University Press.
Alexander Schrijver. 2003. Combinatorial Optimization.
Springer.
Michael L. Littman Susan T. Dumais, Thomas K. Lan-
dauer. 1996. Automatic cross-linguistic information
retrieval using latent semantic indexing. In Working
Notes of the Workshop on Cross-Linguistic Informa-
tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-
land. ACM.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
Advances in Neural Information Processing Systems,
pages 1473?1480, Cambridge, MA. MIT Press.
Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based
method for document alignment in comparable news
corpora. In EACL, pages 843?851.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1128?1137, Up-
psala, Sweden, July. Association for Computational
Linguistics.
940
