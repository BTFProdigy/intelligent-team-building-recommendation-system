Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 857?864,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Event Extraction in a Plot Advice Agent
Harry Halpin
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
H.Halpin@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
J.Moore@ed.ac.uk
Abstract
In this paper we present how the auto-
matic extraction of events from text can
be used to both classify narrative texts ac-
cording to plot quality and produce advice
in an interactive learning environment in-
tended to help students with story writing.
We focus on the story rewriting task, in
which an exemplar story is read to the stu-
dents and the students rewrite the story in
their own words. The system automati-
cally extracts events from the raw text, for-
malized as a sequence of temporally or-
dered predicate-arguments. These events
are given to a machine-learner that pro-
duces a coarse-grained rating of the story.
The results of the machine-learner and the
extracted events are then used to generate
fine-grained advice for the students.
1 Introduction
In this paper we investigate how features of a text
discovered via automatic event extraction can be
used in both natural language understanding and
advice generation in the domain of narrative in-
struction. The background application is a fully
automated plot analysis agent to improve the writ-
ing of students could be used by current nar-
rative tutoring systems (Robertson and Wiemer-
Hastings, 2002). As shown by participatory de-
sign studies, teachers are interested in a plot anal-
ysis agent that can give online natural language
advice and many students enjoy feedback from an
automated agent (Robertson and Cross, 2003). We
use automatic event extraction to create a story-
independent automated agent that can both ana-
lyze the plot of a story and generate appropriate
advice.
1.1 The Story Rewriting Task
A task used in schools is the story rewriting task,
where a story, the exemplar story, is read to the
students, and afterwards the story is rewritten by
each student, providing a corpus of rewritten sto-
ries. This task tests the students ability to both
listen and write, while removing from the student
the cognitive load needed to generate a new plot.
This task is reminiscent of the well-known ?War
of the Ghosts? experiment used in psychology for
studying memory (Bartlett, 1932) and related to
work in fields such as summarization (Lemaire et
al., 2005) and narration (Halpin et al, 2004).
1.2 Agent Design
The goal of the agent is to classify each of the
rewritten stories for overall plot quality. This
rating can be used to give ?coarse-grained? gen-
eral advice. The agent should then provide ?fine-
grained? specific advice to the student on how their
plot could be improved. The agent should be able
to detect if the story should be re-read or a human
teacher summoned to help the student.
To accomplish this task, we extract events that
represent the entities and their actions in the plot
from both the exemplar and the rewritten stories.
A plot comparison algorithm checks for the pres-
ence or absence of events from the exemplar story
in each rewritten story. The results of this algo-
rithm will be used by a machine-learner to clas-
sify each story for overall plot quality and provide
general ?canned? advice to the student. The fea-
tures statistically shared by ?excellent? stories rep-
resent the important events of the exemplar story.
The results of a search for these important events
in a rewritten story provides the input needed by
templates to generate specific advice for a student.
857
2 Corpus
In order to train our agent, we collected a corpus
of 290 stories from primary schools based on two
different exemplar stories. The first is an episode
of ?The Wonderful Adventures of Nils? by Selma
Lagerloff (160 stories) and the second a re-telling
of ?The Treasure Thief? by Herodotus (130 sto-
ries). These will be referred to as the ?Adventure?
and ?Thief? corpora.
2.1 Rating
An experienced teacher, Rater A, designed a rating
scheme equivalent to those used in schools. The
scheme rates the stories as follows:
1. Excellent: An excellent story shows that
the student has ?read beyond the lines? and
demonstrates a deep understanding of the
story, using inference to grasp points that
may not have been explicit in the story. The
student should be able to retrieve all the im-
portant links, and not all the details, but the
right details.
2. Good: A good story shows that the student
understood the story and has ?read between
the lines.? The student recalls the main events
and links in the plot. However, the student
shows no deep understanding of the plot and
does not make use of inference. This can of-
ten be detected by the student leaving out an
important link or emphasizing the wrong de-
tails.
3. Fair: A fair story shows that student has
listened to the story but not understood the
story, and so is only trying to repeat what they
have heard. This is shown by the fact that the
fair story is missing multiple important links
in the story, including a possibly vital part of
the story.
4. Poor: A poor story shows the student has had
trouble listening to the story. The poor story
is missing a substantial amount of the plot,
with characters left out and events confused.
The student has trouble connecting the parts
of the story.
To check the reliability of the rating scheme,
two other teachers (Rater B and Rater C) rated
subsets (82 and 68 respectively) of each of the cor-
pora. While their absolute agreement with Rater A
Class Adventure Thief
1 (Excellent) .231 .146
2 (Good) .300 .377
3 (Fair) .156 .292
4 (Poor) .313 .185
Table 1: Probability Distribution of Ratings
makes the task appear subjective (58% for B and
53% for C), their relative agreement was high, as
almost all disagreements were by one level in the
rating scheme. Therefore we use Cronbach?s ?
and ?b instead of Cohen?s or Fleiss? ? to take into
account the fact that our scale is ordinal. Between
Rater A and B there was a Cronbach?s ? statistic
of .90 and a Kendall?s ?b statistic of .74. Between
Rater B and C there was a Cronbach?s ? statis-
tic of .87 and Kendall?s ?b statistic of .67. These
statistics show the rating scheme to be reliable and
the distribution of plot ratings are given in Table 1.
2.2 Linguistic Issues
One challenge facing this task is the ungrammati-
cal and highly irregular text produced by the stu-
dents. Many stories consist of one long run-on
sentence. This leads a traditional parsing system
with a direct mapping from the parse tree to a se-
mantic representation to fail to achieve a parse on
35% percent of the stories, and as such could not
be used (Bos et al, 2004). The stories exhibit fre-
quent use of reported speech and the switching
from first-person to third-person within a single
sentence. Lastly, the use of incorrect spelling e.g.,
?stalk? for ?stork? appearing in multiple stories
in the corpus, the consistent usage of homonyms
such as ?there? for ?their,? and the invention of
words (?torlix?), all prove to be frequent.
3 Plot Analysis
To automatically rate student writing many tutor-
ing systems use Latent Semantic Analysis, a vari-
ation on the ?bag-of-words? technique that uses
dimensionality reduction (Graesser et al, 2000).
We hypothesize that better results can be achieved
using a ?representational? account that explicitly
represents each event in the plot. These semantic
relationships are important in stories, e.g., ?The
thief jumped on the donkey? being distinctly dif-
ferent from ?The donkey jumped on the thief.?
What characters participate in an action matter,
since ?The king stole the treasure? reveals a major
858
misunderstanding while ?The thief stole the trea-
sure? shows a correct interpretation by the student.
3.1 Stories as Events
We represent a story as a sequence of events,
p1...ph, represented as a list of predicate-
arguments, similar to the event calculus (Mueller,
2003). Our predicate-argument structure is a mini-
mal subset of first-order logic (no quantifiers), and
so is compatible with case-frame and dependency
representations. Every event has a predicate (func-
tion) p that has one or more arguments, n1...na.
In the tradition of Discourse Representation The-
ory (Kamp and Reyle, 1993), our current predi-
cate argument structure could be converted auto-
matically to first order logic by using a default
existential quantification over the predicates and
joining them conjunctively. Predicate names are
often verbs, while their arguments are usually, al-
though not exclusively, nouns or adjectives. When
describing a set of events in the story, a superscript
is used to keep the arguments in an event distinct,
as n25 is argument 2 in event 5. The same argument
name may appear in multiple events. The plot of
any given story is formalized as an event structure
composed of h events in a partial order, with the
partial order denoting their temporal order:
p1(n11, n21, ...na1), ...., ph(n2h, n4h...nch)
An example from the ?Thief? exemplar story is
?The Queen nagged the king to build a treasure
chamber. The king decided to have a treasure
chamber.? This can be represented by an event
structure as:
nag(king, queen)
build(chamber)
decide(king)
have(chamber)
Note due the ungrammatical corpus we cannot at
this time extract neo-Davidsonian events. A sen-
tence maps onto one, multiple, or no events. A
unique name and closed-world assumption is en-
forced, although for purposes of comparing event
we compare membership of argument and predi-
cate names in WordNet synsets in addition to exact
name matches (Fellbaum, 1998).
4 Extracting Events
Paralleling work in summarization, it is hypothe-
sized that the quality of a rewritten story can be
defined by the presence or absence of ?seman-
tic content units? that are crucial details of the
text that may have a variety of syntactic forms
(Nenkova and Passonneau, 2004). We further hy-
pothesize these can be found in chunks of the
text automatically identified by a chunker, and we
can represent these units as predicate-arguments in
our event structure. The event structure of each
story is automatically extracted using an XML-
based pipeline composed of NLP processing mod-
ules, and unlike other story systems, extract full
events instead of filling in a frame of a story script
(Riloff, 1999). Using the latest version of the
Language Technology Text Tokenization Toolkit
(Grover et al, 2000), words are tokenized and sen-
tence boundaries detected. Words are given part-
of-speech tags by a maximum entropy tagger from
the toolkit. We do not attempt to obtain a full parse
of the sentence due to the highly irregular nature
of the sentences. Pronouns are resolved using a
rule-based reimplementation of the CogNIAC al-
gorithm (Baldwin, 1997) and sentences are lem-
matized and chunked using the Cass Chunker (Ab-
ney, 1995). It was felt the chunking method would
be the only feasible way to retrieve portions of the
sentences that may contain complete ?semantic
content units? from the ungrammatical and irregu-
lar text. The application of a series of rules, mainly
mapping verbs to predicate names and nouns to
arguments, to the results of the chunker produces
events from chunks as described in our previous
work (McNeill et al, 2006). The accuracy of our
rule-set was developed by using the grammatical
exemplar stories as a testbed, and a blind judge
found they produced 68% interpretable or ?sen-
sible? events given the ungrammatical text. Stu-
dents usually use the present or past tense exclu-
sively throughout the story and events are usually
presented in order of occurrence. An inspection
of our corpus showed 3% of stories in our corpus
seemed to get the order of events wrong (Hick-
mann, 2003).
4.1 Comparing Stories
Since the student is rewriting the story using their
own words, a certain variance from the plot of the
exemplar story should be expected and even re-
warded. Extra statements that may be true, but
are not explicitly stated in the story, can be in-
ferred by the students. Statements that are true
but are not highly relevant to the course of the
859
plot can likewise be left out. Word similarity
must be taken into account, so that ?The king is
protecting his gold? can be recognized as ?The
pharaoh guarded the treasure.? Characters change
in context, as one character that is described as
the ?younger brother? is from the viewpoint of his
mother ?the younger son.? So, building a model
from the events of two stories and simply check-
ing equivalence can not be used for comparison,
since a wide variety of partial equivalence must be
taken into account.
Instead of using absolute measures of equiva-
lence based on model checking or measures based
on word distribution, we compare each story on
the basis of the presence or absence of events. This
approach takes advantage of WordNet to define
synonym matching and uses the relational struc-
ture of the events to allow partial matching of
predicate functions and arguments. The events
of the exemplar story are assumed to be correct,
and they are searched for in the rewritten story in
the order in which they occur in the exemplar. If
an event is matched (including using WordNet),
then in turn each of the arguments attempts to be
matched.
This algorithm is given more formally in Fig-
ure 1. The complete event structure from the ex-
emplar story, E, and the complete event structure
from the rewritten story R, with each individual
event predicate name labelled as e and r respec-
tively, and their arguments labelled as n in either
Ne and Nr. SYN(x) is the synset of the term x,
including hypernyms and hyponyms except upper
ontology ones. The results of the algorithm are
stored in binary vector F with index i. 1 denotes
an exact match or WordNet synset match, and 0 a
failure to find any match.
4.2 Results
As a baseline system LSA produces a similar-
ity score for each rewritten story by comparing it
to the exemplar, this score is used as a distance
metric for a k-Nearest Neighbor classifier (Deer-
wester et al, 1990). The parameters for LSA were
empirically determined to be a dimensionality of
200 over the semantic space given by the rec-
ommended reading list for American 6th graders
(Landauer and Dumais, 1997). These parameters
resulted in the LSA similarity score having a Pear-
son?s correlation of -.520 with Rater A. k was
found to be optimal at 9.
Algorithm 4.1: PLOTCOMPARE(E,R)
i? 0
f ? ?
for e ? E
do for r ? R
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if e = SYN(r)
then fi ? 1
else fi ? 0
for ne ? Ne
do
?
?
?
?
?
?
?
?
?
?
?
for nr ? Nr
do
?
?
?
?
?
if ne = SYN(nr)
then fi ? 1
else fi ? 0
i = i + 1
Figure 1: Plot Comparison Algorithm
Classifier Corpus Features % Correct
k-NN Adventure LSA 47.5
Naive Bayes Adventure PLOT 55.6
k-NN Thief LSA 41.2
Naive Bayes Thief PLOT 45.4
Table 2: Machine-Learning Results
The results of the plot comparison algorithm
were given as features to machine-learners, with
results produced using ten-fold cross-validation.
A Naive Bayes learner discovers the different sta-
tistical distributions of events for each rating. The
results for both the ?Adventure? and ?Thief? sto-
ries are displayed in Table 2. ?PLOT? means the
results of the Plot Comparison Algorithm were
used as features for the machine-learner while
?LSA? means the similarity scores for Latent Se-
mantic Analysis were used instead. Note that the
same machine-learner could not be used to judge
the effect of LSA and PLOT since LSA scores are
real numbers and PLOT a set of features encoded
as binary vectors.
The results do not seem remarkable at first
glance. However, recall that the human raters had
an average of 56% agreement on story ratings, and
in that light the Naive Bayes learner approaches
the performance of human raters. Surprisingly,
when the LSA score is used as a feature in addition
to the results of the plot comparison algorithm for
the Naive Bayes learners, there is no further im-
provement. This shows features given by the event
860
Class 1 2 3 4
1 (Excellent) 14 22 0 1
2 (Good) 5 36 0 7
3 (Fair) 3 20 0 2
4 (Poor) 0 11 0 39
Table 3: Naive Bayes Confusion Matrix: ?Ad-
venture?
Class Precision Recall
Excellent .64 .38
Good .40 .75
Fair .00 .00
Poor .80 .78
Table 4: Naive Bayes Results: ?Adventure?
structure better characterize plot structure than the
word distribution. Unlike previous work, the use
of both the plot comparison results and LSA did
not improve performance for Naive Bayes, so the
results of using Naive Bayes with both are not re-
ported (Halpin et al, 2004).
The results for the ?Adventure? corpus are in
general better than the results for the ?Thief? cor-
pus. However, this is due to the ?Thief? corpus
being smaller and having an infrequent number of
?Excellent? and ?Poor? stories, as shown in Table
1. In the ?Thief? corpus the learner simply col-
lapses most stories into ?Good,? resulting in very
poor performance. Another factor may be that the
?Thief? story was more complex than the ?Adven-
ture? story, featuring 9 characters over 5 scenes, as
opposed to the ?Adventure? corpus that featured 4
characters over 2 scenes.
For the ?Adventure? corpus, the Naive Bayes
classifier produces the best results, as detailed in
Table 4 and the confusion matrix in Figure 3. A
close inspection of the results shows that in the
?Adventure Corpus? the ?Poor? and ?Good? sto-
ries are classified in general fairly well by the
Naive Bayes learner, while some of the ?Excel-
lent? stories are classified as correctly. A signifi-
cant number of both ?Excellent? and most ?Fair?
stories are classified as ?Good.? The ?Fair? cate-
gory, due to its small size in the training corpus,
has disappeared. No ?Poor? stories are classified
as ?Excellent,? and no ?Excellent? stories are clas-
sified as ?Poor.? The increased difficulty in distin-
guishing ?Excellent? stories from ?Good? stories
is likely due to the use of inference by ?Excellent?
stories, which our system does not use. An inspec-
tion of the rating scale?s wording reveals the sim-
ilarity in wording between the ?Fair? and ?Good?
ratings. This may explain the lack of ?Fair? sto-
ries in the corpus and therefore the inability of
machine-learners to recognize them. As given by
a survey of five teachers experienced in using the
story rewriting task in schools, this level of perfor-
mance is not ideal but acceptable to teachers.
Our technique is also shown to be easily
portable over different domains where a teacher
can annotate around one hundred sample stories
using our scale, although performance seems to
suffer the more complex a story is. Since the Naive
Bayes classifier is fast (able to classify stories in
only a few seconds) and the entire algorithm from
training to advice generation (as detailed below)
is fully automatic once a small training corpus has
been produced, this technique can be used in real-
life tutoring systems and easily ported to other sto-
ries.
5 Automated Advice
The plot analysis agent is not meant to give the
students grades for their stories, but instead use
the automatic ratings as an intermediate step to
produce advice, like other hybrid tutoring systems
(Rose et al, 2002). The advice that the agent can
generate from the automatic rating classification
is limited to coarse-grained general advice. How-
ever, by inspecting the results of the plot com-
parison algorithm, our agent is capable of giving
detailed fine-grained specific advice from the re-
lationships of the events in the story. One tutor-
ing system resembling ours is the WRITE sys-
tem, but we differ from it by using event struc-
ture to represent the information in the system,
instead of using rhetorical features (Burstein et
al., 2003). In this regards it more closely resem-
bles the physics tutoring system WHY-ATLAS, al-
though we deal with narrative stories of a longer
length than physics essays. The WHY-ATLAS
physics tutor identifies missing information in the
explanations of students using theorem-proving
(Rose et al, 2002).
5.1 Advice Generation Algorithm
Different types of stories need different amounts
of advice. An ?Excellent? story needs less ad-
vice than a ?Good? story. One advice statement is
?general,? while the rest are specific. The system
861
produces a total of seven advice statements for a
?Poor? story, and two less statements for each rat-
ing level above ?Poor.?
With the aid of a teacher, a number of ?canned?
text statements offering general advice were cre-
ated for each rating class. These include state-
ments such as ?It?s very good! I only have a few
pointers? for a ?Good? story and ?Let?s get help
from the teacher? for ?Poor? story. The advice
generation begins by randomly selecting a state-
ment suitable for the rating of the story. Those
students whose stories are rated ?Poor? are asked
if they would like to re-read the story and ask a
teacher for help.
The generation of specific advice uses the re-
sults of the plot-comparison algorithm to produce
specific advice. A number of advice templates
were produced, and the results of the Advice Gen-
eration Algorithm fill in the needed values of the
template. The ? most frequent events in ?Excel-
lent? stories are called the Important Event Struc-
ture, which represents the ?important? events in
the story in temporal order. Empirical experiments
led us ? = 10 for the ?Adventure? story, but for
longer stories like the ?Thief? story a larger ?
would be appropriate. These events correspond to
the ones given the highest weights by the Naive
Bayes algorithm. For each event in the event struc-
ture of a rewritten story, a search for a match in
the important event structure is taken. If a pred-
icate name match is found in the important event
structure, the search continues to attempt to match
the arguments. If the event and the arguments do
not match, advice is generated using the structure
of the ?important? event that it cannot find in the
rewritten story.
This advice may use both the predicate name
and its arguments, such as ?Did the stork fly??
from fly(stork). If an argument is missing, the ad-
vice may be about only the argument(s), like ?Can
you tell me more about the stork?? If the event is
out of order, advice is given to the student to cor-
rect the order, as in ?I think something with the
stork happened earlier in the story.?
This algorithm is formalized in Figure 2, with
all variables being the same as in the Plot Anal-
ysis Algorithm, except that W is the Important
Event Structure composed of events w with the
set of arguments Nw. M is a binary vector used
to store the success of a match with index i. The
ADV function, given an event, generates one ad-
Algorithm
5.1: ADVICEGENERATE(W,R)
for w ?W
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M = ?
i = 0
for r ? R
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if w = r or SY N(r)
then mi = 1
else mi = 0
i = i + 1
for nw ? Nw
do
?
?
?
?
?
?
?
?
?
?
?
for nr ? Nr
do
?
?
?
?
?
?
?
?
?
if nw = SYN(nr) or nr
then mi ? 1
else mi ? 0
i = i + 1
ADV (w,M)
Figure 2: Advice Generation Algorithm
vice statement to be given to the student.
An element of randomization was used to gen-
erate a diversity of types of answers. An ad-
vice generation function (ADV ) takes an impor-
tant event (w) and its binary matching vector (M )
and generates an advice statement for w. Per im-
portant event this advice generation function is pa-
rameterized so that it has a 10% chance of deliver-
ing advice based on the entire event, 20% chance
of producing advice that dealt with temporal or-
der (these being parameters being found ideal af-
ter testing the algorithm), and otherwise produces
advice based on the arguments.
5.2 Advice Evaluation
The plot advice algorithm is run using a randomly
selected corpus of 20 stories, 5 from each plot rat-
ing level using the ?Adventure Corpus.? This pro-
duced matching advice for each story, for a total
of 80 advice statements.
5.3 Advice Rating
An advice rating scheme was developed to rate the
advice produced in consultation with a teacher.
1. Excellent: The advice was suitable for the
story, and helped the student gain insight into
the story.
2. Good: The advice was suitable for the story,
862
Rating % Given
Excellent 0
Good 35
Fair 60
Poor 5
Table 5: Advice Rating Results
and would help the student.
3. Fair: The advice was suitable, but should
have been phrased differently.
4. Poor: The advice really didn?t make sense
and would only confuse the student further.
Before testing the system on students, it was de-
cided to have teachers evaluate how well the ad-
vice given by the system corresponded to the ad-
vice they would give in response to a story. A
teacher read each story and the advice. They then
rated the advice using the advice rating scheme.
Each story was rated for its overall advice quality,
and then each advice statement was given com-
ments by the teacher, such that we could derive
how each individual piece of advice contributed
to the global rating. Some of the general ?coarse-
grained? advice was ?Good! You got all the main
parts of the story? for an ?Excellent? story, ?Let?s
make it even better!? for a ?Good? story, and
?Reading the story again with a teacher would be
help!? for a ?Poor? story. Sometimes the ad-
vice generation algorithm was remarkably accu-
rate. In one story the connection between a curse
being lifted by the possession of a coin by the
character Nils was left out by a student. The ad-
vice generation algorithm produced the following
useful advice statement: ?Tell me more about the
curse and Nils.? Occasionally an automatically ex-
tracted event that is difficult to interpret by a hu-
man or simply incorrectly is extracted. This in turn
can cause advice that does not make any sense
can be produced, such as ?Tell me more about a
spot??. Qualitative analysis showed that ?missing
important advice? to be the most significant prob-
lem, followed by ?nonsensical advice.?
5.4 Results
The results are given in Table 5. The majority of
the advice was rated overall as ?fair.? Only one
story was given ?poor? advice, and a few were
given ?good? advice. However, most advice rated
as ?good? was the advice generated by ?excel-
lent? stories, which generate less advice than other
types of stories. ?Poor? stories were given almost
entirely ?fair? advice, although once ?poor? ad-
vice was generated. In general, the teacher found
?coarse-grained? advice to be very useful, and was
very pleased that the agent could detect when the
student needed to re-read the story and when a stu-
dent did not need to write any more. In some cases
the specific advice was shown to help provide a
?crucial detail? and help ?elicit a fact.? The advice
was often ?repetitive? and ?badly phrased.? The
specific advice came under criticism for often not
?being directed enough? and for being ?too literal?
and not ?inferential enough.? The rater noticed
that ?The program can not differentiate between
an unfinished story...and one that is confused.? and
that ?Some why, where and how questions could
be used? in the advice.
6 Conclusion and Future Work
Since the task involved a fine-grained analysis of
the rewritten story, the use of events that take plot
structure into account made sense regardless of
its performance. The use of events as structured
features in a machine-learning classifier outper-
formed a classifier that relied on a unstructured
?bag-of-words? as features. The system achieved
close to human performance on rating the stories.
Since each of the events used as a feature in the
machine-learner corresponds to a particular event
in the story, the features are easily interpretable by
other components in the system and interpretable
by humans. This allows these events to be used
in a template-driven system to generate advice for
students based on the structure of their plot.
Extracting events from text is fraught with er-
ror, particularly in the ungrammatical and infor-
mal domain used in this experiment. This is often
a failure of our system to detect semantic content
units through either not including them in chunks
or only partially including a single unit in a chunk.
Chunking also has difficulty dealing with preposi-
tions, embedded speech, semantic role labels, and
complex sentences correctly. Improvement in our
ability to retrieve semantics would help both story
classification and advice generation.
Advice generation was impaired by the abil-
ity to produce directed questions from the events
using templates. This is because while our sys-
tem could detect important events and their or-
863
der, it could not make explicit their connection
through inference. Given the lack of a large-scale
open-source accessible ?common-sense? knowl-
edge base and the difficulty in extracting infer-
ential statements from raw text, further progress
using inference will be difficult. Progress in ei-
ther making it easier for a teacher to make explicit
the important inferences in the text or improved
methodology to learn inferential knowledge from
the text would allow further progress. Tantaliz-
ingly, this ability for a reader to use ?inference to
grasp points that may not have been explicit in the
story? is given as the hallmark of truly understand-
ing a story by teachers.
References
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax. In
Jennifer Cole, Georgia Green, and Jerry Morgan,
editors, Computational Linguistics and the Founda-
tions of Linguistic Theory, pages 145?164.
Breck Baldwin. 1997. CogNIAC : A High Precision
Pronoun Resolution Engine.
F.C. Bartlett. 1932. Remembering. Cambridge Uni-
versity Press, Cambridge.
Johan Bos, Stephen Clark, Mark Steedman, James Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING ?04). Geneva,
Switzerland.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE Stuff: Automatic Identification
of Discourse Structure in Student Essays. IEEE In-
telligent Systems, pages 32?39.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
For Information Science, (41):391?407.
Christine Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings,
D. Harter, and N. Person. 2000. Using latent se-
mantic analysis to evaluate the contributions of stu-
dents in autotutor. Interactive Learning Environ-
ments, 8:149?169.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT - A Flexible Tokenisa-
tion Tool. In Proceedings of the Second Language
Resources and Evaluation Conference.
Harry Halpin, Johanna Moore, and Judy Robertson.
2004. Automatic analysis of plot for story rewriting.
In In Proceedings of Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Maya Hickmann. 2003. Children?s Discourse: per-
son, space and time across language. Cambridge
University Press, Cambridge, UK.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic.
Thomas. Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and repre-
sentation of knowledge. Psychological Review.
B. Lemaire, S. Mandin, P. Dessus, and G. Denhire.
2005. Computational cognitive models of summa-
rization assessment skills. In In Proceedings of the
27th Annual Meeting of the Cognitive Science Soci-
ety, Stressa, Italy.
Fiona McNeill, Harry Halpin, Ewan Klein, and Alan
Bundy. 2006. Merging stories with shallow seman-
tics. In Proceedings of the Knowledge Representa-
tion and Reasoning for Language Processing Work-
shop at the European Association for Computational
Linguistics, Genoa, Italy.
Erik T. Mueller. 2003. Story understanding through
multi-representation model construction. In Graeme
Hirst and Sergei Nirenburg, editors, Text Meaning:
Proceedings of the HLT-NAACL 2003 Workshop,
pages 46?53, East Stroudsburg, PA. Association for
Computational Linguistics.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In In Proceedings of the Joint Con-
ference of the North American Association for Com-
putational Linguistics and Human Language Tech-
nologies. Boston, USA.
E. Riloff. 1999. Information extraction as a step-
ping stone toward story understanding. In Ash-
win Ram and Kenneth Moorman, editors, Computa-
tional Models of Reading and Understanding. MIT
Press.
Judy Robertson and Beth Cross. 2003. Children?s
perceptions about writing with their teacher and the
StoryStation learning environment. Narrative and
Interactive Learning Environments: Special Issue
of International Journal of Continuing Engineering
Education and Life-long Learning.
Judy Robertson and Peter Wiemer-Hastings. 2002.
Feedback on children?s stories via multiple interface
agents. In International Conference on Intelligent
Tutoring Systems, Biarritz, France.
C. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. VanLehn. 2002. A hybrid language
understanding approach for robust selection of tutor-
ing goals. In International Conference on Intelligent
Tutoring Systems, Biarritz, France.
864
Automatic Analysis of Plot for Story Rewriting
Harry Halpin
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
H.Halpin@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
J.Moore@ed.ac.uk
Judy Robertson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland, UK
judyr@inf.ed.ac.uk
Abstract
A method for automatic plot analysis of narrative
texts that uses components of both traditional sym-
bolic analysis of natural language and statistical
machine-learning is presented for the story rewrit-
ing task. In the story rewriting task, an exemplar
story is read to the pupils and the pupils rewrite the
story in their own words. This allows them to prac-
tice language skills such as spelling, diction, and
grammar without being stymied by content creation.
Often the pupil improperly recalls the story. Our
method of automatic plot analysis enables the tu-
toring system to automatically analyze the student?s
story for both general coherence and specific miss-
ing events.
1 Introduction
StoryStation is an intelligent tutoring system cre-
ated to provide personalized attention and detailed
feedback to children ages 10-12 on their writing
(Roberston and Wiemar-Hastings, 2002). Writing
is viewed as a skill-based task, with skills being
elements of writing such as spelling, diction, and
plot development. Each writing skill is associated
with an animated agent that provides online help.
Evaluations of StoryStation show that children en-
joy the personalized encouragement and construc-
tive comments that StoryStation provides (Robert-
son and Cross, 2003). StoryStation was designed
by researchers in conjunction with two teachers and
a group of students. However, both students and
teachers indicated StoryStation would be signifi-
cantly improved if it were enhanced with an agent
that could give feedback about the plot of a story.
Here we describe how techniques from symbolic
natural language processing and statistical machine-
learning were used to tackle the problem of auto-
mated plot analysis for StoryStation.
2 The Story Rewriting Task
In the story rewriting task, pupils rewrite a story in
their own words, allowing them to focus on their
writing ability instead of plot formulation. This task
is currently used in Scottish schools and thus it was
chosen to be the first feature of the plot analysis
agent. We collected a corpus of 103 stories rewritten
by children from classes at primary schools in Scot-
land. Pupils were told a story, an exemplar story,
by a storyteller and were asked to rewrite the story
in their own words.1 The automated plot analysis
program must be able to give a general rating of the
quality of the rewritten story?s plot and be able to
determine missing or incorrect events. The general
rating can be used by the teacher to find out which
pupils are in need of attention, while the more spe-
cific details can be used by an animated agent in
StoryStation to remind the student of specific events
and characters they have forgotten or misused.
3 Plot Ratings
The stories were rated for plot by three different
raters. A story-teller (Rater B) ranked all of the sto-
ries. Two others (Rater A, a teacher, and Rater C)
ranked the stories as well, although Rater A ranked
only half. The following scale, devised by a teacher
with over forty years of experience, was used.
1. Excellent: An excellent story shows that the
reader understands the ?point? of the story and
should demonstrate some deep understanding
of the plot. The pupil should be able to retrieve
all the important links and, not all the details,
but the right details.
2. Good: A good story shows that the pupil was
listening to the story, and can recall the main
1The exemplar story used in our corpus was ?Nils? Ad-
venture,? a story from ?The Wonderful Adventures of Nils?
(Lagerloff, 1907).
Class Probability Number of Class
1 (Excellent) 0.175 18
2 (Good) 0.320 33
3 (Fair) 0.184 19
4 (Poor) 0.320 33
Table 1: Distribution of Story Ratings
events and links in the plot. However, the
pupil shows no deeper understanding of the
plot, which can often be detected by the pupil
leaving out an important link or emphasizing
the wrong details.
3. Fair: A fair story shows that the pupil is miss-
ing more than one link or chunk of the story,
and not only lacks an understanding of the
?point? but also lacks recall of vital parts of
the story. A fair story does not really flow.
4. Poor: A poor story has definite problems with
recall of events, and is missing substantial
amount of the plot. Characters will be misiden-
tified and events confused. Often the child
writes on the wrong subject or starts off recit-
ing only the beginning of the story.
Rater B and Rater A had an agreement of 39%
while Rater B and Rater C had an agreement of
77%. However, these numbers are misleading as the
rating scale is ordinal and almost all the disagree-
ments were the result of grading a story either one
rank better or worse. In particular Rater A usually
marked incomplete stories as poor while the other
raters assigned partial credit. To evaluate the relia-
bility of the grades both Cronbach?s ? and Kendall?s
?b were used, since these statistics take into account
ordinal scales and inter-rater reliability. Between
Rater A and B there was a Cronbach?s ? statistic
of .86 and a Kendall?s ?b statistic of .72. Between
Rater B and C there was a Cronbach?s ? statistic
of .93 and Kendall?s ?b statistic of .82. These statis-
tics show our rating scheme to be fairly reliable. As
the most qualified expert to rate all the stories, Rater
B?s ratings were used as the gold standard. The dis-
tribution of plot ratings are given in Table 1.
4 A Minimal Event Calculus
The most similar discourse analysis program to the
one needed by StoryStation is the essay-grading
component of ?Criterion? by ETS technologies
(Burstein et al, 2003), which is designed to anno-
tate parts of an essay according to categories such
as ?Thesis, ?Main Points,? ?Support,? and ?Con-
clusion.? Burstein et. al. (2003) uses Rhetorical
Structure Theory to parse the text into discourse re-
lations based on satellites and nuclei connected by
rhetorical relations. Moore and Pollack (1992) note
that Rhetorical Structure Theory conflates the infor-
mational (the information being conveyed) and in-
tentional (the effects on the reader?s beliefs or atti-
tudes) levels of discourse. Narratives are primarily
informational, and so tend to degenerate to long se-
quences of elaboration or sequence relations. Since
in the story rewriting task the students are attempt-
ing to convey information about the narrative, un-
like the primarily persuasive task of an essay, our
system focuses on the informational level as embod-
ied by a simplified event calculus. Another tutoring
system similar to ours is the WHY physics tutoring
system (Rose et al, 2002).
We formulate only three categories to describe
stories: events, event names, and entities. This for-
mulation keeps the categories from being arbitrary
or exploding in number. Entities are both animate
characters, such as ?elves? and ?storks,? and inani-
mate objects like ?sand? and ?weather.? Nouns are
the most common type of entities. Events are com-
posed of the relationships among entities, such as
?the boy becomes an elf,? which is composed of a
?boy? and ?elf? interacting via ?becoming,? which
we call the event name. This is because the use
of such verbs is an indicator of the presence of an
event in the story. In this manner events are relation-
ships labeled with an event name, and entities are
arguments to these relationships as in propositional
logic. Together these can form events such as be-
come(boy,elf), and this formulation maps partially
onto Shanahan?s event calculus which has been
used in other story-understanding models (Mueller,
2003). The key difference between an event calcu-
lus and a collection of propositions is that time is
explicitly represented in the event calculus.
Each story consists of a group of events that are
present in the story, e1...eh. Each event consists of
an event name, a time variable t, and a set of enti-
ties arranged in an ordered set n1...na. An event
must contain one and only one event name. The
event names are usually verbs, while the entities
tend to be, but are not exclusively, nouns. Time is
made explicit through a variable t. Normally, the
Shanahan event calculus has a series of predicates
to deal with relations of achievements, accomplish-
ments, and other types of temporal relations (Shana-
han, 1997), however our calculus does not use these
since it is difficult to extract these from ungrammati-
cal raw text automatically. A story?s temporal order
is a partial ordering of events as denoted by their
time variable t. When incorporating a set of entities
into an event, a superscript is used to keep the enti-
ties distinct, as n13 is entity 1 in event 3. An entity
may appear in multiple events, such as entity 1 ap-
pearing in event 3 (n13) and in event 5 (n15). The plot
of a story can then be considered an event structure
of the following form if it has h events:
e1(t1, (n11, n21, ...na1)), ...., eh(th, (n2h, n4h...nch))
Where time t1 ? t2 ? ...th. An example from a
rewritten story is ?Nils found a coin and he walked
round a sandy beach. He talked to the stork. Asked
a question.? This is represented by an event struc-
ture as:
find(t = 1(Nils, coin)),
walk(t = 1, (Nils, sand, beach)),
talk(t = 2, (stork, Nils)),
ask(t = 3, (question))
Note that the rewritten stories are often ungram-
matical. A sentence may map onto one, multiple, or
no events. Two stories match if they are composed
of the same ordering of events.
5 Extracting the Event Calculus
The event calculus can be extracted from raw text
by layering NLP modules using an XML-based
pipeline. Our main constraint was that the text of the
pupil was rarely grammatical, restricting our choice
of NLP components to those that did not require a
correct parse or were in any other ways dependent
on grammatical sentences. At each level of process-
ing, an XML-enabled natural language processing
component can add mark-up to the text, and use any
mark-up that the previous components made. All
layers in the pipeline are fully automatic. For our
pipeline we used LT-TTT (Language Technology
Text Tokenization Toolkit) (Grover et al, 2000).
Once words are tokenized and sentence boundaries
detected by LT-TTT, LT-POS tags the words using
the Penn Treebank tag-set without parsing the sen-
tences. While a full parse could be generated by a
statistical parser, such parses would likely be incor-
rect for the ungrammatical sentences often gener-
ated by the pupils (Charniak, 2000). Pronouns are
resolved using a cascading rule-based approach di-
rectly inspired by the CogNIAC algorithm (Bald-
win, 1997) with two variations. First, it resolves in
distinct cascades for singular and then plural pro-
nouns. Second, it resolves using only the Cog-
NIAC rules that can be determined using Penn Tree-
bank tags. The words are lemmatized using an aug-
mented version of the SCOL Toolset and sentences
are chunked using the Cass Chunker (Abney, 1995).
There is a trade-off between this chunking approach
that works on ungrammatical sentences and one that
requires a full parse such as those using dependency
grammars. The Cass Chunker is highly precise,
but often inaccurate and misses relations and enti-
ties that are not in a chunk. In its favor, those tu-
ples in chunks that it does identify are usually cor-
rect. SCOL extracts tuples from the chunks to deter-
mine the presence of events, and the remaining ele-
ments in the chunk are inspected via rules for enti-
ties. Time is explicitly identified using a variation of
the ?now point? algorithm (Allen, 1987). We map
each event?s time variable to a time-line, assuming
that events occur in the order in which they appear
in the text. While temporal ordering of events is
hard (Mani and Wilson, 2003), given that children
of this age tend to use a single tense throughout the
narrative and that in narratives events are presented
in order (Hickmann, 2003), this simple algorithm
should suffice for ordering in the domain of chil-
dren?s stories.
6 Plot Comparison Algorithm
Since the story rewriting task involves imperfect re-
call, story events will likely be changed or left out
by the pupil. The story rewriting task involves the
students choosing their own diction and expressing
their own unique mastery of language, so variation
in how the fundamental elements of the story are
rewritten is to be expected. To deal with these is-
sues, an algorithm had to be devised that takes the
event structure of the rewritten story and compares
it to the event structure of the exemplar story, while
disregarding the particularities of diction and gram-
mar. The problem is one of credit allocation for the
similarity of rewritten events to the exemplar event.
The words used in the events of the two story mod-
els may differ. The exemplar story model might
use the event see(Nils,stork), but a rewritten story
may use the word ?bird? instead of the more precise
word ?stork.? However, since the ?bird? is refer-
ring to the stork in the exemplar story, partial credit
should be assigned. A plot comparison algorithm
was created that uses abstract event calculus repre-
sentations of plot and the text of the rewritten story,
taking into account temporal order and word simi-
larity. The exemplar story?s event structure is cre-
ated by applying the event extraction pipeline to the
storyteller?s transcript.
The Plot Comparison Algorithm is given in Fig-
ure 1. In the pseudo-code, E of size h and R of size
j are the event structures of the exemplar story and
rewritten story respectively, with the names of each
of their events denoted as e and r. The set of entities
of each event are denoted as Ne and Nr respectively.
T is the lemmatized tokens of the rewritten story?s
raw text. WordNet(x) denotes the synset of x. The
?now point? of the rewritten story is t, and feature
set is f , which has an index of i. The index i is
incremented every time f is assigned a value. 1 de-
notes an exact match, 2 a WordNet synset match, 3
a match in the text, and 0 a failure to find any match.
The Plot Comparison Algorithm essentially iter-
ates through the exemplar story looking for matches
of the events in the rewritten story. To find if two
events are in or out of order the rewritten story has
a ?now point? that serves as the beginning of its it-
eration. Each event of the event structure of the ex-
emplar story is matched against each event of the
rewritten story starting at the ?now point? and us-
ing the exact text of the event name. If that match
fails a looser match is attempted by giving the event
names of the rewritten story to WordNet and see-
ing if a match to the resultant synset succeeds (Fell-
baum, 1998). If either match attempt succeeds, the
algorithm attempts to match entities in the same
fashion and the ?now point? of the rewritten story
is incremented. Thus the algorithm does not looks
back in the rewritten story for a match. If the event
match fails, one last attempt is made by checking
the event name or entity against every lemmatized
token in the entire rewritten text. If this fails, a fail-
ure is recorded. The results of the algorithm are can
be used as a feature set for machine-learning. The
event calculus extraction pipeline and the Plot Com-
parison Algorithm can produce event calculus rep-
resentations of any English text and compare them.
They have been tested on other stories that do not
have a significant corpus of rewritten stories. The
number of events for an average rewritten story in
our corpus was 26, with each event having an aver-
age of 1 entity.
Included in Figure 2 is sample output from our
algorithm given the exemplar story model ea and a
rewritten story rb whose text is as follows: Nils took
the coin and tossed it away, cause it was worthless.
A city appeared and so he walked in. Everywhere
was gold and the merchant said Buy this Only one
coin Nils has no coin. So he went to get the coin he
threw away but the city vanished just like that right
behind him. Nils asked the bird Hey where the city
go? Let?s go home.
Due to space limitations, we only display selected
events from the transcript and their most likely
match from the rewritten story in Figure 2. The out-
put of the feature set would be the concatenation in
order of every value of fe.
Algorithm
6.1: PLOTCOMPARE(E, R, T )
t? 1
i? 0
for ex ? e1 to eh
do for ry ? rt to rj
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if ex = ry
then fi ? 1 and t? t + 1
else if ex ? WORDNET(ry)
then fi ? 2 and t? t + 1
if fi = 1 or 2
then
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
for each n ? Ne
if n ? N r
then fi ? 1
else if n ? WORDNET(Nr)
then fi ? 2
else if n ? T
then fi ? 3
else fi ? 0
else if ex ? T
then fi ? 3
else fi ? 0
Figure 1: Plot Comparison Algorithm
ea rb fe
throw(Nils, coin) toss(coin) 2, 3, 1
see(Nils, city) appear(city) 0, 3, 3
enter(Nils, city) walk(Nils) 0, 3, 3
ask(Nils, merchant) say(merchant) 0, 3, 3
say(Nils) say(merchant) 1, 3
leave(Nils) go(Nils) 2, 1
disappear(city) vanish(city) 2, 1
inquire(Nils, stork) ask(Nils, bird) 2, 1, 2
fly(stork) go(home) 0, 3
Figure 2: Example of Plot Algorithm
7 Learning the Significance of Events
Machine-learning is crucial to our experiment, as it
will allow our model to discriminate what events
and words in a rewritten story are good predictors
of plot quality as rated by a human expert. We
have restricted our feature set to the results of the
Plot Comparison Algorithm and LSA scores, as we
describe below. Other possible features, such as
the grammatical correctness and the number of con-
junctives, are dealt with by other agents in StoryS-
tation. We are focusing on plot recall quality as
opposed to general writing quality. Two different
machine-learning algorithms with differing assump-
tions were used. These are by no means exhaus-
tive of the options, and extensive tests have been
done with other algorithms. Further experiments
are needed to understand the precise nature of the
relations between the feature set and machine learn-
ing algorithms. All results were created by ten-fold
cross validation over the rated stories, which is es-
pecially important given our small corpus size.
7.1 Nearest Neighbors using LSA
We can classify the stories without using the re-
sults of the Plot Comparison Algorithm, and instead
use only their statistical attributes. Latent Semantic
Analysis (LSA) provides an approximation of ?se-
mantic? similarity based on the hypothesis that the
semantics of a word can be deduced from its context
in an entire document, leading to useful coherency
scores when whole documents are compared (Foltz
et al, 1998). LSA compares the text of each rewrit-
ten story in the corpus for similarity to the transcript
of the exemplar story in a subspace produced by
reducing the dimensionality of the TASA 12 grade
USA reading-level to 200. This dimensionality was
discovered through experimentation to be our prob-
lem?s optimal parameters for LSA given the range
of choices originally used by Landauer (1997). The
stories can be easily classified by grouping them to-
gether based on LSA similarity scores alone, and
this technique is embodied in the simple K-Nearest
Neighbors (K-NN) learner. K-NN makes no para-
metric assumptions about the data and uses no for-
mal symbolic features other than an LSA similarity
score. For K-NN k = 4 gave the best results over
a large range of k, and we expect this k would be
ideal for stories of similar length.
As shown in Table 2, despite its simplicity this al-
gorithm performs fairly well. It is not surprising that
features based primarily on word distributions such
as LSA could correctly discriminate the non-poor
from the poor rewritten stories. Some good rewrit-
ten stories closely resemble the exemplar story al-
most word for word, and so share the same word
distribution with the exemplar story. Poor rewritten
stories usually have little resemblance to the exem-
plar story, and so have a drastically different word
distribution. The high spread of error in classifying
stories is shown in the confusion matrix in Table 3.
This leads to unacceptable errors such as excellent
stories being classified as poor stories.
7.2 Hybrid Model with Naive Bayes
By using both LSA scores and event structures as
features for a statistical machine learner, a hybrid
model of plot rating can be created. In hybrid mod-
Class Precision Recall F-score
1 (Excellent) 0.11 0.17 0.13
2 (Good) 0.42 0.46 0.44
3 (Fair) 0.30 0.16 0.21
4 (Poor) 0.83 0.76 0.79
Table 2: K-Nearest Neighbors Precision and Recall
Class 1 2 3 4
1 (Excellent) 3 10 4 1
2 (Good) 13 15 2 3
3 (Fair) 9 6 3 1
4 (Poor) 2 5 1 25
Table 3: K-Nearest Neighbors: Confusion Matrix
els a formal symbolic model (the event calculus-
based results of a Plot Comparison Algorithm) en-
ters a mutually beneficial relationship with a statis-
tical model of the data (LSA), mediated by a ma-
chine learner (Naive Bayes). One way to combine
LSA similarity scores and the results of the event
structure is by using the Naive Bayes (NB) ma-
chine learner. NB makes the assumptions of both
parametrization and Conditional Independence.
The recall and precision per rank is given in Ta-
ble 4, and it is clear that while no stories are clas-
sified as excellent at all, the majority of good and
poor stories are identified correctly. As shown by
the confusion matrix in Table 5, NB does not de-
tect excellent stories and it collapses the distinction
between good and excellent stories. Compared to
K-NN with LSA, NB shows less spread in its er-
rors, although it does confuse some poor stories as
good and one excellent story as fair. Even though
it mistakenly classifies some poor stories as good,
for many teachers this is better than misidentifying
a good story as a poor story.
The raw accuracy results over all classes of the
machine learning algorithms are summarized in Ta-
ble 6. Note that average human rater agreement
is the average agreement between Rater A and C
(whose agreement ranged from 39% to 77%), since
Rater B?s ratings were used as the gold standard.
This average also assumes Rater A would have con-
tinued marking at the same accuracy for the com-
Class Precision Recall F-Score
1 (Excellent) 0.00 0.00 0.00
2 (Good) 0.43 0.88 0.58
3 (Fair) 0.45 0.26 0.33
4 (Poor) 0.92 0.67 0.77
Table 4: Naive Bayes Precision and Recall
Class 1 2 3 4
1 (Excellent) 0 17 1 0
2 (Good) 1 29 2 1
3 (Fair) 0 13 5 1
4 (Poor) 0 8 3 22
Table 5: Naive Bayes Confusion Matrix
Machine Learner Percentage Correct
K-NN (LSA) 44.66%
ID3 DT (Events) 40.78%
NB (LSA + Events) 54.37%
Rater Agreement 58.37%
Table 6: Machine Learner Comparison
plete corpus. DT refers to an ID3 Decision Tree
algorithm that creates a purely symbolic machine-
learner whose feature set was only the results of the
Plot Comparison Algorithm (Quinlan, 1986). It per-
formed worse than K-NN and thus the details are
not reported any further. Using NB and combining
the LSA scores with the results of the Plot Com-
parison Algorithm produces better raw performance
than K-NN. Recall of 54% for NB may seem dis-
appointing, but given that the raters only have an
average agreement of 58%, the performance of the
machine learner is reasonable. So if the machine-
learner had a recall of 75% it would be suspect.
Statistics to compare the results given the ordinal
nature of our rating scheme are shown in Table 7.
8 Discussion
From these experiments as shown in Table 6 we
see that the type of machine learner and the par-
ticular features are important to correctly classify
children?s stories. Inspection of the results shows
that separating good and excellent stories from poor
stories is best performed by Naive Bayes. For our
application, teachers have indicated that the classi-
fication of an excellent or good story as a poor one
is considered worse than the classifying of a fair
or even poor story as good. Moreover, it uses the
event-based results of the Plot Comparison Algo-
rithm so that the agent in StoryStation may use these
results to inform the student what precise events and
entities are missing or misused. NB is fast enough to
provide possible feedback in real time and its abil-
ity to separate poor stories from good and excellent
stories would allow it to be used in classrooms. It
also has comparable raw accuracy to average human
agreement as shown in Table 6, although it makes
more errors than humans in classifying a story off
by more than one class off as shown by the statistics
Machine Learner Cronbach?s ? Kendall?s ?b
NB to Rater B .78 .59
Rater A to Rater B .86 .72
Rater C to Rater B .93 .82
Table 7: Statistical Comparison
in Table 7. The results most in its favor are shown
highlighted in Table 5. It separates with few errors
both excellent and good stories from the majority of
poor stories.
While the event calculus captures some of the rel-
evant defining characteristics of stories, it does not
capture all of them. The types of stories that give the
machine learners the most difficulty are those which
are excellent and fair. One reason is that these sto-
ries are less frequent in the training data than poor
and good stories. Another reason is that there are
features particular to these stories that are not ac-
counted for by an event structure or LSA. Both ex-
cellent stories and fair stories rely on very subtle
features to distinguish them from good and poor sto-
ries. Good stories were characterized in the rating
criteria as ?parroting off of the main events,? and
the event calculus naturally is good at identifying
this. Poor stories have ?definite problems with the
recall of events,? and so are also easily identified.
However, fair stories show both a lack of ?under-
standing of the point? and ?do not really flow? while
the excellent story shows an ?understanding of the
point.? These characteristics involve relations such
as the ?point? of the story and connections between
events. These ideas of ?flow? and ?point? are much
more difficult to analyze automatically.
9 Conclusion
Due to its practical focus, the plot analysis of our
system is very limited in nature, focusing on just
the story rewriting task. Traditionally ?deep? rep-
resentation systems have attempted to be powerful
general-purpose story understanding or generation
systems. A general plot analysis agent would be
more useful than our current system, which is suc-
cessful by virtue of the story rewriting task being
less complex than full story understanding. How-
ever, our system fulfills an immediate need in the
StoryStation application, in contrast to more tra-
ditional story-understanding and story-generation
systems, which are usually used as testing grounds
for theoretical ideas in artificial intelligence. The
system was tested and developed using a small man-
ually collected corpus of a single rewritten story.
While previous researchers who worked on this
problem felt that the small size of the corpus made
machine-learning unusable, the results shows that
with careful feature selection and relatively simple
algorithms empirical methods can be made to work.
We expect that our technique can be generalized to
larger corpora of diverse types.
Our hybrid system uses both LSA and event
structures to classify plot quality. The use of event
structures in classifying stories allows us to de-
tect whether particular crucial characters and events
have been left out of the rewritten story. Separating
the students who have written good plots from those
who have done so poorly is a boon to the teachers,
since often it is the students who have the most dif-
ficulty with plot that are least likely to ask a teacher
for help. StoryStation is now being used in two
schools as part of their classroom writing instruc-
tion over the course of the next year. Results from
this study will be instrumental in shaping the future
of the plot analysis system in StoryStation and the
expansion of the current system into a general pur-
pose plot analysis system for other writing tasks.
References
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
In Jennifer Cole, Georgia Green, and Jerry Mor-
gan, editors, Computational Linguistics and the
Foundations of Linguistic Theory, pages 145?
164.
James Allen. 1987. Natural Language Understand-
ing. Menlo Park, CA, Benjamin/Cummings Pub-
lishing.
Breck Baldwin. 1997. CogNIAC : A High Preci-
sion Pronoun Resolution Engine.
Jill Burstein, Daniel Marcu, and Kevin Knight.
2003. Finding the WRITE Stuff: Automatic
Identification of Discourse Structure in Student
Essays. IEEE Intelligent Systems, pages 32?39.
Eugene Charniak. 2000. A Maximum-Entropy In-
spired Parser. In Proceedings of the North Amer-
ican Association for Computational Linguistics.
Christine Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence
with Latent Semantic Analysis. Discourse Pro-
cesses, 25(2&3):285?307.
Claire Grover, Colin Matheson, Andrei Mikheev,
and Marc Moens. 2000. LT TTT - A Flexible
Tokenisation Tool. In Proceedings of the Second
Language Resources and Evaluation Conference.
Maya Hickmann. 2003. Children?s Discourse: per-
son, space and time across language. Cambridge
University Press, Cambridge, UK.
Selma Lagerloff. 1907. The Wonderful Adventures
of Nils. Doubleday, Page, and Company, Garden
City, New York.
Thomas. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Re-
view.
I. Mani and G. Wilson. 2003. Robust temporal pro-
cessing of the news. In In Proceedings of Associ-
ation for Computational Linguistics.
Johanna D. Moore and Martha Pollack. 1992.
A problem for RST: The need for multi-level
discourse analysis. Computational Linguistics,
18(4):537?544.
Erik T. Mueller. 2003. Story understanding
through multi-representation model construction.
In Graeme Hirst and Sergei Nirenburg, editors,
Text Meaning: Proceedings of the HLT-NAACL
2003 Workshop, pages 46?53, East Stroudsburg,
PA. Association for Computational Linguistics.
Ross Quinlan. 1986. Induction of decision trees. In
Machine Learning, volume 1. Kluwer Academic
Press.
Judy Roberston and Peter Wiemar-Hastings. 2002.
Feedback on children?s stories via multiple inter-
face agents. In International Conference on In-
telligent Tutoring Systems, Biarritz, France.
Judy Robertson and Beth Cross. 2003. Children?s
perceptions about writing with their teacher and
the StoryStation learning environment. Narrative
and Interactive Learning Environments: Special
Issue of International Journal of Continuing En-
gineering Education and Life-long Learning.
C. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. VanLehn. 2002. A hybrid language
understanding approach for robust selection of tu-
toring goals. In International Conference on In-
telligent Tutoring Systems, Biarritz, France.
Murray Shanahan. 1997. Solving the Frame Prob-
lem. MIT Press, Cambridge, MA.
Merging Stories with Shallow Semantics
Fiona McNeill
School of Informatics
Univ. of Edinburgh
f.j.mcneill@ed.ac.uk
Harry Halpin
School of Informatics
Univ. of Edinburgh
h.halpin@ed.ac.uk
Ewan Klein
School of Informatics
Univ. of Edinburgh
ewan@inf.ed.ac.uk
Alan Bundy
School of Informatics
Univ. of Edinburgh
bundy@inf.ed.ac.uk
Abstract
We demonstrate a proof-of-concept sys-
tem that uses a shallow chunking-based
technique for knowledge extraction from
natural language text, in particular looking
at the task of story understanding. This
technique is extended with a reasoning
engine that borrows techniques from dy-
namic ontology refinement to discover the
semantic similarity of stories and to merge
them together.
1 Introduction
Many NLP applications would benefit from the
availability of broad-coverage knowledge extrac-
tion from natural language text. Despite some re-
cent advances in this direction (Bos et al, 2004), it
is still the case that it is hard to obtain deep seman-
tic analyses which are accurate enough to support
logical inference (Lev et al, 2004).
Our problem can be stated in abstract terms.
Given a formalism F for the semantic representa-
tion of natural language, such as first-order clauses
in predicate calculus, and a set of sentences S, give
a translation function T (S) ? F . The goal of
such a translation would be to solve a problem
P (such as paraphrasing or question-answering)
where F allows P to be solved by some reason-
ing process, or else the domain exhibits a type of
structure easily represented in the formalism F .
If we accept that current parsing technology
cannot reliably combine accurate semantic anal-
ysis with robustness, then the question arises
whether ?noisy? semantics can be ameliorated us-
ing some other techniques. In this paper, we adopt
the hypothesis that methods drawn from dynamic
ontology refinement (McNeill et al, 2004) can in-
deed help with this task. In the limit, we would
like to be able to show that semantic content drawn
from a wide variety of sources can be compared
and merged to reveal the shared common ground.
However, in this paper we have limited ourselves
to a much more modest goal, namely merging and
comparing semantic content from a set of variants
of a single story.1
We obtained the variants by asking adults to
retell the story, based on hearing the original, or
reading it themselves. We have developed a sys-
tem that can take in any number of such stories
and produce a merged version of the stories. Our
re-tellers were instructed not to elaborate upon or
intentionally change the original and consequently
the stories are fairly similar, not just in meaning
but to an extent also in wording.
In the next two sections, we will first describe
how semantic clauses are extracted from text, and
second, how clauses obtained from different texts
are merged.
2 Extracting Clauses from Text
The method we have adopted for extracting first-
order clauses from text can be called ?semantic
chunking.? This seems an appropriate term for
two reasons. First, we use a syntactic chunker
to identify noun groups and verb groups (i.e. non-
recursive clusters of related words with a noun or
verb head respectively). Second, we use a cas-
cade of finite state rules to map from this shallow
syntactic structure into first-order clauses; this cas-
cade is conceptually very similar to the chunking
method pioneered by Abney?s Cass chunker (Ab-
ney, 1996).
The text processing framework we have used
draws heavily on a suite of XML tools developed
1We used a simplified version of Oscar Wilde?s fairy story
The Selfish Giant.
36 KRAQ06
for generic XML manipulation (LTXML (Thomp-
son et al, 1997)) as well as NLP-specific XML
tools (LT-TTT (Grover et al, 2000), LT-CHUNK
(Finch and Mikheev, 1997)). More recently, sig-
nificantly improved upgrades of these tools have
been developed, most notably the program lx-
transduce, which performs rule-based transduc-
tions of XML structures. We have used lxtransduce
both for the syntactic chunking (based on rule de-
veloped by Grover) and for construction of seman-
tic clauses.
The main steps in the processing pipeline are as
follows:
1. Words and sentences are tokenized.
2. The words are tagged for their part of speech
using the CandC tagger (Clark and Curran,
2004) and the Penn Treebank tagset.
3. Pronoun resolution is carried out using
the Glencova Pronoun Resolution algorithm
(Halpin et al, 2004), based on a series of
rules similar to the CogNIAC engine (Bald-
win, 1997), but without gender information-
based rules since this is not provided by the
Penn Treebank tagset.
4. The words are then reduced to their morpho-
logical stem (lemma) using Morpha (Minnen
et al, 2001).
5. The lxtransduce program is used to chunk the
sentence into verb groups and noun groups.
6. In an optional step, words are tagged as
Named Entities, using the CandC tagger
trained on MUC data.
7. The partially chunked sentences are selec-
tively mapped into semantic clauses in a se-
ries of steps, described in more detail below.
8. The XML representation of the clauses is con-
verted using an XSLT stylesheet into a more
conventional syntactic format for use by Pro-
log or other logic-based systems.
The output of the syntactic processing is an
XML file containing word elements which are
heavily annotated with attributes. Following
CoNLL BIO notation (Tjong et al, 2000), chunk
information is recorded at the word level. Heads
of noun groups and verb groups are assigned se-
mantic tags such as arg and rel respectively. In
addition, other semantically relevant forms such
as conjunction, negation, and prepositions are also
tagged. Most other input and syntactic infor-
mation is discarded at this stage. However, we
maintain a record through shared indices of which
terms belong to the same chunks. This is used, for
instance, to build coordinated arguments.
Regular expressions over the semantically
tagged elements are used to compose clauses, us-
ing the heuristic that an arg immediately preced-
ing a pred is the subject of the clause, while args
following the pred are complements. Since the
heads of verb groups are annotated for voice, we
can treat passive clauses appropriately, yielding a
representation that is equivalent to the active con-
gener. We also implement simple heuristics that
allow us to capture simple cases of control and
verb phrase ellipsis in many cases.
3 Knowledge Refinement and Merging
Once the clauses have been extracted from the
text, each story becomes a list of predicates, rep-
resenting verbs, each with a number of arguments
(possibly zero), representing nouns. Two stories
can thus be compared by considering how the
clauses from one story relate to the clauses from
another story. This is done both by considering
how the predicates (verbs) from one story relate to
those from another story and also by considering
the arguments (nouns) that these related predicates
connect. This allows us to consider not just the
similarity of the words used in the story but also,
to some extent, the structure of the sentences.
The aim of merging is to build up, initially, a
working merged story that includes all aspects of
each story so far; then, when all stories have been
merged, to refine the working merged story by re-
moving aspects of it that are considered to be pe-
ripheral to the main core. The output is a single
story in the same format as the inputs, and which
reflects common elements from across the set of
variants.
If text is represented in such clause form, then
the number of ways in which these clausal rep-
resentations of the story can differ is strictly lim-
ited. Clauses have only two attributes: predicates
and arguments. The predicates may find an exact
match, an inexact match or no match. If the predi-
cates find some kind of match, their arguments can
then be examined. Each of these will find an exact,
inexact, or no match with the corresponding ar-
37 KRAQ06
gument in the related predicate; additionally, their
ordering and number may be different. Thus it is
possible to create an exhaustive list of the possible
differences between clauses. We currently con-
sider only WordNet information concerning syn-
onyms, hypernyms and hyponyms when determin-
ing matches: we do not perform inference using
antonyms, for example, nor do we consider impli-
cation cases.
The techniques that are used in the merging
process were inspired by work on dynamic on-
tology refinement (McNeill et al, 2004), which
deals with the problem of reasoning failure caused
by small inconsistencies between largely similar
ontologies. This is achieved by analysing onto-
logical objects that were expected to, but do not,
match, and diagnosing and patching these mis-
matches through consideration of how they dif-
fer through reasoning. Examples of mismatches
that are common between similar ontologies are
changed names of predicates (often to sub- or
super-types), changed arity, and changed types of
arguments. These types of ontological mismatches
are currently handled by our system, since in the
domain of stories people often use different names
or different levels of description for things. The
application of these techniques for determining
differences and similarities between the story rep-
resentations therefore forms the basis of the merg-
ing process.
In order to merge a new story with the current
working merged story (WMS), the facts in the WMS
are examined one by one in an attempt to match
them to a fact in the story to be merged. Such a
match may be exact (the predicates and all their
arguments are identical), inexact (the predicates
have the same name but their arguments differ),
similar (the predicates are synonyms) or related
(the predicates are hyponyms or hypernyms). In-
formation about synonyms, hyponyms and hyper-
nyms is extracted from WordNet and used as the
type hierarchy for our refinement (see Section 5;
for an explanation of our usage of WordNet, see
the WordNet project (Miller, 1995) for general de-
tails). Another potential kind of match is where
the arguments match but no link can be found be-
tween the predicates; however, this is not consid-
ered in the current system. If a match of any kind
is found for a predicate from the WMS, the predi-
cate from the new story with which it matches is
appended to its entry in the WMS. Each entry in the
WMS is annotated with a score to indicate in how
many stories a match of some kind has been found
for it. For example, an entry in the WMS, ([1]
play(child)) may find an inexact match with
cavort(child) in a new story, to create an entry
of ([2] play(child), cavort(child)) in the
new WMS.
Once all the facts in the WMS have been exam-
ined and matched where possible, there will usu-
ally be some facts in the story to be merged that
have not been found as a match for anything in the
WMS. These should not be ignored; they have no
match with anything thus far, but it may be that
stories to be merged in future will find a match
with them. Thus, they are added to the merged
story with a score of 1. The initial merged story
is found by simply annotating the first story to be
merged so that each entry has a score of 1. It
is possible, but not necessary, to provide a range
value to the merging process, so that matches are
only sought in the given vicinity of the fact in the
WMS. If no range value is given, this is set to be ar-
bitrarily large so that all of the story to be merged
is examined.
Once all of the stories to be merged have been
examined, we have a complete WMS, which needs
to be processed to produce the merged output. A
threshold value is used to determine which of these
should be immediately discarded: anything with a
score less than the threshold. Those with a score
of more than or equal to the threshold must be pro-
cessed so that each is represented by a single fact,
rather than a list of different versions of the fact. If
all versions of the fact are identical, this single ver-
sion is the output. Otherwise, both a ?canonical?
name and ?canonical? arguments for a fact must
be calculated. In the current system, a simple ap-
proach is taken to this. For the predicate, if there
is more than one version, then the most commonly
occurring one is chosen as the canonical represen-
tative. If there are two or more that are jointly the
most commonly occurring, then the most specific
of the names is chosen (i.e., the one that is lowest
in the class hierarchy). When choosing the argu-
ments for the merged predicate, any argument that
has a match in at least one other version of the
predicate is included. If the match is inexact ?
i.e., the arguments are not of the same class, but
are of related classes ? then the most specific of
the classes is chosen.
38 KRAQ06
4 Worked Example
We now work through a simplified example to il-
lustrate the merging process. Consider the follow-
ing three lists of facts, which are drawn from dif-
ferent retellings of our example story:
Story 1: come(child), play(garden),
visit(friend), forget(friend),
come(giant), yell(child)
Story 2: go(giant), visit(friend),
be(giant), come(child), play(garden)
Story 3: be(giant), come(giant),
play(garden),
bellow(child,anger,giant),
happy(giant)
The first WMS (working merged story) is pro-
duced by marking up the first story:
([1] come(child)),
([1] play(garden)),
([1] visit(friend)),
([1] forget(friend)),
([1] come(giant)),
([1] yell(child))
This is then merged with the second story. The
first fact of Story 1, come(child), matches exactly
with the fourth fact of the Story 2; the fifth fact
matches inexactly with the fourth fact of the Story
2, and so on. The resulting WMS is:
([2] come(child), come(child)),
([2] play(garden), play(garden)),
([2] visit(friend) visit(friend)),
([1] forget(friend)),
([1] come(giant)),
([1] yell(child)),
([1] come(giant)),
([1] go(giant)),
([1] be(giant))
This is then merged with Story 3 to produce:
([3] come(child), come(child),
come(giant)),
([3] play(garden), play(garden),
play(garden)),
([2] visit(friend) visit(friend)),
([1] forget(friend)),
([1] come(giant)),
([2] yell(child),
bellow(child,anger,giant)),
([1] come(giant)),
([1] go(giant)),
([2] be(giant), be(giant)),
([1] happy(giant))
We then proceed to merge all the automat-
ically extracted knowledge representations of
the three stories. To create the output merged
story, those predicates with a score of 1 are
ignored. The others are each merged to pro-
duce a single predicate. For example, ([3]
come(child), come(child), come(giant))
becomes come(child): giant does not match
with any other argument and is dropped. ([2]
yell(child), bellow(child,anger,giant))
becomes yell(child) because yell is a subclass
of bellow, and thus preferred, and child is the
only argument that has a match in both facts. Thus
the resulting merged story is:
come(child),
play(garden),
visit(friend),
yell(child),
be(giant)
It is clear from this example that our current ap-
proach is, at times, naive. For example, the deci-
sions about which arguments to include in output
facts, and how to order facts that are unmatched
in working merged stories could be made signifi-
cantly more effective. We view this current system
as a proof of concept that such an approach can
be useful; we certainly do not consider the system
to be complete approach to the problem. Further
work on the system would result in improved per-
formance.
5 Extracting Ontological Information
from WordNet
In order to perform some of the tasks involved in
merging and matching, it is necessary to have in-
formation about how words are related. We extract
this information from WordNet by getting the on-
tology refinement engine to call WordNet with a
word and retrieve both its synset (i.e., synomym
set) and its hyponyms and hypernyms. We col-
lect these for every sense of the word, since our
natural language pipeline currently does not in-
clude word-sense disambiguation. When it is nec-
essary during the merging process to obtain in-
formation about whether two words are related
(i.e., when two words do not match exactly), we
extract synonym and hyper/hyponym information
from WordNet for these words and examine it to
discover whether an exact match exists or not. We
treat synonyms as equivalent, and we treat hyper-
nym and hyponym synsets as the super- and sub-
type of a word respectively, and then traverse the
type hierarchy for exact matches. To avoid spu-
rious equivalence we use a bound to restrict the
search, and from our experience a bound of two
type-levels in either direction and a stop-list of
?upper ontology? types yields good results.
6 Related Work
This work breaks some new ground by being the
first to use dynamic refinement to compare and
merge information from natural language texts. In
general, recent work in natural language process-
ing has currently relied heavily on ?purely statisti-
cal? methods for tasks such as text similarity and
39 KRAQ06
summarization. However, there is also a rich log-
ical tradition in linguistic semantics, and work in
this vein can bring the two closer together.
Current work in story understanding is focus-
ing on the use of logical forms, yet these are not
extracted from text automatically (Mueller, 2003).
The natural language processing and story con-
version pipeline are improvements over a pipeline
that was shown to successfully compare stories in
a manner similar to a teacher (Halpin et al, 2004).
The merging task is a more logic-based ap-
proach than similar techniques like information
fusion used in multi-document summarization
(Barzilay et al, 1999). Our approach has some
features in common with (Wan and Dale, 2001),
however, we have chosen to focus exclusively on
merger at the semantic level, rather than trying to
also incorporate syntactic structure.
There has also been a revival in using weighted
logical forms in structured relational learning,
such as Markov Logic Networks (Domingos and
Kok, 2005), and this is related to the scoring
of facts used by the current system in merging
texts. As mentioned at the beginning of this paper,
the conversion of unrestricted text to some logi-
cal form has experienced a recent revival recently
(Bos et al, 2004). Although our approach deliber-
ately ignores much semantic detail, this may be
compensated for by increased robustness due to
the reliance on finite-state methods for semantic
translation and chunking.
7 Further Work
The work we have done thus far suggests many av-
enues for further work. One obvious improvement
would be to enable the system to deal with more
complex input, so that stories could be represented
with nested predicates and with a more complex
notion of time. Time can be conceived of ontologi-
cally in a number of differing manners with highly
differing properties, and relating these notions of
time to the extraction of tense (which the lxtrans-
duce-based chunker currently does automatically)
would be a fruitful task (Hayes, 1996). Making
decisions about what to include in a merged story
is currently done in a fairly naive manner. Fur-
ther research into what constituted a good merged
story and under what circumstances it is advanta-
geous to be sceptical or generous as to what should
be included in the merged story, would allow this
to become much more effective. Once the system
has been suitably improved, it should be tested on
a more complex domain, such as news stories. Fi-
nally, the primary benefit of the use of knowledge
representation is the possibility of using inference.
The current system could easily take advantage
of an external knowledge-base of domain-specific
facts and rules to aid refinement and merging.
We have not implemented a baseline for the sys-
tem using purely word-based statistical features,
such as reducing the words to the their morpho-
logical stem and then using WordNet synsets to
compare the stories without any predicate struc-
ture. This is because at this stage in development
the extraction of the correct clauses is itself the
goal of the task. If connected to larger system,
comparison with a purely statistical model would
be useful. However, we would hazard a guess that
in the domain of computational story understand-
ing, it is unlikely that purely statistical methods
would work well, since stories by their nature con-
sist of events involving the actions of characters in
a particular temporal order, and this type of struc-
tural complexity would seem to be best accounted
for by some structure-preserving features that at-
tempt to model and extract these events explicitly.
8 Conclusion
Although many questions remain unanswered,
the development of the current system demon-
strates that this kind of refinement-based approach
to matching and merging texts can be produce
promising results. Much could be done to im-
prove the effectiveness of both the clause extrac-
tion and the merging components of the system,
and the breadth of task that these techniques have
been tested on remains very narrow. Nevertheless,
this work represents a reasonably successful first
investigation of the problem, and we intend to use
it as the basis for further work.
References
Steven Abney. 1996. Partial parsing via finite-
state cascades. Natural Language Engineering,
2(4):337?344.
Breck Baldwin. 1997. CogNIAC: A High Precision
Pronoun Resolution Engine. In Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts (ACL-97 workshop), pages 38?45.
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In In Proceedings of Association for
40 KRAQ06
Computational Linguistics, pages 550?557, Mary-
land.
Johan Bos, Stephen Clark, Mark Steedman, James Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In In
Proceedings of the 20th International Conference
on Computational Linguistics (COLING ?04), pages
1240?1246, Geneva, Switzerland.
Stephen Clark and James Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2004),
pages 104?111, Barcelona, Spain.
Pedro Domingos and Stanley Kok. 2005. Learning the
structure of Markov Logic Networks. In Proceed-
ings of the International Conference on Machine
Learning, pages 441?448, Bonn.
Steve Finch and Andrei Mikheev. 1997. A workbench
for finding structure in texts. In Walter Daelemans
and Miles Osborne, editors, Proceedings of the Fifth
Conference on Applied Natural Language Process-
ing (ANLP-97). Washington D.C.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT?a flexible tokenisa-
tion tool. In LREC 2000?Proceedings of the 2nd
International Conference on Language Resources
and Evaluation, pages 1147?1154.
Harry Halpin, Johanna Moore, and Judy Robertson.
2004. Automatic analysis of plot for story rewriting.
In Proceedings of Empirical Methods in Natural
Language Processing, pages 127?133, Barcelona,
Spain.
Pat Hayes. 1996. A catalog of temporal theories.
Technical Report UIUC-BI-AI-96-01, University of
Illinois.
Iddo Lev, Bill MacCartney, Christopher D. Manning,
and Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In 2nd Work-
shop on Text Meaning and Interpretation at ACL
2004, pages 9?16.
Fiona McNeill, Alan Bundy, and Chris Walton. 2004.
Facilitating agent communication through detecting,
diagnosing and refining ontological mismatch. In
Proceedings of the KR2004 Doctoral Consortium.
AAAI Technical Report.
George Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 11(38):39?
41.
Guido Minnen, John Carroll, and David Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?203.
Erik T. Mueller. 2003. Story understanding through
multi-representation model construction. In Graeme
Hirst and Sergei Nirenburg, editors, Text Meaning:
Proceedings of the HLT-NAACL 2003 Workshop,
pages 46?53, East Stroudsburg, PA. Association for
Computational Linguistics.
Henry Thompson, Richard Tobin, David McKelvie,
and Chris Brew. 1997. LT XML: Software API and
toolkit for XML processing.
Erik F. Tjong, Kim Sang, and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the Conference on
Natural Language Learning (CoNLL-2000). Lisbon,
Portugal.
Stephen Wan and Robert Dale. 2001. Merging sen-
tences using shallow semantic analysis: A first ex-
periment. In Proceedings of the 2001 Australasian
Natural Language Processing Workshop, Sydney,
April. Macquarie University.
41 KRAQ06
