BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 108?109,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
CBR-Tagger: a case-based reasoning approach to the 
gene/protein mention problem 
 
Mariana Neves Monica Chagoyen 
Biocomputing Unit Biocomputing Unit 
Centro Nacional de Biotecnolog?a - CSIC Centro Nacional de Biotecnolog?a - CSIC 
Madrid, 28049, Spain Madrid, 28049, Spain 
mlara@cnb.csic.es monica.chagoyen@cnb.csic.es 
 
Jos? M. Carazo Alberto Pascual-Montano 
Biocomputing Unit Departamento de Arquitectura de Computadores 
Centro Nacional de Biotecnolog?a - CSIC Facultad de Ciencias F?sicas, UCM 
Madrid, 28049, Spain Madrid, 28040, Spain 
carazo@cnb.csic.es pascual@fis.ucm.es 
 
Abstract 
This work proposes a case-based classifier to tackle 
the gene/protein mention problem in biomedical lit-
erature. The so called gene mention problem con-
sists of the recognition of gene and protein entities in 
scientific texts. A classification process aiming at 
deciding if a term is a gene mention or not is carried 
out for each word in the text. It is based on the selec-
tion of the best or most similar case in a base of 
known and unknown cases. The approach was 
evaluated on several datasets for different organisms 
and results show the suitability of this approach for 
the gene mention problem. 
1 Introduction 
This paper proposes a new method to the gene 
mention problem by using a case-based reasoning 
approach that performs a binary classification 
(gene mention or not) for each word in a text. In a 
first step cases are stored in two bases (known and 
unknown cases), followed by a search in these 
bases for the case most similar to the problem. The 
classification decision is given by the class of the 
case selected. The system was developed using 
Java and MySQL technologies and is available for 
download as part of the Moara project1.  
2 Proposed method 
The method here proposed identifies gene men-
tions in a text by means of classifying each token 
                                                          
1 http://biocomp.cnb.csic.es/~mlara/moara/index.html 
into two possible classes: gene mention or not. The 
system consists of two main steps: the construction 
of the case bases, and the testing phase, when the 
test dataset is presented to the system to identify 
the possible mentions. The words extracted from 
the training documents were the tokens used to 
construct the two case bases, one for known cases 
and the other for unknown cases, as proposed for 
the part-of-speech tagging problem in (Daelemans, 
Zavrel, Berck, & Gillis, 1996). 
The known cases are the ones used by the sys-
tem to classify those words that are not new, i.e. 
those that have were present in the training dataset. 
The attributes used to represent a known case are 
the word itself, the class of the word (if it is a gene 
mention or not), and the class of the preceding 
word (if it is a gene mention or not). 
The system uses a second case base to decide 
about words that are unknown to the system, i.e. 
those that are not present in the training set. The 
attributes of the unknown cases were the shape of 
the word, the class of the word (if it is a gene men-
tion or not), and the class of the preceding word (if 
it is a gene mention or not). Note that instead of 
saving the word itself, a shape of the word is kept 
in order to allow the system to be able to classify 
unknown words by means of looking for cases 
with similar shape. The shape of the word is given 
by its transformation in a set of symbols according 
to the type of character found.  
In the construction of cases, each word repre-
sents a single case, and in order to account for 
repetitions, the frequency of the case is incre-
mented to indicate the number of times that it ap-
pears in the training dataset. The training 
108
documents are read twice, one in the forward (from 
left to right), and one in the backward (from right 
to left) directions, in order to allow a more variety 
of cases.  This is important as the classification of 
a token may be influenced by its preceding and 
following words.  
CBR-Tagger has also been trained with addi-
tional corpora in order to better extract mentions 
from different organisms. These extra corpora are 
the datasets for gene normalization of the BioCrea-
tive task 1B (Hirschman, Colosimo, Morgan, & 
Yeh, 2005) for to yeast, mouse and fly and the 
BioCreative 2 Gene Normalization task (Morgan & 
Hirschman, 2007) for human.  
In the classification procedure, the text is token-
ized and a sliding window is applied first in the 
forward and then in the backward direction. In 
each case, the system keeps track of the class of 
the preceding token (false at the beginning), gets 
the shape of the token and tries to find in the bases 
a case most similar to it. The search procedure is 
divided in two parts, for the known and unknown 
cases. Priority is always given to the known cases 
since it saves the word exactly as they appeared in 
the training documents and the classification may 
be more precise than using the unknown cases.  
A token already classified as positive by the 
forward reading may be used for the backward 
reading as preceding class and might help recog-
nizing mentions composed by many tokens that 
would not have been totally recognized by one of 
the reading procedures only. After the identifica-
tion of the best case for each token, some post-
processing procedures are executed to check 
boundaries (for mentions composed of more than 
one token) as well as abbreviations and corre-
sponding full names. 
3 Results 
The results obtained with the BioCreative 2 gene 
mention task for the CBR-Tagger are shown in 
Table 1 along with the best result of the competi-
tion. Results are showed according to the datasets 
used for the training of the CBR-tagger: BioCrea-
tive 2 Gene Mention task (Wilbur, Smith, & Ta-
nabe, 2007) corpus only (CbrBC2), and the 
combination of it with the BioCreative task 1B 
gene normalization corpus (Hirschman et al, 2005) 
for the yeast (CbrBC2y), mouse (CbrBC2m), fly 
(CbrBC2f) and the three of them (CbrBC2ymf). 
 
Taggers P R FM 
CbrBC2 77.8 75.9 76.9 
CbrBC2y 82.7 52.6 64.7 
CbrBC2m 83.1 47.1 60.1 
CbrBC2f 82.0 65.9 73.0 
CbrBC2ymf 82.5 39.7 53.6 
Best BC2 result 88.5 86.0 87.2 
Table 1: Results for the BC2 gene mention task. 
 
CBR-Tagger has also been applied to the gene 
normalization problem in conjunction with two 
other available taggers: Abner2 and Banner3. Table 
2 summarizes the best mix of taggers configuration 
for each organism. Detailed results may be found 
at the author?s research page4. 
 
Organism Best configuration 
Yeast Abner+CbrBC2 
Mouse Abner+CbrBC2m 
Fly CbrBC2f 
Human Banner+CbrBC2ymf 
Table 2: Best taggers for each organism. 
Acknowledgments 
This work has been partially funded by the Spanish 
grants BIO2007-67150-C03-02, S-Gen-0166/2006, 
TIN2005-5619. APM acknowledges the support of 
the Spanish Ram?n y Cajal program. The authors 
acknowledge support from Integromics, S.L. 
References  
Daelemans, W., Zavrel, J., Berck, P., & Gillis, S. 
(1996). MBT: A Memory-Based Part of Speech Tag-
ger-Generator. Paper presented at the Fourth Work-
shop on Very Large Corpora, Copenhagen, Denmark. 
Hirschman, L., Colosimo, M., Morgan, A., & Yeh, A. 
(2005). Overview of BioCreAtIvE task 1B: normal-
ized gene lists. BMC Bioinformatics, 6 Suppl 1, S11. 
Morgan, A., & Hirschman, L. (2007). Overview of Bio-
Creative II Gene Normalization. Paper presented at 
the Second BioCreative Challenge Evaluation Work-
shop, Madrid-Spain. 
Wilbur, J., Smith, L., & Tanabe, L. (2007). BioCreative 
2. Gene Mention Task. Paper presented at the Second 
BioCreative Challenge Evaluation Workshop, Madrid, 
Spain. 
                                                          
2 http://pages.cs.wisc.edu/~bsettles/abner/ 
3 http://banner.sourceforge.net/ 
4 http://biocomp.cnb.csic.es/~mlara/mention.html 
109
Proceedings of the Workshop on BioNLP: Shared Task, pages 68?76,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extraction of biomedical events using case-based reasoning 
 
 
Mariana L. Neves Jos? M. Carazo 
Biocomputing Unit Biocomputing Unit 
Centro Nacional de Biotecnolog?a - CSIC Centro Nacional de Biotecnolog?a - CSIC 
C/ Darwin 3, Campus de Cantoblanco,  
28049, Madrid, Spain 
C/ Darwin 3, Campus de Cantoblanco,  
28049, Madrid, Spain 
mlara@cnb.csic.es carazo@cnb.csic.es 
 
Alberto Pascual-Montano 
Departamento de Arquitectura de Computadores 
Universidad Complutense de Madrid, Facultad de 
Ciencias F?sicas 
28040, Madrid, Spain 
pascual@fis.ucm.es 
 
Abstract 
The BioNLP?09 Shared Task on Event Extrac-
tion presented an evaluation on the extraction 
of biological events related to genes/proteins 
from the literature. We propose a system that 
uses the case-based reasoning (CBR) machine 
learning approach for the extraction of the enti-
ties (events, sites and location). The mapping 
of the proteins in the texts to the previously ex-
tracted entities is carried out by some simple 
manually developed rules for each of the argu-
ments under consideration (cause, theme, site 
or location). We have achieved an f-measure of 
24.15 and 21.15 for Task 1 and 2, respectively. 
1 Introduction 
The increasing amount of biological data gener-
ated by the high throughput experiments has 
lead to a great demand of computational tools to 
process and interpret such amount of informa-
tion. The protein-protein interactions, as well as 
molecular events related to one entity only, are 
key issues as they take part in many biological 
processes, and many efforts have been dedicate 
to this matter. For example, databases are avail-
able for the storage of such interaction pairs, 
such as the Molecular INTeraction Database 
(Chatr-aryamontri et al, 2007) and IntAct 
(Kerrien et al, 2007). 
In the field of text mining solutions, many ef-
forts have been made. For example, the Bio-
Creative II protein-protein interaction (PPI) task 
(Krallinger, Leitner, Rodriguez-Penagos, & Va-
lencia, 2008) consists of four sub-tasks, includ-
ing the extraction of the protein interaction pairs 
in full-text documents, achieving an f-measure 
of up to 0.30. The initiative of annotation of 
both Genia corpus (J. D. Kim, Ohta, & Tsujii, 
2008) and BioInfer (Pyysalo et al, 2007) is an-
other good example. 
The BioNLP?09 Shared Task on Event Ex-
traction (J.-D. Kim, Ohta, Pyysalo, Kano, & 
Tsujii, 2009) proposes a comparative evaluation 
for the extraction of biological events related to 
one or more gene/protein and even other types 
of entities related to the localization of the re-
ferred event in the cell. The types of events that 
have been considered in the shared task were 
localization, binding, gene expression, transcrip-
tion, protein catabolism, phosphorylation, regu-
lation, positive regulation and negative 
regulation. A corpus that consisted of 800, 150 
and 260 PubMed documents (title and abstract 
text only) was made available for the training, 
development test and testing datasets, respec-
tively. For all documents, the proteins that took 
part in the events were provided. 
The shared task organization proposed three 
tasks. Task 1 (Event detection and characteriza-
tion) required the participants to extract the 
events from the text and map them to its respec-
68
tive theme(s), as an event may be associated to 
one or more themes, e.g. binding. Also, some 
events may have only a gene/protein as theme, 
e.g. protein catabolism, while some other may 
be also associated to another event, e.g. regula-
tion events. Task 2 (Event argument recognition) 
asked the participants to provide the many ar-
guments that may be related to the extracted 
event, such as its cause, that may be an anno-
tated or one of the previously extracted events. 
Other arguments include site and localization, 
which should be first extracted from the texts by 
the system, as they do not come annotated in the 
documents. Task 3 (Recognition of negation and 
speculations) evaluates the presence of negations 
and speculation related to the previously ex-
tracted events. 
Our group has participated in this shared task 
with a system implemented with the case-based 
reasoning (CBR) machine learning technique   
as well as some manual rules. We have pre-
sented results for tasks 1 and 2 exclusively. The 
system described here is part of the Moara pro-
ject1 and was developed in Java programming 
language and use MySQL database. 
2 Methods  
Case-based reasoning (CBR) (Aamodt & Plaza, 
1994) is the machine learning method that was 
used for extracting the terms and events here 
proposed and consists of first learning cases 
from the training documents, by means of saving 
them in a base of case, and further retrieving a 
case the most similar to a given problem during 
the testing step, from which will be given the 
final solution, hereafter called ?case-solution?. 
One of the advantages of the CBR algorithm is 
the possibility of getting an explanation of why 
to a given token has been attributed a certain 
category, by means of checking the features that 
compose the case-solution. Additionally, and 
due to the complexity of the tasks, a rule-based 
post-processing step was built in order to map 
the previously extracted terms and events among 
themselves.  
 
 
 
                                                          
1 http://moara.dacya.ucm.es 
2.1 Retaining the cases 
In this first step, documents of the training data-
set are tokenized according to spaces and punc-
tuations. The resulting tokens are represented in 
the CBR approach as cases composed of some 
predefined features that take into account the 
morphology and grammatical function of the 
tokens in the text as well as specific features 
related to the problem under consideration. The 
resulting cases are then stored in a base of case 
to be further retrieved (Figure 1). 
 
 
Figure 1: Training step in which cases are repre-
sented by some pre-defined features and further 
saved to a base. 
 
Regarding the features that compose a case, 
these were the ones that were considered during 
the training and development phases: the token 
itself (token); the token in lower case (lower-
case); the stem of the token (stem); the shape of 
the token (shape); the part-of-speech tag 
(posTag); the chunk tag (chunkTag); a biomedi-
cal entity tag (entityTag); the type of the term 
(termType); the type of the event (eventType); 
and the part of the term in the event (eventPart). 
The stem of a token was extracted using an 
available Java implementation2 of the Porter al-
gorithm (Porter, 1980), while the part-of-speech, 
chunk and bio-entity tags were taken from the 
GENIA Tagger (Tsuruoka et al, 2005). 
The shape of a token is given by a set of char-
acters that represent its morphology: ?a? for 
lower case letters, ?A? for upper case letters, ?1? 
for numbers, ?g? for Greek letters, ?p? for stop-
                                                          
2 http://www.tartarus.org/~martin/PorterStemmer 
69
words3, ?$? for identifying 3-letters prefixes or 
suffixes or any other symbol represented by it-
self. Here are some few example for the shape 
feature: ?Dorsal? would be represented by ?Aa?, 
?Bmp4? by ?Aa1?, ?the? by ?p?, ?cGKI(alpha)? 
by ?aAAA(g)?, ?patterning? by ?pat$a? (?$? 
symbol separating the 3-letters prefix) and ?ac-
tivity? by ?a$vity? (?$? symbol separating the 4-
letters suffix). No repetition is allowed in the 
case of the ?a? symbol for the lower case letters. 
 
 
Figure 2: Example of the termType, eventType and 
partEvent features. 
 
The last three features listed above are specific 
to the event detection task and were extracted 
from the annotation files (.a1 and .a2) that are 
part of the corpus. The termType feature is used 
to identify the type of the term in the event prob-
lem, and it is extracted from the term lines of 
both annotation files .a1 and .a2, i.e. the ones 
which the identifiers starts with a ?T?. The 
eventType features represent the event itself and 
it is extracted from the event lines of .a2 annota-
tion file, i.e. the ones that starts with an ?E?. Fi-
nally, eventPart represents the token according 
to its role, i.e. entity, theme, cause, site and loca-
tion. The termType, eventType and eventPart 
features are the hereafter called ?feature-
problem?, the features that are unknown to the 
system in the testing phase and which values are 
to be given by the case-solution. Figure 2 illus-
trate one example of these features for an extract 
of the annotation of the document ?1315834? 
from the training dataset. 
Usually, one case corresponds for each token 
of the documents in the training dataset. How-
ever, more than one case may be created from a 
token, as well as none at all, depending on the 
predefined features. For example, some tokens 
may derive in more than one case due to the 
shape feature, as for example, ?patterning? 
                                                          
3 
http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/ 
(?pat$a?, ?a$ing?, ?a?). Also, according to the 
retaining strategy, some tokens may be associ-
ated to no case at all, for example, by restricting 
the value of a determined feature as the retaining 
strategy. In order to reduce the number of re-
tained cases, and consequently reduce the further 
retrieving time, only those tokens related to an 
event are retained, i.e., tokens with not null 
value for the termType feature. 
The text of a document may be read in the 
forward or backward direction during the train-
ing step, and even combining both of them 
(Neves, Chagoyen, Carazo, & Pascual-Montano, 
2008). Here, we have considered the forward 
direction exclusively. Also, another important 
point is the window of tokens under considera-
tion when setting the features of a case, if taking 
into account only the token itself or also the sur-
rounding tokens, the ones which come before or 
after it. Here we consider a window of (-1,0), 
i.e., for each token, we get the feature of the to-
ken itself and of the preceding one, exclusively.  
 
Training Testing Features / Tokens 
-1 0 -1 0 
stem 9 9 9 9 
shape  9  9 
posTag 9 9 9 9 
chunkTag     
entityTag 9 9 9 9 
termType 9 9 9 9 
eventType 9 9 9  
partEvent 9 9 9  
Table 1: Selected features in the training and testing 
steps for the tokens ?0? and ?-1?. The last three fea-
tures are the ones to be inferred.  
 
Many experiments have been carried out in or-
der to choose the best set of features (Table 1). 
The higher the number of features under consid-
eration, the greater is the number of cases to be 
retained and the higher is the time needed to 
search for the case-solution. He relies therefore 
the importance of choosing a small an efficient 
set of features. For this reason, the shape fea-
tures has not been considered for the preceding 
token (-1) in order to reduce the number of 
cases, as this shape usually result in more than 
one case per token. The termType feature is at 
the same time known and unknown in the testing 
step. It is know for the protein terms but is un-
70
known for the remaining entities (events, sites 
and locations).  
By considering these features for the 800 
documents in the training set, about 26,788 
unique cases were generated. It should be noted 
that no repetition of cases with the same values 
for the features are allowed, instead a field for 
the frequency of the case is incremented to keep 
track of the number of times that it has appeared 
during the training phase. The frequency range 
goes from 1 (more than 22,000 cases) to 238 
(one case only). 
2.2 Retrieving a case  
When a new document is presented to the sys-
tem, it is first read in the forward direction and 
tokenized according to space and punctuation 
and the resulting tokens are mapped to cases of 
features, exactly as discussed in the retaining 
step. The only difference here is the set of fea-
ture (cf. Table 1), as some of them are unknown 
to the system and are the ones to be inferred 
from the cases retained during the training step.  
 
 
Figure 3: Retrieval procedure to choose the most 
case-solution with higher frequency and based on 
MMF and MFC parameters.  
 
For each token, the system first creates a case 
(hereafter called ?case-problem?) based on the 
testing features and proceeds to search the base 
of cases for the case-solution the most similar to 
this case-problem (Figure 3). It should be noted 
that a token may have more than one case-
problem, depending of the values of the shape 
feature. The best case-solution among the ones 
found by the system will be the one with the 
higher frequency. The system always tries to 
find a case-solution with the higher number of 
features that have exactly the same value of the 
case-problem?s respective features. The stem is 
the only mandatory feature which value must be 
always matched between the case-problem and 
the case-solution. The value of the two features-
problem (eventType and partEvent) will be 
given by the values of the case-solution?s re-
spective features. If no case solution is found, 
the token is considered of not being related to 
the event domain in none of its parts (entity, 
theme, cause, etc.).  
Two parameters have been taken into consid-
eration in the retaining strategy: the minimum 
matching feature (MMF) and the minimum fre-
quency of the case (MFC). The first one set the 
minimum features that should be matched be-
tween the case-problem and the case-solution, as 
the higher the number of equal features between 
theses cases, the more precise is the decision 
inferred from the case-solution.  
On the other hand, the MFC parameter re-
stricts the cases that are to be considered by the 
search strategy, the ones with frequency higher 
than the value specified by this parameter. The 
higher the minimum frequency asked for a case, 
the lower is the number of cases under consid-
eration and the lower is the time for obtaining 
the case-solution. From the 26,788 cases we 
have retained during the training phase, about 
22,389 of them appeared just once and would 
not be considered by the searching procedure if 
the MFC parameter was set to 2, for example,  
therefore reducing the searching time.  
Experiments have been carried out in order to 
decide the values for both parameters and it re-
sulted that a better performance is achieved (cf. 
3) by setting the MFC to a value higher than 1. 
On the other hand, experiments have shown that 
the recall may decrease considerably when re-
stricting the MMF parameter. 
By repeating this procedure for all the tokens 
of the document, the latter may be then consid-
ered as being tagged with the event entities. 
However, in order to construct the output file 
required by the shared task organization, some 
manual rules have been created in order to map 
the events mapped to its respective arguments, 
as described in the next section. 
2.3 Post-processing rules 
For the tasks 1 and 2, the participants were 
asked to output the events present in the pro-
vided texts along with their respective argu-
ments. The events have been already extracted 
in the previous step; the tokens that were tagged 
as ?Entity? for the ?partEvent? feature (cf. Fig-
71
ure 2), hereafter called ?event-entity?. This en-
tity is the start point from which to search for the 
arguments which are incrementally extracted 
from the text in the following order: theme, 
theme 2, cause, site and location. Figure 4 re-
sumes the rules for each of the arguments. 
 
 
Figure 4: Resume of the post-processing rules for 
each type of argument.  
 
Themes: The theme-candidates for an event-
entity are the annotated proteins (.a1 file) as well 
as the events themselves, in the case of the regu-
lation, positive regulation and negative regula-
tion events. The first step is then to try to map 
each event to its theme and in case that no theme 
is found, the event is not considered anymore by 
the system and it is not printed to the output file. 
The theme searching strategy starts from the 
event-entity and consists of reading the text in 
both directions alternatively, one token in the 
forward direction followed by one token in the 
backward direction until a theme-candidate is 
found (Figure 5). The system halts if the end of 
the sentence is found or if the specified number 
of tokens in each direction is reached, 20 for the 
theme. By analyzing some of the false negatives 
returned from the experiments with the devel-
opment dataset, we have learned that few events 
are associated to themes present in a different 
sentence and although aware these cases, we 
have decided to restrict the searching to the sen-
tence boundaries in order to avoid a high num-
ber of false positives.  
In the case of a second theme, allowed for 
binding events only, a similar searching strategy 
is carried out, except that here the system reads 
up of 10 tokens in each direction, starting from 
the theme entity previously extracted. 
Cause: The cause-candidates are also the an-
notated proteins and, starting from the event-
entity, a similar search is carried out, restricted 
up to 30 tokens in each direction and to the 
boundaries of the same sentence. This procedure 
is carried out for the regulation, positive regula-
tion and negative regulation events only and the 
only extra restriction is that the candidate should 
not be the protein already assigned as theme. If 
no candidate is found, the system considers that 
there is no cause associated to the event under 
consideration. 
Site and Location: Here the candidates are 
the tokens tagged with the values of ?Entity? for 
the termType feature, and ?Site? and ?Location? 
for the partEvent feature, respectively. The 
search for the site is carried out for the binding 
and phosphorylation events and the location 
search for the localization event only. The pro-
cedure is restricted to the sentence boundaries 
and up to 20 and 30 tokens, respectively, starting 
from the event-entity. Once again, if not candi-
date is found, the system consider that there is 
no site or location associated to the event under 
consideration. 
 
 
Figure 5: Contribution of each class of error to the 
275 false positives analyzed here. 
3 Results  
This section presents the results of the experi-
ments carried out with the development and the 
blind test datasets as well as an analysis of the 
false negatives and false positives. Results here 
will be presented for tasks 1 and 2 in terms of 
precision, recall and f-measure. 
Experiments have been carried out with the 
development dataset in order to decide the best 
value of the MMF and MFC parameters (cf. 
2.2). Figure 6 shows the variation of the F-
measure according to both parameters for the 
values of 1, 3, 4, 5, 6, 7 and 8 for MMF; and 1, 
2, 5, 10, 15, 20 and 50 for MFC. 
Usually, recall is higher for a low value of 
MFC, as the searching for the case-solution is 
72
carried out over a greater number of cases and 
the possibility of finding a good case-problem is 
higher. On the other hand, precision increases 
when few cases are under considered by the 
search strategy, as fewer decisions are taken and 
the cases-solution have usually a high frequency, 
avoiding decision based on ?weak? cases of fre-
quency 1, for example.  
Figure 6 shows that the best value for MFC 
ranges from 2 to 20 and for MMF from 5 to 7 
and the best f-measure result is found for the 
values of 2 and 6 for these parameters (f2m6), 
respectively. As these experiments have been 
carried out after the deadline of the test dataset, 
the run that was submitted as the final solution 
was the one with the values of 2 and 1 for the 
MFC and MMF parameters (f2m1), respectively. 
Table 3 and 4 resumes the results obtained for 
the test dataset with the configuration that was 
submitted (f2m1), and the best one (f2m6) after 
accomplishing the experiments above described. 
Results have slightly improved by only trying to 
choose the best values for the parameters here 
considered.  
 
F-Measure
7
9
11
13
15
17
19
21
23
1 3 4 5 6 7 8
Minimum matching features
1 2 5 10 15 20 50
 
Figure 6: F-Measure for the development dataset in 
terms of the MFC (curves) and the MMF (x-axis). 
 
An automatic analysis of the false positives and 
false negatives has been performed for the de-
velopment dataset and for the results obtained 
with the final submission (f2m1), a total of 2502 
false positives and 1300 false negatives. We 
have found out that the mistakes are related 
mainly to the retrieving of the case-solution and 
to the mapping of an event to its arguments. The 
mistakes have been classified in seven groups 
described below and figures 7 and 8 show the 
percent contribution of each class for the false 
positives and false negatives, respectively. 
Events composed of more than one token 
(1): this mistake happens when the system is 
able to find the event with its correct type and 
arguments but with only part of its tokens, such 
as ?regulation? instead of ?up-regulation? and 
?reduced? or ?levels? instead of ?reduced lev-
els?, both in document 10411003. This is mainly 
due to our tokenization strategy of separating the 
tokens according to all punctuation and symbols 
(including hyphens) and also due to the evalua-
tion method that seems not consider alternatives 
to the text of an event. This mistake always re-
sults in one false positive and one false negative. 
 
tasks /  
results recall precision f-measure 
(f2m1) 28.63 20.88 24.15 task 1 
(f2m6) 27.18 23.92 25.45 
(f2m1) 25.02 18.32 21.15 task 2 
(f2m6) 24.49 21.63 22.97 
Table 3: Results for the test dataset (tasks 1 and 2). 
 
(f2m1) (f2m6) Results /  
Events p r fm p r fm 
prot. catab. 78.6 55.0 64.7 71.4 55.6 65.5 
phosphoryl. 49.6 56.1 52.7 46.0 55.2 50.2 
transcript. 48.9 19.8 28.1 38.7 29.6 33.5 
neg. reg. 9.8 7.9 8.8 7.9 7.7 7.8 
pos. reg. 10.0 6.6 7.9 10.2 8.0 9.0 
regulation 8.6 4.5 5.9 7.5 5.3 6.3 
localizat. 28.2 42.9 34.0 23.3 48.9 33.3 
gene expr. 51.8 55.1 53.4 52.6 61.2 56.6 
binding 19.5 12.1 14.9 22.4 14.4 17.5 
Table 4: Results by event for Task 2 on test dataset. 
 
Events and arguments in different sentences 
of the text (2):  as we already discussed in sec-
tion 2.3, our arguments searching strategy is re-
stricted to the boundaries of the sentence. Some 
examples of this mistake may be found in 
document 10395645 in which two events of the 
token ?activation [1354-1364]? is mapped to the 
themes ?caspase-6 [1190-1199]? and ?CPP32 
[1165-1170]?, both located in a different sen-
tence. This mistake usually affects only the false 
negatives but may cause also a false positive if 
the system happens to find a valid (wrong) ar-
73
gument in the same sentences for the event un-
der consideration. 
 
False Positives
case 
decision (3); 
74,3
composed 
tokens (1); 
5,2
site/location 
detection 
(7); 1,6
theme 
detection 
(5); 14,6
cause 
detection 
(6); 1,6
event type 
(4); 2,7
 
Figure 7: Percent contribution of each error to the 
false positives. 
 
False Negatives
theme 
detection 
(5); 56,2
site/location 
detection 
(7); 0,7
cause 
detection 
(6); 4,2
event type 
(4); 10,0
different 
sentences 
(2); 1,4
composed 
tokens (1); 
10,4
case 
decision (3); 
17,2
 
Figure 8: Percent contribution of each error to the 
false negatives. 
 
Decision for a case (3): this class of error is due 
to the selection of a wrong case-solution and we 
include in this class mistakes due to two situa-
tions: when the system fails to find any case-
solution for an event token (false negative) or 
when a case-solution is found for a non-event 
token (false positive). The first situation is only 
dependent of the searching strategy and its two 
parameters (MMF and MFC) while the second 
one is also related to the post-processing step, if 
the latter succeeds to find a theme for the incor-
rectly extracted event. An example of a false 
negative that falls in this group is ?dysregulation 
[727-740]? from document 10229231 that failed 
to be mapped to a case-solution. Regarding the 
false positives, this class of mistake is the major-
ity of them and it is due to the low precision of 
the system that frequently is able to find cases-
solution associated to tokens that are not events 
at all, such as the token ?transcript [392-402]? of 
document 10229231. It should be noted that the 
incorrect association of a token to a case-
solution does not result in a false positive a pri-
ori, but only if the post-processing step happen 
to find a valid theme to it, a mistake further de-
scribed in group 5. 
Wrong type of the event (4): this class of 
mistake is also due to the wrong selection of a 
case-solution, but the difference here is that the 
token is really an event, but the case-solution is 
of the wrong type, i.e. it has a wrong value for 
the eventType feature. The causes of this mis-
take are many, such as, the selection of features 
(cf. Table 1) or the value of the MFC parameter 
that may lead to the selection of a wrong but 
more frequent case. We also include in this 
group the few false negatives mistakes in which 
a token is associated to more than one type of 
event in the gold-standard, such as the token 
?Overexpression [475-489]? from document 
10229231 that is associated both to a Gene Ex-
pression and to a Positive Regulation event. One 
way of overcome it would be to allow the sys-
tem to associated more than one case to a token, 
taking the risk of decreasing the precision. 
Theme detection (5): in this group falls more 
than half of the false negatives and we include 
here only those mistakes in which the token was 
correctly associated to a case-solution of the cor-
rect type. These mistakes may be due to a vari-
ety of situations related to the theme detection, 
such as: the association of the event to another 
event when it should have been done to a protein 
or vice-versa (for the regulation events); the 
mapping of a binding event to one theme only 
when it should have been two theme or vice-
versa; the association of the event to the wrong 
protein theme, especially when there is more 
than one nearby; and even not being able to find 
any theme at all. Also, half of theses mistakes 
happen when an event is associated to more than 
one theme separately, not as a second theme. For 
example, the token ?associated [278-288]?, from 
document 10196286, is associated in the gold 
standard to three themes ? ?tumor necrosis fac-
tor receptor-associated factor (TRAF) 1 [294-
351]?, ?2 [353-354]? and ?3 [359-360]? ? and 
74
we were only able to extract the first of them. 
This is due to the fact that we restrict the system 
to search only one ?first? and one ?second? 
theme for each event. 
Cause detection (6): similar to the previous 
class, these mistakes happens when associating a 
cause to an event (regulation events only) when 
there is no cause related to it or vice-versa. For 
example, in document 10092805, the system has 
correctly mapped the token ?decreases [1230-
1239]? to the theme ?4E-BP1 [1240-1246]? but 
also associated to it an inexistent cause ?4E-BP2 
[1315-1321]?. The evaluation of Task 2 does not 
allow the partial evaluation of an event and 
therefore a false positive and a false negative 
would be returned for the example above. 
Site/Location detection (7): this error is 
similar to the previous one but related only to 
binding, phosphorylation and localization 
events, when the system fails to associate a site 
or a location to an event or vice-versa. For ex-
ample, in document 10395671, the token ?phos-
phorylation [1091-1106]? was correctly mapped 
to the theme ?Janus kinase 3 [1076-1090]? but 
was also associated to an inexistent site ?DNA 
[1200-1203]?. Once again, the evaluation of 
Task 2 does not allow the partial evaluation of 
the event and a false positive and a false nega-
tive would be returned. 
We have also carried out an evaluation of our 
own in order to check the performance of our 
system only on the extraction the entities (event, 
site and location), not taking into account the 
association to the arguments. Table 5 resumes 
the values of precision, recall and f-measure for 
each type of term. The high recall confirm that 
most of the entities were successful extracted 
although the precision is not always satisfactory, 
proving that the tagging of the entities is not as 
hard a task as it is the mapping of the arguments. 
Additional results and more a detailed analysis 
of the errors may be found at Moara page4. 
4 Conclusions 
Results show that our system has performed 
relatively well using a simple methodology of a 
machine learning based extraction of the entities 
and manual rules developed for the post-
                                                          
4 http://moara.dacya.ucm.es/results_shared_task.html 
processing step. The analysis of the mistakes 
presented here confirms the complexity of the 
tasks proposed but not the extraction of the 
event terms (cf. Table 5). 
We consider that the part of our system that 
requires most our attention is the retrieval of the 
case-solution and the theme detection of the 
post-processing step, in order to increase the 
precision and recall, respectively. The decision 
of searching for a second theme and of associat-
ing a single event separately to more than one 
theme is hard to be accomplished by manual 
rules and could better be learned automatically 
using a machine learning algorithm. 
 
(f2m1) (f2m6) Events 
p r fm p r fm 
prot. catab. 70.8 89.5 79.1 69.6 84.2 76.2 
phosphoryl. 75.0 94.7 83.7 79.1 89.5 84.0 
transcript. 22.7 75.9 34.9 36.4 74.6 48.9 
neg. reg. 26.4 56.5 36.0 25.3 43.5 32.0 
pos. reg. 24.3 63.7 35.2 26.5 59.1 36.6 
regulation 20.8 65.9 31.7 22.1 52.5 31.1 
localizat. 47.7 79.5 59.6 49.1 66.7 56.5 
gene expr. 46.5 83.4 59.7 50.8 80.2 62.2 
binding 29.7 71.1 41.9 29.7 64.4 40.7 
entity 12.5 55.3 20.4 16.8 50.0 25.1 
TOTAL 27.5 69.2 39.4 30.9 62.9 41.4 
Table 5: Evaluation of the extraction of the event and 
site/location entities for the development dataset. 
 
The automatic analysis of the false positive and 
false negative mistakes is a hard task since no 
hint is given for the reason of the mistake by the 
evaluation system, if due to the event type or to 
wrong theme, an incorrectly association to an 
event or even a missing cause or site.  
Acknowledgments 
This work has been partially funded by the 
Spanish grants BIO2007-67150-C03-02, S-Gen-
0166/2006, PS-010000-2008-1, TIN2005-5619. 
APM acknowledges the support of the Spanish 
Ram?n y Cajal program. The authors acknowl-
edge support from Integromics, S.L. 
References  
Aamodt, A., & Plaza, E. (1994). Case-Based Reason-
ing: Foundational Issues, Methodological Varia-
tions, and System Approaches. AI 
Communications, 7(1), 39-59. 
75
Chatr-aryamontri, A., Ceol, A., Palazzi, L. M., 
Nardelli, G., Schneider, M. V., Castagnoli, L., et 
al. (2007). MINT: the Molecular INTeraction da-
tabase. Nucleic Acids Res, 35(Database issue), 
D572-574. 
Kerrien, S., Alam-Faruque, Y., Aranda, B., Bancarz, 
I., Bridge, A., Derow, C., et al (2007). IntAct--
open source resource for molecular interaction 
data. Nucleic Acids Res, 35(Database issue), 
D561-565. 
Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii, 
J. i. (2009). Overview of BioNLP'09 Shared Task 
on Event Extraction. Paper presented at the Pro-
ceedings of Natural Language Processing in Bio-
medicine (BioNLP) NAACL 2009 Workshop, 
Boulder, CO, USA. 
Kim, J. D., Ohta, T., & Tsujii, J. (2008). Corpus an-
notation for mining biomedical events from litera-
ture. BMC Bioinformatics, 9, 10. 
Krallinger, M., Leitner, F., Rodriguez-Penagos, C., & 
Valencia, A. (2008). Overview of the protein-
protein interaction annotation extraction task of 
BioCreative II. Genome Biol, 9 Suppl 2, S4. 
Neves, M., Chagoyen, M., Carazo, J. M., & Pascual-
Montano, A. (2008). CBR-Tagger: a case-based 
reasoning approach to the gene/protein mention 
problem. Paper presented at the Proceedings of 
the BioNLP 2008 Workshop at ACL 2008, Co-
lumbus, OH, USA. 
Porter, M. (1980). An algorithm for suffix stripping. 
Program, 14(3), 130-137. 
Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., 
Boberg, J., Jarvinen, J., et al (2007). BioInfer: a 
corpus for information extraction in the biomedi-
cal domain. BMC Bioinformatics, 8, 50. 
Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T., 
McNaught, J., Ananiadou, S., et al (2005). De-
veloping a Robust Part-of-Speech Tagger for 
Biomedical Text. Paper presented at the Advances 
in Informatics - 10th Panhellenic Conference on 
Informatics. 
 
 
76
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 628?635, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
WBI-DDI: Drug-Drug Interaction Extraction using Majority Voting
Philippe Thomas Mariana Neves Tim Rockta?schel Ulf Leser
Humboldt-Universita?t zu Berlin
Knowledge Management in Bioinformatics
Unter den Linden 6
Berlin, 10099, Germany
{thomas,neves,trocktae,leser}@informatik.hu-berlin.de
Abstract
This work describes the participation of the
WBI-DDI team on the SemEval 2013 ? Task
9.2 DDI extraction challenge. The task con-
sisted of extracting interactions between pairs
of drugs from two collections of documents
(DrugBank and MEDLINE) and their clas-
sification into four subtypes: advise, effect,
mechanism, and int. We developed a two-step
approach in which pairs are initially extracted
using ensembles of up to five different clas-
sifiers and then relabeled to one of the four
categories. Our approach achieved the sec-
ond rank in the DDI competition. For interac-
tion detection we achieved F1 measures rang-
ing from 73 % to almost 76 % depending on
the run. These results are on par or even higher
than the performance estimation on the train-
ing dataset. When considering the four inter-
action subtypes we achieved an F1 measure of
60.9 %.
1 Introduction
A drug-drug interaction (DDI) can be described as
interplay between drugs taken during joint adminis-
tration. DDIs usually lead to an increase or decrease
in drug effects when compared to isolated treatment.
For instance, sildenafil (Viagra) in combination with
nitrates can cause a potentially live-threatening de-
crease in blood pressure (Cheitlin et al, 1999). It is
therefore crucial to consider potential DDI effects
when co-administering drugs to patients. As the
level of medication generally is raising all over the
world, the potential risk of unwanted side effects,
such as DDIs, is constantly increasing (Haider et al,
2007).
Only a fraction of knowledge about DDIs is
contained in specialized databases such as Drug-
Bank (Knox et al, 2011). These structured knowl-
edge bases are often the primary resource of infor-
mation for researchers. However, the majority of
new DDI findings are still initially reported in scien-
tific publications, which results in the situation that
structured knowledge bases lag behind recently pub-
lished research results. Thus, there is an urgent need
for researchers and database curators to cope with
the fast growth of biomedical literature (Hunter and
Cohen, 2006).
The SemEval 2013 ? Task 9.2 (Extraction of
Drug-Drug Interactions from BioMedical Texts)
is a competitive evaluation of methods for ex-
tracting mentions of drug-drug interactions from
texts (Segura-Bedmar et al, 2013). For training,
the organizers provide a corpus annotated with drug-
names and interactions between them. This corpus
is composed of 572 articles collected from Drug-
Bank and 142 PubMed abstracts. Interactions are
binary (always between two drugs) and undirected,
as target and agent roles are not annotated. Fur-
thermore, the two interacting drugs are always men-
tioned within the same sentence. In contrast to
the previous DDI-challenge 2011 (Segura-Bedmar
et al, 2011), four different DDI-subtypes (advise,
effect, mechanism, and int) have been introduced.
Details about the four subclasses can be found in the
task?s annotation guideline.
628
Figure 1: Workflow developed for the SemEval 2013
Task 9.2 challenge.
2 Methods
Binary relationship extraction is often tackled as a
pair-wise classification problem, where all
(n
2
)
co-
occurring entities in a sentence are classified as in-
teracting or not. To account for the four different
subtypes of DDIs, the problem definition could be
translated into a multiclass classification problem
between all co-occurring entities.
Contrary to that, we propose a two step strat-
egy: First, we detect general drug-drug interac-
tions regardless of subtype using a multitude of dif-
ferent machine-learning methods. The output of
these methods is aggregated using a majority vot-
ing approach. Second, detected interactions are re-
classified into one of the four possible DDI cate-
gories. The latter is referred to as DDI relabeling
throughout this paper. A detailed view on the pro-
posed workflow is depicted in Figure 1.
2.1 Preprocessing
Sentences have been parsed using Charniak-Johnson
PCFG reranking-parser (Charniak and Johnson,
2005) with a self-trained re-ranking model aug-
mented for biomedical texts (McClosky, 2010). Re-
sulting constituent parse trees have been converted
into dependency graphs using the Stanford con-
verter (De Marneffe et al, 2006). In the last step, we
created an augmented XML using the open source
Corpus Sentences
Pairs
Positive Negative Total
DrugBank 5,675 3,788 22,217 26,005
MEDLINE 1,301 232 1,555 1,787
Table 1: Basic statistics of the DDI training corpus shown
for DrugBank and MEDLINE separately.
framework from Tikk et al (2010). This XML file
encompasses tokens with respective part-of-speech
tags, constituent parse tree, and dependency parse
tree information. This format has been subsequently
transformed into a related XML format1 used by two
of the utilized classifiers. Properties of the training
corpus are shown for DrugBank and MEDLINE in
Table 1.
2.2 Machine Learning Methods
Tikk et al (2010) systematically analyzed nine dif-
ferent machine learning approaches for the extrac-
tion of undirected binary protein-protein interac-
tions. This framework has been successfully applied
to other domains, such as the I2B2 relation extrac-
tion challenge (Solt et al, 2010), the previous DDI
extraction challenge (Thomas et al, 2011), and to
the extraction of neuroanatomical connectivity state-
ments (French et al, 2012).
Drug entities are blinded by replacing the entity
name with a generic string to ensure the generality
of the approach. Without entity blinding drug names
are incorporated as features, which clearly affects
generalization capabilities of a classifier on unseen
entity mentions (Pyysalo et al, 2008).
We decided to use the following methods
provided by the framework: All-paths graph
(APG) (Airola et al, 2008), shallow lin-
guistic (SL) (Giuliano et al, 2006), subtree
(ST) (Vishwanathan and Smola, 2002), subset tree
(SST) (Collins and Duffy, 2001), and spectrum tree
(SpT) (Kuboyama et al, 2007) method. The SL
method uses only shallow linguistic features, i.e.,
token, stem, part-of-speech tag and morphologic
properties of the surrounding words. APG builds
a classifier using surface features and a weighting
1https://github.com/jbjorne/TEES/wiki/
Interaction-XML
629
scheme for dependency parse tree features. The
remaining three classifier (ST, SST, and SpT) build
kernel functions based on different subtree repre-
sentations on the constituent parse tree. To calculate
the constituent?tree kernels ST and SST we used
the SVM-LIGHT-TK toolkit (Moschitti, 2006).
Before applying these methods, constituent parse
trees have been reduced to the shortest-enclosed
parse following the recommendations from Zhang
et al (2006). For a more detailed description
of the different methods we refer to the original
publications.
In addition to the PPI framework, we also
employed the general purpose relationship ex-
traction tool ?Turku Event Extraction System?
(TEES) (Bjo?rne et al, 2011), a customized version
of the case-based reasoning system Moara (Neves
et al, 2009), and a self-developed feature based
classifier which is referred to as SLW. Regarding
TEES, we have used the edge extraction function-
ality for performing relationship extraction. TEES
considers features related to the tokens (e.g., part-of-
speech tags), dependency chains, dependency path
N-grams, entities (e.g., entity types) and external re-
sources, such as hypernyms in WordNet.
Moara is a case-based reasoning system for the
extraction of relationships and events. During train-
ing, interaction pairs are converted into cases and
saved into a HyperSQL database which are re-
trieved through case similarity during the classifica-
tion. Cases are composed by the following features:
the type of the entities (e.g. Brand and Group),
the part-of-speech tag of the tokens between the two
drugs (inclusive), the tags of the shortest depen-
dency path between the two drugs, and the lemma
of the non-entity tokens of the shortest dependency
path using BioLemmatizer (Liu et al, 2012). We
also consider the PHARE ontology (Coulet et al,
2011) in the lemma feature: When a lemma matches
any of the synonyms contained in this ontology, the
category of the respective term is considered instead.
Case similarity is calculated by exact feature match-
ing, except for the part-of-speech tags whose com-
parison is based on global alignment using insertion,
deletion, and substitution costs as proposed by Spa-
sic et al (2005).
SLW is inspired by SL (Giuliano et al, 2006;
Bunescu and Mooney, 2006) and uses the Breeze2
library. We generate n-grams over sequences of
arbitrary features (e.g. POS-tags, morphological
and syntactical features) to describe the global con-
text of an entity pair. Furthermore, we calculate
features from the local context of entities, but in
addition to SL, we include domain-specific fea-
tures used for identifying and classifying pharma-
cological substances (see our paper for DDI Task
9.1 (Rockta?schel et al, 2013)). In addition, we take
the name of the classes of a pair?s two entities as
feature to capture that entities of some class (e.g.
Brand and Group) are more likely to interact than
others (e.g. Brand and Brand).
2.3 Ensemble learning
Several community competitions previously noted
that combinations of predictions from different tools
help to achieve better results than one method
alone (Kim et al, 2009; Leitner et al, 2010). More
importantly, it is well known that ensembles increase
robustness by decreasing the risk of selecting a bad
classifier (Polikar, 2006). In this work we combined
the output of several classifiers by using majority
voting. The ensemble is used to predict DDIs re-
gardless of the four different subtypes. This com-
plies with the partial match evaluation criterion de-
fined by the competition organizers.
2.4 Relabeling
To account for DDI subtypes, we compared two ap-
proaches: (a) using the subtype prediction of TEES;
(b) training a multi-class classifier (SLW) on the
available training data for DDI subtypes. We de-
cided on using TEES, as it generated superior results
over SLW (data not shown). Thus, previously identi-
fied DDIs are relabeled into one of the four possible
subtypes using the most likely interaction subtype
from TEES.
3 Results
3.1 Cross validation
In order to compare the different approaches, we
performed document-wise 10-fold cross validation
(CV) on the training set. It has been shown that such
2http://www.scalanlp.org/
630
Type Pairs Precision Recall F1
total 3,119 78.6 78.6 78.6
effect 1,633 79.8 79.1 79.4
mechanism 1,319 79.8 79.2 79.4
advise 826 77.3 76.4 76.9
int 188 68.5 80.9 74.1
Table 4: Performance estimation for relabeling DDIs.
Pairs denotes the number of instances of this type in the
training corpus.
a setting provides more realistic performance esti-
mates than instance-wise CV (S?tre et al, 2008).
All approaches have been tested using the same
splits to ensure comparability. For APG, ST, SST,
and SpT we followed the parameter optimization
strategy defined by Tikk et al (2010). For TEES
and Moara, we used the cost parameter C (50000)
and best performing features, respectively, based on
the CV results. For SL and SLW, we used the default
parameters.
We performed several different CV experiments:
First, we performed CV on the two corpora (Drug-
Bank and MEDLINE) separately. Second, data
from the other corpus has been additionally used
during the training phase. This allows us to esti-
mate the impact of additional, but potentially differ-
ent text. CV results for DrugBank and MEDLINE
are shown in Table 2 and 3 respectively.
3.2 Relabeling
Performance of relabeling is evaluated by perform-
ing 10-fold CV on the training set using the same
splits as in previous analysis. Note that this experi-
ment is solely performed on positive DDI instances
to estimate separability of the four different DDI-
subtypes. Results for relabeling are shown in Ta-
ble 4.
3.3 Test dataset
For the test set we submitted results using the fol-
lowing three majority voting ensembles. For Run 1
we used Moara+SL+TEES, for Run 2 we used
APG+Moara+SL+SLW+TEES and for Run 3 we
used SL+SLW+TEES. Due to time constraints we
did not use different ensembles for the two corpora.
We rather decided to use ensembles which achieved
generally good results for both training corpora. All
classifiers, except APG, have been retrained on the
combination of MEDLINE and DrugBank using
the parameter setting yielding the highest F1 in the
training phase. For APG, we trained two different
models: One model is trained on MEDLINE and
DrugBank and one model is trained on DrugBank
solely. The first model is applied on the MEDLINE
test set and the latter on the DrugBank test set. Esti-
mated results on the training corpus and official re-
sults on the test corpus are shown in Table 5.
4 Discussion
4.1 Training dataset
Document-wise CV results for the DrugBank corpus
show no clear effect when using MEDLINE as ad-
ditional training data. By using MEDLINE during
the training phase we observe an average decrease of
0.3 percentage points (pp) in F1 and an average in-
crease of 0.7 pp in area under the receiver operating
characteristic curve (AUC). The strongest impact
can be observed for APG with a decrease of 2.3 pp
in F1. We therefore decided to train APG mod-
els for DrugBank without additional MEDLINE
data. For almost all ensembles (with the excep-
tion of APG+SpT+SL) we observe superior results
when using only DrugBank as training data. Inter-
estingly, this effect can mostly be attributed to an
average increase of 3.3 pp in recall, whereas preci-
sion remains fairly stable between ensembles using
DrugBank solely and those with additional training
data.
In contrast for MEDLINE, all methods largely
benefit from additional training data with an aver-
age increase of 9.8 pp and 3.6 pp for F1 and AUC re-
spectively. For the ensemble based approaches, we
observe an average increase of 13.8 pp for F1when
using DrugBank data in addition.
When ranking the different methods by F1 and
calculating correlation between the two differ-
ent corpora, we observe only a weak correlation
(Kendall?s ? = 0.286, p< 1). In other words, ma-
chine learning methods show varying performance-
ranks between the two corpora. This difference is
most pronounced for SL and SpT, with four ranks
difference between DrugBank and MEDLINE. It
is noteworthy that the two corpora are not directly
631
Regular CV Combined CV
Method P R F1 AUC P R F1 AUC
SL 61.5 79.0 69.1 92.8 62.1 78.4 69.2 93.0
APG 77.2 62.6 69.0 91.5 75.9 59.8 66.7 91.6
TEES 77.2 62.0 68.6 87.3 75.5 60.9 67.3 86.9
SLW 73.7 60.0 65.9 91.3 73.4 61.2 66.6 91.3
Moara 72.1 55.2 62.5 ? 72.0 54.7 62.1 ?
SpT 51.4 73.4 60.3 87.3 52.7 71.4 60.6 87.7
SST 51.9 61.2 56.0 85.4 55.1 57.1 56.0 86.1
ST 47.3 64.2 54.2 82.3 48.3 64.3 54.9 82.7
SL+SLW+TEES 76.1 69.9 72.7 ? 75.9 65.3 70.1 ?
APG+SL+TEES 79.3 69.9 74.2 ? 79.2 65.4 71.5 ?
Moara+SL+TEES 79.9 69.6 74.2 ? 79.6 65.1 71.6 ?
Moara+SL+APG 81.4 70.6 75.5 ? 81.3 70.3 75.3 ?
APG+Moara+SL+SLW+TEES 84.0 68.1 75.1 ? 83.7 64.2 72.6 ?
APG+SpT+TEES 76.8 68.0 72.1 ? 77.1 63.4 69.6 ?
APG+SpT+SL 68.7 74.8 71.5 ? 69.7 73.8 71.6 ?
Table 2: Cross validation results on DrugBank corpus. Regular CV is training and evaluation on DrugBank only.
Combined CV is training on DrugBank and MEDLINE and testing on DrugBank. Higher F1 between these two
settings are indicated in boldface for each method. Single methods are ranked by F1.
Regular CV Combined CV
Method P R F1 AUC P R F1 AUC
TEES 70.7 36.0 44.5 82.2 59.6 46.5 51.4 84.9
SpT 37.8 38.6 34.6 78.6 42.3 55.3 47.1 80.4
APG 46.5 44.3 42.4 82.3 38.1 62.2 46.4 82.8
SST 31.3 37.7 31.8 74.1 36.7 61.7 44.9 79.5
SL 43.7 40.1 38.7 78.9 34.7 67.1 44.7 81.1
SLW 58.0 14.3 20.4 73.4 50.1 38.0 42.0 82.4
Moara 49.8 31.9 37.6 ? 45.6 43.2 41.9 ?
ST 25.2 43.8 30.1 70.5 36.1 48.3 39.8 74.2
SL+SLW+TEES 73.6 29.0 37.6 ? 55.2 52.7 53.1 ?
APG+SL+TEES 60.7 37.9 43.4 ? 49.9 62.4 54.3 ?
Moara+SL+TEES 68.0 33.0 42.2 ? 62.1 55.5 57.4 ?
Moara+SL+APG 57.7 36.7 42.4 ? 48.3 60.9 52.8 ?
APG+Moara+SL+SLW+TEES 73.3 28.3 36.8 ? 60.6 54.4 56.5 ?
APG+SpT+TEES 58.5 37.4 41.7 ? 57.5 59.2 57.1 ?
APG+SpT+SL 48.3 39.9 40.0 ? 43.6 64.3 51.0 ?
Table 3: Cross validation results on MEDLINE corpus. Regular CV is training and evaluation on MEDLINE only.
Combined CV is training on DrugBank and MEDLINE and testing on MEDLINE. Higher F1 between these two
settings are indicated in boldface for each method. Single methods are ranked by F1.
632
Evaluation
Training Test
Run 1 Run 2 Run 3 Run 1 Run 2 Run 3
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
Partial 78.7 67.3 72.6 82.9 66.4 73.7 75.2 67.6 71.2 84.1 65.4 73.6 86.1 65.7 74.5 80.1 72.2 75.9
Strict 65.7 56.1 60.5 70.0 56.0 62.2 63.0 56.7 59.7 68.5 53.2 59.9 69.5 53.0 60.1 64.2 57.9 60.9
-mechanism 61.8 49.7 55.1 68.1 50.0 57.7 59.2 50.3 54.4 72.2 51.7 60.2 74.9 52.3 61.6 65.3 58.6 61.8
-effect 68.8 57.9 62.9 71.8 57.6 63.9 66.1 57.4 61.5 63.7 57.5 60.4 63.6 55.8 59.5 60.7 61.4 61.0
-advise 64.6 60.5 62.5 68.2 59.7 63.6 61.1 61.5 61.3 73.3 53.4 61.8 74.5 55.7 63.7 69.0 58.4 63.2
-int 68.6 50.0 57.8 75.4 52.1 61.6 70.9 56.9 63.1 67.8 41.7 51.6 67.3 38.5 49.0 67.8 41.7 51.6
Table 5: Relation extraction results on the training and test set. Run 1 builds a majority voting on Moara+SL+TEES,
Run 2 on APG+Moara+SL+SLW+TEES, and Run 3 on SL+SLW+TEES. Partial characterizes only DDI detection
without classification of subtypes, whereas strict requires correct identification of subtypes as well.
comparable, as DrugBank is one order of magnitude
larger in terms of instances than the MEDLINE cor-
pus. Additionally, documents come from different
sources and it is tempting to speculate that there
might be a certain amount of domain specificity be-
tween DrugBank and MEDLINE sentences.
We tested for domain specificity by performing
cross-corpus experiments, i.e., we trained a classi-
fier on DrugBank, applied it on MEDLINE and vice
versa. When training on MEDLINE and testing
on DrugBank, we observe an average decrease of
about 15 pp in F1 in comparison to DrugBank in-
domain CV results. For the other setting, we observe
a lower decrease of approximately 5 pp in compari-
son to MEDLINE in-domain CV results.
From the current results, it seems that the doc-
uments from DrugBank and MEDLINE have dif-
ferent syntactic properties. However, this requires a
more detailed analysis of different aspects like dis-
tribution of sentence length, negations, or passives
between the two corpora (Cohen et al, 2010; Tikk
et al, 2013). We assume that transfer learning tech-
niques could improve results on both corpora (Pan
and Yang, 2010).
The DDI-relabeling capability of TEES is very
balanced with F1 measures ranging from 74.1 % to
79.4 % for all four DDI subclasses. This is unex-
pected since classes like ?effect? occur almost ten
times more often than classes like ?int? and classi-
fiers often have problems with predicting minority
classes.
4.2 Test dataset
On the test set, our best run achieves an F1 of 76 %
using the partial evaluation schema. This is slightly
better than the performance for DrugBank training
data shown in Table 2 and substantially better than
estimations for MEDLINE (see Table 3). With
F1 measures ranging between 74 % to 76 % only
minor performance differences can be observed be-
tween the three different ensembles.
When switching from partial to strict evaluation
scheme an average decrease of 15 pp in F1 can be ob-
served. As estimated on the training data, relabeling
performance is indeed very similar for the different
DDI-subtypes. Only for the class with the least in-
stances (int), a larger decrease in comparison to the
other three classes can be observed for the test set.
In general, results for test set are on par or higher
than results for the training set.
5 Conclusion
In this paper we presented our approach for the
SemEval 2013 ? Task 9.2 DDI extraction challenge.
Our strategy builds on a cascaded (coarse to fine
grained) classification strategy, where a majority
voting ensemble of different methods is initially
used to find generic DDIs. Predicted interactions
are subsequently relabeled into four different sub-
types. DDI extraction seems to be a more difficult
task for MEDLINE abstracts than for DrugBank ar-
ticles. In our opinion, this cannot be fully attributed
to the slightly higher ratio of positive instances in
DrugBank and points towards structural differences
between the two corpora.
Acknowledgments
This work was supported by the German Research
Foundation (DFG) [LE 1428/3-1] and the Federal
633
Ministry of Economics and Technology (BMWi)
[KF 2205209MS2].
References
A. Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala,
F. Ginter, and T. Salakoski. 2008. All-paths graph
kernel for protein-protein interaction extraction
with evaluation of cross-corpus learning. BMC
Bioinformatics, 9 Suppl 11:S2.
J. Bjo?rne, J. Heimonen, F. Ginter, A. Airola,
T. Pahikkala, and T. Salakoski. 2011. Extracting
Contextualized Complex Biological Events with
Rich Graph-Based Features Sets. Computational
Intelligence, 27(4):541?557.
R. C. Bunescu and R. J. Mooney. 2006. Sub-
sequence Kernels for Relation Extraction. Ad-
vances in Neural Information Processing Sys-
tems, 18:171.
E. Charniak and M. Johnson. 2005. Coarse-to-
Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL?05, pages 173?180.
M. D. Cheitlin, A. M. Hutter, R. G. Brindis, P. Ganz,
S. Kaul, R. O. Russell, and R. M. Zusman. 1999.
Use of sildenafil (viagra) in patients with cardio-
vascular disease. J Am Coll Cardiol, 33(1):273?
282.
K. Cohen, Helen L Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E Hunter.
2010. The structural and content aspects of ab-
stracts versus bodies of full text journal articles
are different. BMC Bioinformatics, 11:492.
M. Collins and N. Duffy. 2001. Convolution Kernels
for Natural Language. In Proc. of NIPS?01, pages
625?632.
A. Coulet, Y. Garten, M. Dumontier, R. Altman,
M. Musen, and N. Shah. 2011. Integration and
publication of heterogeneous text-mined relation-
ships on the semantic web. Journal of Biomedical
Semantics, 2(Suppl 2):S10.
M.C. De Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proc. of LREC
2006, pages 449?454.
L. French, S. Lane, L. Xu, C. Siu, C. Kwok, Y. Chen,
C. Krebs, and P. Pavlidis. 2012. Application and
evaluation of automated methods to extract neu-
roanatomical connectivity statements from free
text. Bioinformatics, 28(22):2963?2970.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting Shallow Linguistic Information for Re-
lation Extraction from Biomedical Literature. In
Proc. of EACL?06, pages 401?408.
S. I. Haider, K. Johnell, M. Thorslund, and J. Fast-
bom. 2007. Trends in polypharmacy and potential
drug-drug interactions across educational groups
in elderly patients in Sweden for the period 1992
- 2002. Int J Clin Pharmacol Ther, 45(12):643?
653.
L. Hunter and K. Cohen. 2006. Biomedical language
processing: what?s beyond PubMed? Mol Cell,
21(5):589?594.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of BioNLP?09 shared task on
event extraction. In Proc. of BioNLP?09, pages
1?9.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frol-
kis, A. Pon, K. Banco, C. Mak, V. Neveu,
Y. Djoumbou, R. Eisner, A. Chi Guo, and D. S
Wishart. 2011. Drugbank 3.0: a comprehensive
resource for ?omics? research on drugs. Nucleic
Acids Res, 39(Database issue):D1035?D1041.
T. Kuboyama, K. Hirata, H. Kashima, K. F. Aoki-
Kinoshita, and H. Yasuda. 2007. A Spectrum
Tree Kernel. Information and Media Technolo-
gies, 2(1):292?299.
F. Leitner, S.A. Mardis, M. Krallinger, G. Ce-
sareni, L.A. Hirschman, and A. Valencia. 2010.
An overview of BioCreative II. 5. IEEE
IEEE/ACM Transactions on Computational Biol-
ogy and Bioinformatics, pages 385?399.
H. Liu, T. Christiansen, W. Baumgartner, and
K. Verspoor. 2012. Biolemmatizer: a lemmatiza-
tion tool for morphological processing of biomed-
ical text. Journal of Biomedical Semantics, 3(1):3.
D. McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Brown University.
A. Moschitti. 2006. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees.
In Proc. of ECML?06, pages 318?329.
M. Neves, J.-M. Carazo, and A. Pascual-Montano.
2009. Extraction of biomedical events using case-
based reasoning. In Proc. of BioNLP?09, pages
68?76.
S. J. Pan and Q. Yang. 2010. A Survey on Transfer
634
Learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
R. Polikar. 2006. Ensemble Based Systems in Deci-
sion Making. IEEE Circuits and Systems Maga-
zine, 6(3):21?45.
S. Pyysalo, R. S?tre, J. Tsujii, and T. Salakoski.
2008. Why Biomedical Relation Extraction Re-
sults are Incomparable and What to do about it.
In Proc. of SMBM?08, pages 149?152.
T. Rockta?schel, T. Huber, M. Weidlich, and U. Leser.
2013. WBI-NER: The impact of domain-specific
features on the performance of identifying and
classifying mentions of drugs. In Proceedings of
the 7th International Workshop on Semantic Eval-
uation (SemEval 2013).
R. S?tre, K. Sagae, and J. Tsujii. 2008. Syntactic
features for protein-protein interaction extraction.
In Proc. of LBM?07.
I. Segura-Bedmar, P. Mart??nez, and M. Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts. In
Proc. of the 7th International Workshop on Se-
mantic Evaluation (SemEval 2013).
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros. 2011. The 1st ddiextraction-2011 chal-
lenge task: Extraction of drug-drug interactions
from biomedical text. In Proc. of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction
2011, pages 1?9.
I. Solt, F. P. Szidarovszky, and D. Tikk. 2010. Con-
cept, Assertion and Relation Extraction at the
2010 i2b2 Relation Extraction Challenge using
parsing information and dictionaries. In Proc. of
i2b2/VA Shared-Task.
I. Spasic, S. Ananiadou, and J. Tsujii. 2005. MaS-
TerClass: a case-based reasoning system for the
classification of biomedical terms. Bioinformat-
ics, 21(11):2748?2758.
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011. Relation extraction for drug-drug interac-
tions using ensemble learning. In Proc. of the
1st Challenge Task on Drug-Drug Interaction Ex-
traction 2011, pages 11?18.
D. Tikk, I. Solt, P. Thomas, and U. Leser. 2013.
A detailed error analysis of 13 kernel methods
for protein-protein interaction extraction. BMC
Bioinformatics, 14(1):12.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of
kernel methods to extract protein-protein interac-
tions from literature. PLoS Comput Biol, 6.
S. V. N. Vishwanathan and A. J. Smola. 2002. Fast
Kernels for String and Tree Matching. In Proc. of
NIPS?02, pages 569?576.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A
Composite Kernel to Extract Relations between
Entities with Both Flat and Structured Features.
In Proc. of ICML?06, pages 825?832.
635
