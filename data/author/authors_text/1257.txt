Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 450?458,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Performance Prediction for Exponential Language Models
Stanley F. Chen
IBM T.J. Watson Research Center
P.O. Box 218, Yorktown Heights, NY 10598
stanchen@watson.ibm.com
Abstract
We investigate the task of performance pre-
diction for language models belonging to the
exponential family. First, we attempt to em-
pirically discover a formula for predicting test
set cross-entropy for n-gram language mod-
els. We build models over varying domains,
data set sizes, and n-gram orders, and perform
linear regression to see whether we can model
test set performance as a simple function of
training set performance and various model
statistics. Remarkably, we find a simple rela-
tionship that predicts test set performance with
a correlation of 0.9997. We analyze why this
relationship holds and show that it holds for
other exponential language models as well, in-
cluding class-based models and minimum dis-
crimination information models. Finally, we
discuss how this relationship can be applied to
improve language model performance.
1 Introduction
In this paper, we investigate the following question
for language models belonging to the exponential
family: given some training data and test data drawn
from the same distribution, can we accurately pre-
dict the test set performance of a model estimated
from the training data? This problem is known as
performance prediction and is relevant for model se-
lection, the task of selecting the best model from a
set of candidate models given data.1
Let us first define some notation. Events have the
form (x, y), where we attempt to predict the cur-
rent word y given previous words x. We denote the
training data as D = (x1, y1), . . . , (xD, yD) and de-
fine p?(x, y) = countD(x, y)/D to be the empirical
distribution of the training data. Similarly, we have
1A long version of this paper can be found at (Chen, 2008).
a test set D? and an associated empirical distribu-
tion p?(x, y). We take the performance of a condi-
tional language model p(y|x) to be the cross-entropy
H(p?, p) between the empirical test distribution p?
and the model p(y|x):
H(p?, p) = ??
x,y
p?(x, y) log p(y|x) (1)
This is equivalent to the negative mean log-
likelihood per event, as well as to log perplexity.
We only consider models in the exponential fam-
ily. An exponential model p?(y|x) is a model with
a set of features {f1(x, y), . . . , fF (x, y)} and equal
number of parameters ? = {?1, . . . , ?F } where
p?(y|x) = exp(
?F
i=1 ?ifi(x, y))
Z?(x) (2)
and where Z?(x) is a normalization factor.
One of the seminal methods for performance pre-
diction is the Akaike Information Criterion (AIC)
(Akaike, 1973). For a model, let ?? be the maxi-
mum likelihood estimate of ? on some training data.
Akaike derived the following estimate for the ex-
pected value of the test set cross-entropy H(p?, p??):
H(p?, p??) ? H(p?, p??) +
F
D (3)
H(p?, p??) is the cross-entropy of the training set, F
is the number of parameters in the model, and D is
the number of events in the training data. However,
maximum likelihood estimates for language mod-
els typically yield infinite cross-entropy on test data,
and thus AIC behaves poorly for these domains.
In this work, instead of deriving a performance
prediction relationship theoretically, we attempt to
empirically discover a formula for predicting test
performance. Initially, we consider only n-gram lan-
guage models, and build models over varying do-
mains, data set sizes, and n-gram orders. We per-
form linear regression to discover whether we can
450
model test set cross-entropy as a simple function of
training set cross-entropy and other model statistics.
For the 200+ n-gram models we evaluate, we find
that the empirical relationship
H(p?, p??) ? H(p?, p??) +
?
D
F?
i=1
|??i| (4)
holds with a correlation of 0.9997 where ? is a con-
stant and where ?? = {??i} are regularized parameter
estimates; i.e., rather than estimating performance
for maximum likelihood models as in AIC, we do
this for regularized models. In other words, test set
cross-entropy can be approximated by the sum of the
training set cross-entropy and the scaled sum of the
magnitudes of the model parameters.
To maximize the correlation achieved by eq. (4),
we find that it is necessary to use the same regular-
ization method and regularization hyperparameters
across models and that the optimal value of ? de-
pends on the values of the hyperparameters. Con-
sequently, we first evaluate several types of regu-
larization and find which of these (and which hy-
perparameter values) work best across all domains,
and use these values in all subsequent experiments.
While `22 regularization gives the best performance
reported in the literature for n-gram models, we find
here that `1 + `22 regularization works even better.
The organization of this paper is as follows: In
Section 2, we evaluate various regularization tech-
niques for n-gram models and select the method and
hyperparameter values that give the best overall per-
formance. In Section 3, we discuss experiments to
find a formula for predicting n-gram model perfor-
mance, and provide an explanation for why eq. (4)
works so well. In Section 4, we evaluate how well
eq. (4) holds for several class-based language mod-
els and minimum discrimination information mod-
els. Finally, in Sections 5 and 6 we discuss related
work and conclusions.
2 Selecting Regularization Settings
In this section, we address the issue of how to per-
form regularization in our later experiments. Fol-
lowing the terminology of Dud??k and Schapire
(2006), the most widely-used and effective methods
for regularizing exponential models are `1 regular-
ization (Tibshirani, 1994; Kazama and Tsujii, 2003;
data token range training voc.
source type of n sents. size
A RH letter 2?7 100?75k 27
B WSJ POS 2?7 100?30k 45
C WSJ word 2?5 100?100k 300
D WSJ word 2?5 100?100k 3k
E WSJ word 2?5 100?100k 21k
F BN word 2?5 100?100k 84k
G SWB word 2?5 100?100k 19k
Table 1: Statistics of data sets. RH = Random House
dictionary; WSJ = Wall Street Journal; BN = Broadcast
News; SWB = Switchboard.
Goodman, 2004) and `22 regularization (Lau, 1994;
Chen and Rosenfeld, 2000; Lebanon and Lafferty,
2001). While not as popular, another regularization
scheme that has been shown to be effective is 2-norm
inequality regularization (Kazama and Tsujii, 2003)
which is an instance of `1+`22 regularization as noted
by Dud??k and Schapire (2006). Under `1 + `22 regu-
larization, the regularized parameter estimates ?? are
chosen to optimize the objective function
O`1+`22(?) = H(p?, p?) +
?
D
F?
i=1
|?i|+ 12?2D
F?
i=1
?2i
(5)
Note that `1 regularization can be considered a spe-
cial case of this (by taking ? = ?) as can `22 regu-
larization (by taking ? = 0).
Here, we evaluate `1, `22, and `1 + `22 regulariza-
tion for exponential n-gram models. An exponen-
tial n-gram model contains a binary feature f? for
each n?-gram ? occurring in the training data for
n? ? n, where f?(x, y) = 1 iff xy ends in ?. We
would like to find the regularization method and as-
sociated hyperparameters that work best across dif-
ferent domains, training set sizes, and n-gram or-
ders. As it is computationally expensive to evalu-
ate a large number of hyperparameter settings over
a large collection of models, we divide this search
into two phases. First, we evaluate a large set of hy-
perparameters on a limited set of models to come up
with a short list of candidate hyperparameters. We
then evaluate these candidates on our full model set
to find the best one.
We build n-gram models over data from five dif-
ferent sources and consider three different vocabu-
lary sizes for one source, giving us seven ?domains?
451
in total. We refer to these domains by the letters A?
G; summary statistics for each domain are given in
Table 1. The domains C?G consist of regular word
data, while domains A and B consist of letter and
part-of-speech (POS) sequences, respectively. Do-
mains C?E differ only in vocabulary.
For each domain, we first randomize the order of
sentences in that data. We partition off two devel-
opment sets and an evaluation set (5000 ?sentences?
each in domain A and 2500 sentences elsewhere) and
use the remaining data as training data. In this way,
we assure that our training and test data are drawn
from the same distribution as is assumed in our later
experiments. Training set sizes in sentences are 100,
300, 1000, 3000, etc., up to the maximums given in
Table 1. Building models for each training set size
and n-gram order in Table 1 gives us a total of 218
models over the seven domains.
In the first phase of hyperparameter search, we
choose a subset of these models (57 total) and evalu-
ate many different values for (?, ?2) with `1+`22 reg-
ularization on each. We perform a grid search, trying
each value ? ? {0.0, 0.1, 0.2, . . . , 1.2} with each
value ?2 ? {1, 1.2, 1.5, 2, 2.5, 3, 4, 5, 6, 7, 8, 10,?}
where ? = ? corresponds to `1 regularization and
? = 0 corresponds to `22 regularization. We use
a variant of iterative scaling for parameter estima-
tion. For each model and each (?, ?2), we denote
the cross-entropy of the development data as Hm?,?
for the mth model, m ? {1, . . . , 57}. Then, for each
m and (?, ?2), we can compute how much worse the
settings (?, ?2) perform with model m as compared
to the best hyperparameter settings for that model:
H?m?,? = Hm?,? ? min??,?? H
m
??,?? (6)
We would like to select (?, ?2) for which H?m?,? tends
to be small; in particular, we choose (?, ?2) that
minimizes the root mean squared (RMS) error
H?RMS?,? =
???? 1
57
57?
m=1
(H?m?,?)2 (7)
For each of `1, `22, and `1 + `22 regularization, we re-
tain the 6?8 best hyperparameter settings. To choose
the best single hyperparameter setting from within
this candidate set, we repeat the same analysis ex-
cept over the full set of 218 models.
statistic RMSE coeff.
1
D
?F
i=1 |??i| 0.043 0.938
1
D
?
i:??i>0 ??i 0.044 0.939
1
D
?F
i=1 ??i 0.047 0.940
1
D
?F
i=1 |??i|
4
3 0.162 0.755
1
D
?F
i=1 |??i|
3
2 0.234 0.669
1
D
?F
i=1 ??2i 0.429 0.443
F 6=0
D 0.709 1.289F 6=0 logD
D 0.783 0.129F
D 0.910 1.109F logD
D 0.952 0.112
1 1.487 1.698
F
D?F?1 2.232 -0.028
F 6=0
D?F 6=0?1 2.236 -0.023
Table 2: Root mean squared error (RMSE) in nats when
predicting difference in development set and training set
cross-entropy as linear function of a single statistic. The
last column is the optimal coefficient for that statistic.
On the development sets, the (?, ?2) value with
the lowest squared error is (0.5, 6), and these are
the hyperparameter settings we use in all later ex-
periments unless otherwise noted. The RMS error,
mean error, and maximum error for these hyperpa-
rameters on the evaluation sets are 0.011, 0.007, and
0.033 nats, respectively.2 An error of 0.011 nats cor-
responds to a 1.1% difference in perplexity which
is generally considered insignificant. Thus, we can
achieve good performance across domains, data set
sizes, and n-gram orders using a single set of hyper-
parameters as compared to optimizing hyperparam-
eters separately for each model.
3 N -Gram Model Performance Prediction
Now that we have established which regularization
method and hyperparameters to use, we attempt to
empirically discover a simple formula for predict-
ing the test set cross-entropy of regularized n-gram
models. The basic strategy is as follows: We first
build a large number of n-gram models over differ-
ent domains, training set sizes, and n-gram orders.
Then, we come up with a set of candidate statistics,
e.g., training set cross-entropy, number of features,
etc., and do linear regression to try to best model test
2All cross-entropy values are reported in nats, or natural
bits, equivalent to log2 e regular bits. This will let us directly
compare ? values with average discounts in Section 3.1.
452
 
0
 
1
 
2
 
3
 
4
 
5
 
6
 
7  0
 
1
 
2
 
3
 
4
 
5
 
6
 
7
test cross-entropy - training cross-entropy (nats)
? |?
i| / D
Figure 1: Graph of optimism on evaluation data vs.
1
D
?F
i=1 |??i| for various n-gram models under `1 + `22
regularization, ? = 0.5 and ?2 = 6. The line repre-
sents the predicted optimism according to eq. (9) with
? = 0.938.
set cross-entropy as a linear function of these statis-
tics. We assume that training and test data come
from the same distribution; otherwise, it would be
difficult to predict test performance.
We use the same 218 n-gram models as in Sec-
tion 2. For each model, we compute training set
cross-entropy H(p?, p??) as well as all of the statis-
tics listed on the left in Table 2. The statistics FD ,
F
D?F?1 , and
F logD
D are motivated by AIC, AICc
(Hurvich and Tsai, 1989), and the Bayesian Infor-
mation Criterion (Schwarz, 1978), respectively. As
features fi with ??i = 0 have no effect, instead of
F we also consider using F 6=0, the number of fea-
tures fi with ??i 6= 0. The statistics 1D
?F
i=1 |??i| and
1
D
?F
i=1 ??2i are motivated by eq. (5). The statistics
with fractional exponents are suggested by Figure 2.
The value 1 is present to handle constant offsets.
After some initial investigation, it became clear
that training set cross-entropy is a very good (par-
tial) predictor of test set cross-entropy with coeffi-
cient 1. As there is ample theoretical support for
this, instead of fitting test set performance directly,
we chose to model the difference between test and
training performance as a function of the remaining
statistics. This difference is sometimes referred to as
the optimism of a model:
optimism(p??) ? H(p?, p??)?H(p?, p??) (8)
First, we attempt to model optimism as a lin-
ear function of a single statistic. For each statis-
tic listed previously, we perform linear regression
to minimize root mean squared error when predict-
ing development set optimism. In Table 2, we dis-
play the RMSE and best coefficient for each statis-
tic. We see that three statistics have by far the lowest
error: 1D
?F
i=1 |??i|, 1D
?
i:??i>0 ??i, and
1
D
?F
i=1 ??i.
In practice, most ??i in n-gram models are positive,
so these statistics tend to have similar values. We
choose the best ranked of these, 1D
?F
i=1 |??i|, and
show in Section 3.1 why this statistic is more appeal-
ing than the others. Next, we investigate modeling
optimism as a linear function of a pair of statistics.
We find that the best RMSE for two variables (0.042)
is only slightly lower than that for one (0.043), so it
is doubtful that a second variable helps.
Thus, our analysis suggests that among our candi-
dates, the best predictor of optimism is simply
optimism ? ?D
F?
i=1
|??i| (9)
where ? = 0.938, with this value being independent
of domain, training set size, and n-gram order. In
other words, the difference between test and train-
ing cross-entropy is a linear function of the sum of
parameter magnitudes scaled per event. Substituting
into eq. (8) and rearranging, we get eq. (4).
To assess the accuracy of eq. (4), we compute var-
ious statistics on our evaluation sets using the best
? from our development data, i.e., ? = 0.938. In
Figure 1, we graph optimism for the evaluation data
against 1D
?F
i=1 |??i| for each of our models; we see
that the linear correlation is very good. The correla-
tion between the actual and predicted cross-entropy
on the evaluation data is 0.9997; the mean absolute
prediction error is 0.030 nats; the RMSE is 0.043
nats; and the maximum absolute error is 0.166 nats.
Thus, on average we can predict test performance to
within 3% in perplexity, though in the worst case we
may be off by as much as 18%.3
3The sampling variation in our test set selection limits the
measured accuracy of our performance prediction. To give
some idea of the size of this effect, we randomly selected 100
test sets in domain D of 2500 sentences each (as in our other
experiments). We evaluated their cross-entropies using mod-
els trained on 100, 1k, 10k, and 100k sentences. The empiri-
453
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
-1  0  1  2  3  4
dis
cou
nt
?
Figure 2: Smoothed graph of discount versus ??i for all
features in ten different models built on domains A and
E. Each smoothed point represents the average of at least
512 raw data points.
If we compute the prediction error of eq. (4) over
the same models except using `1 or `22 regulariza-
tion (with the best corresponding hyperparameter
values found in Section 2), the prediction RMSE is
0.054 and 0.139 nats, respectively. Thus, we find
that choosing hyperparameters carefully in Section 2
was important in doing well in performance predic-
tion. While hyperparameters were chosen to opti-
mize test performance rather than prediction accu-
racy, we find that the chosen hyperparameters are
favorable for the latter task as well.
3.1 Why Does Prediction Work So Well?
The correlation in Figure 1 is remarkably high, and
thus it begs for an explanation. First, let us express
the difference in test and training cross-entropy for
a model in terms of its parameters ?. Substituting
eq. (2) into eq. (1), we get
H(p?, p?) = ?
F?
i=1
?iEp? [fi] +
?
x
p?(x) logZ?(x)
(10)
where Ep? [fi] = ?x,y p?(x, y)fi(x, y). Then, we
can express the difference in test and training per-
formance as
H(p?, p?)?H(p?, p?) =?Fi=1 ?i(Ep?[fi]? Ep? [fi])+?
x(p?(x)? p?(x)) logZ?(x) (11)
cal standard deviation across test sets was found to be 0.0123,
0.0144, 0.0167, and 0.0174 nats, respectively. This effect can
be mitigated by simply using larger test sets.
Ignoring the last term on the right, we see that opti-
mism for exponential models is a linear function of
the ?i?s with coefficients Ep?[fi]? Ep? [fi].
Then, we can ask what Ep?[fi] ? Ep? [fi] values
would let us satisfy eq. (4). Consider the relationship
(Ep?[fi]? Ep? [fi])?D ? ? sgn ??i (12)
If we substitute this into eq. (11) and ignore the last
term on the right again, this gives us exactly eq. (4).
We refer to the value (Ep?[fi]? Ep? [fi])?D as the
discount of a feature. It can be thought of as rep-
resenting how many times less the feature occurs in
the test data as opposed to the training data, if the
test data were normalized to be the same size as the
training data. Discounts for n-grams have been stud-
ied extensively, e.g., (Good, 1953; Church and Gale,
1991; Chen and Goodman, 1998), and tend not to
vary much across training set sizes.
We can check how well eq. (12) holds for actual
regularized n-gram models. We construct a total of
ten n-gram models on domains A and E. We build
four letter 5-gram models on domain A on training
sets ranging in size from 100 words to 30k words,
and six models (either trigram or 5-gram) on do-
main E on training sets ranging from 100 sentences
to 30k sentences. We create large development test
sets (45k words for domain A and 70k sentences for
domain E) to better estimate Ep? [fi].
Because graphs of discounts as a function of ??i
are very noisy, we smooth the data before plotting.
We partition data points into buckets containing at
least 512 points. We average all of the points in
each bucket to get a ?smoothed? data point, and plot
this single point for each bucket. In Figure 2, we
plot smoothed discounts as a function of ??i over the
range ??i ? [?1, 4] for all ten models.
We see that eq. (12) holds at a very rough level
over the ??i range displayed. If we examine how
much different ranges of ??i contribute to the over-
all value of
?F
i=1 ??i(Ep?[fi]?Ep? [fi]), we find that
the great majority of the mass (90?95%+) is concen-
trated in the range ??i ? [0, 4] for all ten models un-
der consideration. Thus, to a first approximation, the
reason that eq. (4) holds with ? = 0.938 is because
on average, feature expectations have a discount of
about this value for ??i in this range.4
4This analysis provides some insight as to when eq. (4)
454
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0  1  2  3  4  5
tes
t c
ros
s-e
ntr
opy
 - t
rain
ing
 cro
ss-
ent
rop
y (n
ats)
? |?i| / D
Figure 3: Graph of optimism on evaluation data vs.
1
D
?F
i=1 |??i| for various models. The ?+? marks corre-
spond to models S, M, and L over different training set
sizes, n-gram orders, and numbers of classes. The ???
marks correspond to MDI models over different n-gram
orders and in-domain training set sizes. The line and
small points are taken from Figure 1.
Due to space considerations, we only summarize
our other findings; a longer discussion is provided
in (Chen, 2008). We find that the absolute error in
cross-entropy tends to be quite small across models
for several reasons. For non-sparse models, there
is significant variation in average discounts, but be-
cause 1D
?F
i=1 |??i| is low, the overall error is low.
In contrast, sparse models are dominated by single-
count n-grams with features whose average discount
is quite close to ? = 0.938. Finally, the last term on
the right in eq. (11) also plays a small but significant
role in keeping the prediction error low.
4 Other Exponential Language Models
In (Chen, 2009), we show how eq. (4) can be used
to motivate a novel class-based language model and
a regularized version of minimum discrimination in-
formation (MDI) models (Della Pietra et al, 1992).
In this section, we analyze whether in addition to
word n-gram models, eq. (4) holds for these other
exponential language models as well.
won?t hold. For example, if a feature function fi is doubled, its
expectations and discount will also double. Thus, eq. (4) won?t
hold in general for models with continuous feature values, as
average discounts may vary widely.
4.1 Class-Based Language Models
We assume a word w is always mapped to the same
class c(w). For a sentence w1 ? ? ?wl, we have
p(w1 ? ? ?wl) =?l+1j=1 p(cj |c1 ? ? ? cj?1, w1 ? ? ?wj?1)?
?l
j=1 p(wj |c1 ? ? ? cj , w1 ? ? ?wj?1) (13)
where cj = c(wj) and where cl+1 is an end-of-
sentence token. We use the notation png(y|?) to
denote an exponential n-gram model as defined in
Section 2, where we have features for each suffix of
each ?y occurring in the training set. We use the
notation png(y|?1, ?2) to denote a model containing
all features in the models png(y|?1) and png(y|?2).
We consider three class models, models S, M, and
L, defined as
pS(cj |c1???cj?1,w1???wj?1)=png(cj |cj?2cj?1)
pS(wj |c1???cj ,w1???wj?1)=png(wj |cj)
pM (cj |c1???cj?1,w1???wj?1)=png(cj |cj?2cj?1,wj?2wj?1)
pM (wj |c1???cj ,w1???wj?1)=png(wj |wj?2wj?1cj)
pL(cj |c1???cj?1,w1???wj?1)=png(cj |wj?2cj?2wj?1cj?1)
pL(wj |c1???cj ,w1???wj?1)=png(wj |wj?2cj?2wj?1cj?1cj)
Model S is an exponential version of the class-based
n-gram model from (Brown et al, 1992); model M
is a novel model introduced in (Chen, 2009); and
model L is an exponential version of the model ind-
expredict from (Goodman, 2001).
To evaluate whether eq. (4) can accurately pre-
dict test performance for these class-based models,
we use the WSJ data and vocabulary from domain
E and consider training set sizes of 1k, 10k, 100k,
and 900k sentences. We create three different word
classings containing 50, 150, and 500 classes using
the algorithm of Brown et al (1992) on the largest
training set. For each training set and number of
classes, we build both 3-gram and 4-gram versions
of each of our three class models.
In Figure 3, we plot optimism (i.e., test minus
training cross-entropy) versus 1D
?F
i=1 |??i| for these
models (66 in total) on our WSJ evaluation set. The
?+? marks correspond to our class n-gram models,
while the small points replicate the points for word
n-gram models from Figure 1. Remarkably, eq. (4)
appears to accurately predict performance for our
455
class n-gram models using the same ? = 0.938
value found for word n-gram models. The mean ab-
solute prediction error is 0.029 nats, comparable to
that found for word n-gram models.
It is interesting that eq. (4) works for class-based
models despite their being composed of two sub-
models, one for word prediction and one for class
prediction. However, taking the log of eq. (13), we
note that the cross-entropy of text can be expressed
as the sum of the cross-entropy of its word tokens
and the cross-entropy of its class tokens. It would
not be surprising if eq. (4) holds separately for the
class prediction model predicting class data and the
word prediction model predicting word data, since
all of these component models are basically n-gram
models. Summing, this explains why eq. (4) holds
for the whole class model.
4.2 Models with Prior Distributions
Minimum discrimination information models (Della
Pietra et al, 1992) are exponential models with a
prior distribution q(y|x):
p?(y|x) = q(y|x)exp(
?F
i=1 ?ifi(x, y))
Z?(x) (14)
The central issue in performance prediction for MDI
models is whether q(y|x) needs to be accounted for.
That is, if we assume q is an exponential model,
should its parameters ?qi be included in the sum in
eq. (4)? From eq. (11), we note that if Ep?[fi] ?
Ep? [fi] = 0 for a feature fi, then the feature does
not affect the difference between test and training
cross-entropy (ignoring its impact on the last term).
By assumption, the training and test set for p come
from the same distribution while q is derived from
an independent data set. It follows that we expect
Ep?[f qi ]?Ep? [f qi ] to be zero for features in q, and we
should ignore q when applying eq. (4).
To evaluate whether eq. (4) holds for MDI mod-
els, we use the same WSJ training and evaluation
sets from domain E as in Section 4.1. We consider
three different training set sizes: 1k, 10k, and 100k
sentences. To train q, we use the 100k sentence BN
training set from domain F. We build both trigram
and 4-gram versions of each model.
In Figure 3, we plot test minus training cross-
entropy versus 1D
?F
i=1 |??i| for these models on our
WSJ evaluation set; the ??? marks correspond to
the MDI models. As expected, eq. (4) appears to
work quite well for MDI models using the same
? = 0.938 value as before; the mean absolute pre-
diction error is 0.077 nats.
5 Related Work
We group existing performance prediction methods
into two categories: non-data-splitting methods and
data-splitting methods. In non-data-splitting meth-
ods, test performance is directly estimated from
training set performance and/or other statistics of a
model. Data-splitting methods involve partitioning
training data into a truncated training set and a surro-
gate test set and using surrogate test set performance
to estimate true performance.
The most popular non-data-splitting methods for
predicting test set cross-entropy (or likelihood) are
AIC and variants such as AICc, quasi-AIC (QAIC),
and QAICc (Akaike, 1973; Hurvich and Tsai, 1989;
Lebreton et al, 1992). In Section 3, we consid-
ered performance prediction formulae with the same
form as AIC and AICc (except using regularized pa-
rameter estimates), and neither performed as well as
eq. (4); e.g., see Table 2.
There are many techniques for bounding test
set classification error including the Occam?s Ra-
zor bound (Blumer et al, 1987; McAllester, 1999),
PAC-Bayes bound (McAllester, 1999), and the sam-
ple compression bound (Littlestone and Warmuth,
1986; Floyd and Warmuth, 1995). These methods
derive theoretical guarantees that the true error rate
of a classifier will be below (or above) some value
with a certain probability. Langford (2005) evalu-
ates these techniques over many data sets; while the
bounds can sometimes be fairly tight, in many data
sets the bounds are quite loose.
When learning an element from a set of target
classifiers, the Vapnik-Chervonenkis (VC) dimen-
sion of the set can be used to bound the true error rate
relative to the training error rate with some probabil-
ity (Vapnik, 1998); this technique has been used to
compute error bounds for many types of classifiers.
For example, Bartlett (1998) shows that for a neural
network with small weights and small training set
squared error, the true error depends on the size of
its weights rather than the number of weights; this
finding is similar in spirit to eq. (4).
456
In practice, the most accurate methods for perfor-
mance prediction in many contexts are data-splitting
methods (Guyon et al, 2006). These techniques in-
clude the hold-out method; leave-one-out and k-fold
cross-validation; and bootstrapping (Allen, 1974;
Stone, 1974; Geisser, 1975; Craven and Wahba,
1979; Efron, 1983). However, unlike non-data-
splitting methods, these methods do not lend them-
selves well to providing insight into model design as
discussed in Section 6.
6 Discussion
We show that for several types of exponential lan-
guage models, it is possible to accurately predict the
cross-entropy of test data using the simple relation-
ship given in eq. (4). When using `1 + `22 regulariza-
tion with (? = 0.5, ?2 = 6), the value ? = 0.938
works well across varying model types, domains,
vocabulary sizes, training set sizes, and n-gram or-
ders, yielding a mean absolute error of about 0.03
nats (3% in perplexity). We evaluate ?300 language
models in total, including word and class n-gram
models and n-gram models with prior distributions.
While there has been a great deal of work in
performance prediction, the vast majority of work
on non-data-splitting methods has focused on find-
ing theoretically-motivated approximations or prob-
abilistic bounds on test performance. In contrast, we
developed eq. (4) on a purely empirical basis, and
there has been little, if any, existing work that has
shown comparable performance prediction accuracy
over such a large number of models and data sets. In
addition, there has been little, if any, previous work
on performance prediction for language modeling.5
While eq. (4) performs well as compared to other
non-data-splitting methods for performance predic-
tion, the prediction error can be several percent in
perplexity, which means we cannot reliably rank
models that are close in quality. In addition, in
speech recognition and many other applications, an
external test set is typically provided, which means
we can measure test set performance directly. Thus,
in practice, eq. (4) is not terribly useful for the task
5Here, we refer to predicting test set performance from
training set performance and other model statistics. However,
there has been a good deal of work on predicting speech recog-
nition word-error rate from test set perplexity and other statis-
tics, e.g., (Klakow and Peters, 2002).
of model selection; instead, what eq. (4) gives us is
insight into model design. That is, instead of select-
ing between candidate models once they have been
built as in model selection, it is desirable to be able
to select between models at the model design stage.
Being able to intelligently compare models (with-
out implementation) requires that we know which
aspects of a model impact test performance, and this
is exactly what eq. (4) tells us.
Intuitively, simpler models should perform better
on test data given equivalent training performance,
and model structure (as opposed to parameter val-
ues) is an important aspect of the complexity of a
model. Accordingly, there have been many meth-
ods for model selection that measure the size of a
model in terms of the number of features or param-
eters in the model, e.g., (Akaike, 1973; Rissanen,
1978; Schwarz, 1978). Surprisingly, for exponential
language models, the number of model parameters
seems to matter not at all; all that matters are the
magnitudes of the parameter values. Consequently,
one can improve such models by adding features (or
a prior model) that reduce parameter values while
maintaining training performance.
In (Chen, 2009), we show how these ideas can be
used to motivate heuristics for improving the perfor-
mance of existing language models, and use these
heuristics to develop a novel class-based model and
a regularized version of MDI models that outper-
form comparable methods in both perplexity and
speech recognition word-error rate on WSJ data. In
addition, we show how the tradeoff between train-
ing set performance and model size impacts aspects
of language modeling as diverse as backoff n-gram
features, class-based models, and domain adapta-
tion. In sum, eq. (4) provides a new and valuable
framework for characterizing, analyzing, and de-
signing statistical models.
Acknowledgements
We thank Bhuvana Ramabhadran and the anony-
mous reviewers for their comments on this and ear-
lier versions of the paper.
References
Hirotugu Akaike. 1973. Information theory and an ex-
tension of the maximum likelihood principle. In Sec-
457
ond Intl. Symp. on Information Theory, pp. 267?281.
David M. Allen. 1974. The relationship between vari-
able selection and data augmentation and a method for
prediction. Technometrics, 16(1):125?127.
Peter L. Bartlett. 1998. The sample complexity of pat-
tern classification with neural networks: the size of the
weights is more important than the size of the network.
IEEE Trans. on Information Theory, 44(2):525?536.
Alselm Blumer, Andrzej Ehrenfeucht, David Haussler,
and Manfred K. Warmuth. 1987. Occam?s razor. In-
formation Processing Letters, 24(6):377?380.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, December.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Tech. Report TR-10-98, Harvard Univ.
Stanley F. Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for maximum entropy mod-
els. IEEE Trans. Speech and Aud. Proc., 8(1):37?50.
Stanley F. Chen. 2008. Performance prediction for ex-
ponential language models. Tech. Report RC 24671,
IBM Research Division, October.
Stanley F. Chen. 2009. Shrinking exponential language
models. In Proc. of HLT-NAACL.
Kenneth W. Church and William A. Gale. 1991. A com-
parison of the enhanced Good-Turing and deleted esti-
mation methods for estimating probabilities of English
bigrams. Computer Speech and Language, 5:19?54.
Peter Craven and Grace Wahba. 1979. Smoothing noisy
data with spline functions: estimating the correct de-
gree of smoothing by the method of generalized cross-
validation. Numerische Mathematik, 31:377?403.
Stephen Della Pietra, Vincent Della Pietra, Robert L.
Mercer, and Salim Roukos. 1992. Adaptive language
modeling using minimum discriminant estimation. In
Proc. Speech and Natural Lang. DARPA Workshop.
Miroslav Dud??k and Robert E. Schapire. 2006. Maxi-
mum entropy distribution estimation with generalized
regularization. In Proc. of COLT.
Bradley Efron. 1983. Estimating the error rate of a pre-
diction rule: Improvement on cross-validation. J. of
the American Statistical Assoc., 78(382):316?331.
Sally Floyd and Manfred Warmuth. 1995. Sample com-
pression, learnability, and the Vapnik-Chervonenkis
dimension. Machine Learning, 21(3):269?304.
Seymour Geisser. 1975. The predictive sample reuse
method with applications. J. of the American Statisti-
cal Assoc., 70:320?328.
I.J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3 and 4):237?264.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. MSR-TR-2001-72, Microsoft Research.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. of NAACL.
Isabelle Guyon, Amir Saffari, Gideon Dror, and Joachim
Buhmann. 2006. Performance prediction challenge.
In Proc. of Intl. Conference on Neural Networks
(IJCNN06), IEEE World Congress on Computational
Intelligence (WCCI06), pp. 2958?2965, July.
Clifford M. Hurvich and Chih-Ling Tsai. 1989. Regres-
sion and time series model selection in small samples.
Biometrika, 76(2):297?307, June.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP, pp. 137?144.
Dietrich Klakow and Jochen Peters. 2002. Testing the
correlation of word error rate and perplexity. Speech
Communications, 38(1):19?28.
John Langford. 2005. Tutorial on practical prediction
theory for classification. J. of Machine Learning Re-
search, 6:273?306.
Raymond Lau. 1994. Adaptive statistical language mod-
elling. Master?s thesis, Department of Electrical En-
gineering and Computer Science, Massachusetts Insti-
tute of Technology, Cambridge, MA.
Guy Lebanon and John Lafferty. 2001. Boosting and
maximum likelihood for exponential models. Tech.
Report CMU-CS-01-144, Carnegie Mellon Univ.
Jean-Dominique Lebreton, Kenneth P. Burnham, Jean
Clobert, and David R. Anderson. 1992. Modeling sur-
vival and testing biological hypotheses using marked
animals: a unified approach with case studies. Eco-
logical Monographs, 62:67?118.
Nick Littlestone and Manfred K. Warmuth. 1986. Re-
lating data compression and learnability. Tech. report,
Univ. of California, Santa Cruz.
David A. McAllester. 1999. PAC-Bayesian model aver-
aging. In Proc. of COLT, pp. 164?170.
Jorma Rissanen. 1978. Modeling by the shortest data
description. Automatica, 14:465?471.
Gideon Schwarz. 1978. Estimating the dimension of a
model. Annals of Statistics, 6(2):461?464.
M. Stone. 1974. Cross-validatory choice and assessment
of statistical predictions. J. of the Royal Statistical So-
ciety B, 36:111?147.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Tech. report, Univ. of Toronto.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley, New York.
458
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 468?476,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Shrinking Exponential Language Models
Stanley F. Chen
IBM T.J. Watson Research Center
P.O. Box 218, Yorktown Heights, NY 10598
stanchen@watson.ibm.com
Abstract
In (Chen, 2009), we show that for a vari-
ety of language models belonging to the ex-
ponential family, the test set cross-entropy of
a model can be accurately predicted from its
training set cross-entropy and its parameter
values. In this work, we show how this rela-
tionship can be used to motivate two heuristics
for ?shrinking? the size of a language model
to improve its performance. We use the first
heuristic to develop a novel class-based lan-
guage model that outperforms a baseline word
trigram model by 28% in perplexity and 1.9%
absolute in speech recognition word-error rate
on Wall Street Journal data. We use the second
heuristic to motivate a regularized version of
minimum discrimination information models
and show that this method outperforms other
techniques for domain adaptation.
1 Introduction
An exponential model p?(y|x) is a model with a set
of features {f1(x, y), . . . , fF (x, y)} and equal num-
ber of parameters ? = {?1, . . . , ?F } where
p?(y|x) = exp(
?F
i=1 ?ifi(x, y))
Z?(x) (1)
and where Z?(x) is a normalization factor. In
(Chen, 2009), we show that for many types of ex-
ponential language models, if a training and test set
are drawn from the same distribution, we have
Htest ? Htrain + ?D
F?
i=1
|??i| (2)
where Htest denotes test set cross-entropy; Htrain de-
notes training set cross-entropy; D is the number of
events in the training data; the ??i are regularized pa-
rameter estimates; and ? is a constant independent
of domain, training set size, and model type.1 This
relationship is strongest if the ?? = {??i} are esti-
mated using `1+`22 regularization (Kazama and Tsu-
jii, 2003). In `1 + `22 regularization, parameters are
chosen to optimize
O`1+`22(?) = Htrain +
?
D
F?
i=1
|?i|+ 12?2D
F?
i=1
?2i (3)
for some ? and ?. With (? = 0.5, ?2 = 6) and
taking ? = 0.938, test set cross-entropy can be pre-
dicted with eq. (2) for a wide range of models with a
mean error of a few hundredths of a nat, equivalent
to a few percent in perplexity.2
In this paper, we show how eq. (2) can be applied
to improve language model performance. First, we
use eq. (2) to analyze backoff features in exponential
n-gram models. We find that backoff features im-
prove test set performance by reducing the ?size? of
a model 1D
?F
i=1 |??i| rather than by improving train-
ing set performance. This suggests the following
principle for improving exponential language mod-
els: if a model can be ?shrunk? without increasing
its training set cross-entropy, test set cross-entropy
should improve. We apply this idea to motivate
two language models: a novel class-based language
model and regularized minimum discrimination in-
formation (MDI) models. We show how these mod-
els outperform other models in both perplexity and
word-error rate on Wall Street Journal (WSJ) data.
The organization of this paper is as follows: In
Section 2, we analyze the use of backoff features in
n-gram models to motivate a heuristic for model de-
sign. In Sections 3 and 4, we introduce our novel
1The cross-entropy of a model p?(y|x) on some data D =
(x1, y1), . . . , (xD, yD) is defined as ? 1D
PD
j=1 log p?(yj |xj).
It is equivalent to the negative mean log-likelihood per event as
well as to log perplexity.
2A nat is a ?natural? bit and is equivalent to log2 e regular
bits. We use nats to be consistent with (Chen, 2009).
468
features Heval Hpred Htrain
? |??i|
D
3g 2.681 2.724 2.341 0.408
2g+3g 2.528 2.513 2.248 0.282
1g+2g+3g 2.514 2.474 2.241 0.249
Table 1: Various statistics for letter trigram models built
on a 1k-word training set. Heval is the cross-entropy of
the evaluation data; Hpred is the predicted test set cross-
entropy according to eq. (2); and Htrain is the training
set cross-entropy. The evaluation data is drawn from the
same distribution as the training; H values are in nats.
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
?
predicted letter
Figure 1: Nonzero ??i values for bigram features in let-
ter bigram model without unigram backoff features. If
we denote bigrams as wj?1wj , each column contains the
??i?s corresponding to all bigrams with a particular wj .
The ??? marks represent the average |??i| in each column;
this average includes history words for which no feature
exists or for which ??i = 0.
class-based model and discuss MDI domain adapta-
tion, and compare these methods against other tech-
niques on WSJ data. Finally, in Sections 5 and 6 we
discuss related work and conclusions.3
2 N -Gram Models and Backoff Features
In this section, we use eq. (2) to explain why backoff
features in exponential n-gram models improve per-
formance, and use this analysis to motivate a general
heuristic for model design. An exponential n-gram
model contains a binary feature f? for each n?-gram
? occurring in the training data for n? ? n, where
f?(x, y) = 1 iff xy ends in ?. We refer to features
corresponding to n?-grams for n? < n as backoff
features; it is well known that backoff features help
3A long version of this paper can be found at (Chen, 2008).
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
?
predicted letter
Figure 2: Like Figure 1, but for model with unigram
backoff features.
performance a great deal. We present statistics in
Table 1 for various letter trigram models built on the
same data set. In these and all later experiments, all
models are regularized with `1 + `22 regularization
with (? = 0.5, ?2 = 6). The last row corresponds to
a normal trigram model; the second row corresponds
to a model lacking unigram features; and the first
row corresponds to a model with no unigram or bi-
gram features. As backoff features are added, we see
that the training set cross-entropy improves, which
is not surprising since the number of features is in-
creasing. More surprising is that as we add features,
the ?size? of the model 1D
?F
i=1 |??i| decreases.
We can explain these results by examining a sim-
ple example. Consider an exponential model con-
sisting of the features f1(x, y) and f2(x, y) with pa-
rameter values ??1 = 3 and ??2 = 4. From eq. (1),
this model has the form
p??(y|x) =
exp(3f1(x, y) + 4f2(x, y))
Z?(x) (4)
Now, consider creating a new feature f3(x, y) =
f1(x, y)+f2(x, y) and setting our parameters as fol-
lows: ?new1 = 0, ?new2 = 1, and ?new3 = 3. Substitut-
ing into eq. (1), we see that p?new(y|x) = p??(y|x)
for all x, y. As the distribution this model de-
scribes does not change, neither will its training per-
formance. However, the (unscaled) size ?Fi=1 |?i|
of the model has been reduced from 3+4=7 to
0+1+3=4, and consequently by eq. (2) we predict
that test performance will improve.4
4When sgn(??1) = sgn(??2),
PF
i=1 |?i| is reduced most by
469
In fact, since p?new = p??, test performance will
remain the same. The catch is that eq. (2) applies
only to the regularized parameter estimates for a
model, and in general, ?new will not be the regu-
larized parameter estimates for the expanded feature
set. We can compute the actual regularized parame-
ters ??new for which eq. (2) will apply; this may im-
prove predicted performance even more.
Hence, by adding ?redundant? features to a model
to shrink its total size
?F
i=1 |??i|, we can improve
predicted performance (and perhaps also actual per-
formance). This analysis suggests the following
technique for improving model performance:
Heuristic 1 Identify groups of features which will
tend to have similar ??i values. For each such fea-
ture group, add a new feature to the model that is
the sum of the original features.
The larger the original ??i?s, the larger the reduction
in model size and the higher the predicted gain.
Given this perspective, we can explain why back-
off features improve n-gram model performance.
For simplicity, consider a bigram model, one with-
out unigram backoff features. It seems intuitive
that probabilities of the form p(wj |wj?1) are sim-
ilar across different wj?1, and thus so are the ??i for
the corresponding bigram features. (If a word has
a high unigram probability, it will also tend to have
high bigram probabilities.) In Figure 1, we plot the
nonzero ??i values for all (bigram) features in a bi-
gram model without unigram features. Each column
contains the ??i values for a different predicted word
wj , and the ??? mark in each column is the average
value of |??i| over all history words wj?1. We see
that the average |??i| for each word wj is often quite
far from zero, which suggests creating features
fwj (x, y) =
?
wj?1
fwj?1wj (x, y) (5)
to reduce the overall size of the model.
In fact, these features are exactly unigram backoff
features. In Figure 2, we plot the nonzero ??i values
for all bigram features after adding unigram backoff
features. We see that the average |??i|?s are closer
to zero, implying that the model size
?F
i=1 |??i| has
setting ?new3 to the ??i with the smaller magnitude, and the size
of the reduction is equal to |?new3 |. If sgn(??1) 6= sgn(??2), no
reduction is possible through this transformation.
Heval Hpred Htrain
? |??i|
D
word n-gram 4.649 4.672 3.354 1.405
model M 4.536 4.544 3.296 1.330
Table 2: Various statistics for word and class trigram
models built on 100k sentences of WSJ training data.
been significantly decreased. We can extend this
idea to higher-order n-gram models as well; e.g., bi-
gram parameters can shrink trigram parameters, and
can in turn be shrunk by unigram parameters. As
shown in Table 1, both training set cross-entropy and
model size can be reduced by this technique.
3 Class-Based Language Models
In this section, we show how we can use Heuris-
tic 1 to design a novel class-based model that outper-
forms existing models in both perplexity and speech
recognition word-error rate. We assume a word w is
always mapped to the same class c(w). For a sen-
tence w1 ? ? ?wl, we have
p(w1 ? ? ?wl) =?l+1j=1 p(cj |c1 ? ? ? cj?1, w1 ? ? ?wj?1)?
?l
j=1 p(wj |c1 ? ? ? cj , w1 ? ? ?wj?1) (6)
where cj = c(wj) and cl+1 is the end-of-sentence
token. We use the notation png(y|?) to denote an ex-
ponential n-gram model, a model containing a fea-
ture for each suffix of each ?y occurring in the train-
ing set. We use png(y|?1, ?2) to denote a model con-
taining all features in png(y|?1) and png(y|?2).
We can define a class-based n-gram model by
choosing parameterizations for the distributions
p(cj | ? ? ? ) and p(wj | ? ? ? ) in eq. (6) above. For exam-
ple, the most widely-used class-based n-gram model
is the one introduced by Brown et al (1992); we re-
fer to this model as the IBM class model:
p(cj |c1 ? ? ? cj?1, w1 ? ? ?wj?1)= png(cj |cj?2cj?1)
p(wj |c1 ? ? ? cj , w1 ? ? ?wj?1)= png(wj |cj) (7)
(In the original work, non-exponential n-gram mod-
els are used.) Clearly, there is a large space of pos-
sible class-based models.
Now, we discuss how we can use Heuristic 1 to
design a novel class-based model by using class in-
formation to ?shrink? a word-based n-gram model.
The basic idea is as follows: if we have an n-gram ?
470
and another n-gram ?? created by replacing a word
in ? with a similar word, then the two correspond-
ing features should have similar ??i?s. For exam-
ple, it seems intuitive that the n-grams on Monday
morning and on Tuesday morning should have sim-
ilar ??i?s. Heuristic 1 tells us how to take advantage
of this observation to improve model performance.
Let?s begin with a word trigram model
png(wj |wj?2wj?1). First, we would like to
convert this model into a class-based model.
Without loss of generality, we have
p(wj |wj?2wj?1) =?cj p(wj , cj |wj?2wj?1)
=?cj p(cj |wj?2wj?1)p(wj |wj?2wj?1cj) (8)
Thus, it seems reasonable to use the distributions
png(cj |wj?2wj?1) and png(wj |wj?2wj?1cj) as the
starting point for our class model. This model can
express the same set of word distributions as our
original model, and hence may have a similar train-
ing cross-entropy. In addition, this transformation
can be viewed as shrinking together word n-grams
that differ only in wj . That is, we expect that pairs
of n-grams wj?2wj?1wj that differ only in wj (be-
longing to the same class) should have similar ??i.
From Heuristic 1, we can make new features
fwj?2wj?1cj (x, y) =
?
wj?cj
fwj?2wj?1wj (x, y) (9)
These are exactly the features in png(cj |wj?2wj?1).
When applying Heuristic 1, all features typically be-
long to the same model, but even when they don?t
one can achieve the same net effect.
Then, we can use Heuristic 1 to also shrink to-
gether n-gram features for n-grams that differ only
in their histories. For example, we can create new
features of the form
fcj?2cj?1cj (x, y) =
?
wj?2?cj?2,wj?1?cj?1
fwj?2wj?1cj (x, y) (10)
This corresponds to replacing png(cj |wj?2wj?1)
with the distribution png(cj |cj?2cj?1, wj?2wj?1).
We refer to the resulting model as model M:
p(cj |c1???cj?1,w1???wj?1)=png(cj |cj?2cj?1,wj?2wj?1)
p(wj |c1???cj ,w1???wj?1)=png(wj |wj?2wj?1cj) (11)
By design, it is meant to have similar training set
cross-entropy as a word n-gram model while being
significantly smaller.
To give an idea of whether this model behaves as
expected, in Table 2 we provide statistics for this
model (as well as for an exponential word n-gram
model) built on 100k WSJ training sentences with 50
classes using the same regularization as before. We
see that model M is both smaller than the baseline
and has a lower training set cross-entropy, similar to
the behavior found when adding backoff features to
word n-gram models in Section 2. As long as eq. (2)
holds, model M should have good test performance;
in (Chen, 2009), we show that eq. (2) does indeed
hold for models of this type.
3.1 Class-Based Model Comparison
In this section, we compare model M against other
class-based models in perplexity and word-error
rate. The training data is 1993 WSJ text with verbal-
ized punctuation from the CSR-III Text corpus, and
the vocabulary is the union of the training vocabu-
lary and 20k-word ?closed? test vocabulary from the
first WSJ CSR corpus (Paul and Baker, 1992). We
evaluate training set sizes of 1k, 10k, 100k, and 900k
sentences. We create three different word classings
containing 50, 150, and 500 classes using the algo-
rithm of Brown et al (1992) on the largest training
set.5 For each training set and number of classes, we
build 3-gram and 4-gram versions of each model.
From the verbalized punctuation data from the
training and test portions of the WSJ CSR corpus,
we randomly select 2439 unique utterances (46888
words) as our evaluation set. From the remaining
verbalized punctuation data, we select 977 utter-
ances (18279 words) as our development set.
We compare the following model types: con-
ventional (i.e., non-exponential) word n-gram mod-
els; conventional IBM class n-gram models in-
terpolated with conventional word n-gram models
(Brown et al, 1992); and model M. All conven-
tional n-gram models are smoothed with modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
except we also evaluate word n-gram models with
Katz smoothing (Katz, 1987). Note: Because word
5One can imagine choosing word classes to optimize model
shrinkage; however, this is not an avenue we pursued.
471
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, Katz
3g 579.3 317.1 196.7 137.5
4g 592.6 325.6 202.4 136.7
interpolated IBM class model
3g, 50c 358.4 224.5 156.8 117.8
3g, 150c 346.5 210.5 149.0 114.7
3g, 500c 372.6 210.9 145.8 112.3
4g, 50c 362.1 220.4 149.6 109.1
4g, 150c 346.3 207.8 142.5 105.2
4g, 500c 371.5 207.9 140.5 103.6
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, modified KN
3g 488.4 270.6 168.2 121.5
4g 486.8 267.4 163.6 114.4
model M
3g, 50c 341.5 210.0 144.5 110.9
3g, 150c 342.6 203.7 140.0 108.0
3g, 500c 387.5 212.7 142.2 108.1
4g, 50c 345.8 209.0 139.1 101.6
4g, 150c 344.1 202.8 135.7 99.1
4g, 500c 390.7 211.1 138.5 100.6
Table 3: WSJ perplexity results. The best performance for each training set for each model type is highlighted in bold.
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, Katz
3g 35.5% 30.7% 26.2% 22.7%
4g 35.6% 30.9% 26.3% 22.7%
interpolated IBM class model
3g, 50c 32.2% 28.7% 25.2% 22.5%
3g, 150c 31.8% 28.1% 25.0% 22.3%
3g, 500c 32.5% 28.5% 24.5% 22.1%
4g, 50c 32.2% 28.6% 25.0% 22.0%
4g, 150c 31.8% 28.0% 24.6% 21.8%
4g, 500c 32.7% 28.3% 24.5% 21.6%
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, modified KN
3g 34.5% 30.5% 26.1% 22.6%
4g 34.5% 30.4% 25.7% 22.3%
model M
3g, 50c 30.8% 27.4% 24.0% 21.7%
3g, 150c 31.0% 27.1% 23.8% 21.5%
3g, 500c 32.3% 27.8% 23.9% 21.4%
4g, 50c 30.8% 27.5% 23.9% 21.2%
4g, 150c 31.0% 27.1% 23.5% 20.8%
4g, 500c 32.4% 27.9% 24.1% 21.1%
Table 4: WSJ lattice rescoring results; all values are word-error rates. The best performance for each training set size
for each model type is highlighted in bold. Each 0.1% in error rate corresponds to about 47 errors.
classes are derived from the largest training set, re-
sults for word models and class models are compa-
rable only for this data set. The interpolated model is
the most popular state-of-the-art class-based model
in the literature, and is the only model here using the
development set to tune interpolation weights.
We display the perplexities of these models on the
evaluation set in Table 3. Model M performs best of
all (even without interpolating with a word n-gram
model), outperforming the interpolated model with
every training set and achieving its largest reduction
in perplexity (4%) on the largest training set. While
these perplexity reductions are quite modest, what
matters more is speech recognition performance.
For the speech recognition experiments, we use
a cross-word quinphone system built from 50 hours
of Broadcast News data. The system contains 2176
context-dependent states and a total of 50336 Gaus-
sians. To evaluate our language models, we use lat-
tice rescoring. We generate lattices on both our de-
velopment and evaluation data sets using the Latt-
AIX decoder (Saon et al, 2005) in the Attila speech
recognition system (Soltau et al, 2005). The lan-
guage model for lattice generation is created by
building a modified Kneser-Ney-smoothed word tri-
gram model on our largest training set; this model is
pruned to contain a total of 350k n-grams using the
algorithm of Stolcke (1998). We choose the acoustic
weight for each model to optimize word-error rate
on the development set.
In Table 4, we display the word-error rates for
each model. If we compare the best performance
of model M for each training set with that of the
state-of-the-art interpolated class model, we find that
model M is superior by 0.8?1.0% absolute. These
gains are much larger than are suggested by the
perplexity gains of model M over the interpolated
model; as has been observed earlier, perplexity is
472
Heval Hpred Htrain
? |??i|
D
baseline n-gram model
1k 5.915 5.875 2.808 3.269
10k 5.212 5.231 3.106 2.265
100k 4.649 4.672 3.354 1.405
MDI n-gram model
1k 5.444 5.285 2.678 2.780
10k 5.031 4.973 3.053 2.046
100k 4.611 4.595 3.339 1.339
Table 5: Various statistics for WSJ trigram models, with
and without a Broadcast News prior model. The first col-
umn is the size of the in-domain training set in sentences.
not a reliable predictor of speech recognition perfor-
mance. While we can only compare class models
with word models on the largest training set, for this
training set model M outperforms the baseline Katz-
smoothed word trigram model by 1.9% absolute.6
4 Domain Adaptation
In this section, we introduce another heuristic for
improving exponential models and show how this
heuristic can be used to motivate a regularized ver-
sion of minimum discrimination information (MDI)
models (Della Pietra et al, 1992). Let?s say we have
a model p?? estimated from one training set and a
?similar? model q estimated from an independent
training set. Imagine we use q as a prior model for
p?; i.e., we make a new model pq?new as follows:
pq?new(y|x) = q(y|x)
exp(?Fi=1 ?newi fi(x, y))
Z?new(x) (12)
Then, choose ?new such that pq?new(y|x) = p??(y|x)
for all x, y (assuming this is possible). If q is ?simi-
lar? to p??, then we expect the size 1D
?F
i=1 |?newi | of
pq?new to be less than that of p??. Since they describe
the same distribution, their training set cross-entropy
will be the same. By eq. (2), we expect pq?new to
have better test set performance than p?? after reesti-
mation.7 In (Chen, 2009), we show that eq. (2) does
indeed hold for models with priors; q need not be
accounted for in computing model size as long as it
is estimated on a separate training set.
6Results for several other baseline language models and with
a different acoustic model are given in (Chen, 2008).
7That is, we expect the regularized parameters ??new to yield
improved performance.
This analysis suggests the following method for
improving model performance:
Heuristic 2 Find a ?similar? distribution estimated
from an independent training set, and use this distri-
bution as a prior.
It is straightforward to apply this heuristic to the task
of domain adaptation for language modeling. In the
usual formulation of this task, we have a test set and
a small training set from the same domain, and a
large training set from a different domain. The goal
is to use the data from the outside domain to max-
imally improve language modeling performance on
the target domain. By Heuristic 2, we can build a
language model on the outside domain, and use this
model as the prior model for a language model built
on the in-domain data. This method is identical to
the MDI method for domain adaptation, except that
we also apply regularization.
In our domain adaptation experiments, our out-
of-domain data is a 100k-sentence Broadcast News
training set. For our in-domain WSJ data, we use
training set sizes of 1k, 10k, and 100k sentences. We
build an exponential n-gram model on the Broad-
cast News data and use this model as the prior model
q(y|x) in eq. (12) when building an exponential n-
gram model on the in-domain data. In Table 5, we
display various statistics for trigram models built on
varying amounts of in-domain data when using a
Broadcast News prior and not. Across training sets,
the MDI models are both smaller in 1D
?F
i=1 |??i| and
have better training set cross-entropy than the un-
adapted models built on the same data. By eq. (2),
the adapted models should have better test perfor-
mance and we verify this in the next section.
4.1 Domain Adaptation Method Comparison
In this section, we examine how MDI adapta-
tion compares to other state-of-the-art methods for
domain adaptation in both perplexity and speech
recognition word-error rate. For these experiments,
we use the same development and evaluation sets
and lattice rescoring setup from Section 3.1.
The most widely-used techniques for domain
adaptation are linear interpolation and count merg-
ing. In linear interpolation, separate n-gram models
are built on the in-domain and out-of-domain data
and are interpolated together. In count merging, the
473
in-domain data (sents.) in-domain data (sents.)
1k 10k 100k 1k 10k 100k
in-domain data only
3g 488.4 270.6 168.2 34.5% 30.5% 26.1%
4g 486.8 267.4 163.6 34.5% 30.4% 25.7%
count merging
3g 503.1 290.9 170.7 30.4% 28.3% 25.2%
4g 497.1 284.9 165.3 30.0% 28.0% 25.3%
linear interpolation
3g 328.3 234.8 162.6 30.3% 28.5% 25.8%
4g 325.3 230.8 157.6 30.3% 28.4% 25.2%
MDI model
3g 296.3 218.7 157.0 30.0% 28.0% 24.9%
4g 293.7 215.8 152.5 29.6% 27.9% 24.9%
Table 6: WSJ perplexity and lattice rescoring results for
domain adaptation models. Values on the left are perplex-
ities and values on the right are word-error rates.
in-domain and out-of-domain data are concatenated
into a single training set, and a single n-gram model
is built on the combined data set. The in-domain
data set may be replicated several times to more
heavily weight this data. We also consider the base-
line of not using the out-of-domain data.
In Table 6, we display perplexity and word-error
rates for each method, for both trigram and 4-gram
models and with varying amounts of in-domain
training data. The last method corresponds to the
exponential MDI model; all other methods employ
conventional (non-exponential) n-gram models with
modified Kneser-Ney smoothing. In count merging,
only one copy of the in-domain data is included in
the training set; including more copies does not im-
prove evaluation set word-error rate.
Looking first at perplexity, MDI models outper-
form the next best method, linear interpolation, by
about 10% in perplexity on the smallest data set and
3% in perplexity on the largest. In terms of word-
error rate, MDI models again perform best of all,
outperforming interpolation by 0.3?0.7% absolute
and count merging by 0.1?0.4% absolute.
5 Related Work
5.1 Class-Based Language Models
In past work, the most common baseline models are
Katz-smoothed word trigram models. Compared to
this baseline, model M achieves a perplexity reduc-
tion of 28% and word-error rate reduction of 1.9%
absolute with a 900k-sentence training set. The most
closely-related existing model to model M is the
model fullibmpredict proposed by Goodman (2001):
p(cj |cj?2cj?1,wj?2wj?1)=
? p(cj |wj?2wj?1)+(1??) p(cj |cj?2cj?1)
p(wj |cj?2cj?1cj ,wj?2wj?1)=
? p(wj |wj?2wj?1cj)+(1??) p(wj |cj?2cj?1cj) (13)
This is similar to model M except that linear in-
terpolation is used to combine word and class his-
tory information, and there is no analog to the fi-
nal term in eq. (13) in model M. Using the North
American Business news corpus, the largest perplex-
ity reduction achieved over a Katz-smoothed trigram
model baseline by fullibmpredict is about 25%, with
a training set of 1M words. In N -best list rescor-
ing with a 284M-word training set, the best result
achieved for an individual class-based model is an
0.5% absolute reduction in word-error rate.
To situate the quality of our results, we also re-
view the best perplexity and word-error rate results
reported for class-based language models relative
to conventional word n-gram model baselines. In
terms of absolute word-error rate, the best gains we
found in the literature are from multi-class com-
posite n-gram models, a variant of the IBM class
model (Yamamoto and Sagisaka, 1999; Yamamoto
et al, 2003). These are called composite models
because frequent word sequences can be concate-
nated into single units within the model; the term
multi-class refers to choosing different word clus-
terings depending on word position. In experiments
on the ATR spoken language database, Yamamoto et
al. (2003) report a reduction in perplexity of 9% and
an increase in word accuracy of 2.2% absolute over
a Katz-smoothed trigram model.
In terms of perplexity, the best gains we found
are from SuperARV language models (Wang and
Harper, 2002; Wang et al, 2002; Wang et al, 2004).
In these models, classes are based on abstract role
values as given by a Constraint Dependency Gram-
mar. The class and word prediction distributions are
n-gram models that back off to a variety of mixed
word/class histories in a specific order. With a WSJ
training set of 37M words and a Katz-smoothed tri-
gram model baseline, a perplexity reduction of up to
474
53% is achieved as well as a decrease in word-error
rate of up to 1.0% absolute.
All other perplexity and absolute word-error rate
gains we found in the literature are considerably
smaller than those listed here. While different data
sets are used in previous work so results are not di-
rectly comparable, our results appear very competi-
tive with the body of existing results in the literature.
5.2 Domain Adaptation
Here, we discuss methods for supervised domain
adaptation that involve only the simple static combi-
nation of in-domain and out-of-domain data or mod-
els. For a survey of techniques using word classes,
topic, syntax, etc., refer to (Bellegarda, 2004).
Linear interpolation is the most widely-used
method for domain adaptation. Jelinek et al (1991)
describe its use for combining a cache language
model and static language model. Another popular
method is count merging; this has been motivated
as an instance of MAP adaptation (Federico, 1996;
Masataki et al, 1997). In terms of word-error rate,
Iyer et al (1997) found linear interpolation to give
better speech recognition performance while Bac-
chiani et al (2006) found count merging to be su-
perior. Klakow (1998) proposes log-linear interpo-
lation for domain adaptation. As compared to reg-
ular linear interpolation for bigram models, an im-
provement of 4% in perplexity and 0.2% absolute in
word-error rate is found.
Della Pietra et al (1992) introduce the idea of
minimum discrimination information distributions.
Given a prior model q(y|x), the goal is to find
the nearest model in Kullback-Liebler divergence
that satisfies a set of linear constraints derived from
adaptation data. The model satisfying these condi-
tions is an exponential model containing one fea-
ture per constraint with q(y|x) as its prior as in
eq. (12). While MDI models have been used many
times for language model adaptation, e.g., (Kneser et
al., 1997; Federico, 1999), they have not performed
as well as linear interpolation in perplexity or word-
error rate (Rao et al, 1995; Rao et al, 1997).
One important issue with MDI models is how to
select the feature set specifying the model. With a
small amount of adaptation data, one should intu-
itively use a small feature set, e.g., containing just
unigram features. However, the use of regulariza-
tion can obviate the need for intelligent feature se-
lection. In this work, we include all n-gram fea-
tures present in the adaptation data for n ? {3, 4}.
Chueh and Chien (2008) propose the use of inequal-
ity constraints for regularization (Kazama and Tsu-
jii, 2003); here, we use `1+`22 regularization instead.
We hypothesize that the use of state-of-the-art regu-
larization is the primary reason why we achieve bet-
ter performance relative to interpolation and count
merging as compared to earlier work.
6 Discussion
For exponential language models, eq. (2) tells us
that with respect to test set performance, the num-
ber of model parameters seems to matter not at all;
all that matters are the magnitudes of the parame-
ter values. Consequently, one can improve exponen-
tial language models by adding features (or a prior
model) that shrink parameter values while maintain-
ing training performance, and from this observa-
tion we develop Heuristics 1 and 2. We use these
ideas to motivate a novel and simple class-based
language model that achieves perplexity and word-
error rate improvements competitive with the best
reported results for class-based models in the litera-
ture. In addition, we show that with regularization,
MDI models can outperform both linear interpola-
tion and count merging in language model combina-
tion. Still, Heuristics 1 and 2 are quite vague, and
it remains to be seen how to determine when these
heuristics will be effective.
In summary, we have demonstrated how the trade-
off between training set performance and model size
impacts aspects of language modeling as diverse as
backoff n-gram features, class-based models, and
domain adaptation. In particular, we can frame
performance improvements in all of these areas as
methods that shrink models without degrading train-
ing set performance. All in all, eq. (2) is an impor-
tant tool for both understanding and improving lan-
guage model performance.
Acknowledgements
We thank Bhuvana Ramabhadran and the anony-
mous reviewers for their comments on this and ear-
lier versions of the paper.
475
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42(1):93?108.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, December.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard University.
Stanley F. Chen. 2008. Performance prediction for expo-
nential language models. Technical Report RC 24671,
IBM Research Division, October.
Stanley F. Chen. 2009. Performance prediction for expo-
nential language models. In Proc. of HLT-NAACL.
Chuang-Hua Chueh and Jen-Tzung Chien. 2008. Reli-
able feature selection for language model adaptation.
In Proc. of ICASSP, pp. 5089?5092.
Stephen Della Pietra, Vincent Della Pietra, Robert L.
Mercer, and Salim Roukos. 1992. Adaptive language
modeling using minimum discriminant estimation. In
Proc. of the Speech and Natural Language DARPA
Workshop, February.
Marcello Federico. 1996. Bayesian estimation methods
for n-gram language model adaptation. In Proc. of IC-
SLP, pp. 240?243.
Marcello Federico. 1999. Efficient language model
adaptation through MDI estimation. In Proc. of Eu-
rospeech, pp. 1583?1586.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish. 1997.
Using out-of-domain data to improve in-domain lan-
guage models. IEEE Signal Processing Letters,
4(8):221?223, August.
Frederick Jelinek, Bernard Merialdo, Salim Roukos, and
Martin Strauss. 1991. A dynamic language model for
speech recognition. In Proc. of the DARPA Workshop
on Speech and Natural Language, pp. 293?295, Mor-
ristown, NJ, USA.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400?401, March.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP, pp. 137?144.
Dietrich Klakow. 1998. Log-linear interpolation of lan-
guage models. In Proc. of ICSLP.
Reinhard Kneser, Jochen Peters, and Dietrich Klakow.
1997. Language model adaptation using dynamic
marginals. In Proc. of Eurospeech.
Hirokazu Masataki, Yoshinori Sagisaka, Kazuya Hisaki,
and Tatsuya Kawahara. 1997. Task adaptation us-
ing MAP estimation in n-gram language modeling. In
Proc. of ICASSP, volume 2, pp. 783?786, Washington,
DC, USA. IEEE Computer Society.
Douglas B. Paul and Janet M. Baker. 1992. The de-
sign for the Wall Street Journal-based CSR corpus.
In Proc. of the DARPA Speech and Natural Language
Workshop, pp. 357?362, February.
P. Srinivasa Rao, Michael D. Monkowski, and Salim
Roukos. 1995. Language model adaptation via mini-
mum discrimination information. In Proc. of ICASSP,
volume 1, pp. 161?164.
P. Srinivasa Rao, Satya Dharanipragada, and Salim
Roukos. 1997. MDI adaptation of language models
across corpora. In Proc. of Eurospeech, pp. 1979?
1982.
George Saon, Daniel Povey, and Geoffrey Zweig. 2005.
Anatomy of an extremely fast LVCSR decoder. In
Proc. of Interspeech, pp. 549?552.
Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, and Geoffrey Zweig. 2005. The
IBM 2004 conversational telephony system for rich
transcription. In Proc. of ICASSP, pp. 205?208.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. of the DARPA Broad-
cast News Transcription and Understanding Work-
shop, pp. 270?274, Lansdowne, VA, February.
Wen Wang and Mary P. Harper. 2002. The Super-
ARV language model: Investigating the effectiveness
of tightly integrating multiple knowledge sources. In
Proc. of EMNLP, pp. 238?247.
Wen Wang, Yang Liu, and Mary P. Harper. 2002.
Rescoring effectiveness of language models using dif-
ferent levels of knowledge and their integration. In
Proc. of ICASSP, pp. 785?788.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pp. 261?264.
Hirofumi Yamamoto and Yoshinori Sagisaka. 1999.
Multi-class composite n-gram based on connection di-
rection. In Proc. of ICASSP, pp. 533?536.
Hirofumi Yamamoto, Shuntaro Isogai, and Yoshinori
Sagisaka. 2003. Multi-class composite n-gram lan-
guage model. Speech Communication, 41(2-3):369?
379.
476
