Automatically Constructing a Lexicon of
Verb Phrase Idiomatic Combinations
Afsaneh Fazly
Department of Computer Science
University of Toronto
Toronto, ON M5S 3H5
Canada
afsaneh@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON M5S 3H5
Canada
suzanne@cs.toronto.edu
Abstract
We investigate the lexical and syntactic
flexibility of a class of idiomatic expres-
sions. We develop measures that draw
on such linguistic properties, and demon-
strate that these statistical, corpus-based
measures can be successfully used for dis-
tinguishing idiomatic combinations from
non-idiomatic ones. We also propose
a means for automatically determining
which syntactic forms a particular idiom
can appear in, and hence should be in-
cluded in its lexical representation.
1 Introduction
The term idiom has been applied to a fuzzy cat-
egory with prototypical examples such as by and
large, kick the bucket, and let the cat out of the
bag. Providing a definitive answer for what idioms
are, and determining how they are learned and un-
derstood, are still subject to debate (Glucksberg,
1993; Nunberg et al, 1994). Nonetheless, they are
often defined as phrases or sentences that involve
some degree of lexical, syntactic, and/or semantic
idiosyncrasy.
Idiomatic expressions, as a part of the vast fam-
ily of figurative language, are widely used both in
colloquial speech and in written language. More-
over, a phrase develops its idiomaticity over time
(Cacciari, 1993); consequently, new idioms come
into existence on a daily basis (Cowie et al, 1983;
Seaton and Macaulay, 2002). Idioms thus pose a
serious challenge, both for the creation of wide-
coverage computational lexicons, and for the de-
velopment of large-scale, linguistically plausible
natural language processing (NLP) systems (Sag
et al, 2002).
One problem is due to the range of syntactic
idiosyncrasy of idiomatic expressions. Some id-
ioms, such as by and large, contain syntactic vio-
lations; these are often completely fixed and hence
can be listed in a lexicon as ?words with spaces?
(Sag et al, 2002). However, among those idioms
that are syntactically well-formed, some exhibit
limited morphosyntactic flexibility, while others
may be more syntactically flexible. For example,
the idiom shoot the breeze undergoes verbal inflec-
tion (shot the breeze), but not internal modification
or passivization (?shoot the fun breeze, ?the breeze
was shot). In contrast, the idiom spill the beans
undergoes verbal inflection, internal modification,
and even passivization. Clearly, a words-with-
spaces approach does not capture the full range of
behaviour of such idiomatic expressions.
Another barrier to the appropriate handling of
idioms in a computational system is their seman-
tic idiosyncrasy. This is a particular issue for those
idioms that conform to the grammar rules of the
language. Such idiomatic expressions are indistin-
guishable on the surface from compositional (non-
idiomatic) phrases, but a computational system
must be capable of distinguishing the two. For ex-
ample, a machine translation system should trans-
late the idiom shoot the breeze as a single unit of
meaning (?to chat?), whereas this is not the case
for the literal phrase shoot the bird.
In this study, we focus on a particular class of
English phrasal idioms, i.e., those that involve the
combination of a verb plus a noun in its direct ob-
ject position. Examples include shoot the breeze,
pull strings, and push one?s luck. We refer to these
as verb+noun idiomatic combinations (VNICs).
The class of VNICs accommodates a large num-
ber of idiomatic expressions (Cowie et al, 1983;
Nunberg et al, 1994). Moreover, their peculiar be-
337
haviour signifies the need for a distinct treatment
in a computational lexicon (Fellbaum, 2005). De-
spite this, VNICs have been granted relatively lit-
tle attention within the computational linguistics
community.
We look into two closely related problems
confronting the appropriate treatment of VNICs:
(i) the problem of determining their degree of flex-
ibility; and (ii) the problem of determining their
level of idiomaticity. Section 2 elaborates on the
lexicosyntactic flexibility of VNICs, and how this
relates to their idiomaticity. In Section 3, we pro-
pose two linguistically-motivated statistical mea-
sures for quantifying the degree of lexical and
syntactic inflexibility (or fixedness) of verb+noun
combinations. Section 4 presents an evaluation
of the proposed measures. In Section 5, we put
forward a technique for determining the syntac-
tic variations that a VNIC can undergo, and that
should be included in its lexical representation.
Section 6 summarizes our contributions.
2 Flexibility and Idiomaticity of VNICs
Although syntactically well-formed, VNICs in-
volve a certain degree of semantic idiosyncrasy.
Unlike compositional verb+noun combinations,
the meaning of VNICs cannot be solely predicted
from the meaning of their parts. There is much ev-
idence in the linguistic literature that the seman-
tic idiosyncrasy of idiomatic combinations is re-
flected in their lexical and/or syntactic behaviour.
2.1 Lexical and Syntactic Flexibility
A limited number of idioms have one (or more)
lexical variants, e.g., blow one?s own trumpet and
toot one?s own horn (examples from Cowie et al
1983). However, most are lexically fixed (non-
productive) to a large extent. Neither shoot the
wind nor fling the breeze are typically recognized
as variations of the idiom shoot the breeze. Simi-
larly, spill the beans has an idiomatic meaning (?to
reveal a secret?), while spill the peas and spread
the beans have only literal interpretations.
Idiomatic combinations are also syntactically
peculiar: most VNICs cannot undergo syntactic
variations and at the same time retain their id-
iomatic interpretations. It is important, however,
to note that VNICs differ with respect to the degree
of syntactic flexibility they exhibit. Some are syn-
tactically inflexible for the most part, while others
are more versatile; as illustrated in 1 and 2:
1. (a) Tim and Joy shot the breeze.
(b) ?? Tim and Joy shot a breeze.
(c) ?? Tim and Joy shot the breezes.
(d) ?? Tim and Joy shot the fun breeze.
(e) ?? The breeze was shot by Tim and Joy.
(f) ?? The breeze that Tim and Joy kicked was fun.
2. (a) Tim spilled the beans.
(b) ? Tim spilled some beans.
(c) ?? Tim spilled the bean.
(d) Tim spilled the official beans.
(e) The beans were spilled by Tim.
(f) The beans that Tim spilled troubled Joe.
Linguists have explained the lexical and syntac-
tic flexibility of idiomatic combinations in terms
of their semantic analyzability (e.g., Glucksberg
1993; Fellbaum 1993; Nunberg et al 1994). Se-
mantic analyzability is inversely related to id-
iomaticity. For example, the meaning of shoot the
breeze, a highly idiomatic expression, has nothing
to do with either shoot or breeze. In contrast, a less
idiomatic expression, such as spill the beans, can
be analyzed as spill corresponding to ?reveal? and
beans referring to ?secret(s)?. Generally, the con-
stituents of a semantically analyzable idiom can be
mapped onto their corresponding referents in the
idiomatic interpretation. Hence analyzable (less
idiomatic) expressions are often more open to lex-
ical substitution and syntactic variation.
2.2 Our Proposal
We use the observed connection between id-
iomaticity and (in)flexibility to devise statisti-
cal measures for automatically distinguishing id-
iomatic from literal verb+noun combinations.
While VNICs vary in their degree of flexibility
(cf. 1 and 2 above; see also Moon 1998), on the
whole they contrast with compositional phrases,
which are more lexically productive and appear in
a wider range of syntactic forms. We thus propose
to use the degree of lexical and syntactic flexibil-
ity of a given verb+noun combination to determine
the level of idiomaticity of the expression.
It is important to note that semantic analyzabil-
ity is neither a necessary nor a sufficient condi-
tion for an idiomatic combination to be lexically
or syntactically flexible. Other factors, such as
the communicative intentions and pragmatic con-
straints, can motivate a speaker to use a variant
in place of a canonical form (Glucksberg, 1993).
Nevertheless, lexical and syntactic flexibility may
well be used as partial indicators of semantic ana-
lyzability, and hence idiomaticity.
338
3 Automatic Recognition of VNICs
Here we describe our measures for idiomaticity,
which quantify the degree of lexical, syntactic, and
overall fixedness of a given verb+noun combina-
tion, represented as a verb?noun pair. (Note that
our measures quantify fixedness, not flexibility.)
3.1 Measuring Lexical Fixedness
AVNIC is lexically fixed if the replacement of any
of its constituents by a semantically (and syntac-
tically) similar word generally does not result in
another VNIC, but in an invalid or a literal expres-
sion. One way of measuring lexical fixedness of
a given verb+noun combination is thus to exam-
ine the idiomaticity of its variants, i.e., expressions
generated by replacing one of the constituents by
a similar word. This approach has two main chal-
lenges: (i) it requires prior knowledge about the
idiomaticity of expressions (which is what we are
developing our measure to determine); (ii) it needs
information on ?similarity? among words.
Inspired by Lin (1999), we examine the strength
of association between the verb and noun con-
stituents of the target combination and its variants,
as an indirect cue to their idiomaticity. We use the
automatically-built thesaurus of Lin (1998) to find
similar words to the noun of the target expression,
in order to automatically generate variants. Only
the noun constituent is varied, since replacing the
verb constituent of a VNIC with a semantically re-
lated verb is more likely to yield another VNIC, as
in keep/lose one?s cool (Nunberg et al, 1994).
Let
 
		Unsupervised Type and Token Identification
of Idiomatic Expressions
Afsaneh Fazly?
University of Toronto
Paul Cook??
University of Toronto
Suzanne Stevenson?
University of Toronto
Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it
is not clear exactly how people learn and understand them. They are of special interest to
linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic
idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the
properties of idioms in the linguistics literature, there is not much agreement on which properties
are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have
mostly been overlooked by researchers in computational linguistics. In this article, we look
into the usefulness of some of the identified linguistic properties of idioms for their automatic
recognition. Specifically, we develop statistical measures that each model a specific property
of idiomatic expressions by looking at their actual usage patterns in text. We use these sta-
tistical measures in a type-based classification task where we automatically separate idiomatic
expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface
literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of
the measures in a token identification task where we distinguish idiomatic and literal usages of
potentially idiomatic expressions in context.
1. Introduction
Idioms form a heterogeneous class, with prototypical examples such as by and large, kick
the bucket, and let the cat out of the bag. It is hard to find a single agreed-upon definition
that covers all members of this class (Glucksberg 1993; Cacciari 1993; Nunberg, Sag,
and Wasow 1994), but they are often defined as sequences of words involving some de-
gree of semantic idiosyncrasy or non-compositionality. That is, an idiom has a different
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: afsaneh@cs.toronto.edu.
?? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: pcook@cs.toronto.edu.
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: suzanne@cs.toronto.edu.
Submission received: 12 September 2007; revised submission received: 29 February 2008; accepted for
publication: 6 May 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
meaning from the simple composition of the meaning of its component words. Idioms
are widely and creatively used by speakers of a language to express ideas cleverly, eco-
nomically, or implicitly, and thus appear in all languages and in all text genres (Sag et al
2002). Many expressions acquire an idiomatic meaning over time (Cacciari 1993); conse-
quently, new idioms come into existence on a daily basis (Cowie, Mackin, and McCaig
1983; Seaton and Macaulay 2002). Automatic tools are therefore necessary for assisting
lexicographers in keeping lexical resources up to date, as well as for creating and ex-
tending computational lexicons for use in natural language processing (NLP) systems.
Though completely frozen idioms, such as by and large, can be represented as
words with spaces (Sag et al 2002), most idioms are syntactically well-formed phrases
that allow some variability in expression, such as shoot the breeze and hold fire (Gibbs
and Nayak 1989; d?Arcais 1993; Fellbaum 2007). Such idioms allow a varying degree
of morphosyntactic flexibility?for example, held fire and hold one?s fire allow for an
idiomatic reading, whereas typically only a literal interpretation is available for fire was
held and held fires. Clearly, a words-with-spaces approach does not work for phrasal
idioms. Hence, in addition to requiring NLP tools for recognizing idiomatic expressions
(types) to include in a lexicon, methods for determining the allowable and preferred
usages (a.k.a. canonical forms) of such expressions are also needed. Moreover, in many
situations, an NLP system will need to distinguish a usage (token) of a potentially
idiomatic expression as either idiomatic or literal in order to handle a given sequence of
words appropriately. For example, a machine translation system must translate held fire
differently in The army held their fire and The worshippers held the fire up to the idol.
Previous studies focusing on the automatic identification of idiom types have often
recognized the importance of drawing on their linguistic properties, such as their se-
mantic idiosyncrasy or their restricted flexibility, pointed out earlier. Some researchers
have relied on a manual encoding of idiom-specific knowledge in a lexicon (Copestake
et al 2002; Odijk 2004; Villavicencio et al 2004), whereas others have presented ap-
proaches for the automatic acquisition of more general (hence less distinctive) knowl-
edge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work
that looks into the acquisition of the distinctive properties of idioms has been limited,
both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid,
and Spranger 2004). Our goal is to develop unsupervised means for the automatic
acquisition of lexical, syntactic, and semantic knowledge about a broadly documented
class of idiomatic expressions.
Specifically, we focus on a cross-linguistically prominent class of phrasal idioms
which are commonly and productively formed from the combination of a frequent verb
and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg,
Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and
push one?s luck. We refer to these as verb+noun idiomatic combinations or VNICs.1
We present a comprehensive analysis of the distinctive linguistic properties of phrasal
idioms, including VNICs (Section 2), and propose statistical measures that capture each
property (Section 3). We provide a multi-faceted evaluation of the measures (Section 4),
showing their effectiveness in the recognition of idiomatic expressions (types)?that is,
separating them from similar-on-the-surface literal phrases?as well as their superiority
to existing state-of-the-art techniques. Drawing on these statistical measures, we also
propose an unsupervised method for the automatic acquisition of an idiom?s canonical
1 We use the abbreviation VNIC and the term expression to refer to a verb+noun type with a potential
idiomatic meaning. We use the terms instance and usage to refer to a token occurrence of an expression.
62
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
forms (e.g., shoot the breeze as opposed to shoot a breeze), and show that it can successfully
accomplish the task (Section 5).
It is possible for a single VNIC to have both idiomatic and non-idiomatic (literal)
meanings. For example, make a face is ambiguous between an idiom, as in The little girl
made a funny face at her mother, and a literal combination, as in She made a face on the
snowman using a carrot and two buttons. Despite the common perception that phrases
that can be idioms are mainly used in their idiomatic sense, our analysis of 60 idioms
has shown otherwise. We found that close to half of these also have a clear literal
meaning; and of those with a literal meaning, on average around 40% of their usages
are literal. Distinguishing token phrases as idiomatic or literal combinations of words is
thus essential for NLP tasks, such as semantic parsing and machine translation, which
require the identification of multiword semantic units.
Most recent studies focusing on the identification of idiomatic and non-idiomatic
tokens either assume the existence of manually annotated data for a supervised clas-
sification (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually
encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005;
Hashimoto, Sato, and Utsuro 2006), or even ignore the specific properties of non-
literal language and rely mainly on general purpose methods for the task (Birke and
Sarkar 2006). We propose unsupervised methods that rely on automatically acquired
knowledge about idiom types to identify their token occurrences as idiomatic or literal
(Section 6). More specifically, we explore the hypothesis that the type-based knowledge
we automatically acquire about an idiomatic expression can be used to determine
whether an instance of the expression is used literally or idiomatically (token-based
knowledge). Our experimental results show that the performance of the token-based
idiom identificationmethods proposed here is comparable to that of existing supervised
techniques (Section 7).
2. Idiomaticity, Semantic Analyzability, and Flexibility
Although syntactically well-formed, phrasal idioms (including VNICs) involve a certain
degree of semantic idiosyncrasy. This means that phrasal idioms are to some extent
nontransparent; that is, even knowing the meaning of the individual component words,
the meaning of the idiom is hard to determine without special context or previous ex-
posure. There is much evidence in the linguistics literature that idiomatic combinations
also have idiosyncratic lexical and syntactic behavior. Here, we first define semantic
analyzability and elaborate on its relation to semantic idiosyncrasy or idiomaticity. We
then expound on the lexical and syntactic behavior of VNICs, pointing out a suggestive
relation between the degree of idiomaticity of a VNIC and the degree of its lexicosyn-
tactic flexibility.
2.1 Semantic Analyzability
Idioms have been traditionally believed to be completely non-compositional (Fraser
1970; Katz 1973). This means that unlike compositional combinations, the meaning
of an idiom cannot be solely predicted from the meaning of its parts. Nonetheless,
many linguists and psycholinguists argue against such a view, providing evidence
from idioms that show some degree of semantic compositionality (Nunberg, Sag, and
Wasow 1994; Gibbs 1995). The alternative view suggests that many idioms in fact do
63
Computational Linguistics Volume 35, Number 1
have internal semantic structure, while recognizing that they are not compositional in a
simplistic or traditional sense. To explain the semantic behavior of idioms, researchers
who take this alternative view thus use new terms such as semantic decomposability
and/or semantic analyzability in place of compositionality.
To say that an idiom is semantically analyzable to some extent means that the
constituents contribute some sort of independent meaning?not necessarily their literal
semantics?to the overall idiomatic interpretation. Generally, the more semantically
analyzable an idiom is, the easier it is to map the idiom constituents onto their cor-
responding idiomatic referents. In other words, the more semantically analyzable an
idiom is, the easier it is to make predictions about the idiomatic meaning from the
meaning of the idiom parts. Semantic analyzability is thus inversely related to semantic
idiosyncrasy.
Many linguists and psycholinguists conclude that idioms clearly form a heteroge-
neous class, not all of them being truly non-compositional or unanalyzable (Abeille?
1995; Moon 1998; Grant 2005). Rather, semantic analyzability in idioms is a matter of
degree. For example, the meaning of shoot the breeze (?to chat idly?), a highly idiomatic
expression, has nothing to do with either shoot or breeze. A less idiomatic expression,
such as spill the beans (?to reveal a secret?), may be analyzed as spill metaphorically
corresponding to ?reveal? and beans referring to ?secret(s).? An idiom such as pop the
question is even less idiomatic because the relations between the idiom parts and their
idiomatic referents are more directly established, namely, pop corresponds to ?suddenly
ask? and question refers to ?marriage proposal.? As we will explain in the following
section, there is evidence that the difference in the degree of semantic analyzability of
idiomatic expressions is also reflected in their lexical and syntactic behavior.
2.2 Lexical and Syntactic Flexibility
Most idioms are known to be lexically fixed, meaning that the substitution of a near syn-
onym (or a closely related word) for a constituent part does not preserve the idiomatic
meaning of the expression. For example, neither shoot the wind nor hit the breeze are valid
variations of the idiom shoot the breeze. Similarly, spill the beans has an idiomatic meaning,
while spill the peas and spread the beans have only literal interpretations. There are, how-
ever, idiomatic expressions that have one (or more) lexical variants. For example, blow
one?s own trumpet and toot one?s own horn have the same idiomatic interpretation (Cowie,
Mackin, and McCaig 1983); also keep one?s cool and lose one?s cool have closely related
meanings (Nunberg, Sag, and Wasow 1994). Nonetheless, it is not the norm for idioms
to have lexical variants; when they do, there are usually unpredictable restrictions on
the substitutions they allow.
Idiomatic combinations are also syntactically distinct from compositional combi-
nations. Many VNICs cannot undergo syntactic variations and at the same time retain
their idiomatic interpretations. It is important, however, to note that VNICs differ with
respect to the extent to which they can tolerate syntactic operations, that is, the degree
of syntactic flexibility they exhibit. Some are syntactically inflexible for the most part,
whereas others are more versatile, as illustrated in the sentences in Examples (1) and (2):
1. (a) Sam and Azin shot the breeze.
(b) ?? Sam and Azin shot a breeze.
(c) ?? Sam and Azin shot the breezes.
(d) ?? Sam and Azin shot the casual breeze.
64
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
(e) ?? The breeze was shot by Sam and Azin.
(f) ?? The breeze that Sam and Azin shot was quite refreshing.
(g) ?? Which breeze did Sam and Azin shoot?
2. (a) Azin spilled the beans.
(b) ? Azin spilled some beans.
(c) ?? Azin spilled the bean.
(d) Azin spilled the Enron beans.
(e) The beans were spilled by Azin.
(f) The beans that Azin spilled caused Sam a lot of trouble.
(g) Which beans did Azin spill?
Linguists have often explained the lexical and syntactic flexibility of idiomatic
combinations in terms of their semantic analyzability (Fellbaum 1993; Gibbs 1993;
Glucksberg 1993; Nunberg, Sag, and Wasow 1994; Schenk 1995). The common belief
is that because the constituents of a semantically analyzable idiom can be mapped onto
their corresponding referents in the idiomatic interpretation, analyzable (less idiomatic)
expressions are often more open to lexical substitution and syntactic variation. Psy-
cholinguistic studies also support this hypothesis: Gibbs and Nayak (1989) and Gibbs
et al (1989), through a series of psychological experiments, demonstrate that there is
variation in the degree of lexicosyntactic flexibility of idiomatic combinations. (Both
studies narrow their focus to verb phrase idiomatic combinations, mainly of the form
verb+noun.) Moreover, their findings provide evidence that the lexical and syntactic
flexibility of VNICs is not arbitrary, but rather correlates with the semantic analyzability
of these idioms as perceived by the speakers participating in the experiments.
Corpus-based studies such as those by Moon (1998), Riehemann (2001), and Grant
(2005) conclude that idioms are not as fixed as most have assumed. These claims are
often based on observing certain idiomatic combinations in a form other than their so-
called canonical forms. For example, Moon mentions that she has observed both kick
the pail and kick the can as variations of kick the bucket. Also, Grant finds evidence of
variations such as eat one?s heart (out) and eat one?s hearts (out) in the BNC. Riehemann
concludes that in contrast to non-idiomatic combinations of words, ?idioms have a
strongly preferred canonical form, but at the same time the occurrence of lexical and
syntactic variations of idioms is too common to be ignored? (page 67). Our understand-
ing of such findings is that idiomatic combinations are not inherently frozen and that it
is possible for them to appear in forms other than their agreed-upon canonical forms.
However, it is important to note that most such observed variations are constrained,
often with unpredictable restrictions.
We are well aware that semantic analyzability is neither a necessary nor a sufficient
condition for an idiomatic combination to be lexically or syntactically flexible. Other
factors, such as communicative intentions and pragmatic constraints, can motivate a
speaker to use a variant in place of a canonical form (Glucksberg 1993). For exam-
ple, journalism is well known for manipulating idiomatic expressions for humor or
cleverness (Grant 2005). The age and the degree of familiarity of an idiom have also
been shown to be important factors that affect its flexibility (Gibbs and Nayak 1989).
Nonetheless, linguists often use observations about lexical and syntactic flexibility of
VNICs in order to make judgments about their degree of idiomaticity (Kyto? 1999;
Tanabe 1999). We thus conclude that lexicosyntactic behavior of a VNIC, although
affected by historical and pragmatic factors, can be at least partially explained in terms
of semantic analyzability or idiomaticity.
65
Computational Linguistics Volume 35, Number 1
3. Automatic Acquisition of Type-Based Knowledge about VNICs
We use the observed connection between idiomaticity and (in)flexibility to devise sta-
tistical measures for automatically distinguishing idiomatic verb+noun combinations
(types) from literal phrases. More specifically, we aim to identify verb?noun pairs such
as ?keep, word? as having an associated idiomatic expression (keep one?s word), and
also distinguish these from verb?noun pairs such as ?keep, fish? which do not have
an idiomatic interpretation. Although VNICs vary in their degree of flexibility (cf.
Examples (1) and (2)), on the whole they contrast with fully compositional phrases,
which are more lexically productive and appear in a wider range of syntactic forms. We
thus propose to use the degree of lexical and syntactic flexibility of a given verb+noun
combination to determine the level of idiomaticity of the expression.
Note that our assumption here is in line with corpus-linguistic studies on idioms:
we do not claim that it is inherently impossible for VNICs to undergo lexical sub-
stitution or syntactic variation. In fact, for each given idiomatic combination, it may
well be possible to find a specific situation in which a lexical or a syntactic variant of
the canonical form is perfectly plausible. However, the main point of the assumption
here is that VNICs are more likely to appear in fixed forms (known as their canonical
forms), more so than non-idiomatic phrases. Therefore, the overall distribution of a
VNIC in different lexical and syntactic forms is expected to be notably different from
the corresponding distribution of a typical verb+noun combination.
The following subsections describe our proposed statistical measures for idiomatic-
ity, which quantify the degree of lexical, syntactic, and overall fixedness of a given
verb+noun combination (represented as a verb?noun pair).
3.1 Measuring Lexical Fixedness
A VNIC is lexically fixed if the replacement of any of its constituents by a semantically
(and syntactically) similar word does not generally result in another VNIC, but in
an invalid or a literal expression. One way of measuring lexical fixedness of a given
verb+noun combination is thus to examine the idiomaticity of its variants, that is,
expressions generated by replacing one of the constituents by a similar word. This
approach has twomain challenges: (i) it requires prior knowledge about the idiomaticity
of expressions (which is what we are developing our measure to determine); (ii) it can
only measure the lexical fixedness of idiomatic combinations, and so could not apply to
literal combinations. We thus interpret this property statistically in the following way:
We expect a lexically fixed verb+noun combination to appear much more frequently
than its variants in general.
Specifically, we examine the strength of association between the verb and the
noun constituent of a combination (the target expression or its lexical variants) as
an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the
automatically built thesaurus of Lin (1998) to find words similar to each constituent,
in order to automatically generate variants.2 Variants are generated by replacing either
2 We also replicated our experiments with an automatically built thesaurus created from the British
National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results
were similar, hence we do not report them here.
66
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
the noun or the verb constituent of a pair with a semantically (and syntactically) similar
word.3
Examples of automatically generated variants for the pair ?spill, bean? are ?pour,
bean?, ?stream, bean?, ?spill, corn?, and ?spill, rice?.
Let Ssim(v) = {vi | 1 ? i ? Kv} be the set of the Kv most similar verbs to the verb v
of the target pair ?v,n?, and Ssim(n) =
{
nj | 1 ? j ? Kn
}
be the set of the Kn most similar
nouns to the noun n (according to Lin?s thesaurus). The set of variants for the target pair
is thus:
Ssim(v,n) = {?vi,n?| 1 ? i ? Kv} ?
{
?v,nj?| 1 ? j ? Kn
}
.
We calculate the association strength for the target pair and for each of its variants
using an information-theoretic measure called pointwise mutual information or PMI
(Church et al 1991):
PMI(vr, nt) = log
P(vr, nt)
P(vr)P(nt)
= log
Nv+n f (vr, nt)
f (vr, ?) f (?, nt)
(1)
where ?vr, nt? ? {?v, n?} ? Ssim(v,n); Nv+n is the total number of verb?object pairs in the
corpus; f (vr, nt) is the frequency of vr and nt co-occurring as a verb?object pair; f (vr, ?)
is the total frequency of the target (transitive) verb with any noun as its direct object;
and f (?, nt) is the total frequency of the noun nt in the direct object position of any verb
in the corpus.
In his work, Lin (1999) assumes that a target expression is non-compositional if and
only if its PMI value is significantly different from that of all the variants. Instead, we
propose a novel technique that brings together the association strengths (PMI values)
of the target and the variant expressions into a single measure reflecting the degree of
lexical fixedness for the target pair. We assume that the target pair is lexically fixed to
the extent that its PMI deviates from the average PMI of its variants. By ourmeasure, the
target pair is considered lexically fixed (i.e., is given a high fixedness score) only if the
difference between its PMI value and that of most of its variants?not necessarily all, as
in themethod of Lin (1999)?is high.4 Ourmeasure calculates this deviation, normalized
using the sample?s standard deviation:
Fixednesslex(v, n)
.
=
PMI(v, n)? PMI
s (2)
3 In an early version of this work (Fazly and Stevenson 2006), only the noun constituent was varied
because we expected replacing the verb constituent with a related verb to be more likely to yield another
VNIC, as in keep/lose one?s cool, give/get the bird, crack/break the ice (Nunberg, Sag, and Wasow 1994; Grant
2005). Later experiments on the development data showed that variants generated by replacing both
constituents, one at a time, produce better results.
4 This way, even if an idiom has a few frequently used variants (e.g., break the ice and crack the ice), it may
still be assigned a high fixedness score if most other variants are uncommon. Note also that it is possible
that some variants of a given idiom are frequently used literal expressions (e.g., make biscuit for take
biscuit). It is thus important to use a flexible formulation that relies on the collective evidence (e.g.,
average PMI) and hence is less sensitive to individual cases.
67
Computational Linguistics Volume 35, Number 1
where PMI is the mean and s the standard deviation of the following sample:
{
PMI(vr, nt) | ?vr, nt? ? {?v, n?} ? Ssim(v,n)
}
PMI can be negative, zero, or positive; thus Fixednesslex(v, n) ? [??,+?], where high
positive values indicate higher degrees of lexical fixedness.
3.2 Measuring Syntactic Fixedness
Compared to literal (non-idiomatic) verb+noun combinations, VNICs are expected to
appear in more restricted syntactic forms. To quantify the syntactic fixedness of a target
verb?noun pair, we thus need to: (i) identify relevant syntactic patterns, namely, those
that help distinguish VNICs from literal verb+noun combinations; and (ii) translate the
frequency distribution of the target pair in the identified patterns into a measure of
syntactic fixedness.
3.2.1 Identifying Relevant Patterns. Determining a unique set of syntactic patterns appro-
priate for the recognition of all idiomatic combinations is difficult indeed: Exactly which
forms an idiomatic combination can occur in is not entirely predictable (Sag et al 2002).
Nonetheless, there are hypotheses about the difference in behavior of VNICs and literal
verb+noun combinations with respect to particular syntactic variations (Nunberg, Sag,
and Wasow 1994). Linguists note that semantic analyzability of VNICs is related to
the referential status of the noun constituent (i.e., the process of idiomatization of a
verb+noun combination is believed to be accompanied by a change from concreteness
to abstractness for the noun). The referential status of the noun is in turn assumed to
be related to the participation of the combination in certain morpho-syntactic forms.
In what follows, we describe three types of syntactic variation that are assumed to be
mostly tolerated by literal combinations, but less tolerated by many VNICs.
Passivization. There is much evidence in the linguistics literature that VNICs often do
not undergo passivization. Linguists mainly attribute this to the fact that in most cases,
only referential nouns appear as the surface subject of a passive construction (Gibbs
and Nayak 1989). Due to the non-referential status of the noun constituent in most
VNICs, we expect that they do not undergo passivization as often as literal verb+noun
combinations do. Another explanation for this assumption is that passives are mainly
used to put focus on the object of a clause or sentence. For most VNICs, no such
communicative purpose can be served by topicalizing the noun constituent through
passivization (Jackendoff 1997). The passive construction is thus considered as one of
the syntactic patterns relevant to measuring syntactic flexibility.5
Determiner type. A strong correlation has been observed between the flexibility of the
determiner preceding the noun in a verb+noun combination and the overall flexibility
of the phrase (Fellbaum 1993; Kearns 2002; Desbiens and Simon 2003). It is however
5 Note that there are idioms that appear primarily in a passivized form, for example, the die is cast (?the
decision is made and will not change?). Our measure can in principle recognize such idioms because we
do not require that an idiom appears mainly in active form; rather, we include voice (passive or active) as
an important part of the syntactic pattern of an idiomatic combination.
68
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
important to note that the nature of the determiner is also affected by other factors,
such as the semantic properties of the noun. For this reason, determiner flexibility is
sometimes argued not to be a good predictor of the overall syntactic flexibility of an ex-
pression. Nonetheless, many researchers consider it as an important part in the process
of idiomatization of a verb+noun combination (Akimoto 1999; Kyto? 1999; Tanabe 1999).
We thus expect a VNIC to mainly appear with one type of determiner.
Pluralization. Although the verb constituent of a VNIC is morphologically flexible, the
morphological flexibility of the noun relates to its referential status (Grant 2005). Again,
one should note that the use of a singular or plural noun in a VNICmay also be affected
by the semantic properties of the noun. Recall that during the idiomatization process,
the noun constituent may become more abstract in meaning. In this process, the noun
may lose some of its nominal features, including number (Akimoto 1999). The non-
referential noun constituent of a VNIC is thus expected to mainly appear in just one of
the singular or plural forms.
Merging the three types of variation results in a pattern set, P , of 11 distinct syntac-
tic patterns that are displayed in Table 1 along with examples for each pattern. When
developing this set of patterns, we have taken into account the linguistic theories about
the syntactic constraints on idiomatic expressions; for example, our choice of patterns
is consistent with the idiom typology developed by Nicolas (1995). Note that we merge
some of the individual patterns into one; for example, we include only one passive
pattern independently of the choice of the determiner or the number of the noun. The
motivation here is to merge low frequency patterns (i.e., those that are expected to
be less common) in order to acquire more reliable evidence on the distribution of a
particular verb?noun pair over the resulting pattern set. In principle, however, the set
can be expanded to include more patterns; it can also be modified to contain different
patterns for different classes of idiomatic combinations.
3.2.2 Devising a Statistical Measure. The second step is to devise a statistical measure
that quantifies the degree of syntactic fixedness of a verb?noun pair, with respect to
Table 1
Patterns used in the syntactic fixedness measure, along with examples for each. A pattern
signature is composed of a verb v in active (vact) or passive (vpass) voice; a determiner (det) that
can be NULL, indefinite (a/an), definite (the), demonstrative (DEM), or possessive (POSS); and a
noun n that can be singular (nsg) or plural (npl).
Pattern No. Pattern Signature Example
1 vact det:NULL nsg give money
2 vact det:a/an nsg give a book
3 vact det:the nsg give the book
4 vact det:DEM nsg give this book
5 vact det:POSS nsg give my book
6 vact det:NULL npl give books
7 vact det:the npl give the books
8 vact det:DEM npl give those books
9 vact det:POSS npl give my books
10 vact det:OTHER nsg,pl give many books
11 vpass det:ANY nsg,pl a/the/this/my book/books was/were given
69
Computational Linguistics Volume 35, Number 1
the selected set of patterns, P . We propose a measure that compares the syntactic
behavior of the target pair with that of a ?typical? verb?noun pair. Syntactic behav-
ior of a typical pair is defined as the prior probability distribution over the patterns in
P . The maximum likelihood estimate for the prior probability of an individual pattern
pt ? P is calculated as
P(pt) =
?
vi?V
?
nj?N
f (vi, nj, pt)
?
vi?V
?
nj?N
?
ptk?P
f (vi, nj, ptk)
=
f (?, ?, pt)
f (?, ?, ?)
(3)
where V is the set of all instances of transitive verbs in the corpus, andN is the set of all
instances of nouns appearing as the direct object of some verb.
The syntactic behavior of the target verb?noun pair ?v,n? is defined as the posterior
probability distribution over the patterns, given the particular pair. The maximum like-
lihood estimate for the posterior probability of an individual pattern pt is calculated as
P(pt | v, n) =
f (v, n, pt)
?
ptk?P
f (v, n, ptk)
=
f (v, n, pt)
f (v, n, ?)
. (4)
The degree of syntactic fixedness of the target verb?noun pair is estimated as
the divergence of its syntactic behavior (the posterior distribution over the patterns)
from the typical syntactic behavior (the prior distribution). The divergence of the two
probability distributions is calculated using a standard information-theoretic measure,
the Kullback Leibler (KL-) divergence (Cover and Thomas 1991):
Fixednesssyn (v, n)
.
= D(P(pt | v,n) ||P(pt))
=
?
ptk?P
P(ptk | v, n) log
P(ptk | v, n)
P(ptk)
(5)
KL-divergence has proven useful in many NLP applications (Resnik 1999; Dagan,
Pereira, and Lee 1994). KL-divergence is always non-negative and is zero if and only
if the two distributions are exactly the same. Thus, Fixednesssyn(v, n) ? [0,+?], where
large values indicate higher degrees of syntactic fixedness.
3.3 A Unified Measure of Fixedness
VNICs are hypothesized to be, in most cases, both lexically and syntactically more fixed
than literal verb+noun combinations (see Section 2). We thus propose a new measure
70
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
of idiomaticity to be a measure of the overall fixedness of a given pair. We define
Fixednessoverall (v, n) as a weighted combination of Fixednesslex and Fixednesssyn:
Fixednessoverall (v, n)
.
= ? Fixednesssyn (v, n) + (1? ?) Fixednesslex (v, n) (6)
where ?weights the relative contribution of the measures in predicting idiomaticity.
Recall that Fixednesslex(v, n) ? [??,+?], and Fixednesssyn(v, n) ? [0,+?]. To
combine them in the overall fixedness measure, we rescale them, so that they fall in
the range [0, 1]. Thus, Fixednessoverall(v, n) ? [0, 1], where values closer to 1 indicate a
higher degree of overall fixedness.
4. VNIC Type Recognition: Evaluation
To evaluate our proposed fixedness measures, we analyze their appropriateness for
determining the degree of idiomaticity of a set of experimental expressions (in the form
of verb?noun pairs, extracted as described in Section 4.1). More specifically, we first use
each measure to assign scores to the experimental pairs. We then use the scores assigned
by each measure to perform two different tasks, and assess the overall goodness of the
measure by looking at its performance in both.
First, we look into the classification performance of each measure by using the
scores to separate idiomatic verb?noun pairs from literal ones in a mixed list. This is
done by setting a threshold, here the median score, where all pairs with scores higher
than the threshold are labeled as idiomatic and the rest as literal.6 For classification, we
report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random
(chance) baseline, referred to as Rand. Second, we examine the retrieval performance
of our fixedness measures by using the scores to rank verb?noun pairs according to
their degree of idiomaticity. For retrieval, we present the precision?recall curves, as
well as the interpolated three-point average precision or IAP?that is, the average of
the interpolated precisions at the recall levels of 20%, 50%, and 80%. The interpolated
average precision and precision?recall curves are commonly used for the evaluation of
information retrieval systems (Manning and Schu?tze 1999), and reflect the goodness of
a measure in placing the relevant items (here, idioms) before the irrelevant ones (here,
literals).
Idioms are often assumed to exhibit collocational behavior to some extent, that is,
the components of an idiom are expected to appear together more often than expected
by chance. Hence, someNLP systems have used collocational measures to identify them
(Smadja 1993; Evert and Krenn 2001). However, as discussed in Section 2, idioms have
distinctive syntactic and semantic properties that separate them from simple colloca-
tions. For example, although collocations involve some degree of semantic idiosyncrasy
(strong tea vs. ?powerful tea), compared to idioms, they typically have a more transparent
meaning, and their syntactic behavior is more similar to that of literal expressions. We
thus expect our fixedness measures that draw on the distinctive linguistic properties
of idioms to be more appropriate than measures of collocation for the identification of
idioms. To verify this hypothesis, in both the classification and retrieval tasks, we com-
pare the performance of the fixedness measures with that of two collocation extraction
measures: an informed baseline, PMI, and a position-based fixedness measure proposed
6 We adopt the median for this particular (balanced) data set, understanding that in practice a suitable
threshold would need to be determined, e.g., based on development data.
71
Computational Linguistics Volume 35, Number 1
by Smadja (1993), which we refer to as Smadja. Next, we provide more details on PMI
and Smadja.
PMI is a widely used measure for extracting statistically significant combinations
of words or collocations. It has also been used for the recognition of idioms (Evert and
Krenn 2001), warranting its use as an informed baseline here for comparison.7 As in
Equation (1), our calculation of PMI here restricts the counts of the verb?noun pair to
the direct object relation. Smadja (1993) proposes a collocation extraction method which
measures the fixedness of a word sequence (e.g., a verb?noun pair) by examining the
relative position of the component words across their occurrences together. We replicate
Smadja?s method, where we measure fixedness of a target verb?noun pair as the spread
(variance) of the co-occurrence frequency of the verb and the noun over 10 relative
positions within a five-word window.8
Recall from Section 3.1 that our Fixednesslex measure is intended as an improve-
ment over the non-compositionalitymeasure of Lin (1999). For the sake of completeness,
we also compare the classification performance of our Fixednesslex with that of Lin?s
(1999) measure, which we refer to as Lin.9
We first elaborate on the methodological aspects of our experiments in Section 4.1,
and then present a discussion of the experimental results in Section 4.2.
4.1 Experimental Setup
4.1.1 Corpus and Data Extraction. We use the British National Corpus (BNC; Burnard
2000); to extract verb?noun pairs, along with information on the syntactic patterns they
appear in. We automatically parse the BNC using the Collins parser (Collins 1999), and
augment it with information about verb and noun lemmas, automatically generated
using WordNet (Fellbaum 1998). We further process the corpus using TGrep2 (Rohde
2004) in order to extract syntactic dependencies. For each instance of a transitive verb,
we use heuristics to extract the noun phrase (NP) in either the direct object position
(if the sentence is active), or the subject position (if the sentence is passive). We then
automatically find the head noun of the extracted NP, its number (singular or plural),
and the determiner introducing it.
4.1.2 Experimental Expressions. We select our development and test expressions from
verb?noun pairs that involve a member of a predefined list of transitive verbs, referred
to as basic verbs. Basic verbs, in their literal use, refer to states or acts that are central
to human experience. They are thus frequent, highly polysemous, and tend to combine
with other words to form idiomatic combinations (Cacciari 1993; Claridge 2000; Gentner
and France 2004). An initial list of such verbs was selected from several linguistic and
psycholinguistic studies on basic vocabulary (Ogden 1968; Clark 1978; Nunberg, Sag,
andWasow 1994; Goldberg 1995; Pauwels 2000; Claridge 2000; Newman and Rice 2004).
We further augmented this initial list with verbs that are semantically related to another
7 PMI has been shown to perform better than or comparable to many other association measures (Inkpen
2003; Mohammad and Hirst, submitted). In our experiments, we also found that PMI consistently
performs better than two other association measures, the Dice coefficient and the log-likelihood measure.
Experiments by Krenn and Evert (2001) showed contradicting results for PMI; however, these
experiments were performed on small-sized corpora, and on data which contained items with very low
frequency.
8 We implement the method as explained in Smadja (1993), taking into account the part-of-speech tags of
the target component words.
9 We implement the method as explained in Lin (1999), using 95% confidence intervals. We thus need to
ignore variants with frequency lower than 4 for which no confidence interval can be formed.
72
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
verb already in the list; for example, lose is added in analogy with find. Here is the final
list of the 28 verbs in alphabetical order:
blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move,
place, pull, push, put, see, set, shoot, smell, take, throw, touch
From the corpus, we extract all the verb?noun pairs (lemmas) that contain any
of these listed basic verbs, and that appear at least 10 times in the corpus in a direct
object relation (irrespective of any intervening determiners or adjectives). From these,
we select a subset that are idiomatic, and another subset that are literal, as follows: A
verb?noun pair is considered idiomatic if it appears in an idiom listed in a credible
dictionary such as the Oxford Dictionary of Current Idiomatic English (ODCIE; Cowie,
Mackin, and McCaig 1983), or the Collins COBUILD Idioms Dictionary (CCID; Seaton
and Macaulay 2002).10 To decide whether a verb?noun pair has appeared in an idiom,
we look for all idioms containing the verb and the noun in a direct-object relation,
irrespective of any intervening determiners or adjectives, and/or any other arguments.
The pair is considered literal if it involves a physical act or state (i.e., the basic semantics
of the verb) and does not appear in any of the mentioned dictionaries as an idiom (or
part of an idiom). From the set of idiomatic pairs, we then randomly pull out 80 de-
velopment pairs and 100 test pairs, ensuring that we have items of both low and high
frequency. We then double the size of each data set (development and test) by adding
equal numbers of literal pairs, with similar frequency distributions. Some of the idioms
corresponding to the experimental idiomatic pairs are: kick the habit,move mountains, lose
face, and keep one?s word. Examples of literal pairs include: move carriage, lose ticket, and
keep fish.
Development expressions are used in devising the fixedness measures, as well as
in determining the values of their parameters as explained in the next subsection. Test
expressions are saved as unseen data for the final evaluation.
4.1.3 Parameter Settings. Our lexical fixedness measure in Equation (2) involves two
parameters, Kv and Kn, which determine the number of lexical variants considered in
measuring the lexical fixedness of a given verb?noun pair. We make the least-biased
assumption on the proportion of variants generated by replacing the verb (Kv) and
those generated by replacing the noun (Kn)?that is, we assume Kv = Kn.
11 We perform
experiments on the development data, where we set the total number of variants (i.e.,
Kv + Kn) from 10 to 100 by steps of 10. (For simplicity, we refer to the total number
of variants as K). Figure 1(a) shows the change in performance of Fixednesslex as a
function of K. Recall that Acc is the classification accuracy, and IAP reflects the average
precision of a measure in ranking idiomatic pairs before non-idiomatic ones. According
to these results, there is not much variation in the performance of the measure for
10 Our development data also contains items from several other dictionaries, such as Chambers Idioms
(Kirkpatrick and Schwarz 1982). However, our test data, which is also used in the token-based
experiments, however, only contains idioms from the two dictionaries ODCIE and CCID. Results
reported in this article are all on test pairs; development pairs are mainly used for the development of the
methods.
11 We also performed experiments on the development data in which we did not restrict the number of
variants, and hence did not enforce the condition Kv = Kn. Instead, we tried using a variety of thresholds
on the similarity scores (from the thesaurus) in order to find the set of most similar words to a given verb
or noun. We found that fixing the number of most similar words is more effective than using a similarity
threshold, perhaps because the actual scores can be very different for different words.
73
Computational Linguistics Volume 35, Number 1
Figure 1
%IAP and %Acc of Fixednesslex and Fixednessoverall over development data.
K ? 20. We thus choose an intermediate value for K that yields the highest accuracy
and a reasonably high precision; specifically, we set K to 50.
The overall fixedness measure defined in Equation (6) also uses a parameter, ?,
which determines the relative weights given to the individual fixedness measures in
the linear combination. We experiment on the development data with different values
of ? ranging from 0 to 1 by steps of .02; results are shown in Figure 1(b). As can be seen
in the figure, the accuracy of Fixednessoverall is not affected much by the change in the
value of ?. The average precision (IAP), however, shows that the combined measure
performs best when somewhat equal weights are given to the two individual measures,
and performs worst when the lexical fixedness component is completely ignored (i.e.,
? is close to 1). These results also reinforce that a complete evaluation of our fixedness
measures should include both metrics, accuracy, and average precision, as they reveal
different aspects of performance. Here, for example, Fixednesssyn (? = 1) has compa-
rable accuracy to Fixednesslex (? = 0), reflecting that the two measures generally give
higher scores to idioms. However, the ranking precision of the latter is much higher
than that of the former, showing that Fixednesslex ranks many of the idioms at the very
top of the list. In all our experiments reported here, we set ? to .6, a value for which
Fixednessoverall shows reasonably good performance according to both Acc and IAP.
4.2 Experimental Results and Analysis
In this section, we report the results of evaluating our measures on unseen test expres-
sions, with parameters set to the values determined in Section 4.1.3. (Results on devel-
opment data have similar trends to those on test data.) We analyze the classification
performance of the individual lexical and syntactic fixedness measures in Section 4.2.1,
and discuss their effectiveness for retrieval in Section 4.2.2. Section 4.2.3 then looks into
the performance of the overall fixedness measure, and Section 4.2.4 presents a summary
and discussion of the results.
4.2.1 Classification Performance. Here, we look into the performance of the individual
fixedness measures, Fixednesslex and Fixednesssyn, in classifying a mixed set of verb?
noun pairs into idiomatic and literal classes. We compare their performance against the
74
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 2
Accuracy and relative error reduction for the two fixedness measures, the two baseline
measures, and Smadja, over all test pairs (TESTall), and test pairs divided by frequency
(TESTflow and TESTfhigh ).
TESTall TESTflow
TESTfhigh
Measure %Acc (%ERR) %Acc (%ERR) %Acc (%ERR)
Rand 50 50 50
PMI 63 (26) 56 (12) 70 (40)
Smadja 54 (8) 64 (28) 62 (24)
Fixednesslex 68 (36) 70 (40) 70 (40)
Fixednesssyn 71 (42) 72 (44) 82 (64)
two baselines, Rand and PMI, as well as the two state-of-the-art methods, Smadja and
Lin. For analytical purposes, we further divide the set of all test expressions, TESTall,
into two sets corresponding to two frequency bands: TESTflow contains 50 idiomatic
and 50 literal pairs, each with total frequency (across all syntactic patterns under
consideration) between 10 and 40; TESTfhigh consists of 50 idiomatic and 50 literal pairs,
each with total frequency of 40 or greater. Classification performances of all measures
except Lin are given in Table 2. Lin does not assign scores to the test verb?noun pairs,
hence we cannot calculate its classification accuracy the same way we do for the other
methods (i.e., using median as the threshold). A separate comparison between Lin and
Fixednesslex is provided at the end of this section.
As can be seen in the first two columns of Table 2, the informed baseline, PMI, shows
a large improvement over the random baseline (26% error reduction) on TESTall. This
shows that many VNICs have turned into institutionalized (i.e., statistically significant)
co-occurrences. Hence, one can get relatively good performance by treating verb+noun
idiomatic combinations as collocations. Fixednesslex performs considerably better than
the informed baseline (36% vs. 26% error reduction on TESTall). Fixednesssyn has the best
performance (shown in boldface), with 42% error reduction over the random baseline,
and 21.6% error reduction over PMI. These results demonstrate that lexical and syntactic
fixedness are good indicators of idiomaticity, better than a simple measure of colloca-
tion such as PMI. On TESTall, Smadja performs only slightly better than the random
baseline (8% error reduction), reflecting that a position-based fixedness measure is not
sufficient for identifying idiomatic combinations. These results suggest that looking into
deep linguistic properties of VNICs is necessary for the appropriate treatment of these
expressions.12
PMI is known to perform poorly on low frequency items. To examine the effect of
frequency on the measures, we analyze their performance on the two divisions of the
12 Performing the ?2 test of statistical significance, we find that the differences between Smadja and our
lexical and syntactic fixedness measures are statistically significant at p < 0.05. However, the differences
in performance between fixedness measures and PMI are not statistically significant. Note that this does
not imply that the differences are not substantial, rather that there is not enough evidence in the observed
data to reject the null hypothesis (that two methods perform the same in general) with high confidence.
Moreover, ?2 is a non-parametric (distribution free) test and hence it has less power to reject a null
hypothesis. Later, when we take into account the actual scores assigned by the measures, we find that all
differences are statistically significant (see Sections 4.2.2?4.2.3 for more details). All significance tests are
performed using the R (2004) package.
75
Computational Linguistics Volume 35, Number 1
test data, corresponding to the two frequency bands, TESTflow and TESTfhigh . Results are
given in the four rightmost columns of Table 2, with the best performance shown in
boldface. As expected, the performance of PMI drops substantially for low frequency
items. Interestingly, although it is a PMI-based measure, Fixednesslex has comparable
performance on all data sets. The performance of Fixednesssyn improves quite a bit
when it is applied to high frequency items, while maintaining similar performance on
the low frequency items. These results show that the lexical and syntactic fixedness
measures perform reasonably well on both low and high frequency items.13 Hence they
can be used with a higher degree of confidence, especially when applied to data that is
heterogeneous with regard to frequency. This is important because, while some VNICs
are very common, others have very low frequency, as noted by Grant (2005). Smadja
shows a notable improvement in performance when data is divided by frequency. This
effect is likely due to the fact that fixedness is measured as the spread of the position-
based (raw) co-occurrence frequencies. Nonetheless, on both data sets the performance
of Smadja remains substantially worse than that of our two fixedness measures (the
differences are statistically significant in three out of the four comparisons at p < .05).
Collectively, these results show that our linguisticallymotivated fixednessmeasures
are particularly suited for identifying idiomatic combinations, especially in comparison
with more general collocation extraction techniques, such as PMI or the position-based
fixedness measure of Smadja (1993). Especially, our measures tend to perform well on
low frequency items, perhaps due to their reliance on distinctive linguistic properties of
idioms.
We now compare the classification performance of Fixednesslex to that of Lin. Unlike
Fixednesslex, Lin does not assign continuous scores to the verb?noun pairs, but rather
classifies them as idiomatic or non-idiomatic. Thus, we cannot use the same threshold
(e.g., median) for the two methods to calculate their classification accuracies in a com-
parable way. Recall also from Section 3.1 that the performance of both these methods
depends on the value of K (the number of variants). We thus measure the classification
precision of the methods at equivalent levels of recall, using the same number of
variants K at each recall level for the two measures. Varying K from 2 to 100 by steps of
4, Lin and Fixednesslex achieve an average classification precision of 81.5% and 85.8%,
respectively. Performing a t-test on the precisions of the two methods confirms that
the difference between the two is statistically significant at p < .001. In addition, our
method has the advantage of assigning a score to a target verb?noun reflecting its degree
of lexical fixedness. Such information can help a lexicographer decide whether a given
verb?noun should be placed in a lexicon.
4.2.2 Retrieval Performance. The classification results suggest that the individual fixed-
ness measures are overall better than a simple measure of collocation at separating
idiomatic pairs from literal ones. Here, we have a closer look at their performance
by examining their goodness in ranking verb?noun pairs according to their degree
of idiomaticity. Recall that the fixedness measures are devised to reflect the degree of
fixedness and hence the degree of idiomaticity of a target verb?noun pair. Thus, the
result of applying eachmeasure to a list of mixed pairs is a list that is ranked in the order
13 In fact, the results show that the performance of both fixedness measures is better when data is divided
by frequency. Although we expect better performance over high frequency items, more investigation is
needed to verify whether the improvement in performance over low frequency items is a meaningful
effect or merely an accident of the data at hand.
76
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
of idiomaticity. For a measure to be considered good at retrieval, we expect idiomatic
pairs to be very frequent near the top of the ranked list, and to become less frequent
towards the bottom. Precision?recall curves are very indicative of this trend: The ideal
measure will have a precision of 100% for all values of recall, namely, themeasure places
all idiomatic pairs at the very top of the ranked list. In reality, although the precision
drops as recall increases, we expect a good measure to keep high precision at most
levels of recall.
Figure 2 depicts the interpolated precision?recall curves for PMI and Smadja, and
for the lexical, syntactic, and overall fixedness measures, over TESTall. Note that the
minimum interpolated precision is 50% due to the equal number of idiomatic and literal
pairs in the test data. In this section, we discuss the retrieval performance of the two
individual fixedness measures; the next section analyzes the performance of the overall
fixedness measure.
The precision?recall curves of Smadja and PMI are nearly flat (with PMI consis-
tently higher than Smadja), showing that the distribution of idiomatic pairs in the lists
ranked by these two measures is only slightly better than random. A close look at the
precision?recall curve of Fixednesslex reveals that, up to the recall level of 50%, the
precision of this measure is substantially higher than that of PMI. This means that,
compared to PMI, Fixednesslex places more idiomatic pairs at the very top of the list.
At higher recall levels (50% and higher), Fixednesslex still consistently outperforms PMI.
Nonetheless, at these recall values, the twomeasures have relatively low precision (com-
pared to the other measures), suggesting that both measures also put many idiomatic
pairs near the bottom of the list. In contrast, the precision?recall curve of Fixednesssyn
shows that its performance is consistently much better than that of PMI: Even at the
recall level of 90%, its precision is close to 70% (cf. 55% precision of PMI).
A comparison of the precision?recall curves of the two individual fixedness mea-
sures reveals their complementary nature. Compared to Fixednesslex, Fixednesssyn
maintains higher precision at very high levels of recall, suggesting that the syntactic
fixedness measure places fewer idiomatic pairs at the bottom of the ranked list. In con-
trast, Fixednesslex has notably higher precision than Fixednesssyn at recall levels of up to
40%, suggesting that the former puts more idiomatic pairs at the top of the ranked list.
Statistical significance tests confirm these observations: Using the Wilcoxon Signed
Rank test (1945), we find that both Fixednesslex and Fixednesssyn produce significantly
different rankings from PMI and Smadja (p  .001). Also, the rankings of the items
Figure 2
Precision?recall curves for PMI, Smadja, and for the fixedness measures, over TESTall.
77
Computational Linguistics Volume 35, Number 1
Table 3
Classification and retrieval performance of the overall fixedness measure over TESTall.
Measure %Acc (%ERR) %IAP
PMI 63 (26) 63.5
Smadja 54 (8) 57.2
Fixednesslex 68 (36) 75.3
Fixednesssyn 71 (42) 75.9
Fixednessoverall 74 (48) 84.7
produced by the two individual fixedness measures are found to be significantly differ-
ent at p < .01.
4.2.3 Performance of the Overall Fixedness Measure. We now look at the classification
and retrieval performance of the overall fixedness measure. Table 3 presents %Acc,
%ERR, and %IAP of Fixednessoverall, repeating that of PMI, Smadja, Fixednesslex, and
Fixednesssyn, for comparison. Here again the error reductions are relative to the random
baseline of 50%. Looking at classification performance (expressed in terms of %Acc
and %ERR), we can see that Fixednessoverall notably outperforms all other measures,
including lexical and syntactic fixedness (18.8% error reduction relative to Fixednesslex,
and 10% error reduction relative to Fixednesssyn). According to the classification
results, each of the lexical and syntactic fixedness measures are good at separating
idiomatic from literal combinations, with syntactic fixedness performing better. Here
we demonstrate that combining them into a single measure of fixedness, while giving
more weight to the better measure, results in a more effective classifier.14 The overall
behavior of this measure as a function of ? is displayed in Figure 3.
As can be seen in Table 3, Fixednesslex and Fixednesssyn have comparable IAP:
75.3% and 75.9%, respectively. In comparison, Fixednessoverall has a much higher IAP
of 84.7%, reinforcing the claim that combining evidence from both lexical and syntac-
tic fixedness is beneficial. Recall from Section 4.2.2 that the two individual fixedness
measures exhibit complementary behavior, as observed in their precision?recall curves
shown in Figure 2. The precision?recall curve of the overall fixedness measure shows
that this measure in fact combines advantages of the two individual measures: At most
recall levels, Fixednessoverall has a higher precision than both individual measures. Sta-
tistical significance tests that look at the actual scores assigned by the measures confirm
that the observed differences in performance are significant. The Wilcoxon Signed Rank
test shows that the Fixednessoverall measure produces a ranking that is significantly
different from those of the individual fixedness measures, the baseline PMI, and Smadja
(at p .001).
4.2.4 Summary and Discussion. Overall, the worst performance belongs to the two collo-
cation extraction methods, PMI and Smadja, both in classifying test pairs as idiomatic or
14 Using a ?2 test, we find a statistically significant difference between the classification performance of
Fixednessoverall and that of Smadja (p < 0.01), and also a marginally significant difference between the
performance of Fixednessoverall and that of PMI (p < .1). Recall from footnote 12 (page 15) that none
of the individual measures? performances significantly differed from that of PMI. Nonetheless, no
significant differences are found between the classification performance of Fixednessoverall and that
of the individual fixedness measures.
78
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Figure 3
Classification performance of Fixednessoverall on test data as a function of ?.
literal, and in ranking the pairs according to their degree of idiomaticity. This suggests
that although some VNICs are institutionalized, many do not appear with markedly
high frequency, and hence only looking at their frequency is not sufficient for their
recognition. Moreover, a position-based fixedness measure does not seem to sufficiently
capture the syntactic fixedness of VNICs in contrast to the flexibility of literal phrases.
Fixednessoverall is the best performer of all, supporting the hypothesis that many VNICs
are both lexically and syntactically fixed, more so than literal verb+noun combinations.
In addition, these results demonstrate that incorporating such linguistic properties into
statistical measures is beneficial for the recognition of VNICs.
Althoughwe focus on experimental expressionswith frequency higher than 10, PMI
still shows great sensitivity to frequency differences, performing especially poorly on
items with frequency between 10 and 40. In contrast, none of the fixedness measures
are as sensitive to such frequency differences. Especially interesting is the consistent
performance of Fixednesslex, which is a PMI-based measure, on low and high frequency
items. These observations put further emphasis on the importance of devising new
methods for extracting multiword expressions with particular syntactic and semantic
properties, such as VNICs.
To further analyze the performance of the fixedness measures, we look at the top
and bottom 20 pairs (10%) in the lists ranked by each fixedness measure. Interestingly,
the list ranked by Fixednessoverall contains no false positives ( fp) in the top 20 items,
and no false negatives ( fn) in the bottom 20 items, once again reinforcing the usefulness
of combining evidence from the individual lexical and syntactic fixedness measures.
False positive and false negative errors found in the top and bottom 20 ranked pairs,
respectively, for the syntactic and lexical fixedness measures are given in Table 4. (Note
that fp errors are the non-idiomatic pairs ranked at the top, whereas fn errors are the
idiomatic pairs ranked at the bottom.)
We first look at the errors made by Fixednesssyn. The first fp error, throw hat, is
an interesting one: even though the pair is not an idiomatic expression on its own,
it is part of the larger idiomatic phrase throw one?s hat in the ring, and hence exhibits
syntactic fixedness. This shows that our methods can be easily extended to identify
other types of verb phrase idiomatic combinations which exhibit syntactic behavior
similar to VNICs. Looking at the frequency distribution of the occurrence of the other
two fp errors, touch finger and lose home, in the 11 patterns from Table 1, we observe that
both pairs tend to appear mainly in the patterns ?vact det:POSS nsg? (touch one?s finger,
lose one?s home) and/or ?vact det:POSS npl? (touch one?s fingers). These examples show
79
Computational Linguistics Volume 35, Number 1
Table 4
Errors found in the top and bottom 20 pairs in the lists ranked by the two individual fixedness
measures; fp stands for false positive, fn stands for false negative.
Measure: Fixednesssyn Fixednesslex
Error Type: fp fn fp fn
throw hat make pile push barrow have moment
touch finger keep secret blow bridge give way
lose home keep hand
that syntactic fixedness is not a sufficient condition for idiomaticity. In other words,
it is possible for non-idiomatic expressions to be syntactically fixed for reasons other
than semantic idiosyncrasy. In these examples, the nouns finger and home tend to be
introduced by a possessive determiner, because they often belong to someone. It is also
important to note that these two patterns have a low prior (i.e., verb?noun pairs do
not typically appear in these patterns). Hence, an expression with a strong tendency to
appear in such patterns will be given a high syntactic fixedness score.
The frequency distribution of the two fn errors for Fixednesssyn reveals that they are
given low scores mainly because their distributions are similar to the prior. Even though
make pile preferably appears in the two patterns ?vact det:a/an nsg? and ?vact det:NULL
npl,? both patterns have reasonably high prior probabilities. Moreover, because of the
low frequency of make pile (< 40), the evidence is not sufficient to distinguish it from a
typical verb?noun pair. The pair keep secret has a high frequency, but its occurrences are
scattered across all 11 patterns, closely matching the prior distribution. The latter exam-
ple shows that syntactic fixedness is not a necessary condition for idiomaticity either.15
Analyzing the errors made by Fixednesslex is more difficult as many factors may
affect scores given by this measure. Most important is the quality of the automatically
generated variants. We find that in one case, push barrow, the first 25 distributionally
similar nouns (taken from the automatically built thesaurus) are proper nouns, perhaps
because Barrow is a common last name. In general, it seems that the similar verbs and
nouns for a target verb?noun pair are not necessarily related to the same sense of the
target word. Another possible source of error is that in this measure we use PMI as an
indirect clue to idiomaticity. In the case of give way and keep hand, many of the variants
are plausible combinations with very high frequency of occurrence, for example, give
opportunity, give order, find way for the former, and hold hand, put hand, keep eye for the
latter. Whereas some of these high-frequency variants are literal (e.g., hold hand) or
idiomatic (e.g., keep eye), many have metaphorical interpretations (e.g., give opportunity,
find way). In our ongoing work, we use lexical and syntactic fixedness measures, in com-
bination with other linguistically motivated features, to distinguish such metaphori-
cal combinations from both literal and idiomatic expressions (Fazly and Stevenson,
to appear).
One way to decrease the likelihood of making any of these errors is to combine
evidence from the lexical and syntactic fixedness of idioms. As can be seen in Table 4, the
two fixedness measures make different errors, and combining them results in a measure
15 One might argue that keep secret is more semantically analyzable and hence less idiomatic than an
expression such as shoot the breeze. Nonetheless, it is still semantically more idiosyncratic than a fully
literal combination such as keep a pen, and hence should not be ranked at the very bottom of the list.
80
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
(the overall fixedness) that makes fewer errors. In the future, we intend to also look into
other properties of idioms, such as their semantic non-compositionality, as extra sources
of information.
5. Determining the Canonical Forms of VNICs
Our evaluation of the fixedness measures demonstrates their usefulness for the au-
tomatic recognition of VNICs. Recall from Section 2 that idioms appear in restricted
syntactic forms, often referred to as their canonical forms (Glucksberg 1993; Riehemann
2001; Grant 2005). For example, the idiom pull one?s weight mainly appears in this form
(when used idiomatically). The lexical representation of an idiomatic combination thus
must contain information about its canonical forms. Such information is necessary both
for automatically generating appropriate forms (e.g., in a natural language generation
system or a machine translation system), and for inclusion in dictionaries for learners
(e.g., in the context of computational lexicography).
Because VNICs are syntactically fixed, they are mostly expected to have a small
number of canonical forms. For example, shoot the breeze is listed in many idiom dictio-
naries as the canonical form for ?shoot, breeze?. Also, hold fire and hold one?s fire are listed
in CCID as canonical forms for ?hold, fire?. We expect a VNIC to occur in its canonical
form(s) with substantially higher frequency than in any other syntactic patterns. We
thus devise an unsupervised method that discovers the canonical form(s) of a given
idiomatic verb?noun pair by examining its frequency of occurrence in each syntactic
pattern under consideration. Specifically, the set of the canonical form(s) of the target
pair ?v, n? is defined as
C(v, n) = {ptk ? P | z(v, n, ptk) > Tz} (7)
Here, P is the set of patterns (see Table 1), and the condition z(v, n, ptk) > Tz determines
whether the frequency of the target pair ?v,n? in ptk is substantially higher than its
frequency in other patterns; z(v, n, ptk) is calculated using the statistic z-score as in
Equation (8), and Tz is a predefined threshold.
z(v, n, ptk) =
f (v, n, ptk)? f
s (8)
where f is the sample mean and s the sample standard deviation.
The statistic z(v, n, ptk) indicates how far and in which direction the frequency of
occurrence of the target pair ?v, n? in a particular pattern ptk deviates from the sample
mean, expressed in units of the sample standard deviation. To decide whether ptk is a
canonical pattern for the target pair, we check whether its z-score, z(v, n, ptk), is greater
than a threshold Tz. Here, we set Tz to 1, based on the distribution of z and through
examining the development data.
We evaluate our unsupervised canonical form identification method by verifying
its predicted forms against ODCIE and CCID. Specifically, for each of the 100 idiomatic
pairs in TESTall, we calculate the precision and recall of its predicted canonical forms
(those whose z-scores are above Tz), compared to the canonical forms listed in the two
dictionaries. The average precision across the 100 test pairs is 81.2%, and the average
recall is 88% (with 68 of the pairs having 100% precision and 100% recall). Moreover, we
81
Computational Linguistics Volume 35, Number 1
find that for the overwhelming majority of the pairs, 86%, the predicted canonical form
with the highest z-score appears in the dictionary entry of the pair.
According to the entries in ODCIE and CCID, 93 out of the 100 idiomatic pairs in
TESTall have one canonical form. Our canonical form extractionmethod on average finds
1.2 canonical forms for these 100 pairs (one canonical form for 79 of them, two for 18,
and three for 3 of these). Generally, our method tends to extract more canonical forms
than listed in the dictionaries. This is a desired property, because idiom dictionaries
often do not exhaustively list all canonical forms, but themost dominant ones. Examples
of such cases include: see the sights for which our method also finds see sights as a canon-
ical form, and catch one?s attention for which our method also finds catch the attention.
There are also cases where our method finds canonical forms for a given expression due
to noise resulting from the use of the expression in a non-idiomatic sense. For example,
for hold one?s horses, our method also finds hold the horse and hold the horses as canonical
forms. Similarly, for get the bird, our method also finds get a bird.
In a few cases (4 out of 100), our method finds fewer canonical forms than listed
in the dictionaries. These are catch the/one?s imagination, have a/one?s fling, make a/one?s
mark, and have a/the nerve. For the first two of these, the z-score of the missed pattern
is only slightly lower than our predefined threshold. In other cases (8 out of 100), none
of the canonical forms extracted by our method match those in a dictionary. Some of
these expressions also have a non-idiomatic sense which might be more dominant than
the idiomatic usage. For example, for give the push and give the flick, our method finds
give a push and give a flick, respectively, perhaps due to the common use of the latter
forms as light verb constructions. Formake one?s peace, our method finds a different form,
make peace, which seems a plausible canonical form; and moreover, the canonical form
listed in the dictionaries (make one?s peace) has a z-score which is only slightly lower
than our threshold. There is also one case where our method finds a canonical form
that corresponds to a different idiom using the same verb+noun: we find lose touch as
a canonical form, whereas the dictionaries list an idiom with a different canonical form
(lose one?s touch) as the idiom with lose and touch.
In general, canonical forms extracted by our method are reasonably accurate, but
may need to be further analyzed by a lexicographer to filter out incorrectly found
patterns. Moreover, our method extracts new canonical forms for some expressions,
which could be used to augment dictionaries.
6. Automatic Identification of VNIC Tokens
In previous sections, we have provided an analysis of the lexical and syntactic behavior
of idiomatic expressions. We have shown that our proposed techniques that draw on
such properties can successfully distinguish an idiomatic verb+noun combination (a
VNIC type) such as get the sack from a non-idiomatic (literal) one such as get the bag. It is
important, however, to note that a potentially idiomatic expression such as get the sack
can also have a literal interpretation in a given context, as in Joe got the sack from the top
shelf . This is true of many potential idioms, although the relative proportion of literal
usages may differ from one expression to another. For example, an expression such as
see stars is much more likely to have a literal interpretation than get the sack (according to
our findings in the BNC). Identification of idiomatic tokens in context is thus necessary
for a full understanding of text, and this will be the focus of Sections 6 and 7.
Recent studies addressing token identification for idiomatic expressions mainly
perform the task as one of word sense disambiguation, and draw on the local context of
82
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
a token to disambiguate it. Such techniques either do not use any information regarding
the linguistic properties of idioms (Birke and Sarkar 2006), or mainly focus on the
property of non-compositionality (Katz and Giesbrecht 2006). Studies that do make
use of deep linguistic information often handcode the knowledge into the systems
(Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006). Our goal is
to develop techniques that draw on the specific linguistic properties of idioms for their
identification, without the need for handcoded knowledge or manually labelled train-
ing data. Such unsupervised techniques can also help provide automatically labelled
(noisy) training data to bootstrap (semi-)supervised methods.
In Sections 3 and 4, we showed that the lexical and syntactic fixedness of idioms
is especially relevant to their type-based recognition. We expect such properties to also
be relevant for their token identification. Moreover, we have shown that it is possible to
learn about the fixedness of idioms in an unsupervised manner. Here, we propose unsu-
pervised techniques that draw on the syntactic fixedness of idioms to classify individual
tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward a
classification technique that combines such information (in the form of noisy training
data) with evidence from the local context of usages of an expression. In Section 6.1,
we elaborate on the underlying assumptions of our token identification techniques.
Section 6.2 then describes our proposed methods that draw on these assumptions to
perform the task.
6.1 Underlying Assumptions
Although there may be fine-grained differences in meaning across the idiomatic us-
ages of an expression, as well as across its literal usages, we assume that the idiomatic
and literal usages correspond to two coarse-grained senses of the expression. We will
refer then to each of the literal and idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that each may have multiple fine-grained
senses.
Recall from Section 2 that idioms tend to be somewhat fixed with respect to the
syntactic configurations in which they occur. For example, pull one?s weight tends to
mainly appear in this form when used idiomatically. Other forms of the expression,
such as pull the weights, typically are only used with a literal meaning. In other words,
an idiom tends to have one (or a small number of) canonical form(s), which are its most
preferred syntactic patterns.16 Here we assume that, in most cases, idiomatic usages of
an expression tend to occur in its canonical form(s). We also assume that, in contrast,
the literal usages of an expression are less syntactically restricted, and are expressed
in a greater variety of patterns. Because of their relative unrestrictedness, literal usages
may occur in a canonical form for that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative syntactic patterns for the expression,
which we refer to as the non-canonical forms of the expression, are more likely to be
literal.
Drawing on these assumptions, we develop unsupervised methods that deter-
mine, for each verb+noun token in context, whether it has an idiomatic or a literal
16 As noted previously, 93 out of the 100 idiomatic pairs in TESTall have one canonical form, according to the
entries in ODCIE and CCID. Also, our canonical form extraction method on average finds 1.2 canonical
forms for the 100 test idioms.
83
Computational Linguistics Volume 35, Number 1
interpretation. Clearly, the success of our methods depends on the extent to which these
assumptions hold (we will return to these assumptions in Section 7.2.3).
6.2 Proposed Methods
This section elaborates on our proposed methods for identifying the idiomatic and
literal usages of a verb+noun combination: the CFORM method that uses knowledge
of canonical forms only, and the CONTEXT method that also incorporates distributional
evidence about the local context of a token. Both methods draw on our assumptions
described herein, that usages in the canonical form(s) for a potential idiom are more
likely to be idiomatic, and those in other forms are more likely to be literal. Because
our methods need information about canonical forms of an expression, we use the
unsupervisedmethod described in Section 5 to find these automatically. In the following
discussion, we describe each method in more detail.
CFORM. This method classifies an instance (token) of an expression as idiomatic if it
occurs in one of the automatically determined canonical form(s) for that expression
(e.g., pull one?s weight), and as literal otherwise (e.g., pull a weight, pull the weights). The
underlying assumption of this method is that information about the canonical form(s) of
an idiom type can provide a reasonably accurate classification of its individual instances
as literal or idiomatic.
CONTEXT. Recall our assumption that the idiomatic and literal usages of an idiom corre-
spond to two coarse-grained meanings of the expression. It is natural to further assume
that the literal and idiomatic usages have more in common semantically within each
group than between the two groups. Adopting a distributional approach to meaning?
where the meaning of an expression is approximated by the words with which it co-
occurs (Firth 1957)?we would expect the literal and idiomatic usages of an expression
to typically occur with different sets of words.
Indeed, in a supervised setting, Katz and Giesbrecht (2006) show that the local
context of an idiom usage is useful in identifying its sense. Inspired by this work, we
propose an unsupervisedmethod that incorporates distributional information about the
local context of the usages of an idiom, in addition to the (syntactic) knowledge about
its canonical forms, in order to determine if its token usages are literal or idiomatic.
To achieve this, the method compares the context surrounding a test instance of an
expression to ?gold-standard? contexts for the idiomatic and literal usages of the expres-
sion, which are taken from noisy training data automatically labelled using canonical
forms.17
For each test instance of an expression, the CONTEXT method thus compares its
co-occurring words to two sets of gold-standard co-occurring words: one typical of
idiomatic usages and one typical of literal usages of the expression (we will shortly
explain precisely how we find these). If the test token is determined to be (on aver-
age) more similar to the idiomatic usages, then it is labelled as idiomatic. Other-
wise, it is labelled as literal. To measure similarity between two sets of words, we use
17 The two CONTEXT methods in our earlier work (Cook, Fazly, and Stevenson 2007) were biased because
they used information about the canonical form of a test token (in addition to context information).
We found that when the bias was removed, the similarity measure used in those techniques was not
as effective, and hence we have developed a different method here.
84
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
a standard distributional similarity measure, Jaccard, defined subsequently.18 In the
following equation A and B represent the two sets of words to be compared:
Jaccard(A,B) = A ? B
A ? B (9)
Nowwe explain how the CONTEXT method finds typically co-occurring words for each
of the idiomatic and literal meanings of an expression. Note that unlike in a supervised
setting, here we do not assume access to manually annotated training data. We thus use
knowledge of automatically acquired canonical forms to find these.
The CONTEXT method labels usages of an expression in a leave-one-out strategy,
where each test token is labelled by using the other tokens as noisy training (gold-
standard) data. Specifically, to provide gold-standard data for each instance of an
expression, we first divide the other instances (of the same expression) into likely-
idiomatic and likely-literal groups, where the former group contains usages in canonical
form(s) and the latter contains usages in non-canonical form(s). We then pick represen-
tative usages from each group by selecting the K instances that are most similar to the
instance being labelled (the test token) according to the Jaccard similarity score.
Recall that we assume canonical form(s) are predictive of the idiomatic usages and
non-canonical form(s) are indicative of the literal usages of an expression. We thus
expect the co-occurrence sets of the selected canonical and non-canonical instances to
reflect the idiomatic and literal meanings of the expression, respectively. We take the
average similarity of the test token to the K nearest canonical instances (likely idiomatic)
and the K nearest non-canonical instances (likely literal), and label the test token accord-
ingly.19 In the event that there are less than K canonical or non-canonical form usages
of an expression, we take the average similarity over however many instances there are
of this form. If we have no instances of one of these forms, we classify each token as
idiomatic, the label we expect to be more frequent.
7. VNIC Token Identification: Evaluation
To evaluate the performance of our proposed token identification methods, we use
each in a classification task, in which the method indicates for each instance of a given
expression whether it has an idiomatic or a literal interpretation. Section 7.1 explains
the details of our experimental setup. Section 7.2 then presents the experimental results
as well as some discussion and analysis.
7.1 Experimental Setup
7.1.1 Experimental Expressions and Annotation. In our token classification experiments,
we use a subset of the 180 idiomatic expressions in the development and test data sets
used in the type-based experiments of Section 4. From the original 180 expressions, we
discard those whose frequency in the BNC is lower than 20, to increase the likelihood
that there are both literal and idiomatic usages of each expression. We also discard any
18 It is possible to incorporate extra knowledge sources, such as WordNet, for measuring similarity
between two sets of words. However, our intention is to focus on purely unsupervised, knowledge-lean
approaches.
19 We also tried using the average similarity of the test token to all instances in each group. However,
we found that focusing on the most similar instances from each group performs better.
85
Computational Linguistics Volume 35, Number 1
expression that is not from the two dictionaries ODCIE and CCID (see Section 4.1.2
for more details on the original data sets). This process results in the selection of
60 candidate verb?noun pairs.
For each of the selected pairs, 100 sentences containing its usage were randomly ex-
tracted from the automatically parsed BNC, using themethod described in Section 4.1.1.
For a pair which occurs less than 100 times in the BNC, all of its usages were extracted.
Two judges were asked to independently label each use of each candidate expression as
literal, idiomatic, or unknown. When annotating a token, the judges had access to only
the sentence in which it occurred, and not the surrounding sentences. If this context was
insufficient to determine the class of the expression, the judge assigned the unknown
label. In an effort to assure high agreement between the judges? annotations, the judges
were also provided with the dictionary definitions of the idiomatic meanings of the
expressions.
Idiomaticity is not a binary property; rather it is known to fall on a continuum
from completely semantically transparent, or literal, to entirely opaque, or idiomatic.
The human annotators were required to pick the label, literal or idiomatic, that best fit
the usage in their judgment; they were not to use the unknown label for intermediate
cases. Figurative extensions of literal meanings were classified as literal if their overall
meaning was judged to be fairly transparent, as in You turn right when we hit the road
at the end of this track (taken from the BNC). Sometimes an idiomatic usage, such as have
word in At the moment they only had the word of Nicola?s husband for what had happened
(also taken from the BNC), is somewhat directly related to its literal meaning, which
is not the case for more semantically opaque idioms such as hit the roof. This sentence
was classified as idiomatic because the idiomatic meaning is muchmore salient than the
literal meaning.
First, our primary judge, a native English speaker and an author of this paper,
annotated each use of each candidate expression. Based on this judge?s annotations, we
removed the 25 expressions with fewer than 5 instances of either of their literal or idi-
omatic meanings, leaving 28 expressions.20 (We will revisit the 25 removed expressions
in Section 7.2.4.) The remaining expressions were then split into development (DEV) and
test (TEST) sets of 14 expressions each. The data was divided such that DEV and TEST
would be approximately equal with respect to the frequency of their expressions, as
well as their proportion of idiomatic-to-literal usages (according to the primary judge?s
annotations). At this stage, DEV and TEST contained a total of 813 and 743 tokens,
respectively.
Our second judge, also a native English-speaking author of this paper, then anno-
tated DEV and TEST sentences. The observed agreement and unweighted kappa score
(Cohen 1960) on TEST were 76% and 0.62, respectively. The judges discussed tokens on
which they disagreed to achieve a consensus annotation. Final annotations were gener-
ated by removing tokens that received the unknown label as the consensus annotation,
leaving DEV and TEST with a total of 573 and 607 tokens, and an average of 41 and 43 to-
kens per expression, respectively. Table 5 shows the DEV and the TEST verb?noun pairs
used in our experiments. The table also contains information on the number of tokens
considered for each pair, as well as the percentage of its usages which are idiomatic.
20 From the original set of 60 expressions, seven were excluded because our primary annotator did not
provide any annotations for them. These include catch one?s breath, cut one?s losses, and push one?s luck (for
which our annotator did not have access to a literal interpretation); and blow one?s (own) horn, pull one?s
hair, give a lift, and get the bird (for which our annotator did not have access to an idiomatic meaning).
86
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 5
Experimental DEV and TEST verb?noun pairs, their token frequency (FRQ), and the percentage of
their usages that are idiomatic (%IDM), ordered in decreasing %IDM.
DEV TEST
verb?noun FRQ %IDM verb?noun FRQ %IDM
find foot 52 90 have word 89 90
make face 30 90 lose thread 20 90
get nod 26 89 get sack 50 86
pull weight 33 82 make mark 85 85
kick heel 38 79 cut figure 43 84
hit road 31 77 pull punch 22 82
take heart 79 73 blow top 28 82
pull plug 65 69 make scene 48 58
blow trumpet 29 66 make hay 17 53
hit roof 17 65 get wind 29 45
lose head 38 55 make hit 14 36
make pile 25 32 blow whistle 78 35
pull leg 51 22 hold fire 23 30
see star 61 8 hit wall 61 11
7.1.2 Baselines, Parameters, and Performance Measures. We compare the performance of
our proposed methods, CFORM and CONTEXT, with the baseline of always predicting
an idiomatic interpretation, the most frequent meaning in our development data. We
also compare the unsupervised methods against a supervised method, SUP, which is
similar to CONTEXT, except that it forms the idiomatic and literal co-occurrence sets
from manually annotated data (instead of automatically labelled data using canonical
forms). Like CONTEXT, SUP also classifies tokens in a leave-one-out methodology using
the K idiomatic and literal instances which are most similar to a test token. For both
CONTEXT and SUP, we set the value of K (the number of similar instances used as
gold-standard) to 5, since experiments on DEV indicated that performance did not vary
substantially using a range of values of K.
For all methods, we report the accuracy macro-averaged over all expressions in
TEST. We use the individual accuracies (accuracies for the individual expressions) to
perform t-tests for verifying whether different methods have significantly different
performance. To further analyze the performance of the methods, we also report their
recall and precision on identifying usages from each of the idiomatic and literal classes.
7.2 Experimental Results and Analysis
We first discuss the overall performance of our proposed unsupervised methods in
Section 7.2.1. Results reported in Section 7.2.1 are on TEST (results on DEV have similar
trends, unless noted otherwise). Next, we look into the performance of our methods
on expressions with different proportions of idiomatic-to-literal usages in Section 7.2.2,
which presents results on TEST and DEV combined, as explained subsequently. Sec-
tion 7.2.3 provides an analysis of the errors made because of using canonical forms, and
identifies some possible directions for future work. In Section 7.2.4, we present results
on a new data set containing expressions with highly skewed proportion of idiomatic-
to-literal usages.
87
Computational Linguistics Volume 35, Number 1
Table 6
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on TEST expressions.
Method %Acc (%ERR)
Baseline 61.9
Unsupervised CONTEXT 65.8 (10.2)
CFORM 72.4 (27.6)
Supervised SUP 82.7 (54.6)
7.2.1 Overall Performance. Table 6 shows the macro-averaged accuracy on TEST of our
two unsupervised methods, as well as that of the baseline and the supervised method
for comparison. The best unsupervised performance is indicated in boldface.
As the table shows, both of our unsupervised methods as well as the supervised
method outperform the baseline, confirming that the canonical forms of an expression,
and local context, are both informative in distinguishing literal and idiomatic instances
of the expression.21 Moreover, CFORM outperforms CONTEXT (difference is marginally
significant at p < .06), which is somewhat unexpected, as CONTEXT was proposed
as an improvement over CFORM in that it combines contextual information along
with the syntactic information provided by CFORM. We return to these results later
(Section 7.2.3) to offer some reasons as to why this might be the case. However, the
results using CFORM confirm our hypothesis that canonical forms?which reflect the
overall behavior of a verb+noun type?are strongly informative about the class of a
token. Importantly, this is the case even though the canonical forms that we use are
imperfect knowledge obtained automatically through an unsupervised method.
Comparing CFORM with SUP, we observe that even though on average the latter
outperforms the former, the difference is not statistically significant (p > .1). A close
look at the performance of these methods on the individual expressions reveals that
neither consistently outperforms the other on all (or even most) expressions. Moreover,
as we will see in Section 7.2.2, SUP seems to gain most of its advantage over CFORM on
expressions with a low proportion of idiomatic usages, for which canonical forms tend
to have less predictive value (see Section 7.2.3 for details).
Recall that both CONTEXT and SUP label each token by comparing its local context
to those of its K nearest ?idiomatic? and its K nearest ?literal? usages. The difference is
that CONTEXT uses noisy (automatically) labelled data to identify these nearest usages
for each token, whereas SUP uses manually labelled data. One possible direction for fu-
ture work is thus to investigate whether providing substantially larger amounts of data
alleviates the effect of noise, as is often found to be the case by researchers in the field.
7.2.2 Performance Based on Class Distribution. Recall from Section 6 that both of our un-
supervised techniques for token identification depend on how accurately the canonical
forms of an expression can be acquired. The canonical form acquisition technique which
we use here works well if the idiomatic meaning of an expression is sufficiently frequent
compared to its literal usage. In this section, we thus examine the performance of the
21 Performing a paired t-test, we find that the difference between the baseline and CFORM is marginally
significant, p < .06, whereas the difference between baseline and CONTEXT is not statistically significant.
The difference between the baseline and SUP is significant at p < .01. The trend on DEV is somewhat
similar: baseline and CFORM are significantly different at p < .05; SUP is marginally different from
baseline at p < .06.
88
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 7
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 28 expressions
in DT (DEV and TEST combined), divided according to the proportion of idiomatic-to-literal
usages (high and low).
DTIhigh DTIlow
Method %Acc (%ERR) %Acc (%ERR)
Baseline 81.4 35.0
Unsupervised CONTEXT 80.6 (?4.3) 44.6 (14.8)
CFORM 84.7 (17.7) 53.4 (28.3)
Supervised SUP 84.4 (16.1) 76.8 (64.3)
token identification methods for expressions with different proportions of idiomatic-to-
literal usages.
We merge DEV and TEST (referring to the new set as DT), and then divide the re-
sulting set of 28 expressions according to their proportion of idiomatic-to-literal usages
(as determined by the human annotations) as follows.22 Looking at the proportion of
idiomatic usages of our expressions in Table 5, we can see that there are gaps between
55% and 65% in DEV, and between 58% and 82% in TEST, in terms of proportion
of idiomatic usages. The value of 65% thus serves as a natural lower bound for dominant
idiomatic usage, and the value of 58% as a natural upper bound for non-dominant
idiomatic usage. We therefore split DT into two sets: DTIhigh contains 17 expressions with
65?90% of their usages being idiomatic (i.e., their idiomatic usage is dominant), whereas
DTIlow contains 11 expressions with 8?58% of their occurrences being idiomatic (i.e., their
idiomatic usage is not dominant).
Table 7 shows the average accuracy of all the methods on these two groups of
expressions, with the best performance on each group shown in boldface. We first look
at the performance of our methods on DTIhigh . On these expressions, CFORM outperforms
both the baseline (difference is not statistically significant) and CONTEXT (difference is
statistically significant at p < .05). CFORM also has a comparable performance to the su-
pervised method, reinforcing that for these expressions accurate canonical forms can be
acquired and that such knowledge can be used with high confidence for distinguishing
idiomatic and literal usages in context.
We now look into the performance on expressions in DTIlow . On these, both CFORM
and CONTEXT outperform the baseline, showing that even for expressions whose idi-
omatic meaning is not dominant, automatically acquired canonical forms can help with
their token classification. Nonetheless, both these methods perform substantially worse
than the supervised method, reinforcing that the automatically acquired canonical
forms are noisier, and hence less predictive, than they are for expressions in DTIhigh .
The poor performance of the unsupervised methods on expressions in DTIlow (com-
pared to the supervised performance) is likely to be mostly due to the less predictive
canonical forms extracted for these expressions. In general, we can conclude that when
canonical forms can be extracted with a high accuracy, the performance of the CFORM
method is comparable to that of a supervised method. One possible way of improving
the performance of unsupervised methods is thus to develop more accurate techniques
for the automatic acquisition of canonical forms.
22 We combine the two sets in order to have a sufficient number of expressions in each group after division.
89
Computational Linguistics Volume 35, Number 1
Table 8
Confusion matrix for CFORM on expression blow trumpet. idm = idiomatic class; lit = literal class;
tp = true positive; fp = false positive; fn = false negative; tn = true negative.
True Class
idm lit
Predicted idm 17 = tp 6 = fp
Class lit 2 = fn 4 = tn
Table 9
Formulas for calculating Sens and PPV (recall and precision for the idiomatic class), and Spec
and NPV (recall and precision for the literal class) from a confusion matrix.
recall (R) precision (P)
idm Sens =
tp
tp+ fn
PPV =
tp
tp+ fp
lit Spec = tn
tn+ fp
NPV = tn
tn+ fn
Accuracy is often not a sufficient measure for the evaluation of a binary (two-class)
classifier, especially when the number of items in the two classes (here, idiomatic and
literal) differ. Instead, one can have a closer look at the performance of a classifier by
examining its confusion matrix, which compares the labels predicted by the classifier
for each item with its true label. As an example, the confusion matrix of the CFORM
method for the expression blow trumpet is given in Table 8.
Note that the choice of idiomatic as the positive class (and literal as the negative
class) is arbitrary; however, because our ultimate goal is to identify idiomatic usages,
there is a natural reason for this choice. To summarize a confusion matrix, four standard
measures are often used, which are calculated from the cells in the matrix. The measures
are sensitivity (Sens), positive predictive value (PPV), specificity (Spec), and negative
predictive value (NPV), and are calculated as in Table 9. As stated in the table, Sens
and PPV are equivalents of recall and precision for the positive (idiomatic) class, also
referred to as Ridm and Pidm later in the article. Similarly, Spec and NPV are equivalents
of recall and precision for the negative (literal) class, also referred to as Rlit and Plit.
23
Table 10 gives the trimmed mean values of these four performance measures over
expressions in DTIhigh and DTIlow for the baseline, the two unsupervised methods, and the
supervised method.24 (The performance measures on individual expressions are given
in Tables 12, 13, and 14 in the Appendix.) Table 10 shows that, as expected, the baseline
has very high Sens (100% recall on identifying idiomatic usages), but very low Spec (0%
23 We mainly refer to these measures using their standard names in the literature: Sens, PPV, Spec, and
NPV. Alongside the standard names, we use the more expressive names Ridm, Pidm, Rlit, and Plit, to
remind the reader about the semantics of the measures.
24 When averaging interdependent measures, such as precision and recall, one needs to make sure that
the observed trend in the averages is consistent with that in the individual values. Trimmed mean is a
standard statistic used in such cases, which is equivalent to the mean after discarding a percentage (often
between 5 and 25) of the sample data at the high and low ends. Here, we report a 14%-trimmed mean,
which involves removing two data points from each end. The analysis presented here is based on the
trimmed means, as well as the individual values of the performance measures.
90
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 10
Detailed classification performance of all methods over DTIhigh and DTIlow . Performance is given
using four measures: Sens or Ridm, PPV or Pidm, Spec or Rlit, and NPV or Plit, macro-averaged
using 14%-trimmed mean.
Data Set Method Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
Baseline 1.00 .82 0.00 0.00
DTIhigh CONTEXT .97 .84 .11 .18
CFORM .95 .92 .61 .71
SUP .99 .86 .22 .53
Data Set Method Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
Baseline 1.00 .36 0.00 0.00
DTIlow CONTEXT .89 .37 .22 .63
CFORM .86 .43 .36 .86
SUP .44 .62 .88 .80
recall on identifying literal usages). We thus expect a well-performing method to have
lower Sens than the baseline, but higher Spec and also higher PPV and NPV (i.e., higher
precision on both idiomatic and literal usages).
Looking at performance on DTIhigh , we find that all three methods have reasonably
high Sens and PPV, revealing that the methods are good at labeling idiomatic usages.
Performance on literal usages, however, differs across the three methods. CONTEXT has
very low Spec andNPV, showing that it tends to label most tokens?including the literal
ones?as idiomatic. A close look at the performance of this method on the individual
expressions also confirms this tendency: on many expressions (10 out of 17) the Spec
and NPV of CONTEXT are both zero (see Table 13 in the Appendix). As we will see in
Section 7.2.3, this tendency is partly due to the distribution of the idiomatic and literal
usages in canonical and non-canonical forms; because literal usages can also appear in
a canonical form, for many expressions there are often not many non-canonical form
instances. (Recall that, for training, CONTEXT uses instances in canonical form as being
idiomatic and those in non-canonical form as being literal.) Thus, in many cases, it
is a priori more likely that a token is more similar to the K most similar canonical
form instances. Interestingly, CFORM is the method with the highest Spec and NPV,
even higher than those of the supervised method. Nonetheless, even CFORM is overall
much better at identifying idiomatic tokens than literal ones (see Section 7.2.3 for more
discussion on this).
We now turn to performance on DTIlow . CFORM has a high Sens, but a low PPV,
indicating that most idiomatic usages are identified correctly, but many literal usages
are also misclassified as idiomatic (hence a low Spec). CONTEXT shows the same trend
as CFORM, though overall it has poorer performance. Performance of SUP varies across
the expressions in this group: SUP is very good at identifying literal usages of these
expressions (high Spec and NPV for all expressions). Nonetheless, SUP has a low recall
in identifying idiomatic usages (low Sens) for many of these expressions.
7.2.3 Discussion and Error Analysis. In this section, we examine twomain issues. First, we
look into the plausibility of our original assumptions regarding the predictive value of
canonical forms (and non-canonical forms). Second, we investigate the appropriateness
of our automatically extracted canonical forms.
91
Computational Linguistics Volume 35, Number 1
To learn more about the predictive value of canonical forms, we examine the per-
formance of CFORM on the 28 expressions under study. More specifically, we look at
the values of Sens, PPV, Spec, and NPV on these expressions, as shown in Table 12
in the Appendix. On expressions in DTIhigh , CFORM has both high Sens and high PPV.
The formulas in Table 9 indicate that if both Sens and PPV are high, then tp fn and
tp fp. Thus, most idiomatic usages of expressions in DTIhigh appear in a canonical form,
and most usages in a canonical form are idiomatic. The values of Spec and NPV on the
same expressions are in general lower (compared to Sens and PPV), showing that tn is
not much higher than fp or fn.
On expressions in DTIlow , CFORM generally has high Sens but low-to-medium PPV.
This indicates that for these expressions, most idiomatic usages appear in a canonical
form, but not all usages in a canonical form are idiomatic. On these expressions, CFORM
has generally high NPV, but mostly low Spec. These indicate that tn fn, that is, most
usages in a non-canonical form are literal, and that tn is often lower than fp, that is, many
literal usages also appear in a canonical form. For example, almost all usages of hit wall
in a non-canonical form are literal, but most of its literal usages appear in a canonical
form.
Generally, it seems that, as we expected, literal usages are less restricted in terms
of the syntactic form they appear in; they can appear in both canonical form(s) and
in non-canonical form(s). For an expression with a low proportion of literal usages,
we can thus acquire canonical forms that are both accurate and have high predictive
value for identifying idiomatic usages in context. On the contrary, for expressions
with a relatively high proportion of literal usages, automatically acquired canonical
forms are less accurate and also have low predictive value (i.e., they are not specific
to idiomatic usages). We expected that using contextual information would help in
such cases. However, our CONTEXT method relies on noisy training data automatically
labelled using information about canonical forms. Given these findings, it is not sur-
prising that this method performs substantially worse than a corresponding supervised
method that uses similar contextual information, but manually labelled training data. It
remains to be tested in the future whether providing more noisy data will help. Another
possible future direction is to develop context methods that can better exploit noisy
labelled data.
Now we look at a few cases where our automatically extracted canonical forms are
not sufficiently accurate. For a verb+noun such as make pile (i.e., make a pile of money),
we correctly identify only some of the canonical forms. The automatically determined
canonical forms for make pile are make a pile and make piles. However, we find that idi-
omatic usages of this expression are sometimes of the formmake one?s pile. Furthermore,
we find that the frequency of this form is much higher than that of the non-canonical
forms, and not substantially lower than the frequency cut-off for selection as a canonical
form. This indicates that our heuristic for selecting patterns as canonical forms could be
fine-tuned to yield an improvement in performance.
For the expression pull plug, we identify its canonical form as pull the plug, but find a
mixture of literal and idiomatic usages in this form. However, many of the literal usages
are verb-particle constructions using out (pull the plug out), while many of the idiomatic
usages occur with a prepositional phrase headed by on (pull the plug on). This indi-
cates that incorporating information about particles and prepositions could improve
the quality of the canonical forms. Other syntactic categories, such as adjectives, may
also be informative in determining canonical forms for expressions which are typically
used idiomatically with words of a particular syntactic category, as in blow one?s own
trumpet.
92
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 11
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 23 expressions
in SKEWED-IDM and on the 37 expressions in the combination of TEST and SKEWED-IDM (ALL).
SKEWED-IDM ALL
Method %Acc (%ERR) %Acc (%ERR)
Baseline 97.9 84.3
Unsupervised CONTEXT 94.2 (?176.2) 83.3 (?6.4)
CFORM 86.7 (?533.3) 81.3 (?19.1)
Supervised SUP 97.9 (0.0) 92.1 (49.7)
7.2.4 Performance on Expressions with Skewed Distribution. Recall from Section 7.1.1 that,
from the original set of 60 candidate expressions, we excluded those that had fewer than
5 instances of either of their literal or idiomatic meanings. It is nonetheless important to
see how well our methods perform on such expressions. In this section, we thus report
the performance of our measures on the set of 23 expressions with mostly idiomatic
usages, referred to as SKEWED-IDM. Table 11 presents the macro-averaged accuracy of
our methods on these expressions. This table also shows the accuracy on all unseen test
expressions, that is, the combination of SKEWED-IDM and TEST, referred to as ALL, for
comparison.25
On SKEWED-IDM, the supervised method performs as well as the baseline, whereas
both unsupervised methods perform worse.26 Note that for 19 out of the 23 expressions
in SKEWED-IDM, all instances are idiomatic, and the baseline accuracy is thus 100%. On
these, SUP also has 100% accuracy because no literal instances are available, and thus
SUP labels every token as idiomatic (same as the baseline). As for the unsupervised
methods, we can see that, unlike on TEST, the CONTEXT method outperforms CFORM
(the difference is statistically significant at p < .001). We saw previously that CONTEXT
tends to label usages as idiomatic. This bias might be partially responsible for the
better performance of CONTEXT on this data set. Moreover, we find that many of these
expressions tend to appear in a highly frequent canonical form, but also in less frequent
syntactic forms which we (perhaps incorrectly) consider as non-canonical forms. When
considering the performance on all unseen test expressions (ALL), neither unsupervised
method performs as well as the baseline, but the supervised method offers a substantial
improvement over the baseline.27
Our annotators pointed out that for many of the expressions in SKEWED-IDM,
either a literal interpretation was almost impossible (as for catch one?s imagination),
or extremely implausible (as for kick the habit). Hence, the annotators could predict
beforehand that the expression would be mainly used with an idiomatic meaning. A
semi-supervised approach that combines expert human knowledge with automatically
extracted corpus-drawn information can thus be beneficial for the task of identifying
25 The results obtained on the two excluded expressions which are predominantly used literally in terms
of percent accuracy using the various methods are as follows. Baseline: 4.2, Unsupervised CONTEXT: 6.5,
Unsupervised CFORM: 16.2, Supervised: 43.5. However, because there are only two such expressions,
it is difficult to draw conclusions from these results, and we do not further consider these expressions.
26 According to a paired t-test, on SKEWED-IDM, all the observed differences are statistically significant at
p < .05.
27 According to a paired t-test, on ALL, the differences between the supervised method and the three other
methods are statistically significant at p < .01; none of the other differences are statistically significant.
93
Computational Linguistics Volume 35, Number 1
idiomatic expressions in context. A human expert (e.g., a lexicographer) could first
filter out expressions for which a literal interpretation is highly unlikely. For the rest
of the expressions, a simple unsupervised method such as CFORM?that relies only on
automatically extracted information?can be used with reasonable accuracy.
8. Related Work
8.1 Type-Based Recognition of Idioms and Other Multiword Expressions
Our work relates to previous studies on determining the compositionality (the inverse
of idiomaticity) of idioms and other multiword expressions (MWEs). Most previous
work on the compositionality of MWEs either treats them as collocations (Smadja 1993),
or examines the distributional similarity between the expression and its constituents
(Baldwin et al 2003; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Others have identified MWEs by looking into specific linguistic cues,
such as the lexical fixedness of non-compositional MWEs (Lin 1999; Wermter and Hahn
2005), or the lexical flexibility of productive noun compounds (Lapata and Lascarides
2003). Venkatapathy and Joshi (2005) combine aspects of this work, by incorporating
lexical fixedness, distributional similarity, and collocation-based measures into a
set of features which are used to rank verb+noun combinations according to their
compositionality. Our work differs from such studies in that it considers various kinds
of fixedness as surface behaviors that are tightly related to the underlying semantic
idiosyncrasy (idiomaticity) of expressions. Accordingly, we propose novel methods
for measuring the degree of lexical, syntactic, and overall fixedness of verb+noun
combinations, and use these as indirect ways of measuring degree of idiomaticity.
Earlier research on the lexical encoding of idiom types mainly relied on the exis-
tence of human annotations, especially for detecting which syntactic variations (e.g.,
passivization) an idiom can undergo (Odijk 2004; Villavicencio et al 2004). Evert, Heid,
and Spranger (2004) and Ritz and Heid (2006) propose methods for automatically
determining morphosyntactic preferences of idiomatic expressions. However, they treat
individual morphosyntactic markers (e.g., the number of the noun in a verb+noun
combination) as independent features, and rely mainly on the relative frequency of
each possible value for a feature (e.g., plural for number) as an indicator of a preference
for that value. If the relative frequency of a particular value of a feature for a given
combination (or the lower bound of the confidence interval, in the case of Evert, Heid,
and Spranger?s approach) is higher than a certain threshold, then the expression is
said to have a preference for that value. These studies recognize that morphosyntactic
preferences can be employed as clues to the identification of idiomatic combinations;
however, none proposes a systematic approach for such a task. Moreover, only subjec-
tive evaluations of the proposed methods are presented.
Others have also drawn on the notion of syntactic fixedness for the detection
of idioms and other MWEs. Widdows and Dorow (2005), for example, look into the
fixedness of a highly constrained type of idiom, namely, those of the form ?X conj X?
where X is a noun or an adjective, and conj is a conjunction such as and, or, but. Smadja
(1993) also notes the importance of syntactic fixedness in identifying strongly associated
multiword sequences, including collocations and idioms. Nonetheless, in both these
studies, the notion of syntactic fixedness is limited to the relative position of words
within the sequence. Such a general notion of fixedness does not take into account some
of the important syntactic properties of idioms (e.g., the choice of the determiner), and
hence cannot distinguish among different subtypes of MWEs which may differ on such
94
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
grounds. Our syntactic fixedness measure looks into a set of linguistically informed
patterns associated with a coherent, though large, class of idiomatic expressions. Results
presented in this article show that the fixedness measures can successfully separate
idioms from literal phrases. Corpus analysis of the measures proves that they can also
be used to distinguish idioms from other MWEs, such as light verb constructions and
collocations (Fazly and Stevenson 2007; Fazly and Stevenson, to appear). Bannard (2007)
proposes an extension of our syntactic fixedness measure?which first appeared in
Fazly and Stevenson (2006)?where he uses different prior distributions for different
syntactic variations.
Work on the identification of MWE types has also looked at evidence from another
language. For example, Melamed (1997a) assumes that non-compositional compounds
(NCCs) are usually not translated word-for-word to another language. He thus pro-
poses to discover NCCs by maximizing the information-theoretic predictive value of
a translation model between two languages. The sample extracted NCCs reveal an
important drawback of the proposed method: It relies on a translation model only,
without taking into account any prior linguistic knowledge about possible NCCswithin
a language. Nonetheless, such a technique is capable of identifying many NCCs that are
relevant for a translation task. Villada Moiro?n and Tiedemann (2006) propose measures
for distinguishing idiomatic expressions from literal ones (in Dutch), by examining
their automatically generated translations into a second language, such as English or
Spanish. Their approach is based on the assumptions that idiomatic expressions tend
to have fewer predictable translations and fewer compositional meanings, compared
to the literal ones. The first property is measured as the diversity in the translations
for the expression, estimated using an entropy-based measure proposed by Melamed
(1997b). The non-compositionality of an expression is measured as the overlap between
the meaning of an expression (i.e., its translations) and those of its component words.
General approaches (such as those explained in the previous paragraph) may be
more easily extended to different domains and languages. Our measures incorporate
language-specific information about idiomatic expressions, thus extra work may be
required to extend and apply them to other languages and other expressions. (Though
see Van de Cruys and Villada Moiro?n [2007] for an extension of our measures to Dutch
idioms of the form verb plus prepositional phrase.) Nonetheless, because our measures
capture deep linguistic information, they are also expected to acquire more detailed
knowledge?for example, they can be used for identifying other classes of MWEs (Fazly
and Stevenson 2007).
8.2 Token-Based Identification of Idioms and Other Multiword Expressions
A handful of studies have focused on identifying idiomatic and non-idiomatic usages
(tokens) of words or MWEs. Birke and Sarkar (2006) propose a minimally supervised
algorithm for distinguishing between literal and non-literal usages of verbs in context.
Their algorithm uses seed sets of literal and non-literal usages that are automatically
extracted from online resources such as WordNet. The similarity between the context of
a target token and that of each seed set determines the class of the token. The approach is
general in that it uses a slightly modified version of an existing word sense disambigua-
tion algorithm. This is both an advantage and a drawback: The algorithm can be easily
extended to other parts of speech and other languages; however, such a general method
ignores the specific properties of non-literal (metaphorical and/or idiomatic) language.
Similarly, the supervised token classification method of Katz and Giesbrecht (2006)
relies primarily on the local context of a token, and fails to exploit specific linguistic
95
Computational Linguistics Volume 35, Number 1
properties of non-literal language. Our results suggest that such properties are often
more informative than the local context, in determining the class of an MWE token.
The supervised classifier of Patrick and Fletcher (2005) distinguishes between com-
positional and non-compositional usages of English verb-particle constructions. Their
classifier incorporates linguistically motivated features, such as the degree of separation
between the verb and particle. Here, we focus on a different class of English MWEs,
namely, the class of idiomatic verb+noun combinations. Moreover, by making a more
direct use of their syntactic behavior, we develop unsupervised token classification
methods that perform well. The unsupervised token classifier of Hashimoto, Sato, and
Utsuro (2006) uses manually encoded information about allowable and non-allowable
syntactic transformations of Japanese idioms, which are roughly equivalent to our
notions of canonical and non-canonical forms. The rule-based classifier of Uchiyama,
Baldwin, and Ishizaki (2005) incorporates syntactic information about Japanese com-
pound verbs (JCVs), a type of MWE composed of two verbs. In both cases, although the
classifiers incorporate syntactic information about MWEs, their manual development
limits the scalability of the approaches.
Uchiyama, Baldwin, and Ishizaki (2005) also propose a statistical token classifica-
tion method for JCVs. This method is similar to ours, in that it also uses type-based
knowledge to determine the class of each token in context. However, their method is
supervised, whereas our methods are unsupervised. Moreover, Uchiyama, Baldwin,
and Ishizaki only evaluate their methods on a set of JCVs that are mostly monosemous.
Our main focus here is on MWEs that are harder to disambiguate, that is, those that
have two clear idiomatic and literal meanings, and that are frequently used with either
meaning.
9. Conclusions
The significance of the role idioms play in language has long been recognized; however,
due to their peculiar behavior, they have been mostly overlooked by researchers in
computational linguistics. In this work, we focus on a broadly documented and cross-
linguistically frequent class of idiomatic MWEs: those that involve the combination
of a verb and a noun in its direct object position, which we refer to as verb+noun
idiomatic combinations or VNICs. Although a great deal of research has focused on
non-compositionality of MWEs, less attention has been paid to other properties relevant
to their semantic idiosyncrasy, such as lexical and syntactic fixedness. Drawing on such
properties, we have developed techniques for the automatic recognition of VNIC types,
as well as methods for their token identification in context.
We propose techniques for the automatic acquisition and encoding of knowledge
about the lexicosyntactic behavior of idiomatic combinations. More specifically, we
propose novel statistical measures that quantify the degree of lexical, syntactic, and
overall fixedness of a verb+noun combination. We demonstrate that these measures
can be successfully applied to the task of automatically distinguishing idiomatic ex-
pressions (types) from non-idiomatic ones. Our results show that the syntactic and
overall fixedness measures substantially outperform existing measures of collocation
extraction, even when they incorporate some syntactic information. We put forward
an unsupervised means for automatically discovering the set of syntactic variations
that are preferred by a VNIC type (its canonical forms) and that should be included
in its lexical representation. In addition, we show that the canonical form extraction
method can effectively be used in identifying idiomatic and literal usages (tokens) of an
expression in context.
96
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
We have annotated a total of 2, 465 tokens for 51 VNIC types according to whether
they are a literal or idiomatic usage. We found that for 28 expressions (1, 180 tokens),
approximately 40% of the usages were literal. For the remaining 23 expressions (1, 285
tokens), almost all usages were idiomatic. These figures indicate that automatically
determining whether a particular instance of an expression is used idiomatically or lit-
erally is of great importance for NLP applications. We have proposed two unsupervised
methods that perform such a task.
Our proposed methods incorporate automatically acquired knowledge about the
overall syntactic behavior of a VNIC type, in order to do token classification. More
specifically, our methods draw on the syntactic fixedness of VNICs?a property which
has been largely ignored in previous studies of MWE tokens. Our results confirm the
usefulness of this property as incorporated into our methods. On the 23 expressions
whose usages are predominantly idiomatic, because the baseline is very high none
of the methods outperform it. Nonetheless, as pointed out by our human annotators,
for many of these expressions it can be predicted beforehand that they are mainly
idiomatic and that a literal interpretation is impossible or highly implausible. On the
28 expressions with frequent literal usages, all our methods outperform the baseline of
always predicting themost dominant class (idiomatic). Moreover, on these, the accuracy
of our best unsupervised method is not substantially lower than the accuracy of a
standard supervised approach.
Appendix: Performance on the Individual Expressions
This Appendix contains the values of the four performance measures, Sens, PPV, Spec,
and NPV, for our two unsupervised methods (i.e., CFORM and CONTEXT) as well as for
the supervised method, SUP, on individual expressions in DTIhigh and DTIlow . Expressions
(verb?noun pairs) in each data set are ordered alphabetically.
Table 12
Performance of CFORM on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.92 0.60 1.00
blow trumpet 0.89 0.89 0.80 0.80
cut figure 0.97 0.97 0.86 0.86
find foot 0.98 0.92 0.20 0.50
get nod 0.96 1.00 1.00 0.75
get sack 1.00 0.96 0.71 1.00
have word 0.56 0.96 0.78 0.17
hit road 1.00 0.80 0.14 1.00
DTIhigh hit roof 1.00 0.65 0.00 0.00
kick heel 1.00 0.81 0.12 1.00
lose thread 0.94 0.94 0.50 0.50
make face 0.74 0.95 0.67 0.22
make mark 0.85 1.00 1.00 0.54
pull plug 0.89 0.77 0.40 0.62
pull punch 0.83 0.94 0.75 0.50
pull weight 1.00 0.93 0.67 1.00
take heart 1.00 0.97 0.88 1.00
97
Computational Linguistics Volume 35, Number 1
Table 12
(continued)
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow whistle 0.93 0.44 0.37 0.90
get wind 0.85 0.73 0.75 0.86
hit wall 0.86 0.11 0.09 0.83
hold fire 1.00 0.37 0.25 1.00
lose head 0.76 0.62 0.41 0.58
DTIlow make hay 1.00 0.56 0.12 1.00
make hit 1.00 0.71 0.78 1.00
make pile 0.25 0.14 0.29 0.45
make scene 0.82 0.68 0.45 0.64
pull leg 0.64 0.23 0.40 0.80
see star 0.80 0.10 0.38 0.95
Table 13
Performance of CONTEXT on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.85 0.20 1.00
blow trumpet 0.89 0.74 0.40 0.67
cut figure 1.00 0.84 0.00 0.00
find foot 1.00 0.90 0.00 0.00
get nod 1.00 0.88 0.00 0.00
get sack 1.00 0.86 0.00 0.00
have word 0.70 0.95 0.67 0.20
hit road 1.00 0.77 0.00 0.00
DTIhigh hit roof 1.00 0.65 0.00 0.00
kick heel 0.97 0.78 0.00 0.00
lose thread 1.00 0.90 0.00 0.00
make face 0.85 0.88 0.00 0.00
make mark 1.00 0.91 0.46 1.00
pull plug 0.96 0.69 0.05 0.33
pull punch 0.94 0.89 0.50 0.67
pull weight 1.00 0.82 0.00 0.00
take heart 0.90 0.85 0.38 0.50
blow whistle 0.89 0.36 0.18 0.75
get wind 0.85 0.65 0.62 0.83
hit wall 1.00 0.11 0.00 0.00
hold fire 1.00 0.30 0.00 0.00
lose head 0.90 0.56 0.12 0.50
DTIlow make hay 0.78 0.50 0.12 0.33
make hit 0.60 0.38 0.44 0.67
make pile 0.50 0.25 0.29 0.56
make scene 0.96 0.66 0.30 0.86
pull leg 0.82 0.22 0.20 0.80
see star 1.00 0.12 0.32 1.00
98
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 14
Performance of SUP on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.85 0.20 1.00
blow trumpet 0.95 0.72 0.30 0.75
cut figure 1.00 0.84 0.00 0.00
find foot 1.00 0.90 0.00 0.00
get nod 0.91 0.91 0.33 0.33
get sack 1.00 0.86 0.00 0.00
have word 1.00 0.90 0.00 0.00
hit road 1.00 0.80 0.14 1.00
DTIhigh hit roof 0.82 0.64 0.17 0.33
kick heel 0.97 0.78 0.00 0.00
lose thread 1.00 0.95 0.50 1.00
make face 1.00 0.96 0.67 1.00
make mark 1.00 0.91 0.46 1.00
pull plug 0.98 0.90 0.75 0.94
pull punch 1.00 0.90 0.50 1.00
pull weight 1.00 0.82 0.00 0.00
take heart 0.93 0.83 0.25 0.50
blow whistle 0.52 0.78 0.92 0.78
get wind 0.77 0.71 0.75 0.80
hit wall 0.00 0.00 1.00 0.89
hold fire 0.00 0.00 0.88 0.67
lose head 0.48 0.62 0.65 0.50
DTIlow make hay 0.89 0.80 0.75 0.86
make hit 0.40 1.00 1.00 0.75
make pile 0.38 0.75 0.94 0.76
make scene 0.89 0.69 0.45 0.75
pull leg 0.55 0.75 0.95 0.88
see star 0.00 0.00 1.00 0.92
99
Computational Linguistics Volume 35, Number 1
Acknowledgments
This article is an extended and updated
combination of two papers that appeared,
respectively, in the proceedings of EACL
2006 and the proceedings of the ACL 2007
Workshop on A Broader Perspective on
Multiword Expressions. We wish to thank
the anonymous reviewers of those papers
for their helpful recommendations. We also
thank the anonymous reviewers of this
article for their insightful comments which
we believe have helped us improve the
quality of the work. We are grateful to Eric
Joanis for providing us with the NP-head
extraction software, and to Afra Alishahi
and Vivian Tsang for proofreading the
manuscript. Our work is financially
supported by the Natural Sciences and
Engineering Research Council of Canada,
the Ontario Graduate Scholarship program,
and the University of Toronto.
References
Abeille?, Anne. 1995. The flexibility of French
idioms: A representation with lexicalized
Tree Adjoining Grammar. In Everaert
et al, editors, Idioms: Structural and
Psychological Perspectives. LEA, Mahwah,
NJ, pages 15?42.
Akimoto, Minoji. 1999. Collocations and
idioms in Late Modern English. In L. J.
Brinton and M. Akimoto. Collocational and
Idiomatic Aspects of Composite Predicates in
the History of English. John Benjamins
Publishing Company, Amsterdam,
pages 207?238.
Baldwin, Timothy, Colin Bannard, Takaaki
Tanaka, and Dominic Widdows. 2003. An
empirical model of multiword expression
decomposability. In Proceedings of the
ACL-SIGLEX Workshop on Multiword
Expressions: Analysis, Acquisition and
Treatment, pages 89?96, Sapporo.
Bannard, Colin. 2007. A measure of syntactic
flexibility for automatically identifying
multiword expressions in corpora. In
Proceedings of the ACL?07 Workshop on a
Broader Perspective on Multiword
Expressions, pages 1?8, Prague.
Bannard, Colin, Timothy Baldwin, and
Alex Lascarides. 2003. A statistical
approach to the semantics of
verb-particles. In Proceedings of the
ACL-SIGLEX Workshop on Multiword
Expressions: Analysis, Acquisition and
Treatment, pages 65?72, Sapporo.
Birke, Julia and Anoop Sarkar. 2006. A
clustering approach for the nearly
unsupervised recognition of nonliteral
language. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL?06), pages 329?336, Trento.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition),
second edition. Available at www.natcorp.
ox.ac.uk.
Cacciari, Cristina. 1993. The place of idioms
in a literal and metaphorical world. In C.
Cacciari and P. Tabossi, Idioms: Processing,
Structure, and Interpretation. LEA, Mahwah,
NJ, pages 27?53.
Church, Kenneth, William Gale, Patrick
Hanks, and Donald Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik,
editor, Lexical Acquisition: Exploiting
On-Line Resources to Build a Lexicon. LEA,
Mahwah, NJ, pages 115?164.
Claridge, Claudia. 2000.Multi-word Verbs in
Early Modern English: A Corpus-based Study.
Editions Rodopi B. V., Amsterdam.
Clark, Eve V. 1978. Discovering what words
can do. Papers from the Parasession on the
Lexicon, 14:34?57.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Cook, Paul, Afsaneh Fazly, and Suzanne
Stevenson. 2007. Pulling their weight:
Exploiting syntactic forms for the
automatic identification of idiomatic
expressions in context. In Proceedings of the
ACL?07 Workshop on a Broader Perspective on
Multiword Expressions, pages 41?48,
Prague.
Copestake, Ann, Fabre Lambeau, Aline
Villavicencio, Francis Bond, Timothy
Baldwin, Ivan A. Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic
precision and reusability. In Proceedings of
the 4th International Conference on Language
Resources and Evaluation (LREC?02),
pages 1941?47, Las Palmas.
Cover, Thomas M. and Joy A. Thomas. 1991.
Elements of Information Theory. John Wiley
and Sons, Inc., New York.
Cowie, Anthony P., Ronald Mackin, and
Isabel R. McCaig. 1983. Oxford Dictionary of
Current Idiomatic English, volume 2. Oxford
University Press.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word co-occurrence probabilities. In
Proceedings of the 32nd Anuual Meeting of the
100
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Association for Computational Linguistics
(ACL?94), pages 272?278, Las Cruces, NM.
d?Arcais, Giovanni B. Flores. 1993. The
comprehension and semantic
interpretation of idioms. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 79?98.
Desbiens, Marguerite Champagne and Mara
Simon. 2003. De?terminants et locutions
verbales. Manuscript. Available at
www.er.uqam.ca/nobel/scilang/cesla02/
mara margue.pdf.
Evert, Stefan, Ulrich Heid, and Kristina
Spranger. 2004. Identifying
morphosyntactic preferences in
collocations. In Proceedings of the 4th
International Conference on Language
Resources and Evaluation (LREC?04),
pages 907?910, Lisbon.
Evert, Stefan and Brigitte Krenn. 2001.
Methods for the qualitative evaluation of
lexical association measures. In Proceedings
of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL?01),
pages 188?195, Toulouse.
Fazly, Afsaneh and Suzanne Stevenson. 2006.
Automatically constructing a lexicon of
verb phrase idiomatic combinations. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?06),
pages 337?344, Trento.
Fazly, Afsaneh and Suzanne Stevenson. 2007.
Distinguishing subtypes of multiword
expressions using linguistically-motivated
statistical measures. In Proceedings of the
ACL?07 Workshop on a Broader Perspective
on Multiword Expressions, pages 9?16,
Prague.
Fazly, Afsaneh and Suzanne Stevenson. A
distributional account of the semantics of
multiword expressions. To appear in the
Italian Journal of Linguistics.
Fellbaum, Christiane. 1993. The determiner
in English idioms. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure,
and Interpretation. LEA, Mahwah, NJ,
pages 271?295.
Fellbaum, Christiane, editor. 1998.WordNet,
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fellbaum, Christiane. 2002. VP idioms in the
lexicon: Topics for research using a very
large corpus. In Proceedings of the
KONVENS 2002 Conference, pages 7?11,
Saarbruecken, Germany.
Fellbaum, Christiane. 2007. The ontological
loneliness of idioms. In Andrea Schalley
and Dietmar Zaefferer, editors,
Ontolinguistics. Mouton de Gruyter, Berlin,
pages 419?434.
Firth, John R. 1957. A synopsis of linguistic
theory 1930?1955. In Studies in Linguistic
Analysis (special volume of the Philological
Society). The Philological Society, Oxford,
pages 1?32.
Fraser, Bruce. 1970. Idioms within a
transformational grammar. Foundations of
Language, 6:22?42.
Gentner, Dedre and Ilene M. France. 2004.
The verb mutability effect: Studies of the
combinatorial semantics of nouns and
verbs. In Steven L. Small, Garrison W.
Cottrell, and Michael K. Tanenhaus,
editors, Lexical Ambiguity Resolution:
Perspectives from Psycholinguistics,
Neuropsychology, and Artificial Intelligence.
Kaufmann, San Mateo, CA, pages 343?382.
Gibbs, Raymond W. Jr. 1993. Why idioms are
not dead metaphors. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 57?77.
Gibbs, Raymond W. Jr. 1995. Idiomaticity
and human cognition. In Everaert et al,
editors, Idioms: Structural and Psychological
Perspectives. LEA, Mahwah, NJ,
pages 97?116.
Gibbs, Raymond W. Jr. and Nandini P.
Nayak. 1989. Psychololinguistic studies on
the syntactic behavior of idioms. Cognitive
Psychology, 21:100?138.
Gibbs, Raymond W. Jr., Nandini P. Nayak,
J. Bolton, and M. Keppel. 1989. Speaker?s
assumptions about the lexical flexibility
of idioms.Memory and Cognition,
17:58?68.
Glucksberg, Sam. 1993. Idiom meanings and
allusional content. In C. Cacciari and P.
Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 3?26.
Goldberg, Adele E. 1995. Constructions: A
Construction Grammar Approach to
Argument Structure. The University of
Chicago Press.
Grant, Lynn E. 2005. Frequency of ?core
idioms? in the British National Corpus
(BNC). International Journal of Corpus
Linguistics, 10(4):429?451.
Hashimoto, Chikara, Satoshi Sato, and
Takehito Utsuro. 2006. Japanese idiom
recognition: Drawing a line between
literal and idiomatic meanings. In
Proceedings of the 17th International
Conference on Computational Linguistics
and the 36th Annual Meeting of the
101
Computational Linguistics Volume 35, Number 1
Association for Computational Linguistics
(COLING-ACL?06), pages 353?360, Sydney.
Inkpen, Diana. 2003. Building a Lexical
Knowledge-Base of Near-Synonym Differences.
Ph.D. thesis, University of Toronto.
Jackendoff, Ray. 1997. The Architecture of the
Language Faculty. MIT Press, Cambridge,
MA.
Katz, Graham and Eugenie Giesbrecht. 2006.
Automatic identification of
non-compositional multi-word
expressions using Latent Semantic
Analysis. In Proceedings of the ACL?06
Workshop on Multiword Expressions:
Identifying and Exploiting Underlying
Properties, pages 12?19, Sydney.
Katz, Jerrold J. 1973. Compositionality,
idiomaticity, and lexical substitution. In
S. Anderson and P. Kiparsky, editors, A
Festschrift for Morris Halle. Holt, Rinehart
and Winston, New York, pages 357?376.
Kearns, Kate. 2002. Light verbs in English.
Manuscript. Available at www.ling.
canterbury.ac.nz/people/kearns.html.
Kirkpatrick, E. M. and C. M. Schwarz,
editors. 1982. Chambers Idioms. W & R
Chambers Ltd, Edinburgh.
Krenn, Brigitte and Stefan Evert. 2001. Can
we do better than frequency? A case study
on extracting PP-verb collocations. In
Proceedings of the ACL?01 Workshop on
Collocations, pages 39?46, Toulouse.
Kyto?, Merja. 1999. Collocational and
idiomatic aspects of verbs in Early Modern
English. In L. J. Brinton and M. Akimoto.
Collocational and Idiomatic Aspects of
Composite Predicates in the History of
English. John Benjamins Publishing
Company, Amsterdam, pages 167?206.
Lapata, Mirella and Alex Lascarides. 2003.
Detecting novel compounds: The role of
distributional evidence. In Proceedings of
the 11th Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 235?242, Budapest.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics and the 36th
Annual Meeting of the Association for
Computational Linguistics
(COLING-ACL?98), pages 768?774,
Montreal.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL?99), pages 317?324, College Park,
Maryland.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum
of compositionality in phrasal verbs.
In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment,
pages 73?80, Sapporo.
Melamed, I. Dan. 1997a. Automatic
discovery of non-compositional
compounds in parallel data. In Proceedings
of the 2nd Conference on Empirical Methods in
Natural Language Processing (EMNLP?97),
pages 97?108, Providence, RI.
Melamed, I. Dan. 1997b. Measuring semantic
entropy. In Proceedings of the ACL-SIGLEX
Workshop on Tagging Text with Lexical
Semantics: Why, What and How,
pages 41?46, Washington, DC.
Mohammad, Saif and Graeme Hirst.
Distributional measures as proxies for
semantic relatedness. Submitted.
Moon, Rosamund. 1998. Fixed Expressions and
Idioms in English: A Corpus-Based Approach.
Oxford University Press.
Newman, John and Sally Rice. 2004. Patterns
of usage for English SIT, STAND, and LIE:
A cognitively inspired exploration in
corpus linguistics. Cognitive Linguistics,
15(3):351?396.
Nicolas, Tim. 1995. Semantics of idiom
modification. In Everaert et al, editors,
Idioms: Structural and Psychological
Perspectives. LEA, Mahwah, NJ,
pages 233?252.
Nunberg, Geoffrey, Ivan A. Sag, and Thomas
Wasow. 1994. Idioms. Language,
70(3):491?538.
Odijk, Jan. 2004. A proposed standard for the
lexical representations of idioms. In
Proceedings of Euralex?04, pages 153?164,
Lorient.
Ogden, Charles Kay. 1968. Basic English,
International Second Language. Harcourt,
Brace, and World, New York.
Patrick, Jon and Jeremy Fletcher. 2005.
Classifying verb-particle constructions
by verb arguments. In Proceedings of
the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 200?209,
Colcheter.
Pauwels, Paul. 2000. Put, Set, Lay and Place: A
Cognitive Linguistic Approach to Verbal
Meaning. LINCOM EUROPA, Munich.
102
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
R 2004. Notes on R: A Programming
Environment for Data Analysis and Graphics.
Available at www.r-project.org.
Resnik, Philip. 1999. Semantic similarity in a
taxonomy: An information-based measure
and its application to problems of
ambiguity in natural language. Journal of
Artificial Intelligence Research (JAIR),
(11):95?130.
Riehemann, Susanne. 2001. A Constructional
Approach to Idioms and Word Formation.
Ph.D. thesis, Stanford University.
Ritz, Julia and Ulrich Heid. 2006. Extraction
tools for collocations and their
morphosyntactic specificities. In
Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC?06), pages 1925?30,
Genoa.
Rohde, Douglas L. T. 2004. TGrep2 User
Manual. Available at http://tedlab.mit.
edu/?dr/Tgrep2.
Sag, Ivan A., Timothy Baldwin, Francis
Bond, Ann Copestake, and Dan Flickinger.
2002. Multiword expressions: A pain in the
neck for NLP. In Proceedings of the 3rd
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing?02), pages 1?15, Mexico City.
Schenk, Andre?. 1995. The syntactic behavior
of idioms. In Everaert et al, editors, Idioms:
Structural and Psychological Perspectives.
LEA, Mahwah, NJ, chapter 10,
pages 253?271.
Seaton, Maggie and Alison Macaulay,
editors. 2002. Collins COBUILD Idioms
Dictionary. HarperCollins Publishers,
second edition, New York.
Smadja, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143?177.
Tanabe, Harumi. 1999. Composite predicates
and phrasal verbs in The Paston Letters. In
L. J. Brinton and M. Akimoto. Collocational
and Idiomatic Aspects of Composite Predicates
in the History of English. John Benjamins
Publishing Company, Amsterdam,
pages 97?132.
Uchiyama, Kiyoko, Timothy Baldwin, and
Shun Ishizaki. 2005. Disambiguating
Japanese compound verbs. Computer
Speech and Language, 19:497?512.
Van de Cruys, Tim and Begon?a
Villada Moiro?n. 2007. Semantics-based
multiword expression extraction. In
Proceedings of the ACL?07 Workshop on a
Broader Perspective on Multiword
Expressions, pages 25?32, Prague.
Venkatapathy, Sriram and Aravid Joshi. 2005.
Measuring the relative compositionality of
verb-noun (V-N) collocations by
integrating features. In Proceedings of Joint
Conference on Human Language Technology
and Empirical Methods in Natural Language
Processing (HLT-EMNLP?05),
pages 899?906, Vancouver.
Villada Moiro?n, Begon?a and Jo?rg Tiedemann.
2006. Identifying idiomatic expressions
using automatic word-alignment. In
Proceedings of the EACL?06 Workshop on
Multiword Expressions in a Multilingual
Context, pages 33?40, Trento.
Villavicencio, Aline, Ann Copestake,
Benjamin Waldron, and Fabre Lambeau.
2004. Lexical encoding of multiword
expressions. In Proceedings of the 2nd ACL
Workshop on Multiword Expressions:
Integrating Processing, pages 80?87,
Barcelona.
Wermter, Joachim and Udo Hahn. 2005.
Paradigmatic modifiability statistics for
the extraction of complex multi-word
terms. In Proceedings of Joint Conference on
Human Language Technology and Empirical
Methods in Natural Language Processing
(HLT-EMNLP?05), pages 843?850,
Vancouver.
Widdows, Dominic and Beate Dorow. 2005.
Automatic extraction of idioms using
graph analysis and asymmetric
lexicosyntactic patterns. In Proceedings of
ACL?05 Workshop on Deep Lexical
Acquisition, pages 48?56, Ann Arbor, MI.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1(6):80?83.
103

Statistical Measures of the Semi-Productivity of Light Verb Constructions
Suzanne Stevenson and Afsaneh Fazly and Ryan North
Department of Computer Science
University of Toronto
Toronto, Ontario M5S 3G4
Canada
 
suzanne,afsaneh,ryan  @cs.toronto.edu
Abstract
We propose a statistical measure for the degree of
acceptability of light verb constructions, such as
take a walk, based on their linguistic properties. Our
measure shows good correlations with human rat-
ings on unseen test data. Moreover, we find that our
measure correlates more strongly when the poten-
tial complements of the construction (such as walk,
stroll, or run) are separated into semantically similar
classes. Our analysis demonstrates the systematic
nature of the semi-productivity of these construc-
tions.
1 Light Verb Constructions
Much research on multiword expressions involv-
ing verbs has focused on verb-particle constructions
(VPCs), such as scale up or put down (e.g., Bannard
et al, 2003; McCarthy et al, 2003; Villavicencio,
2003). Another kind of verb-based multiword ex-
pression is light verb constructions (LVCs), such as
the examples in (1).
(1) a. Sara took a stroll along the beach.
b. Paul gave a knock on the door.
c. Jamie made a pass to her teammate.
These constructions, like VPCs, may extend the
meaning of the component words in interesting
ways, may be (semi-)productive, and may or may
not be compositional. Interestingly, despite these
shared properties, LVCs are in some sense the oppo-
site of VPCs. Where VPCs involve a wide range of
verbs in combination with a small number of parti-
cles, LVCs involve a small number of verbs in com-
bination with a wide range of co-verbal elements.
An LVC occurs when a light verb, such as take,
give, or make in (1), is used in conjunction with
a complement to form a multiword expression. A
verb used as a light verb can be viewed as drawing
on a subset of its more general semantic features
(Butt, 2003). This entails that most of the distinc-
tive meaning of a (non-idiomatic) LVC comes from
the complement to the light verb. This property can
be seen clearly in the paraphrases of (1) given below
in (2): in each, the complement of the light verb in
(1a?c) contributes the main verb of the correspond-
ing paraphrase.1
(2) a. Sara strolled along the beach.
b. Paul knocked on the door.
c. Jamie passed to her teammate.
The linguistic importance and crosslinguistic fre-
quency of LVCs is well attested (e.g., Butt, 2003;
Folli et al, 2003). Furthermore, LVCs have partic-
ular properties that require special attention within
a computational system. For example, many LVCs
(such as those in (1) above) exhibit composi-
tional and semi-productive patterns, while others
(such as take charge) may be more fixed. Thus,
LVCs present the well-known problem with multi-
word expressions of determining whether and how
they should be listed in a computational lexicon.
Moreover, LVCs are divided into different classes
of constructions, which have distinctive syntactic
and semantic properties (Wierzbicka, 1982; Kearns,
2002). In general, there is no one ?light verb con-
struction? that can be dealt with uniformly in a com-
putational system, as is suggested by Sag et al
(2002), and generally assumed by earlier compu-
tational work on these constructions (Fontenelle,
1993; Grefenstette and Teufel, 1995; Dras and John-
son, 1996). Rather there are different types of
LVCs, each with unique properties.
In our initial computational investigation of light
verb phenomena, we have chosen to focus on a par-
ticular class of semi-productive LVCs in English,
exemplified by such expressions as take a stroll,
take a run, take a walk, etc. Specifically, we in-
vestigate the degree to which we can determine, on
the basis of corpus statistics, which words form a
valid complement to a given light verb in this type
of construction.
1The two expressions differ in aspectual properties. It has
been argued that the usage of a light verb adds a telic compo-
nent to the event in most cases (Wierzbicka, 1982; Butt, 2003);
though see Folli et al (2003) for telicity in Persian LVCs.
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 1-8
Our approach draws on a linguistic analysis, pre-
sented in Section 2, in which the complement of
this type of LVC (e.g., a walk in take a walk) is?in
spite of the presence of the determiner a?actually
a verbal element (Wierzbicka, 1982; Kearns, 2002).
Section 3 describes how this analysis motivates both
a method for generalizing over verb classes to find
potential valid complements for a light verb, and a
mutual information measure that takes the linguis-
tic properties of this type of LVC into account. In
Section 4, we outline how we collect the corpus
statistics on which we base our measures intended
to distinguish ?good? LVCs from poor ones. Sec-
tion 5 describes the experiments in which we deter-
mine human ratings of potential LVCs, and correlate
those with our mutual information measures. As
predicted, the correlations reveal interesting class-
based behaviour among the LVCs. Section 6 ana-
lyzes the relation of our approach to the earlier com-
putational work on LVCs cited above. Our investi-
gation is preliminary, and Section 7 discusses our
current and future research on LVCs.
2 Linguistic Properties of LVCs
An LVC is a multiword expression that combines
a light verb with a complement of type noun, ad-
jective, preposition or verb, as in, respectively, give
a speech, make good (on), take (NP) into account,
or take a walk. The light verb itself is drawn from
a limited set of semantically general verbs; among
the commonly used light verbs in English are take,
give, make, have, and do. LVCs are highly pro-
ductive in some languages, such as Persian, Urdu,
and Japanese (Karimi, 1997; Butt, 2003; Miyamoto,
2000). In languages such as French, Italian, Spanish
and English, LVCs are semi-productive construc-
tions (Wierzbicka, 1982; Alba-Salas, 2002; Kearns,
2002).
The syntactic and semantic properties of the com-
plement of an LVC determine distinct types of con-
structions. Kearns (2002) distinguishes between
two usages of light verbs in LVCs: what she calls
a true light verb (TLV), as in give a groan, and
what she calls a vague action verb (VAV), as in
give a speech. The main difference between these
two types of light verb usages is that the comple-
ment of a TLV is claimed to be headed by a verb.
Wierzbicka (1982) argues that although the com-
plement in such constructions might appear to be
a zero-derived nominal, its syntactic category when
used in an LVC is actually a verb, as indicated by
the properties of such TLV constructions. For exam-
ple, Kearns (2002) shows that, in contrast to VAVs,
the complement of a TLV usually cannot be definite
(3), nor can it be the surface subject of a passive
construction (4) or a fronted wh-element (5).
(3) a. Jan gave the speech just now.
b. * Jan gave the groan just now.
(4) a. A speech was given by Jan.
b. * A groan was given by Jan.
(5) a. Which speech did Jan give?
b. * Which groan did Jan give?
Because of their interesting and distinctive prop-
erties, we have restricted our initial investigation to
light verb constructions with TLVs, i.e. ?LV a V?
constructions, as in give a groan. For simplicity,
we will continue to refer to them here generally as
LVCs. The meaning of an LVC of this type is almost
equivalent to the meaning of the verbal complement
(cf. (1) and (2) in Section 1). However, the light
verb does contribute to the meaning of the construc-
tion, as can be seen by the fact that there are con-
straints on which light verb can occur with which
complement (Wierzbicka, 1982). For example, one
can give a cry but not *take a cry. The acceptability
depends on semantic properties of the complement,
and, as we explore below, may generalize in consis-
tent ways across semantically similar (complement)
verbs, as in give a cry, give a moan, give a howl;
*take a cry, *take a moan, *take a howl.
Many interesting questions pertaining to the syn-
tactic and semantic properties of LVCs have been
examined in the linguistic literature: How does the
semantics of an LVC relate to the semantics of its
parts? How does the type of the complement affect
the meaning of an LVC? Why do certain light verbs
select for certain complements? What underlies the
(semi-)productivity of the creation of LVCs?
Given the crosslinguistic frequency of LVCs,
work on computational lexicons will depend heav-
ily on the answers to these questions. We also be-
lieve that computational investigation can help to
precisely answer the questions as well, by using sta-
tistical corpus-based analysis to explore the range
and properties of these constructions. While details
of the underlying semantic representation of LVCs
are beyond the scope of this paper, we address the
questions of their semi-productivity.
3 Our Proposal
The initial goal in our investigation of semi-
productivity is to find a means for determining how
well particular light verbs and complements go to-
gether. We focus on the ?LV a V? constructions be-
cause we are interested in the hypothesis that the
complement to the LV is a verb, and think that the
properties of this construction may place interesting
restrictions on what forms a valid LVC.
3.1 Generalizing over Verb Classes
As noted above, there are constraints in an ?LV a
V? construction on which complements can occur
with particular light verbs. Moreover, similar po-
tential complements pattern alike in this regard?
that is, semantically similar complements may have
the same pattern of co-occurrence across different
light verbs. Since the complement is hypothesized
to be a verbal element, we look to verb classes to
capture the relevant semantic similarity. The lexical
semantic classes of Levin (1993) have been used as
a standard verb classification within the computa-
tional linguistics community. We thus propose us-
ing these classes as the semantically similar groups
over which to compare acceptability of potential
complements with a given light verb.2
Our approach is related to the idea of substi-
tutability in multiword expressions. Substituting
pieces of a multiword expression with semantically
similar words from a thesaurus can be used to deter-
mine productivity?higher degree of substitutabil-
ity indicating higher productivity (Lin, 1999; Mc-
Carthy et al, 2003).3 Instead of using a thesaurus-
based measure, Villavicencio (2003) uses substi-
tutability over semantic verb classes to determine
potential verb-particle combinations.
Our method is somewhat different from these ear-
lier approaches, not only in focusing on LVCs, but
in the precise goal. While Villavicencio (2003) uses
verb classes to generalize over verbs and then con-
firms whether an expression is attested, we seek to
determine how good an expression is. Specifically,
we aim to develop a computational approach not
only for characterizing the set of complements that
can occur with a given light verb in these LVCs, but
also to quantify the acceptability.
In investigating light verbs and their combina-
tion with complements from various verb semantic
classes, we expect that these LVCs are not fully id-
iosyncratic, but exhibit systematic behaviour. Most
importantly, we hypothesize that they show class-
based behaviour?i.e., that the same light verb will
show distinct patterns of acceptability with comple-
ments across different verb classes. We also ex-
2We also need to compare generalizability over semantic
noun classes to further test the linguistic hypothesis. We ini-
tially performed such experiments on noun classes in Word-
Net, but, due to the difficulty of deciding an appropriate level
of generalization in the hierarchy, we left this as future work.
3Note that although Lin characterizes his work as detecting
non-compositionality, we agree with Bannard et al (2003) that
it is better thought of as tapping into productivity.
plore whether the light verbs themselves show dif-
ferent patterns in terms of how they are used semi-
productively in these constructions.
We choose to focus on the light verbs take, give,
and make. We choose take and give because they
seem similar in their ability to occur in a range of
LVCs, and yet they have almost the opposite se-
mantics. We hope that the latter will reveal inter-
esting patterns in occurrence with the different verb
classes. On the other hand, make seems very dif-
ferent from both take and give. It seems much less
restrictive in its combinations, and also seems diffi-
cult to distinguish in terms of light versus ?heavy?
uses. We expect it to show different generalization
behaviour from the other two light verbs.
3.2 Devising an Acceptability Measure
Given the experimental focus, we must devise a
method for determining acceptability of LVCs. One
possibility is to use a standard measure for detect-
ing collocations, such as pointwise mutual informa-
tion (Church et al, 1991). ?LV a V? constructions
are well-suited to collocational analysis, as the light
verb can be seen as the first component of a colloca-
tion, and the string ?a V? as the second component.
Applying this idea to potential LVCs, we calculate
pointwise mutual information, I(lv; aV).
In addition, we use the linguistic properties of
the ?LV a V? construction to develop a more in-
formed measure. As noted in Section 2, generally
only the indefinite determiner a (or an) is allowed
in this type of LVC. We hypothesize then that for a
?good? LVC, we should find a much higher mutual
information value for ?LV a V? than for ?LV [det]
V?, where [det] is any determiner other than the in-
definite. While I(lv; aV) should tell us whether ?LV
a V? is a good collocation (Church et al, 1991), the
difference between the two, I(lv; aV) - I(lv; detV),
should tell us whether the collocation is an LVC.
To summarize, we assume that:
  if I(lv; aV)  0 then
?LV a V? is likely not a good collocation;
  if I(lv; aV) - I(lv; detV)  0 then
?LV a V? is likely not a true LVC.
In order to capture these two conditions in a sin-
gle measure, we combine them by using a linear ap-
proximation to the two lines given by I(lv; aV)  0
and I(lv; aV) - I(lv; detV)  0. The most straight-
forward line approximating the combined effect of
these two conditions is:
2  I(lv; aV) - I(lv; detV)  0
We hypothesize that this combined measure?
i.e., 2  I(lv; aV) - I(lv; detV)?will correlate bet-
Development Classes
Levin # Name Count
10.4.1* Wipe Verbs, Manner 30
17.1 Throw Verbs 30
51.3.2* Run Verbs 30
Test Classes
Levin # Name Count
18.1,2 Hit and Swat Verbs 35
30.3 Peer Verbs 18
43.2* Sound Emission 35
51.4.2 Motion (non-vehicle) 10
Table 1: Levin classes used in our experiments. A
?*? indicates a random subset of verbs in the class.
ter with human ratings of the LVCs than the mutual
information of the ?LV a V? construction alone.
For I(lv; detV), we explore several possible sets
of determiners standing in for ?det?, including the,
this, that, and the possessive determiners. We find,
contrary to the linguistic claim, that the is not al-
ways rare in ?LV a V? constructions, and the mea-
sures excluding the perform best on development
data.4
4 Materials and Methods
4.1 Experimental Classes
Three Levin classes are used for the development
set, and four classes for the test set, as shown in Ta-
ble 1. Each set of classes covers a range of LVC pro-
ductivity with the light verbs take, give, and make,
from classes in which we felt no LVCs were possi-
ble with a given LV, to classes in which many verbs
listed seemed to form valid LVCs with a given LV.
4.2 Corpora
Even the 100M words of the British National Cor-
pus (BNC Reference Guide, 2000) do not give an
acceptable level of LVC coverage: a very common
LVC such as take a stroll, for instance, is attested
only 23 times. To ensure sufficient data to detect
less common LVCs, we instead use the Web as our
corpus (in particular, the subsection indexed by the
Google search engine, http://www.google.com).
Using the Web to overcome data sparseness has
been attempted before (Keller et al, 2002); how-
ever, there are issues: misspellings, typographic er-
rors, and pages in other languages all contribute to
noise in the results. Moreover, punctuation is ig-
4Cf. I took the hike that was recommended. This finding
supports a statistical corpus-based approach to LVCs, as their
usage may be more nuanced than linguistic theory suggests.
Determiner Search Strings
Indefinite give/gives/gave a cry
Definite give/gives/gave the cry
Demons. give/gives/gave this/that cry
Possessive give/gives/gave my/.../their cry
Table 2: Searches for light verb give and verb cry.
nored in Google searches, meaning that search re-
sults can cross phrase or sentence boundaries. For
instance, an exact phrase search for ?take a cry?
would return a web page which had the text It was
too much to take. A cry escaped his lips. When
searching for an unattested LVC, these noisy results
can begin to dominate. In ongoing work, we are
devising some automatic clean-up methods to elim-
inate some of the false positives.
On the other hand, it should be pointed out that
not all ?good? LVCs will appear in our corpus, de-
spite its size. In this view we differ from Villavi-
cencio (2003), who assumes that if a multiword ex-
pression is not found in the Google index, then it is
not a good construction. As an example, consider
The clown took a cavort across the stage. The LVC
seems plausible; however, Google returns no results
for ?took a cavort?. This underlines the need for de-
termining plausible (as opposed to attested) LVCs,
which class-based generalization has the potential
to support.
4.3 Extraction
To measure mutual information, we gather several
counts for each potential LVC: the frequency of the
LVC (e.g., give a cry), the frequency of the light
verb (e.g., give), and the frequency of the comple-
ment of the LVC (e.g., a cry). To achieve broader
coverage, counts of the light verbs and the LVCs
are collapsed across three tenses: the base form, the
present, and the simple past. Since we are interested
in the differences across determiners, we search for
both the LVC (?give [det] cry?) and the complement
alone (?[det] cry?) using all singular determiners.
Thus, for each LVC, we require a number of LVC
searches, as exemplified in Table 2, and analogous
searches for ?[det] V?.
All searches were performed using an exact string
search in Google, during a 24-hour period in March,
2004. The number of results returned is used as the
frequency count. Note that this is an underestimate,
since an LVC may occur than once in a single web
page; however, examining each document to count
the actual occurrences is infeasible, given the num-
ber of possible results. The size of the corpus (also
needed in calculating our measures) is estimated at
5.6 billion, the number of hits returned in a search
for ?the?. This is also surely an underestimate, but
is consistent with our other frequency counts.
NSP is used to calculate pointwise mutual in-
formation over the counts (Banerjee and Pedersen,
2003).
5 Experimental Results
In these initial experiments, we compare human rat-
ings of the target LVCs to several mutual informa-
tion measures over our corpus counts, using Spear-
man rank correlation. We have two goals: to see
whether these LVCs show differing behaviour ac-
cording to the light verb and/or the verb class of
the complement, and to determine whether we can
indeed predict acceptability from corpus statistics.
We first describe the human ratings, then the corre-
lation results on our development and test data.
5.1 Human Ratings
We use pilot results in which two native speakers
of English rated each combination of ?LV a V? in
terms of acceptability. For the development classes,
we used integer ratings of 1 (unacceptable) to 5
(completely natural), allowing for ?in-between? rat-
ings as well, such as 2.5. For the test classes, we set
the top rating at 4, since we found that ratings up to
5 covered a larger range than seemed natural. The
test ratings yielded linearly weighted Kappa values
of .72, .39, and .44, for take, give, and make, respec-
tively, and .53 overall.5
To determine a consensus rating, the human raters
first discussed disagreements of more than one rat-
ing point. In the test data, this led to 6% of the rat-
ings being changed. (Note that this is 6% of ratings,
not 6% of verbs; fewer verbs were changed, since
for some verbs both raters changed their rating after
discussion.) We then simply averaged each pair of
ratings to yield a single consensus rating for each
item.
In order to see differences in human ratings
across the light verbs and the semantic classes of
their complements, we put the (consensus) human
ratings in bins of low (ratings   2) , medium (rat-
ings  2,   3), and high (ratings  3). (Even a
score of 2 meant that an LVC was ?ok?.) Table 3
shows the distribution of medium and high scores
for each of the light verbs and test classes. We can
see that some classes generally allow more LVCs
5Agreement on the development set was much lower (lin-
early weighted Kappa values of .37, .23, and .56, for take, give,
and make, respectively, and .38 overall), due to differences in
interpretation of the ratings. Discussion of these issues by the
raters led to more consistency in test data ratings.
Class # N take give make
18.1,2 35 8 (23%) 15 (43%) 8 (23%)
30.3 18 5 (28%) 5 (28%) 3 (17%)
43.2 35 1 (3%) 11 (31%) 9 (26%)
51.4.2 10 7 (70%) 2 (20%) 1 (10%)
Table 3: Number of medium and high scores for
each LV and class. N is the number of test verbs.
across the light verbs (e.g., 18.1,2) than others (e.g,
43.2). Furthermore, the light verbs show very differ-
ent patterns of acceptability for different classes?
e.g., give is fairly good with 43.2, while take is very
bad, and the pattern is reversed for 51.4.2. In gen-
eral, give allows more LVCs on the test classes than
do the other two light verbs.
5.2 Correlations with Statistical Measures
Our next step is to see whether the ratings, and the
patterns across light verbs and classes, are reflected
in the statistical measures over corpus data. Because
our human ratings are not normally distributed (gen-
erally having a high proportion of values less than
2), we use the Spearman rank correlation coefficient
 to compare the consensus ratings to the mutual in-
formation measures.6
As described in Section 3.2, we use pointwise
mutual information over the ?LV a V? string, as well
as measures we developed that incorporate the lin-
guistic observation that these LVCs typically do not
occur with definite determiners. On our develop-
ment set, we tested several of these measures and
found that the following had the best correlations
with human ratings:
  MI: I(lv; aV)
  DiffAll: 2  I(lv; aV) - I(lv; detV)
where I(lv; detV) is the mutual information over
strings ?LV [det] V?, and det is any determiner other
than a, an, or the. Note that DiffAll is the most
general of our combined measures; however, some
verbs are not detected with other determiners, and
thus DiffAll may apply to a smaller number of items
than MI.
We focus on the analysis of these two measures
on test data, but the general patterns are the same
6Experiments on the development set to determine a thresh-
old on the different measures to classify LVCs as good or not
showed promise in their coarse match with human judgments.
However, we set this work aside for now, since the correlation
coefficients are more informative regarding the fine-grained
match of the measures to human ratings, which cover a fairly
wide range of acceptability.
MI DiffAll
LV Class #  (  ) N  (  ) N
18.1,2 .52 (   .01) 34 .51 (   .01) 33
30.3 .53 (.02) 18 .59 (.02) 15
take 43.2 .24 (.20) 31 .32 (.10) 27
51.4.2 .68 (.03) 10 .65 (.04) 10
all .53 (   .01) 93 .52 (   .01) 85
18.1,2 .26 (.14) 33 .30 (.10) 32
30.3 .33 (.20) 17 .27 (.33) 15
give 43.2 .38 (.03) 33 .58 (   .01) 25
51.4.2 .09 (.79) 10 -.13 (.71) 10
all .28 (.01) 93 .33 (   .01) 82
18.1,2 .51 (   .01) 34 .49 (   .01) 34
30.3 .16 (.52) 18 -.11 (.68) 17
make 43.2 -.12 (.52) 34 -.19 (.29) 33
51.4.2 -.08 (.81) 10 -.20 (.58) 10
all .36 (   .01) 96 .26 (.01) 94
Table 4: Spearman rank correlation coefficents  , with   values and number of items N, between the mutual
information measures and the consensus human ratings, on unseen test data.
on the development set. Table 4 shows the correla-
tion results on our unseen test LVCs. We get rea-
sonably good correlations with the human ratings
across a number of the light verbs and classes, indi-
cating that these measures may be helpful in deter-
mining which light verb plus complement combina-
tions form valid LVCs. In what follows, we examine
more detailed patterns, to better analyze the data.
First, comparing the test correlations to Table 3,
we find that the classes with a low number of ?good?
LVCs have poor correlations. When we examine the
correlation graphs, we see that, in general, there is
a good correlation between the ratings greater than
1 and the corresponding measure, but when the rat-
ing is 1, there is often a wide range of values for the
corpus-based measure. One cause could be noise
in the data, as mentioned earlier?that is, for bad
LVCs, we are picking up too many ?false hits?, due
to the limitations of using Google searches on the
web. To confirm this, we examine one develop-
ment class (10.4.1, the Wipe manner verbs), which
was expected to be bad with take. We find a large
number of hits for ?take a V? that are not good
LVCs, such as ?take a strip [of tape/of paper]?, ?take
a pluck[-and-play approach]?. On the other hand,
some examples with unexpectedly high corpus mea-
sures are LVCs the human raters were simply not
aware of (?take a skim through the manual?), which
underscores the difficulty of human rating of a semi-
productive construction.
Second, we note that we get very good cor-
relations with take, somewhat less good correla-
tions with give, and generally poor correlations with
make. We had predicted that take and give would
behave similarly (and the difference between take
and give is less pronounced in the development
data). We think one reason give has poorer correla-
tions is that it was harder to rate (it had the highest
proportion of disagreements), and so the human rat-
ings may not be as consistent as for take. Also, for a
class like 30.3, which we expected to be good with
give (e.g., give a look, give a glance), we found that
the LVCs were mostly good only in the dative form
(e.g., give her a look, give it a glance). Since we
only looked for exact matches to ?LV a V?, we did
not detect this kind of construction.
We had predicted that make would behave dif-
ferently from take and give, and indeed, except in
one case, the correlations for make are poorer on
the individual classes. Interestingly, the correlation
overall attains a much better value using the mutual
information of ?LV a V? alone (i.e., the MI mea-
sure). We think that the pattern of correlations with
make may be because it is not necessarily a ?true
light verb? construction in many cases, but rather a
?vague action verb? (see Section 2). If so, its be-
haviour across the complements may be somewhat
more arbitrary, combining different uses.
Finally, we compare the combined measure Diff-
All to the mutual information, MI, alone. We hy-
pothesized that while the latter should indicate a
collocation, the combined measure should help to
focus on LVCs in particular, because of their lin-
guistic property of occurring primarily with an in-
definite determiner. On the individual classes, when
considering correlations that are statistically signif-
icant or marginally so (i.e., at the confidence level
of 90%), the DiffAll measure overall has somewhat
stronger correlations than MI. Over all complement
verbs together, DiffAll is roughly the same as MI
for take; is somewhat better for give, and is worse
for make.7
Better performance over the individual classes in-
dicates that when applying the measures, at least to
take and give, it is helpful to separate the data ac-
cording to semantic verb class. For make, the ap-
propriate approach is not as clear, since the results
on the individual classes are so skewed. In gen-
eral, the results confirm our hypothesis that seman-
tic verb classes are highly relevant to measuring the
acceptability of LVCs of this type. The results also
indicate the need to look in more detail at the prop-
erties of different light verbs.
6 Related Work
Other computational research on LVCs differs from
ours in two key aspects. First, the work has looked
at any nominalizations as complements of poten-
tial light verbs (what they term ?support verbs?)
(Fontenelle, 1993; Grefenstette and Teufel, 1995;
Dras and Johnson, 1996). Our work differs in fo-
cusing on verbal nouns that form the complement
of a particular type of LVC, allowing us to explore
the role of class information in restricting the com-
plements of these constructions. Second, this earlier
work has viewed all verbs as possible light verbs,
while we look at only the class of potential light
verbs identified by linguistic theory.
The difference in focus on these two aspects of
the problem leads to the basic differences in ap-
proach: while they attempt to find probable light
verbs for nominalization complements, we try to
find possible (verbal) noun complements for given
light verbs. Our work differs both practically, in the
type of measure used, and conceptually, in the for-
mulation of the problem. For example, Grefenstette
and Teufel (1995) used some linguistic properties to
weed out potential light verbs from lists sorted by
raw frequency, while Dras and Johnson (1996) used
frequency of the verb weighted by a weak predictor
of its prior probability as a light verb. We instead
use a standard collocation detection measure (mu-
tual information), the terms of which we modify to
7The development data is similar to the test data in favour-
ing DiffAll over MI across the individual classes. Over all de-
velopment verbs together, DiffAll is somewhat better than MI
for take, is roughly the same for give, and is somewhat worse
for make.
capture linguistic properties of the construction.
More fundamentally, our proposal differs in its
emphasis on possible class-based generalizations
in LVCs that have heretofore been unexplored. It
would be interesting to apply this idea to the broader
classes of nominalizations investigated in earlier
work. Moreover, our approach could draw on ideas
from the earlier proposals to detect the light verbs
automatically, since the precise set of LVs differs
crosslinguistically?and LV status may indeed be a
continuum rather than a discrete distinction.
7 Conclusions and Future Work
Our results demonstrate the benefit of treating LVCs
as more than just a simple collocation. We exploit
linguistic knowledge particular to the ?LV a V? con-
struction to devise an acceptability measure that cor-
relates reasonably well with human judgments. By
comparing the mutual information with indefinite
and definite determiners, we use syntactic patterns
to tap into the distinctive underlying properties of
the construction.
Furthermore, we hypothesized that, because the
complement in these constructions is a verb, we
would see systematic behaviour across the light
verbs in terms of their ability to combine with com-
plements from different verb classes. Our human
ratings indeed showed class-based tendencies for
the light verbs. Moreover, our acceptability measure
showed higher correlations when the verbs were di-
vided by class. This indicates that there is greater
consistency within a verb class between the cor-
pus statistics and the ability to combine with a light
verb. Thus, the semantic classes provide a useful
way to increase the performance of the acceptabil-
ity measure.
The correlations are far from perfect, however. In
addition to noise in the data, one problem may be
that these classes are too coarse-grained. Explo-
ration is needed of other possible verb (and noun)
classes as the basis for generalizing the comple-
ments of these constructions. However, we must
also look to the measures themselves for improv-
ing our techniques. Several linguistic properties dis-
tinguish these constructions, but our measures only
drew on one. In ongoing work, we are explor-
ing methods for incorporating other linguistic be-
haviours into a measure for these constructions, as
well as for LVCs more generally.
We are widening this investigation in other direc-
tions as well. Our results reveal interesting differ-
ences among the light verbs, indicating that the set
of light verbs is itself heterogeneous. More research
is needed to determine the properties of a broader
range of light verbs, and how they influence the
valid combinations they form with semantic classes.
Finally, we plan to collect more extensive rating
data, but are concerned with the difficulty found in
judging these constructions. Gathering solid human
ratings is a challenge in this line of investigation, but
this only serves to underscore the importance of de-
vising corpus-based acceptability measures in order
to better support development of accurate computa-
tional lexicons.
Acknowledgments
We thank Ted Pedersen (U. of Minnesota), Diana
Inkpen (U. of Ottawa), and Diane Massam (U. of
Toronto) for helpful advice and discussion, as well
as three anonymous reviewers for their useful feed-
back. We gratefully acknowledge the support of
NSERC of Canada.
References
J. Alba-Salas. 2002. Light Verb Constructions in
Romance: A Syntactic Analysis. Ph.D. thesis,
Cornell University.
S. Banerjee and T. Pedersen. 2003. The design,
implementation, and use of the Ngram Statistic
Package. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing
and Computational Linguistics.
C. Bannard, T. Baldwin, and A. Lascarides. 2003.
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL-2003 Work-
shop on Multiword Expressions: Analysis, Acqui-
sition and Treatment, p. 65?72.
BNC Reference Guide. 2000. Reference Guide
for the British National Corpus (World Edition).
http://www.hcu.ox.ac.uk/BNC, second edition.
M. Butt. 2003. The light verb jungle.
http://www.ai.mit.edu/people/jimmylin/papers
/Butt03.pdf.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991.
Using Statistics in Lexical Analysis, p. 115?164.
Lawrence Erlbaum.
M. Dras and M. Johnson. 1996. Death and light-
ness: Using a demographic model to find support
verbs. In Proceedings of the Fifth International
Conference on the Cognitive Science of Natural
Language Processing, Dublin, Ireland.
R. Folli, H. Harley, and S. Karimi. 2003. Determi-
nants of event type in Persian complex predicates.
Cambridge Working Papers in Linguistics.
T. Fontenelle. 1993. Using a bilingual computerized
dictionary to retrieve support verbs and combina-
torial information. Acta Linguistica Hungarica,
41(1?4):109?121.
G. Grefenstette and S. Teufel. 1995. A corpus-
based method for automatic identification of sup-
port verbs for nominalisations. In Proceedings of
EACL, p. 98?103, Dublin, Ireland.
S. Karimi. 1997. Persian complex verbs: Idiomatic
or compositional? Lexicology, 3(1):273?318.
K. Kearns. 2002. Light verbs in En-
glish. http://www.ling.canterbury.ac.nz/kate
/lightverbs.pdf.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Us-
ing the Web to overcome data sparseness. In
Proceedings of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing, p.
230?237, Philadelphia, USA.
B. Levin. 1993. English Verb Classes and Alterna-
tions, A Preliminary Investigation. University of
Chicago Press.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-
99, p. 317?324.
D. McCarthy, B. Keller, and J. Carroll. 2003.
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL-
SIGLEX Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment.
T. Miyamoto. 2000. The Light Verb Construction
in Japanese: the role of the verbal noun. John
Benjamins.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A
pain in the neck for NLP. In Proceedings of
the Third International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLING), p. 1?15.
A. Villavicencio. 2003. Verb-particle constructions
in the world wide web. In Proceedings of the
ACL-SIGSEM Workshop on the Linguistic Di-
mensions of Prepositions and their use in Com-
putational Linguistics Formalisms and Applica-
tions.
A. Wierzbicka. 1982. Why can you Have a Drink
when you can?t *Have an Eat? Language,
58(4):753?799.
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 38?47,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatically Distinguishing Literal and Figurative Usages
of Highly Polysemous Verbs
Afsaneh Fazly and Ryan North and Suzanne Stevenson
Department of Computer Science
University of Toronto
 
afsaneh,ryan,suzanne  @cs.toronto.edu
Abstract
We investigate the meaning extensions
of very frequent and highly polysemous
verbs, both in terms of their compositional
contribution to a light verb construction
(LVC), and the patterns of acceptability of
the resulting LVC. We develop composi-
tionality and acceptability measures that
draw on linguistic properties specific to
LVCs, and demonstrate that these statisti-
cal, corpus-based measures correlate well
with human judgments of each property.
1 Introduction
Due to a cognitive priority for concrete, easily visu-
alizable entities, abstract notions are often expressed
in terms of more familiar and concrete things and
situations (Newman, 1996; Nunberg et al, 1994).
This gives rise to a widespread use of metaphor
in language. In particular, certain verbs easily un-
dergo a process of metaphorization and meaning
extension (e.g., Pauwels, 2000; Newman and Rice,
2004). Many such verbs refer to states or acts that
are central to human experience (e.g., sit, put, give);
hence, they are often both highly polysemous and
highly frequent. An important class of verbs prone
to metaphorization are light verbs, on which we fo-
cus in this paper.
A light verb, such as give, take, or make, com-
bines with a wide range of complements from differ-
ent syntactic categories (including nouns, adjectives,
and prepositions) to form a new predicate called a
light verb construction (LVC). Examples of LVCs
include:
1. (a) Azin took a walk along the river.
(b) Sam gave a speech to a few students.
(c) Joan takes care of him when I am away.
(d) They made good on their promise to win.
(e) You should always take this into account.
The light verb component of an LVC is ?seman-
tically bleached? to some degree; consequently, the
semantic content of an LVC is assumed to be de-
termined primarily by the complement (Butt, 2003).
Nevertheless, light verbs exhibit meaning variations
when combined with different complements. For ex-
ample, give in give (someone) a present has a literal
meaning, i.e., ?transfer of possession? of a THING
to a RECIPIENT. In give a speech, give has a figura-
tive meaning: an abstract entity (a speech) is ?trans-
ferred? to the audience, but no ?possession? is in-
volved. In give a groan, the notion of transfer is
even further diminished.
Verbs exhibiting such meaning variations are
widespread in many languages. Hence, successful
NLP applications?especially those requiring some
degree of semantic interpretation?need to identify
and treat them appropriately. While figurative uses
of a light verb are indistinguishable on the surface
from a literal use, this distinction is essential to a
machine translation system, as Table 1 illustrates. It
is therefore important to determine automatic mech-
anisms for distinguishing literal and figurative uses
of light verbs.
Moreover, in their figurative usages, light verbs
tend to have similar patterns of cooccurrence with
semantically similar complements (e.g., Newman,
1996). Each similar group of complement nouns can
even be viewed as a possible meaning extension for
a light verb. For example, in give advice, give or-
ders, give a speech, etc., give contributes a notion of
38
Sentence in English Intermediate semantics Translation in French
Azin gave Sam a book. (e1/give Azin a donne? un livre a` Sam.
:agent (a1/?Azin?) Azin gave a book to Sam.
:theme (b1/?book?)
:recepient (s1/?Sam?))
Azin gave the lasagna a try. (e2/give-a-try   try Azin a essaye? le lasagne.
:agent (a1/?Azin?) Azin tried the lasagna.
:theme (l1/?lasagna?))
Table 1: Sample sentences with literal and figurative usages of give.
?abstract transfer?, while in give a groan, give a cry,
give a moan, etc., give contributes a notion of ?emis-
sion?. There is much debate on whether light verbs
have one highly abstract (underspecified) meaning,
further determined by the context, or a number of
identifiable (related) subsenses (Pustejovsky, 1995;
Newman, 1996). Under either view, it is important
to elucidate the relation between possible interpreta-
tions of a light verb and the sets of complements it
can occur with.
This study is an initial investigation of techniques
for the automatic discovery of meaning extensions
of light verbs in English. As alluded to above, we
focus on two issues: (i) the distinction of literal ver-
sus figurative usages, and (ii) the role of semanti-
cally similar classes of complements in refining the
figurative meanings.
In addressing the first task, we note the connection
between the literal/figurative distinction and the de-
gree to which a light verb contributes composition-
ally to the semantics of an expression. In Section 2,
we elaborate on the syntactic properties that relate
to the compositionality of light verbs, and propose
a statistical measure incorporating these properties,
which places light verb usages on a continuum of
meaning from literal to figurative. Figure 1(a) de-
picts such a continuum in the semantic space of give,
with the literal usages represented as the core.
The second issue above relates to our long-term
goal of dividing the space of figurative uses of a
light verb into semantically coherent segments, as
shown in Figure 1(b). Section 3 describes our hy-
pothesis on the class-based nature of the ability of
potential complements to combine with a light verb.
At this point we cannot spell out the different figura-
tive meanings of the light verb associated with such
classes. We take a preliminary step in proposing a
statistical measure of the acceptability of a combi-
nation of a light verb and a class of complements,
and explore the extent to which this measure can re-
veal class-based behaviour.
Subsequent sections of the paper present the cor-
pus extraction methods for estimating our composi-
tionality and acceptability measures, the collection
of human judgments to which the measures will be
compared, experimental results, and discussion.
2 Compositionality of Light Verbs
2.1 Linguistic Properties: Syntactic Flexibility
We focus on a broadly-documented subclass of light
verb constructions, in which the complement is an
activity noun that is often the main source of seman-
tic predication (Wierzbicka, 1982). Such comple-
ments are assumed to be indefinite, non-referential
predicative nominals (PNs) that are often morpho-
logically related to a verb (see the complements in
examples (1a?c) above). We refer to this class of
light verb constructions as ?LV+PN? constructions,
or simply LVCs.
There is much linguistic evidence that semantic
properties of a lexical item determine, to a large ex-
tent, its syntactic behaviour (e.g., Rappaport Hovav
and Levin, 1998). In particular, the degree of com-
positionality (decomposability) of a multiword ex-
pression has been known to affect its participation
in syntactic transformations, i.e., its syntactic flexi-
bility (e.g., Nunberg et al, 1994). English ?LV+PN?
constructions enforce certain restrictions on the syn-
tactic freedom of their noun components (Kearns,
2002). In some, the noun may be introduced by a
definite article, pluralized, passivized, relativized, or
even wh-questioned:
39
give a book
give a present
give money
give rightgive advice
give opportunity
give orders
give permission
give a speech
give a smile
give a laugh give a yell
give a groan
give a sweep
give a push
give a dust
give a wipe
give a pull
give a kick
more figurative
give a book
give a present
give money
give a wipe
give a sweep
give a dust
give a push
give a kick
give a pull
give orders
give a speech
give advice
give permission
give right
give opportunity
give a yell
give a laugh
give a groan
give a smile
(a) (b)
Figure 1: Two possible partitionings of the semantic space of give.
2. (a) Azin gave a speech to a few students.
(b) Azin gave the speech just now.
(c) Azin gave a couple of speeches last night.
(d) A speech was given by Azin just now.
(e) Which speech did Azin give?
Others have little or no syntactic freedom:
3. (a) Azin gave a groan just now.
(b) * Azin gave the groan just now.
(c) ? Azin gave a couple of groans last night.
(d) * A groan was given by Azin just now.
(e) * Which groan did Azin give?
Recall that give in give a groan is presumed to be
a more abstract usage than give in give a speech. In
general, the degree to which the light verb retains
aspects of its literal meaning?and contributes them
compositionally to the LVC?is reflected in the de-
gree of syntactic freedom exhibited by the LVC. We
exploit this insight to devise a statistical measure of
compositionality, which uses evidence of syntactic
(in)flexibility of a potential LVC to situate it on a
scale of literal to figurative usage of the light verb:
i.e., the more inflexible the expression, the more fig-
urative (less compositional) the meaning.
2.2 A Statistical Measure of Compositionality
Our proposed measure quantifies the degree of syn-
tactic flexibility of a light verb usage by looking
at its frequency of occurrence in any of a set of
relevant syntactic patterns, such as those in exam-
ples (2) and (3). The measure, COMP   LV  N  , as-
signs a score to a given combination of a light verb
(LV) and a noun (N):
COMP
 
LV  N 
ASSOC
 
LV;N 
DIFF
 
ASSOC
 
LV;N  PSpos  ASSOC
 
LV;N  PSneg 
That is, the greater the association between LV and
N, and the greater the difference between their asso-
ciation with positive syntactic patterns and negative
syntactic patterns, the more figurative the meaning
of the light verb, and the higher the score.
The strength of the association between the light
verb and the complement noun is measured using
pointwise mutual information (PMI) whose standard
formula is given here:1
ASSOC
 
LV;N 	 log Pr
 
LV  N 
Pr
 
LV  Pr
 
N 

 log n f
 
LV  N 
f   LV  f   N 
where n is an estimate of the total number of verb
and object noun pairs in the corpus.
1PMI is subject to overestimation for low frequency items
(Dunning, 1993), thus we require a minimum frequency of oc-
currence for the expressions under study.
40
PSpos represents the set of syntactic patterns pre-
ferred by less-compositional (more figurative) LVCs
(e.g., as in (3a)), and PSneg represents less preferred
patterns (e.g., those in (3b?e)). Typically, these pat-
terns most affect the expression of the complement
noun. Thus, to measure the strength of association
between an expression and a set of patterns, we use
the PMI of the light verb, and the complement noun
appearing in all of the patterns in the set, as in:
ASSOC
 
LV;N  PSpos   PMI
 
LV;N  PSpos 
 log
Pr
 
LV  N  PSpos 
Pr
 
LV  Pr
 
N  PSpos 

 log
n f   LV  N  PSpos 
f   LV  f   N  PSpos 
in which counts of occurrences of N in syntactic
contexts represented by PSpos are summed over all
patterns in the set. ASSOC(LV;N  PSneg) is defined
analogously using PSneg in place of PSpos.
DIFF measures the difference between the asso-
ciation strengths of the positive and negative pat-
tern sets, referred to as ASSOC pos and ASSOCneg ,
respectively. Our calculation of ASSOC uses max-
imum likelihood estimates of the true probabilities.
To account for resulting errors, we compare the two
confidence intervals,

ASSOC pos  ?ASSOC pos  and

ASSOCneg  ?ASSOCneg  , as in Lin (1999). We take
the minimum distance between the two as a conser-
vative estimate of the true difference:
DIFF
 
ASSOC
 
LV;N  PSpos  ASSOC
 
LV;N  PSneg  

 
ASSOC pos  ?ASSOCpos 

 
ASSOCneg  ?ASSOCneg 
Taking the difference between confidence intervals
lessens the effect of differences that are not statisti-
cally significant. (The confidence level, 1

?, is set
to 95% in all experiments.)
3 Acceptability Across Semantic Classes
3.1 Linguistic Properties: Class Behaviour
In this aspect of our work, we narrow our focus onto
a subclass of ?LV+PN? constructions that have a PN
complement in a stem form identical to a verb, pre-
ceded (typically) by an indefinite determiner (as in
(1a?b) above). Kearns (2002), Wierzbicka (1982),
and others have noted that the way in which LVs
combine with such PNs to form acceptable LVCs
is semantically patterned?that is, PNs with similar
semantics appear to have the same trends of cooc-
currence with an LV.
Our hypothesis is that semantically similar
LVCs?i.e., those formed from an LV plus any of
a set of semantically similar PNs?distinguish a fig-
urative subsense of the LV. In the long run, if this is
true, it could be exploited by using class information
to extend our knowledge of acceptable LVCs and
their likely meaning (cf. such an approach to verb
particle constructions by Villavicencio, 2003).
As steps to achieving this long-term goal, we must
first devise an acceptability measure which deter-
mines, for a given LV, which PNs it successfully
combines with. We can even use this measure to
provide evidence on whether the hypothesized class-
based behaviour holds, by seeing if the measure ex-
hibits differing behaviour across semantic classes of
potential complements.
3.2 A Statistical Measure of Acceptability
We develop a probability formula that captures the
likelihood of a given LV and PN forming an accept-
able LVC. The probability depends on both the LV
and the PN, and on these elements being used in an
LVC:
ACPT
 
LV  PN 
 Pr
 
LV  PN  LVC 
 Pr
 
PN  Pr
 
LVC  PN  Pr
 
LV  PN  LVC 
The first factor, Pr
 
PN  , reflects the linguistic
observation that higher frequency words are more
likely to be used as LVC complements (Wierzbicka,
1982). We estimate this factor by f   PN  n, where n
is the number of words in the corpus.
The probability that a given LV and PN form an
acceptable LVC further depends on how likely it is
that the PN combines with any light verbs to form an
LVC. The frequency with which a PN forms LVCs is
estimated as the number of times we observe it in the
prototypical ?LV a/an PN? pattern across LVs. (Note
that such counts are an overestimate, since we can-
not determine which usages are indeed LVCs vs. lit-
eral uses of the LV.) Since these counts consider the
PN only in the context of an indefinite determiner,
41
we normalize over counts of ?a/an PN? (noted as
aPN) to form the conditional probability estimate of
the second factor:
Pr
 
LVC  PN  

v
?
i   1
f   LV i  aPN 
f   aPN 
where v is the number of light verbs considered.
The third factor, Pr
 
LV  PN  LVC  , reflects that
different LVs have varying degrees of acceptability
when used with a given PN in an LVC. We similarly
estimate this factor with counts of the given LV and
PN in the typical LVC pattern: f   LV  aPN  f   aPN  .
Combining the estimates of the three factors
yields:
ACPT
 
LV  PN  

f   PN 
n

v
?
i   1
f   LV i  aPN 
f   aPN 

f   LV  aPN 
f   aPN 
4 Materials and Methods
4.1 Light Verbs
Common light verbs in English include give, take,
make, get, have, and do, among others. We focus
here on two of them, i.e., give and take, that are
frequently and productively used in light verb con-
structions, and are highly polysemous. The Word-
Net polysemy count (number of different senses) of
give and take are 44 and 42, respectively.
4.2 Experimental Expressions
Experimental expressions?i.e., potential LVCs us-
ing give and take?are drawn from two sources.
The development and test data used in experiments
of compositionality (bncD and bncT, respectively)
are randomly extracted from the BNC (BNC Ref-
erence Guide, 2000), yielding expressions cover-
ing a wide range of figurative usages of give and
take, with complements from different semantic cat-
egories. In contrast, in experiments that involve ac-
ceptability, we need figurative usages of ?the same
type?, i.e., with semantically similar complement
nouns, to further examine our hypothesis on the
class-based behaviour of light verb combinations.
Since in these LVCs the complement is a predica-
tive noun in stem form identical to a verb, we form
development and test expressions by combining give
or take with verbs from selected semantic classes of
Levin (1993), taken from Stevenson et al (2004).
4.3 Corpora
We gather estimates for our COMP measure from the
BNC, processed using the Collins parser (Collins,
1999) and TGrep2 (Rohde, 2004). Because some
LVCs can be rare in classical corpora, our ACPT es-
timates are drawn from the World Wide Web (the
subsection indexed by AltaVista). In our compari-
son of the two measures, we use web data for both,
using a simplified version of COMP. The high level
of noise on the web will influence the performance
of both measures, but COMP more severely, due to
its reliance on comparisons of syntactic patterns.
Web counts are based on an exact-phrase query to
AltaVista, with the number of pages containing the
search phrase recorded as its frequency.2 The size
of the corpus is estimated at 3.7 billion, the number
of hits returned in a search for the. These counts are
underestimates of the true frequencies, as a phrase
may appear more than once in a web page, but we
assume all counts to be similarly affected.
4.4 Extraction
Most required frequencies are simple counts of a
word or string of words, but the syntactic patterns
used in the compositionality measure present some
complexity. Recall that PSpos and PSneg are pattern
sets representing the syntactic contexts of interest.
Each pattern encodes several syntactic attributes: v,
the voice of the extracted expression (active or pas-
sive); d, the type of the determiner introducing N
(definite or indefinite); and n, the number of N (sin-
gular or plural). In our experiments, the set of pat-
terns associated with less-compositional use, PSpos,
consists of the single pattern with values active, in-
definite, and singular, for these attributes. PSneg con-
sists of all patterns with at least one of these at-
tributes having the alternative value.
While our counts on the BNC can use syntac-
tic mark-up, it is not feasible to collect counts on
the web for some of the pattern attributes, such as
voice. We develop two different variations of the
measure, one for BNC counts, and a simpler one for
2All searches were performed March 15?30, 2005.
42
give take
Human Ratings bncD bncT bncD bncT
?low? 20 10 36 19
?medium? 35 16 9 5
?high? 24 10 27 10
Total 79 36 72 34
Table 2: Distribution of development and test expressions with
respect to human compositionality ratings.
web counts. We thus subscript COMP with abbre-
viations standing for each attribute in the measure:
COMPvdn for a measure involving all three attributes
(used on BNC data), and COMPd for a measure in-
volving determiner type only (used on web data).
5 Human Judgments
5.1 Judgments of Compositionality
To determine how well our proposed measure
of compositionality captures the degree of lit-
eral/figurative use of a light verb, we compare its
scores to human judgments on compositionality.
Three judges (native speakers of English with suf-
ficient linguistic knowledge) answered yes/no ques-
tions related to the contribution of the literal mean-
ing of the light verb within each experimental ex-
pression. The combination of answers to these ques-
tions is transformed to numerical ratings, ranging
from 0 (fully non-compositional) to 4 (largely com-
positional). The three sets of ratings yield linearly
weighted Kappa values of .34 and .70 for give and
take, respectively. The ratings are averaged to form
a consensus set to be used for evaluation.3
The lists of rated expressions were biased toward
figurative usages of give and take. To achieve a spec-
trum of literal to figurative usages, we augment the
lists with literal expressions having an average rating
of 5 (fully compositional). Table 2 shows the distri-
bution of the experimental expressions across three
intervals of compositionality degree, ?low? (ratings
  1), ?medium? (1  ratings  3), and ?high? (rat-
ings  3). Table 3 presents sample expressions with
different levels of compositionality ratings.
3We asked the judges to provide short paraphrases for each
expression, and only use those expressions for which the major-
ity of judges expressed the same sense.
Sample Expressions
Human Ratings give take
?low? give a squeeze take a shower
?medium? give help take a course
?high? give a dose take an amount
Table 3: Sample expressions with different levels of composi-
tionality ratings.
5.2 Judgments of Acceptability
Our acceptability measure is compared to the hu-
man judgments gathered by Stevenson et al (2004).
Two expert native speakers of English rated the ac-
ceptability of each potential ?LV+PN? construction
generated by combining give and take with candi-
date complements from the development and test
Levin classes. Ratings were from 1 (unacceptable)
to 5 (completely natural; this was capped at 4 for
test data), allowing for ?in-between? ratings as well,
such as 2.5. On test data, the two sets of ratings
yielded linearly weighted Kappa values of .39 and
.72 for give and take, respectively. (Interestingly,
a similar agreement pattern is found in our human
compositionality judgments above.) The consensus
set of ratings was formed from an average of the two
sets of ratings, once disagreements of more than one
point were discussed.
6 Experimental Results
To evaluate our compositionality and acceptability
measures, we compare them to the relevant con-
sensus human ratings using the Spearman rank cor-
relation coefficient, rs. For simplicity, we report
the absolute value of rs for all experiments. Since
in most cases, correlations are statistically signifi-
cant (p  01), we omit p values; those rs values
for which p is marginal (i.e.,  01   p    10) are
subscripted with an ?m? in the tables. Correlation
scores in boldface are those that show an improve-
ment over the baseline, PMILVC .
The PMILVC measure is an informed baseline, since
it draws on properties of LVCs. Specifically, PMILVC
measures the strength of the association between a
light verb and a noun appearing in syntactic patterns
preferred by LVCs, i.e., PMILVC  PMI
 
LV;N  PSpos  .
Assuming that an acceptable LVC forms a detectable
collocation, PMILVC can be interpreted as an informed
baseline for degree of acceptability. PMILVC can also
43
PMILVC COMPvdn
LV Data Set n rs rs
bncT 36 .62 .57
give bncDT 114 .68 .70
bncDT/a 79 .68 .75
bncT 34 .51 .59
take bncDT 106 .52 .61
bncDT/a 68 .63 .72
Table 4: Correlations (rs; n = # of items) between human com-
positionality ratings and COMP measure (counts from BNC).
be considered as a baseline for the degree of compo-
sitionality of an expression (with respect to the light
verb component), under the assumption that the less
compositional an expression, the more its compo-
nents appear as a fixed collocation.
6.1 Compositionality Results
Table 4 displays the correlation scores of the human
compositionality ratings with COMPvdn, our com-
positionality measure estimated with counts from
the BNC. Given the variety of light verb usages
in expressions used in the compositionality data,
we report correlations not only on test data (bncT),
but also on development and test data combined
(bncDT) to get more data points and hence more re-
liable correlation scores. Compared to the baseline,
COMPvdn has generally higher correlations with hu-
man ratings of compositionality.
There are two different types of expressions
among those used in compositionality experiments:
expressions with an indefinite determiner a (e.g.,
give a kick) and those without a determiner (e.g.,
give guidance). Despite shared properties, the two
types of expressions may differ with respect to syn-
tactic flexibility, due to differing semantic proper-
ties of the noun complements in the two cases. We
thus calculate correlation scores for expressions with
the indefinite determiner only, from both develop-
ment and test data (bncDT/a). We find that COMPvdn
has higher correlations (and larger improvements
over the baseline) on this subset of expressions.
(Note that there are comparable numbers of items
in bncDT and bncDT/a, and the correlation scores
are highly significant?very small p values?in both
cases.)
To explore the effect of using a larger but noisier
corpus, we compare the performance of COMPvdn
Levin class: 18.1,2 30.3 43.2
LV n=35 n=18 n=35
give % fair/good ratings 51 44 54
log of mean ACPT -6 -4 -5
take % fair/good ratings 23 28 3
log of mean ACPT -4 -3 -6
Table 5: Comparison of the proportion of human ratings consid-
ered ?fair? or ?good? in each class, and the log10 of the mean
ACPT score for that class.
with COMPd , the compositionality measure using
web data. The correlation scores for COMPd on
bncDT are .41 and .35, for give and take, respec-
tively, compared to a baseline (using web counts) of
.37 and .32. We find that COMPvdn has significantly
higher correlation scores (larger rs and much smaller
p values), as well as larger improvements over the
baseline. This is a confirmation that using more syn-
tactic information, from less noisy data, improves
the performance of our compositionality measure.4
6.2 Acceptability Results
We have two goals in assessing our ACPT measure:
one is to demonstrate that the measure is indeed in-
dicative of the level of acceptability of an LVC, and
the other is to explore whether it helps to indicate
class-based patterns of acceptability.
Regarding the latter, Stevenson et al (2004) found
differing overall levels of (human) acceptability for
different Levin classes combined with give and take.
This indicates a strong influence of semantic simi-
larity on the possible LV and complement combina-
tions. Our ACPT measure also yields differing pat-
terns across the semantic classes. Table 5 shows,
for each light verb and test class, the proportion of
acceptable LVCs according to human ratings, and
the log of the mean ACPT score for that LV and
class combination. For take, the ACPT score gener-
ally reflects the difference in proportion of accepted
expressions according to the human ratings, while
for give, the measure is less consistent. (The three
development classes show the same pattern.) The
ACPT measure thus appears to reflect the differing
patterns of acceptability across the classes, at least
4Using the automatically parsed BNC as a source of less
noisy data improves performance. However, since these con-
structions may be infrequent with any particular complement,
we do not expect the use of cleaner but more plentiful text (such
as existing treebanks) to improve the performance any further.
44
Levin PMILVC ACPT
LV Class n rs rs
18.1,2 35 .39m .55
give 30.3 18 .38m .73
43.2 35 .30m .34m
18.1.2 35 .57 .61
take 30.3 18 .55 .64
43.2 35 .43 .47
Table 6: Correlations (rs; n = # of items) between acceptability
measures and consensus human ratings (counts from web).
Human PMILVC ACPT COMPd
Ratings LV n rs rs rs
accept. give 88 .31 .42 .40
(Levin) take 88 .58 .61 .56
compos. give 114 .37 .21m .41
(bncDT) take 106 .32 .30 .35
Table 7: Correlations (rs; n = # of items) between each measure
and each set of human ratings (counts from web).
for take.
To get a finer-grained notion of the degree to
which ACPT conforms with human ratings, we
present correlation scores between the two, in
Table 6. The results show that ACPT has higher
correlation scores than the baseline?substantially
higher in the case of give. The correlations for give
also vary more widely across the classes.
These results together indicate that the accept-
ability measure may be useful, and indeed taps into
some of the differing levels of acceptability across
the classes. However, we need to look more closely
at other linguistic properties which, if taken into ac-
count, may improve the consistency of the measure.
6.3 Comparing the Two Measures
Our two measures are intended for different pur-
poses, and indeed incorporate differing linguistic in-
formation about LVCs. However, we also noted that
PMILVC can be viewed as a baseline for both, indicat-
ing some underlying commonality. It is worth ex-
ploring whether each measure taps into the differ-
ent phenomena as intended. To do so, we correlate
COMP with the human ratings of acceptability, and
ACPT with the human ratings of compositionality,
as shown in Table 7. (The formulation of the ACPT
measure here is adapted for use with determiner-less
LVCs.) For comparability, both measures use counts
from the web. The results confirm that COMPd cor-
relates better than does ACPT with compositionality
ratings, while ACPT correlates best with acceptabil-
ity ratings.
7 Discussion and Concluding Remarks
Recently, there has been increasing awareness of the
need for appropriate handling of multiword expres-
sions (MWEs) in NLP tasks (Sag et al, 2002). Some
research has concentrated on the automatic acqui-
sition of semantic knowledge about certain classes
of MWEs, such as compound nouns or verb parti-
cle constructions (VPCs) (e.g., Lin, 1999; McCarthy
et al, 2003; Villavicencio, 2003). Previous research
on LVCs, on the other hand, has primarily focused
on their automatic extraction (e.g., Grefenstette and
Teufel 1995; Dras and Johnson 1996; Moiro?n 2004;
though see Stevenson et al 2004).
Like most previous studies that focus on seman-
tic properties of MWEs, we are interested in the is-
sue of compositionality. Our COMP measure aims to
identify a continuum along which a light verb con-
tributes to the semantics of an expression. In this
way, our work combines aspects of earlier work on
VPC semantics. McCarthy et al (2003) determine a
continuum of compositionality of VPCs, but do not
distinguish the contribution of the individual compo-
nents. Bannard et al (2003), on the other hand, look
at the separate contribution of the verb and particle,
but assume that a binary decision on the composi-
tionality of each is sufficient.
Previous studies determine compositionality by
looking at the degree of distributional similarity be-
tween an expression and its component words (e.g.,
McCarthy et al, 2003; Bannard et al, 2003; Bald-
win et al, 2003). Because light verbs are highly pol-
ysemous and frequently used in LVCs, such an ap-
proach is not appropriate for determining their con-
tribution to the semantics of an expression. We in-
stead examine the degree to which a light verb usage
is ?similar? to the prototypical LVC, through a sta-
tistical comparison of its behaviour within different
syntactic patterns. Syntactic flexibility and semantic
compositionality are known to be strongly correlated
for many types of MWEs (Nunberg et al, 1994). We
thus intend to extend our approach to include other
polysemous verbs with metaphorical extensions.
Our compositionality measure correlates well
with the literal/figurative spectrum represented in
45
human judgments. We also aim to determine finer-
grained distinctions among the identified figurative
usages of a light verb, which appear to relate to the
semantic class of its complement. Semantic class
knowledge may enable us to elucidate the types of
relations between a light verb and its complement
such as those determined in the work of Wanner
(2004), but without the need for the manually la-
belled training data which his approach requires.
Villavicencio (2003) used class-based knowledge to
extend a VPC lexicon, but assumed that an unob-
served VPC is not acceptable. We instead believe
that more robust application of class-based knowl-
edge can be achieved with a better estimate of the
acceptability of various expressions.
Work indicating acceptability of MWEs is largely
limited to collocational analysis using PMI-based
measures (Lin, 1999; Stevenson et al, 2004). We
instead use a probability formula that enables flex-
ible integration of LVC-specific linguistic proper-
ties. Our ACPT measure yields good correlations
with human acceptability judgments; indeed, the av-
erage increase over the baseline is about twice as
high as that of the acceptability measure proposed
by Stevenson et al (2004). Although ACPT also
somewhat reflects different patterns across seman-
tic classes, the results clearly indicate the need for
incorporating more knowledge into the measure to
capture class-based behaviour more consistently.
The work presented here is preliminary, but is the
first we are aware of to tie together the two issues of
compositionality and acceptability, and relate them
to the notion of class-based meaning extensions of
highly polysemous verbs. Our on-going work is fo-
cusing on the role of the noun component of LVCs,
to determine the compositional contribution of the
noun to the semantics of the expression, and the role
of noun classes in influencing the meaning exten-
sions of light verbs.
References
Baldwin, T., Bannard, C., Tanaka, T., and Wid-
dows, D. (2003). An empirical model of multi-
word expression decomposability. In Proceedings
of the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment,
pages 89?96.
Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment, pages 65?72.
BNC Reference Guide (2000). Reference Guide for
the British National Corpus (World Edition), sec-
ond edition.
Butt, M. (2003). The light verb jungle. Workshop
on Multi-Verb Constructions.
Collins, M. (1999). Head-Driven Statistical Models
for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania.
Dras, M. and Johnson, M. (1996). Death and light-
ness: Using a demographic model to find support
verbs. In Proceedings of the Fifth International
Conference on the Cognitive Science of Natural
Language Processing.
Dunning, T. (1993). Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Grefenstette, G. and Teufel, S. (1995). Corpus-
based method for automatic identification of sup-
port verbs for nominalization. In Proceedings of
the 7th Meeting of the EACL.
Kearns, K. (2002). Light verbs in English.
manuscript.
Levin, B. (1993). English Verb Classes and Alterna-
tions: A Preliminary Investigation. The Univer-
sity of Chicago Press.
Lin, D. (1999). Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the ACL, pages 317?324.
McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment.
Moiro?n, M. B. V. (2004). Discarding noise in an au-
tomatically acquired lexicon of support verb con-
structions. In Proceedings of the 4th International
Conference on Language Resources and Evalua-
tion (LREC).
Newman, J. (1996). Give: A Cognitive Linguistic
Study. Mouton de Gruyter.
46
Newman, J. and Rice, S. (2004). Patterns of usage
for English SIT, STAND, and LIE: A cognitively
inspired exploration in corpus linguistics. Cogni-
tive Linguistics, 15(3):351?396.
Nunberg, G., Sag, I. A., and Wasow, T. (1994). Id-
ioms. Language, 70(3):491?538.
Pauwels, P. (2000). Put, Set, Lay and Place: A
Cognitive Linguistic Approach to Verbal Mean-
ing. LINCOM EUROPA.
Pustejovsky, J. (1995). The Generative Lexicon.
MIT Press.
Rappaport Hovav, M. and Levin, B. (1998). Build-
ing verb meanings. In Butt and Geuder, editors,
The Projection of Arguments: Lexical and Com-
putational Factors, pages 97?134. CSLI Publica-
tions.
Rohde, D. L. T. (2004). TGrep2 User Manual.
Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. (2002). Multiword expressions: A
pain in the neck for NLP. In Proceedings of the
3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CI-
CLING?02), pages 1?15.
Stevenson, S., Fazly, A., and North, R. (2004). Sta-
tistical measures of the semi-productivity of light
verb constructions. In Proceedings of the ACL-04
Workshop on Multiword Expressions: Integrating
Processing, pages 1?8.
Villavicencio, A. (2003). Verb-particle construc-
tions and lexical resources. In Proceedings of
the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment,
pages 57?64.
Wanner, L. (2004). Towards automatic fine-grained
semantic classification of verb-noun collocations.
Natural Language Engineering, 10(2):95?143.
Wierzbicka, A. (1982). Why can you Have a Drink
when you can?t *Have an Eat? Language,
58(4):753?799.
47
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Distinguishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures
Afsaneh Fazly
Department of Computer Science
University of Toronto
Toronto, Canada
afsaneh@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
suzanne@cs.toronto.edu
Abstract
We identify several classes of multiword ex-
pressions that each require a different encod-
ing in a (computational) lexicon, as well as
a different treatment within a computational
system. We examine linguistic properties
pertaining to the degree of semantic idiosyn-
crasy of these classes of expressions. Ac-
cordingly, we propose statistical measures to
quantify each property, and use the measures
to automatically distinguish the classes.
1 Motivation
Multiword expressions (MWEs) are widely used in
written language as well as in colloquial speech. An
MWE is composed of two or more words that to-
gether form a single unit of meaning, e.g., frying pan,
take a stroll, and kick the bucket. Most MWEs behave
like any phrase composed of multiple words, e.g.,
their components may be separated, as in She took a
relaxing stroll along the beach. Nonetheless, MWEs
are distinct from multiword phrases because they in-
volve some degree of semantic idiosyncrasy, i.e., the
overall meaning of an MWE diverges from the com-
bined contribution of its constituent parts. Because of
their frequency and their peculiar behaviour, MWEs
pose a great challenge to the creation of natural lan-
guage processing (NLP) systems (Sag et al, 2002).
NLP applications, such as semantic parsing and ma-
chine translation should not only identify MWEs, but
also should know how to treat them when they are
encountered.
Semantic idiosyncrasy is a matter of degree (Nun-
berg et al, 1994). The idiom shoot the breeze is
largely idiosyncratic, because its meaning (?to chat?)
does not have much to do with the meaning of shoot
or breeze. MWEs such as give a try (?try?) and make
a decision (?decide?) are semantically less idiosyn-
cratic (more predictable). These are MWEs because
the overall meaning of the expression diverges from
the combined meanings of the constituents. Nonethe-
less, there is some degree of predictability in their
meanings that makes them distinct from idioms. In
these, the complement of the verb (here, a noun) de-
termines the primary meaning of the overall expres-
sion. This class of expressions is referred to as light
verb constructions (LVCs) in the linguistics literature
(Miyamoto, 2000; Butt, 2003).
Clearly, a computational system should distinguish
idioms and LVCs, both from each other, and from
similar-on-the-surface (literal) phrases such as shoot
the bird and give a present. Idioms are largely id-
iosyncratic; a computational lexicographer thus may
decide to list idioms such as shoot the breeze in a lex-
icon along with their idiomatic meanings. In contrast,
the meaning of MWEs such as make a decision can
be largely predicted, given that they are LVCs. Ta-
ble 1 shows the different underlying semantic struc-
ture of a sentence containing an idiom (shoot the
breeze) and a sentence containing an LVC (give a
try). As can be seen, such MWEs should also be
treated differently when translated into another lan-
guage. Note that in contrast to a literal combination,
such as shoot the bird, for idioms and LVCs, the num-
ber of arguments expressed syntactically may differ
from the number of the semantic participants.
Many NLP applications also need to distinguish
another group of MWEs that are less idiosyncratic
9
Class English sentence Semantic representation French translation
Literal Jill and Tim shot the bird. (event/SHOOT Jill et Tim ont abattu l?oiseau.
:agent (?Jill ? Tim?) Jill and Tim shot down the bird.
:theme (?bird?))
Abstract Jill makes a living singing in pubs. (event/EARN-MONEY Jill gagne sa vie en chantant dans des bars.
:agent (?Jill?)) Jill makes a living by singing in the pubs.
LVC Jill gave the lasagna a try. (event/TRY Jill a essaye? le lasagne.
:agent (?Jill?) Jill tried the lasagna.
:theme (?lasagna?))
Idiom Jill and Tim shot the breeze. (event/CHAT Jill et Tim ont bavarde?.
:agent (?Jill ? Tim?)) Jill and Tim chatted.
Table 1: Sample English MWEs and their translation in French.
than idioms and LVCs, but more so than literal com-
binations. Examples include give confidence and
make a living. These are idiosyncratic because the
meaning of the verb is a metaphorical (abstract)
extension of its basic physical semantics. More-
over, they often take on certain connotations be-
yond the compositional combination of their con-
stituent meanings. They thus exhibit behaviour of-
ten attributed to collocations, e.g., they appear with
greater frequency than semantically similar combina-
tions. For example, searching on Google, we found
much higher frequency for give confidence compared
to grant confidence. As can be seen in Table 1, an ab-
stract combination such as make a living, although
largely compositional, may not translate word-for-
word. Rather, it should be translated taking into ac-
count that the verb has a metaphorical meaning, dif-
ferent from its basic semantics.
Here, we focus on a particular class of English
MWEs that are formed from the combination of a
verb with a noun in its direct object position, re-
ferred to as verb+noun combinations. Specifically,
we provide a framework for identifying members of
the following semantic classes of verb+noun combi-
nations: (i) literal phrases (LIT), (ii) abstract combi-
nations (ABS), (iii) light verb constructions (LVC),
and (iv) idiomatic combinations (IDM). Section 2
elaborates on the linguistic properties related to the
differences in the degree of semantic idiosyncrasy
observed in members of the above four classes. In
Section 3, we propose statistical measures for quan-
tifying each of these properties, and use them as fea-
tures for type classification of verb+noun combina-
tions. Section 4 and Section 5 present an evaluation
of our proposed measures. Section 6 discusses the
related studies, and Section 7 concludes the paper.
2 Semantic Idiosyncrasy: Linguistic
Properties
Linguists and lexicographers often attribute certain
characteristics to semantically idiosyncratic expres-
sions. Some of the widely-known properties are in-
stitutionalization, lexicosyntactic fixedness, and non-
compositionality (Cowie, 1981; Gibbs and Nayak,
1989; Moon, 1998). The following paragraphs elab-
orate on each property, as well as on its relevance to
the identification of the classes under study.
Institutionalization is the process through which a
combination of words becomes recognized and ac-
cepted as a semantic unit involving some degree of
semantic idiosyncrasy. IDMs, LVCs, and ABS com-
binations are institutionalized to some extent.
Lexicosyntactic fixedness refers to some degree of
lexical and syntactic restrictiveness in a semantically
idiosyncratic expression. An expression is lexically
fixed if the substitution of a semantically similar
word for any of its constituents does not preserve its
original meaning (e.g., compare spill the beans and
spread the beans). In contrast to LIT and ABS com-
binations, IDMs and LVCs are expected to exhibit
lexical fixedness to some extent.
An expression is syntactically fixed if it cannot un-
dergo syntactic variations and at the same time retain
its original semantic interpretation. IDMs and LVCs
are known to show strong preferences for the syn-
tactic patterns they appear in (Cacciari and Tabossi,
1993; Brinton and Akimoto, 1999). E.g., compare
10
Joe gave a groan with ?A groan was given by Joe,
and Tim kicked the bucket with *Tim kicked the buck-
ets (in the idiom reading). Nonetheless, the type and
degree of syntactic fixedness in LVCs and IDMs are
different. For example, most LVCs prefer the pattern
in which the noun is introduced by the indefinite arti-
cle a (as in give a try and make a decision), whereas
this is not the case with IDMs (e.g., shoot the breeze
and kick the bucket). IDMs and LVCs may also ex-
hibit preferences with respect to adjectival modifica-
tion of their noun constituent. LVCs are expected to
appear both with and without an adjectival modifier,
as in give a (loud) groan and make a (wise) decision.
IDMs, on the other hand, mostly appear either with
an adjective, as in keep an open mind (cf. ?keep a
mind), or without, as in shoot the breeze (cf. ?shoot
the fun breeze).
Non-compositionality refers to the situation where
the meaning of a word combination deviates from
the meaning emerging from a word-by-word inter-
pretation of it. IDMs are largely non-compositional,
whereas LVCs are semi-compositional since their
meaning can be mainly predicted from the noun con-
stituent. ABS and LIT combinations are expected to
be largely compositional.
None of the above-mentioned properties are suffi-
cient criteria by themselves for determining which
semantic class a given verb+noun combination be-
longs to. Moreover, semantic properties of the con-
stituents of a combination are also known to be rele-
vant for determining its class (Uchiyama et al, 2005).
Verbs may exhibit strong preferences for appearing
in MWEs from a particular class, e.g., give, take and
make commonly form LVCs. The semantic category
of the noun is also relevant to the type of MWE, e.g.,
the noun constituent of an LVC is often a predicative
one. We hypothesize that if we look at evidence from
all these different sources, we will find members of
the same class to be reasonably similar, and members
of different classes to be notably different.
3 Statistical Measures of Semantic
Idiosyncrasy
This section introduces measures for quantifying the
properties of idiosyncratic MWEs, mentioned in the
previous section. The measures will be used as fea-
tures in a classification task (see Sections 4?5).
3.1 Measuring Institutionalization
Corpus-based approaches often assess the degree of
institutionalization of an expression by the frequency
with which it occurs. Raw frequencies drawn from
a corpus are not reliable on their own, hence asso-
ciation measures such as pointwise mutual informa-
tion (PMI) are also used in many NLP applications
(Church et al, 1991). PMI of a verb+noun combina-
tion ?v , n? is defined as:
PMI (v , n) .= log P (v , n)P (v)P (n)
? log f (?, ?)f (v , n)f (v , ?) f (?, n) (1)
where all frequency counts are calculated over
verb?object pairs in a corpus. We use both frequency
and PMI of a verb+noun combination to measure its
degree of institutionalization. We refer to this group
of measures as INST.
3.2 Measuring Fixedness
To measure fixedness, we use statistical measures of
lexical, syntactic, and overall fixedness that we have
developed in a previous study (Fazly and Stevenson,
2006), as well as some new measures we introduce
here. The following paragraphs give a brief descrip-
tion of each.
Fixednesslex quantifies the degree of lexical fixed-
ness of the target combination, ?v ,n?, by compar-
ing its strength of association (measured by PMI)
with those of its lexical variants. Like Lin (1999),
we generate lexical variants of the target automati-
cally by replacing either the verb or the noun con-
stituent by a semantically similar word from the
automatically-built thesaurus of Lin (1998). We then
use a standard statistic, the z -score, to calculate
Fixednesslex:
Fixednesslex(v , n) .=
PMI(v , n) ? PMI
std (2)
where PMI is the mean and std the standard devia-
tion over the PMI of the target and all its variants.
Fixednesssyn quantifies the degree of syntactic
fixedness of the target combination, by comparing
its behaviour in text with the behaviour of a typical
verb?object, both defined as probability distributions
over a predefined set of patterns. We use a stan-
dard information-theoretic measure, relative entropy,
11
v det:NULL nsg v det:NULL npl
v det:a/an nsg
v det:the nsg v det:the npl
v det:DEM nsg v det:DEM npl
v det:POSS nsg v det:POSS npl
v det:OTHER nsg,pl det:ANY nsg,pl be vpassive
Table 2: Patterns for syntactic fixedness measure.
to calculate the divergence between the two distribu-
tions as follows:
Fixednesssyn (v , n)
.= D(P(pt |v ,n) ||P(pt))
=
?
ptk?P
P(ptk | v , n) log
P(ptk | v , n)
P(ptk )
(3)
where P is the set of patterns (shown in Table 2)
known to be relevant to syntactic fixedness in LVCs
and IDMs. P(pt | v , n) represents the syntactic be-
haviour of the target, and P(pt) represents the typical
syntactic behaviour over all verb?object pairs.
Fixednesssyn does not show which syntactic pat-
tern the target prefers the most. We thus use an addi-
tional measure, Patterndom, to determine the domi-
nant pattern for the target:
Patterndom(v , n) .= argmax
ptk?P
f (v , n, ptk ) (4)
In addition to the individual measures of fixedness,
we use Fixednessoverall, which quantifies the degree
of overall fixedness of the target:
Fixednessoverall (v , n)
.= ? Fixednesssyn (v , n)
+ (1 ? ?) Fixednesslex (v , n) (5)
where ? weights the relative contribution of lexi-
cal and syntactic fixedness in predicting semantic id-
iosyncrasy.
Fixednessadj quantifies the degree of fixedness
of the target combination with respect to adjectival
modification of the noun constituent. It is similar to
the syntactic fixedness measure, except here there are
only two patterns that mark the presence or absence
of an adjectival modifier preceding the noun:
Fixednessadj(v , n) .= D(P(ai |v ,n) ||P(ai )) (6)
where ai ? {present, absent}. Fixednessadj does
not determine which pattern of modification the tar-
get combination prefers most. We thus add another
measure?the odds of modification?to capture this:
Oddsadj(v , n) .=
P(ai = present|v ,n)
P(ai = absent|v ,n)
(7)
Overall, we use six measures related to fixedness;
we refer to the group as FIXD.
3.3 Measuring Compositionality
Compositionality of an expression is often approxi-
mated by comparing the ?context? of the expression
with the contexts of its constituents. We measure
the degree of compositionality of a target verb+noun
combination, t =?v ,n?, in a similar fashion.
We take the context of the target (t) and each of its
constituents (v and n) to be a vector of the frequency
of nouns cooccurring with it within a window of ?5
words. We then measure the ?similarity? between the
target and each of its constituents, Simdist (t , v) and
Simdist (t , n), using the cosine measure.1
Recall that an LVC can be roughly paraphrased by
a verb that is morphologically related to its noun con-
stituent, e.g., to make a decision nearly means to de-
cide. For each target t , we thus add a third measure,
Simdist (t , rv), where rv is a verb morphologically
related to the noun constituent of t , and is automati-
cally extracted from WordNet (Fellbaum, 1998).2
We use abbreviation COMP to refer to the group of
measures related to compositionality.
3.4 The Constituents
Recall that semantic properties of the constituents of
a verb+noun combination are expected to be relevant
to its semantic class. We thus add two simple fea-
ture groups: (i) the verb itself (VERB); and (ii) the
semantic category of the noun according to WordNet
(NSEM). We take the semantic category of a noun to
be the ancestor of its first sense in the hypernym hier-
archy of WordNet 2.1, cut at the level of the children
1Our preliminary experiments on development data from Fa-
zly and Stevenson (2006) revealed that the cosine measure and a
window size of ?5 words resulted in the best performance.
2If no such verb exists, Simdist (t , rv) is set to zero. If more
than one verb exist, we choose the one that is identical to the
noun or the one that is shorter in length.
12
of ENTITY (which will include PHYSICAL ENTITY
and ABSTRACT ENTITY).3
4 Experimental Setup
4.1 Corpus and Experimental Expressions
We use the British National Corpus (BNC),4 auto-
matically parsed using the Collins parser (Collins,
1999), and further processed with TGrep2.5 We
select our potential experimental expressions from
pairs of verb and direct object that have a minimum
frequency of 25 in the BNC and that involve one
of a predefined list of basic (transitive) verbs. Ba-
sic verbs, which in their literal uses refer to states or
acts central to human experience (e.g., give and put),
commonly form MWEs in combination with their di-
rect object argument (Cowie et al, 1983). We use 12
such verbs ranked highly according to the number of
different nouns they appear with in the BNC. Here
are the verbs in alphabetical order:
bring, find, get, give, hold, keep, lose, make, put, see, set, take
To guarantee that the final set of expressions con-
tains pairs from all four classes, we pseudo-randomly
select them from the initial list of pairs extracted from
the BNC as explained above. To ensure the inclusion
of IDMs, we consult two idioms dictionaries (Cowie
et al, 1983; Seaton and Macaulay, 2002). To en-
sure we include LVCs, we select pairs in which the
noun has a morphologically related verb according
to WordNet. We also select pairs whose noun is not
morphologically related to any verb to ensure the in-
clusion of LIT combinations.
This selection process resulted in 632 pairs, re-
duced to 563 after annotation (see Section 4.2 for
details on annotation). Out of these, 148 are LIT,
196 are ABS, 102 are LVC, and 117 are IDM. We
randomly choose 102 pairs from each class as our
final experimental expressions. We then pseudo-
randomly divide these into training (TRAIN), devel-
opment (DEV), and test (TEST) data sets, so that each
set has an equal number of pairs from each class. In
addition, we ensure that pairs with the same verb that
belong to the same class are divided equally among
the three sets. Our final TRAIN, DEV, and TEST sets
3Experiments on development data show that looking at all
senses of a noun degrades performance.
4http://www.natcorp.ox.ac.uk.
5http://tedlab.mit.edu/?dr/Tgrep2.
contain 240, 84, and 84 pairs, respectively.
4.2 Human Judgments
We asked four native speakers of English with suf-
ficient linguistic background to annotate our exper-
imental expressions. The annotation task was ex-
pected to be time-consuming, hence it was not feasi-
ble for all the judges to annotate all the expressions.
Instead, we asked one judge to be our primary anno-
tator, PA henceforth. (PA is an author of this paper,
but the other three judges are not.)
First, PA annotated all the 632 expressions selected
as described in Section 4.1, and removed 69 of them
that could be potential sources of disagreement for
various reasons (e.g., if an expression was unfamil-
iar or was likely to be part of a larger phrase). Next,
we divided the remaining 563 pairs into three equal-
sized sets, and gave each set to one of the other
judges to annotate. The judges were given a com-
prehensive guide for the task, in which the classes
were defined solely in terms of their semantic prop-
erties. Since expressions were annotated out of con-
text (type-based), we asked the judges to annotate the
predominant meaning of each expression.
We use the annotations of PA as our gold standard
for evaluation, but use the annotations of the others
to measure inter-annotator agreement. The observed
agreement (po) between PA and each of the other
three annotators are 79.8%, 72.2%, and 67%, respec-
tively. The kappa (?) scores are .72, .62, and .56.
The reasonably high agreement scores confirm that
the classes are coherent and linguistically plausible.
4.3 Classification Strategy and Features
We use the decision tree induction system C5.0 as
our machine learning software, and the measures pro-
posed in Section 3 as features in our classification ex-
periments.6 We explore the relevance of each feature
group in the overall classification, as well as in iden-
tifying members of each individual class.
5 Experimental Results
We performed experiments on DEV to find features
most relevant for classification. These experiments
6Experiments on DEV using a Support Vector Machine algo-
rithm produced poorer results; we thus do not report them.
13
revealed that removing Simdist (t , v) resulted in bet-
ter performance. This is not surprising given that ba-
sic verbs are highly polysemous, and hence the distri-
butional context of a basic verb may not correspond
to any particular sense of it. We thus remove this
feature (from COMP) in experiments on TEST. Re-
sults presented here are on the TEST set; those on the
DEV set have similar trends. Here, we first look at the
overall performance of classification in Section 5.1.
Section 5.2 presents the results of classification for
the individual classes.
5.1 Overall Classification Performance
Table 3 presents the results of classification?in
terms of average accuracy (%Acc) and relative er-
ror reduction (%RER)?for the individual feature
groups, as well as for all groups combined. The base-
line (chance) accuracy is 25% since we have four
equal-sized classes in TEST. As can be seen, INST
features yield the lowest overall accuracy, around
36%, with a relative error reduction of only 14%
over the baseline. This shows that institutionaliza-
tion, although relevant, is not sufficient for distin-
guishing among different levels of semantic idiosyn-
crasy. Interestingly, FIXD features achieve the high-
est accuracy, 50%, with a relative error reduction of
33%, showing that fixedness is a salient aspect of se-
mantic idiosyncrasy. COMP features achieve reason-
ably good accuracy, around 40%, though still notably
lower than the accuracy of FIXD features. This is es-
pecially interesting since much previous research has
focused solely on the non-compositionality of MWEs
to identify them (McCarthy et al, 2003; Baldwin et
al., 2003; Bannard et al, 2003). Our results confirm
the relevance of this property, while at the same time
revealing its insufficiency. Interestingly, features re-
lated to the semantic properties of the constituents,
VERB and NSEM, overall perform comparably to the
compositionality features. However, a closer look at
their performance on the individual classes (see Sec-
tion 5.2) reveals that, unlike COMP, they are mainly
good at identifying items from certain classes. As
hypothesized, we achieve the highest performance,
an accuracy of 58% and a relative error reduction of
44%, when we combine all features.
Table 4 displays classification performance, when
we use all the feature groups except one. These re-
sults are more or less consistent with those in Ta-
Only the features in group %Acc (%RER)
INST 35.7 (14.3)
FIXD 50 (33.3)
COMP 40.5 (20.7)
VERB 42.9 (23.9)
NSEM 39.3 (19.1)
ALL 58.3 (44.4)
Table 3: Accuracy (%Acc) and relative error reduction
(%RER) over TEST pairs, for the individual feature groups, and
for all features combined.
All features except those in group %Acc (%RER)
INST 53.6 (38.1)
FIXD 47.6 (30.1)
COMP 56 (41.3)
VERB 48.8 (31.7)
NSEM 46.4 (28.5)
ALL 58.3 (44.4)
Table 4: Accuracy (%Acc) and relative error reduction
(%RER) over TEST pairs, removing one feature group at a time.
ble 3 above, except some differences which we dis-
cuss below. Removing FIXD features results in a
drastic decrease in performance (10.7%), while the
removal of INST and COMP features cause much
smaller drops in performance (4.7% and 2.3%, re-
spectively). Here again, we can see that features re-
lated to the semantics of the verb and the noun are
salient features. Removing either of these results
in a substantial decrease in performance?9.5% and
11.9%, respectively?which is comparable to the de-
crease resulting from removing FIXD features. This
is an interesting observation, since VERB and NSEM
features, on their own, do not perform nearly as well
as FIXD features. It is thus necessary to futher in-
vestigate the performance of these groups on larger
data sets with more variability in the verb and noun
constituents of the expressions.
5.2 Performance on Individual Classes
We now look at the performance of the feature
groups, both separately and combined, on the indi-
vidual classes. For each combination of class and
feature group, the F -measures of classification are
given in Table 5, with the two highest F -measures
for each class shown in boldface.7 These results
show that the combination of all feature groups yields
the best or the second-best performance on all four
classes. (In fact, in only one case is the performance
7Our F -measure gives equal weights to precision and recall.
14
Only the features in group
Class INST FIXD COMP VERB NSEM ALL
LIT .48 .42 .51 .54 .57 .60
ABS .40 .32 .17 .27 .49 .46
LVC .21 .58 .47 .55 - .68
IDM .33 .67 .42 0 - .56
Table 5: F -measures on TEST pairs, for individual feature
groups and all features combined.
ANNOTATOR1 ANNOTATOR2 ANNOTATOR3
Class %po ? %po ? %po ?
LIT 93.6 .83 88.3 .67 91.4 .78
ABS 83 .63 76.6 .46 78 .52
LVC 91 .71 83 .54 87.7 .61
IDM 92 .73 87.2 .63 87.2 .59
Table 6: Per-class observed agreement and kappa score be-
tween PA and each of the three annotators.
of ALL features notably smaller than the best perfor-
mance achieved by a single feature group.)
Looking at the performance of ALL features, we
can see that we get reasonably high F -measure for
all classes, except for ABS. The relatively low values
of po and ? on this class, as shown in Table 6, suggest
that this class was also the hardest to annotate. It is
possible that members of this class share properties
with other classes. The extremely poor performance
of the COMP features on ABS also reflects that per-
haps members of this class are not coherent in terms
of their degree of compositionality (e.g, compare give
confidence and make a living). In the future, we need
to incorporate more coherent membership criteria for
this class into our annotation procedure.
According to Table 5, the most relevant feature
group for identifying members of the LIT and ABS
classes is NSEM. This is expected since NSEM is a bi-
nary feature determining whether the noun is a PHYS-
ICAL ENTITY or an ABSTRACT ENTITY.8 Among
other feature groups, INST features also perform rea-
sonably well on both these classes. The most relevant
feature group for LVC and IDM is FIXD. (Note that
for IDM, the performance of this group is notably
higher than ALL). On the other hand, INST features
have a very poor performance on these classes, rein-
forcing that IDMs and LVCs may not necessarily ap-
pear with significantly high frequency of occurrence
in a given corpus. Fixedness features thus prove to be
8Since this is a binary feature, it can only distinguish two
classes. In the future, we need to include more semantic classes.
particularly important for the identification of highly
idiosyncratic MWEs, such as LVCs and IDMs.
6 Related Work
Much recent work on classifying MWEs focuses on
determining different levels of compositionality in
verb+particle combinations using a measure of distri-
butional similarity (McCarthy et al, 2003; Baldwin
et al, 2003; Bannard et al, 2003). Another group of
research attempts to classify a particular MWE sub-
type, such as verb-particle constructions (VPCs) or
LVCs, according to some fine-grained semantic crite-
ria (Wanner, 2004; Uchiyama et al, 2005; Cook and
Stevenson, 2006). Here, we distinguish subtypes of
MWEs that are defined according to coarse-grained
distinctions in their degree of semantic idiosyncrasy.
Wermter and Hahn (2004) recognize the impor-
tance of distinguishing MWE subtypes that are sim-
ilar to our four classes, but only focus on separat-
ing MWEs as one single class from literal combina-
tions. For this, they use a measure that draws on the
limited modifiability of MWEs, in addition to their
expected high frequency. Krenn and Evert (2001)
attempt to separate German idioms, LVCs, and lit-
eral phrases (of the form verb+prepositional phrase).
They treat LVCs and idioms as institutionalized ex-
pressions, and use frequency and several association
measures, such as PMI, for the task. The main goal
of their work is to find which association measures
are particularly suited for identifying which of these
classes. Here, we look at properties of MWEs other
than their institutionalization (the latter we quantify
using an association measure).
The work most similar to ours is that of Venkata-
pathy and Joshi (2005). They propose a minimally-
supervised classification schema that incorporates a
variety of features to group verb+noun combinations
according to their level of compositionality. Their
work has the advantage of requiring only a small
amount of manually-labeled training data. However,
their classes are defined on the basis of composition-
ality only. Here, we consider classes that are linguis-
tically salient, and moreover need special treatment
within a computational system. Our work is also dif-
ferent in that it brings in a new group of features, the
fixedness measures, which prove to be very effective
in identifying particular classes of MWEs.
15
7 Conclusions
We have provided an analysis of the important char-
acteristics pertaining to the semantic idiosyncrasy of
MWEs. We have also elaborated on the relation-
ship between these properties and four linguistically-
motivated classes of verb+noun combinations, falling
on a continuum from less to more semantically id-
iosyncratic. On the basis of such analysis, we
have developed statistical, corpus-based measures
that quantify each of these properties. Our results
confirm that these measures are effective in type clas-
sification of the MWEs under study. Our class-
based results look into the interaction between the
measures (each capturing a property of MWEs) and
the classes (which are defined in terms of seman-
tic idiosyncrasy). Based on this, we can see which
measures?or properties they relate to?are most or
least relevant for identifying each particular class of
verb+noun combinations. We are currently expand-
ing this work to investigate the use of similar mea-
sures in token classification of verb+noun combina-
tions in context.
Acknowledgements
We thank Eric Joanis for providing us with NP-head extraction
software. We thank Saif Mohammad for the CooccurrenceMa-
trix and the DistributionalDistance packages.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of mul-
tiword expression decomposability. In Proc. of ACL-
SIGLEX Wkshp. on Multiword Expressions, 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of ACL-SIGLEX Wkshp. on Multi-
word Expressions, 65?72.
Laurel J. Brinton and Minoji Akimoto, eds. 1999. Col-
locational and Idiomatic Aspects of Composite Predi-
cates in the History of English. John Benjamins.
Miriam Butt. 2003. The light verb jungle. Workshop on
Multi-Verb Constructions.
Cristina Cacciari and Patrizia Tabossi, eds. 1993. Idioms:
Processing, Structure, and Interpretation. Lawrence
Erlbaum Associates, Publishers.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analysis.
In Uri Zernik, editor, Lexical Acquisition: Exploiting
On-Line Resources to Build a Lexicon, 115?164.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, UPenn.
Paul Cook and Suzanne Stevenson. 2006. Classifying
particle semantics in English verb-particle construc-
tions. In Proc. of COLING-ACL?06 Wkshp. on Multi-
word Expressions, 45?53.
Anthony P. Cowie, Ronald Mackin, and Isabel R. McCaig.
1983. Oxford Dictionary of Current Idiomatic English,
volume 2. OUP.
Anthony P. Cowie. 1981. The treatment of collocations
and idioms in learner?s dictionaries. Applied Linguis-
tics, II(3):223?235.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proc. of EACL?06, 337?344.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press.
Raymond W., Jr. Gibbs and Nandini P. Nayak. 1989. Psy-
chololinguistic studies on the syntactic behaviour of id-
ioms. Cognitive Psychology, 21:100?138.
Brigitte Krenn and Stefan Evert. 2001. Can we do bet-
ter than frequency? A case study on extracting PP-verb
collocations. In Proc. of ACL?01 Wkshp. on Colloca-
tions, 39?46.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL?98, 768?774.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of ACL?99, 317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proc. of ACL-SIGLEX Wkshp. on Multiword
Expressions, 73?80.
Tadao Miyamoto. 2000. The Light Verb Construction
in Japanese: the Role of the Verbal Noun. John Ben-
jamins.
Rosamund Moon. 1998. Fixed Expressions and Idioms in
English: A Corpus-Based Approach. Oxford Univer-
sity Press.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70(3):491?538.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proc. of CI-
CLing?02, 1?15.
Maggie Seaton and Alison Macaulay, eds. 2002. Collins
COBUILD Idioms Dictionary. HarperCollins.
Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.
2005. Disambiguating Japanese compound verbs.
Computer Speech and Language, 19:497?512.
Sriram Venkatapathy and Aravind Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N)
collocations by integrating features. In Proc. of HLT-
EMNLP?05, 899?906.
Leo Wanner. 2004. Towards automatic fine-grained se-
mantic classification of verb-noun collocations. Natu-
ral Language Engineering, 10(2):95?143.
Joachim Wermter and Udo Hahn. 2004. Collocation ex-
traction based on modifiability statistics. In Proc. of
COLING?04, 980?986.
16
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Pulling their Weight: Exploiting Syntactic Forms for the Automatic
Identification of Idiomatic Expressions in Context
Paul Cook and Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
fpcook,afsaneh,suzanneg@cs.toronto.edu
Abstract
Much work on idioms has focused on type
identification, i.e., determining whether a se-
quence of words can form an idiomatic ex-
pression. Since an idiom type often has a
literal interpretation as well, token classifi-
cation of potential idioms in context is criti-
cal for NLP. We explore the use of informa-
tive prior knowledge about the overall syn-
tactic behaviour of a potentially-idiomatic
expression (type-based knowledge) to de-
termine whether an instance of the expres-
sion is used idiomatically or literally (token-
based knowledge). We develop unsuper-
vised methods for the task, and show that
their performance is comparable to that of
state-of-the-art supervised techniques.
1 Introduction
Identification of multiword expressions (MWEs),
such as car park, make a decision, and kick the
bucket, is extremely important for accurate natural
language processing (NLP) (Sag et al, 2002). Most
MWEs need to be treated as single units of mean-
ing, e.g., make a decision roughly means ?decide?.
Nonetheless, the components of an MWE can be
separated, making it hard for an NLP system to iden-
tify the expression as a whole. Many researchers
have recently developed methods for the automatic
acquisition of various properties of MWEs from cor-
pora (Lin, 1999; Krenn and Evert, 2001; Baldwin et
al., 2003; McCarthy et al, 2003; Venkatapathy and
Joshi, 2005; Villada Moiro?n and Tiedemann, 2006;
Fazly and Stevenson, 2006). These studies look
into properties, such as the collocational behaviour
of MWEs, their semantic non-compositionality, and
their lexicosyntactic fixedness, in order to distin-
guish them from similar-on-the-surface literal com-
binations.
Most of these methods have been aimed at rec-
ognizing MWE types; less attention has been paid
to the identification of instances (tokens) of MWEs
in context. For example, most such techniques (if
successful) would identify make a face as a poten-
tial MWE. This expression is, however, ambiguous
between an idiom, as in The little girl made a funny
face at her mother, and a literal combination, as in
She made a face on the snowman using a carrot and
two buttons. Despite the common perception that
phrases that can be idioms are mainly used in their
idiomatic sense, our analysis of 60 idioms has shown
otherwise. We found that close to half of these id-
ioms also have a clear literal meaning; and of the ex-
pressions with a literal meaning, on average around
40% of their usages are literal. Distinguishing token
phrases as MWEs or literal combinations of words is
thus essential for NLP applications that require the
identification of multiword semantic units, such as
semantic parsing and machine translation.
Recent studies addressing MWE token classifi-
cation mainly perform the task as one of word
sense disambiguation, and draw on the local con-
text of an expression to disambiguate it. Such
techniques either do not use any information re-
garding the linguistic properties of MWEs (Birke
and Sarkar, 2006), or mainly focus on their non-
compositionality (Katz and Giesbrecht, 2006). Pre-
41
vious work on the identification of MWE types,
however, has found other properties of MWEs, such
as their syntactic fixedness, to be relevant to their
identification (Evert et al, 2004; Fazly and Steven-
son, 2006). In this paper, we propose techniques that
draw on this property to classify individual tokens of
a potentially idiomatic phrase as literal or idiomatic.
We also put forward classification techniques that
combine such information with evidence from the
local context of an MWE.
We explore the hypothesis that informative prior
knowledge about the overall syntactic behaviour of
an idiomatic expression (type-based knowledge) can
be used to determine whether an instance of the
expression is used literally or idiomatically (token-
based knowledge). Based on this hypothesis, we de-
velop unsupervised methods for token classification,
and show that their performance is comparable to
that of a standard supervised method.
Many verbs can be combined with one or more of
their arguments to form MWEs (Cowie et al, 1983;
Fellbaum, 2002). Here, we focus on a broadly doc-
umented class of idiomatic MWEs that are formed
from the combination of a verb with a noun in its di-
rect object position, as in make a face. In the rest
of the paper, we refer to these verb+noun combi-
nations, which are potentially idiomatic, as VNCs.
In Section 2, we propose unsupervised methods that
classify a VNC token as an idiomatic or literal usage.
Section 3 describes our experimental setup, includ-
ing experimental expressions and their annotation.
In Section 4, we present a detailed discussion of our
results. Section 5 compares our work with similar
previous studies, and Section 6 concludes the paper.
2 Unsupervised Idiom Identification
We first explain an important linguistic property at-
tributed to idioms?that is, their syntactic fixedness
(Section 2.1). We then propose unsupervised meth-
ods that draw on this property to automatically dis-
tinguish between idiomatic and literal usages of an
expression (Section 2.2).
2.1 Syntactic Fixedness and Canonical Forms
Idioms tend to be somewhat fixed with respect to
the syntactic configurations in which they occur
(Nunberg et al, 1994). For example, pull one?s
weight tends to mainly appear in this form when
used idiomatically. Other forms of the expression,
such as pull the weights, typically are only used
with a literal meaning. In their work on automati-
cally identifying idiom types, Fazly and Stevenson
(2006)?henceforth FS06?show that an idiomatic
VNC tends to have one (or at most a small number
of) canonical form(s), which are its most preferred
syntactic patterns. The preferred patterns can vary
across different idiom types, and can involve a num-
ber of syntactic properties: the voice of the verb (ac-
tive or passive), the determiner introducing the noun
(the, one?s, etc.), and the number of the noun (singu-
lar or plural). For example, while pull one?s weight
has only one canonical form, hold fire and hold one?s
fire are two canonical forms of the same idiom, as
listed in an idiom dictionary (Seaton and Macaulay,
2002).
In our work, we assume that in most cases, id-
iomatic usages of an expression tend to occur in a
small number of canonical form(s) for that idiom.
We also assume that, in contrast, the literal usages
of an expression are less syntactically restricted, and
are expressed in a greater variety of patterns. Be-
cause of their relative unrestrictiveness, literal us-
ages may occur in a canonical idiomatic form for
that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative
syntactic patterns for the expression, which we refer
to as the non-canonical forms of the idiom, are more
likely to be literal. Drawing on these assumptions,
we develop three unsupervised methods that deter-
mine, for each VNC token in context, whether it has
an idiomatic or a literal interpretation.
2.2 Statistical Methods
The following paragraphs elaborate on our proposed
methods for identifying the idiomatic and literal us-
ages of a VNC: the CForm method that uses knowl-
edge of canonical forms only, and two Diff methods
that draw on further contextual evidence as well. All
three methods draw on our assumptions described
above, that usages in the canonical form for an id-
iom are more likely to be idiomatic, and those in
other forms are more likely to be literal. Thus, for
all three methods, we need access to the canonical
form of the idiom. Since we want our token iden-
tification methods to be unsupervised, we adopt the
42
unsupervised statistical method of FS06 for finding
canonical forms for an idiomatic VNC. This method
determines the canonical forms of an expression to
be those forms whose frequency is much higher than
the average frequency of all its forms.
CForm: The underlying assumption of this
method is that information about the canonical
form(s) of an idiom type is extremely informative
in classifying the meaning of its individual instances
(tokens) as literal or idiomatic. Our CForm classi-
fies a token as idiomatic if it occurs in the automat-
ically determined canonical form(s) for that expres-
sion, and as literal otherwise.
Di : Our two Di methods combine local con-
text information with knowledge about the canon-
ical forms of an idiom type to determine if its to-
ken usages are literal or idiomatic. In developing
these methods, we adopt a distributional approach
to meaning, where the meaning of an expression is
approximated by the words with which it co-occurs
(Firth, 1957). Although there may be fine-grained
differences in meaning across the idiomatic usages
of an expression, as well as across its literal usages,
we assume that the idiomatic and literal usages cor-
respond to two coarse-grained senses of the expres-
sion. Since we further assume these two groups
of usages will have more in common semantically
within each group than between the two groups, we
expect that literal and idiomatic usages of an ex-
pression will typically occur with different sets of
words. We will refer then to each of the literal and
idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that
each may have multiple fine-grained senses. Clearly,
the success of our method depends on the extent to
which these assumptions hold.
We estimate the meaning of a set of usages of an
expression e as a word frequency vector ~v
e
where
each dimension i of ~v
e
is the frequency with which
e co-occurs with word i across the usages of e. We
similarly estimate the meaning of a single token of
an expression t as a vector ~v
t
capturing that usage.
To determine if an instance of an expression is literal
or idiomatic, we compare its co-occurrence vector to
the co-occurrence vectors representing each of the
literal and idiomatic meanings of the expression. We
use a standard measure of distributional similarity,
cosine, to compare co-occurrence vectors.
In supervised approaches, such as that of Katz and
Giesbrecht (2006), co-occurrence vectors for literal
and idiomatic meanings are formed from manually-
annotated training data. Here, we propose unsuper-
vised methods for estimating these vectors. We use
one way of estimating the idiomatic meaning of an
expression, and two ways for estimating its literal
meaning, yielding two methods for token classifica-
tion.
Our first Diff method draws further on our expec-
tation that canonical forms are more likely idiomatic
usages, and non-canonical forms are more likely lit-
eral usages. We estimate the idiomatic meaning of
an expression by building a co-occurrence vector,
~v
I -CF
, for all uses of the expression in its auto-
matically determined canonical form(s). Since we
hypothesize that idiomatic usages of an expression
tend to occur in its canonical form, we expect these
co-occurrence vectors to be largely representative of
the idiomatic usage of the expression. We similarly
estimate the literal meaning by constructing a co-
occurrence vector, ~v
L-NCF
, of all uses of the expres-
sion in its non-canonical forms. We use the term
Di
I-CF;L-NCF
to refer to this method.
Our second Diff method also uses the vector
~v
I -CF
to estimate the idiomatic meaning of an ex-
pression. However, this approach follows that of
Katz and Giesbrecht (2006) in assuming that literal
meanings are compositional. The literal meaning of
an expression is thus estimated by composing (sum-
ming and then normalizing) the co-occurrence vec-
tors for its component words. The resulting vec-
tor is referred to as ~v
L-Comp
, and this method as
Di
I-CF;L-Comp
.
For both Diff methods, if the meaning of
an instance of an expression is determined to
be more similar to its idiomatic meaning (e.g.,
cosine (~v
t
; ~v
I-CF
) > cosine (~v
t
; ~v
L-NCF
)), then
we label it as an idiomatic usage. Otherwise, it is
labeled as literal.1
1We also performed experiments using a KNN classifier
in which the co-occurrence vector for a token was compared
against the co-occurrence vectors for the canonical and non-
canonical forms of that expression, which were assumed to
be idiomatic and literal usages respectively. However, perfor-
mance was generally worse using this method.
43
Note that all three of our proposed techniques for
token identification depend on how accurately the
canonical forms of an expression can be acquired.
FS06?s canonical form acquisition technique, which
we use here, works well if the idiomatic usage of
a VNC is sufficiently frequent compared to its lit-
eral usage. In our experiments, we examine the
performance of our proposed classification methods
for VNCs with different proportions of idiomatic-to-
literal usages.
3 Experimental Setup
3.1 Experimental Expressions and Annotation
We use data provided by FS06, which consists of a
list of VNCs and their canonical forms. From this
data, we discarded expressions whose frequency in
the British National Corpus2 (BNC) is lower than
20, in an effort to make sure that there would be lit-
eral and idiomatic usages of each expression. The
frequency cut-off further ensures an accurate esti-
mate of the vectors representing each of the lit-
eral and idiomatic meanings of the expression. We
also discarded expressions that were not found in at
least one of two dictionaries of idioms (Seaton and
Macaulay, 2002; Cowie et al, 1983). This process
resulted in the selection of 60 candidate expressions.
For each of these 60 expressions, 100 sentences
containing its usage were randomly selected from
the automatically parsed BNC (Collins, 1999), using
the automatic VNC identification method described
by FS06. For an expression which occurs less than
100 times in the BNC, all of its usages were ex-
tracted. Our primary judge, a native English speaker
and an author of this paper, then annotated each use
of each candidate expression as one of literal, id-
iomatic, or unknown. When annotating a token, the
judge had access to only the sentence in which it oc-
curred, and not the surrounding sentences. If this
context was insufficient to determine the class of the
expression, the judge assigned the unknown label.
Idiomaticity is not a binary property, rather it is
known to fall on a continuum from completely se-
mantically transparent, or literal, to entirely opaque,
or idiomatic. The human annotators were required
to pick the label, literal or idiomatic, that best fit the
2http://www.natcorp.ox.ac.uk
usage in their judgment; they were not to use the un-
known label for intermediate cases. Figurative ex-
tensions of literal meanings were classified as literal
if their overall meaning was judged to be fairly trans-
parent, as in You turn right when we hit the road at
the end of this track (taken from the BNC). Some-
times an idiomatic usage, such as had words in I
was in a bad mood, and he kept pestering me, so
we had words, is somewhat directly related to its
literal meaning, which is not the case for more se-
mantically opaque idioms such as hit the roof. The
above sentence was classified as idiomatic since the
idiomatic meaning is much more salient than the lit-
eral meaning.
Based on the primary judge?s annotations, we re-
moved expressions with fewer than 5 instances of
either of their literal or idiomatic meanings, leav-
ing 28 expressions. The remaining expressions were
then split into development (DEV) and test (TEST)
sets of 14 expressions each. The data was divided
such that DEV and TEST would be approximately
equal with respect to the frequency, and proportion
of idiomatic-to-literal usages, of their expressions.
Before consensus annotation, DEV and TEST con-
tained a total of 813 and 743 tokens, respectively.
A second human judge, also a native English-
speaking author of this paper, then annotated DEV
and TEST. The observed agreement and unweighted
kappa score on TEST were 76% and 0:62 respec-
tively. The judges discussed tokens on which they
disagreed to achieve a consensus annotation. Final
annotations were generated by removing tokens that
received the unknown label as the consensus anno-
tation, leaving DEV and TEST with a total of 573 and
607 tokens, and an average of 41 and 43 tokens per
expression, respectively.
3.2 Creation of Co-occurrence Vectors
We create co-occurrence vectors for each expression
in our study from counts in the BNC. We form co-
occurrence vectors for the following items.
 Each token instance of the target expression
 The target expression in its automatically deter-
mined canonical form(s)
 The target expression in its non-canonical
form(s)
44
 The verb in the target expression
 The noun in the target expression
The co-occurrence vectors measure the frequency
with which the above items co-occur with each of
1000 content bearing words in the same sentence.3
The content bearing words were chosen to be the
most frequent words in the BNC which are used as
a noun, verb, adjective, adverb, or determiner. Al-
though determiners are often in a typical stoplist, we
felt it would be beneficial to use them here. Deter-
miners have been shown to be very informative in
recognizing the idiomaticity of MWE types, as they
are incorporated in the patterns used to automati-
cally determine canonical forms (Fazly and Steven-
son, 2006).4
3.3 Evaluation and Baseline
Our baseline for comparison is that of always pre-
dicting an idiomatic label, the most frequent class
in our development data. We also compare our un-
supervised methods against the supervised method
proposed by Katz and Giesbrecht (2006). In this
study, co-occurrence vectors for the tokens were
formed from uses of a German idiom manually an-
notated as literal or idiomatic. Tokens were classi-
fied in a leave-one-out methodology using k-nearest
neighbours, with k = 1. We report results using this
method (1NN) as well as one which considers a to-
ken?s 5 nearest neighbours (5NN). In all cases, we
report the accuracy macro-averaged across the ex-
perimental expressions.
4 Experimental Results and Analysis
In Section 4.1, we discuss the overall performance
of our proposed unsupervised methods. Section 4.2
explores possible causes of the differences observed
in the performance of the methods. We examine
our estimated idiomatic and literal vectors, and com-
pare them with the actual vectors calculated from
3We also considered 10 and 20 word windows on either side
of the target expression, but experiments on development data
indicated that using the sentence as a window performed better.
4We employed singular value decomposition (Deerwester et
al., 1990) to reduce the dimensionality of the co-occurrence
vectors. This had a negative effect on the results, likely be-
cause information about determiners, which occur frequently
with many expressions, is lost in the dimensionality reduction.
Method %Acc (%RER)
Baseline 61.9 -
Unsupervised Di
I -CF ; L-Comp
67.8 (15.5)
Di
I -CF ; L-NCF
70.1 (21.5)
CForm 72.4 (27.6)
Supervised 1NN 72.4 (27.6)
5NN 76.2 (37.5)
Table 1: Macro-averaged accuracy (%Acc) and relative error
reduction (%RER) over TEST.
manually-annotated data. Results reported in Sec-
tions 4.1 and 4.2 are on TEST (results on DEV have
very similar trends). Section 4.3 then examines the
performance of the unsupervised methods on ex-
pressions with different proportions of idiomatic-to-
literal usages. This section presents results on TEST
and DEV combined, as explained below.
4.1 Overall Performance
Table 4.1 shows the macro-averaged accuracy on
TEST of our three unsupervised methods, as well as
that of the baseline and the two supervised methods
for comparison (see Section 3.3). The best super-
vised performance and the best unsupervised perfor-
mance are indicated in boldface. As the table shows,
all three unsupervised methods outperform the base-
line, confirming that the canonical forms of an ex-
pression, and local context, are both informative in
distinguishing literal and idiomatic instances of the
expression.
The table also shows that Di
I -CF ;L-NCF
per-
forms better than Di
I -CF ;L-Comp
. This suggests
that estimating the literal meaning of an expression
using the non-canonical forms is more accurate than
using the composed vector, ~v
L-Comp
. In Section 4.2
we find more evidence for this. Another interesting
observation is that CForm has the highest perfor-
mance (among unsupervised methods), very closely
followed by Di
I -CF ;L-NCF
. These results confirm
our hypothesis that canonical forms?which reflect
the overall behaviour of a VNC type?are strongly
informative about the class of a token, perhaps even
more so than the local context of the token. Im-
portantly, this is the case even though the canonical
forms that we use are imperfect knowledge obtained
automatically through an unsupervised method.
Our results using 1NN, 72:4%, are comparable
45
Vectors cosine Vectors cosine
~a
idm
and ~a
lit
.55
~v
I -CF
and ~a
lit
.70 ~v
I -CF
and ~a
idm
.90
~v
L-NCF
and ~a
lit
.80 ~v
L-NCF
and ~a
idm
.60
~v
L-Comp
and ~a
lit
.72 ~v
L-Comp
and ~a
idm
.76
Table 2: Average similarity between the actual vectors (~a) and
the estimated vectors (~v), for the idiomatic and literal meanings.
to those of Katz and Giesbrecht (2006) using this
method on their German data (72%). However, their
baseline is slightly lower than ours at 58%, and
they only report results for 1 expression with 67 in-
stances. Interestingly, our best unsupervised results
are in line with the results using 1NN and not sub-
stantially lower than the results using 5NN.
4.2 A Closer Look into the Estimated Vectors
In this section, we compare our estimated idiomatic
and literal vectors with the actual vectors for these
usages calculated from manually-annotated data.
Such a comparison helps explain some of the differ-
ences we observed in the performance of the meth-
ods. Table 4.2 shows the similarity between the esti-
mated and actual vectors representing the idiomatic
and literal meanings, averaged over the 14 TEST ex-
pressions. Actual vectors, referred to as ~a
idm
and
~a
lit
, are calculated over idiomatic and literal usages
of the expressions as determined by the human an-
notations. Estimated vectors, ~v
I -CF
, ~v
L-CF
, and
~v
L-Comp
, are calculated using our methods described
in Section 2.2.
For comparison purposes, the first row of Ta-
ble 4.2 shows the average similarity between the
actual idiomatic and literal vectors, ~a
idm
and ~a
lit
.
These vectors are expected to be very dissimilar,
hence the low average cosine between them serves
as a baseline for comparison. We now look into the
relative similarity of each estimated vector, ~v
I -CF
,
~v
L-CF
, ~v
L-Comp
, with these two vectors.
The second row of the table shows that, as de-
sired, our estimated idiomatic vector, ~v
I -CF
, is no-
tably more similar to the actual idiomatic vector than
to the actual literal vector. Also, ~v
L-NCF
is more
similar to the actual literal vector than to the actual
idiomatic vector (third row). Surprisingly, however,
~v
L-Comp
is somewhat similar to both actual literal
and idiomatic vectors (in fact it is slightly more simi-
lar to the latter). These results suggest that the vector
composed of the context vectors for the constituents
of an expression may not always be the best estimate
of the literal meaning of the expression.5 Given this
observation, the overall better-than-baseline perfor-
mance of Di
I-CF;L-Comp
might seem unjustified at
a first glance. However, we believe this performance
is mainly due to an accurate estimate of ~v
I -CF
.
4.3 Performance Based on Class Distribution
We further divide our 28 DEV and TEST expres-
sions according to their proportion of idiomatic-to-
literal usages, as determined by the human annota-
tors. In order to have a sufficient number of expres-
sions in each group, here we merge DEV and TEST
(we refer to the new set as DT). DT
I
high
contains
17 expressions with 65%?90% of their usages be-
ing idiomatic?i.e., their idiomatic usage is domi-
nant. DT
I
low
contains 11 expressions with 8%?58%
of their occurrences being idiomatic?i.e., their id-
iomatic usage is not dominant.
Table 4.3 shows the average accuracy of all the
methods on these two groups of expressions, with
the best performance on each group shown in bold-
face. On DT
I
high
, both Di
I -CF ;L-NCF
and CForm
outperform the baseline, with CForm having the
highest reduction in error rate. The two methods per-
form similarly to each other on DT
I
low
, though note
that the error reduction of CForm is more in line
with its performance on DT
I
high
. These results show
that even for VNCs whose idiomatic meaning is
not dominant?i.e., those in DT
I
low
?automatically-
acquired canonical forms can help with their token
classification.
An interesting observation in Table 4.3 is the
inconsistent performance of Di
I -CF ;L-Comp
: the
method has a very poor performance on DT
I
high
, but
outperforms the other two unsupervised methods on
DT
I
low
. As we noted earlier in Section 2.2, the more
frequent the idiomatic meaning of an expression,
the more reliable the acquired canonical forms for
that expression. Since the performance of CForm
and Di
I -CF ;L-NCF
depends highly on the accu-
racy of the automatically acquired canonical forms,
it is not surprising that these two methods perform
5This was also noted by Katz and Giesbrecht (2006) in their
second experiment.
46
Method DT
I
high
DT
I
low
Baseline 81.4 (-) 35.0 (-)
Unsuper- Di
I -CF ; L-Comp
73.1 (-44.6) 58.6 (36.3)
vised Di
I -CF ; L-NCF
82.3 (4.8) 52.7 (27.2)
CForm 84.7 (17.7) 53.4 (28.3)
Super- 1NN 78.3 (-16.7) 65.8 (47.4)
vised 5NN 82.3 (4.8) 72.4 (57.5)
Table 3: Macro-averaged accuracy over DEV and TEST, di-
vided according to the proportion of idiomatic-to-literal usages.
worse than Di
I -CF ;L-Comp
on VNCs whose id-
iomatic usage is not dominant.
The high performance of the supervised meth-
ods on DT
I
low
also confirms that the poorer perfor-
mance of the unsupervised methods on these VNCs
is likely due to the inaccuracy of the canonical forms
extracted for them. Interestingly, when canonical
forms can be extracted with a high accuracy (i.e.,
for VNCs in DT
I
high
) the performance of the unsu-
pervised methods is comparable to (or even slightly
better than) that of the best supervised method. One
possible way of improving the performance of unsu-
pervised methods is thus to develop more accurate
techniques for the automatic acquisition of canoni-
cal forms.
5 Related Work
Various properties of MWEs have been exploited
in developing automatic identification methods for
MWE types (Lin, 1999; Krenn and Evert, 2001; Fa-
zly and Stevenson, 2006). Much research has ad-
dressed the non-compositionality of MWEs as an
important property related to their idiomaticity, and
has used it in the classification of both MWE types
and tokens (Baldwin et al, 2003; McCarthy et al,
2003; Katz and Giesbrecht, 2006). We also make
use of this property in an MWE token classification
task, but in addition, we draw on other salient char-
acteristics of MWEs which have been previously
shown to be useful for their type classification (Evert
et al, 2004; Fazly and Stevenson, 2006).
The idiomatic/literal token classification methods
of Birke and Sarkar (2006) and Katz and Giesbrecht
(2006) rely primarily on the local context of a to-
ken, and fail to exploit specific linguistic properties
of non-literal language. Our results suggest that such
properties are often more informative than the local
context, in determining the class of an MWE token.
The supervised classifier of Patrick and Fletcher
(2005) distinguishes between compositional and
non-compositional English verb-particle con-
struction tokens. Their classifier incorporates
linguistically-motivated features, such as the degree
of separation between the verb and particle. Here,
we focus on a different class of English MWEs,
verb+noun combinations. Moreover, by making
a more direct use of their syntactic behaviour, we
develop unsupervised token classification methods
that perform well. The unsupervised token classifier
of Hashimoto et al (2006) uses manually-encoded
information about allowable and non-allowable
syntactic transformations of Japanese idioms?that
are roughly equivalent to our notions of canonical
and non-canonical forms. The rule-based classifier
of Uchiyama et al (2005) incorporates syntac-
tic information about Japanese compound verbs
(JCVs), a type of MWE composed of two verbs.
In both cases, although the classifiers incorporate
syntactic information about MWEs, their manual
development limits the scalability of the approaches.
Uchiyama et al (2005) also propose a statistical
token classification method for JCVs. This method
is similar to ours, in that it also uses type-based
knowledge to determine the class of each token
in context. However, their method is supervised,
whereas our methods are unsupervised. Moreover,
Uchiyama et al (2005) evaluate their methods on a
set of JCVs that are mostly monosemous. Here, we
intentionally exclude such cases from consideration,
and focus on those MWEs that have two clear id-
iomatic and literal meanings, and that are frequently
used with either meaning.
6 Conclusions
While a great deal of research has focused on prop-
erties of MWE types, such as their compositional-
ity, less attention has been paid to issues surround-
ing MWE tokens. In this study, we have developed
techniques for a semantic classification of tokens of
a potential MWE in context. We focus on a broadly
documented class of English MWEs that are formed
from the combination of a verb and a noun in its
direct object position, referred to as VNCs. We an-
notated a total of 1180 tokens for 28 VNCs accord-
47
ing to whether they are a literal or idiomatic usage,
and we found that approximately 40% of the to-
kens were literal usages. These figures indicate that
automatically determining whether a VNC token is
used idiomatically or literally is of great importance
for NLP applications. In this work, we have pro-
posed three unsupervised methods that perform such
a task. Our proposed methods incorporate automati-
cally acquired knowledge about the overall syntactic
behaviour of a VNC type, in order to do token classi-
fication. More specifically, our methods draw on the
syntactic fixedness of VNCs?a property which has
been largely ignored in previous studies of MWE
tokens. Our results confirm the usefulness of this
property as incorporated into our methods. All our
methods outperform the baseline of always predict-
ing the most frequent class. Moreover, considering
our approach is unsupervised, our best accuracy of
72:4% is not substantially lower than the accuracy
of a standard supervised approach at 76:2%.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of EACL-06, 329?336.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Anthony P. Cowie, Ronald Mackin, and Isabel R. Mc-
Caig. 1983. Oxford Dictionary of Current Idiomatic
English, volume 2. Oxford University Press.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert, Ulrich Heid, and Kristina Spranger. 2004.
Identifying morphosyntactic preferences in colloca-
tions. In Proceedings LREC-04.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of EACL-06, 337?344.
Christiane Fellbaum. 2002. VP idioms in the lexicon:
Topics for research using a very large corpus. In
S. Busemann, editor, Proceedings of the KONVENS-
02 Conference.
John R. Firth. 1957. A synopsis of linguistic theory
1930?1955. In Studies in Linguistic Analysis (special
volume of the Philological Society), 1?32. The Philo-
logical Society, Oxford.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Japanese idiom recognition: Drawing a line be-
tween literal and idiomatic meanings. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, 353?360.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, 12?19.
Brigitte Krenn and Stefan Evert. 2001. Can we do better
than frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL-01 Workshop
on Collocations.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70(3):491?538.
Jon Patrick and Jeremy Fletcher. 2005. Classifying verb-
particle constructions by verb arguments. In Proceed-
ings of the Second ACL-SIGSEM Workshop on the Lin-
guistic Dimensions of Prepositions and their use in
Computational Linguistics Formalisms and Applica-
tions, 200?209.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of CICLing-02, 1?15.
Maggie Seaton and Alison Macaulay, editors. 2002.
Collins COBUILD Idioms Dictionary. HarperCollins
Publishers, second edition.
Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.
2005. Disambiguating Japanese compound verbs.
Computer Speech and Language, Special Issue on
Multiword Expressions, 19(4):497?512.
Sriram Venkatapathy and Aravid Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N)
collocations by integrating features. In Proceedings of
HLT/EMNLP-05, 899?906.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL-06
Workshop on Multiword Expressions in a Multilingual
Context, 33?40.
48
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 244?254,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Cognitive Model of Semantic Network Learning
Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
{aida,afsaneh,suzanne}@cs.toronto.edu
Abstract
Child semantic development includes
learning the meaning of words as well as
the semantic relations among words. A
presumed outcome of semantic develop-
ment is the formation of a semantic net-
work that reflects this knowledge. We
present an algorithm for simultaneously
learning word meanings and gradually
growing a semantic network, which ad-
heres to the cognitive plausibility require-
ments of incrementality and limited com-
putations. We demonstrate that the seman-
tic connections among words in addition
to their context is necessary in forming a
semantic network that resembles an adult?s
semantic knowledge.
1 Introduction
Child semantic development includes the acquisi-
tion of word-to-concept mappings (part of word
learning), and the formation of semantic connec-
tions among words/concepts. There is consid-
erable evidence that understanding the semantic
properties of words improves child vocabulary ac-
quisition. In particular, children are sensitive to
commonalities of semantic categories, and this
abstract knowledge facilitates subsequent word
learning (Jones et al., 1991; Colunga and Smith,
2005). Furthermore, representation of semantic
knowledge is significant as it impacts how word
meanings are stored in, searched for, and retrieved
from memory (Steyvers and Tenenbaum, 2005;
Griffiths et al., 2007).
Semantic knowledge is often represented as a
graph (a semantic network) in which nodes cor-
respond to words/concepts
1
, and edges specify
1
Here we assume that the nodes of a semantic network
are word forms and its edges are determined by the semantic
features of those words.
the semantic relations (Collins and Loftus, 1975;
Steyvers and Tenenbaum, 2005). Steyvers and
Tenenbaum (2005) demonstrated that a seman-
tic network that encodes adult-level knowledge of
words exhibits a small-world and scale-free struc-
ture. That is, it is an overall sparse network with
highly-connected local sub-networks, where these
sub-networks are connected through high-degree
hubs (nodes with many neighbours).
Much experimental research has investigated
the underlying mechanisms of vocabulary learn-
ing and characteristics of semantic knowledge
(Quine, 1960; Bloom, 1973; Carey and Bartlett,
1978; Gleitman, 1990; Samuelson and Smith,
1999; Jones et al., 1991; Jones and Smith,
2005). However, existing computational models
focus on certain aspects of semantic acquisition:
Some researchers develop computational models
of word learning without considering the acqui-
sition of semantic connections that hold among
words, or how this semantic knowledge is struc-
tured (Siskind, 1996; Regier, 2005; Yu and Bal-
lard, 2007; Frank et al., 2009; Fazly et al., 2010).
Another line of work is to model formation of
semantic categories but this work does not take
into account how word meanings/concepts are ac-
quired (Anderson and Matessa, 1992; Griffiths et
al., 2007; Fountain and Lapata, 2011).
Our goal in this work is to provide a cognitively-
plausible and unified account for both acquiring
and representing semantic knowledge. The re-
quirements for cognitive plausibility enforce some
constraints on a model to ensure that it is compa-
rable with the cognitive process it is formulating
(Poibeau et al., 2013). As we model semantic ac-
quisition, the first requirement is incrementality,
which means that the model learns gradually as
it processes the input. Also, there is a limit on
the number of computations the model performs
at each step.
In this paper, we present an algorithm for si-
244
multaneously learning word meanings and grow-
ing a semantic network, which adheres to the cog-
nitive plausibility requirements of incrementality
and limited computations. We examine networks
created by our model under various conditions,
and explore what is required to obtain a structure
that has appropriate semantic connections and has
a small-world and scale-free structure.
2 Related Work
Models of Word Learning. Given a word learn-
ing scenario, there are potentially many possible
mappings between words in a sentence and their
meanings (real-world referents), from which only
some mappings are correct (the mapping prob-
lem). One of the most dominant mechanisms
proposed for vocabulary acquisition is cross-
situational learning: people learn word mean-
ings by recognizing and tracking statistical reg-
ularities among the contexts of a word?s usage
across various situations, enabling them to nar-
row in on the meaning of a word that holds across
its usages (Siskind, 1996; Yu and Smith, 2007;
Smith and Yu, 2008). A number of computa-
tional models attempt to solve the mapping prob-
lem by implementing this mechanism, and have
successfully replicated different patterns observed
in child word learning (Siskind, 1996; Yu and Bal-
lard, 2007; Fazly et al., 2010). These models have
provided insight about underlying mechanisms of
word learning, but none of them consider the se-
mantic relations that hold among words, or how
the semantic knowledge is structured. Recently,
we have investigated properties of the semantic
structure of the resulting (final) acquired knowl-
edge of such a learner (Nematzadeh et al., 2014).
However, that work did not address how such
structural knowledge might develop and evolve in-
crementally within the learning model.
Models of Categorization. Computational mod-
els of categorization focus on the problem of form-
ing semantic clusters given a defined set of fea-
tures for words (Anderson and Matessa, 1992;
Griffiths et al., 2007; Sanborn et al., 2010). An-
derson and Matessa (1992) note that a cognitively
plausible categorization algorithm needs to be in-
cremental and only keep track of one potential
partitioning; they propose a Bayesian framework
(the Rational Model of Categorization or RMC)
that specifies the joint distribution on features and
category labels, and allows an unbounded number
of clusters. Sanborn et al. (2010) examine differ-
ent categorization models based on RMC. In par-
ticular, they compare the performance of the ap-
proximation algorithm of Anderson and Matessa
(1992) (local MAP) with two other approximation
algorithms (Gibbs Sampling and Particle Filters)
in various human categorization paradigms. San-
born et al. (2010) find that in most of the simula-
tions the local MAP algorithm performs as well as
the two other algorithms in matching human be-
havior.
The Representation of Semantic Knowledge.
There is limited work on computational models
of semantic acquisition that examine the represen-
tation of the semantic knowledge. Steyvers and
Tenenbaum (2005) propose an algorithm for build-
ing a network with small-world and scale-free
structure. The algorithm starts with a small com-
plete graph, incrementally adds new nodes to the
graph, and for each new node uses a probabilistic
mechanism for selecting a subset of current nodes
to connect to. However, their approach does not
address the problem of learning word meanings or
the semantic connections among them. Fountain
and Lapata (2011) propose an algorithm for learn-
ing categories that also creates a semantic network
by comparing all the possible word pairs. How-
ever, they too do not address the word learning
problem, and do not investigate the structure of the
learned semantic network to see whether it has the
properties observed in adult knowledge.
3 The Incremental Network Model
We propose here a model that unifies the incre-
mental acquisition of word meanings and forma-
tion of a semantic network structure that reflects
the similarities among those meanings. We use
an existing model to learn the meanings of words
(Section 3.1), and use those incrementally devel-
oping meanings as the input to the algorithm pro-
posed here for gradually growing a semantic net-
work (Section 3.2).
3.1 The Word Learner
We use the model of Fazly et al. (2010); this learn-
ing algorithm is incremental and involves limited
calculations, thus satisfying basic cognitive plausi-
bility requirements. A naturalistic language learn-
ing scenario consists of linguistic data in the con-
text of non-linguistic data, such as the objects,
245
Utterance: {let, find, a, picture, to, color }
Scene: {LET, PRONOUN, HAS POSSESSION, CAUSE,
ARTIFACT, WHOLE, CHANGE, . . .}
Table 1: A sample utterance-scene pair.
events, and social interactions that a child per-
ceives. This kind of input is modeled here as
a pair of an utterance (the words a child hears)
and a scene (the semantic features representing the
meaning of those words), as shown in Table 1 (and
described in more detail in Section 5.1). The word
learner is an instance of cross-situational learn-
ing applied to a sequence of such input pairs: for
each pair of a word w and a semantic feature f ,
the model incrementally learns P (f |w) from co-
occurrences of w and f across all the utterance-
scene pairs.
For each word, the probability distribution
over all semantic features, P (.|w), represents the
word?s meaning. The estimation of P (.|w) is
made possible by introducing a set of latent vari-
ables, alignments, that correspond to the possible
mappings between words and features in a given
utterance?scene pair. The learning problem is then
to find the mappings that best explain the data,
which is solved by using an incremental version
of the expectation?maximization (EM) algorithm
(Neal and Hinton, 1998). We skip the details of
the derivations and only report the resulting for-
mulas.
The model processes one utterance-scene pair at
a time. For the input pair processed at time t, first
the probability of each possible alignment (align-
ment probability) is calculated as:
2
P (a
ij
|u, f
i
) =
P
t?1
(f
i
|w
j
)
?
w
?
?u
P
t?1
(f
i
|w
?
)
(1)
where u is the utterance, and a
ij
is the alignment
variable specifying the word w
j
that is mapped
to the feature f
i
. P
t?1
(f
i
|w
j
) is taken from the
model?s current learned meaning of word w
j
. Ini-
tially, P
0
(f
i
|w
j
) is uniformly distributed. After
calculating the alignment probabilities, the learned
meanings are updated as:
P
t
(f
i
|w
j
) =
?
u?U
t
P (a
ij
|u, f
i
)
?
f
?
?M
?
u?U
t
P (a
ij
|u, f
?
)
(2)
where U
t
is the set of utterances processed so far,
andM is the set of features that the model has ob-
served. Note that for each w?f pair, the value of
the summations in this formula can be incremen-
tally updated after processing any utterance that
2
This corresponds to the expectation step of EM.
containsw; the summation does not have to be cal-
culated at every step.
3.2 Growing a Semantic Network
In our extended model, as we learn words incre-
mentally (as above), we also structure those words
into a semantic network based on the (partially)
learned meanings. At any given point in time, the
network will include as its nodes all the word types
the word learner has been exposed to. Weighted
edges (capturing semantic distance) will connect
those pairs of word types whose learned meanings
at that point are sufficiently semantically similar
(according to a threshold). Since the probabilis-
tic meaning of a word is adjusted each time it is
observed, a word may either lose or gain connec-
tions in the network after each input is processed.
Thus, to incrementally develop the network, at
each time step, our algorithm must both examine
existing connections (to see which edges should be
removed) and consider potential new connections
(to see which edges should be added).
A simple approach to achieve this is to examine
the current semantic similarity between a word w
in the input and all the current words in the net-
work, and include edges between only those word
pairs that are sufficiently similar. However, com-
paring w to all the words in the network each time
it is observed is computationally intensive (and not
cognitively plausible).
We present an approach for incrementally grow-
ing a semantic network that limits the computa-
tions when processing each input word w; see Al-
gorithm 1. After the meaning of w is updated, we
first check all the words that w is currently (di-
rectly) connected to, to see if any of those edges
need to be removed, or have their weight adjusted.
Next, to look for new connections forw, the idea is
to select only a small subset of words, S , to which
w will be compared. The challenge then is to se-
lect S in a way that will yield a network whose se-
mantic structure reasonably approximates the net-
work that would result from full knowledge of
comparing w to all the words.
Previous work has suggested picking ?impor-
tant? words (e.g., high-degree words) indepen-
dently of the target word w ? assuming these
might be words for which a learner might need
to understand their relationship to w in the future
(Steyvers and Tenenbaum, 2005). Our proposal
is instead to consider for S those words that are
246
Algorithm 1 Growing a network after each in-
put u.
for all w in u do
update P (.|w) using Eqn. (2)
update current connections of w
select S(w), a subset of words in the network
for all w
?
in S(w) do
if w and w
?
are sufficiently similar then
connect w and w
?
with an edge
end if
end for
end for
likely to be similar to w. That is, since the net-
work only needs to connect similar words to w, if
we can guess what (some of) those words are, then
we will do best at approximating the situation of
comparing w to all words.
The question now is how to find semantically
similar words to w that are not already connected
to w in the network. To do so, we incrementally
track semantic similarity among words usages as
their meanings are developing. Specifically we
cluster word tokens (not types) according to their
current word meanings. Since the probabilistic
meanings of words are continually evolving, in-
cremental clusters of word tokens can capture de-
veloping similarities among the various usages of
a word type, and be a clue to which words (types)
w might be similar to. In the next section, we de-
scribe the Bayesian clustering process we use to
identify potentially similar words.
3.3 Semantic Clustering of Word Tokens
We use the Bayesian framework of Anderson and
Matessa (1992) to form semantic clusters.
3
Recall
that for each word w, the model learns its mean-
ings as a probability distribution over all seman-
tic features, P (.|w). We represent this probability
distribution as a vector F whose length is the num-
ber of possible semantic features. Each element of
the vector holds the value P (f |w) (which is con-
tinuous). Given a word w and its vector F , we
need to calculate the probability that w belongs to
each existing cluster, and also allow for the pos-
sibility of it forming a new cluster. Using Bayes
rule we have:
P (k|F ) =
P (k)P (F |k)
?
k
?
P (k
?
)P (F |k
?
)
(3)
3
The distribution specified by this model is equivalent to
that of a Dirichlet Process Mixture Model (Neal, 2000).
where k is a given cluster. We thus need to calcu-
late the prior probability, P (k), and the likelihood
of each cluster, P (F |k).
Calculation of Prior. The prior probability that
word n + 1 is assigned to cluster k is calculated
as:
P (k) =
{
n
k
n+?
n
k
> 0
?
n+?
n
k
= 0 (new cluster)
(4)
where n
k
is the number of words in cluster k, n
is the number of words observed so far, and ? is a
parameter that determines how likely the creation
of a new cluster is. The prior favors larger clusters,
and also discourages the creation of new clusters
in later stages of learning.
Calculation of Likelihood. To calculate the like-
lihood P (F |k) in Eqn. (3), we assume that the fea-
tures are independent:
P (F |k) =
?
f
i
?F
P (f
i
= v|k) (5)
where P (f
i
= v|k) is the probability that the value
of the feature in dimension i is equal to v given
the cluster k. To derive P (f
i
|k), following An-
derson and Matessa (1992), we assume that each
feature given a cluster follows a Gaussian distri-
bution with an unknown variance ?
2
and mean ?.
(In the absence of any prior information about a
variable, it is often assumed to have a Gaussian
distribution.) The mean and variance of this dis-
tribution are inferred using Bayesian analysis: We
assume the variance has an inverse ?
2
prior, where
?
2
0
is the prior variance and a
0
is the confidence in
the prior variance:
?
2
? Inv-?
2
(a
0
, ?
2
0
) (6)
The mean given the variance has a Gaussian dis-
tribution with ?
0
as the prior mean and ?
0
as the
confidence in the prior mean.
?|? ? N(?
0
,
?
2
?
0
) (7)
Given the above conjugate priors, P (f
i
|k) can
be calculated analytically and is a Student?s t dis-
tribution with the following parameters:
P (f
i
|k) ? t
a
i
(?
i
, ?
2
i
(1 +
1
?
i
)) (8)
?
i
= ?
0
+ n
k
(9)
a
i
= a
0
+ n
k
(10)
247
?i
=
?
0
?
0
+ n
k
?
f
?
0
+ n
k
(11)
?
2
i
=
a
0
?
2
0
+ (n
k
? 1)s
2
+
?
0
n
k
?
0
+n
k
(?
0
+
?
f)
2
a
0
+ n
k
(12)
where
?
f and s
2
are the sample mean and variance
of the values of f
i
in k.
Note that in the above equations, the mean and
variance of the distribution are simply derived by
combining the sample mean and variance with
the prior mean and variance while considering the
confidence in the prior mean (?
0
) and variance
(a
0
). This means that the number of computations
to calculate P (F |K) is limited as w is only com-
pared to the ?prototype? of each cluster, which is
represented by ?
i
and ?
i
of different features.
Adding a word w to a cluster. We add w to
the cluster k with highest posterior probability,
P (k|F ), as calculated in Eqn. (3).
4
The parame-
ters of the selected cluster (k, ?
i
, ?
i
, ?
i
, and a
i
for
each feature f
i
) are then updated incrementally.
Using the Clusters to Select the Words in S(w).
We can now form S(w) in Algorithm 1 by select-
ing a given number of words n
s
whose tokens are
probabilistically chosen from the clusters accord-
ing to how likely each cluster k is given w: the
number of word tokens picked from each k is pro-
portional to P (k|F ) and is equal to P (k|F )?n
s
.
4 Evaluation
We evaluate a semantic network in two regards:
The semantic connectivity of the network ? to
what extent the semantically-related words are
connected in the network; and the structure of the
network ? whether it exhibits a small-world and
scale-free structure or not.
4.1 Evaluating Semantic Connectivity
The distance between the words in the network in-
dicates their semantic similarity: the more similar
a word pair, the smaller their distance. For word
pairs that are connected via a path in the network,
this distance is the weighted shortest path length
between the two words. If there is no path be-
tween a word pair, their distance is considered to
be? (which is represented with a large number).
We refer to this distance as the ?learned? semantic
similarity.
4
This approach is referred to as local MAP (Sanborn et al.,
2010); because of the incremental nature of the algorithm, it
maximizes the current posterior distribution as opposed to the
?global? posterior.
To evaluate the semantic connectivity of the
learned network, we compare these learned sim-
ilarity scores to ?gold-standard? similarity scores
that are calculated using the WordNet similarity
measure of Wu and Palmer (1994) (also known as
the WUP measure). We choose this measure since
it captures the same type of similarity as in our
model: words are considered similar if they belong
to the same semantic category. Moreover, this
measure does not incorporate information about
other types of similarities, for example, words are
not considered similar if they occur in similar con-
texts. Thus, the scores calculated with this mea-
sure are comparable with those of our learned net-
work.
Given the gold-standard similarity scores for
each word pair, we evaluate the semantic con-
nectivity of the network based on two perfor-
mance measures: coefficient of correlation and
the median rank of the first five gold-standard as-
sociates. Correlation is a standard way to com-
pare two lists of similarity scores (Budanitsky
and Hirst, 2006). We create two lists, one con-
taining the gold-standard similarity scores for all
word pairs, and the other containing their corre-
sponding learned similarity scores. We calculate
the Spearman?s rank correlation coefficient, ?, be-
tween these two lists of similarity scores. Note
that the learned similarity scores reflect the seman-
tic distance among words whereas the WordNet
scores reflect semantic closeness. Thus, a nega-
tive correlation is best in our evaluation, where the
value of -1 corresponds to the maximum correla-
tion.
Following Griffiths et al. (2007), we also cal-
culate the median learned rank of the first five
gold-standard associates for all words: For each
word w, we first create a ?gold-standard? asso-
ciates list: we sort all other words based on their
gold-standard similarity to w, and pick the five
most similar words (associates) to w. Similarly,
we create a ?learned associate list? for w by sort-
ing all words based on their learned semantic simi-
larity tow. For all words, we find the ranks of their
first five gold-standard associates in their learned
associate list. For each associate, we calculate the
median of these ranks for all words. We only re-
port the results for the first three gold-standard as-
sociates since the pattern of results is similar for
the fourth and fifth associates; we refer to the me-
dian rank of first three gold-standard associates as
248
1st
, 2
nd
, and 3
rd
.
4.2 Evaluating the Structure of the Network
A network exhibits a small-world structure when
it is characterized by short path length between
most nodes and highly-connected neighborhoods
(Watts and Strogatz, 1998). We first explain how
these properties are measured for a graph with N
nodes and E edges. Then we discuss how these
properties are used in assessing the small-world
structure of a graph.
5
.
Short path lengths. Most of the nodes of
a small-world network are reachable from other
nodes via relatively short paths. For a connected
network (i.e., all the node pairs are reachable from
each other), this can be measured as the average
distance between all node pairs (Watts and Stro-
gatz, 1998). Since our networks are not connected,
we instead measure this property using the median
of the distances (d
median
) between all node pairs
(Robins et al., 2005), which is well-defined even
when some node pairs have a distance of?.
Highly-connected neighborhoods. The neigh-
borhood of a node n in a graph consists of n and
all of the nodes that are connected to it. A neigh-
borhood is maximally connected if it forms a com-
plete graph ?i.e., there is an edge between all
node pairs. Thus, the maximum number of edges
in the neighborhood of n is k
n
(k
n
? 1)/2, where
k
n
is the number of neighbors. A standard metric
for measuring the connectedness of neighbors of
a node n is called the local clustering coefficient
(C) (Watts and Strogatz, 1998), which calculates
the ratio of edges in the neighborhood of n (E
n
)
to the maximum number of edges possible for that
neighborhood:
C =
E
n
k
n
(k
n
? 1)/2
(13)
The local clustering coefficient C ranges between
0 and 1. To estimate the connectedness of all
neighborhoods in a network, we take the average
of C over all nodes, i.e., C
avg
.
Small-world structure. A graph exhibits a
small-world structure if d
median
is relatively small
and C
avg
is relatively high. To assess this for
a graph g, these values are typically compared
to those of a random graph with the same num-
ber of nodes and edges as g (Watts and Strogatz,
5
We take the description of these measures from Ne-
matzadeh et al. (2014)
1998; Humphries and Gurney, 2008). The ran-
dom graph is generated by randomly rearranging
the edges of the network under consideration (Er-
dos and R?enyi, 1960). Because any pair of nodes
is equally likely to be connected as any other, the
median of distances between nodes is expected to
be low for a random graph. In a small-world net-
work, this value d
median
is expected to be as small
as that of a random graph: even though the random
graph has edges more uniformly distributed, the
small-world network has many locally-connected
components which are connected via hubs. On the
other hand, C
avg
is expected to be much higher
in a small-world network compared to its corre-
sponding random graph, because the edges of a
random graph typically do not fall into clusters
forming highly connected neighborhoods.
Given these two properties, the ?small-
worldness? of a graph g is measured as follows
(Humphries and Gurney, 2008):
?
g
=
C
avg
(g)
C
avg
(random)
d
median
(g)
d
median
(random)
(14)
where random is the random graph correspond-
ing to g. In a small-world network, it is ex-
pected that C
avg
(g)  C
avg
(random) and
d
median
(g) ? d
median
(random), and thus ?
g
>
1.
Note that Steyvers and Tenenbaum (2005) made
the empirical observation that small-world net-
works of semantic knowledge had a single con-
nected component that contained the majority of
nodes in the network. Thus, in addition to ?
g
,
we also measure the relative size of a network?s
largest connected component having size N
lcc
:
size
lcc
=
N
lcc
N
(15)
Scale-free structure. A scale-free network has
a relatively small number of high-degree nodes
that have a large number of connections to other
nodes, while most of its nodes have a small de-
gree, as they are only connected to a few nodes.
Thus, if a network has a scale-free structure, its de-
gree distribution (i.e., the probability distribution
of degrees over the whole network) will follow a
power-law distribution (which is said to be ?scale-
free?). We evaluate this property of a network by
plotting its degree distribution in the logarithmic
scale, which (if a power-law distribution) should
appear as a straight line. None of our networks ex-
249
hibit a scale-free structure; thus, we do not report
the results of this evaluation, and leave it to future
work for further investigation.
5 Experimental Set-up
5.1 Input Representation
Recall that the input to the model consists of a
sequence of utterance?scene pairs intended to re-
flect the linguistic data a child is exposed to, along
with the associated meaning a child might grasp.
As in much previous work (Yu and Ballard, 2007;
Fazly et al., 2010), we take child-directed utter-
ances from the CHILDES database (MacWhinney,
2000) in order to have naturalistic data. In partic-
ular, we use the Manchester corpus (Theakston et
al., 2001), which consists of transcripts of conver-
sations with 12 British children between the ages
of 1; 8 and 3; 0. We represent each utterance as
a bag of lemmatized words (see Utterance in Ta-
ble 1).
For the scene representation, we have no large
corpus to draw on that encodes the semantic por-
tion of language acquisition data.
6
We thus auto-
matically generate the semantics associated with
an utterance, using a scheme first introduced in
Fazly et al. (2010). The idea is to first create an
input generation lexicon that provides a mapping
between all the words in the input data and their
associated meanings. A scene is then represented
as a set that contains the meanings of all the words
in the utterance. We use the input generation lexi-
con of Nematzadeh et al. (2012) because the word
meanings reflect information about their semantic
categories, which is crucial to forming the seman-
tic clusters as in Section 3.3.
In this lexicon, the ?true? meaning for each
word w is a vector over a set of possible seman-
tic features for each part of speech; in the vec-
tor, each feature is associated with a score for that
word (see Figure 1). Depending on the word?s part
of speech, the features are extracted from various
6
Yu and Ballard (2007) created a corpus by hand-coding
the objects and cues that were present in the environment,
but that corpus is very small. Frank et al. (2013) provide a
larger manually annotated corpus (5000 utterances), but it is
still very small for longitudinal simulations of word learn-
ing. (Our corpus contains more than 100,000 utterances.)
Moreover, the corpus of Frank et al. (2013) is limited be-
cause a considerable number of words are not semantically
coded. (Only a subset of concrete objects in the environment
are coded.)
apple: { FOOD:1, SOLID:.72, ? ? ? , PLANT-PART:.22,
PHYSICAL-ENTITY:.17, WHOLE:.06, ? ? ? }
Figure 1: Sample true meaning features & their scores for
apple from Nematzadeh et al. (2012).
lexical resources such as WordNet
7
, VerbNet
8
, and
Harm (2002). The score for each feature is calcu-
lated using a measure similar to tf-idf that reflects
the association of the feature with the word and
with its semantic category: term frequency indi-
cates the strength of association of the feature with
the word, and inverse document frequency (where
the documents are the categories) indicates how
informative a feature is for that category. The se-
mantic categories of nouns (which we focus on in
our networks) are given by WordNet lex-names
9
,
a set of 25 general categories of entities. (We use
only nouns in our semantic networks because the
semantic similarity of words with different parts
of speech cannot be compared, since their seman-
tic features are drawn from different resources.)
The input generation lexicon is used to generate
a scene representation for an utterance as follows:
For each word w in the utterance, we probabilisti-
cally sample features, in proportion to their score,
from the full set of features in its true meaning.
The probabilistic sampling allows us to simulate
the noise and uncertainty in the input a child per-
ceives by omitting some meaning features from
the scene. The scene representation is the union
of all the features sampled for all the words in the
utterance (see Scene in Table 1).
5.2 Methods
We experiment with our network-growth method
that draws on the incremental clustering, and cre-
ate ?upper-bound? and baseline networks for com-
parison. Note that all the networks are created
using our Algorithm 1 (page 4) to grow networks
incrementally, drawing on the learned meanings of
words and updating their connections on the basis
of this evolving knowledge. The only difference
in creating the networks resides in how the com-
parison set S(w) is chosen for each target word w
that is being added to the growing network at each
time step. We provide more details in the para-
graphs below.
7
http://wordnet.princeton.edu
8
http://verbs.colorado.edu/
?
mpalmer/
projects/verbnet.html
9
http://wordnet.princeton.edu/wordnet/
man/lexnames.5WN.html
250
Upper-bound. Recall that one of our main goals
is to substantially reduce the number of similar-
ity comparisons needed to grow a semantic net-
work, in contrast to the straightforward method of
comparing each w to all current words. At the
same time, we need to understand the impact of
the increased efficiency on the quality of the re-
sulting networks. We thus need to compare the
target properties of our networks that are learned
using a small comparison set S , to those of an
?upper-bound? network that takes into account all
the pair-wise comparisons among words. We cre-
ate this upper-bound network by setting S(w) to
contain all words currently in the network.
Baselines. On the other hand, we need to evalu-
ate the (potential) benefit of our cluster-driven se-
lection process over a more simplistic approach to
selecting S(w). To do so, we consider three base-
lines, each using a different criteria for choosing
the comparison set S(w): The Random baseline
chooses the members of this set randomly from
the set of all observed words. The Context base-
line can be seen as an ?informed? baseline that at-
tempts to incorporate some semantic knowledge:
Here, we select words that are in the recent context
prior to w in the input, assuming that such words
are likely to be semantically related to w. We also
include a third baseline, Random+Context, that
picks half of the members of S randomly and half
of them from the prior context.
Cluster-based Methods. We report results for
three cluster-based networks that differ in their
choice of S(w) as follows: The Clusters-only net-
work chooses words in S(w) from the set of clus-
ters, proportional to the probability of each clus-
ter k given word w (as explained in Section 3.3).
In order to incorporate different types of semantic
information in selecting S, we also create a Clus-
ters+Context network that picks half of the mem-
bers of S from clusters (as above), and half from
the prior context. For completeness, we include a
Clusters+Random network that similarly chooses
half of words in S from clusters and half randomly
from all observed words.
We have experimented with several other meth-
ods, but they all performed substantially worse
than the baselines, and hence we do not report
them here. E.g., we tried picking words in S from
the best cluster. We also tried a few methods in-
spired by (Steyvers and Tenenbaum, 2005): E.g.,
we examined a method where if a member of S(w)
was sufficiently similar to w, we added the direct
neighbors of that word to S. We also tried to grow
networks by choosing the members of S according
to the degree or frequency of nodes in the network.
5.3 Experimental Parameters
We use 20, 000 utterance?scene pairs as our train-
ing data. Recall that we use clustering to help
guide our semantic network growth algorithm.
Given the clustering algorithm in Section 3.3, we
are interested to find the set of clusters that best
explain the data. (Other clustering algorithms can
be used instead of this algorithm.) We perform
a search on the parameter space, and select the
parameter values that result in the best clustering,
based on the number of clusters and their average
F-score. The value of the clustering parameters
are as follows: ? = 49, ?
0
= 1.0, a
0
= 2.0,
?
0
= 0.0, and ?
0
= 0.05. Two nouns with fea-
ture vectors F
1
and F
2
are connected in the net-
work if cosine(F1, F2) is greater than or equal to
0.6. (This threshold was selected following em-
pirical examination of the similarity values we ob-
serve among the ?true? meaning in our input gen-
eration lexicon.) The weight on the edge that con-
nects these nouns specifies their semantic distance,
which is calculated as 1? cosine(F1, F2).
Because we aim for a network creation method
that is cognitively plausible in performing a lim-
ited number of word-to-word comparisons, we
need to ensure that all the different methods of
selecting the comparison set S(w) yield roughly
similar numbers of such comparisons. Keeping
the size of S constant does not guarantee this,
because each method can yield differing num-
bers of connections of the target word w to other
words. We thus parameterize the size of S for
each method to keep the number of computations
similar, based on experiments on the development
data. In development work we also found that hav-
ing an increasing size of S over time improved
the results, as more words were compared as the
knowledge of learned meanings improved. To
achieve this, we use a percentage of the words
in the network as the size of S. In practice, the
setting of this parameter yields a number of com-
parisons across all methods that is about 8% of
the maximum possible word-to-word comparisons
that would be performed in the naive (computa-
tionally intensive) approach.
251
Note that all the Cluster-based, Random and
Random+Context methods include a random se-
lection mechanism; thus, we run each of these
methods 50 times and report the average ?, me-
dian ranks and size
lcc
(see Section 4). For the net-
works (out of 50 runs) that exhibit a small-world
structure (small-worldness greater than one), we
report the average small-worldness. We also re-
port the percentage of runs whose resulting net-
work exhibit a small-world structure.
6 Experimental Results and Discussion
Table 2 presents our results, including the eval-
uation measures explained above, for the Upper-
bound, Baseline, and Cluster-based networks cre-
ated by the various methods described in Sec-
tion 5.2.
10
Recall that the Upper-bound network is formed
from examining a word?s similarity to all other
(observed) words when it is added to the network.
We can see that this network is highly connected
(0.85) and has a small-world structure (5.5). There
is a statistically significant correlation of the net-
work?s similarity measures with the gold standard
ones (?0.38). For this Upper-bound structure, the
median ranks of the first three associates are be-
tween 31 and 42. These latter two measures on
the Upper-bound network give an indication of the
difficulty of learning a semantic network whose
knowledge matches gold-standard similarities.
Considering the baseline networks, we note that
the Random network is actually somewhat bet-
ter (in connectivity and median ranks) than the
Context network that we thought would provide
a more informed baseline. Interestingly, the cor-
relation value for both networks is no worse than
for the Upper-bound. The combination of Ran-
dom+Context yields a slightly lower correlation,
and no better ranks or connectivity than Random.
Note that none of the baseline networks exhibit a
small world structure (?
g
 1 for all three, except
for one out of 50 runs for the Random method).
Recall that the Random network is not a net-
work resulting from randomly connecting word
pairs, but one that incrementally compares each
target word with a set of randomly chosen words
when considering possible new connections. We
suspect that this approach performs reasonably
well because it enables the model to find a broad
10
All the reported co-efficients of correlation (?) are statis-
tically significant at p < 0.01.
range of similar words to the target; this might be
effective especially because the learned meanings
of words are changing over time.
Turning to the Cluster-based methods, we see
that indeed some diversity in the comparison set
for a target word might be necessary to good
performance. We find that the measures on the
Clusters-only network are roughly the same as on
the Random one, but when we combine the two in
Clusters+Random we see an improvement in the
ranks achieved. It is possible that the selection
from clusters does not have sufficient diversity to
find some of the valid new connections for a word.
We note that the best results overall occur with
the Clusters+Context network, which combines
two approaches to selecting words that have good
potential to be similar to the target word. The
correlation coefficient for this network is at a re-
spectable 0.36, and the median ranks are the sec-
ond best of all the network-growth methods. Im-
portantly, this network shows the desired small-
world structure in most of the runs (77%), with
the highest connectivity and a small-world mea-
sure well over 1.
The fact that the Clusters+Context network is
better overall than the networks of the Clusters-
only and Context methods indicates that both clus-
ters and context are important in making ?in-
formed guesses? about which words are likely
to be similar to a target word. Given the small
number of similarity comparisons used in our ex-
periments (only around 8% of all possible word-
to-word comparisons), these observations suggest
that both the linguistic context and the evolving
relations among word usages (captured by the in-
cremental clustering of learned meanings) contain
information crucial to the process of growing a se-
mantic network in a cognitively plausible way.
7 Conclusions
We propose a unified model of word learning and
semantic network formation, which creates a net-
work of words in which connections reflect struc-
tured knowledge of semantic similarity between
words. The model adheres to the cognitive plau-
sibility requirements of incrementality and use of
limited computations. That is, when incremen-
tally adding or updating a word?s connections in
the network, the model only looks at a subset of
words rather than comparing the target word to all
the nodes in the network. We demonstrate that
252
Comparing all Pairs
Semantic Connectivity Small World
Method ? 1
st
2
nd
3
rd
size
lcc
?
g
(%)
Upper-bound ?0.38 31 41 42 0.85 5.5
Baselines
Random ?0.38 56 76.9 68.9 0.6 5.2 (2)
Context ?0.39 97 115 89 0.5 0
Random+Context ?0.36 63.3 87.2 79.1 0.6 0 (0)
Cluster-based Methods
Clusters-only ?0.32 58.6 72.0 71.6 0.7 5.5 (43)
Clusters+Context ?0.36 53.9 67.6 64.8 0.7 7.2 (77)
Clusters+Random ?0.35 48.1 61.2 58.1 0.7 6.9 (48)
Table 2: Connectivity and small-worldness measures for the Upper-bound, Baseline, and Cluster-based
network-growth methods; best performances across the Baseline and Cluster-based methods are shown
in bold. ?: co-efficient of correlation between similarities of word pairs in network and in gold-standard;
1
st
, 2
nd
, 3
rd
: median ranks of corresponding gold-standard associates given network similarities; size
lcc
:
proportion of network in the largest connected component; ?
g
: overall ?small-worldness?, should be
greater than 1; %: the percentage of runs whose resulting networks exhibit a small-world structure. Note
there are 1074 nouns in each network.
using the evolving knowledge of semantic con-
nections among words as well as their context of
usage enables the model to create a network that
shows the properties of adult semantic knowledge.
This suggests that the information in the semantic
relations among words and their context can effi-
ciently guide semantic network growth.
Acknowledgments
We would like to thank Varada Kolhatkar for valu-
able discussion and feedback. We are also grateful
for the financial support from NSERC of Canada,
and University of Toronto.
References
John R. Anderson and Michael Matessa. 1992. Ex-
plorations of an incremental, bayesian algorithm for
categorization. Machine Learning, 9(4):275?308.
Lois Bloom. 1973. One word at a time: The use of
single word utterances before syntax, volume 154.
Mouton The Hague.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word.
Allan M. Collins and Elizabeth F. Loftus. 1975. A
spreading-activation theory of semantic processing.
Psychological review, 82(6):407.
Eliana Colunga and Linda B. Smith. 2005. From the
lexicon to expectations about kinds: A role for asso-
ciative learning. Psychological Review, 112(2):347?
382.
Paul Erdos and Alfr?ed R?enyi. 1960. On the evolution
of random graphs. Publ. Math. Inst. Hungar. Acad.
Sci, 5:17?61.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science,
34(6):1017?1063.
Trevor Fountain and Mirella Lapata. 2011. Incremen-
tal models of natural language category acquisition.
In Proceedings of the 32st Annual Conference of the
Cognitive Science Society.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Using speakers referential inten-
tions to model early cross-situational word learning.
Psychological Science.
Michael C. Frank, Joshua B. Tenenbaum, and Anne
Fernald. 2013. Social and discourse contributions
to the determination of reference in cross-situational
word learning. Language Learning and Develop-
ment, 9(1):1?24.
Lila Gleitman. 1990. The structural sources of verb
meanings. Language Acquisition, 1(1):3?55.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological review, 114(2):211.
Michael W. Harm. 2002. Building large scale dis-
tributed semantic feature sets with WordNet. Tech-
nical Report PDP.CNS.02.1, Carnegie Mellon Uni-
versity.
253
Mark D. Humphries and Kevin Gurney. 2008. Net-
work small-world-ness: a quantitative method for
determining canonical network equivalence. PLoS
One, 3(4):e0002051.
Susan S. Jones and Linda B. Smith. 2005. Object name
learning and object perception: a deficit in late talk-
ers. J. of Child Language, 32:223?240.
Susan S. Jones, Linda B. Smith, and Barbara Landau.
1991. Object properties and knowledge in early lex-
ical learning. Child Development, 62(3):499?516.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk, volume 2: The Database.
Erlbaum, 3rd edition.
Radford M. Neal and Geoffrey E. Hinton. 1998. A
view of the em algorithm that justifies incremental,
sparse, and other variants. In Learning in graphical
models, pages 355?368. Springer.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for dirichlet process mixture models. Journal
of computational and graphical statistics, 9(2):249?
265.
Aida Nematzadeh, Afsaneh Fazly, and Suzanne
Stevenson. 2012. Interaction of word learning and
semantic category formation in late talking. In Proc.
of CogSci?12.
Aida Nematzadeh, Afsaneh Fazly, and Suzanne
Stevenson. 2014. Structural differences in the se-
mantic networks of simulated word learners.
Thierry Poibeau, Aline Villavicencio, Anna Korhonen,
and Afra Alishahi, 2013. Computational Modeling
as a Methodology for Studying Human Language
Learning. Springer.
Willard Van Orman Quine. 1960. Word and Object.
MIT Press.
Terry Regier. 2005. The emergence of words: Atten-
tional learning in form and meaning. Cognitive Sci-
ence, 29:819?865.
Garry Robins, Philippa Pattison, and Jodie Woolcock.
2005. Small and other worlds: Global network
structures from local processes1. American Journal
of Sociology, 110(4):894?936.
Larissa K. Samuelson and Linda B. Smith. 1999. Early
noun vocabularies: do ontology, category structure
and syntax correspond? Cognition, 73(1):1 ? 33.
Adam N. Sanborn, Thomas L. Griffiths, and Daniel J.
Navarro. 2010. Rational approximations to rational
models: alternative algorithms for category learning.
Jeffery Mark Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39?91.
Linda B. Smith and Chen Yu. 2008. Infants rapidly
learn word-referent mappings via cross-situational
statistics. Cognition, 106(3):1558?1568.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The
large-scale structure of semantic networks: Statisti-
cal analyses and a model of semantic growth. Cog-
nitive science, 29(1):41?78.
Anna L. Theakston, Elena V. Lieven, Julian M. Pine,
and Caroline F. Rowland. 2001. The role of
performance limitations in the acquisition of verb?
argument structure: An alternative account. J. of
Child Language, 28:127?152.
Duncan J. Watts and Steven H. Strogatz. 1998. Col-
lective dynamics of small-worldnetworks. nature,
393(6684):440?442.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138. Association for Com-
putational Linguistics.
Chen Yu and Dana H. Ballard. 2007. A unified
model of early word learning: Integrating statistical
and social cues. Neurocomputing, 70(1315):2149
? 2165. Selected papers from the 3rd Interna-
tional Conference on Development and Learning
(ICDL 2004), Time series prediction competition:
the CATS benchmark.
Chen Yu and Linda B. Smith. 2007. Rapid word learn-
ing under uncertainty via cross-situational statistics.
Psychological Science, 18(5):414?420.
254
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85?89,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Disambiguation of Image Captions
Wesley May, Sanja Fidler, Afsaneh Fazly, Sven Dickinson, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada, M5S 3G4
{wesley,fidler,afsaneh,sven,suzanne}@cs.toronto.edu
Abstract
Given a set of images with related captions,
our goal is to show how visual features can
improve the accuracy of unsupervised word
sense disambiguation when the textual con-
text is very small, as this sort of data is com-
mon in news and social media. We extend
previous work in unsupervised text-only dis-
ambiguation with methods that integrate text
and images. We construct a corpus by using
Amazon Mechanical Turk to caption sense-
tagged images gathered from ImageNet. Us-
ing a Yarowsky-inspired algorithm, we show
that gains can be made over text-only disam-
biguation, as well as multimodal approaches
such as Latent Dirichlet Allocation.
1 Introduction
We examine the problem of performing unsuper-
vised word sense disambiguation (WSD) in situa-
tions with little text, but where additional informa-
tion is available in the form of an image. Such situ-
ations include captioned newswire photos, and pic-
tures in social media where the textual context is of-
ten no larger than a tweet.
Unsupervised WSD has been shown to work very
well when the target word is embedded in a large
We thank NSERC and U. Toronto for financial support. Fi-
dler and Dickinson were sponsored by the Army Research Lab-
oratory and this research was accomplished in part under Co-
operative Agreement Number W911NF-10-2-0060. The views
and conclusions contained in this document are those of the au-
thors and should not be interpreted as representing the official
policies, either express or implied, of the Army Research Lab-
oratory or the U.S. Government.
Figure 1: ?The crane was so massive it blocked the sun.?
Which sense of crane? With images the answer is clear.
quantity of text (Yarowsky, 1995). However, if the
only available text is ?The crane was so massive it
blocked the sun? (see Fig. 1), then text-only dis-
ambiguation becomes much more difficult; a human
could do little more than guess. But if an image is
available, the intended sense is much clearer. We
develop an unsupervised WSD algorithm based on
Yarowsky?s that uses words in a short caption along
with ?visual words? from the captioned image to
choose the best of two possible senses of an ambigu-
ous keyword describing the content of the image.
Language-vision integration is a quickly develop-
ing field, and a number of researchers have explored
the possibility of combining text and visual features
in various multimodal tasks. Leong and Mihal-
cea (2011) explored semantic relatedness between
words and images to better exploit multimodal con-
tent. Jamieson et al (2009) and Feng and Lap-
ata (2010) combined text and vision to perform ef-
fective image annotation. Barnard and colleagues
(2003; 2005) showed that supervised WSD by could
be improved with visual features. Here we show that
unsupervised WSD can similarly be improved. Lo-
eff, Alm and Forsyth (2006) and Saenko and Darrell
(2008) combined visual and textual information to
solve a related task, image sense disambiguation, in
85
an unsupervised fashion. In Loeff et al?s work, little
gain was realized when visual features were added
to a great deal of text. We show that these features
have more utility with small textual contexts, and
that, when little text is available, our method is more
suitable than Saenko and Darrell?s.
2 Our Algorithm
We model our algorithm after Yarowsky?s (1995) al-
gorithm for unsupervised WSD: Given a set of doc-
uments that contain a certain ambiguous word, the
goal is to label each instance of that word as some
particular sense. A seed set of collocations that
strongly indicate one of the senses is initially used to
label a subset of the data. Yarowsky then finds new
collocations in the labelled data that are strongly as-
sociated with one of the current labels and applies
these to unlabelled data. This process repeats iter-
atively, building a decision list of collocations that
indicate a particular sense with a certain confidence.
In our algorithm (Algorithm 1), we have a docu-
ment collection D of images relevant to an ambigu-
ous keyword k with senses s1 and s2 (though the al-
gorithm is extensible to more than two senses). Such
a collection might result from an internet image
search using an ambiguous word such as ?mouse?.
Each Di is an image?caption pair repsented as a
bag-of-words that includes both lexical words from
the caption, and ?visual words? from the image. A
visual word is simply an abstract representation that
describes a small portion of an image, such that sim-
ilar portions in other images are represented by the
same visual word (see Section 3.2 for details). Our
seed sets consist of the words in the definitions of s1
and s2 from WordNet (Fellbaum, 1998). Any docu-
ment whose caption contains more words from one
sense definition than the other is initially labelled
with that sense. We then iterate between two steps
that (i) find additional words associated with s1 or
s2 in currently labelled data, and (ii) relabel all data
using the word sense associations discovered so far.
We let V be the entire vocabulary of words across
all documents. We run experiements both with and
without visual words, but when we use visual words,
they are included in V . In the first step, we com-
pute a confidence Ci for each word Vi. This con-
fidence is a log-ratio of the probability of seeing
Vi in documents labelled as s1 as opposed to doc-
uments labelled as s2. That is, a positive Ci indi-
cates greater association with s1, and vice versa. In
the second step we find, for each document Dj , the
word Vi ? Dj with the highest magnitude of Ci. If
the magnitude of Ci is above a labelling threshold
?c, then we label this document as s1 or s2 depend-
ing on the sign of Ci. Note that all old labels are dis-
carded before this step, so labelled documents may
become unlabelled, or even differently labelled, as
the algorithm progresses.
Algorithm 1 Proposed Algorithm
D: set of documents D1 ... Dd
V : set of lexical and visual words V1 ... Vv in D
Ci: log-confidence Vi is sense 1 vs. sense 2
S1 and S2: bag of dictionary words for each sense
L1 and L2: documents labelled as sense 1 or 2
for all Di do . Initial labelling using seed set
if |Di ? S1| > |Di ? S2| then
L1 ? L1 ? {Di}
else if |Di ? S1| < |Di ? S2| then
L2 ? L2 ? {Di}
end if
end for
repeat
for all i ? 1..v do . Update word conf.
Ci ? log
(
P (Vi|L1)
P (Vi|L2)
)
end for
L1 ? ?, L2 ? ? . Update document conf.
for all Di do
. Find word with highest confidence
m? argmax
j?1..v,Vj?Di
|Cj |
if Cm > ?c then
L1 ? L1 ? {Di}
else if Cm < ??c then
L2 ? L2 ? {Di}
end if
end for
until no change to L1 or L2
3 Creation of the Dataset
We require a collection of images with associated
captions. We also require sense annotations for
the keyword for each image to use for evalua-
tion. Barnard and Johnson (2005) developed the
86
?Music is an important
means of expression for
many teens.?
?Keeping your office sup-
plies organized is easy, with
the right tools.?
?The internet has opened up
the world to people of all
nationalities.?
?When there is no cheese I
will take over the world.?
Figure 2: Example image-caption pairs from our dataset,
for ?band? (top) and ?mouse? (bottom).
ImCor dataset by associating images from the Corel
database with text from the SemCor corpus (Miller
et al, 1993). Loeff et al (2006) and Saenko and
Darrell (2008) used Yahoo!?s image search to gather
images with their associated web pages. While these
datasets contain images paired with text, the textual
contexts are much larger than typical captions.
3.1 Captioning Images
To develop a large set of sense-annotated image?
caption pairs with a focus on caption-sized text, we
turned to ImageNet (Deng et al, 2009). ImageNet is
a database of images that are each associated with
a synset from WordNet. Hundreds of images are
available for each of a number of senses of a wide
variety of common nouns. To gather captions, we
used Amazon Mechanical Turk to collect five sen-
tences for each image. We chose two word senses
for each of 20 polysemous nouns and for each sense
we collected captions for 50 representative images.
For each image we gathered five captions, for a to-
tal of 10,000 captions. As we have five captions for
each image, we split our data into five sets. Each set
has the same images, but each image is paired with
a different caption in each set.
We specified to the Turkers that the sentences
should be relevant to, but should not talk directly
about, the image, as in ?In this picture there is a
blue fish?, as such captions are very unnatural. True
captions generally offer orthogonal information that
is not readily apparent from the image. The key-
word for each image (as specified by ImageNet) was
not presented to the Turkers, so the captions do not
necessarily contain it. Knowledge of the keyword is
presumed to be available to the algorithm in the form
of an image tag, or filename, or the like. We found
that forcing a certain word to be included in the cap-
tion also led to sentences that described the picture
very directly. Sentences were required to be a least
ten words long, and have acceptable grammar and
spelling. We remove stop words from the captions
and lemmatize the remaining words. See Figure 2
for some examples.
3.2 Computing the Visual Words
We compute visual words for each image with Ima-
geNet?s feature extractor. This extractor lays down
a grid of overlapping squares onto the image and
computes a SIFT descriptor (Lowe, 2004) for each
square. Each descriptor is a vector that encodes the
edge orientation information in a given square. The
descriptors are computed at three scales: 1x, 0.5x
and 0.25x the original side lengths. These vectors
are clustered with k-means into 1000 clusters, and
the labels of these clusters (arbitrary integers from 1
to 1000) serve as our visual words.
It is common for each image to have a ?vocab-
ulary? of over 300 distinct visual words, many of
which only occur once. To denoise the visual data,
we use only those visual words which account for at
least 1% of the total visual words for that image.
4 Experiments and Results
To show that the addition of visual features improves
the accuracy of sense disambiguation for image?
caption pairs, we run our algorithm both with and
without the visual features. We also compare our re-
sults to three different baseline methods: K-means
(K-M), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), and an unsupervised WSD algorithm
(PBP) explained below. We use accuracy to measure
performance as it is commonly used by the WSD
community (See Table 1).
For K-means, we set k = 2 as we have two senses,
and represent each document with a V -dimensional
87
Table 1: Results (Average accuracy across all five sets of
data). Bold indicates best performance for that word.
Ours Ours K-M K-M LDA LDA PBP
text w/vis text w/vis text w/vis text
band .80 .82 .66 .65 .64 .56 .73
bank .77 .78 .71 .59 .52 .67 .62
bass .94 .94 .90 .88 .61 .62 .49
chip .90 .90 .73 .58 .57 .66 .75
clip .70 .79 .65 .58 .48 .53 .65
club .80 .84 .80 .81 .61 .73 .63
court .79 .79 .61 .53 .62 .82 .57
crane .62 .67 .76 .76 .52 .54 .66
game .78 .78 .60 .66 .60 .66 .70
hood .74 .73 .73 .70 .51 .45 .55
jack .76 .74 .62 .53 .58 .66 .47
key .81 .92 .79 .54 .57 .70 .50
mold .67 .68 .59 .67 .57 .66 .54
mouse .84 .84 .71 .62 .62 .69 .68
plant .54 .54 .56 .53 .52 .50 .72
press .60 .59 .60 .54 .58 .62 .48
seal .70 .80 .61 .67 .55 .53 .62
speaker .70 .69 .57 .53 .55 .62 .63
squash .89 .95 .84 .92 .55 .67 .79
track .78 .85 .71 .66 .51 .54 .69
avg. .76 .78 .69 .65 .56 .63 .62
vector, where the ith element is the proportion of
word Vi in the document. We run K-means both with
and without visual features.
For LDA, we use the dictionary sense model from
Saenko and Darrell (2008). A topic model is learned
where the relatedness of a topic to a sense is based
on the probabilities of that topic generating the seed
words from its dictionary definitions. Analogously
to k-means, we learn a model for text alone, and a
model for text augmented with visual information.
For unsupervised WSD (applied to text only),
we use WordNet::SenseRelate::TargetWord, here-
after PBP (Patwardhan et al, 2007), the highest
scoring unsupervised lexical sample word sense dis-
ambiguation algorithm at SemEval07 (Pradhan et
al., 2007). PBP treats the nearby words around the
target word as a bag, and uses the WordNet hierar-
chy to assign a similarity score between the possible
senses of words in the context, and possible senses
of the target word. As our captions are fairly short,
we use the entire caption as context.
The most important result is the gain in accuracy
after adding visual features. While the average gain
across all words is slight, it is significant at p < 0.02
(using a paired t-test). For 12 of the 20 words, the
visual features improve performance, and in 6 of
those, the improvement is 5?11%.
For some words there is no significant improve-
ment in accuracy, or even a slight decrease. With
words like ?bass? or ?chip? there is little room to
improve upon the text-only result. For words like
?plant? or ?press? it seems the text-only result is not
strong enough to help bootstrap the visual features
in any useful way. In other cases where little im-
provement is seen, the problem may lie with high
intra-class variation, as our visual words are not very
robust features, or with a lack of orthogonality be-
tween the lexical and visual information.
Our algorithm also performs significantly better
than the baseline measurements. K-means performs
surprisingly well compared to the other baselines,
but seems unable to make much sense of the visual
information present. Saenko and Darrell?s (2008)
LDA model makes substansial gains by using vi-
sual features, but does not perform as well on this
task. We suspect that a strict adherence to the seed
words may be to blame: while both this LDA model
and our algorithm use the same seed definitions ini-
tially, our algorithm is free to change its mind about
the usefulness of the words in the definitions as it
progresses, whereas the LDA model has no such
capacity. Indeed, words that are intuitively non-
discriminative, such as ?carry?, ?lack?, or ?late?, are
not uncommon in the definitions we use.
5 Conclusion and Future Work
We present an approach to unsupervised WSD that
works jointly with the visual and textual domains.
We showed that this multimodal approach makes
gains over text-only disambiguation, and outper-
forms previous approaches for WSD (both text-only,
and multimodal), when textual contexts are limited.
This project is still in progress, and there are many
avenues for further study. We do not currently ex-
ploit collocations between lexical and visual infor-
mation. Also, the bag-of-SIFT visual features that
we use, while effective, have little semantic content.
More structured representations over segmented im-
age regions offer greater potential for encoding se-
mantic content (Duygulu et al, 2002).
88
References
Kobus Barnard and Matthew Johnson. 2005. Word
sense disambiguation with pictures. In Artificial In-
telligence, volume 167, pages 13?130.
Kobus Barnard, Matthew Johnson, and David Forsyth.
2003. Word sense disambiguation with pictures.
In Workshop on Learning Word Meaning from Non-
Linguistic Data, Edmonton, Canada.
David M. Blei, Andrew Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. In JMLR, volume 3, pages
993?1022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hierar-
chical image database. In IEEE Conference on Com-
puter Vision and Pattern Recognition.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image vo-
cabulary. In European Conference on Computer Vi-
sion, Copenhagen, Denmark.
Christiane Fellbaum. 1998. Wordnet: An electronic lex-
ical database. In Bradford Books.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Annual
Conference of the North American Chapter of the ACL,
pages 831?839, Los Angeles, California.
Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson,
Sven Dickinson, and Sven Wachsmuth. 2009. Using
language to learn structured appearance models for im-
age annotation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 32(1):148?164.
Chee Wee Leong and Rada Mihalcea. 2011. Measuring
the semantic relatedness between words and images.
In International Conference on Semantic Computing,
Oxford, UK.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David
Forsyth. 2006. Discriminating image senses by clus-
tering with multimodal features. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 547?554, Sydney, Australia.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91?110.
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In Proceed-
ings of the 3rd DARPA Workshop on Human Language
Technology, pages 303?308.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2007. UMND1: Unsupervised word sense
disambiguation using contextual semantic relatedness.
In Proceedings of SemEval-2007, pages 390?393,
Prague, Czech Republic.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Task 17: English lexical sam-
ple, SRL and all words. In Proceedings of SemEval-
2007, pages 87?92, Prague, Czech Republic.
Kate Saenko and Trevor Darrell. 2008. Unsupervised
learning of visual sense models for polysemous words.
In Proceedings of Neural Information Processing Sys-
tems, Vancouver, Canada.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the ACL, pages
189?196, Cambridge, Massachusetts.
89
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57?64
Manchester, August 2008
Fast Mapping in Word Learning: What Probabilities Tell Us
Afra Alishahi and Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
{afra,afsaneh,suzanne}@cs.toronto.edu
Abstract
Children can determine the meaning of a
new word from hearing it used in a familiar
context?an ability often referred to as fast
mapping. In this paper, we study fast map-
ping in the context of a general probabilistic
model of word learning. We use our model
to simulate fast mapping experiments on chil-
dren, such as referent selection and retention.
The word learning model can perform these
tasks through an inductive interpretation of
the acquired probabilities. Our results suggest
that fast mapping occurs as a natural conse-
quence of learning more words, and provides
explanations for the (occasionally contradic-
tory) child experimental data.
1 Fast Mapping
An average six-year-old child knows over 14, 000
words, most of which s/he has learned from hearing
other people use them in ambiguous contexts (Carey,
1978). Children are thus assumed to be equipped with
powerful mechanisms for performing such a complex
task so efficiently. One interesting ability children as
young as two years of age show is that of correctly and
immediately mapping a novel word to a novel object
in the presence of other familiar objects. The term
?fast mapping? was first used by Carey and Bartlett
(1978) to refer to this phenomenon.
Carey and Bartlett?s goal was to examine how much
children learn about a word when presented in an am-
biguous context, as opposed to concentrated teaching.
They used an unfamiliar name (chromium) to refer to
an unfamiliar color (olive green), and then asked
a group of four-year-old children to select an object
from among a set, upon hearing a sentence explicitly
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
asking for the object of the new color, as in: bring
the chromium tray, not the blue one. Children were
generally good at performing this ?referent selection?
task. In a production task performed six weeks later,
when children had to use the name of the new color,
they showed signs of having learned something about
the new color name, but were not successful at pro-
ducing it. On the basis of these findings, Carey and
Bartlett suggest that fast mapping and word learning
are two distinct, yet related, processes.
Extending Carey and Bartlett?s work, much re-
search has concentrated on providing an explanation
for fast mapping, and on examining its role in word
learning. These studies also show that children are
generally good at referent selection, given a novel tar-
get. However, there is not consistent evidence regard-
ing whether children actually learn the novel word
from one or a few such exposures (retention). For
example, whereas the children in the experiments of
Golinkoff et al (1992) and Halberda (2006) showed
signs of nearly-perfect retention of the fast-mapped
words, those in the studies reported by Horst and
Samuelson (2008) did not (all participating children
were close in age range).
There are also many speculations about the possible
causes of fast mapping. Some researchers consider
it as a sign of a specialized (innate) mechanism for
word learning. Markman and Wachtel (1988), for ex-
ample, argue that children fast map because they ex-
pect each object to have only one name (mutual exclu-
sivity). Golinkoff et al (1992) attribute fast mapping
to a (hard-coded) bias towards mapping novel names
to nameless object categories. Some even suggest a
change in children?s learning mechanisms, at around
the time they start to show evidence of fast mapping
(which coincides with a sudden burst in their vocab-
ulary), e.g., from associative to referential (Gopnik
and Meltzoff, 1987; Reznick and Goldfield, 1992). In
contrast, others see fast mapping as a phenomenon
that arises from more general processes of learning
57
and/or communication, which also underlie the im-
pressive rate of lexical acquisition in children (e.g.,
Clark, 1990; Diesendruck and Markson, 2001; Regier,
2005; Horst et al, 2006; Halberda, 2006).
In our previous work (Fazly et al, 2008), we pre-
sented a word learning model which proposes a prob-
abilistic interpretation of cross-situational learning,
and bootstraps its own partially-learned knowledge of
the word meanings to accelerate word learning over
time. We have shown that the model can learn reason-
able word?meaning associations from child-directed
data, and that it accounts for observed learning pat-
terns in children, such as vocabulary spurt, without
requiring a developmental change in the underlying
learning mechanism. Here, we use this computational
model to investigate fast mapping and its relation to
word learning. Specifically, we take a close look at
the onset of fast mapping in our model by simulat-
ing some of the psychological experiments mentioned
above. We examine the behaviour of the model in var-
ious referent selection and retention tasks, and pro-
vide explanations for the (occasionally contradictory)
experimental results reported in the literature. We also
study the effect of exposure to more input on the per-
formance of the model in fast mapping.
Our results suggest that fast mapping can be ex-
plained as an induction process over the acquired as-
sociations between words and meanings. Our model
learns these associations in the form of probabilities
within a unified framework; however, we argue that
different interpretations of such probabilities may be
involved in choosing the referent of a familiar as op-
posed to a novel target word (as noted by Halberda,
2006). Moreover, the overall behaviour of our model
confirms that the probabilistic bootstrapping approach
to word learning naturally leads to the onset of fast
mapping in the course of lexical development, with-
out hard-coding any specialized learning mechanism
into the model to account for this phenomenon.
2 Overview of the Computational Model
This section summarizes the model presented in Fa-
zly et al (2008). Our word learning algorithm is an
adaptation of the IBM translation model proposed by
Brown et al (1993). However, our model is incre-
mental, and does not require a batch process over the
entire data.
2.1 Utterance and Meaning Representations
The input to our word learning model consists of a set
of utterance?scene pairs that link an observed scene
(what the child perceives) to the utterance that de-
scribes it (what the child hears). We represent each
utterance as a sequence of words, and the correspond-
ing scene as a set of meaning symbols. To simulate
referential uncertainty (i.e., the case where the child
perceives aspects of the scene that are unrelated to the
perceived utterance), we include additional symbols
in the representation of the scene, e.g.:
Utterance: Joe rolled the ball
Scene: {joe, roll, the, ball, mommy, hand, talk}
In Section 3.1, we explain how the utterances and
the corresponding semantic symbols are selected, and
how we add referential uncertainty.
Given a corpus of such utterance?scene pairs, our
model learns the meaning of each word w as a prob-
ability distribution, p(.|w), over the semantic sym-
bols appearing in the corpus. In this representation,
p(m|w) is the probability of a symbol m being the
meaning of a word w. In the absence of any prior
knowledge, all symbols are equally likely to be the
meaning of a word. Hence, prior to receiving any us-
ages of a given word, the model assumes a uniform
distribution over semantic symbols as its meaning.
2.2 Meaning Probabilities
Our model combines probabilistic interpretations of
cross-situational learning (Quine, 1960) and of a
variation of the principle of contrast (Clark, 1990),
through an interaction between two types of prob-
abilistic knowledge acquired and refined over time.
Given an utterance?scene pair received at time t, i.e.,
(U
(t)
, S
(t)
), the model first calculates an alignment
probability a for each w ? U(t) and each m ? S(t),
using the meaning probabilities p(.|w) of all the
words in the utterance prior to this time. The model
then revises the meaning of the words in U(t) by in-
corporating the alignment probabilities for the current
input pair. This process is repeated for all the input
pairs, one at a time.
Step 1: Calculating the alignment probabilities.
We estimate the alignment probabilities of words
and meaning symbols based on a localized version
of the principle of contrast: that a meaning sym-
bol in a scene is likely to be highly associated with
only one of the words in the corresponding utter-
ance.1 For a symbol m ? S(t) and a word w ? U(t),
the higher the probability of m being the meaning
of w (according to p(m|w)), the more likely it is
that m is aligned with w in the current input. In
other words, a(w |m, U(t), S(t)) is proportional to
p
(t?1)
(m|w). In addition, if there is strong evidence
that m is the meaning of another word in U(t)?
i.e., if p(t?1)(m|w?) is high for some w? ? U(t) other
1Note that this differs from what is widely known as the prin-
ciple of contrast (Clark, 1990), in that the latter assumes contrast
across the entire vocabulary rather than within an utterance.
58
than w?the likelihood of aligning m to w should de-
crease. Combining these two requirements:
a(w |m, U
(t)
, S
(t)
) =
p
(t?1)
(m|w)
?
w
?
?U
(t)
p
(t?1)
(m|w
?
)
(1)
Due to referential uncertainty, some of the meaning
symbols in the scene might not have a counterpart
in the utterance. To accommodate for such cases, a
dummy word is added to each utterance before the
alignment probabilities are calculated, in order to let
a meaning symbol not be (strongly) aligned with any
of the words in the current utterance.
Step 2: Updating the word meanings. We need to
update the probabilities p(.|w) for all words w ? U(t),
based on the evidence from the current input pair re-
flected in the alignment probabilities. We thus add
the current alignment probabilities for w and the sym-
bols m ? S(t) to the accumulated evidence from prior
co-occurrences of w and m. We summarize this
cross-situational evidence in the form of an associa-
tion score, which is updated incrementally:
assoc
(t)
(w, m) = assoc
(t?1)
(w, m) +
a(w|m, U
(t)
, S
(t)
) (2)
where assoc(t?1)(w, m) is zero if w and m have not
co-occurred before. The association score of a word
and a symbol is basically a weighted sum of their co-
occurrence counts.
The model then uses these association scores to up-
date the meaning of the words in the current input:
p
(t)
(m|w) =
assoc
(t)
(m, w) + ?
?
m
j
?M
assoc
(t)
(m
j
, w) + ? ? ?
(3)
where M is the set of all symbols encountered prior to
or at time t, ? is the expected number of symbol types,
and ? is a small smoothing factor. The denominator is
a normalization factor to get valid probabilities. This
formulation results in a uniform probability of 1/?
over all m ? M for a novel word w, and a probability
smaller than ? for a meaning symbol m that has not
been previously seen with a familiar word w.
Our model updates the meaning of a word ev-
ery time it is heard in an utterance. The strength
of learning of a word at time t is reflected in
p
(t)
(m = m
w
|w), where m
w
is the ?correct? mean-
ing of w: for a learned word w, the probability dis-
tribution p(.|w) is highly skewed towards the correct
meaning m
w
, and therefore hearing w will trigger the
retrieval of the meaning m
w
.
2
2An input-generation lexicon contains the correct meaning for
each word, as described in Section 3.1. Note that the model does
not have access to this lexicon for learning; it is used only for
input generation and evaluation.
From this point on, we simply use p(m|w) (omit-
ting the superscript (t)) to refer to the meaning prob-
ability of m for w at the present time of learning.
2.3 Referent Probabilities
The meaning probability p(m|w) is used to retrieve
the most probable meaning for w among all the possi-
ble meaning symbols m. However, in the referent se-
lection tasks performed by children, the subject is of-
ten forced to select the referent of a target word from
among a limited set of objects, even when the mean-
ing of the target word has not been accurately learned
yet. For our model to perform such tasks, it has to de-
cide how likely it is for a target word w to refer to a
particular object m, based on its previous knowledge
about the mapping between m and w (i.e., p(m|w)),
as well as the mapping between m and other words in
the lexicon.3
The likelihood of using a particular name w to refer
to a given object m is calculated as:
rf (w|m) = p(w|m)
=
p(m|w) ? p(w)
p(m)
=
p(m|w) ? p(w)
?
w
?
?V
p(m|w
?
) ? p(w
?
)
(4)
where V is the set of all words that the model has seen
so far, and p(w) is the relative frequency of w:
p(w) =
freq(w)
?
w
?
?V
freq(w
?
)
(5)
The referent of a target word w among the present ob-
jects, therefore, will be the object m with the highest
referent probability rf (w|m).
3 Experimental Setup
3.1 The Input Corpora
We extract utterances from the Manchester corpus
(Theakston et al, 2001) in the CHILDES database
(MacWhinney, 2000). This corpus contains tran-
scripts of conversations with children between the
ages of 1; 8 and 3; 0 (years;months). We use the
mother?s speech from transcripts of 6 children, re-
move punctuation and lemmatize the words, and con-
catenate the corresponding sessions as input data.
There is no semantic representation of the corre-
sponding scenes available from CHILDES. There-
fore, we automatically construct a scene representa-
tion for each utterance, as a set containing the seman-
tic referents of the words in that utterance. We get
these from an input-generation lexicon that contains
a symbol associated with each word as its semantic
3All through the paper, we use m as both the meaning and the
referent of a word w.
59
referent. We use every other sentence from the orig-
inal corpus, preserving their chronological order. To
simulate referential uncertainty in the input, we then
pair each sentence with its own scene representation
as well as that of the following sentence in the origi-
nal corpus. (Note that the latter sentence is not used
as an utterance in our input.) The extra semantic sym-
bols that are added to each utterance thus correspond
to meaningful semantic representations, as opposed
to randomly selected symbols. In the resulting corpus
of 92, 239 input pairs, each utterance is, on average,
paired with 78% extra meaning symbols, reflecting a
high degree of referential uncertianty.
3.2 The Model Parameters
We set the parameters of our learning algorithm using
a development data set which is similar to our training
and test data, but is selected from a non-overlapping
portion of the Manchester corpus. The expected num-
ber of symbols, ? in Eq. (3), is set to 8500 based on
the total number of distinct symbols extracted for the
development data. Therefore, the default probability
of a symbol for a novel word will be 1/8500. A famil-
iar word, on the other hand, has been seen with some
symbols before. Therefore, the probability of a previ-
ously unseen symbol for it (which, based on Eq. (3),
has an upper bound of ?) must be less than the default
probability mentioned above. Accordingly, we set ?
to 10?5.
3.3 The Training Procedure
In the next section, we report results from the com-
putational simulation of our model for a number of
experiments. All of the simulations use the same pa-
rameter settings (as described in the previous section),
but different input: in each simulation, a random por-
tion of 1000 utterance?scene pairs is selected from
the input corpus, and incrementally processed by the
model. The size of the training corpus is chosen arbi-
trarily to reflect a sample point in learning, and further
experiments have shown that increasing this number
does not change the pattern observed in the results. In
order to avoid behaviour that is specific to a particu-
lar sequence of input items, the reported results in the
next section are averaged over 10 simulations.
4 Experimental Results and Analysis
4.1 Referent Selection
In a typical word learning scenario, the child faces
a scene where a number of familiar and unfamiliar
objects are present. The child then hears a sentence,
which describes (some part of) the scene, and is com-
posed of familiar and novel words (e.g., hearing Joe is
eating a cheem, where cheem is a previously unseen
fruit). In such a setting, our model aligns the objects
in the scene with the words in the utterance based on
its acquired knowledge of word meanings, and then
updates the meanings of the words accordingly. The
model can align a familiar word with its referent with
high confidence, since the previously learned mean-
ing probability of the familiar object given the famil-
iar word, or p(m|w), is much higher than the meaning
probability of the same object given any other word in
the sentence. In a similar fashion, the model can eas-
ily align a novel word in the sentence with a novel
object in the scene, because the meaning probability
of the novel object given the novel word (1/?, ac-
cording to Eq. (3)) is higher than the meaning proba-
bility of that object for any previously heard word in
the sentence (the latter probability is smaller than ? in
Eq. (3), as explained in Section 3.2).
Earlier fast mapping experiments on children as-
sumed that it is such a contrast between the familiar
and novel words in the same sentence that helps chil-
dren select the correct target object in a referent selec-
tion task. For example, in Carey and Bartlett?s (1978)
experiment, to introduce a novel word?meaning as-
sociation (e.g., chromium?olive), the authors use
both the familiar and the novel words in one sentence
(bring me the chromium tray, not the blue one.). How-
ever, further experiments show that children can suc-
cessfully select the correct referent even if such a con-
trast is not present in the sentence. Many researchers
have performed experiments where young subjects
are forced to choose between a novel and a familiar
object upon hearing a request, such as give me the
ball (familiar target), or give me the dax (novel tar-
get). In all of the reported experimental results, chil-
dren can readily pick the correct referent for a famil-
iar or a novel target word in such a setting (Golinkoff
et al, 1992; Halberda and Goldman, 2008; Halberda,
2006; Horst and Samuelson, 2008).
However, Halberda?s eye-tracking experiments on
both adults and pre-schoolers suggest that the pro-
cesses involved for referent selection in the familiar
target situation may be different from those in the
novel target situation. In the latter situation, subjects
appear to systematically reject the familiar object as
the referent of the novel name before mapping the
novel object to the novel name. In the familiar target
situation, however, there is no need to reject the novel
distractor object, because the subject already knows
the referent of the target.
The difference between these two conditions can be
explained in terms of the meaning and referent proba-
bilities of our model explained in Section 2. In a typi-
cal referent selection experiment, the child is asked to
60
?get the ball? while facing a ball and a novel object
(dax). We assume that the child knows the meaning
of verbs and determiners such as get and the, therefore
we simplify the familiar target condition in the form
of the following input item:
ball (FAMILIAR TARGET)
{ball, dax}
A familiar word such as ball has a meaning prob-
ability highly skewed towards its correct meaning.
That is, upon hearing ball, the model can confidently
retrieve its meaning ball, which is the one with
the highest probability p(m|ball) among all possible
meanings m. In such a case, if ball is present in the
scene, the model can easily pick it as the referent of
the familiar target name, without processing the other
objects in the scene.
Now consider the condition where a novel target
name is used in the presence of a familiar and a pre-
viously unseen object:
dax (NOVEL TARGET)
{ball, dax}
Since this is the first time the model has heard the
word dax, both meanings ball and dax are equally
likely because p(.|dax ) is uniform. Thus the mean-
ing probabilities cannot be solely used for selecting
the referent of dax, and the model has to perform
some kind of induction on the potential referents in
the scene based on what it has learned about each
of them. The model can infer the referent of dax
by comparing the referent probabilities rf (dax |ball)
and rf (dax |dax) from Eq. (4) after processing the in-
put item. Since ball has strong associations with an-
other word ball, its referent probability for the novel
name dax is much lower than the referent probability
of dax, which does not have strong associations with
any of the words in the learned lexicon.
We simulate the process of referent selection in our
model as follows. We train the model as described
in Section 3.3. We then present the model with one
more input item, which represents either the FAMIL-
IAR TARGET or the NOVEL TARGET condition. For
each condition, we compare the meaning probability
p(object|target) for both familiar and novel objects
in the scene (see Table 1, top panel). In the FA-
MILIAR TARGET condition, the model demonstrates
a strong preference towards choosing the familiar ob-
ject as the referent, whereas in the NOVEL TARGET
condition, the model shows no preference towards any
of the objects based on the meaning probabilities of
the target word. Therefore, for the NOVEL TARGET
condition, we also compare the referent probabilities
rf (target |object) for both objects after processing
Table 1: Referent selection in FAMILIAR and NOVEL
TARGET conditions.
UPON HEARING THE TARGET WORD
Condition p(ball|target ) p(dax|target )
FAMILIAR TARGET 0.843 ?0.056 ? 0.0001
NOVEL TARGET 0.0001 ?0.00 0.0001 ?0.00
AFTER PERFORMING INDUCTION
Condition rf (target |ball) rf (target |dax)
NOVEL TARGET 0.127 ?0.127 0.993 ?0.002
the input item as a training pair, simulating the in-
duction process that humans go through to select the
referent in such cases. This time, the model shows a
strong preference towards the novel object as the ref-
erent of the target word (see Table 1, bottom panel).
Our results confirm that in both conditions, the model
consistently selects the correct referent for the target
word across all the simulations.
4.2 Retention
As discussed in the previous section, results from
the human experiments as well as our computational
simulations show that the referent of a novel target
word can be selected based on the previous knowl-
edge about the present objects and their names. How-
ever, the success of a subject in a referent selection
task does not necessarily mean that the child/model
has learned the meaning of the novel word based on
that one trial. In order to better understand what and
how much children learn about a novel word from a
single ambiguous exposure, some studies have per-
formed retention trials after the referent selection ex-
periments. Often, various referent selection trials are
performed in one session, where in each trial a novel
object?name pair is introduced among familiar ob-
jects. Some of the recently introduced objects are
then put together in one last trial, and the subjects
are asked to choose the correct referent for one of the
(recently heard) novel target words. The majority of
the reported experiments show that children can suc-
cessfully perform the retention task (Golinkoff et al,
1992; Halberda and Goldman, 2008; Halberda, 2006).
We simulate a similar retention experiment by
training the model as usual. We further present the
model with two experimental training items similar to
the one used in the NOVEL TARGET condition in the
previous section, with different familiar and novel ob-
jects and words in each input:
dax (REFERENT SELECTION TRIAL 1)
{ball, dax}
cheem (REFERENT SELECTION TRIAL 2)
{pen, cheem}
61
Table 2: Retention of a novel target word from a set
of novel objects.
2-OBJECT RETENTION TRIAL
rf (dax |dax) rf (dax |cheem)
0.996 ?0.001 0.501 ?0.068
3-OBJECT RETENTION TRIAL
rf (dax |dax) rf (dax |cheem) rf (dax |lukk)
0.995 ?0.001 0.407 ?0.062 0.990 ?0.001
The training session is followed by a retention trial,
where the two novel objects used in the previous ex-
perimental inputs are paired with one of the novel tar-
get words:
dax (2-OBJECT RETENTION TRIAL)
{cheem, dax}
After processing the retention input, we com-
pare the referent probabilities rf (dax |cheem) and
rf (dax |dax) to see if the model can choose the cor-
rect novel object in response to the target word dax.
The top panel in Table 2 summarizes the results of this
experiment. The model consistently shows a strong
preference towards the correct novel object as the ref-
erent of the novel target word across all simulations.
Unlike studies on referent selection, experimental
results for retention have not been consistent across
various studies. Horst and Samuelson (2008) per-
form experiments with two-year-old children involv-
ing both referent selection and retention, and report
that their subjects perform very poorly at the retention
task. One factor that discriminates the experimental
setup of Horst and Samuelson from others (e.g., Hal-
berda, 2006) is that, in their retention trials, they put
together two recently observed novel objects with a
third novel object that has not been seen in any of the
experimental sessions before. The authors do not at-
tribute their contradictory results to the presence of
this third object, but this factor can in fact affect the
performance considerably. We simulate this condition
by using the same input items for referent selection
trials as in the previous simulation, but we replace the
retention trial with the following:
dax (3-OBJECT RETENTION TRIAL)
{cheem, dax, lukk}
The third object, lukk, has not been seen by the
model before. Results under the new condition are re-
ported in the bottom panel of Table 2. As can be seen,
the model shows a strong tendency towards the cor-
rect novel referent dax for the novel target dax, com-
pared to the other recently seen novel object cheem.
However, the probability of the unseen object lukk
is also very high for the target word dax. That is be-
cause the model cannot use any previously acquired
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
x 104
0
5
10
15
20
25
30
35
time of first exposure
nu
m
be
r o
f u
sa
ge
s n
ee
de
d 
to
 le
ar
n
Figure 1: Number of usages needed to learn a word,
as a function of the word?s age of exposure.
knowledge about lukk (i.e., associating it with an-
other word) to rule it out as a referent for dax. These
results show that introducing a new object for the first
time in a retention trial considerably increases the dif-
ficulty of the task. This can explain the contradictory
results reported in the literature: when the referent
probabilities are not informative, other factors may
influence the outcome of the experiment, such as the
amount of training received for a novel word?object,
or a possible delay between training and test sessions.
4.3 The Effect of Exposure to More Input
The fast mapping ability observed in children implies
that once children have learned a repository of words,
they can easily link novel words to novel objects in a
familiar context based only on a few exposures. We
examine this effect in our model: we train the model
on 20, 000 input pairs, looking at the relation between
the time of first exposure to a word, and the number
of usages that the model needs for learning that word.
Figure 1 plots this for words that have been learned at
some point during the training.4 We can see that the
model shows clear fast mapping behaviour?that is,
words received later in time, on average, require fewer
usages to be learned. These results show that our
model exhibits fast mapping patterns once it has been
exposed to enough word usages, and that no change
in the underlying learning mechanism is needed.5
The effect of exposure to more input on fast map-
ping can be described in terms of context familiarity:
the more input the model has processed so far, the
more likely it is that the context of the usage of a novel
word (the other words in the sentence and the objects
in the scene) is familiar to the model. This pattern
has been studied through a number of experiments on
4We consider a word w as learned if the meaning probability
p(m
w
|w) is higher than a certain threshold ?. For this experi-
ment, we set ? = 0.70.
5In Fazly et al (2008), we reported a variation of this exper-
iment, where we used a smaller training set, and also a different
semantic representation for word meanings.
62
children. For example, Gershkoff-Stowe and Hahn
(2007) taught 16- to 18-month-olds the names of 24
unfamiliar objects over 12 training sessions, where
unfamiliar objects were presented with varying fre-
quency. Data were compared to a control group of
children who were exposed to the same experimen-
tal words at the first and last sessions only. Their re-
sults show that for children in the experimental group,
extended practice with a novel set of words led to
the rapid acquisition of a second set of low-practice
words. Children in the control group did not show the
same lexical advantage.
Inspired by Gershkoff-Stowe and Hahn (2007), we
perform an experiment to study the effect of con-
text familiarity on fast mapping in our model. We
choose two sets of words, CONTEXT (containing 20
words) and TARGET (containing 10 words), to con-
duct a referent selection task as follows. First, we
train our model on a sequence of utterance?scene
pairs constructed from the set CONTEXT ? TARGET,
as follows: the unified set is randomly shuffled and
divided into two subsets, words in each subset are
put together to form an utterance, and the meanings
of the words in that utterance are put together to
form the corresponding scene. We repeat this process
twice, so that each word appears in exactly two input
pairs. We train our model on the constructed pairs.6
Next, we perform a referent selection task on each
word in the TARGET set: we pair each target word
w with the meaning of 10 randomly selected words
from CONTEXT ? TARGET, including the meaning of
the target word itself (m
w
), and have the model pro-
cess this test pair. We compare the referent probabil-
ity of w and each m ? CONTEXT ? TARGET to see
whether the model can correctly map the target word
to its referent. We call this setting the LOW TRAIN-
ING condition.
In the above setting, the context words in the ref-
erent selection trials are as new to the model as the
target words. We thus repeat this experiment with
a familiar context: we first train the model over in-
put pairs that are randomly constructed from words
in CONTEXT only, using the same training proce-
dure as described above. This context-familiarization
process is followed by a similar training session on
CONTEXT ? TARGET, and a test session on target
words, similar to the previous condition. Again, we
count the number of correct mappings between a tar-
get word and its referent based on the referent proba-
bilities. We call this setting the HIGH TRAINING con-
dition. Table 3 shows the results for both conditions.
It can be seen that the accuracy of finding the referent
6Unlike in previous experiments, here we do not use child-
directed data as we want to control the familiarity of the context.
Table 3: Average number of correct mappings and the
referent probabilities of target words for two condi-
tions, LOW and HIGH TRAINING.
Condition Correct mappings P (target |m
target
)
LOW TRAINING %54 0.216?0.04
HIGH TRAINING %90 0.494?0.79
for a target word, as well as the referent probability of
a target word for its correct meaning, increase as a re-
sult of more training on the context. In other words, a
more familiar context helps the model perform better
in a fast mapping task.
5 Related Computational Models
The rule-based model of Siskind (1996), and the con-
nectionist model proposed by Regier (2005), both
show that learning gets easier as the model is exposed
to more input?that is, words heard later are learned
faster. These findings confirm that fast mapping may
simply be a result of learning more words, and that
no explicit change in the underlying learning mech-
anism is needed. However, these studies do not ex-
amine various aspects of fast mapping, such as ref-
erent selection and retention. Horst et al (2006) ex-
plicitly test fast mapping in their connectionist model
of word learning by performing referent selection and
retention tasks. The behaviour of their model matches
the child experimental data reported in a study by the
same authors (Horst and Samuelson, 2008), but not
that of the contradictory findings of other similar ex-
periments. Moreover, the model?s learning capacity
is limited, and the fast mapping experiments are per-
formed on a very small vocabulary. Frank et al (2007)
examine fast mapping in their Bayesian model by test-
ing its performance in a novel target referent selection
task. However, the experiment is performed on an ar-
tifical corpus. Moreover, since the learning algorithm
is non-incremental, the success of the model in refer-
ent selection is determined implicitly: each possible
word?meaning mapping from the test input is added
to the current lexicon, and the consistency of the new
lexicon is checked against the training corpus.
6 Discussion and Concluding Remarks
We have used a general computational model of word
learning (first introduced in Fazly et al, 2008) to study
fast mapping. Our model learns a probabilistic asso-
ciation between a word and its meaning, from expo-
sure to word usages in naturalistic contexts. We have
shown that these probabilities can be used to simu-
late various fast mapping experiments performed on
children, such as referent selection and retention. Our
63
experimental results suggest that fast mapping can be
explained as an induction process over the acquired
associations between words and objects. In that sense,
fast mapping is a general cognitive ability, and not
a hard-coded, specialized mechanism of word learn-
ing.7 In addition, our results confirm that the onset
of fast mapping is a natural consequence of learning
more words, which in turn accelerates the learning of
new words. This bootstrapping approach results in a
rapid pace of vocabulary acquisition in children, with-
out requiring a developmental change in the underly-
ing learning mechanism.
Results of the referent selection experiments show
that our model can successfully find the referent of
a novel target word in a familiar context. Moreover,
our retention experiments show that the model can
map a recently heard novel word to its recently seen
novel referent (among other novel objects) after only
one exposure. However, the strength of the associa-
tion of a novel pair after one exposure shows a no-
table difference compared to the association between
a ?typical? familiar word and its meaning.8 This is
consistent with what is commonly assumed in the lit-
erature: even though children learn something about
a word from only one exposure, they often need more
exposure to reliably learn its meaning (Carey, 1978).
Various kinds of experiments have been performed to
examine how strongly children learn novel words in-
troduced to them in experimental settings. For exam-
ple, children are persuaded to produce a fast-mapped
word, or to use the novel word to refer to objects
that are from the same category as its original refer-
ent (e.g., Golinkoff et al, 1992; Horst and Samuelson,
2008). We intend to look at these new tasks in our fu-
ture research.
References
Behrend, Douglas A., Jason Scofield, and Erica E.
Kleinknecht 2001. Beyond fast mapping: Young chil-
dren?s extensions of novel words and novel facts. De-
velopmental Psychology, 37(5):698?705.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer 1993. The mathematics
of statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Carey, Susan 1978. The child as word learner. In Halle, M.,
J. Bresnan, and G. A. Miller, editors, Linguistic Theory
and Psychological Reality. The MIT Press.
Carey, Susan and Elsa Bartlett 1978. Acquiring a single
new word. Papers and reports on Child Language De-
velopment, 15:17?29.
7In fact, similar fast mapping effects have been studied in con-
texts other than language. For example, Behrend et al (2001) re-
port on children?s fast mapping of novel facts about novel objects.
8After processing 1000 input pairs, the average meaning prob-
ability of familiar words (those with frequency higher than 10) is
0.77, whereas that of the novel word after one exposure is 0.64.
Clark, Eve 1990. On the pragmatics of contrast. Journal
of Child Language, 17:417?431.
Diesendruck, Gil and Lori Markson 2001. Children?s
avoidance of lexical overlap: A pragmatic account. De-
velopmental Psychology, 37(5):630?641.
Fazly, Afsaneh, Afra Alishahi, and Suzanne Steven-
son 2008. A probabilistic incremental model of word
learning in the presence of referential uncertainty. In
Proceedings of the 30th Annual Conference of the Cog-
nitive Science Society.
Frank, Michael C., Noah D. Goodman, and Joshua B.
Tenenbaum 2007. A bayesian framework for cross-
situational word-learning. In Advances in Neural Infor-
mation Processing Systems, volume 20.
Gershkoff-Stowe, Lisa and Erin R. Hahn 2007. Fast map-
ping skills in the developing lexicon. Journal of Speech,
Language, and Hearing Research, 50:682?697.
Golinkoff, Roberta Michnick, Kathy Hirsh-Pasek,
Leslie M. Bailey, and Neil R. Wegner 1992. Young
children and adults use lexical principles to learn new
nouns. Developmental Psychology, 28(1):99?108.
Gopnik, Alison and Andrew Meltzoff 1987. The develop-
ment of categorization in the second year and its relation
to other cognitive and linguistic developments. Child
Development, 58(6):1523?1531.
Halberda, Justin 2006. Is this a dax which I see before
me? use of the logical argument disjunctive syllogism
supports word-learning in children and adults. Cognitive
Psychology, 53:310?344.
Halberda, Justin and Julie Goldman 2008. One-trial learn-
ing in 2-year-olds: Children learn new nouns in 3 sec-
onds flat. (in submission).
Horst, Jessica S., Bob McMurray, and Larissa K. Samuel-
son 2006. Online processing is essential for learning:
Understanding fast mapping and word learning in a dy-
namic connectionist architecture. In Proc. of CogSci?06.
Horst, Jessica S. and Larissa K. Samuelson 2008. Fast
mapping but poor retention by 24-month-old infants. In-
fancy, 13(2):128?157.
MacWhinney, B. 2000. The CHILDES Project: Tools for
Analyzing Talk, volume 2: The Database. MahWah, NJ:
Lawrence Erlbaum Associates, third edition.
Markman, Ellen M. and Gwyn F. Wachtel 1988. Children?s
use of mutual exclusivity to constrain the meanings of
words. Cognitive Psychology, 20:121?157.
Quine, W.V.O. 1960. Word and Object. Cambridge, MA:
MIT Press.
Regier, Terry 2005. The emergence of words: Atten-
tional learning in form and meaning. Cognitive Science,
29:819?865.
Reznick, J. Steven and Beverly A. Goldfield 1992. Rapid
change in lexical development in comprehension and
production. Developmental Psychology, 28(3):406?413.
Siskind, Jeffery Mark 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39?91.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: An alternative
account. Journal of Child Language, 28:127?152.
64
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 89?96
Manchester, August 2008
An Incremental Bayesian Model for Learning Syntactic Categories
Christopher Parisien, Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON, Canada
[chris,afsaneh,suzanne]@cs.toronto.edu
Abstract
We present an incremental Bayesian model for
the unsupervised learning of syntactic cate-
gories from raw text. The model draws infor-
mation from the distributional cues of words
within an utterance, while explicitly bootstrap-
ping its development on its own partially-
learned knowledge of syntactic categories.
Testing our model on actual child-directed
data, we demonstrate that it is robust to noise,
learns reasonable categories, manages lexical
ambiguity, and in general shows learning be-
haviours similar to those observed in children.
1 Introduction
An important open problem in cognitive science and
artificial intelligence is how children successfully
learn their native language despite the lack of explicit
training. A key challenge in the early stages of lan-
guage acquisition is to learn the notion of abstract
syntactic categories (e.g., nouns, verbs, or determin-
ers), which is necessary for acquiring the syntactic
structure of language. Indeed, children as young as
two years old show evidence of having acquired a
good knowledge of some of these abstract categories
(Olguin and Tomasello, 1993); by around six years of
age, they have learned almost all syntactic categories
(Kemp et al, 2005). Computational models help to
elucidate the kinds of learning mechanisms that may
be capable of achieving this feat. Such studies shed
light on the possible cognitive mechanisms at work
in human language acquisition, and also on potential
means for unsupervised learning of complex linguis-
tic knowledge in a computational system.
Learning the syntactic categories of words has
been suggested to be based on the morphological and
phonological properties of individual words, as well
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as on the distributional information about the con-
texts in which they appear. Several computational
models have been proposed that draw on one or more
of the above-mentioned properties in order to group
words into discrete unlabeled categories. Most ex-
isting models only intend to show the relevance of
such properties to the acquisition of adult-like syn-
tactic categories such as nouns and verbs; hence, they
do not necessarily incorporate the types of learning
mechanisms used by children (Schu?tze, 1993; Red-
ington et al, 1998; Clark, 2000; Mintz, 2003; Onnis
and Christiansen, 2005). For example, in contrast to
the above models, children acquire their knowledge
of syntactic categories incrementally, processing the
utterances they hear one at a time. Moreover, chil-
dren appear to be sensitive to the fact that syntactic
categories are partially defined in terms of other cat-
egories, e.g., nouns tend to follow determiners, and
can be modified by adjectives.
We thus argue that a computational model should
be incremental, and should use more abstract cate-
gory knowledge to help better identify syntactic cat-
egories. Incremental processing also allows a model
to incorporate its partially-learned knowledge of cat-
egories, letting the model bootstrap its development.
To our knowledge, the only incremental model of
category acquisition that also incorporates bootstrap-
ping is that of Cartwright and Brent (1997). Their
template-based model, however, draws on very spe-
cific linguistic constraints and rules to learn cate-
gories. Moreover, their model has difficulty with the
variability of natural language data.
We address these shortcomings by developing an
incremental probabilistic model of syntactic category
acquisition that uses a domain-general learning algo-
rithm. The model also incorporates a bootstrapping
mechanism, and learns syntactic categories by look-
ing only at the general patterns of distributional sim-
ilarity in the input. Experiments performed on actual
(noisy) child-directed data show that an explicit boot-
strapping component improves the model?s ability to
89
learn adult-like categories. The model?s learning tra-
jectory resembles some relevant behaviours seen in
children, and we also show that the categories that
our model learns can be successfully used in a lexical
disambiguation task.
2 Overview of the Computational Model
We adapt a probabilistic incremental model of un-
supervised categorization (i.e., clustering) proposed
by Anderson (1991). The original model has been
used to simulate human categorization in a variety
of domains, including the acquisition of verb argu-
ment structure (Alishahi and Stevenson, 2008). Our
adaptation of the model incorporates an explicit boot-
strapping mechanism and a periodic merge of clus-
ters, both facilitating generalization over input data.
Here, we explain the input to our model (Section 2.1),
the categorization model itself (Section 2.2), how we
estimate probabilities to facilitate bootstrapping (Sec-
tion 2.3), and our approach for merging similar clus-
ters (Section 2.4).
2.1 Input Frames
We aim to learn categories of words, and we do this
by looking for groups of similar word usages. Thus,
rather than categorizing a word alone, we categorize a
word token with its context from that usage. The ini-
tial input to our model is a sequence of unannotated
utterances, that is, words separated by spaces. Before
being categorized by the model, each word usage in
the input is processed to produce a frame that con-
tains the word itself (the head word of the frame) and
its distributional context (the two words before and
after it). For example, in the utterance ?I gave Josie
a present,? when processing the head word Josie, we
create the following frame for input to the categoriza-
tion system:
feature w
?2
w
?1
w
0
w
+1
w
+2
I gave Josie a present
where w
0
denotes the head word feature, and w
?2
,
w
?1
, w
+1
, w
+2
are the context word features. A con-
text word may be ?null? if there are fewer than two
preceding or following words in the utterance.
2.2 Categorization
Using Anderson?s (1991) incremental Bayesian cat-
egorization algorithm, we learn clusters of word us-
ages (i.e., the input frames) by drawing on the overall
similarity of their features (here, the head word and
the context words). The clusters themselves are not
predefined, but emerge from similarities in the input.
More formally, for each successive frame F in the
input, processed in the order of the input words, we
place F into the most likely cluster, either from the
K existing clusters, or a new one:
BestCluster(F ) = argmax
k
P (k|F ) (1)
where k = 0, 1, ..,K, including the new cluster
k = 0. Using Bayes? rule, and dropping P (F ) from
the denominator, which is constant for all k, we find:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of k, P (k), is given by:
P (k) =
cn
k
(1? c) + cn
, 1 ? k ? K (3)
P (0) =
1? c
(1? c) + cn
(4)
where n
k
is the number of frames in k, and n is
the total number of frames observed at the time of
processing frame F . Intuitively, a well-entrenched
(large) cluster should be a more likely candidate for
categorization than a small one. We reserve a small
probability for creating a new cluster (Eq. 4). As the
model processes more input overall, it should become
less necessary to create new clusters to fit the data, so
P (0) decreases with large n. In our experiments, we
set c to a large value, 0.95, to further increase the
likelihood of using existing clusters.1
The probability of a frame F given a cluster k,
P (F |k), depends on the probabilities of the features
in F given k. We assume that the individual fea-
tures in a frame are conditionally independent given
k, hence:
P (F |k) = P
H
(w
0
|k)
?
i?{?2,?1,+1,+2}
P (w
i
|k) (5)
where P
H
is the head word probability, i.e., the like-
lihood of seeing w
0
as a head word among the frames
in cluster k. The context word probability P (w
i
|k) is
the likelihood of seeing w
i
in the ith context position
of the frames in cluster k. Next, we explain how we
estimate each of these probabilities from the input.
2.3 Probabilities and Bootstrapping
For the head word probability P
H
(w
0
|k), we use a
smoothed maximum likelihood estimate (i.e., the pro-
portion of frames in cluster k with head word w
0
).
For the context word probability P (w
i
|k), we can
form two estimates. The first is a simple maximum
likelihood estimate, which enforces a preference for
creating clusters of frames with the same context
words. That is, head words in the same cluster will
1The prior P (k) is equivalent to the prior in a Dirichlet pro-
cess mixture model (Sanborn et al, 2006), commonly used for
sampling clusters of objects.
90
tend to share the same adjacent words. We call this
word-based estimate P
word
.
Alternatively, we may consider the likelihood of
seeing not just the context word w
i
, but similar words
in that position. For example, if w
i
can be used as a
noun or a verb, then we want the likelihood of seeing
other nouns or verbs in position i of frames in cluster
k. Here, we use the partial knowledge of the learned
clusters. That is, we look over all existing clusters
k
?
, estimate the probability that w
i
is the head word
of frames in k?, then estimate the probability of using
the head words from those other clusters in position i
in cluster k. We refer to this category-based estimate
as P
cat
:
P
cat
(w
i
|k) =
?
k
?
P
H
(w
i
|k
?
)P
i
(k
?
|k) (6)
where P
i
(k
?
|k) is the probability of finding usages
from cluster k? in position i given cluster k. To sup-
port this we record the categorization decisions the
model has made. When we categorize the frames of
an utterance, we get a sequence of clusters for that
utterance, which gives additional information to sup-
plement the frame. We use this information to esti-
mate P
i
(k
?
|k) for future categorizations, again using
a smoothed maximum likelihood formula.
In contrast to the P
word
estimate, the estimate in
Eq. (6) prefers clusters of frames that use the same
categories as context. While some of the results of
these preferences will be the same, the latter approach
lets the model make second-order inferences about
categories. There may be no context words in com-
mon between the current frame and a potential clus-
ter, but if the context words in the cluster have been
found to be distributionally similar to those in the
frame, it may be a good cluster for that frame.
We equally weight the word-based and the
category-based estimates for P (w
i
|k) to get the like-
lihood of a context word; that is:
P (w
i
|k) ?
1
2
P
word
(w
i
|k) +
1
2
P
cat
(w
i
|k) (7)
This way, the model sees an input utterance simulta-
neously as a sequence of words and as a sequence of
categories. It is the P
cat
component, by using devel-
oping category knowledge, that yields the bootstrap-
ping abilities of our model.
2.4 Generalization
Our model relies heavily on the similarity of word
contexts in order to find category structure. In nat-
ural language, these context features are highly vari-
able, so it is difficult to draw consistent structure from
the input in the early stages of an incremental model.
When little information is available, there is a risk of
incorrectly generalizing, leading to clustering errors
which may be difficult to overcome. Children face
a similar problem in early learning, but there is ev-
idence that they may manage the problem by using
conservative strategies (see, e.g., Tomasello, 2000).
Children may form specific hypotheses about each
word type, only later generalizing their knowledge to
similar words. Drawing on this observation, we form
early small clusters specific to the head word type,
then later aid generalization by merging these smaller
clusters. By doing this, we ensure that the model only
groups words of different types when there is suffi-
cient evidence for their contextual similarity.
Thus, when a cluster has been newly created, we
require that all frames put into the cluster share the
same head word type.2 When clusters are small, this
prevents the model from making potentially incorrect
generalizations to different words. Periodically, we
evaluate a set of reasonably-sized clusters, and merge
pairs of clusters that have highly similar contexts (see
below for details). If the model decides to merge two
clusters with different head word types?e.g., one
cluster with all instances of dog, and another with
cat?it has in effect made a decision to generalize.
Intuitively, the model has learned that the contexts
in the newly merged cluster apply to more than one
word type. We now say that any word type could be
a member of this cluster, if its context is sufficiently
similar to that of the cluster. Thus, when categoriz-
ing a new word token (represented as a frame F ),
our model can choose from among the clusters with
a matching head word, and any of these ?generalized?
clusters that contain mixed head words.
Periodically, we look through a subset of the clus-
ters to find similar pairs to merge. In order to limit
the number of potential merges to consider, we only
examine pairs of clusters in which at least one cluster
has changed since the last check. Thus, after pro-
cessing every 100 frames of input, we consider the
clusters used to hold those recent 100 frames as can-
didates to be merged with another cluster. We only
consider clusters of reasonable size (here, at least 10
frames) as candidates for merging. For each candi-
date pair of clusters, k
1
and k
2
, we first evaluate a
heuristic merge score that determines if the pair is
appropriate to be merged, according to some local
criteria, i.e., the size and the contents of the candi-
date clusters. For each suggested merge (a pair whose
merge score exceeds a pre-determined threshold), we
then look at the set of all clusters, the global evidence,
to decide whether to accept the merge.
The merge score combines two factors: the en-
trenchment of the two clusters, and the similarity of
2However, a word type may exist in several clusters (e.g., for
distinct noun and verb usages), thus handling lexical ambiguity.
91
their context features. The entrenchment measure
identifies clusters that contain enough frames to show
a significant trend. We take a sigmoid function over
the number of frames in the clusters, giving a soft
threshold approaching 0 for small clusters and 1 for
large clusters. The similarity measure identifies pairs
of clusters with similar distributions of word and cat-
egory contexts. Given two clusters, we measure the
symmetric Kullback-Leibler divergence for each cor-
responding pair of context feature probabilities (in-
cluding the category contexts P
i
(k
?
|k), 8 pairs in to-
tal), then place the sum of those measures on another
sigmoid function. The merge score is the sum of the
entrenchment and similarity measures.
Since it is only a local measure, the merge score is
not sufficient on its own for determining if a merge
is appropriate. For each suggested merge, we thus
examine the likelihood of a sample of input frames
(here, the last 100 frames) under two states: the set
of clusters before the merge, and the set of clusters if
the merge is accepted. We only accept a merge if it
results in an increase in the likelihood of the sample
data. The likelihood of a sample set of frames, S ,
over a set of clusters, K, is calculated as in:
P (S) =
?
F?S
?
k?K
P (F |k)P (k) (8)
3 Evaluation Methodology
To test our proposed model, we train it on a sample of
language representative of what children would hear,
and evaluate its categorization abilities. We have
multiple goals in this evaluation. First, we determine
the model?s ability to discover adult-level syntactic
categories from the input. Since this is intended to be
a cognitively plausible learning model, we also com-
pare the model?s qualitative learning behaviours with
those of children. In the first experiment (Section 4),
we compare the model?s categorization with a gold
standard of adult-level syntactic categories and exam-
ine the effect of the bootstrapping component. The
second experiment (Section 5) examines the model?s
development of three specific parts of speech. De-
velopmental evidence suggests that children acquire
different syntactic categories at different ages, so we
compare the model?s learning rates of nouns, verbs,
and adjectives. Lastly, we examine our model?s abil-
ity to handle lexically ambiguous words (Section 6).
English word forms commonly belong to more than
one syntactic category, so we show how our model
uses context to disambiguate a word?s category.
In all experiments, we train and test the model us-
ing the Manchester corpus (Theakston et al, 2001)
from the CHILDES database (MacWhinney, 2000).
The corpus contains transcripts of mothers? conver-
sations with 12 British children between the ages of
1;8 (years;months) and 3;0. There are 34 one-hour
sessions per child over the course of a year. The age
range of the children roughly corresponds with the
ages at which children show the first evidence of syn-
tactic categories.
We extract the mothers? speech from each of the
transcripts, then concatenate the input of all 12 chil-
dren (all of Anne?s sessions, followed by all of Aran?s
sessions, and so on). We remove all punctuation. We
spell out contractions, so that each token in the input
corresponds to only one part-of-speech (PoS) label
(noun, verb, etc.). We also remove single-word ut-
terances and utterances with a single repeated word
type, since they contain no distributional informa-
tion. We randomly split the data into development
and evaluation sets, each containing approximately
683,000 tokens. We use the development set to fine-
tune the model parameters and develop the experi-
ments, then use the evaluation set as a final test of
the model. We further split the development set into
about 672,000 tokens (about 8,000 types) for training
and 11,000 tokens (1,300 types) for validation. We
split the evaluation set comparably, into training and
test subsets. All reported results are for the evaluation
set. A conservative estimate suggests that children
are exposed to at least 1.5 million words of child-
directed speech annually (Redington et al, 1998), so
this corpus represents only a small portion of a child?s
available input.
4 Experiment 1: Adult Categories
4.1 Methods
We use three separate versions of the categorization
model, in which we change the components used to
estimate the context word probability, P (w
i
|k) (as
used in Eq. (5), Section 2.2). In the word-based
model, we estimate the context probabilities using
only the words in the context window, by directly
using the maximum-likelihood P
word
estimate. The
bootstrap model uses only the existing clusters to es-
timate the probability, directly using the P
cat
esti-
mate from Eq. (6). The combination model uses an
equally-weighted combination of the two probabili-
ties, as presented in Eq. (7).
We run the model on the training set, categoriz-
ing each of the resulting frames in order. After every
10,000 words of input, we evaluate the model?s cate-
gorization performance on the test set. We categorize
each of the frames of the test set as usual, treating the
text as regular input. So that the test set remains un-
seen, the model does not record these categorizations.
4.2 Evaluation
The PoS tags in the Manchester corpus are too fine-
grained for our evaluation, so for our gold standard
92
we map them to the following 11 tags: noun, verb,
auxiliary, adjective, adverb, determiner, conjunction,
negation, preposition, infinitive to, and ?other.? When
we evaluate the model?s categorization performance,
we have two different sets of clusters of the words in
the test set: one set resulting from the gold standard,
and another as a result of the model?s categorization.
We compare these two clusterings, using the adjusted
Rand index (Hubert and Arabie, 1985), which mea-
sures the overall agreement between two clusterings
of a set of data points. The measure is ?corrected for
chance,? so that a random grouping has an expected
score of zero. This measure tends to be very con-
servative, giving values much lower than an intuitive
percentage score. However, it offers a useful relative
comparison of overall cluster similarity.
4.3 Results
Figure 1 gives the adjusted Rand scores of the three
model variants, word-based, bootstrap, and combi-
nation. Higher values indicate a better fit with the
gold-standard categorization scheme. The adjusted
Rand score is corrected for chance, thus providing a
built-in baseline measure. Since the expected score
for a random clustering is zero, all three model vari-
ants operate at above-baseline performance.
As seen in Figure 1, the word-based model gains
an early advantage in the comparison, but its per-
formance approaches a plateau at around 200,000
words of input. This suggests that while simple
word distributions provide a reliable source of infor-
mation early in the model?s development, the infor-
mation is not sufficient to sustain long-term learn-
ing. The bootstrap model learns much more slowly,
which is unsurprising, given that it depends on hav-
ing some reasonable category knowledge in order to
develop its clusters?leading to a chicken-and-egg
problem. However, once started, its performance im-
proves well beyond the word-based model?s plateau.
These results suggest that on its own, each compo-
nent of the model may be effectively throwing away
useful information. By combining the two models,
the combination model appears to gain complemen-
tary benefits from each component, outperforming
both. The word-based component helps to create a
base of reliable clusters, which the bootstrap compo-
nent uses to continue development.
After all of the training text, the combination
model uses 411 clusters to categorize the test tokens
(compared to over 2,000 at the first test point). While
this seems excessive, we note that 92.5% of the test
tokens are placed in the 25 most populated clusters.3
3See www.cs.toronto.edu/?chris/syncat for examples.
0 1 2 3 4 5 6
x 105
0
0.05
0.1
0.15
0.2
Training set size (words)
R a
dj
Combination
Word?based
Bootstrap
Figure 1: Adjusted Rand Index of each of three mod-
els? clusterings of the test set, as compared with the
PoS tags of the test data.
5 Experiment 2: Learning Trends
A common trend observed in children is that differ-
ent syntactic categories are learned at different rates.
Children appear to have learned the category of nouns
by 23 months of age, verbs shortly thereafter, and
adjectives relatively late (Kemp et al, 2005). Our
goal in this experiment is to look for these specific
trends in the behaviour of our model. We thus simu-
late an experiment where a child uses a novel word?s
linguistic context to infer its syntactic category (e.g.,
Tomasello et al, 1997). For our experiment, we ran-
domly generate input frames with novel head words
using contexts associated with nouns, verbs, and ad-
jectives, then examine the model?s categorization in
each case. We expect that our model should approxi-
mate the developmental trends of children, who tend
to learn the category of ?noun? before ?verb,? and both
of these before ?adjective.?
5.1 Methods
We generate new input frames using the most com-
mon syntactic patterns in the training data. For each
of the noun, verb, and adjective categories (from the
gold standard), we collect the five most frequent PoS
sequences in which these are used, bounded by the
usual four-word context window. For example, the
Adjective set includes the sequence ?V Det Adj N
null?, where the sentence ends after the N. For each
of the three categories, we generate each of 500 input
frames by sampling one of the five PoS sequences,
weighted by frequency, then sampling words of the
right PoS from the lexicon, also weighted by fre-
quency. We replace the head word with a novel word,
forcing the model to use only the context for cluster-
ing. Since the context words are chosen at random,
most of the word sequences generated will be novel.
This makes the task more difficult, rather than sim-
ply sampling utterances from the corpus, where rep-
93
etitions are common. While a few of the sequences
may exist in the training data, we expect the model
to mostly use the underlying category information to
cluster the frames.
We intend to show that the model uses context to
find the right category for a novel word. To evaluate
the model?s behaviour, we let it categorize each of
the randomly generated frames. We score each frame
as follows: if the frame gets put into a new cluster,
it earns score zero. Otherwise, its score is the pro-
portion of frames in the chosen cluster matching the
correct part of speech (we use a PoS-tagged version
of the training corpus; for example, a noun frame put
into a cluster with 60% nouns would get 0.6). We re-
port the mean score for each of the noun, verb, and
adjective sets. Intuitively, the matching score indi-
cates how well the model recognizes that the given
contexts are similar to input it has seen before. If the
model clusters the novel word frame with others of
the right type, then it has formed a category for the
contextual information in that frame.
We use the full combination model (Eq. (7)) to
evaluate the learning rates of individual parts of
speech. We run the model on the training subset of
the evaluation corpus. After every 10,000 words of
input, we use the model to categorize the 1,500 con-
text frames with novel words (500 frames each for
noun, verb, and adjective). As in experiment 1, the
model does not record these categorizations.
5.2 Results
Figure 2 shows the mean matching scores for each
of the tested parts of speech. Recall that since the
frames each use a novel head word, a higher match-
ing score indicates that the model has learned to cor-
rectly recognize the contexts in the frames. This does
not necessarily mean that the model has learned sin-
gle, complete categories of ?noun,? ?verb,? and ?ad-
jective,? but it does show that when the head word
gives no information, the model can generalize based
on the contextual patterns alone. The model learns
to categorize novel nouns better than verbs until late
in training, which matches the trends seen in children.
Adjectives progress slowly, and show nearly no learn-
ing ability by the end of the trial. Again, this appears
to reflect natural behaviour in children, although the
effect we see here may simply be a result of the over-
all frequency of the PoS types. Over the entire corpus
(development and evaluation), 35.4% of the word to-
kens are nouns and 24.3% are verbs, but only 2.9%
are tagged as adjectives. The model, and similarly a
child, may need much more data to learn adjectives
than is available at this stage.
The scores in Figure 2 tend to fluctuate, partic-
ularly for the noun contexts. This fluctuation cor-
responds to periods of overgeneralization, followed
0 1 2 3 4 5 6
x 105
0
0.05
0.1
0.15
0.2
0.25
Training set size (words)
M
at
ch
in
g 
sc
or
e
Nouns
Verbs
Adjectives
Figure 2: Comparative learning trends of noun, verb,
and adjective patterns.
by recovery (also observed in children; see, e.g.,
Tomasello, 2000). When the model merges two clus-
ters, the contents of the resulting cluster can initially
be quite heterogeneous. Furthermore, the new cluster
is much larger, so it becomes a magnet for new cate-
gorizations. This results in overgeneralization errors,
giving the periodic drops seen in Figure 2. While our
formulation in Section 2.4 aims to prevent such er-
rors, they are likely to occur on occasion. Eventually,
the model recovers from these errors, and it is worth
noting that the fluctuations diminish over time. As the
model gradually improves with more input, the dom-
inant clusters become heavily entrenched, and incon-
sistent merges are less likely to occur.
6 Experiment 3: Disambiguation
The category structure of our model allows a single
word type to be a member of multiple categories. For
example, kiss could belong to a category of predom-
inantly noun usages (Can I have a kiss?) and also
to a category of verb usages (Kiss me!). As a result,
the model easily represents lexical ambiguity. In this
experiment, inspired by disambiguation work in psy-
cholinguistics (see, e.g., MacDonald, 1993), we ex-
amine the model?s ability to correctly disambiguate
category memberships.
6.1 Methods
Given a word that the model has previously seen as
various different parts of speech, we examine how
well the model can use that ambiguous word?s con-
text to determine its category in the current usage.
For example, by presenting the word kiss in sepa-
rate noun and verb contexts, we expect that the model
should categorize kiss as a noun, then as a verb, re-
spectively. We also wish to examine the effect of the
target word?s lexical bias, that is, the predominance of
a word type to be used as one category over another.
As with adults, if kiss is mainly used as a noun, we
expect the model to more accurately categorize the
94
N V N V N V N V N V N V
0
0.1
0.2
0.3
0.4
Po
S 
pr
op
or
tio
n 
in 
ch
os
en
 cl
us
te
rs
Nouns
Verbs
Context:
Noun only Noun biased Equibiased Verb biased Verb only Novel wordWord bias:
Figure 3: Syntactic category disambiguation. Shown are the proportions of nouns and verbs in the chosen
clusters for ambiguous words used in either noun (N) or verb (V) contexts.
word in a noun context than in a verb context.
We focus on noun/verb ambiguities. We artificially
generate input frames for noun and verb contexts as
in experiment 2, with the following exceptions. To
make the most use of the context information, we al-
low no null words in the input frames. We also ensure
that the contexts are distinctive enough to guide dis-
ambiguation. For each PoS sequence surrounding a
noun (e.g., ?V Det head Prep Det?), we ensure that
over 80% of the instances of that pattern in the cor-
pus are for nouns, and likewise for verbs.
We test the model?s disambiguation in six con-
ditions, with varying degrees of lexical bias. Un-
ambiguous (?noun/verb only?) conditions test words
seen in the corpus only as nouns or verbs (10 words
each). ?Biased? conditions test words with a clear
bias (15 with average 93% noun bias; 15 with aver-
age 84% verb bias). An ?equibiased? condition uses 4
words of approximately equal bias, and a novel word
condition provides an unbiased case.
For the six sets of test words, we measure the ef-
fect of placing each of these words in both noun and
verb contexts. That is, each word in each condition
was used as the head word in each of the 500 noun
and 500 verb disambiguating frames. For example,
we create 500 frames where book is used as a noun,
and 500 frames where it is used as a verb. We then
use the fully-trained ?combination? model (Eq. (7)) to
categorize each frame. Unlike in the previous experi-
ment, we do not let the model create new clusters. For
each frame, we choose the best-fitting existing clus-
ter, then examine that cluster?s contents. As in ex-
periment 2, we measure the proportions of each PoS
of the frames in this cluster. We then average these
measures over all tested frames in each condition.
6.2 Results
Figure 3 presents the measured PoS proportions for
each of the six conditions. For both the equibias and
novel word conditions, we see that the clusters cho-
sen for the noun context frames (labeled N) contain
more nouns than verbs, and the clusters chosen for
the verb context frames (V) contain more verbs than
nouns. This suggests that although the model?s past
experience with the head word is not sufficiently in-
formative, the model can use the word?s context to
disambiguate its category. In the ?unambiguous? and
the ?biased? conditions, the head words? lexical biases
are too strong for the model to overcome.
However, the results show a realistic effect of the
lexical bias. Note the contrasts from the ?noun only?
condition, to the ?noun biased? condition, to ?equibi-
ased? (and likewise for the verb biases). As the lex-
ical bias weakens, the counter-bias contexts (e.g., a
noun bias with a verb context) show a stronger ef-
fect on the chosen clusters. This is a realistic effect
of disambiguation seen in adults (MacDonald, 1993).
Strongly biased words are more difficult to categorize
in conflict with their bias than weakly biased words.
7 Related Work
Several existing computational models use distribu-
tional cues to find syntactic categories. Schu?tze
(1993) employs co-occurrence statistics for common
words, while Redington et al (1998) build word dis-
tributional profiles using corpus bigram counts. Clark
(2000) also builds distributional profiles, introducing
an iterative clustering method to better handle am-
biguity and rare words. Mintz (2003) shows that
even very simple three-word templates can effec-
tively define syntactic categories. Each of these mod-
els demonstrates that by using the kinds of simple in-
formation to which children are known to be sensi-
tive, syntactic categories are learnable. However, the
specific learning mechanisms they use, such as the
hierarchical clustering methods of Redington et al
(1998), are not intended to be cognitively plausible.
In contrast, Cartwright and Brent (1997) propose
95
an incremental model of syntactic category acquisi-
tion that uses a series of linguistic preferences to find
common patterns across sentence-length templates.
Their model presents an important incremental al-
gorithm which is very effective for discovering cat-
egories in artificial languages. However, the model?s
reliance on templates limits its applicability to tran-
scripts of actual spoken language data, which contain
high variability and noise.
Recent models that apply Bayesian approaches
to PoS tagging are not incremental and assume a
fixed number of tags (Goldwater and Griffiths, 2007;
Toutanova and Johnson, 2008). In syntactic cate-
gory acquisition, the true number of categories is un-
known, and must be inferred from the input.
8 Conclusions and Future Directions
We have developed a computational model of syn-
tactic category acquisition in children, and demon-
strated its behaviour on a corpus of naturalistic child-
directed data. The model is based on domain-general
properties of feature similarity, in contrast to earlier,
more linguistically-specific methods. The incremen-
tal nature of the algorithm contributes to a substantial
improvement in psychological plausibility over pre-
vious models of syntactic category learning. Further-
more, due to its probabilistic framework, our model
is robust to noise and variability in natural language.
Our model successfully uses a syntactic bootstrap-
ping mechanism to build on the distributional proper-
ties of words. Using its existing partial knowledge
of categories, the model applies a second level of
analysis to learn patterns in the input. By making
few assumptions about prior linguistic knowledge,
the model develops realistic syntactic categories from
the input data alone. The explicit bootstrapping com-
ponent improves the model?s ability to learn adult cat-
egories, and its learning trajectory resembles relevant
behaviours seen in children. Using the contextual
patterns of individual parts of speech, we show dif-
ferential learning rates across nouns, verbs, and ad-
jectives that mimic child development. We also show
an effect of a lexical bias in category disambiguation.
The algorithm is currently only implemented as an
incremental process. However, comparison with a
batch version of the algorithm, such as by using a
Gibbs sampler (Sanborn et al, 2006), would help us
further understand the effect of incrementality on lan-
guage fidelity.
While we have only examined the effects of learn-
ing categories from simple distributional information,
the feature-based framework of our model could eas-
ily be extended to include other sources of informa-
tion, such as morphological and phonological cues.
Furthermore, it would also be possible to include se-
mantic features, thereby allowing the model to draw
on correlations between semantic and syntactic cate-
gories in learning.
Acknowledgments
We thank Afra Alishahi for valuable discussions,
and the anonymous reviewers for their comments.
We gratefully acknowledge the financial support of
NSERC of Canada and the University of Toronto.
References
Alishahi, A. and S. Stevenson 2008. A computational
model for early argument structure acquisition. Cog-
nitive Science, 32(5).
Anderson, J. R. 1991. The adaptive nature of human cate-
gorization. Psychological Review, 98(3):409?429.
Cartwright, T. A. and M. R. Brent 1997. Syntactic catego-
rization in early language acquisition: formalizing the
role of distributional analysis. Cognition, 63:121?170.
Clark, A. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL2000, pp. 91?94.
Goldwater, S. and T. L. Griffiths 2007. A fully bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL2007, pp. 744?751.
Hubert, L. and P. Arabie 1985. Comparing partitions.
Journal of Classification, 2:193?218.
Kemp, N., E. Lieven, and M. Tomasello 2005. Young chil-
dren?s knowledge of the ?determiner? and ?adjective?
categories. J. Speech Lang. Hear. R., 48:592?609.
MacDonald, M. C. 1993. The interaction of lexical and
syntactic ambiguity. J. Mem. Lang., 32:692?715.
MacWhinney, B. 2000. The CHILDES Project: Tools for
analyzing talk, volume 2: The Database. Lawrence Erl-
baum, Mahwah, NJ, 3 edition.
Mintz, T. H. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cognition,
90:91?117.
Olguin, R. and M. Tomasello 1993. Twenty-five-month-
old children do not have a grammatical category of verb.
Cognitive Development, 8:245?272.
Onnis, L. and M. H. Christiansen 2005. New beginnings
and happy endings: psychological plausibility in com-
putational models of language acquisition. CogSci2005.
Redington, M., N. Chater, and S. Finch 1998. Distribu-
tional information: A powerful cue for acquiring syn-
tactic categories. Cognitive Science, 22(4):425?469.
Sanborn, A. N., T. L. Griffiths, and D. J. Navarro 2006. A
more rational model of categorization. CogSci2006.
Schu?tze, H. 1993. Part of speech induction from scratch.
In Proc. of ACL1993, pp. 251?258.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: an alternative
account. J. Child Lang., 28:127?152.
Tomasello, M. 2000. Do young children have adult syn-
tactic competence? Cognition, 74:209?253.
Tomasello, M., N. Akhtar, K. Dodson, and L. Rekau 1997.
Differential productivity in young children?s use of
nouns and verbs. J. Child Lang., 24:373?387.
Toutanova, K. and M. Johnson 2008. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In NIPS2008.
96
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 1?10,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Modeling the Acquisition of Mental State Verbs
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
Children acquire mental state verbs (MSVs)
much later than other, lower-frequency, words.
One factor proposed to contribute to this de-
lay is that children must learn various seman-
tic and syntactic cues that draw attention to the
difficult-to-observe mental content of a scene.
We develop a novel computational approach
that enables us to explore the role of such cues,
and show that our model can replicate aspects
of the developmental trajectory of MSV acqui-
sition.
1 Introduction
Mental State Verbs (MSVs), such as think, know,
and want, are very frequent in child-directed lan-
guage, yet children use them productively much
later than lower-frequency action verbs, such as fall
and throw (Johnson and Wellman, 1980; Shatz et al,
1983). Psycholinguistic theories have suggested that
there is a delay in the acquisition of MSVs because
they require certain cognitive and/or linguistic skills
that are not available during the early stages of lan-
guage development. For example, MSVs typically
occur with a sentential complement (SC) that refers
to the propositional content of the mental state, as in
He thinks Mom went home. Children have to reach a
stage of syntactic development that includes some
facility with SCs in order to fully acquire MSVs.
However, even at 3?5 years old, children are able to
process SCs only imperfectly (e.g., Asplin, 2002).
Even when children are able to produce SCs with
other verbs (such as verbs of communication, as in
He said Mom went home), there is a lag before they
productively use MSVs referring to actual mental
content (Diessel and Tomasello, 2001).1 Psycholin-
guists have suggested that young children lack the
conceptual ability to conceive that others have men-
tal states separate from their own (Bartsch and Well-
man, 1995; Gopnik and Meltzoff, 1997), further de-
laying the acquisition of MSVs.
Another factor suggested to contribute to the dif-
ficulty of acquiring MSVs is their informational re-
quirements (Gleitman et al, 2005; Papafragou et al,
2007). Children learn word meanings by figuring
out which aspects of an observed scene are referred
to by a particular word (Quine, 1960). MSVs of-
ten refer to aspects of the world that are not directly
observable (i.e., the beliefs and desires of another
entity). Thus, in addition to the above-mentioned
challenges posed by children?s developing linguis-
tic/conceptual abilities, children may simply have
difficulty in identifying the relevant mental content
necessary to learning MSVs.
In particular, Papafragou et al (2007) [PCG] have
shown that even given adequate conceptual and lin-
guistic abilities (as in adults) the mental events in a
scene (the actors? internal states) are not attended
to as much as the actions, unless there are cues
that heighten the salience of the mental content.
PCG further demonstrate that children?s sensitivity
to such cues lags behind that of adults, suggesting an
additional factor in the acquisition of MSVs which
1Researchers have noted that children use MSVs in fixed
phrases, in a performative use or as a pragmatic marker, well be-
fore they use them to refer to actual mental content (e.g., Diessel
and Tomasello, 2001; Shatz et al, 1983). Here by ?acquisition
of MSVs?, we are specifically referring to children learning us-
ages that genuinely refer to mental content.
1
is the developmental change in how strongly such
cues are associated with the relevant mental content.
We develop a computational model of MSV ac-
quisition (the first, to our knowledge) to further il-
luminate these issues. We extend an existing model
of verb argument structure acquisition (Alishahi and
Stevenson, 2008) to enable the representation and
processing of mental state semantics and syntax.
We simulate the developmental change proposed by
PCG through a gradually increasing ability in the
model to appropriately attend to the mental content
of a scene. In addition, we suggest that even when
the learner?s semantic representation is biased to-
wards the action content, the learner attends to the
observed SC syntax in an MSV utterance. This is
especially important to account for the pattern of er-
rors in child data. Our model thus extends the ac-
count of PCG to show that a probabilistic interplay
of the semantic and syntactic features of a partial and
somewhat erroneous perception of the input, com-
bined with a growing ability to attend to cues indica-
tive of mental content, can help to account for chil-
dren?s developmental trajectory in learning MSVs.
2 Background and Our Approach
To investigate the linguistic and contextual cues that
could help in learning MSVs, PCG use a procedure
called the Human Simulation Paradigm (originally
proposed by Gillette et al, 1999). In this paradigm,
subjects are put in situations intended to simulate
various word learning conditions of young children.
E.g., in one condition, adults watch silent videos of
caregivers interacting with children, and are asked
to predict the verb uttered by the caregiver. In an-
other condition, subjects hear a sentence containing
a nonce verb (e.g., gorp) after watching the video,
and are asked what gorp might mean.
We focus on two factors investigated by PCG in
the performance of adults and children in identifying
MSVs. The first factor they investigated involved
the syntactic frame used when subjects were given a
sentence with a nonce verb. PCG hypothesized that
an SC frame would be a cue to mental content (and
an MSV), since the SC refers to propositional con-
tent. The second factor PCG examined was whether
the video described a ?true belief? or a ?false be-
lief? scene: A true belief scene shows an ordinary
situation which unfolds as the character in the scene
expects ? e.g., a little boy takes food to his grand-
mother, and she is there in the house as expected.
The corresponding false belief scene has an unex-
pected outcome for the character ? in this case, an-
other character has replaced the grandmother in her
bed. Here the hypothesis was that such false belief
scenes would heighten the salience of mental activ-
ity in the scene and lead to greater belief verb re-
sponses in describing them.
PCG?s results showed that both adults and chil-
dren were sensitive to both the scene and syntax
cues, but children?s ability to draw on such cues was
inferior to that of adults. They thus propose that the
difference between children and adults is that chil-
dren have not yet formed as strong an association
as adults between the cues and the mental content
of a scene as required to match the performance of
adults. Nonetheless, their results suggest that the
participating children had the conceptual and lin-
guistic abilities required for MSVs, since they were
able to produce them under conditions with suffi-
ciently strong cues.
We simulate PCG?s experiments using a novel
computational approach. Following PCG, we as-
sume that even when a learner is able to perceive
the general semantic and syntactic properties of a
belief scene and associated utterance, they may not
attend to the mental content in every situation, and
that this ability improves over time. We model a de-
velopmental change in a learner?s attention to mental
content: At early stages, corresponding to the state
of young children, the learner largely focuses on the
action aspects of a belief scene, even in the presence
of an utterance using an MSV. Over time, the learner
gradually increases in the ability to attend appropri-
ately to the mental aspects of such a scene and ut-
terance, until adult-like competence is achieved in
associating the available cues with mental content.
Importantly, our work extends the proposal of
PCG by bringing in evidence from other relevant
studies on children?s ability to process SCs. More
specifically, we suggest that when children hear a
sentence like I think Mom went home, they recog-
nize (and record) the existence of an SC, while at
the same time they focus on the action semantics
as the main (most salient) event. In other words,
we assume that children?s imperfect syntactic abil-
2
ities are at least sufficient to recognize the SC us-
age (Nelson et al, 1989; Asplin, 2002). However,
their attention is mostly directed towards the action
expressed in the embedded complement, either be-
cause mental content is less easily observable than
action (Papafragou et al, 2007), or due to the lin-
guistic saliency of the embedded clause (Diessel and
Tomasello, 2001; Dehe and Wichmann, 2010). As
mentioned above, we model this misrepresentation
by considering the possibility of not attending to
mental content in a belief scene. Specifically, we
assume that (i) the model is very likely to overlook
the mental content at earlier stages (corresponding to
children?s observed behaviour); and (ii) as the model
?ages? (i.e., receives more input), its attentional abil-
ities improve and thus the model is more likely to
focus on the mental content as the main proposition.
Our results suggest that these changes to the model
lead to a match between our model?s behaviour and
PCG?s differential results for children and adults.
3 The Computational Model
A number of computational models have examined
the role of interacting syntactic and semantic cues
in the acquisition of verb argument structure (e.g.,
Niyogi, 2002; Buttery, 2006; Alishahi and Steven-
son, 2008; Perfors et al, 2010; Parisien and Steven-
son, 2011). However, to our knowledge no com-
putational model has addressed the developmental
trajectory in the acquisition of MSVs. Here we ex-
tend the verb argument structure acquisition model
of Alishahi and Stevenson (2008) to enable it to ac-
count for MSV acquisition. Specifically, we use
their core Bayesian learning algorithm, but modify
the input processing component to reflect a develop-
mental change in attention to the mental state con-
tent of an MSV usage and its consequent represen-
tation, as noted above.
We use this model for the following reasons: (i) it
focuses on argument structure learning, and the in-
terplay between syntax and semantics, which are key
to MSV acquisition; (ii) it is probabilistic and hence
can naturally capture gradient responses to different
cues; and (iii) it is incremental, which allows us to
investigate changes in behaviour over time. We first
give an overview of the original model, and then ex-
plain our extensions.
3.1 Model Overview
The input to the model is a sequence of utterances
(what the child hears), each paired with a scene
(what the child perceives); see Table 1 for an ex-
ample. First, the frame extraction component of
the model extracts from the input pair a frame?
a collection of features. We use features that in-
clude both semantic properties (?event primitives?
and ?event participants?) and syntactic properties
(?syntactic pattern? and ?verb count?). See Table 2
for examples of two possible frames extracted from
the pair in Table 1. Second, the learning component
of the model incrementally clusters the extracted
frames one by one. These clusters correspond to
constructions that reflect probabilistic associations
of semantic and syntactic features across similar us-
ages, such as an agentive intransitive or causative
transitive. The model can use these associations to
simulate various language tasks as the prediction of
a missing feature given others. For example, to sim-
ulate the human simulation paradigm setting, we can
use the model to predict a missing verb on the basis
of the available semantic and syntactic information
(as in Alishahi and Pyykkon?en, 2011).
3.2 Algorithm for Learning Constructions
The model clusters the input frames into construc-
tions on the basis of their overall similarity in the
values of their features. Importantly, the model
learns these constructions incrementally, consider-
ing the possibility of creating a new construction for
a given frame if the frame is not sufficiently similar
to any of the existing constructions. Formally, the
model finds the best construction (including a new
one) for a given frame F as in:
BestConstruction(F ) = argmax
k?Constructions
P (k|F )
(1)
where k ranges over all existing constructions and a
new one. Using Bayes rule:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of each construction P (k) is
estimated as the proportion of observed frames that
are in k, assigning a higher prior to constructions
3
Think[state,consider,cogitate](I[experiencer,preceiver,considerer ],Go[physical,act,move](MOM[agent,actor,change],HOME[location,destination]))
I think Mom went home.
Table 1: A sample Scene?Utterance input pair.
(a) Interpretation#1 (mental event is attended to) (b) Interpretation#2 (mental event not attended to)
main predicate think main predicate go
other predicate go other predicate think
event primitives { state, consider , cogitate } event primitives { physical , act ,move}
event participants { experiencer , perceiver , considerer} event participants { agent , actor , change}
{ preposition, action, perceivable} { location, destination}
syntactic pattern arg1 verb arg-S syntactic pattern arg1 verb arg-S
verb count 2 verb count 2
Table 2: Two frames extracted from the scene?utterance pair in Table 1. The bottom left and right panels of the table
describe the two possible interpretations given the input pair. (a) Interpretation#1 assumes that the mental event is the
focus of attention. Here think is interpreted as the main predicate, which the event primitives and participants refer
to. (b) Interpretation#2 assumes that attention is mostly directed to the physical action in the scene, and thus go is
taken to be the main predicate, which also determines the extracted event primitives and participants. Note that for
both interpretations, the learner is assumed to perceive the utterance in full, thus both verbs are heard in the context
of the sentential complement syntax (i.e., syntactic pattern with SC and 2 verbs), without fully extracting the syntactic
relations between the clauses.
that are more entrenched (i.e., observed more fre-
quently). The likelihood P (F |k) is estimated based
on the values of features in F and the frames in k:
P (F |k) =
?
i?frameFeatures
Pi(j|k) (3)
where i refers to the ith feature of F and j refers
to its value. The conditional probability of a feature
i to have the value j in construction k, Pi(j|k), is
calculated with a smoothed version of:
Pi(j|k) =
counti(j, k)
nk
(4)
where counti(j, k) reflects the number of times fea-
ture i has the value j in construction k, and nk is the
number of frames in k. We have two types of fea-
tures: single-valued and set-valued. The result of the
counti operator for a single-valued feature is based
on exact match to the value j, while the result for a
set-valued feature is based on the degree of overlap
between the compared sets, as in the original model.
3.3 Modeling Developmental Changes in
Attending to Mental Content
We extend the model above to account for the in-
crease in the ability to attend to cues associated with
MSVs, as observed by PCG. In addition, we pro-
pose that children?s representation of this situation
includes the observed syntax of the MSV. That is,
children do not simply ignore the MSV usage, focus-
ing only on the action expressed in its complement
? they must also note that this action semantics oc-
curs in the context of an SC usage.
To adapt the model in these ways, we change
the frame extraction component to allow two pos-
sible interpretations for a mental event input. First,
to reflect PCG?s proposal, we incorporate a mecha-
nism into the model?s frame-extraction process that
takes into account the probability of attending to
mental content. Specifically, we assume that when
presented with an input pair containing an MSV,
as in Table 1, a learner attends to the perceptu-
ally salient action/state expressed in the comple-
ment (here Go) with probability p, and to the non-
perceptually salient mental event expressed in the
main verb (here Think) with probability 1? p. This
probability p is a function over time, correspond-
ing to the observed developmental progression. At
very early stages, p will be high (close to 1), sim-
ulating the much greater saliency of physical ac-
tions compared to mental events for younger chil-
dren. With subsequent input, p will decrease, giv-
ing more and more attention to the mental content
of a scene with a mental event, gradually approach-
ing adult-like abilities.
4
We adopt the following function for p:
p =
1
? ? t+ 1
, 0 < ?  1 (5)
where t is the current time, expressed as the total
number of scene?utterance pairs observed thus far
by the model, and the parameter ? is set to a small
value to assign a high probability to the physical ac-
tion interpretation of the scene in the initial stages of
learning (when t is small).
We must specify the precise make-up of the
frames that correspond to the two possible inter-
pretations considered with probability p and 1 ? p.
PCG state only that children and adults differen-
tially attend to the action vs. mental content of the
scene. We operationalize this by forming two pos-
sible frames in response to an MSV usage. We pro-
pose that one of the frames (with probability 1?p) is
the interpretation of the mental content usage, as in
Table 2(a). However, we extend the account of PCG
by proposing that the other frame considered is not
simply a standard representation of an action scene?
utterance pair. Rather, we suggest that the interpre-
tation of an MSV scene?utterance pair that focuses
on the action semantics does so within the context of
the SC syntax, given the assumed stage of linguistic
abilities of the learner. This leads to the frame (with
probability p) as in Table 2(b), which represents the
action semantics within a two-verb construction as-
sociated with the SC syntax.
4 Experimental Setup
4.1 Input Data
We generate artificial corpora for our simulations,
since we do not have access to sufficient data of ac-
tual utterances paired with scene representations. In
order to create naturalistic data that resembles what
children are exposed to, we follow the approach of
Alishahi and Stevenson (2008) to build an input-
generation lexicon that has the distributional prop-
erties of actual child-directed speech (CDS). Their
original lexicon contains only high-frequency phys-
ical action verbs that appear in limited syntactic pat-
terns. Our expanded lexicon also includes mental
state, perception, and communication verbs, all of
which can appear with SCs.
We extracted our verbs and their distributional
properties from the child-directed speech of 8
children in the CHILDES database (MacWhinney,
2000).2 We selected 28 verbs from different se-
mantic classes and different frequency ranges: 12
physical action verbs taken from the original model
(come, go, fall, eat, play, get, give, take, make, look,
put, sit), 6 perception and communication verbs
(see, hear, watch, say, tell, ask), 5 belief verbs (think,
know, guess, bet, believe), and 5 desire verbs (want,
wish, like, mind, need). For each verb, we manually
analyzed a random sample of 100 CDS usages (or
all usages if fewer than 100) to extract distributional
information about its argument structures.
We construct the input-generation lexicon by list-
ing each of the 28 verbs (i.e. the ?main predicate?),
along with its overall frequency, as well as the fre-
quency with which it appears with each argument
structure. Each entry contains values of the syn-
tactic and semantic features (see Table 2 for ex-
amples), including ?event primitives?, ?event partic-
ipants?, ?syntactic pattern?, and ?verb count?. By
including these features, we assume that a learner
is capable of understanding basic syntactic proper-
ties of an utterance, including word syntactic cat-
egories (e.g., noun and verb), word order, and the
appearance of SCs (e.g., Nelson et al, 1989). We
also assume that a learner has the ability to perceive
and conceptualize the general semantic properties
of events ? including mental, perceptual, commu-
nicative, and physical actions ? as well as those
of the event participants. Values for the semantic
features (the event primitives and event participants)
are taken from Alishahi and Stevenson (2008) for
the action verbs, and from several sources including
VerbNet (Kipper et al, 2008) and Dowty (1991) for
the additional verbs.
For each simulation in our experiments (explained
below), we use the input-generation lexicon to
automatically generate an input corpus of scene?
utterance pairs that reflects the observed frequency
distribution in CDS.3 For an input utterance that
contains an MSV, we randomly pick one of the ac-
tion verbs as the verb appearing within the sentential
complement (the ?other predicate?).
2Corpora of Brown (1973); Suppes (1974); Kuczaj (1977);
Bloom et al (1974); Sachs (1983); Lieven et al (2009).
3The model does not use the input-generation lexicon in
learning.
5
4.2 Setup of Simulations
We perform simulations by training the model on
a randomly generated input corpus, and examin-
ing changes in its performance over time with pe-
riodic tests. Specifically, we perform simulations of
the verb identification task in the human simulation
paradigm as follows: At each test point, we present
the model with a partial test frame with missing
predicate (verb) values, and different amounts of in-
formation for the other features. The tests corre-
spond to the scenarios in the original experiments of
PCG, where each scenario is represented by a partial
frame as follows:
1. scene-only scenario: Corresponds to subjects
watching a silent video depicting either an Ac-
tion or a Belief scene. Our test frame includes
values for the semantic features (event primi-
tives and event participants) corresponding to
the scene type, but no syntactic features.
2. syntax-only scenario: Corresponds to subjects
hearing either an SC or a non-SC utterance.
The test frame includes the corresponding syn-
tactic pattern and verb count of the utterance
type heard, but no semantic features.
3. syntax & scene scenario: Corresponds to sub-
jects watching a silent video (with Action or
Belief content), and hearing an associated (non-
SC or SC) utterance. The test frame includes all
the relevant syntactic and semantic features.
We perform 100 simulations, each on 15000
randomly-generated training frames, and examine
the type of verbs that the model predicts in response
to test frames for the three scenarios. For each
scenario and each simulation, we generate a test
frame by including the relevant feature values of a
randomly-selected physical action or belief verb us-
age from the input-generation lexicon.
PCG code the individual verb responses of their
human subjects into various verb classes. To analo-
gously code our model?s response to each test frame,
we estimate the likelihood of each of two verb
groups, Belief and Action,4 by summing over the
4The Action verbs include action, communication, and per-
ception verbs, as in PCG. Verbs from the desire group are not
considered here, also as in PCG.
Figure 1: Likelihood of Belief verb prediction given Ac-
tion or Belief input.
likelihood of all the verbs in that group. In the re-
sults below, these likelihood scores are averaged for
each test point over the 100 simulations.
When our model is presented with a test frame
containing a Belief scene, we assume that the model
(like a language learner) may not attend to the men-
tal content, resulting in one of the two interpreta-
tions described in Section 3.3 (see Table 2). We thus
calculate the verb class likelihoods using a weighted
average of the verbs predicted under the two inter-
pretations. Following PCG, we test our model with
two types of Belief scenes: True Belief and False
Belief, with the latter having a higher level of be-
lief saliency. We model the difference between these
two scene types as a difference in the probabilities
of perceiving the two interpretations, with a higher
probability for the belief interpretation given a False
Belief test frame. In the experiments presented here,
we set this probability to 80% for False Belief, and
to 60% (just above chance) for True Belief. (Un-
like in training, where we assume a change over time
in the probability of a belief interpretation, for each
presentation of the test frame we use the same prob-
abilities of the two interpretations.)
5 Experimental Results
We present two sets of results: In Section 5.1, we
examine the role of syntactic and semantic cues in
MSV identification, by comparing the likelihoods
of the model?s Belief verb predictions across the
three scenarios. Here we test the model after pro-
cessing 15000 input frames, simulating an adult-like
behaviour (as in PCG). At this stage, we present
the model with an Action test frame (Action scene
and/or Transitive syntax), or a Belief test frame
6
(False Belief scene and/or SC syntax). In Sec-
tion 5.2, we look into the role of semantic cues
that enhance belief saliency, by comparing the like-
lihoods of Belief vs. Action verb predictions in the
syntax & scene scenario. The test frames depict ei-
ther a True Belief or a False Belief scene, paired with
an SC utterance. Here, we test our model periodi-
cally to examine the developmental pattern of MSV
identification, comparing our results with the differ-
ence in the behaviour of children and adults in PCG.
5.1 Linguistic Cues for Belief Verb Prediction
The left side of Figure 1 presents the results of PCG
(for adult subjects); the right side shows the likeli-
hood of Belief verb prediction by our model. Simi-
lar to the results of PCG, our model?s likelihood of
Belief verb prediction is extremely low when given
an Action test frame (Action scene and/or Transi-
tive syntax), whereas it is much higher when the
model is presented with a Belief test frame (False
Belief scene and/or SC syntax). Moreover, as in
PCG, when the model is tested with Belief content,
the lowest likelihood is for the scene-only scenario
and the highest is for the syntax & scene scenario.
PCG found, somewhat surprisingly, that the
syntax-only scenario was more informative for MSV
prediction than the scene-only scenario. Our results
replicate this finding, which we believe is due to the
way our Bayesian clustering groups verb usages to-
gether. Non-SC usages of MSVs are often grouped
with action verbs that frequently appear with non-
SC syntax, and this results in constructions with
mixed (action and belief) semantics. When using
MSV semantic features to make the verb predic-
tion, the action verbs get a higher likelihood based
on such mixed constructions. However, the frequent
usage of MSVs with SC results in entrenched con-
structions of mostly MSVs. Although other verbs,
such as see and say, may also be used with SC syn-
tax, they are grouped with verbs such as watch and
tell into constructions with mixed (SC and non-SC)
syntax. When given SC syntax in verb prediction,
the more coherent MSV constructions result in a
high likelihood of predicting Belief verbs.
5.2 Belief Saliency in Verb Prediction
Figure 2(a) shows the PCG results, for children
and adults, and for True Belief and False Belief.
(a)
(b)
(c)
Figure 2: Verb class likelihood: (a) PCG results for
adults and children (aged 3;7?5;9); (b) Model?s results
given True Belief; (c) Model?s results given False Belief.
Figures 2(b) and (c) present the likelihoods of the
model?s Belief vs. Action verb prediction, over time,
for True and False Belief situations (True/False Be-
lief scene and SC syntax), respectively. We first
compare the responses of our model at the final stage
of training to those of adults in PCG. At this stage,
the model?s verb predictions (for both True and False
Belief) follow a similar trend to that of adult sub-
jects in PCG. The likelihood of Belief verbs is much
higher than the likelihood of Action verbs given a
False Belief situation. Moreover, the likelihood of
Belief verbs is higher given a False Belief situation,
compared to a True Belief situation.
Next, we compare the developmental pattern of
Belief/Action verb predictions in the model with the
difference in behaviour of children and adults in
PCG. We focus on the model?s responses after pro-
7
cessing about 3000 input pairs, as it corresponds to
the trends observed for the children in PCG. At this
stage, the likelihood of Belief verbs is lower than
that of Action verbs for the True Belief situation,
but the pattern is reversed for False Belief; a pattern
similar to children?s behaviour in PCG (see Figure
2(a)). As in PCG, the likelihood of Belief verb pre-
dictions in our model is higher than that of Action
verbs for the False Belief situation, in both ?child?
and ?adult? stages, with a larger difference as the
model ?ages? (i.e., processes more input). For the
True Belief situation also the pattern is similar to
that of PCG: Belief verbs are less likely than Action
verbs to be predicted at early stages, but as the model
receives more input, the likelihood of Belief verbs
becomes slightly higher than that of Action verbs.
PCG?s hypothesis of greater attention to the action
content of a scene implicitly implies that children
focus on the action semantics and syntax of the em-
bedded SC of a Belief verb. We have suggested in-
stead that the focus is on the action semantics within
the context of the SC syntax of the MSV. To directly
evaluate the necessity of our latter assumption, we
performed a simulation using both action syntax and
semantics to represent the physical interpretation of
the belief scene. Specifically, the syntactic features
in this representation were non-SC structure with
only one verb. Based on these settings, the model
predicted high likelihood for the Belief verbs from a
very early stage, not showing the same delayed ac-
quisition pattern exhibited by PCG?s results. This
result suggests that the SC syntax plays an impor-
tant role in MSV acquisition.
6 Discussion
Various studies have considered why mental state
verbs (MSVs) appear relatively late in children?s
productions (e.g., Shatz et al, 1983; Bartsch and
Wellman, 1995). The Human Simulation Paradigm
has revealed that adult participants tend to focus on
the physical action cues of a scene (Gleitman et al,
2005). PCG?s results further show that cues empha-
sizing mental content lead to a significant increase
in MSV responses in such tasks. Moreover, they
show that a sentential complement (SC) structure is
a stronger cue to an MSV than the semantic cues
emphasizing mental content.
In this paper we adapt a computational Bayesian
model to analyze such semantic and syntactic cues
in the ability of children to identify them. We sim-
ulate an attentional mechanism of the growing sen-
sitivity to mental content in a scene into the model.
We show that both the ability to observe the obscure
mental content and the ability to recognize the use of
an SC structure are essential to replicate PCG?s ob-
servations. Moreover, our results predict the strong
association of MSVs to the SC syntax, for the first
time (to our knowledge) in a computational model.
Children often use verbs other than MSVs in ex-
perimental settings in which MSVs would be the ap-
propriate or correct verb choice (Asplin, 2002; Kidd
et al, 2006; Papafragou et al, 2007). Our model
presents similar variability in verb choice. One un-
derlying cause of this behaviour in the model is its
association of action semantics to SC syntax, due to
the tendency to observe the physical cues in a scene
associated with an utterance using an MSV with an
SC. Preliminary results (not reported here) imply
that the association of perception and communica-
tion verbs that frequently appear with SC contribute
to this pattern of verb choice (see de Villiers, 2005,
for theoretical support). Our results require further
work to fully understand this behaviour.
Finally, our model will facilitate future work in re-
gards to the performative usage of MSVs, in which
MSVs do not indicate mental content, but rather di-
rect the conversation. Several studies (e.g., Diessel
and Tomasello, 2001; Howard et al, 2008), have re-
ferred to the role performative use likely plays in
MSV acquisition, since the first MSV usages by
children are performative. The semantic properties
MSVs take in performative usages is not currently
represented in our lexicon. However, the physical
interpretation of the mental scene that we have used
in our experiments here is similar to the performa-
tive usage: i.e., the main perceived action and the
observed syntactic structure are the same. At the
moment, our results imply that the association of
MSVs with their genuine mental meaning is delayed
by interpretations of the mental scene which over-
look the mental content. In the future, we aim to in-
corporate the semantic representation of performa-
tive usages to better analyze their effect on MSV ac-
quisition.
8
References
Afra Alishahi and Pirita Pyykkon?en. 2011. The on-
set of syntactic bootstrapping in word learning:
Evidence from a computational study. In Pro-
ceedings of the 33st Annual Conference of the
Cognitive Science Society.
Afra Alishahi and Suzanne Stevenson. 2008. A com-
putational model of early argument structure ac-
quisition. Cognitive Science, 32(5):789?834.
Kristen N. Asplin. 2002. Can complement frames
help children learn the meaning of abstract
verbs? Ph.D. thesis, UMass Amherst.
Karen Bartsch and Henry M. Wellman. 1995. Chil-
dren talk about the mind.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development: If,
when, and why. Cognitive Psychology, 6(3):380?
420.
Roger Brown. 1973. A first language: The early
stages. Harvard U. Press.
Paula J. Buttery. 2006. Computational models
for first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Jill G. de Villiers. 2005. Can language acquisition
give children a point of view. In Why Language
Matters for Theory of Mind, pages 199?232. Ox-
ford University Press.
Nicole Dehe and Anne Wichmann. 2010. Sentence-
initial I think (that) and i believe (that): Prosodic
evidence for use as main clause, comment clause
and dicourse marker. Stuides in Language,
34(1):36?74.
Holger Diessel and Michael Tomasello. 2001. The
acquisition of finite complement clauses in en-
glish: A corpus-based analysis. Cognitive Lin-
guistics, 12(2):97?142.
David Dowty. 1991. Thematic Proto-Roles and Ar-
gument Selection. Language, 67(3):547?619.
Jane Gillette, Lila Gleitman, Henry Gleitman, and
Anne Lederer. 1999. Human simulations of lexi-
cal acquisition. Cognition, 73(2):135?176.
Lila R. Gleitman, Kimberly Cassidy, Rebecca
Nappa, Anna Papafragou, and John C. Trueswell.
2005. Hard words. Language Learning and De-
velopment, 1(1):23?64.
Alison Gopnik and Andrew N. Meltzoff. 1997.
Words, thoughts, and theories.
Alice A. Howard, Lara Mayeux, and Letitia R.
Naigles. 2008. Conversational correlates of chil-
dren?s acquisition of mental verbs and a theory of
mind. First Language, 28(4):375.
Carl Nils Johnson and Henry M. Wellman. 1980.
Children?s developing understanding of mental
verbs: Remember, know, and guess. Child De-
velopment, 51(4):1095?1102.
Evan Kidd, Elena Lieven, and Michael Tomasello.
2006. Examining the role of lexical frequency in
the acquisition and processing of sentential com-
plements. Cognitive Development, 21(2):93?107.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Eval-
uation, 42(1):21?40?40.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
Elena Lieven, Dorothe? Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
Deborah G. Kemler Nelson, Kathy Hirsh-Pasek, Pe-
ter W. Jusczyk, and Kimberly Wright Cassidy.
1989. How the prosodic cues in motherese might
assist language learning. Journal of child Lan-
guage, 16(1):55?68.
Sourabh Niyogi. 2002. Bayesian learning at the
syntax-semantics interface. In Proceedings of the
24th Annual Conference of the Cognitive Science
Society.
Anna Papafragou, Kimberly Cassidy, and Lila Gleit-
man. 2007. When we think about thinking:
The acquisition of belief verbs. Cognition,
105(1):125?165.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
9
learned verb classes. In Proceedings of the 33rd
Annual Meeting of the Cognitive Science Society.
Amy Perfors, Joshua B. Tenenbaum, and Elizabeth
Wonnacott. 2010. Variability, negative evidence,
and the acquisition of verb argument construc-
tions. Journal of Child Language, 37(03):607?
642.
Willard .V.O. Quine. 1960. Word and object, vol-
ume 4. The MIT Press.
Jacqueline Sachs. 1983. Talking about the there and
then: The emergence of displaced reference in
parent-child discourse. Children?s Language, 4.
Marilyn Shatz, Henry M. Wellman, and Sharon Sil-
ber. 1983. The acquisition of mental verbs: A
systematic investigation of the first reference to
mental state. Cognition, 14(3):301?321.
Patrick Suppes. 1974. The semantics of children?s
language. American Psychologist, 29(2):103.
10
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 80?89,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Computational Model of Memory, Attention, and Word Learning
Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
{aida,afsaneh,suzanne}@cs.toronto.edu
Abstract
There is considerable evidence that people
generally learn items better when the presen-
tation of items is distributed over a period of
time (the spacing effect). We hypothesize that
both forgetting and attention to novelty play
a role in the spacing effect in word learning.
We build an incremental probabilistic compu-
tational model of word learning that incorpo-
rates a forgetting and attentional mechanism.
Our model accounts for experimental results
on children as well as several patterns ob-
served in adults.
1 Memory, Attention, and Word Learning
Learning the meaning of words is an important com-
ponent of language acquisition, and an extremely
challenging task faced by young children (e.g.,
Carey, 1978; Bloom, 2000). Much psycholinguis-
tic research has investigated the mechanisms under-
lying early word learning, and the factors that may
facilitate or hinder this process (e.g., Quine, 1960;
Markman and Wachtel, 1988; Golinkoff et al, 1992;
Carpenter et al, 1998). Computational modeling has
been critical in this endeavor, by giving precise ac-
counts of the possible processes and influences in-
volved (e.g., Siskind, 1996; Regier, 2005; Yu, 2005;
Fazly et al, 2010). However, computational models
of word learning have generally not given sufficient
attention to the broader interactions of language ac-
quisition with other aspects of cognition and cogni-
tive development.
Memory limitations and attentional mechanisms
are of particular interest, with recent computational
studies reconfirming their important role in aspects
of word learning. For example, Frank et al (2010)
show that memory limitations are key to matching
human performance in a model of word segmenta-
tion, while Smith et al (2010) further demonstrate
how attention plays a role in word learning by form-
ing the basis for abstracting over the input. But
much potential remains for computational modeling
to contribute to a better understanding of the role of
memory and attention in word learning.
One area where there is much experimental evi-
dence relevant to these interactions is in the investi-
gation of the spacing effect in learning (Ebbinghaus,
1885; Glenberg, 1979; Dempster, 1996; Cepeda
et al, 2006). The observation is that people gen-
erally show better learning when the presentations
of the target items to be learned are ?spaced? ? i.e.,
distributed over a period of time ? instead of be-
ing ?massed? ? i.e., presented together one after
the other. Investigations of the spacing effect often
use a word learning task as the target learning event,
and such studies have looked at the performance of
adults as well as children (Glenberg, 1976; Pavlik
and Anderson, 2005; Vlach et al, 2008). While
this work involves controlled laboratory conditions,
the spacing effect is very robust across domains and
tasks (Dempster, 1989), suggesting that the underly-
ing cognitive processes likely play a role in natural
conditions of word learning as well.
Hypothesized explanations for the spacing effect
have included both memory limitations and atten-
tion. For example, many researchers assume that the
process of forgetting is responsible for the improved
performance in the spaced presentation: Because
participants forget more of what they have learned
in the longer interval, they learn more from sub-
sequent presentations (Melton, 1967; Jacoby, 1978;
80
Cuddy and Jacoby, 1982). However, the precise re-
lation between forgetting and improved learning has
not been made clear. It has also been proposed that
subjects attend more to items in the spaced presen-
tation because accessing less recent (more novel)
items in memory requires more effort or attention
(Hintzman, 1974). However, the precise attentional
mechanism at work in the spacing experiments is not
completely understood.
While such proposals have been discussed for
many years, to our knowledge, there is as yet no de-
tailed computational model of the precise manner in
which forgetting and attention to novelty play a role
in the spacing effect. Moreover, while mathemat-
ical models of the effect help to clarify its proper-
ties (Pavlik and Anderson, 2005), it is very impor-
tant to situate these general cognitive mechanisms
within a model of word learning in order to under-
stand clearly how these various processes might in-
teract in the natural word learning setting.
We address this gap by considering memory con-
straints and attentional mechanisms in the context
of a computational model of word-meaning acquisi-
tion. Specifically, we change an existing probabilis-
tic incremental model of word learning (Fazly et al,
2010) by integrating two new factors: (i) a forgetting
mechanism that causes the learned associations be-
tween words and meanings to decay over time; and
(ii) a mechanism that simulates the effects of atten-
tion to novelty on in-the-moment learning. The re-
sult is a more cognitively plausible word learning
model that includes a precise formulation of both
forgetting and attention to novelty. In simulations
using this new model, we show that a possible ex-
planation for the spacing effect is the interplay of
these two mechanisms, neither of which on its own
can account for the effect.
2 The Computational Model
We extend the model of Fazly et al (2010) ? hence-
forth referred to as FAS10 ? by integrating new
functionality to capture forgetting and attention to
novelty. The model of FAS10 is an appropriate start-
ing point for our study because it is an incremen-
tal model of word learning that learns probabilis-
tic associations between words and their semantic
properties from naturalistic data. Nonetheless, the
model assumes equal attention to all words and ob-
jects present in the input, and, although incremental,
it has a perfect memory for the internal represen-
tation of each processed input. Hence, as we will
show, it is incapable of simulating the spacing ef-
fects observed in humans.
2.1 The FAS10 Model
The input to the model is a sequence of utterances (a
set of words), each paired with a scene representa-
tion (a set of semantic features, representing what is
perceived when the words are heard), as in:
Utterance: { she, drinks, milk }
Scene: { ANIMATE, PERSON, FEMALE, CONSUME,
DRINK, SUBSTANCE, FOOD, DAIRY-PRODUCT }
For each word, the model of FAS10 learns a proba-
bility distribution over all possible features, p(.|w),
called the meaning probability of the word. Before
processing any input, all features are equally likely
for a word, and the word?s meaning probability is
uniform over all features. At each time step t, an
input utterance?scene pair (similar to the above
example) is processed. For each word w and seman-
tic feature f in the input pair, an alignment score,
at(w| f ), is calculated that specifies how strongly
the w? f pair are associated at time t. The alignment
score in FAS10 uses the meaning probabilities of
all the words in the utterance, which reflect the
knowledge of the model of word meanings up to
that point, as in:
at(w|f ) =
pt?1(f |w)
?
w??Ut
pt?1(f |w
?)
(1)
where pt?1( f |w) is the probability of f being part of
the meaning of word w at time t?1.
In the FAS10 model, pt(.|w) is then updated for
all the words in the utterance, using the accumulated
evidence from all prior and current co-occurrences
of w? f pairs. Specifically, an association score is
defined between a word and a feature, assoct(w, f ),
which is a summation of all the alignments for that
w and f up to time t.1 This association score is then
normalized using a smoothed version of the follow-
1In FAS10, assoct(w, f ) = assoct?1(w, f )+at(w| f ).
81
ing to yield pt( f |w):
pt( f |w) =
assoct( f , w)
?
f ??M
assoct( f
?, w)
(2)
whereM is the set of all observed features.
There are two observations to make about the
FAS10 model in the context of our desire to explore
attention and forgetting mechanisms in word learn-
ing. First, the calculation of alignments at(w| f ) in
Eqn. (1) treats all words equally, without special at-
tention to any particular item(s) in the input. Sec-
ond, the assoct( f ,w) term in Eqn. (2) encodes per-
fect memory of all calculated alignments since it is a
simple accumulated sum. These properties motivate
the changes to the formulation of the model that we
describe next.
2.2 Adding Attention to Novelty to the Model
As noted just above, the FAS10 model lacks any
mechanism to focus attention on certain words, as is
suggested by theories on the spacing effect (Hintz-
man, 1974). One robust observation in studies on
attention is that people attend to new items in a
learning scenario more than other items, leading to
improved learning of the novel items (e.g., Snyder
et al, 2008; MacPherson and Moore, 2010; Horst
et al, 2011). We thus model the effect of attention
to novelty when calculating alignments in our new
model: attention to a more novel word increases the
strength of its alignment with a feature ? and con-
sequently the learned word?feature association ?
compared to the alignment of a less novel word.
We modify the original alignment formulation of
FAS10 to incorporate a multiplicative novelty term
as follows (cf. Eqn. (1)):
at(w, f ) =
pt(f |w)
?
w??Ut
pt(f |w
?)
?noveltyt(w) (3)
where noveltyt(w) specifies the degree of novelty of
a word as a simple inverse function of recency. That
is, we assume that the more recently a word has been
observed by the model, the less novel it appears to
the model. Given a word w at time t that was last
observed at time tlastw , we calculate noveltyt(w) as:
noveltyt(w) = 1? recency(t, tlastw) (4)
where recency(t, tlastw) is inversely proportional
to the difference between t and tlastw . We set
novelty(w) to be 1 for the first exposure of the word.
2.3 Adding a Forgetting Mechanism to the
Model
Given the observation above (see end of Section 2.1)
that assoct(w, f ) embeds perfect memory in the
FAS10 model, we add a forgetting mechanism by re-
formulating assoct(w, f ) to incorporate a decay over
time of the component alignments at(w| f ). In or-
der to take a cognitively plausible approach to calcu-
lating this function, we observe that assoct(w, f ) in
FAS10 serves a similar function to activation in the
ACT-R model of memory (Anderson and Lebiere,
1998). In ACT-R, activation of an item is the sum
of individual memory strengthenings for that item,
just as assoct(w, f ) is a sum of individual align-
ment strengths for the pair (w, f ). A crucial dif-
ference is that memory strengthenings in ACT-R
undergo decay. Specifically, activation of an item
m after t presentations is calculated as: act(m)t =
ln(?tt ?=1 1/(t?t
?)d), where t ? is the time of each pre-
sentation, and d is a constant decay parameter.
We adapt this formulation for assoct(w, f ) with
the following changes: First, in the act formula, the
constant 1 in the numerator is the basic strength of
each presentation to memory. In our model, this
is not a constant but rather the strength of align-
ment, at(w| f ). Second, we assume that stronger
alignments should be more entrenched in memory
and thus decay more slowly than weaker alignments.
Thus, each alignment undergoes a decay which is
dependent on the strength of the alignment rather
than a constant decay d. We thus define assoct(w, f )
to be:
assoct( f ,w) = ln(
t
?
t ?=1
at ?(w| f )
(t? t ?)dat?
) (5)
where the decay for each alignment dat? is:
dat? =
d
at ?(w| f )
(6)
where d is a constant parameter. Note that the dat?
decreases as at ?(w| f ) increases.
82
apple: { FOOD:1, SOLID:.72, PRODUCE:.63,
EDIBLE-FRUIT:.32, PLANT-PART:.22,
PHYSICAL-ENTITY:.17, WHOLE:.06, ? ? ? }
Figure 1: True meaning features & scores for apple.
3 Input Generation
The input data consists of a set of utterances paired
with their corresponding scene representations. The
utterances are taken from the child-directed speech
(CDS) portion of the Manchester corpus (Theakston
et al, 2001), from CHILDES (MacWhinney, 2000),
which includes transcripts of conversations with 12
British children, ages 1;8 to 3;0. Every utterance
is considered as a bag of lemmatized words. Half of
the data is used as the development set, and the other
half in the final experiments.
Because no manually-annotated semantic repre-
sentation is available for any such large corpus of
CDS, we use the approach of Nematzadeh et al
(2012) to generate scene representations. For each
utterance a scene representation is generated artifi-
cially, by first creating an input-generation lexicon
that contains the true meaning (t(w)) of all the words
(w) in our corpus. The true meaning is a vector
of semantic features and their assigned scores (Fig-
ure 1). The semantic features for a word, depend-
ing on its part of speech, are chosen from different
sources such as WordNet.2 The score of each feature
is calculated automatically to give a higher value to
the more specific features (such as FRUIT for apple),
rather than more general features (like PHYSICAL-
ENTITY for apple).
To generate the scene representation S of an utter-
ance U, we probabilistically sample a subset of fea-
tures from the features in t(w) for each word w ?U.
Thus, in each occurrence of w some of its features
are missing from the scene, resulting in an imper-
fect sampling. This imperfect sampling allows us to
simulate noise and uncertainty in the input, as well
as the uncertainty of a child in determining the rele-
vant meaning elements in a scene. The scene S is the
union of all the features sampled for all the words in
the utterance. We note that the input-generation lex-
icon is only used in creating input corpora that are
naturalistic (based on child-directed speech), and not
in the learning of the model.
2http://wordnet.princeton.edu
4 Experiments
First, we examine the overall word learning be-
haviour in our new model. Then we look at spacing
effects in the learning of novel words. In both these
experiments, we compare the behavior of our model
with the model of FAS10 to clearly illustrate the ef-
fects of forgetting and attention to novelty in the new
model. Next we turn to further experiments explor-
ing in more detail the interaction of forgetting and
attention to novelty in producing spacing effects.
4.1 Word Learning over Time
Generally, the model of FAS10 has increasing com-
prehension of words as it is exposed to more input
over time. In our model, we expect attention to nov-
elty to facilitate word learning, by focusing more
on newly observed words, whereas forgetting is ex-
pected to hinder learning. We need to see if the new
model is able to learn words effectively when sub-
ject to the combined effects of these two influences.
To measure how well a word w is learned in each
model, we compare its learned meaning l(w) (a vec-
tor holding the values of the meaning probability
p(.|w)) to its true meaning t(w) (see Section 3):
acq(w) = sim(l(w), t(w)) (7)
where sim is the cosine similarity between the two
meaning vectors, t(w) and l(w). The better the
model learns the meaning of w, the closer l(w)
would get to t(w), and the higher the value of sim
would become. To evaluate the overall behaviour of
a model, at each point in time, we average the acq
score of all the words that the model has seen.
We train each model on 10,000 input utterance?
scene pairs and compare their patterns of word learn-
ing over time (Figure 2).3 We can see that in the
original model, the average acq score is mostly in-
creasing over time before leveling off. Our model,
starts at a higher average acq score compared to
FAS10?s model, since the effect of attention to nov-
elty is stronger than the effect of forgetting in early
stages of training. There is a sharp decrease in the
acq scores after the early training stage, which then
levels off. The early decrease in acq scores oc-
curs because many of the words the model is ex-
3The constant decay parameter d in Eqn. (6) is set to 0.03 in
this experiment.
83
Figure 2: Average acq score of the words over time, for
our model and FAS10?s model.
posed to early on are not learned very well initially,
and so forgetting occurs at a higher rate during that
stage. The model subsequently stabilizes, and the
acq scores level off although at a lower absolute
level than the FAS10 model. Note that when com-
paring these two models, we are interested in the
pattern of learning; in particular, we need to en-
sure that our new word learning model will even-
tually stabilize as expected. Our model stabilizes
at a lower average acq score since unlike FAS10?s
model, it does not implement a perfect memory.
4.2 The Spacing Effect in Novel Word
Learning
Vlach et al (2008) performed an experiment to in-
vestigate the effect of presentation spacing in learn-
ing novel word?object pairs in three-year-old chil-
dren. Each pair was presented 3 times in each of
two settings, either consecutively (massed presenta-
tion), or with a short play interval between each pre-
sentation (spaced presentation). Children were then
asked to identify the correct object corresponding to
the novel word. The number of correct responses
was significantly higher when the pairs were in the
spaced presentation compared to the massed presen-
tation. This result clearly demonstrates the spacing
effect in novel word learning in children.
Experiments on the spacing effect in adults have
typically examined and compared different amounts
of time between the spaced presentations, which we
refer to as the spacing interval. Another important
parameter in such studies is the time period between
the last training trial and the test trial(s), which we
refer to as the retention interval (Glenberg, 1976;
Bahrick and Phelps, 1987; Pavlik and Anderson,
2005). Since the experiment of Vlach et al (2008)
was designed for very young children, the proce-
dures were kept simple and did not vary these two
parameters. We design an experiment similar to that
of Vlach et al (2008) to examine the effect of spac-
ing in our model, but extend it to also study the role
of various spacing and retention intervals, for com-
parison to earlier adult studies.
4.2.1 Experimental Setup
First, the model is trained on 100 utterance?scene
pairs to simulate the operation of normal word learn-
ing prior to the experiment.4 Then a randomly
picked novel word (nw) that did not appear in the
training trials is introduced to the model in 3 teach-
ing trials, similar to Vlach et al?s (2008) experiment.
For each teaching trial, nw is added to a different ut-
terance, and its probabilistically-generated meaning
representation (see Section 3) is added to the corre-
sponding scene. We add nw to an utterance?scene
pair from our corpus to simulate the presentation of
the novel word during the natural interaction with
the child in the experimental setting.
The spacing interval between each of these 3
teaching trials is varied from 0 to 29 utterances, re-
sulting in 30 different simulations for each nw. For
example, when the spacing interval is 5, there are
5 utterances between each presentation of nw. A
spacing of 0 utterances yields the massed presenta-
tion. We run the experiment for 20 randomly-chosen
novel words to ensure that the pattern of the results
is not related to the meaning representation of a spe-
cific word.
For each spacing interval, we look at the acq score
of the novel word at two points in time, to simu-
late two retention intervals: One immediately after
the last presentation of the novel word (imm condi-
tion) and one at a later point in time (lat condition).
By looking at these two conditions, we can further
observe the effect of forgetting in our model, since
the decay in the model?s memory would be more se-
vere in the lat condition, compared to the imm con-
dition.5 The results reported here for each spacing
4In the experiments of Section 4.2.2 and Section 4.3, the
constant decay parameter d is equal to 0.04.
5Recall that each point of time in our model corresponds to
84
Figure 3: Average acq score of novel words over spacing
intervals, in our model and FAS10?s model.
interval average the acq scores of all the novel words
at the corresponding points in time.
4.2.2 The Basic Spacing Effect Results
Figure 3 shows the results of the simulations in
our model and the FAS10 model. We assume that
very small spacing intervals (but greater than 0)
correspond to the spaced presentation in the Vlach
et al (2008) experiments, while a spacing of 0 cor-
responds to the massed presentation. In the FAS10
model, the average acq score of words does not
change with spacing, and there is no difference be-
tween the imm and lat conditions, confirming that
this model fails to mimic the observed spacing ef-
fects. By contrast, in our model the average acq
score is greater in the small spacing intervals (1-
3) than in the massed presentation, mimicking the
Vlach et al (2008) results on children. This happens
because a nw appears more novel with larger spacing
intervals between each of its presentations resulting
in stronger alignments.
We can see two other interesting patterns in our
model: First, the average acq score of words for all
spacing intervals is greater in the imm condition than
in the lat condition. This occurs because there is
more forgetting in the model over the longer reten-
tion interval of lat. Second, in both conditions the
average acq score initially increases from a massed
presentation to the smaller spacing intervals. How-
ever, at spacing intervals between about 3 and 5,
processing an input pair. The acq score in the imm condition is
calculated at time t, which is immediately after the last presen-
tation of nw. The lat condition corresponds to t +20.
the acq score begins to decrease as spacing intervals
grow larger. As explained earlier, the initial increase
in acq scores for small spacing intervals results from
novelty of the words in a spaced presentation. How-
ever, for bigger spacing intervals the effect of nov-
elty is swamped by the much greater degree of for-
getting after a bigger spacing interval.
Although Vlach et al (2008) did not vary their
spacing and retention intervals, other spacing effect
studies on adults have done so. For example, Glen-
berg (1976) presented adults with word pairs to learn
under varying spacing intervals, and tested them af-
ter several different retention intervals (his experi-
ment 1). Our pattern of results in Figure 3 is in line
with his results. In particular, he found a nonmono-
tonic pattern of spacing similar to the pattern in our
model: learning of pairs was improved with increas-
ing spacing intervals up to a point, but there was a
decrease in performance for larger spacing intervals.
Also, the proportion of recalled pairs decreased for
longer retention intervals, similar to our lower per-
formance in the lat condition.
4.3 The Role of Forgetting and Attention
To fully understand the role as well as the neces-
sity of, both forgetting and attention to novelty in
our results, we test two other models under the same
conditions as the previous spacing experiment: (a) a
model with our mechanism for attention to novelty
but not forgetting, and (b) a model with our forget-
ting mechanism but no attention to novelty; see Fig-
ure 4 and Figure 5, respectively.
In the model that attends to novelty but does not
incorporate a memory decay mechanism (Figure 4),
the average acq score consistently increases as spac-
ing intervals grow bigger. This occurs because the
novel words appear more novel following bigger
spacing intervals, and thus attract more alignment
strength. Since the model does not forget, there is
no difference between the immediate (imm) and later
(lat) retention intervals. This pattern does not match
the spacing effect patterns of people, suggesting that
forgetting is a necessary aspect of our model?s abil-
ity to do so in the previous section.
On the other hand, in the model with forgetting
but no attentional mechanism (Figure 5), we see two
different behaviors in the imm and lat conditions. In
the imm condition, the average acq score decreases
85
Figure 4: Average acq score of the novel words over spac-
ing intervals, for the model with novelty but without for-
getting.
Figure 5: Average acq score of the novel words over spac-
ing intervals, for the model with forgetting but without
novelty.
consistently over spacing intervals. This is as ex-
pected, because the greater time between presenta-
tions means a greater degree of forgetting. Specif-
ically, the alignment scores decay more between
presentations of the word to be learned, given the
greater passage of time in larger spacing intervals.
The weaker alignments then lead to lower acq scores
in this condition.
Paradoxically, although this effect on learning
also holds in the lat condition, another factor is at
play, leading to better performance than in the imm
condition at all spacing intervals. Here the greater
retention interval ? the time between the last learn-
ing presentation and the test time ? leads to greater
forgetting in a manner that instead improves the acq
scores. Consider that the meaning representation
for a word includes some probability mass assigned
to irrelevant features ? i.e., those features that oc-
curred in an utterance?scene pair with the word but
are not part of its true meaning. Because such fea-
tures generally have lower probability than relevant
features (which are observed more consistently with
a word), a longer retention interval leads to them de-
caying more than the relevant features. Thus the lat
condition enables the model to better focus on the
features relevant to a word.
In conclusion, neither attention to novelty nor for-
getting alone achieves the pattern typical of the spac-
ing effects in people that our model shows in the
lower two plots in Figure 3. Hence we conclude that
both factors are necessary to our account, suggesting
that it is an interaction between the two that accounts
for people?s behaviour.
4.4 The ?Spacing Crossover Interaction?
In our model with attention to novelty and forgetting
(see Section 4.2), the average acq score was always
better in the imm condition than the lat condition.
However, researchers have observed other patterns
in spacing experiments. A particularly interesting
pattern found in some studies is that the plots of the
results for earlier and later retention intervals cross
as the spacing intervals are increased. That is, with
smaller spacing intervals, a shorter retention inter-
val (such as our imm condition) leads to better re-
sults, but with larger spacing intervals, a longer re-
tention interval (such as our lat condition) leads to
better results (Bahrick, 1979; Pavlik and Anderson,
2005). This interaction of spacing and retention in-
tervals results in a pattern referred to as the spacing
crossover interaction (Pavlik and Anderson, 2005).
This pattern is different from Glenberg?s (1976) ex-
periment and from the pattern of results shown ear-
lier for our model (Figure 3).
We looked at an experiment in which the spac-
ing crossover pattern was observed: Pavlik and An-
derson (2005) taught Japanese?English pairs to sub-
jects, varying the spacing and retention intervals.
One difference we noticed between the experiment
of Pavlik and Anderson (2005) and Glenberg (1976)
was that in the former, the presentation period of the
stimulus was 5 seconds, whereas in the latter, it was
3 seconds. We hypothesize that the difference be-
tween the amount of time for the presentation peri-
86
Figure 6: Average acq score of the novel words over spac-
ing intervals
ods might explain the different spacing patterns in
these experiments.
We currently cannot model presentation time di-
rectly in our model, since having access to an in-
put longer does not change its computation of align-
ments between words and features. However, we
can indirectly model a difference in presentation
time by modifying the amount of memory decay:
We assume that when an item is presented longer, it
is learned better and therefore subject to less forget-
ting. We run the spacing experiment with a smaller
forgetting parameter to model the longer presenta-
tion period used in Pavlik and Anderson?s (2005)
versus Glenberg (1976).6
Our results using the decreased level of forgetting,
given in Figure 6, show the expected crossover inter-
action between the retention and spacing intervals:
for smaller spacing intervals, the acq scores are bet-
ter in the imm condition, whereas for larger spacing
intervals, they are better in the lat condition. Thus,
our model suggests an explanation for the observed
crossover: in tasks which strengthen the learning of
the target item ? and thus lessen the effect of forget-
ting ? we expect to see a benefit of later retention
trials in experiments with people.
5 General Discussion and Future Work
The spacing effect (where people learn items better
when multiple presentations are spread over time)
has been studied extensively and is found to be ro-
bust over different types of tasks and domains. Many
6Here, the decay parameter is set to 0.03.
experiments have examined the spacing effect in the
context of word learning and other similar tasks.
Particularly, in a recent study of Vlach et al (2008),
young children demonstrated a spacing effect in a
novel word learning task.
We use computational modeling to show that by
changing a probabilistic associative model of word
learning to include both a forgetting and attentional
mechanism, the new model can account not only for
the child data, but for various patterns of spacing ef-
fect data in adults. Specifically, our model shows the
nonmonotonic pattern of spacing observed in the ex-
perimental data, where learning improves in shorter
spacing intervals, but worsens in bigger spacing in-
tervals. Our model can also replicate the observed
cross-over interaction between spacing and retention
intervals: for smaller spacing intervals, performance
is better when tested after a shorter retention inter-
val, whereas for bigger spacing intervals, it is better
after longer retention intervals. Finally, our results
confirm that by modelling word learning as a stan-
dalone development process, we cannot account for
the spacing effect. Instead, it is important to con-
sider word learning in the context of fundamental
cognitive processes of memory and attention.
Much remains to be investigated in our model.
For example, most human experiments examine the
effect of frequency of presentations of target items.
Also, the range of retention intervals that has been
studied is greater than what we have considered
here. In the future, we plan to study the effect of
these two parameters. In addition, with our current
model, the amount of time an item is presented to
the learner does not play a role. We can also re-
formulate our alignment mechanism to incorporate
a notion of the amount of time to consider an item
to be learned. Another interesting future direction,
especially in the context of word learning, is to de-
velop a more complete attentional mechanism, that
considers different parameters such as social cues
and linguistic cues. Finally, we will study the role
of forgetting and attention in modelling other rele-
vant experimental data (e.g., Kachergis et al, 2009;
Vlach and Sandhofer, 2010).
87
References
John .R. Anderson and Christian Lebiere. 1998. The
atomic components of thought. Lawrence Erl-
baum Associates.
Harry P. Bahrick. 1979. Maintenance of knowl-
edge: Questions about memory we forgot to ask.
Journal of Experimental Psychology: General,
108(3):296?308.
Harry P. Bahrick and Elizabeth Phelps. 1987. Reten-
tion of Spanish vocabulary over 8 years. Journal
of Experimental Psychology: Learning, Memory,
and Cognition, 13(2):344?349.
Paul Bloom. 2000. How Children Learn the Mean-
ings of Words. MIT Press.
Susan Carey. 1978. The child as word learner. In
M. Halle, J. Bresnan, and G. A. Miller, editors,
Linguistic Theory and Psychological Reality. The
MIT Press.
Malinda Carpenter, Katherine Nagell, Michael
Tomasello, George Butterworth, and Chris
Moore. 1998. Social cognition, joint attention,
and communicative competence from 9 to 15
months of age. Monographs of the Society for
Research in Child Development, 63(4).
Nicholas J. Cepeda, Harold Pashler, Edward Vul,
John T. Wixted, and Doug Rohrer. 2006. Dis-
tributed practice in verbal recall tasks: A review
and quantitative synthesis. Psychological Bul-
letin, 132(3):354 ? 380.
Lauren J. Cuddy and Larry L. Jacoby. 1982. When
forgetting helps memory: an analysis of repetition
effects. Journal of Verbal Learning and Verbal
Behavior, 21(4):451 ? 467.
Frank Dempster. 1989. Spacing effects and their im-
plications for theory and practice. Educational
Psychology Review, 1:309?330.
Frank N. Dempster. 1996. Distributing and manag-
ing the conditions of encoding and practice. Mem-
ory, pages 317?344.
Hermann Ebbinghaus. 1885. Memory: A contri-
bution to experimental psychology. New York,
Teachers College, Columbia University.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model
of cross-situational word learning. Cognitive Sci-
ence, 34(6):1017?1063.
Michael C. Frank, Sharon Goldwater, Thomas L.
Griffiths, and Joshua B. Tenenbaum. 2010. Mod-
eling human performance in statistical word seg-
menation. Cognition, 117:107?125.
Arthur Glenberg. 1979. Component-levels theory of
the effects of spacing of repetitions on recall and
recognition. Memory and Cognition, 7:95?112.
Arthur M. Glenberg. 1976. Monotonic and non-
monotonic lag effects in paired-associate and
recognition memory paradigms. Journal of Ver-
bal Learning & Verbal Behavior, 15(1).
Roberta M. Golinkoff, Kathy Hirsh-Pasek, Leslie M.
Bailey, and Neil R. Wegner. 1992. Young chil-
dren and adults use lexical principles to learn new
nouns. Developmental Psychology, 28(1):99?
108.
Douglas L. Hintzman. 1974. Theoretical implica-
tions of the spacing effect.
Jessica S. Horst, Larissa K. Samuelson, Sarah C.
Kucker, and Bob McMurray. 2011. Whats new?
children prefer novelty in referent selection. Cog-
nition, 118(2):234 ? 244.
Larry L. Jacoby. 1978. On interpreting the effects
of repetition: Solving a problem versus remem-
bering a solution. Journal of Verbal Learning and
Verbal Behavior, 17(6):649 ? 667.
George Kachergis, Chen Yu, and Richard Shiffrin.
2009. Temporal contiguity in cross-situational
statistical learning.
Amy C. MacPherson and Chris Moore. 2010. Un-
derstanding interest in the second year of life. In-
fancy, 15(3):324?335.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk, volume 2: The
Database. Erlbaum, 3rd edition.
Ellen M. Markman and Gwyn F. Wachtel. 1988.
Children?s use of mutual exclusivity to constrain
the meanings of words. Cognitive Psychology,
20:121?157.
Arthur W. Melton. 1967. Repetition and retrieval
from memory. Science, 158:532.
88
Aida Nematzadeh, Afsaneh Fazly, and Suzanne
Stevenson. 2012. Interaction of word learning and
semantic category formation in late talking. In
Proc. of CogSci?12. To appear.
Philip I. Pavlik and John R. Anderson. 2005. Prac-
tice and forgetting effects on vocabulary memory:
An activationbased model of the spacing effect.
Cognitive Science, 29:559?586.
W.V.O. Quine. 1960. Word and Object. MIT Press.
Terry Regier. 2005. The emergence of words: At-
tentional learning in form and meaning. Cognitive
Science, 29:819?865.
Jeffery Mark Siskind. 1996. A computational study
of cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61:39?91.
Linda B Smith, Eliana Colunga, and Hanako
Yoshida. 2010. Knowledge as process:
Contextually-cued attention and early word
learning. Cogn Sci, 34(7):1287?314.
Kelly A. Snyder, Michael P. BlanK, and chad J. Mar-
solek. 2008. What form of memory underlies nov-
elty preferences? Psychological Bulletin and Re-
view, 15(2):315 ? 321.
Anna L. Theakston, Elena V. Lieven, Julian M. Pine,
and Caroline F. Rowland. 2001. The role of per-
formance limitations in the acquisition of verb?
argument structure: An alternative account. J. of
Child Language, 28:127?152.
Haley A Vlach and Catherine M Sandhofer. 2010.
Desirable difficulties in cross-situational word
learning.
Haley A. Vlach, Catherine M. Sandhofer, and Nate
Kornell. 2008. The Spacing Effect in Children?s
Memory and Category Induction. Cognition,
109(1):163?167, October.
Chen Yu. 2005. The emergence of links between
lexical acquisition and object categorization: A
computational study. Connection Science, 17(3?
4):381?397.
89
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 231?240,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Acquisition of Desires before Beliefs: A Computational Investigation
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
The acquisition of Belief verbs lags be-
hind the acquisition of Desire verbs in
children. Some psycholinguistic theo-
ries attribute this lag to conceptual differ-
ences between the two classes, while oth-
ers suggest that syntactic differences are
responsible. Through computational ex-
periments, we show that a probabilistic
verb learning model exhibits the pattern of
acquisition, even though there is no dif-
ference in the model in the difficulty of
the semantic or syntactic properties of Be-
lief vs. Desire verbs. Our results point
to the distributional properties of various
verb classes as a potentially important, and
heretofore unexplored, factor in the ob-
served developmental lag of Belief verbs.
1 Introduction
Psycholinguistic studies have shown great inter-
est in the learning of Mental State Verbs (MSVs),
such as think and want, given the various cogni-
tive and linguistic challenges in their acquisition.
MSVs refer to an entity?s inner states, such as
thoughts and wishes, which the language learner
must be able to perceive and conceptualize appro-
priately. Moreover, such verbs often appear in a
Sentential Complement (SC) construction, which
is complex for children because of the embedded
clause.
Despite some shared properties, MSVs are
a heterogeneous group, with different types of
verbs exhibiting different developmental patterns.
Specifically, a wealth of research shows that chil-
dren produce Desire verbs, such as want and
wish, earlier than Belief verbs, such as think and
know (Shatz et al, 1983; Bartsch and Wellman,
1995; Asplin, 2002; Perner et al, 2003; de Vil-
liers, 2005; Papafragou et al, 2007; Pascual et al,
2008). Some explanations for this pattern posit
that differences in the syntactic usages of Desire
and Belief verbs underlie the observed develop-
mental lag of the latter (de Villiers, 2005; Pas-
cual et al, 2008). In particular, Desire verbs oc-
cur mostly with an infinitival SC (as in I want
(her) to leave), while Belief verbs occur mostly
with a finite SC (a full tensed embedded clause,
as in I think (that) she left). Notably, infiniti-
vals appear earlier than finite SCs in the speech
of young children (Bloom et al, 1984, 1989).
Others suggest that Desire verbs are conceptu-
ally simpler (Bartsch and Wellman, 1995) or prag-
matically/communicatively more salient (Perner,
1988; Fodor, 1992; Perner et al, 2003). Propo-
nents of the conceptual and pragmatic accounts ar-
gue that syntax alone cannot explain the delay in
the acquisition of Belief verbs, because children
use finite SCs with verbs of Communication (e.g.,
say) and Perception (e.g., see) long before they
use them with Belief verbs (Bartsch and Wellman,
1995).
We use a computational model of verb argu-
ment structure acquisition to shed light on the fac-
tors that might be responsible for the developmen-
tal gap between Desire and Belief verbs. Impor-
tantly, our model exhibits the observed pattern of
learning Desire before Belief verbs, without hav-
ing to encode any differences in difficulty between
the two classes in terms of their syntactic or con-
ceptual/pragmatic requirements. The behaviour of
the model can thus be attributed to its probabilistic
learning mechanisms in conjunction with the dis-
tributional properties of the input. In particular, we
investigate how the model?s learning mechanism
interacts with the distributions of several classes
of verbs ? including Belief, Desire, Perception,
Communication, and Action ? in the finite and
infinitival SC syntax to produce the observed pat-
tern of acquisition of Desire and Belief verbs. Us-
ing a computational model can reveal the poten-
231
tial effects of interactions of verb classes in hu-
man language acquisition which would be difficult
to investigate experimentally. Our results suggest
that the distributional properties of relevant verb
classes are a potentially important, and heretofore
unexplored, factor in experimental studies of the
developmental lag of Belief verbs.
2 The Computational Model
We require an incremental model in which we
can examine developmental patterns as it gradu-
ally learns relevant aspects of argument structures.
This task calls for an ability to represent the se-
mantic and syntactic properties of verb usages, in-
cluding those containing MSVs and other kinds of
verbs taking sentential complements (SCs). Most
computational models of verb argument structure
acquisition have largely focused on physical ac-
tion verbs (Alishahi and Stevenson, 2008; Chang,
2009; Perfors et al, 2010; Parisien and Steven-
son, 2011). Recently, Barak et al (2012) ex-
tended the incremental Bayesian model of Al-
ishahi and Stevenson (2008) to include the syntac-
tic and semantic features required for the process-
ing of MSVs and other verbs that take SCs. While
Barak et al (2012) modeled some developmental
patterns of MSVs overall, their work did not ac-
count for the difference between Desire and Be-
lief verbs. In this section, we present their model,
which we adopt for our experiments. In Section 3,
we describe how we modify the representation of
the input in Barak et al (2012) to enable our inves-
tigation of the differences among the MSV classes.
2.1 Overview of the Model
The input to the Barak et al (2012) model is a
sequence of frames, where each frame is a col-
lection of syntactic and semantic features repre-
senting what the learner might extract from an ut-
terance s/he has heard paired with a scene s/he
has perceived. In particular, we consider syntactic
properties, including syntactic pattern, argument
count, and complement type, as well as seman-
tic properties, including event primitives and event
participants. Table 1 presents a sample frame il-
lustrating possible values for these features.
The model incrementally groups the input
frames into clusters that reflect probabilistic as-
sociations of the syntactic and semantic features
across similar verb usages. Each learned cluster
is a probabilistic (and possibly noisy) representa-
head predicate think
other predicate make
Syntactic Features:
syntactic pattern arg1 verb arg2 verb arg3
argument count 3
complement type SC-fin
Semantic Features:
event primitives { state, consider , cogitate, action }
event participants { experiencer , perceiver , considerer}
{ agent , animate}
{ theme, changed}
Table 1: An example input frame. The Syntactic features
reflect an utterance such as He thinks Mom made pancakes:
i.e., syntactic pattern ?arg1 verb arg2 verb arg3?, 3 arguments,
and finite SC. The Semantic features reflect a corresponding
conceptualized belief event with a physical action described
in the SC ({state, consider , cogitate, action}) whose
?arg1? participant ({experiencer , perceiver , considerer})
perceives the ?arg2? ({agent , animate}) acting on the ?arg3?
({theme, changed}).
tion of an argument structure construction: e.g.,
a cluster containing frames corresponding to us-
ages such as I eat apples, She took the ball, and
He got a book, etc., represents a Transitive Action
construction.1 Note that a cluster operates as more
than simply a set of similar frames: The model
can use the probabilistic associations among the
various features of the frames in a cluster to gen-
eralize over the individual verb usages that it has
seen. For example, if the model is presented with a
frame corresponding to a transitive utterance using
a verb it has not observed before, such as She gor-
ped the ball, the example cluster above would lead
the model to predict that gorp has semantic event
primitives in common with other Action verbs like
eat, take, and get. Such probabilistic reasoning is
especially powerful because clusters involve com-
plex interactions of features, and the model rea-
sons across all such clusters to make suitable gen-
eralizations over its learned knowledge.
2.2 Algorithm for Learning Clusters
The model groups input frames into clusters on
the basis of the overall similarity in the values of
their syntactic and semantic features. Importantly,
the model learns these clusters incrementally; the
number and type of clusters is not predetermined.
The model considers the creation of a new cluster
for a given frame if the frame is not sufficiently
similar to any of the existing clusters. Formally,
the model finds the best cluster for a given input
1Note that, because the associations are probabilistic, a
construction may be represented by more than one cluster.
232
frame F as in:
BestCluster(F ) = argmax
k?Clusters
P (k|F ) (1)
where k ranges over all existing clusters and a new
one. Using Bayes rule:
P (k|F ) = P (k)P (F |k)P (F ) ? P (k)P (F |k) (2)
The prior probability of a cluster P (k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
prior to larger clusters, representing more frequent
constructions. The likelihood P (F |k) is estimated
based on the match of feature values in F and in
the frames of k (assuming independence of the
features):
P (F |k) =
?
i?Features
Pi(j|k) (3)
where i refers to the ith feature of F and j refers
to its value, and Pi(j|k) is calculated using a
smoothed version of:
Pi(j|k) =
counti(j, k)
nk
(4)
where counti(j, k) is the number of times feature
i has the value j in cluster k, and nk is the number
of frames in k.
2.3 Attention to Mental Content
One factor proposed to play an important role in
the acquisition of MSVs is the difficulty children
have in being aware of (or perceiving the salience
of) the mental content of a scene that an utterance
may be describing (Papafragou et al, 2007). This
difficulty arises because the aspects of a scene as-
sociated with an MSV ? the ?believing? or the
?wanting? ? are not directly observable, as they
involve the inner states of an event participant. In-
stead, younger children tend to focus on the phys-
ical (observable) parts of the scene, which gener-
ally correspond to the event described in the em-
bedded clause of an MSV utterance. For instance,
young children may focus on the ?making? action
in He thinks Mom made pancakes, rather than on
the ?thinking?.
A key component of the model of Barak
et al (2012) is a mechanism that simulates the
gradually-developing ability in children to attend
to the mental content rather than solely to the (em-
bedded) physical action. This mechanism basi-
cally entails that the model may ?misinterpret? an
input frame containing an MSV as focusing on the
semantics of the action in the sentential comple-
ment. Specifically, when receiving an input frame
with an MSV, as in Table 1, there is a probability p
that the frame is perceived with attention to the se-
mantics corresponding to the physical action verb
(here, make). In this case, the model correctly in-
cludes the syntactic features as in Table 1, on the
assumption that the child can accurately note the
number and pattern of arguments. However, the
model replaces the semantic features with those
that correspond to the physical action event and its
participants. At very early stages, p is very high
(close to 1), simulating the much greater saliency
of physical actions compared to mental events for
younger children. As the model ?ages? (i.e., re-
ceives more input), p decreases, giving more and
more attention to the mental content, gradually ap-
proaching adult-like abilities.
3 Experimental Setup
3.1 Generation of the Input Corpora
Because there are no readily available large cor-
pora of actual child-directed speech (CDS) associ-
ated with appropriate semantic representations, we
generate artificial corpora for our simulations that
mimic the relevant syntactic properties of CDS
along with automatically-produced semantic prop-
erties. Importantly, these artificial corpora have
the distributional properties of the argument struc-
tures for the verbs under investigation based on
an analysis of verb usages in CDS. To accomplish
this, we adopt and extend the input-generation lex-
icon of Barak et al (2012), which is used to au-
tomatically generate the syntactic and semantic
features of the frames that serve as input to the
model. Using this lexicon, each simulation cor-
pus is created through a probabilistic generation of
argument structure frames according to their rela-
tive frequencies of occurrence in CDS. Since the
corpora are probabilistically generated, all exper-
imental results are averaged over simulations on
100 different input corpora, to ensure the results
are not dependent on idiosyncratic properties of a
single generated corpus.
Our input-generation lexicon contains 31 verbs
from various semantic classes and different fre-
quency ranges; these verbs appear in a variety
233
Semantic Verb Frequency % Relative
class frequency with
SC-fin SC-inf
Belief think 13829 100 -
bet 391 100 -
guess 278 76 -
know 7189 61 -
believe 78 21 -
Desire wish 132 94 -
hope 290 86 -
want 8425 - 76
like 6944 - 51
need 1690 - 60
Communication tell 2953 64 -
say 8622 60 -
ask 818 29 10
speak 62 - -
talk 1322 - -
Perception hear 1370 21 25
see 9717 14 -
look 5856 9 -
watch 1045 - 27
listen 413 33 2
Action go 20364 - 5
get 16493 - 14
make 4165 - 10
put 8794 - -
come 6083 - -
eat 3894 - -
take 3239 - -
play 2565 - -
sit 2462 - -
give 2341 - -
fall 1555 - -
Table 2: The list of our 31 verbs from the five semantic
classes, along with their overall frequency, and their rela-
tive frequency with the finite SC (SC-fin) or the infinitival
SC (SC-inf).
of syntactic patterns including the sentential com-
plement (SC) construction. Our focus here is on
learning the Belief and Desire classes; however,
we include verbs from other classes to have a re-
alistic context of MSV acquisition in the presence
of other types of verbs. In particular, we include
(physical) Action verbs because of their frequent
usage in CDS, and we include Communication
and Perception groups because of their suggested
role in the acquisition of MSVs (Bloom et al,
1989; de Villiers, 2005). Table 2 lists the verbs of
each semantic class, along with their overall fre-
quency and their relative frequency with the finite
(SC-fin) and infinitival SC (SC-inf) in our data.
For each of these 31 verbs, the distributional in-
formation about its argument structure was manu-
ally extracted from a random sample of 100 CDS
usages (or all usages if fewer than 100) from eight
corpora from CHILDES (MacWhinney, 2000).2
The input-generation lexicon then contains the
overall frequency of each verb, as well as the rela-
tive frequency with which it appears with each of
its argument structures. Each argument structure
entry for a verb also contains the values for all the
syntactic and semantic features in a frame (see Ta-
ble 1 for an example), which are determined from
the manual inspection of the usages.
The values for syntactic features are based on
simple observation of the order and number of
verbs and arguments in the usage, and, if an ar-
gument is an SC, whether it is finite or infiniti-
val. We add this latter feature (the type of the
SC) to the syntactic representation used by Barak
et al (2012) to allow distinguishing the syntac-
tic properties associated with Desire and Belief
verbs. Note that this feature does not incorporate
any potential level of difficulty in processing an
infinitival vs. finite SC; the feature simply records
that there are three different types of embedded ar-
guments: SC-inf, SC-fin, or none. Thus, while
Desire and Belief verbs that typically occur with
an SC-inf or SC-fin have a distinguishing feature,
there is nothing in this representation that makes
Desire verbs inherently easier to process. This
syntactic representation reflects our assumptions
that a learner: (i) understands basic syntactic prop-
erties of an utterance, such as syntactic categories
(e.g., noun and verb) and word order; and (ii) dis-
tinguishes between a finite complement, as in He
thinks that Mom left, and an infinitival, as in He
wants Mom to leave.
The values for the semantic features of a verb
and its arguments are based on a simple taxonomy
of event and participant role properties adapted
from several resources, including Alishahi and
Stevenson (2008), Kipper et al (2008), and Dowty
(1991). In particular, we assume that the learner is
able to perceive and conceptualize the general se-
mantic properties of different kinds of events (e.g.,
state and action), as well as those of the event par-
ticipants (e.g., agent, experiencer, and theme). In
an adaptation of the lexicon of Barak et al, we
make minimal assumptions about shared seman-
tics across verb classes. Specifically, to encode
suitable semantic distinctions among MSVs, and
between MSVs and other verbs, we aimed for a
representation that would capture reasonable as-
2Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al (1974); Sachs (1983); Lieven et al (2009).
234
sumptions about high-level similarities and differ-
ences among the verb classes. As with the syn-
tactic features, we ensured that we did not simply
encode the result we are investigating (that chil-
dren have facility with Desire verbs before Be-
lief verbs) by making the representation for Desire
verbs easier to learn.
In the results presented in Section 4, ?our
model? refers to the computational model of Barak
et al (2012) together with our modifications to the
input representation.
3.2 Simulations and Verb Prediction
Psycholinguistic studies have used variations of
a novel verb prediction task to examine how
strongly children (or adults) have learned to asso-
ciate the various syntactic and semantic properties
of a typical MSV usage. In particular, the typical
Desire verb usage combines desire semantics with
an infinitival SC syntax, while the typical Belief
verb usage combines belief semantics with a finite
SC syntax. In investigating the salience of these
associations in human experiments, participants
are presented with an utterance containing a nonce
verb with an SC (e.g., He gorped that his grand-
mother was in the bed), sometimes paired with a
corresponding scene representing a mental event
(e.g., a picture or a silent video depicting a think-
ing event with heightened saliency). An experi-
menter then asks each participant what the nonce
verb (gorp) ?means? ? i.e., what existing English
verb does it correspond to (see, e.g., Asplin, 2002;
Papafragou et al, 2007). The expectation is that,
e.g., if a participant has a well-entrenched Belief
construction, then they should have a strong as-
sociation between the finite-SC syntax and belief
semantics, and hence should produce more Belief
verbs as the meaning of a novel verb in an finite-
SC utterance (and analogously for infinitival SCs
and Desire verbs).
We perform simulations that are based on such
psycholinguistic experiments. After training the
model on some number of input frames, we then
present it with a test frame in which the main verb
(head predicate) is replaced by a nonce verb like
gorp (a verb that doesn?t occur in our lexicon).
Analogously to the human experiments, in order
to study the differences in the strength of associ-
ation between the syntax and semantics of Desire
and Belief verbs, we present the model with two
types of test frames: (i) a typical desire test frame,
with syntactic features corresponding to the infini-
tival SC syntax, optionally paired (depending on
the experiment) with semantic features associated
with a Desire verb in our lexicon; and (ii) a typi-
cal belief test frame, with syntactic features corre-
sponding to the finite SC syntax, optionally paired
with semantic features from a Belief verb.3
Given a test frame Ftest, we use the clusters
learned by the model to calculate the likelihood of
each of the 31 verbs v as the response of the model
indicating the meaning of the novel verb, as in:
P (v|Ftest) (5)
=
?
k?Clusters
Phead(v|k)P (k|Ftest)
?
?
k?Clusters
Phead(v|k)P (Ftest|k)P (k)
where Phead(v|k) is the probability of the head
feature having the value v in cluster k, calculated
as in Eqn. (4); P (Ftest|k) is the probability of the
test frame Ftest given cluster k, calculated as in
Eqn. (3); and P (k) is the prior probability of clus-
ter k, calculated as explained in Section 2.2.
What we really want to know is the likelihood
of the model producing a verb from each of the
semantic classes, rather than the likelihood of any
particular verb. For each test frame, we calculate
the likelihood of each semantic class by summing
the likelihoods of the verbs in that class:
P (Class|Ftest) =
?
vc?Class
P (vc|Ftest)
where vc is one of the verbs in Class, and Class
ranges over the 5 classes in Table 2. We average
the verb class likelihoods across the 100 simula-
tions.
4 Experimental Results
The novel verb prediction experiments described
above have found differences in the performance
of children across the two MSV classes (e.g., As-
plin, 2002; Papafragou et al, 2007). For exam-
ple, children performed better at predicting that a
novel verb is a Desire verb in a typical desire con-
text (infinitival-SC utterance paired with a desire
scene), compared to their performance at identify-
ing a novel verb as a Belief verb in a typical belief
3Table 2 shows that, in our data, Belief verbs occur ex-
clusively with finite clauses in an SC usage. Although Desire
verbs occur in both SC-inf and SC-fin usages, the former out-
number the latter by almost 30 to 1 over all Desire verbs.
235
context (finite-SC utterance accompanied by a be-
lief scene). In Section 4.1, we examine whether
the model exhibits this behaviour in our verb class
prediction task, thereby mimicking children?s lag
in facility with Belief verbs compared to Desire
verbs.
Recall that some researchers attribute the
above-mentioned developmental gap to the con-
ceptual and pragmatic differences between the two
MSV classes, whereas others suggest it is due to a
difference in the syntactic requirements of the two
classes. As noted in Section 3.1, we have tailored
our representation of Desire and Belief verbs to
not build in any differences in the ease or difficulty
of acquiring their syntactic or semantic properties.
Moreover, the possibility in the model for ?misin-
terpretation? of mental content as action semantics
(see Section 2.3) also applies equally to both types
of verbs. Thus, any observed performance gap in
the model reflects an interaction between its pro-
cessing approach and the distributional properties
of CDS. To better understand the role of the in-
put, in Section 4.2 we examine how the distribu-
tional pattern of appearances of various semantic
classes of verbs (including Belief, Desire, Com-
munication, Perception and Action verbs) with the
finite and infinitival SC constructions affects the
learning of the two types of MSVs.
4.1 Verb Prediction Simulations
Here we compare the verb prediction responses of
the participants in the experiments of Papafragou
et al (2007) (PCG), with those of the model when
presented with a novel verb in a typical desire or
belief test frame. (See Section 3.2 for how we con-
struct these frames.) PCG report verb responses
for the novel verb meaning as desire, belief, or ac-
tion, where the latter category contains all other
verb responses. Looking closely at the latter cat-
egory in PCG, we find that most verbs are what
we have termed (physical) Action verbs. We thus
report the verb class likelihoods of the model for
the Belief, Desire, and Action verbs in our lexi-
con. To compare the model?s responses with those
of the children and adults in PCG, we report the
responses of the model to the test frames at two
test points: after training the model with 500 in-
put frames, resembling the ?Child stage?, and after
presenting the model with 10, 000 input frames,
representing the ?Adult stage?.
Figure 1(a) gives the percent verb types from
(a) Human participants in Papafragou et al (2007)
(b) The model
Figure 1: (a) Percent verb types produced by adult and
child participants given a desire or belief utterance and scene.
(b) The model?s verb class likelihoods given a desire or be-
lief test frame. Child stage is represented by 500 input frames
compared to the 10, 000 input frames for Adult stage.
PCG;4 Figure 1(b) presents the results of the
model. Similarly to the children in PCG, the
model at earlier stages of learning (?Child stage?)
is better at predicting Desire verbs for a desire test
frame (.56) than it is at predicting Belief verbs for
a belief test frame (.42) ? cf. 59% Desire vs.
41% Belief prediction for PCG. In addition, as for
both the children and adult participants of PCG,
the model produces more Action verbs in a desire
context than in a belief context at both stages.
We note that although the adult participants of
PCG perform well at identifying both Desire and
Belief verbs, the model does not identify Belief
verbs with the same accuracy as it does Desire
verbs, even after processing 10, 000 input frames
(i.e., the ?Adult stage?). In Section 4.2, we will see
that this is due to the model forming strong asso-
ciations between the Communication and Percep-
tion verbs and the SC-fin usage (the typical syn-
tax of Belief verbs). These associations might be
4Based on results presented in Table 4, Page 149 in Pa-
pafragou et al (2007), for the utterance and scene condition.
236
overly strong in our model because of the limited
number of verbs and verb classes ? an issue we
will need to address in the future. We also note
that, unlike the results of PCG, the model only
rarely produces Desire verbs in a Belief context.
This also may be due to our choice of Desire verbs,
which have extremely few SC-fin usages overall.
To summarize, similarly to children (Asplin,
2002; Papafragou et al, 2007), the model per-
forms better at identifying Desire verbs compared
to Belief verbs. Moreover, we replicate the ex-
perimental results of PCG without encoding any
conceptual or syntactic differences in difficulty be-
tween the two types of verbs. Specifically, because
the representation of Desire and Belief classes in
our experiments does not build in a bias due to the
ease of processing Desire verbs, the differential
results in the model must be due to the interac-
tion of the different distributional patterns in CDS
(see Table 2) and the processing approach of the
model. Although this finding does not rule out the
role of conceptual or syntactic differences between
Desire and Belief verbs in delayed acquisition of
the latter, it points to the importance of the dis-
tributional patterns as a potentially important and
relevant factor worth further study in human ex-
periments. We further investigate this hypothesis
in the following section.
4.2 A Closer Look at the Role of Syntax
The goal of the experiments presented here is to
understand how an interaction among the 5 dif-
ferent semantic classes of verbs, in terms of their
distribution of appearance with the two types of
SC constructions, coupled with the probabilistic
?misinterpretation? of MSVs in the model, might
play a role in the acquisition of Desire before Be-
lief verbs. Because our focus is on the syntactic
properties of the verbs, we present the model with
partial test frames containing a novel verb and syn-
tactic features that correspond to either a finite SC
usage (the typical use of a Belief verb) or an infini-
tival SC usage (the typical use of a Desire verb).5
We refer to the partial test frames as SC-fin or SC-
inf test frames. We test the model periodically,
over the course of 10, 000 input frames, in order
to examine the progression of the verb class like-
5Verb prediction given an isolated utterance has been per-
formed with adult participants (e.g., Gleitman et al, 2005;
Papafragou et al, 2007). Here we simulate the settings of
such experiments, but do not compare our results with the
experimental data, since they have not included children.
(a) Model?s likelihoods given SC-inf test frame
(b) Model?s likelihoods given SC-fin test frame
Figure 2: The model?s verb class likelihoods for the indi-
vidual semantic classes.
lihoods over time.
First, we examine the verb class prediction like-
lihoods, given an SC-inf test frame; see Fig-
ure 2(a). We can see that all through training,
the likelihoods are mainly divided between Desire
and Action verbs, with the Desire likelihood im-
proving over time. Looking at Table 2, we note
that the Desire and Action verbs have the highest
frequency of occurrence with SC-inf (taking into
account both the overall frequency of verbs, and
their relative frequency with SC-inf), contributing
to their strength of association with the infinitival-
SC syntax. Note that the very high likelihood of
Action verbs given an SC-inf test frame, especially
at the earlier stages of training, cannot be solely
due to their occurrence with SC-inf, since these
verbs mostly occur with other syntactic patterns.
Recall that the model incorporates a mechanism
that simulates a higher probability of erroneously
attending to the physical action (as opposed to the
mental event) at earlier stages, simulating what has
been observed in young children (see Section 2.3
for details). We believe that this mechanism is re-
237
sponsible for some of the Action verb responses of
the model for an SC-inf test frame.
Next, we look at the pattern of verb class likeli-
hoods given an SC-fin test frame; see Figure 2(b).
We can see that the likelihoods here are divided
across a larger number of classes ? namely, Ac-
tion, Communication, and Perception ? com-
pared with Figure 2(a) for the SC-inf test frame.
Since Action verbs do not occur in our data with
SC-fin (see Table 2), their likelihood here comes
from the misinterpretation of mental events (ac-
companied with SC-fin) as action. The initially
high likelihoods of Communication and Percep-
tion verbs results from their high frequency of oc-
currence with SC-fin. Because at this stage Belief
verbs are not always correctly associated with SC-
fin due to the high probability of misinterpreting
them as action, we see a lower likelihood of pre-
dicting Belief verbs. Eventually, the model pro-
duces more Belief responses than any other verb
class, since Beliefs have the highest frequency of
occurrence with the finite-SC syntax.
To summarize, our results here confirm our hy-
pothesis that the distributional properties of the
verb classes with the finite and infinitival SC pat-
terns, coupled with the learning mechanisms of
the model, account for the observed developmen-
tal pattern of MSV acquisition in our model.
5 Discussion
We use a computational model of verb argument
structure learning to shed light on the factors that
might underlie the earlier acquisition of Desire
verbs (e.g., wish and want) than Belief verbs (e.g.,
think and know). Although this developmental gap
has been noted by many researchers, there are at
least two competing theories as to what might be
the important factors: differences in the concep-
tual/pragmatic requirements (e.g., Fodor, 1992;
Bartsch and Wellman, 1995; Perner et al, 2003),
or differences in the syntactic properties (e.g., de
Villiers, 2005; Pascual et al, 2008). Using a com-
putational model, we suggest other factors that
may play a role in an explanation of the observed
gap, and should be taken into account in experi-
mental studies on human subjects.
First, we show that the model exhibits a simi-
lar pattern to children, in that it performs better at
predicting Desire verbs compared to Belief verbs,
given a novel verb paired with typical Desire or
Belief syntax and semantics, respectively. This
difference in performance suggests that the model
forms a strong association between the desire se-
mantics and the infinitival-SC syntax ? one that
is formed earlier and is stronger than the associa-
tion it forms between the belief semantics and the
finite-SC syntax. Importantly, the replication of
this behaviour in the model does not require an
explicit encoding of conceptual/pragmatic differ-
ences between Desire and Belief verbs, nor of a
difference between the two types of SC syntax (fi-
nite and infinitival) with respect to their ease of
acquisition. Instead, we find that what is responsi-
ble for the model?s behaviour is the distribution of
the semantic verb classes (Desire, Belief, Percep-
tion, Communication, and Action) with the finite
and infinitival SC syntactic patterns in the input.
Children are also found to produce
semantically-concrete verbs, such as Com-
munication (e.g., say) and Perception verbs (e.g.,
see), with the finite SC before they produce
(more abstract) Belief verbs with the same syntax.
Psycholinguistic theories have different views
on what this observation tells us about the delay
in the acquisition of Belief verbs. For example,
Bartsch and Wellman (1995) suggest that the
earlier production of Communication verbs shows
that even when children have learned the finite-SC
syntax (and use it with more concrete verbs),
they lack the required conceptual development
to talk about the beliefs of others. Our results
suggest a different take on these same findings:
because Communication (and Perception) verbs
also frequently appear with the finite-SC syntax in
the input, the model learns a relatively strong as-
sociation between each of these semantic classes
and the finite SC. This in turn causes a delay in
the formation of a sufficiently-strong association
between the Belief verbs and that same syntax,
compared with the association between the Desire
verbs and the infinitival SC.
de Villiers (2005) suggests that associating
Communication verbs with the finite-SC syntax
has a facilitating effect on the acquisition of Be-
lief verbs. In our model, we observe a competi-
tion between Communication and Belief verbs, in
terms of their association with the finite-SC syn-
tax. To further explore the hypothesis of de Vil-
liers (2005) will require expanding our model with
enriched semantic representations that enable us to
investigate the bootstrapping role of Communica-
tion verbs in the acquisition of Beliefs.
238
References
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789?
834.
Kristen N. Asplin. 2002. Can complement frames
help children learn the meaning of abstract
verbs? Ph.D. thesis, UMass Amherst.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2012. Modeling the acquisition of mental
state verbs. NAACL-HLT 2012.
Karen Bartsch and Henry M. Wellman. 1995.
Children talk about the mind. New York: Ox-
ford Univ. Press.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380?420.
Lois Bloom, Matthew Rispoli, Barbara Gartner,
and Jeremie Hafitz. 1989. Acquisition of com-
plementation. Journal of Child Language,
16(01):101?120.
Lois Bloom, Jo Tackeff, and Margaret Lahey.
1984. Learning to in complement constructions.
Journal of Child Language, 11(02):391?406.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Nancy Chih-Lin Chang. 2009. Constructing
grammar: A computational model of the emer-
gence of early constructions. Ph.D. thesis, Uni-
versity of California, Berkeley.
Jill G. de Villiers. 2005. Can language acquisi-
tion give children a point of view. In Why Lan-
guage Matters for Theory of Mind, pages 199?
232. Oxford Univ. Press.
David Dowty. 1991. Thematic Proto-Roles and
Argument Selection. Language, 67(3):547?
619.
Jerry A Fodor. 1992. A theory of the child?s theory
of mind. Cognition, 44(3):283?296.
Lila R. Gleitman, Kimberly Cassidy, Rebecca
Nappa, Anna Papafragou, and John C.
Trueswell. 2005. Hard words. Language
Learning and Development, 1(1):23?64.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classifica-
tion of English verbs. Language Resources and
Evaluation, 42(1):21?40.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
Elena Lieven, Dorothe? Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
Anna Papafragou, Kimberly Cassidy, and Lila
Gleitman. 2007. When we think about think-
ing: The acquisition of belief verbs. Cognition,
105(1):125?165.
Christopher Parisien and Suzanne Stevenson.
2011. Generalizing between form and meaning
using learned verb classes. In Proceedings of
the 33rd Annual Meeting of the Cognitive Sci-
ence Society.
Bele?n Pascual, Gerardo Aguado, Mar??a Sotillo,
and Jose C Masdeu. 2008. Acquisition of men-
tal state language in Spanish children: a longitu-
dinal study of the relationship between the pro-
duction of mental verbs and linguistic develop-
ment. Developmental Science, 11(4):454?466.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607?642.
Josef Perner. 1988. Developing semantics for the-
ories of mind: From propositional attitudes to
mental representation. Developing theories of
mind, pages 141?172.
Josef Perner, Manuel Sprung, Petra Zauner, and
Hubert Haider. 2003. Want That is understood
well before Say That, Think That, and False Be-
lief: A test of de Villiers?s linguistic determin-
ism on German?speaking children. Child devel-
opment, 74(1):179?188.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent?child discourse. Children?s lan-
guage, 4.
Marilyn Shatz, Henry M. Wellman, and Sharon
Silber. 1983. The acquisition of mental verbs:
A systematic investigation of the first reference
to mental state. Cognition, 14(3):301?321.
239
Patrick Suppes. 1974. The semantics of children?s
language. American psychologist, 29(2):103.
240
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37?45,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Learning Verb Classes in an Incremental Model
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
The ability of children to generalize over
the linguistic input they receive is key to
acquiring productive knowledge of verbs.
Such generalizations help children extend
their learned knowledge of constructions
to a novel verb, and use it appropriately in
syntactic patterns previously unobserved
for that verb?a key factor in language
productivity. Computational models can
help shed light on the gradual development
of more abstract knowledge during verb
acquisition. We present an incremental
Bayesian model that simultaneously and
incrementally learns argument structure
constructions and verb classes given nat-
uralistic language input. We show how the
distributional properties in the input lan-
guage influence the formation of general-
izations over the constructions and classes.
1 Introduction
Usage-based accounts of language learning note
that young children rely on verb-specific knowl-
edge to produce their early utterances (e.g.,
Tomasello, 2003). However, evidence suggests
that even young children can generalize their
verb knowledge to novel verbs and syntactic
frames (e.g., Fisher, 2002), and that the abstract
knowledge gradually strengthens over time (e.g.,
Tomasello and Abbot-Smith, 2002). One area of
verb usage where more sophisticated abstraction
appears necessary for fully adult productivity in
language is the knowledge of verb alternations.
A verb alternation is a pairing of constructions
shared by a number of verbs, in which the two
constructions express related argument structures
(Levin, 1993): e.g., the dative alternation involves
the related forms of the prepositional dative (PD;
X gave Y to Z) and the double-object dative (DO; X
gave Z Y). Such alternations enable language users
to readily adapt new and low frequency verbs to
appropriate constructions of the language by gen-
eralizing the observed use of one such form to the
other.
1
For example, Conwell and Demuth (2007) show
that 3-year-old children understand that a novel
verb observed only in the DO dative (John gor-
ped Heather the book) can also be used in the PD
form (John gorped the book to Heather), though
the children can only generalize such knowledge
under certain experimental conditions. Wonnacott
et al. (2008) demonstrate the proficiency of adults
in making such generalizations within an artificial
language learning scenario, which enables the re-
searchers to explore the distributional properties
of the linguistic input that facilitate the acquisition
of such generalizations. The results suggest that
the overall frequency of the syntactic patterns as
well as the distribution of verbs across the patterns
play a facilitatory role in the formation of abstract
verb knowledge (in the form of verb alternations)
in adult language learners.
In this work, we propose a computational
model that extends an existing Bayesian model of
verb argument structure acquisition (Alishahi and
Stevenson, 2008)[AS08] to support the learning of
verb classes over the acquired constructions. Our
model is novel in its approach to verb class forma-
tion, because it clusters tokens of a verb that reflect
the distribution of the verb over the learned con-
structions each time the verb is used in an input.
That is, the model forms verb classes by cluster-
ing verb tokens that reflect the evolving usages of
the verbs in various constructions.
We use this new model to analyze the role of
the classes and the distributional properties of the
input in learning abstract verb knowledge, given
1
The generalization of an alternation refers to a speaker
using one variant of an alternation for a verb (e.g., PD) having
only observed the verb in the other variant (e.g., DO).
37
naturalistic input that contains many verbs and
many constructions. The model can form higher-
level generalizations such as learning verb alterna-
tions, which is not possible with the AS08 model
(cf. the findings of Parisien and Stevenson, 2010).
Moreover, because our model gradually forms its
representations of constructions and classes over
time (in contrast to other Bayesian models, such
as Parisien and Stevenson, 2010; Perfors et al.,
2010), it is possible to analyze the monotonically-
growing representations and show their compati-
bility with the developmental patterns seen in chil-
dren (Conwell and Demuth, 2007). We also repli-
cate some of the observations of Wonnacott et al.
(2008) on the role of distributional properties of
the language in influencing the degree of general-
ization over an alternation.
2 Related Work
To explore the properties of learning mechanisms
that are capable of mimicking child and adult psy-
cholinguistic observations, a number of cognitive
modeling studies have focused on learning ab-
stract verb knowledge from individual verb usages
(e.g., Alishahi and Stevenson, 2008; Perfors et al.,
2010; Parisien and Stevenson, 2010). Here we fo-
cus on such computational models that enable the
sort of higher-level generalization that people do
across verb alternations, unlike the AS08 model.
The hierarchical Bayesian models of Perfors
et al. (2010) and Parisien and Stevenson (2010)
focus on learning this kind of higher-level general-
ization. The model of Perfors et al. (2010) learns
verb alternations, i.e., pairs of syntactic patterns
shared by certain groups of verbs. By incorpo-
rating this sort of abstract knowledge into their
model, Perfors et al. are able to simulate the abil-
ity of adults to generalize across verb alternations
(as in Wonnacott et al., 2008). That is, Perfors
et al. predict the ability of a novel verb to occur
in a syntactic structure after exposure to it in the
alternative pattern of that alternation. However,
this model is trained on data that contains only a
limited number of verbs and syntactic patterns un-
like naturalistic Child-directed Speech (CDS) and
moreover incorporates built-in information about
verb constructions.
The hierarchical Dirichlet model of Parisien
and Stevenson (2010) addresses these limitations
by working with natural child-directed speech
(CDS) data. Moreover, the model of Parisien and
Stevenson simultaneously learns constructions as
in AS08 and verb classes based on verb alterna-
tion behaviour, showing that the latter level of ab-
straction is necessary to support effective learn-
ing of verb alternations. Still, the models of both
Parisien and Stevenson and Perfors et al. can only
be utilized as a batch process and hence are lim-
ited in the analysis of developmental trajectories.
Although it is possible to simulate development by
training such models on increasing portions of in-
put, such an approach does not ensure that the rep-
resentations given n + i inputs can be developed
from the representation given n inputs.
In this paper, we propose a significant extension
to the model of AS08, by adding an extra level of
abstraction that incrementally learns verb classes
by drawing on the distribution of verbs over the
learned constructions. The new model combines
the advantages of having a monotonic clustering
model that enables the analysis of developing clus-
ters, with the simultaneous learning of construc-
tions and verb classes.
3 The Computational Model
As mentioned above, our model is an extension
of the model of AS08 in which we add a level of
learned abstract knowledge about verbs. Specif-
ically, our model uses a Bayesian clustering pro-
cess to learn clusters of verb usages that occur in
similar argument structure constructions, as in the
original model of AS08. To this, we add another
level of abstraction that learns clusters of verbs
that exhibit similar distributional patterns of oc-
currence across the learned constructions?that is,
classes of verbs that occur in similar sets of con-
structions, and in similar proportions. To distin-
guish between the clusters of the two levels of ab-
straction in our new model, we refer to the clusters
of verb usages as constructions, and to the group-
ings of verbs given their distribution over those
constructions as verb classes.
3.1 Overview of the Model
The model learns from a sequence of frames,
where each frame is a collection of features rep-
resenting what the learner might extract from an
utterance s/he has heard. Similarly to previous
computational studies (e.g., Parisien and Steven-
son, 2010), here we focus on syntactic features
since our goal is to understand the acquisition of
acceptable syntactic structures of verbs indepen-
38
Figure 1: A visual representation of the two levels of ab-
straction in the model, with sample verb usages input (and
extracted input frames), constructions, and classes.
dently of their meaning, as in some relevant psy-
cholinguistic (Wonnacott et al., 2008) and com-
putational studies (Parisien and Stevenson, 2010).
We focus particularly on properties such as syn-
tactic slots and argument count. (These features,
as in Parisien and Stevenson (2010), provide a
more flexible and generalizable representation of a
syntactic structure than the syntactic pattern string
used by AS08.) See the bottom rows of boxes in
Figure 1 for sample input verb usages with their
extracted frames.
The model incrementally clusters the extracted
input frames into constructions that reflect prob-
abilistic associations of the features across simi-
lar verb usages; see the middle level of Figure 1.
Each learned cluster is a probabilistic (and possi-
bly noisy) representation of an argument structure
construction: e.g., a cluster containing frames cor-
responding to usages such as I eat apples, She took
the ball, and He got a book, etc., represents a Tran-
sitive Action construction.
2
Such constructions al-
low for some degree of generalization over the ob-
served input; e.g., when seeing a novel verb in a
Transitive utterance, the model predicts the simi-
larity of this verb to other Action verbs appearing
in that pattern (Alishahi and Stevenson, 2008).
Grouping of verb usages into constructions may
not be sufficient for making higher-level general-
izations across verb alternations. Knowledge of al-
ternations is only captured indirectly in construc-
tions (because usages of the same verb can oc-
cur in multiple clusters). Following Parisien and
Stevenson (2010), we hypothesize that true gen-
eralization behaviour requires explicit knowledge
that verbs have commonalities in their patterns of
occurrence across constructions; this is the basis
2
Because the associations are probabilistic, a linguistic
construction may be represented by more than one cluster.
for verb classes (Levin, 1993; Merlo and Steven-
son, 2000; Schulte im Walde and Brew, 2002).
To capture this, our model learns groupings of
verbs that have similar distributions across the
learned constructions. These groupings form verb
classes that provide a higher-level of abstraction
over the input; see the top level in Figure 1. Con-
sider the dative alternation: the classes capture the
fact that some verbs may occur only in preposi-
tional dative (PD) forms, such as sing, while oth-
ers occur only in double object (DO) forms (call),
while still others alternate ? i.e., they occur in both
(bring).
Our model simultaneously learns both of these
types of knowledge: constructions are clusters of
verb usages, and classes are clusters of verb dis-
tributions over those constructions. Importantly, it
does so incrementally, which allows us to exam-
ine the developmental trajectory of acquiring al-
ternations such as the dative as the learned clus-
ters grow over time. Moreover, both types of clus-
tering are monotonic, i.e., we do not re-structure
the groupings that our model learns. However, the
model in both levels is clustering verb tokens ? i.e.,
the features corresponding to the verb at that time
in the input, its usage or its current distribution ?
so that the same verb type may be added to various
clusters at different stages in the training.
3.2 Learning Constructions of Verb Usages
The model of AS08 groups input frames into clus-
ters on the basis of the overall similarity in the
values of their features. Importantly, the model
learns these clusters incrementally in response to
the input; the number and type of clusters is not
predetermined. The model considers the creation
of a new cluster for a given frame if the frame is
not sufficiently similar to any of the existing clus-
ters. Formally, the model finds the best cluster for
a given input frame F as in:
BestCluster(F ) = argmax
k?Clusters
P (k|F ) (1)
where k ranges over all existing clusters and a new
one. Using Bayes rule:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of a cluster P (k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
39
prior more frequent constructions. The likelihood
P (F |k) is estimated based on the match of fea-
ture values in F and in the frames of k (assuming
independence of the features):
P (F |k) =
?
i?Features
P
i
(j|k) (3)
where j is the value of the i
th
feature of F , and
P
i
(j|k) is calculated using a smoothed version of:
P
i
(j|k) =
count
i
(j, k)
n
k
(4)
where count
i
(j, k) is the number of times feature i
has the value j in cluster k, and n
k
is the number of
frames in k. We compare the slot features as sets to
capture similarities in overlapping syntactic slots
rather than enforcing an exact match. The model
uses the Jaccard similarity score to measure the
degree of overlap between two feature sets, instead
of the direct count of occurrence in Eqn. (4):
sim score(S
1
, S
2
) =
|S
1
? S
2
|
|S
1
? S
2
|
(5)
where S
1
and S
2
in our experiments here are the
sets of syntactic slot features.
3.3 Learning Verb Classes
Our new model extends the construction-
formation model of AS08 by grouping verbs into
classes on the basis of their distribution across
the learned constructions. That is, verbs that have
statistically-similar patterns of occurrence across
the learned constructions will be considered as
forming a verb class. For example, in Figure 1 we
see that bring and read may be put into the same
class because they both occur in a similar relative
frequency across the DO and PD constructions
(the leftmost and rightmost constructions in the
figure).
We use the same incremental Bayesian cluster-
ing algorithm for learning the verb classes as for
learning constructions. At the class level, the fea-
ture used for determining similarity of items in
clustering is the distribution of each verb across
the learned constructions. As for constructions,
the model learns the verb classes incrementally;
the number and type is not predetermined. More-
over, just as constructions are gradually formed
from successively processing a particular verb us-
age at each input step, the model forms verb
classes from a sequence of snapshots of the input
verb?s distribution over the constructions at each
input step. This means that our model is forming
classes of verb tokens rather than types; if a verb?s
behaviour changes over the duration of the input,
subsequent tokens (the distributions over construc-
tions at later points in time) may be clustered into
a different class (or classes) than earlier tokens,
even though prior decisions cannot be undone.
Formally, after clustering the input frame at
time t into a construction, as explained above, the
model extracts the current distribution d
v
t
of its
head verb v over the learned constructions; this is
estimated as a smoothed version of v?s relative fre-
quency in each construction:
P (k|v) =
count(v, k)
n
v
(6)
where count(v, k) is the number of times that in-
puts with verb v have been clustered into construc-
tion k, and n
v
is the number of times v has oc-
curred in the input thus far.
To cluster this snapshot of the verb?s distribu-
tion, d
v
t
, it is compared to the distributions en-
coded by the model?s classes. The distribution d
c
of an existing class c is the weighted average of
the distributions of its member verb tokens:
d
c
=
1
|c|
?
v?c
count(v, c)? d
v
(7)
where |c| is the size of class c, count(v, c) is the
number of occurrences of v that have been as-
signed to c, and d
v
is the distribution of the verb v
given by the tokens of v (the ?snapshots? of distri-
butions of v assigned to class c). That is, d
v
in c is
an average of the distributions of all d
v
t
for verb v
that have been clustered into c.
The model finds the best class for a given verb
distribution d
v
t
based on its similarity to the dis-
tributions of all existing classes and a new one:
BestClass(d
v
t
) = argmax
c?Classes
(1?D
JS
(d
c
?d
v
t
))
(8)
where c ranges over all existing classes as well as
a new class that is represented as a uniform dis-
tribution over the existing constructions. Jensen?
Shannon divergence, D
JS
, is a popular method for
measuring the distance between two distributions:
It is based on the KL?divergence, but it is symmet-
ric and has a finite value between 0 and 1:
D
JS
(p?q) =
1
2
D
KL
(p?
1
2
(p+ q)) +
1
2
D
KL
(q?
1
2
(p+ q)) (9)
40
non-ALT ALT
DO-only PD-only DO PD
Number of verbs 12 5 6
Relative frequency 14% 2% 2% 1%
Table 1: Number of non-alternating (non-ALT) and alter-
nating (ALT) verbs in our lexicon, as well as the relative fre-
quency of each construction in our generated input corpora.
4 Experimental Setup
4.1 Generation of the Input Corpora
We follow the input generation method of AS08
to create naturalistic corpora that are based on the
distributional properties of verbs over various con-
structions, as observed in child-directed speech
(CDS). Our input-generation lexicon contains 71
verbs drawn from AS08 (11 action verbs) and
Barak et al. (2013) (31 verbs of varying syntac-
tic patterns), plus an additional 40 of the most fre-
quent verbs in CDS, in order to have a range of
verbs that occur with the PD and DO construc-
tions. Table 4.1 shows the number of verbs that
appear in the DO or PD construction only (non-
alternating), as well as those that alternate across
the two. (The table also gives the relative fre-
quency of each dative construction in our gener-
ated input corpora.) Each verb lexical entry in-
cludes its overall frequency, and its relative fre-
quency with each of a number of observed syn-
tactic constructions. The frequencies are extracted
from a manual annotation of a sample of 100
child-directed utterances per verb from a collec-
tion of eight corpora from CHILDES (MacWhin-
ney, 2000).
3
An input corpus is generated by it-
eratively selecting a random verb and a syntactic
construction based on their frequencies according
to the lexicon, so that all input corpora used in our
simulations have the distributional properties ob-
served in CDS, but show some variation in precise
make-up and ordering of verb usages. The gener-
ated input consists of frames (a set of features) that
correspond to verb usages in CDS.
4.2 Simulations
Because the generation of the input data is prob-
abilistic, we conduct 100 simulations for each
experiment (each using a different input cor-
pus) to avoid any dependency on specific id-
iosyncratic properties of a single generated cor-
pus. For each simulation, we train our model
3
Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al. (1974); Sachs (1983); Lieven et al. (2009).
on an automatically-generated corpus of 15, 000
frames, from which the model learns construc-
tions and verb classes. At specified points in
the input, we present the model with usages of
a novel verb in a DO and/or PD frame, and
then test the model?s generalization ability by
predicting DO and PD frames given that verb.
Since we are interested in the relative likeli-
hoods of the two frames, we report the differ-
ence between the log-likelihood of the DO frame
and the log-likelihood of the PD frame, i.e.,
log-likelihood(DO)? log-likelihood(PD).
Specifically, we form a partial frame F
test
(con-
taining all usage features except for the verb) that
reflects either the PD or the DO syntax, and assess
the probability P (F
test
|v) for each of these, as in:
P (F
test
|v) =
?
k?Constructions
P (F
test
|k)P (k|v)
(10)
where P (F
test
|k) is calculated as in Eqn. (3).
We can calculate P (k|v) in two different ways:
using only the knowledge in the constructions of
the model, and using the knowledge that takes into
account the verb classes over the constructions.
For model predictions based on the construction
level only, we calculate P (k|v) as in Eqn. (6),
which is the smoothed relative frequency of the
verb v over construction k.
Predictions using knowledge of the verb classes
will instead determine P (k|v) drawing on the fit
of verb v to the various classes (specifically, the
similarity of v?s distribution over constructions to
the distribution encoded in each class), and the
likelihood of each construction k for each class c
(specifically, the likelihood of k given the distribu-
tion over constructions encoded in c), as in:
P (k|v) ?
?
c?Classes
P (k|c)P (c|v) (11)
where P (k|c) is the probability of construction
k given class c?s distribution over constructions
(d
c
); and P (c|v) is the probability of c given verb
v?s distribution d
v
over the constructions (using
Jensen-Shannon divergence as in Eqn. (9)).
Due to the different number of clusters in each
of the construction and class layers of the model,
the likelihoods computed for each will differ in
the range of values. For this reason, specific val-
ues cannot be directly compared across the layers
of the model, rather we must analyze the general
trends of the construction-only and class-based re-
sults.
41
5 Evaluation
In this section we examine whether and how our
model generalizes across the two variants of the
dative alternation, the double-object dative (DO)
and the prepositional dative (PD). To do so, we
measure the tendency of the model to produce a
novel verb observed in one dative frame in that
same frame, or in the other dative frame (unob-
served for that verb). Our goal is to understand the
impact of the learned constructions and classes on
this generalization behaviour. Following Parisien
and Stevenson (2010), we examine three input
conditions in which the novel verb occurs: (i)
twice with the DO syntax (non-alternating); (ii)
twice with the PD syntax (non-alternating); or (iii)
once each with DO and PD syntax (alternating).
4
We then ask the model to predict the likelihood of
producing each dative frame with that verb. Our
focus here is on comparing the generalization abil-
ities of the two levels of abstract knowledge in our
model: the constructions versus the verb classes.
As a reminder, we use the dative alternation as
one example for considering this kind of higher-
level generalization behaviour observed in adults
and to a lesser extent in children. Moreover, we
perform the analysis in the context of naturalistic
input that contains many verbs (those that appear
in the dative and those that do not), and a variety of
constructions , to provide a realistic setting for the
task. Our settings differ from the psycholinguis-
tic studies in the variability of constructions com-
pared with the artificial language used by Won-
nacott et al., and in focusing only on the syntac-
tic properties unlike Conwell and Demuth. How-
ever, we follow the settings of these studies in an-
alyzing the syntactic properties of a generated ut-
terance given minimal exposure to a novel verb.
Therefore, we aim to replicate their general ob-
servations by showing that (i) children are limited
in their ability to generalize across verb alterna-
tions compared with adults, and (ii) the frequency
of a construction has a positive correlation with the
generalization rate of the construction.
5.1 Generalization of Learned Knowledge
We examine the generalization patterns of our
model when presented with a novel verb in DO/PD
forms after being trained on 15, 000 inputs, which
we compare to the performance of adults in such
4
For the alternating condition, half the simulations have
DO first, and half have PD first.
Figure 2: The difference between the log-likelihood values
of the DO and PD frames, given each of the three input con-
ditions: DO only, PD only, and Alternating. Values above
zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
language tasks. We first consider the case where
the model predictions are based solely on the
knowledge of constructions. Here we expect the
predictions to correspond to the syntactic proper-
ties of the two inputs observed for the novel verb,
with limited generalization. That is, we expect a
non-alternating verb to be much more likely in the
observed dative frame, and an alternating verb to
be equally likely in both frames. The left hand
side of Figure 2 presents the differences in log-
likelihoods of the predicted DO and PD frames for
the novel verb using the construction-based prob-
abilities. The results confirm our expectation that
the knowledge of constructions can support only
limited generalization across the variants of an al-
ternation. For the non-alternating conditions, the
observed frame is highly favoured, and for the
Alternating test scenario, the DO and PD frames
have nearly equal likelihoods.
We next turn to using the knowledge of verb
classes, which we expect to enable generaliza-
tions that correspond to verb alternation behaviour
? that is, we expect the model predictions here
to reflect the knowledge that verbs that occur in
one form of the alternation also often occur in
the other form of the alternation. This is possible
because the classes in the model encode the dis-
tributional patterns of verbs across constructions.
In the absence of other factors, we would expect
the Alternating condition to again show near equal
likelihoods for the two frames, and the two non-
alternating conditions to show a slight preference
for the observed frame (rather than the strong pref-
erence seen in the construction-based predictions),
because the unobserved frame is also likely due to
the knowledge here of the alternation.
The right hand side of Figure 2 presents the
42
difference in the log-likelihoods of the DO and
PD frames when using the knowledge encoded
in the verb classes. The results are not directly
in line with the simple prediction above: The
non-alternating (DO-only and PD-only) condi-
tions show a weak preference (as expected) for one
frame over another, but both favour the DO frame,
as does the Alternating condition. That is, the PD-
only and Alternating conditions show a preference
for the DO frame that does not follow simply from
the knowledge of alternations.
The DO preference in the PD-only and Alter-
nating conditions arises due to distributional fac-
tors in the input, related to the frequencies of the
constructions reported in Table 1. First, the DO
frame is overall much more likely than the PD
frame, causing generalization in the PD-only and
Alternating conditions to lean more to that frame.
Second, fully 1/3 of the uses of the PD frame in
the corpus are with verbs that alternate (i.e., 1%
of the corpus are PD frames of alternating PD-
DO verbs, out of a total of 3% of the corpus be-
ing PD frames), while only 1/8 of the uses of the
DO frame are with alternating rather than non-
alternating verbs. Recall that our classes encode
the distribution (roughly relative frequency) of the
verbs in the class occurring across the different
constructions. This means that in our class-based
predictions, greater weight will be given to con-
structions with DO when observing a PD frame
than to constructions with PD when observing a
DO frame. These results underline the importance
of using naturalistic input and considering the im-
pact of various distributional factors on general-
ization of verb knowledge.
In contrast to the construction-based results, our
class-based results conform with the experimental
findings of Wonnacott et al. (2008), who show that
adult (artificial) language learners robustly gener-
alize a newly-learned verb observed in a single
syntactic form by producing it in the alternating
syntactic form under certain language conditions.
Moreover, we show similar distributional effects
to theirs ? the overall frequency of the syntactic
patterns, as well as the distribution of verbs across
those patterns ? in the level of preference for one
form over another, within the context of our nat-
uralistic data with multiple verbs, constructions,
and alternations. These results show that the verb
classes in the model are able to capture useful ab-
stract knowledge that is key to understanding the
human ability to make high-level generalizations
across verb alternations.
5.2 Development of Generalizations
Next, we present the results of our model evalu-
ated throughout the course of training in order to
understand the developmental pattern of general-
ization. We perform the same construction-based
or class-based prediction tasks (the likelihoods of
a DO and PD frame), following the same input
conditions (a novel verb with two DO frames, two
PD frames, or one of each) at given points during
the 15, 000 inputs. As above, we present the dif-
ference in the log-likelihood values of the DO and
the PD frames in order to focus on the relative like-
lihoods of the two frames within each condition of
construction-based or class-based predictions.
Figure 3(a) presents the results for the DO-
only test scenario. As in Section 5.1, for
both construction-based and class-based predic-
tions there is a higher likelihood for the DO frame
throughout the course of training. In contrast, the
incremental results for the PD-only test scenario,
in Figure 3(b), display a developing level of gen-
eralization throughout the training stage for the
class-based predictions. While the construction-
based predictions reflect a much higher likelihood
for the PD frame, the results from the verb classes
are in favor of the PD frame only initially; after
training on 5000 input frames, the likelihood of
the DO frame becomes higher for this test sce-
nario. These results indicate that using construc-
tion knowledge alone does not enable generaliza-
tion from the PD frame to the DO frame; in con-
trast, the verb class knowledge enables the grad-
ual acquisition of generalization ability over the
course of training.
Finally, Figure 3(c) presents the results for the
Alternating test scenario for the two types of pre-
dictions. As in Section 5.1, both construction-
based and class-based predictions have a small
preference for the DO frame. In the construction-
based predictions, this preference lessens over
time to where the likelihoods for DO and PD are
almost equal, while the class-based predictions
stay relatively constant in their preference for the
DO frame. In some ways the construction-based
predictions are more expected in response to an
apparently alternating verb; however, the class-
based predictions show a higher degree of general-
ization, responding to the higher frequency of the
43
(a) DO only (b) PD only (c) Alternating
Figure 3: Difference of log-likelihood values of the DO and PD frames over the course of training for the constructions and
the verb classes for each of the 3 test scenarios. Values above zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
DO frame and the higher association of PD frames
with DO alternates. These results again empha-
size the importance of further exploring the role
of distributional factors on generalization of verb
knowledge in children.
The developmental results presented here are in
line with the suggestions of Tomasello (2003) that
the productions of younger children follow ob-
served patterns in the input, and only later reflect
robust generalizations of their knowledge across
verbs. Conwell and Demuth (2007) for example,
found evidence of generalization across verb al-
ternations in 3-year-old children, but their produc-
tion of unobserved forms for a novel verb was
very sensitive to the precise context of the ex-
periment and the distributional patterns across the
novel verbs. In accord with these observations, the
developmental trajectories in our model show that
our class-based predictions increase in their degree
of generalization over time, and are sensitive to
various distributional factors in the input, such as
the overall expectation for a frame and the expec-
tation that a verb will alternate.
6 Discussion
We present a novel computational model that
probabilistically learns two levels of abstractions
over individual verb usages: constructions that
are clusters of similar verb usages, and classes of
verbs with similar distributional behaviour across
the constructions. Specifically, we extend the
model of AS08 by incrementally learning token-
based verb classes that generalize over the con-
struction knowledge level. In contrast to the mod-
els of Parisien and Stevenson and Perfors et al.,
our model is incremental, and hence enables the
analysis of the monotonically developing classes
to show the relation to the development of gener-
alization ability in human learners.
We analyze how generalization is supported by
each level of learning in our model: constructions
and verb classes. Our results confirm (cf. Parisien
and Stevenson, 2010) that a higher-level knowl-
edge of the verb classes is required to replicate the
observed patterns of generalization, such as pro-
ducing a novel verb gorp in the in the prepositional
dative pattern after hearing it in the double object
dative pattern. In addition, our analysis of the in-
crementally developing verb classes shows that the
generalization knowledge gradually emerges over
time, similar to what is observed in children.
The flexibility of input representation of our
model enables us to further explore the properties
of the input in learning abstract knowledge, fol-
lowing psycholinguistic studies. Our results repli-
cate the findings of Wonnacott et al. on the role
of the distributional properties over the alternat-
ing syntactic forms, but in naturalistic settings of
many constructions. In future, we plan to extend
this analysis by manipulating the distributions of
our input data to replicate the exact settings of the
artificial language used by Wonnacott et al.. More-
over, in this study, we followed the settings of pre-
vious computational and psycholinguistic studies
that focused on the syntactic properties of the in-
put (Perfors et al., 2010; Parisien and Stevenson,
2010; Wonnacott et al., 2008; Conwell and De-
muth, 2007). However, we can further our anal-
ysis by incorporating semantic features in the in-
put to study syntactic bootstrapping effects (Scott
and Fisher, 2009) as well as the role of seman-
tic properties in constraining the generalizations
across the alternating forms.
Acknowledgments
The authors would like to thank Afra Alishahi for
providing us with the code and data for her model,
and to Chris Parisien for sharing his data with us.
44
References
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789?
834.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2013. Acquisition of desires before beliefs:
A computational investigation. In Proceedings
of CoNLL-2013.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380?420.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Erin Conwell and Katherine Demuth. 2007. Early
syntactic productivity: Evidence from dative
shift. Cognition, 103(2):163?179.
Cynthia Fisher. 2002. The role of abstract syntac-
tic knowledge in language acquisition: A reply
to Tomasello. Cognition, 82(3):259?278.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
B. Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation, volume 348.
University of Chicago press Chicago, IL.
Elena Lieven, Doroth?e Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
P. Merlo and S. Stevenson. 2000. Automatic verb
classification based on statistical distributions
of argument structure. Computational Linguis-
tics, 27(3):373?408.
Christopher Parisien and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the
32nd annual meeting of the Cognitive Science
Society.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607?642.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent?child discourse. Children?s lan-
guage, 4.
Sabine Schulte im Walde and Chris Brew. 2002.
Inducing German semantic verb classes from
purely syntactic subcategorisation information.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 223?230, Philadelphia, PA.
Rose M Scott and Cynthia Fisher. 2009. Two-
year-olds use distributional cues to interpret
transitivity-alternating verbs. Language and
cognitive processes, 24(6):777?803.
Patrick Suppes. 1974. The semantics of children?s
language. American psychologist, 29(2):103.
Michael Tomasello. 2003. Constructing a lan-
guage: A usage-based theory of language ac-
quisition.
Michael Tomasello and Kirsten Abbot-Smith.
2002. A tale of two theories: Response to
Fisher. Cognition, 83(2):207?214.
Elizabeth Wonnacott, Elissa L Newport, and
Michael K Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cog-
nitive psychology, 56(3):165?209.
45
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46?54,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
A Usage-Based Model of Early Grammatical Development
Barend Beekhuizen
LUCL
Leiden University
b.f.beekhuizen@hum.leidenuniv.nl
Rens Bod
ILLC
University of Amsterdam
l.w.m.bod@uva.nl
Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
afsaneh,suzanne@cs.toronto.edu
Arie Verhagen
LUCL
Leiden University
a.verhagen@hum.leidenuniv.nl
Abstract
The representations and processes yield-
ing the limited length and telegraphic style
of language production early on in acqui-
sition have received little attention in ac-
quisitional modeling. In this paper, we
present a model, starting with minimal lin-
guistic representations, that incrementally
builds up an inventory of increasingly long
and abstract grammatical representations
(form+meaning pairings), in line with the
usage-based conception of language ac-
quisition. We explore its performance on
a comprehension and a generation task,
showing that, over time, the model bet-
ter understands the processed utterances,
generates longer utterances, and better ex-
presses the situation these utterances in-
tend to refer to.
1 Introduction
A striking aspect of language acquisition is the dif-
ference between children?s and adult?s utterances.
Simulating early grammatical production requires
a specification of the nature of the linguistic repre-
sentations underlying the short, telegraphic utter-
ances of children. In the usage-based view, young
children?s grammatical representions are thought
to be less abstract than adults?, e.g. by having
stricter constraints on what can be combined with
them (cf. Akhtar and Tomasello 1997; Bannard
et al. 2009; Ambridge et al. 2012). The represen-
tations and processes yielding the restricted length
of these early utterances, however, have received
little attention. Following Braine (1976), we adopt
the working hypothesis that the early learner?s
grammatical representations are more limited in
length (or: arity) than those of adults.
Similarly, in computational modeling of gram-
mar acquisition, comprehension has received more
attention than language generation. In this pa-
per we attempt to make the mechanisms underly-
ing early production explicit within a model that
can parse and generate utterances, and that in-
crementally learns constructions (Goldberg, 1995)
on the basis of its previous parses. The model?s
search through the hypothesis space of possible
grammatical patterns is highly restricted. Start-
ing from initially small and concrete representa-
tions, it learns incrementally long representations
(syntagmatic growth) as well as more abstract
ones (paradigmatic growth). Several models ad-
dress either paradigmatic (Alishahi and Stevenson,
2008; Chang, 2008; Bannard et al., 2009) or syn-
tagmatic (Freudenthal et al., 2010) growth. This
model aims to explain both, thereby contribut-
ing to the understanding of how different learning
mechanisms interact. As opposed to other models
involving grammars with semantic representations
(Alishahi and Stevenson, 2008; Chang, 2008), but
similar to Kwiatkowski et al. (2012), the model
starts without an inventory of mappings of single
words to meanings.
Based on motivation from usage-based and con-
struction grammar approaches, we define several
learning principles that allow the model to build
up an inventory of linguistic representations. The
model incrementally processes pairs of an utter-
ance U , consisting of a string of words w
1
. . . w
n
,
and a set of situations S, one of which is the situa-
tion the speaker intends to refer to. The other situ-
ations contribute to propositional uncertainty (the
uncertainty over which proposition the speaker is
trying to express; Siskind 1996). The model tries
to identify the intended situation and to understand
how parts of the utterance refer to certain parts of
that situation. To do so, the model uses its growing
inventory of linguistic representations (Section 2)
to analyze U , producing a set of structured seman-
tic analyses or parses (Fig. 1, arrow 1; Section 3).
46
The resulting best parse, U and the selected situa-
tion are then stored in a memory buffer (arrow 2),
which is used to learn new constructions (arrow
3) using several learning mechanisms (Section 4).
The learned constructions can then be used to gen-
erate utterances as well. We describe two experi-
ments: in the comprehension experiment (Section
5), we evaluate the model?s ability to parse the
stream of input items. In the generation experi-
ment (Section 6), the model generates utterances
on the basis of a given situation and its linguistic
knowledge. We evaluate the generated utterances
given different amounts of training items to con-
sider the development of the model over time.
2 Representations
We represent linguistic knowledge as construc-
tions: pairings of a signifying form and a signi-
fied (possibly incomplete) semantic representation
(Goldberg, 1995). The meaning is represented as
a graph with the nodes denoting entities, events,
and their relations, connected by directed unla-
beled edges. The conceptual content of each node
is given by a set of semantic features. We assume
that meaning representations are rooted trees. The
signifying form consists of a positive number of
constituents. Every constituent has two elements:
a phonological form, and a pointer to a node in the
signified meaning (in line with Verhagen 2009).
Both can be specified, or one can be left empty.
Constituents with unspecified phonological forms
are called open, denoted with  in the figures. The
head constituent of a construction is defined as
the constituent that has a pointer to the root node
of the signified meaning. We furthermore require
that no two constituents point to the same node of
the signified meaning.
This definition generalizes over lexical ele-
ments (one phonologically specified constituent)
as well as larger linguistic patterns. Fig. 2, for in-
stance, shows two larger constructions being com-
bined with each other. We call the set of construc-
tions the learner has at some moment in time the
constructicon C (cf. Goldberg 2003).
3 Parsing
3.1 Parsing operations
We first define a derivation d as an assembly
of constructions in C, using four parsing opera-
tions defined below. In parsing, derivations are
constrained by the utterance U and the situations
utterancesituation 1
situation n
situation 2...
situationsinput item
construction 1construction 2construction 3construction nconstructicon
analysis
(utterance, intended situation, analysis)
...
...
memory buffer
1 12
3(utterance, intended situation, analysis)(utterance, intended situation, analysis)
Figure 1: The global flow of the model
S, whereas in production, only a situation s con-
strains the derivation. The leaf nodes of a deriva-
tion must consist of phonological constraints of
constructions that (in parsing) are satisfied by U .
All constructions used in a derivation must map to
the same situation s ? S. A construction cmaps to
s iff the meaning of c constitutes a subgraph of s,
with the features on each of the nodes in the mean-
ing of c being a subset of the features on the corre-
sponding node of s. Moreover, each construction
must map to a different part of s. This constitutes
a mutual exclusivity effect in analyzing U : every
part of the analysis must contribute to the compos-
ite meaning. A derivation d thus gives us a map-
ping between the composed meaning of all con-
structions used in d and one situation s ? S. The
aggregate mapping specifies a subgraph of s that
constitutes the interpretation of that derivation.
The central parsing operation is the COMBINA-
TION operator ?. In c
i
? c
j
, the leftmost open con-
stituent of c
i
is combined with c
j
. Fig. 2 illus-
trates COMBINATION. COMBINATION succeeds if
both the semantic pointer of the leftmost open con-
stituent of c
i
and the semantic pointer of the head
constituent of c
j
map to the same semantic node
of a situation s
Initially, the model has few constructions to an-
alyze the utterance with. Therefore, we define
three other operations that allow the model to cre-
ate a derivation over the full utterance without
combining constructions. First, a known or un-
known word that cannot be fit into a derivation,
can be IGNOREd. Second, an unknown word can
be used to fill an open constituent slot of a con-
struction with the BOOTSTRAP operator. Boot-
strapping entails that the unknown word will be
associated with the semantics of the node. Finally,
the learner can CONCATENATE multiple deriva-
tions, by linearly sequencing them, thus creating a
more complex derivation without combining con-
47
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn uly1(c.an
,d)bf n???????f ,d)bf? ????f
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn
?? ? ?? ?
) ?
,d)bf n??????f ,d)bf? ????f
uteranu?copaio tera3n u2e.copei sec2nue??.o aiopo? uly1(c.an
u?copaio tera3nue??.o aiopo? ,d)bf n???????f ,d)bf? ????f
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn
,d)bf n??????f ,d)bf? ????f
uteranu?copaio tera3n u2e.copei sec2nue??.o aiopo? uly1(c.an
??????n????n????????????n??????? ?? ?n??????n?n??????????n????????
Figure 2: Combining constructions. The dashed lines represent semantic pointers, either from con-
stituents to the constructional meaning (black) or from the constructions to the situation (red and blue).
uttereanc
n s
io ta1
21n i o. p . m
21n i o. pn p n m n 3
. p
21n i o.t .ne1 ne1
Figure 3: The CONCATENATE, IGNORE and
BOOTSTRAP operators (internal details of the con-
structions left out).
structions. This allows the learner to interpret a
larger part of the situation than with COMBINA-
TION only. The resulting sequences may be ana-
lyzed in the learning process as constituting one
larger construction, consisting of the parts of the
concatenated derivations. Fig. 3 illustrates these
three operations.
3.2 Selecting the best analysis
Multiple derivations can be highly similar in the
way they map parts of U to parts of an s ? S. We
define a parse to be a set of derivations that have
the same internal structure and the same mappings
to a situation, but that use different constructions
in doing so (cf. multiple licensing; Kay 2002). We
take the most probable parse of U to be the best
analysis of U . The most probable parse points to a
situation, which the model then assumes to be the
identified situation or s
identified
. If no parse can be
made, s
identified
is selected at random from S.
The probability of a parse p is given by the sum
of the probabilities of the derivations d subsumed
under that parse, which in turn are defined as the
product of the probabilities of the constructions c
used in d.
P (p) =
?
d?p
P (d) (1)
P (d) =
?
c?d
P (c) (2)
The probability of a construction P (c) is given
by its relative frequency (count) in the construc-
ticon C, smoothed with Laplace smoothing. We
assume that the simple parsing operations of IG-
NORE, BOOTSTRAP, and CONCATENATION reflect
usages of an unseen construction with a count of
0.
P (c) =
c.count+ 1
?
c
?
?C
c
?
.count+ |C|+ 1
(3)
The most probable parse, U and s
identified
are
added to the memory buffer. The memory buffer
has a pre-set maximal length, discarding the oldest
exemplars upon reaching this length. In the future,
we plan to consider more realistic mechanisms for
the memory buffer, such as graceful degradation,
and attention effects.
48
4 Learning mechanisms
The model uses the best parse of the utterance to
develop its knowledge of the constructions in the
constructicon C. Two simple operations, UPDATE
and ASSOCIATION, are used to create initial con-
structions and reinforce existing ones respectively.
Two additional operations, PARADIGMATIZATION
and SYNTAGMATIZATION, are key to the model?s
ability to extend these initial representations by
inducing novel constructions that are richer and
more abstract than existing ones.
4.1 Direct learning from the best parse
The best parse is used to UPDATE C. For this
mechanism, the model uses the concrete mean-
ing of s
identified
rather than the (potentially more
abstract) meaning of the constructions in the best
parse.
1
Every construction in the parse is assigned
the subgraph of s
identified
it maps to as its new
meaning, and the count of the adjusted construc-
tion is incremented with 1, or added to C with a
count of 1, if it does not yet exist. This includes
applications of the BOOTSTRAP operation, creat-
ing a mapping of the previously unknown word to
a situational meaning.
ASSOCIATE constitutes a form of simple cross-
situational learning over the memory buffer. The
intuition is that co-occurring word sequences
and meaning components that remain unanalyzed
across multiple parses might themselves comprise
the form-meaning pairing of a construction. If the
unanalyzed parts of two situations contain an over-
lapping subgraph, and the unanalyzed parts of two
utterances an overlapping subsequence of words,
the two are mapped to each other and added to C
with a count of 0.
4.2 Qualitative extension of the best parse
Syntagmatization Some of the processes de-
scribed thus far yield analyses of the input in
which constructions are linearly associated but
lack appropriate relational structure among them.
The model requires a process, which we call SYN-
TAGMATIZATION, that enables it to induce further
hierarchical structure.
In order for the learner to acquire constructions
in which the different constituents point to differ-
ent parts of the construction?s meaning, the ASSO-
1
This follows Langacker?s (2009) claim that the processed
concrete usage events should leave traces in the learner?s
mind.
CIATE operation does not suffice. We assume that
the learner is able to learn such constructions by
using concatenated derivations. The process we
propose is SYNTAGMATIZATION. In this process,
the various concatenated derivations are taken as
constituents of a novel construction. This instanti-
ates the idea that joint processing of two (or more)
events gradually leads to a joint representation of
these, previously independent, events.
More precisely, the process starts by taking the
top nodes T of the derivations in the best parse,
where T consists of the single top node if no CON-
CATENATION has been applied, or the set of con-
catenated nodes of the parse tree if CONCATENA-
TION has been applied (e.g. for the derivation in
Fig. 3, |T | = 2). For each top node t ? T , we take
the root node of the construction?s meaning, and
define its semantic frame to consist of all children
(roles) and grandchildren (role-fillers) of the node
in the situation it maps to. The model then forms a
novel construction c
syn
by taking all the construc-
tions in the parse whose semantic root nodes point
to a node in this semantic frame, referring to those
as the set R of semantically related constructions.
As the novel meaning of c
syn
, the model takes the
subgraph of the situation mapped to by the joint
mapping of all constructional meanings of con-
structions in R.
R, as well as all phonologically specified con-
stituents of t itself, are then linearized as the con-
stituents of c
syn
. The novel construction thus con-
stitutes a construction with a higher arity, ?joining?
several previously independent constructions. Fig.
4 illustrates the syntagmatization mechanism.
Paradigmatization Due to our usage-driven ap-
proach, all learning mechanisms so far give us
maximally concrete constructions. In order for the
model to generalize beyond the observed input,
some degree of abstraction is needed. The model
does so with the PARADIGMATIZATION mecha-
nism. This mechanism recursively looks for min-
imal abstractions (cf. Tomasello 2003, 123) over
the constructions in C and adds those to C, thus
creating a full-inheritance network (cf. Langacker
1989, 63-76).
An abstraction over a set of constructions is
made if there is an overlapping subgraph between
the meanings of the constructions, where every
node of the subgraph is the non-empty feature
set intersection between two mapped nodes of the
constructional meanings. Furthermore, the con-
49
uterauncsiricots 111autoi2tr. p.tm.ma
uio3.l.o3.ory.(i,ra
d)b d)bf??? n? f??? ?
uter 2cn.auncsiricots 111a uio3.l.o3.ory.(i,rad)bf??? ??
ecoetr.otr.
uc?.er .orir? ?ssa
d)bf??? ??
uterauncsiricots 111autoi2tr. p.tm.ma
uio3.l.o3.ory.(i,ra
d)b d)bf??? n? f??? ??
uc?.er .orir? ?ssa
d)bf??? ??
????????????????? ??rt??r ? ??????????????????
Figure 4: The SYNTAGMATIZATION mechanism. The mechanism takes a derivation as its input and
reinterprets it as a novel construction of higher arity).
stituents must be mappable: both constructions
have the same number of constituents and the
paired constituents point to a mapped node of the
meaning. The meaning of the abstracted construc-
tion is then set to this overlapping subgraph, which
is the lowest possible semantic abstraction over
the constructions. The constituents of this new ab-
straction have a specified phonological form if the
more concrete constructions share the same word,
and an unspecified one otherwise. The count of an
abstracted construction is given by the cardinality
of the set of its direct descendants in the network.
This generalizes Bybee?s (1995) idea about type
frequency as a proxy for productivity to a network
structure. Fig. 5 illustrates the paradigmatization
mechanism.
5 Experimental set-up
The model is incrementally presented with U, S
pairings based on Alishahi & Stevenson?s (2010)
generation procedure. In this procedure, an utter-
ance and a semantic frame expressing its meaning
(a situation) are generated. The generation pro-
cedure follows distributions occurring in a corpus
of child-directed speech. As we are interested in
the performance of the model under propositional
uncertainty, we add a parametrized number of ran-
domly sampled situations, so that S consists of the
situation the speaker intends to refer to (s
correct
)
and a number of situations the speaker does not
intend to refer to.
2
Here, we set the number of ad-
2
We are currently researching the effects of sampling non-
correct situations that have a greater likelihood of overlap
ditional situations to be 1 or 5; the other parameter
of the model, the size of the memory buffer, is set
to 5 exemplars.
For the comprehension experiment, we eval-
uate the model?s performance parsing the input
items, averaging over every 50 U, S pairs. We
track the ability to identify the intended situation
from S. Identification succeeds if the best parse
maps to s
correct
, i.e. if s
identified
= s
correct
. Next,
situation coverage expresses what proportion of
s
identified
has been interpreted and thus how rich the
meanings of the used constructions are. It is de-
fined as the number of nodes of the interpretation
of the best parse, divided by the number of nodes
of s
identified
. Finally, utterance coverage tells us
what proportion of U has been parsed with con-
structions (excluding IGNORED; including BOOT-
STRAPPED words). The measure expresses the
proportion of the signal that the learner (correctly
or incorrectly) is able to interpret.
For exploring language production, the model
receives a situation, and (given the constructicon)
finds the most probable, maximally expressive,
fully lexicalized derivation expressing it. That is:
among all derivations terminating in phonologi-
cally specified constituents, it selects the deriva-
tions that cover the most semantic nodes of the
given situation. In the case of multiple such
derivations, it selects the most probable one, fol-
lowing the probability model in Section 3. We
only allow for the COMBINATION operator in the
derivations, as BOOTSTRAPPING and IGNORE re-
with the intended situation, to reflect more realistic input (cf.
Siskind 1996).
50
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.(ct3e2l 
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ???
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.(c.e?n 
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.( 
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ?
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ???
?? ???? ?? ???n???n????? ???? ? ??? ?? ??? ??? t3e2l??? .e?n ?
? ?????? ????? ????u1ite.2ipcnp.2.(ct3e2l ??? ?u1ite.2ipcnp.2.(c?.e?n 
Figure 5: The PARADIGMATIZATION mechanism. The construction on top is an abstraction obtained
over the two constructions at the bottom.
fer to words in a given U , and CONCATENATE is a
back-off method for analyzing more of U than the
constructicon allows for. The situations used in the
generation experiment do not occur in the training
items, so that we truly measure the model?s ability
to generate utterances for novel situations.
The phonologically specified leaf nodes of the
best derivation constitute the generated utterance
U
gen
. U
gen
is evaluated on the basis of its mean
length, in number of words, its situation cover-
age, as defined in the comprehension experiment,
and its utterance precision and utterance recall.
To calculate these, we take the maximally overlap-
ping subsequenceU
overlap
between the actual utter-
ance U
act
associated with the situation and U
gen
.
Utterance precision (how many words are gener-
ated correctly) and utterance recall (how many of
the correct words are generated) are defined as:
Utterance precision =
|U
overlap
|
|U
gen
|
(4)
Utterance recall =
|U
overlap
|
|U
act
|
(5)
Because the U, S-pairs on which the model was
trained, are generated randomly, we show results
for comprehension and production averaged over
5 simulations.
6 Experiments
A central motivation for the development of this
model is to account for early grammatical produc-
tion: can we simulate the developmental pattern
of the growth of utterance length and a growing
potential for generalization? The same construc-
tions underlying these productions should, at the
same time, also account for the learner?s increas-
ing grasp of the meaning of U . To explore the
model?s performance in both domains, we present
a comprehension and a generation experiment.
6.1 Comprehension results
Fig. 6a gives us the results over time of the com-
prehension measures given a propositional un-
certainty of 1, i.e. one situation besides s
correct
in S. Overall, the model understands the utter-
ances increasingly well. After 2000 input items,
the model identifies s
correct
in 95% of the cases.
With higher levels of propositional uncertainty
(not shown here), performance is still relatively
robust: given 5 incorrect situations in S, s
correct
is identified in 62% of all cases (random guess-
ing gives a score of 17%, or
1
6
). Similarly, the
proportion of the situation interpreted and the pro-
portion of the utterance analyzed go up over time.
This means that the model builds up an increasing
repertoire of constructions that allow it to analyze
larger parts of the utterance and the situations it
identifies. It is important to realize that these mea-
51
0 500 1000 1500 2000
0.0
0.2
0.4
0.6
0.8
1.0
time
prop
ortion
measures
situation coverageutterance coverageidentification
(a) Comprehension results over time
0 500 1000 1500 2000
0.0
0.5
1.0
1.5
2.0
2.5
3.0
time
utter
anc
e len
gth in
 word
s
(b) Length of U
gen
over time
0 500 1000 1500 2000
0.0
0.2
0.4
0.6
0.8
1.0
time
prop
ortion
measures
situation coverageutterance precisionutterance recall
(c) Generation results over time
Figure 6: Quantitative results for the comprehension and generation experiments
sures do not display what proportion of the utter-
ance or situation is analyzed correctly.
6.2 Generation results
Quantitative results Fig. 6b shows that the av-
erage utterance length increases over time. This
indicates that the number of constituents of the
used constructions grows. Next, Fig. 6c shows the
performance of the model on the generation task.
After 2000 input items, the model generates pro-
ductions expressing 93% of the situation, with an
utterance precision of 0.91, and an utterance recall
of 0.81. Given a propositional uncertainty of 5,
these go down to 79%, 0.76 and 0.59 respectively.
Comparing the utterance precision and recall
over time, we can see that the utterance preci-
sion is high from the start, whereas the recall
gradually increases. This is in line with the ob-
servation that children predominantly produce er-
rors of omission (leaving linguistic material out an
adult speaker would produce), and few errors of
comission (producing linguistic material an adult
speaker would not produce).
Qualitative results Tracking individual produc-
tions given specific situations over time allows us
to study in detail what the model is doing. Here,
we look at one case qualitatively. Given the sit-
uation for which the U
act
is she put them away,
the model generates, over time, the utterances in
Table 1. The brackets show the internal hierarchi-
cal structure of the derivation. This development
illustrates several interesting aspects of the model.
First, as discussed earlier, the model mostly makes
errors of omission: earlier productions leave out
more words found in the adult utterances. Only at
t = 550, the model makes an error of commission,
using the word in erroneously.
[
[
s
h
e
]
p
u
t
]
[
s
h
e
[
p
u
t
]
]
[
[
s
h
e
]
[
p
u
t
]
[
i
n
]
]
[
[
s
h
e
]
p
u
t
t
h
e
m
[
a
w
a
y
]
]
[
[
s
h
e
]
p
u
t
[
t
h
e
m
]
]
[
[
s
h
e
]
p
u
t
t
h
e
m
[
a
w
a
y
]
]
[
[
s
h
e
]
p
u
t
[
t
h
e
m
]
a
w
a
y
]
[
[
s
h
e
]
p
u
t
t
h
e
m
a
w
a
y
]
t 50 500 550 600 950 1000 1050 1400
Table 1: Generations over time t for one situation.
Starting from t = 600 (except at t = 950),
the model generates the correct utterance, but the
derivations leading to this production differ. At
t = 550, for instance, the learner combines a
completely non-phonologically specific construc-
tion for which the constituents refer to the agent,
action and goal location, with three ?lexical? con-
structions that fill in the words for those items..
The constructions used after t = 550 are all more
specific, combining 3, or even only 2 constructions
(t ? 1400) where the entire sequence of words
?put them away? arises from a single construction.
Using less abstract constructions over time
seems contrary to the usage-based idea that con-
structions become more abstract over the course of
acquisition. However, this result follows from the
way the probability model is defined. More spe-
cific constructions that are able to account for the
input will entail fewer combinations, and a deriva-
tion with fewer combination operations will often
be more likely than one with more such opera-
tions. Given equal expressivity of the situation,
the former derivation will be selected over the lat-
ter in generation.
The effect is indeed in line with another concept
hypothesized to play a role in language acquisition
on a usage-based account, viz. pre-emption (Gold-
52
uterancsion uoi12.2ipe1cmmmccteran31ite.2ip u1ite.2ipclna.2pe.2ip uep2se.ncmm ueyynt.nlcmmmca.e.2ipe(, uid)nt.ce(.nyet. uid)nt.cbe(.nyet. f?? f?? f?? f?? f?????b n?????b ? ???b ? ???b ? ???b ?
u(na.cet. uoi12.2ipe1cbmmm uep2se.ncmmm ulna.2pe.2ipc1ite.2ip u1ite.2ipcnp.2., f?? f?? f????? ? ??? ? ??? ?
uet. uoi12.2ipe1cmmm uep2se.nc?ne(n( u2pln?np3lnp.3n?a. uid)nt.ce(.nyet. f?? f?? f?????b ? ? ???b ????
(b) (c)
(a)
Figure 7: Some representations at t = 2000
berg, 2006, 94-95). Pre-emption is the effect that
a language user will select a more concrete rep-
resentation over the combination of more abstract
ones. The effect can be reconceptualized in this
model as an epiphenomenon of the way the prob-
ability model works: simply because combining
fewer constructions in a derivation is often more
probable than combining more constructions, the
former derivation will be selected over the lat-
ter. Pre-emption is typically invoked to explain the
blocking of overgeneralization patterns, and an in-
teresting future step will be to see if the model can
simulate developmental patterns for well-known
cases of overgeneralization errors.
The potential for abstraction The paradigma-
tization operation allows the model to go beyond
observed concrete instances of form-meaning
pairings: without it, unseen situations could never
be fully expressed. Despite this potential, we have
seen that the model relies on highly concrete con-
structions. The concreteness of the used patterns,
however, does not imply the absence of more ab-
stract representations. Fig. 7 gives three exam-
ples of constructions in C in one simulation. Con-
struction (a) could be seen as a verb-island con-
struction (Tomasello, 1992, 23-24). The second
constituent is phonologically specified with put,
and the other arguments are open, but mapped to
specific semantic functions. This pattern allows
for the expression of many caused-motion events.
Construction (b) is the inverse of (a): the argu-
ments are phonologically specified, but the verb-
slot is open. This would be a case of a pronominal
argument frame [you V it], which have been found
to be helpful in the bootstrapping of verbal mean-
ings (Tomasello, 2001). Finally, (c) presents a case
of full abstraction. This construction licenses ut-
terances such as I sit here, you stay there and er-
roneous ones like he sits on (which, again, will be
pre-empted in the generation of utterances if more
concrete constructions licence he sits on it).
Summarizing, abstract constructions are ac-
quired, but only used for those cases in which no
concrete construction is available. This is in line
with the usage-based hypotheses that abstract con-
structions do emerge, but that for much of lan-
guage production, a language user can rely on
highly concrete patterns. A next step will be
to measure the development of abstractness and
length over the constructions themselves, rather
than the parses and generations they allow.
7 Conclusion
This, admittedly complex, model forms an attempt
to model different learning mechanisms in interac-
tion from a usage-based constructionist perspec-
tive. Starting with an empty set of linguistic rep-
resentations, the model acquires words and gram-
matical constructions simultaneously. The learn-
ing mechanisms allow the model to build up in-
creasingly abstract, as well as increasingly long
constructions. With these developing representa-
tions, we showed how the model gets better over
time at understanding the input item, performing
relatively robustly under propositional uncertainty.
Moreover, in the generation experiment, the
model shows patterns of production (increasingly
long utterances) similar to those of children. An
important future step will be to look at these pro-
ductions more closely and investigate if they also
converge on more detailed patterns of develop-
ment in the production of children (e.g. item-
specificity, as hypothesized on the usage-based
view). Despite highly concrete constructions suf-
ficing for most of production, inspection of the ac-
quired representations tells us that more abstract
constructions are learned as well. Here, an inter-
esting next step would be to simulate patterns of
overgeneralization in children?s production.
Acknowledgements
We would like to thank three anonymous review-
ers for their valuable and thoughtful comments.
We gratefully acknowledge the funding of BB
through NWO of the Netherlands (322.70.001)
and AF and SS through NSERC of Canada.
53
References
Nameera Akhtar and Michael Tomasello. 1997.
Young Children?s Productivity With Word Or-
der and Verb Morphology. Developmental Psy-
chology, 33(6):952?965.
Afra Alishahi and Suzanne Stevenson. 2008 A
Computational Model of Early Argument Struc-
ture Acquisition. Cognitive Science, 32(5):789?
834.
Afra Alishahi and Suzanne Stevenson. 2010. A
computational model of learning semantic roles
from child-directed language. Language and
Cognitive Processes, 25(1):50?93.
Ben Ambridge, Julian M Pine, and Caroline F
Rowland. 2012. Semantics versus statistics in
the retreat from locative overgeneralization er-
rors. Cognition, 123(2):260?79.
Colin Bannard, Elena Lieven, and Michael
Tomasello. 2009. Modeling children?s early
grammatical knowledge. Proceedings of the
National Academy of Sciences of the United
States of America, 106(41):17284?9.
Martin D.S. Braine. 1976. Children?s first word
combinations. University of Chicago Press,
Chicago, IL.
Joan Bybee. 1995. Regular morphology and the
lexicon. Language and Cognitive Processes, 10
(5):425?455.
Nancy C.-L. Chang. 2008. Constructing Gram-
mar: A computational model of the emergence
of early constructions. Dissertation, University
of California, Berkeley.
Daniel Freudenthal, Julian Pine, and Fernand Go-
bet. 2010. Explaining quantitative variation in
the rate of Optional Infinitive errors across lan-
guages: a comparison of MOSAIC and the Vari-
ational Learning Model. Journal of Child Lan-
guage, 37(3):643?69.
Adele E. Goldberg. 1995. Constructions. A
Construction Grammar Approach to Argument
Structure. Chicago University Press, Chicago,
IL.
Adele E Goldberg. 2003. Constructions: a new
theoretical approach to language. Trends in
Cognitive Sciences, 7(5):219?224.
Adele E. Goldberg. 2006. Constructions at Work.
The Nature of Generalization in Language. Ox-
ford University Press, Oxford.
Paul Kay. 2002. An Informal Sketch of a Formal
Architecture for Construction Grammar. Gram-
mars, 5:1?19.
Tom Kwiatkowski, Sharon Goldwater, Luke
Zettlemoyer, and Mark Steedman. 2012. A
Probabilistic Model of Syntactic and Seman-
tic Acquisition from Child-Directed Utterances
and their Meanings. In Proceedings EACL.
Ronald W. Langacker. 1989. Foundations of Cog-
nitive Grammar, Volume I. Stanford University
Press.
Ronald W. Langacker. 2009. A dynamic view of
usage and language acquisition. Cognitive Lin-
guistics, 20(3):627?640.
Jeffrey M Siskind. 1996. A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):39?
91.
Michael Tomasello. 1992. First Verbs: A study
of early grammatical development. Cambridge
University Press, Cambridge, UK.
Michael Tomasello. 2001 Perceiving intentions
and learning words in the second year of life.
In Melissa Bowerman and Stephen C. Levinson,
editors, Language Acquisition and Conceptual
Development, chapter 5, pages 132?158. Cam-
bridge University Press, Cambridge, UK.
Michael Tomasello. 2003. Constructing a lan-
guage: A Usage-Based Theory of Language
Acquisition. Harvard University Press, Cam-
bridge, MA.
Arie Verhagen. 2009 The conception of construc-
tions as complex signs. Emergence of struc-
ture and reduction to usage. Constructions and
Frames, 1:119?152.
54
