Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1080?1089,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Latent-Variable Modeling of String Transductions
with Finite-State Methods?
Markus Dreyer and Jason R. Smith and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{markus,jsmith,jason}@cs.jhu.edu
Abstract
String-to-string transduction is a central prob-
lem in computational linguistics and natural
language processing. It occurs in tasks as di-
verse as name transliteration, spelling correc-
tion, pronunciation modeling and inflectional
morphology. We present a conditional log-
linear model for string-to-string transduction,
which employs overlapping features over la-
tent alignment sequences, and which learns la-
tent classes and latent string pair regions from
incomplete training data. We evaluate our ap-
proach on morphological tasks and demon-
strate that latent variables can dramatically
improve results, even when trained on small
data sets. On the task of generating mor-
phological forms, we outperform a baseline
method reducing the error rate by up to 48%.
On a lemmatization task, we reduce the error
rates in Wicentowski (2002) by 38?92%.
1 Introduction
A recurring problem in computational linguistics
and language processing is transduction of charac-
ter strings, e.g., words. That is, one wishes to model
some systematic mapping from an input string x to
an output string y. Applications include:
? phonology: underlying representation ? surface
representation
? orthography: pronunciation? spelling
? morphology: inflected form ? lemma, or differ-
ently inflected form
? fuzzy name matching (duplicate detection) and
spelling correction: spelling? variant spelling
?This work was supported by the Human Language Tech-
nology Center of Excellence and by National Science Founda-
tion grant No. 0347822 to the final author. We would also like
to thank Richard Wicentowski for providing us with datasets for
lemmatization, and the anonymous reviewers for their valuable
feedback.
? lexical translation (cognates, loanwords, translit-
erated names): English word? foreign word
We present a configurable and robust framework
for solving such word transduction problems. Our
results in morphology generation show that the pre-
sented approach improves upon the state of the art.
2 Model Structure
A weighted edit distance model (Ristad and Yian-
ilos, 1998) would consider each character in isola-
tion. To consider more context, we pursue a very
natural generalization. Given an input x, we evalu-
ate a candidate output y by moving a sliding window
over the aligned (x, y) pair. More precisely, since
many alignments are possible, we sum over all these
possibilities, evaluating each alignment separately.1
At each window position, we accumulate log-
probability based on the material that appears within
the current window. The window is a few charac-
ters wide, and successive window positions over-
lap. This stands in contrast to a competing approach
(Sherif and Kondrak, 2007; Zhao et al, 2007)
that is inspired by phrase-based machine translation
(Koehn et al, 2007), which segments the input string
into substrings that are transduced independently, ig-
noring context.2
1At the other extreme, Freitag and Khadivi (2007) use no
alignment; each feature takes its own view of how (x, y) relate.
2We feel that this independence is inappropriate. By anal-
ogy, it would be a poor idea for a language model to score a
string highly if it could be segmented into independently fre-
quent n-grams. Rather, language models use overlapping n-
grams (indeed, it is the language model that rescues phrase-
based MT from producing disjointed translations). We believe
phrase-based MT avoids overlapping phrases in the channel
model only because these would complicate the modeling of
reordering (though see, e.g., Schwenk et al (2007) and Casacu-
berta (2000)). But in the problems of section 1, letter reordering
is rare and we may assume it is local to a window.
1080
Figure 1: One of many possible alignment strings A for
the observed pair breaking/broke, enriched with latent
strings `1 and `2. Observed letters are shown in bold. The
box marks a trigram to be scored. See Fig. 2 for features
that fire on this trigram.
Joint n-gram models over the input and output di-
mensions have been used before, but not for mor-
phology, where we will apply them.3 Most notable
is the local log-linear grapheme-to-phoneme model
of Chen (2003), as well as generative models for
that task (Deligne et al (1995), Galescu and Allen
(2001), Bisani and Ney (2002)).
We advance that approach by adding new latent
dimensions to the (input, output) tuples (see Fig. 1).4
This enables us to use certain linguistically inspired
features and discover unannotated information. Our
features consider less or more than a literal n-gram.
On the one hand, we generalize with features that
abstract away from the n-gram window contents; on
the other, we specialize the n-gram with features that
make use of the added latent linguistic structure.
In section 5, we briefly sketch our framework for
concisely expressing and efficiently implementing
models of this form. Our framework uses familiar
log-linear techniques for stochastic modeling, and
weighted finite-state methods both for implementa-
tion and for specifying features. It appears general
enough to cover most prior work on word transduc-
tion. We imagine that it will be useful for future
work as well: one might easily add new, linguisti-
cally interesting classes of features, each class de-
fined by a regular expression.
2.1 Basic notation
We use an input alphabet ?x and output alphabet
?y. We conventionally use x ? ??x to denote the
input string and y ? ??y to denote the output string.
3Clark (2001) does use pair HMMs for morphology.
4Demberg et al (2007) similarly added extra dimensions.
However, their added dimensions were supervised, not latent,
and their model was a standard generative n-gram model whose
generalization was limited to standard n-gram smoothing.
There are many possible alignments between x
and y. We represent each as an alignment string
A ? ??
xy
, over an alignment alphabet of ordered
pairs, ?xy
def
= ((?x ? {})? (?y ? {}))? {(, )}.
For example, one alignment of x = breaking
with y = broke is the 9-character string A =
(b,b)(r,r)(e,o)(a, )(k,k)(,e)(i, )(n, )(g, ).
It is pictured in the first two lines of Fig. 1.
The remainder of Fig. 1 shows how we intro-
duce latent variables, by enriching the alignment
characters to be tuples rather than pairs. Let ?
def
=
(?xy ? ?`1 ? ?`2 ? ? ? ? ? ?`K ), where ?`i are al-
phabets used for the latent variables `i.
FSA and FST stand for ?finite-state acceptor? and
?finite-state transducer,? while WFSA and WFST
are their weighted variants. The ? symbol denotes
composition.
Let T be a relation and w a string. We write T [w]
to denote the image of w under T (i.e., range(w ?
T )), a set of 0 or more strings. Similarly, if W is a
weighted language (typically encoded by a WFSA),
we write W [w] to denote the weight of w in W .
Let pix ? ?? ? ??x denote the deterministic reg-
ular relation that projects an alignment string to its
corresponding input string, so that pix[A] = x. Sim-
ilarly, define piy ? ?? ? ??y so that piy[A] = y. Let
Axy be the set of alignment strings A compatible
with x and y; formally, Axy
def
= {A ? ?? : pix[A] =
x?piy[A] = y}. This set will range over all possible
alignments between x and y, and also all possible
configurations of the latent variables.
2.2 Log-linear modeling
We use a standard log-linear model whose features
are defined on alignment strings A ? Axy, allow-
ing them to be sensitive to the alignment of x and y.
Given a collection of features fi : ?? ? R with as-
sociated weights ?i ? R, the conditional likelihood
of the training data is
p?(y | x) =
?
A?Axy exp
?
i ?ifi(A)
?
y?
?
A?Axy?
exp
?
i ?ifi(A)
(1)
Given a parameter vector ?, we compute equa-
tion (1) using a finite-state machine. We define a
WFSA,U?, such thatU?[A] yields the unnormalized
probability u?(A)
def
= exp
?
i ?ifi(A) for any A ?
??. (See section 5 for the construction.) To obtain
1081
the numerator of equation (1), with its
?
A?Axy , we
sum over all paths in U? that are compatible with x
and y. That is, we build x ? pi?1x ? U? ? piy ? y and
sum over all paths. For the denominator we build the
larger machine x ? pi?1x ? U? and again compute the
pathsum. We use standard algorithms (Eisner, 2002)
to compute the pathsums as well as their gradients
with respect to ? for optimization (section 4.1).
Below, we will restrict our notion of valid align-
ment strings in ??. U? is constructed not to accept
invalid ones, thus assigning them probability 0.
Note that the possible output strings y? in the de-
nominator in equation (1) may have arbitrary length,
leading to an infinite summation over alignment
strings. Thus, for some values of ?, the sum in
the denominator diverges and the probability dis-
tribution is undefined. There exist principled ways
to avoid such ? during training. However, in our
current work, we simply restrict to finitely many
alignment strings (given x), by prohibiting as invalid
those with > k consecutive insertions (i.e., charac-
ters like (,a)).5 Finkel et al (2008) and others have
similarly bounded unary rule cycles in PCFGs.
2.3 Latent variables
The alignment between x and y is a latent ex-
planatory variable that helps model the distribution
p(y | x) but is not observed in training. Other latent
variables can also be useful. Morphophonological
changes are often sensitive to phonemes (whereas x
and y may consist of graphemes); syllable bound-
aries; a conjugation class; morpheme boundaries;
and the position of the change within the form.
Thus, as mentioned in section 2.1, we enrich the
alignment string A so that it specifies additional la-
tent variables to which features may wish to refer.
In Fig. 1, two latent strings are added, enabling the
features in Fig. 2(a)?(h). The first character is not
5We set k to a value between 1 and 3, depending on the tasks,
always ensuring that no input/output pairs observed in training
are excluded. The insertion restriction does slightly enlarge the
FSA U?: a state must keep track of the number of consecutive
 symbols in the immediately preceding x input, and for a few
states, this cannot be determined just from the immediately pre-
ceding (n ? 1)-gram. Despite this, we found empirically that
our approximation is at least as fast as the exact method of Eis-
ner (2002), who sums around cyclic subnetworks to numerical
convergence. Furthermore, our approximation does not require
us to detect divergence during training.
Figure 2: The boxes (a)-(h) represent some of the features
that fire on the trigram shown in Fig. 1. These features are
explained in detail in section 3.
just an input/output pair, but the 4-tuple (b,b,2,1).
Here, `1 indicates that this form pair (breaking /
broke) as a whole is in a particular cluster, or word
class, labeled with the arbitrary number 2. Notice in
Fig. 1 that the class 2 is visible in all local windows
throughout the string. It allows us to model how cer-
tain phenomena, e.g. the vowel change from ea to
o, are more likely in one class than in another. Form
pairs in the same class as the breaking / broke ex-
ample might include the following Germanic verbs:
speak, break, steal, tear, and bear.
Of course, word classes are latent (not labeled in
our training data). Given x and y, Axy will include
alignment strings that specify class 1, and others
that are identical except that they specify class 2;
equation (1) sums over both possibilities.6 In a valid
alignment stringA, `1 must be a constant string such
as 111... or 222..., as in Fig. 1, so that it spec-
ifies a single class for the entire form pair. See sec-
tions 4.2 and 4.3 for examples of what classes were
learned in our experiments.
The latent string `2 splits the string pair into num-
bered regions. In a valid alignment string, the re-
gion numbers must increase throughout `2, although
numbers may be skipped to permit omitted regions.
To guide the model to make a useful division into
regions, we also require that identity characters such
as (b,b) fall in even regions while change charac-
ters such as (e,o) (substitutions, deletions, or inser-
6The latent class is comparable to the latent variable on the
tree root symbol S in Matsuzaki et al (2005).
1082
tions) fall in odd regions.7 Region numbers must not
increase within a sequence of consecutive changes
or consecutive identities.8 In Fig. 1, the start of re-
gion 1 is triggered by e:o, the start of region 2 by
the identity k:k, region 3 by :e.
Allowing region numbers to be skipped makes it
possible to consistently assign similar labels to sim-
ilar regions across different training examples. Ta-
ble 2, for example, shows pairs that contain a vowel
change in the middle, some of which contain an ad-
ditional insertion of ge in the begining (verbinden
/ verbunden, reibt / gerieben). We expect the model
to learn to label the ge insertion with a 1 and vowel
change with a 3, skipping region 1 in the examples
where the ge insertion is not present (see section
4.2, Analysis).
In the next section we describe features over these
enriched alignment strings.
3 Features
One of the simplest ways of scoring a string is an n-
gram model. In our log-linear model (1), we include
ngram features fi(A), each of which counts the oc-
currences in A of a particular n-gram of alignment
characters. The log-linear framework lets us include
ngram features of different lengths, a form of back-
off smoothing (Wu and Khudanpur, 2000).
We use additional backoff features on alignment
strings to capture phonological, morphological, and
orthographic generalizations. Examples are found in
features (b)-(h) in Fig. 2. Feature (b) matches vowel
and consonant character classes in the input and
output dimensions. In the id/subst ngram feature,
we have a similar abstraction, where the character
classes ins, del, id, and subst are defined over in-
put/output pairs, to match insertions, deletions, iden-
tities (matches), and substitutions.
In string transduction tasks, it is helpful to in-
clude a language model of the target. While this
can be done by mixing the transduction model with
a separate language model, it is desirable to in-
clude a target language model within the transduc-
7This strict requirement means, perhaps unfortunately, that a
single region cannot accommodate the change ayc:xyz unless
the two y?s are not aligned to each other. It could be relaxed,
however, to a prior or an initialization or learning bias.
8The two boundary characters #, numbered 0 and max
(max=6 in our experiments), are neither changes nor identities.
tion model. We accomplish this by creating target
language model features, such as (c) and (g) from
Fig. 2, which ignore the input dimension. We also
have features which mirror features (a)-(d) but ig-
nore the latent classes and/or regions (e.g. features
(e)-(h)).
Notice that our choice of ? only permits mono-
tonic, 1-to-1 alignments, following Chen (2003).
We may nonetheless favor the 2-to-1 alignment
(ea,o) with bigram features such as (e,o)(a,). A
?collapsed? version of a feature will back off from
the specific alignment of the characters within a win-
dow: thus, (ea,o) is itself a feature. Currently, we
only include collapsed target language model fea-
tures. These ignore epsilons introduced by deletions
in the alignment, so that collapsed ok fires in a win-
dow that contains ok.
4 Experiments
We evaluate our model on two tasks of morphol-
ogy generation. Predicting morphological forms has
been shown to be useful for machine translation and
other tasks.9 Here we describe two sets of exper-
iments: an inflectional morphology task in which
models are trained to transduce verbs from one form
into another (section 4.2), and a lemmatization task
(section 4.3), in which any inflected verb is to be re-
duced to its root form.
4.1 Training and decoding
We train ? to maximize the regularized10 conditional
log-likelihood11
?
(x,y?)?C
log p?(y
? | x) + ||?||2/2?2, (2)
where C is a supervised training corpus. To max-
imize (2) during training, we apply the gradient-
based optimization method L-BFGS (Liu and No-
cedal, 1989).12
9E.g., Toutanova et al (2008) improve MT performance
by selecting correct morphological forms from a knowledge
source. We instead focus on generalizing from observed forms
and generating new forms (but see with rootlist in Table 3).
10The variance ?2 of the L2 prior is chosen by optimizing on
development data. We are also interested in trying an L1 prior.
11Alternatives would include faster error-driven methods
(perceptron, MIRA) and slower max-margin Markov networks.
12This worked a bit better than stochastic gradient descent.
1083
To decode a test example x, we wish to find
y? = argmaxy???y p?(y | x). Constructively, y? is the
highest-probability string in the WFSA T [x], where
T = pi?1x ?U??piy is the trained transducer that maps
x nondeterministically to y. Alas, it is NP-hard to
find the highest-probability string in a WFSA, even
an acyclic one (Casacuberta and Higuera, 2000).
The problem is that the probability of each string y
is a sum over many paths in T [x] that reflect differ-
ent alignments of y to x. Although it is straightfor-
ward to use a determinization construction (Mohri,
1997)13 to collapse these down to a single path per
y (so that y? is easily read off the single best path),
determinization can increase the WFSA?s size expo-
nentially. We approximate by pruning T [x] back to
its 1000-best paths before we determinize.14
Since the alignments, classes and regions are not
observed in C, we do not enjoy the convex objec-
tive function of fully-supervised log-linear models.
Training equation (2) therefore converges only to
some local maximum that depends on the starting
point in parameter space. To find a good starting
point we employ staged training, a technique in
which several models of ascending complexity are
trained consecutively. The parameters of each more
complex model are initialized with the trained pa-
rameters of the previous simpler model.
Our training is done in four stages. All weights
are initialized to zero. ? We first train only fea-
tures that fire on unigrams of alignment charac-
ters, ignoring features that examine the latent strings
or backed-off versions of the alignment characters
(such as vowel/consonant or target language model
features). The resulting model is equivalent to
weighted edit distance (Ristad and Yianilos, 1998).
? Next,15 we train all n-grams of alignment charac-
ters, including higher-order n-grams, but no backed-
off features or features that refer to latent strings.
13Weighted determinization is not always possible, but it is
in our case because our limit to k consecutive insertions guar-
antees that T [x] is acyclic.
14This value is high enough; we see no degradations in per-
formance if we use only 100 or even 10 best paths. Below that,
performance starts to drop slightly. In both of our tasks, our
conditional distributions are usually peaked: the 5 best output
candidates amass > 99% of the probability mass on average.
Entropy is reduced by latent classes and/or regions.
15When unclamping a feature at the start of stages ???, we
initialize it to a random value from [?0.01, 0.01].
13SIA. liebte, pickte, redete, rieb, trieb, zuzog
13SKE. liebe, picke, rede, reibe, treibe, zuziehe
2PIE. liebt, pickt, redet, reibt, treibt, zuzieht
13PKE.lieben, picken, reden, reiben, treiben, zuziehen
2PKE. abbrechet, entgegentretet, zuziehet
z. abzubrechen, entgegenzutreten, zuzuziehen
rP. redet, reibt, treibt, verbindet, u?berfischt
pA.geredet, gerieben, getrieben, verbunden, u?berfischt
Table 2: CELEX forms used in our experiments. Changes
from one form to the other are in bold (information not
given in training). The changes from rP to pA are very
complex. Note also the differing positions of zu in z.
? Next, we add backed-off features as well as all
collapsed features. ? Finally, we train all features.
In our experiments, we permitted latent classes 1?
2 and, where regions are used, regions 0?6. For
speed, stages ??? used a pruned ? that included
only ?plausible? alignment characters: a may not
align to b unless it did so in the trained stage-(1)
model?s optimal alignment of at least one training
pair (x, y?).
4.2 Inflectional morphology
We conducted several experiments on the CELEX
morphological database. We arbitrarily consid-
ered mapping the following German verb forms:16
13SIA ? 13SKE, 2PIE ? 13PKE, 2PKE ? z,
and rP ? pA.17 We refer to these tasks as 13SIA,
2PIE, 2PKE and rP. Table 2 shows some examples
of regular and irregular forms. Common phenomena
include stem changes (ei:ie), prefixes inserted af-
ter other morphemes (abzubrechen) and circumfixes
(gerieben).
We compile lists of form pairs from CELEX. For
each task, we sample 2500 data pairs without re-
placement, of which 500 are used for training, 1000
as development and the remaining 1000 as test data.
We train and evaluate models on this data and repeat
16From the available languages in CELEX (German, Dutch,
and English), we selected German as the language with the
most interesting morphological phenomena, leaving the mul-
tilingual comparison for the lemmatization task (section 4.3),
where there were previous results to compare with. The 4 Ger-
man datasets were picked arbitrarily.
17A key to these names: 13SIA=1st/3rd sg. ind. past;
13SKE=1st/3rd sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.;
13PKE=1st/3rd pl. subjunct. pres.; 2PKE=2nd pl. subjunct.
pres.; z=infinitive; rP=imperative pl.; pA=past part.
1084
Features Task
ng vc tlm tlm-coll id lat.cl. lat.reg. 13SIA 2PIE 2PKE rP
ngrams x 82.3 (.23) 88.6 (.11) 74.1 (.52) 70.1 (.66)
ngrams+x
x x 82.8 (.21) 88.9 (.11) 74.3 (.52) 70.0 (.68)
x x 82.0 (.23) 88.7 (.11) 74.8 (.50) 69.8 (.67)
x x x 82.5 (.22) 88.6 (.11) 74.9 (.50) 70.0 (.67)
x x x 81.2 (.24) 88.7 (.11) 74.5 (.50) 68.6 (.69)
x x x x 82.5 (.22) 88.8 (.11) 74.5 (.50) 69.2 (.69)
x x 82.4 (.22) 88.9 (.11) 74.8 (.51) 69.9 (.68)
x x x 83.0 (.21) 88.9 (.11) 74.9 (.50) 70.3 (.67)
x x x 82.2 (.22) 88.8 (.11) 74.8 (.50) 70.0 (.67)
x x x x 82.9 (.21) 88.6 (.11) 75.2 (.50) 69.7 (.68)
x x x x 81.9 (.23) 88.6 (.11) 74.4 (.51) 69.1 (.68)
x x x x x 82.8 (.21) 88.7 (.11) 74.7 (.50) 69.9 (.67)
ngrams+x
+latent
x x x x x x 84.8 (.19) 93.6 (.06) 75.7 (.48) 81.8 (.43)
x x x x x x 87.4 (.16) 93.8 (.06) 88.0 (.28) 83.7 (.42)
x x x x x x x 87.5 (.16) 93.4 (.07) 87.4 (.28) 84.9 (.39)
Moses3 73.9 (.40) 92.0 (.09) 67.1 (.70) 67.6 (.77)
Moses9 85.0 (.21) 94.0 (.06) 82.3 (.31) 70.8 (.67)
Moses15 85.3 (.21) 94.0 (.06) 82.8 (.30) 70.8 (.67)
Table 1: Exact-match accuracy and average edit distance (the latter in parentheses) versus the correct answer on the
German inflection task, using different combinations of feature classes. The label ngrams corresponds to the second
stage of training, ngrams+x to the third where backoff features may fire (vc = vowel/consonant, tlm = target LM, tlm-
coll = collapsed tlm, id = identity/substitution/deletion features), and ngrams+x+latent to the fourth where features
sensitive to latent classes and latent regions are allowed to fire. The highest n-gram order used is 3, except for Moses9
and Moses15 which examine windows of up to 9 and 15 characters, respectively. We mark in bold the best result for
each dataset, along with all results that are statistically indistinguishable (paired permutation test, p < 0.05).
the process 5 times. All results are averaged over
these 5 runs.
Table 1 and Fig. 3 report separate results after
stages ?, ?, and ? of training, which include suc-
cessively larger feature sets. These are respectively
labeled ngrams, ngrams+x, and ngrams+x+latent.
In Table 1, the last row in each section shows the
full feature set at that stage (cf. Fig. 3), while earlier
rows test feature subsets.18
Our baseline is the SMT toolkit Moses (Koehn et
al., 2007) run over letter strings rather than word
strings. It is trained (on the same data splits) to
find substring-to-substring phrase pairs and translate
from one form into another (with phrase reordering
turned off). Results reported as moses3 are obtained
from Moses runs that are constrained to the same
context windows that our models use, so the maxi-
mum phrase length and the order of the target lan-
guage model were set to 3. We also report results
using much larger windows, moses9 and moses15.
18The number k of consecutive insertions was set to 3.
Results. The results in Table 1 show that including
latent classes and/or regions improves the results
dramatically. Compare the last line in ngrams+x
to the last line in ngrams+x+latent. The accuracy
numbers improve from 82.8 to 87.5 (13SIA), from
88.7 to 93.4 (2PIE), from 74.7 to 87.4 (2PKE), and
from 69.9 to 84.9 (rP).19 This shows that error re-
ductions between 27% and 50% were reached. On
3 of 4 tasks, even our simplest ngrams method beats
the moses3 method that looks at the same amount of
context.20 With our full model, in particular using
latent features, we always outperform moses3?and
even outperform moses15 on 3 of the 4 datasets, re-
ducing the error rate by up to 48.3% (rP). On the
fourth task (2PIE), our method and moses15 are sta-
tistically tied. Moses15 has access to context win-
dows of five times the size than we allowed our
methods in our experiments.
19All claims in the text are statistically significant under a
paired permutation test (p < .05).
20This bears out our contention in footnote 2 that a ?segment-
ing? channel model is damaging. Moses cannot fully recover by
using overlapping windows in the language model.
1085
While the gains from backoff features in Table 1
were modest (significant gains only on 13SIA), the
learning curve in Fig. 3 suggests that they were help-
ful for smaller training sets on 2PKE (see ngrams vs
ngrams+x on 50 and 100) and helped consistently
over different amounts of training data for 13SIA.
Analysis. The types of errors that our system (and
the moses baseline) make differ from task to task.
Due to lack of space, we mainly focus on the com-
plex rP task. Here, most errors come from wrongly
copying the input to the output, without making a
change (40-50% of the errors in all models, except
for our model with latent classes and no regions,
where it accounts for only 30% of the errors). This
is so common because about half of the training ex-
amples contain identical inputs and outputs (as in
the imperative berechnet and the participle (ihr habt)
berechnet). Another common error is to wrongly as-
sume a regular conjugation (just insert the prefix ge-
at the beginning). Interestingly, this error by sim-
plification is more common in the Moses models
(44% of moses3 errors, down to 40% for moses15)
than in our models, where it accounts for 37% of
the errors of our ngrams model and only 19% if la-
tent classes or latent regions are used; however, it
goes up to 27% if both latent classes and regions
are used.21 All models for rP contain errors where
wrong analogies to observed words are made (ver-
schweisst/verschwissen in analogy to the observed
durchweicht/durchwichen, or bebt/geboben in anal-
ogy to hebt/gehoben). In the 2PKE task, most errors
result from inserting the zu morpheme at a wrong
place or inserting two of them, which is always
wrong. This error type was greatly reduced by la-
tent regions, which can discover different parame-
ters for different positions, making it easier to iden-
tify where to insert the zu.
Analysis of the 2 latent classes (when used) shows
that a split into regular and irregular conjugations
has been learned. For the rP task we compute,
for each data pair in development data, the poste-
rior probabilities of membership in one or the other
class. 98% of the regular forms, in which the past
participle is built with ge- . . . -t, fall into one class,
21We suspect that training of the models that use classes and
regions together was hurt by the increased non-convexity; an-
nealing or better initialization might help.
Figure 3: Learning curves for German inflection tasks,
13SIA (left) and 2PKE (right), as a function of the num-
ber of training pairs. ngrams+x means all backoff fea-
tures were used, ngrams+x+latent means all latent fea-
tures were used in addition. Moses15 examines windows
of up to 15 characters.
which in turn consists nearly exclusively (96%) of
these forms. Different irregular forms are lumped
into the other class.
The learned regions are consistent across different
pairs. On development data for the rP task, 94.3%
of all regions that are labeled 1 are the insertion se-
quence (,ge), region 3 consists of vowel changes
93.7% of the time; region 5 represents the typical
suffixes (t,en), (et,en), (t,n) (92.7%). In the
2PKE task, region 0 contains different prefixes (e.g.
entgegen in entgegenzutreten), regions 1 and 2 are
empty, region 3 contains the zu affix, region 4 the
stem, and region 5 contains the suffix.
The pruned alignment alphabet excluded a few
gold standard outputs so that the model contains
paths for 98.9%?99.9% of the test examples. We
verified that the insertion limit did not hurt oracle
accuracy.
4.3 Lemmatization
We apply our models to the task of lemmatization,
where the goal is to generate the lemma given an in-
flected word form. We compare our model to Wicen-
towski (2002, chapter 3), an alternative supervised
approach. Wicentowski?s Base model simply learns
how to replace an arbitrarily long suffix string of an
input word, choosing some previously observed suf-
fix? suffix replacement based on the input word?s
1086
Without rootlist (generation) With rootlist (selection)
Wicentowski (2002) This paper Wicentowski (2002) This paper
Lang. Base Af. WFA. n n+x n+x+l Base Af. WFA. n n+x n+x+l
Basque 85.3 81.2 80.1 91.0 (.20) 91.1 (.20) 93.6 (.14) 94.5 94.0 95.0 90.9 (.29) 90.8 (.31) 90.9 (.30)
English 91.0 94.7 93.1 92.4 (.09) 93.4 (.08) 96.9 (.05) 98.3 98.6 98.6 98.7 (.04) 98.7(.04) 98.7(.04)
Irish 43.3 - 70.8 96.8 (.07) 97.0 (.06) 97.8 (.04) 43.9 - 89.1 99.6 (.02) 99.6 (.02) 99.5 (.03)
Tagalog 0.3 80.3 81.7 80.5 (.32) 83.0 (.29) 88.6 (.19) 0.8 91.8 96.0 97.0 (.07) 97.2 (.07) 97.7 (.05)
Table 3: Exact-match accuracy and average edit distance (the latter in parentheses) on the 8 lemmatization tasks (2
tasks ? 4 languages). The numbers from Wicentowski (2002) are for his Base, Affix and WFAffix models. The
numbers for our models are for the feature sets ngrams, ngrams+x, ngrams+x+latent. The best result per task is in
bold (as are statistically indistinguishable results when we can do the comparison, i.e., for our own models). Corpus
sizes: Basque 5,842, English 4,915, Irish 1,376, Tagalog 9,479.
final n characters (interpolating across different val-
ues of n). His Affix model essentially applies the
Base model after stripping canonical prefixes and
suffixes (given by a user-supplied list) from the input
and output. Finally, his WFAffix uses similar meth-
ods to also learn substring replacements for a stem
vowel cluster and other linguistically significant re-
gions in the form (identified by a deterministic align-
ment and segmentation of training pairs). This ap-
proach is a bit like our change regions combined
with Moses?s region-independent phrase pairs.
We compare against all three models. Note that
Affix and WFAffix have an advantage that our mod-
els do not, namely, user-supplied lists of canonical
affixes for each language. It is interesting to see
how our models with their more non-committal tri-
gram structure compare to this. Table 3 reports re-
sults on the data sets used in Wicentowski (2002),
for Basque, English, Irish, and Tagalog. Follow-
ing Wicentowski, 10-fold cross-validation was used.
The columns n+x and n+x+l mean ngram+x and
ngram+x+latent, respectively. As latent variables,
we include 2 word classes but no change regions.22
For completeness, Table 3 also compares with Wi-
centowski (2002) on a selection (rather than genera-
tion) task. Here, at test time, the lemma is selected
from a candidate list of known lemmas, namely, all
the output forms that appeared in training data.23
These additional results are labeled with rootlist in
the right half of Table 3.
On the supervised generation task without rootlist,
22The insertion limit k was set to 2 for Basque and 1 for the
other languages.
23Though test data contained no (input, output) pairs from
training data, it reused many of the output forms, since many
inflected inputs are to be mapped to the same output lemma.
our models outperform Wicentowski (2002) by a
large margin. Comparing our results that use la-
tent classes (n+x+l) with Wicentowski?s best mod-
els we observe error reductions ranging from about
38% (Tagalog) to 92% (Irish). On the selection task
with rootlist, we outperform Wicentowski (2002) in
English, Irish, and Tagalog.
Analysis. We examined the classes learned on En-
glish lemmatization by our ngrams+x+latent model.
For each of the input/output pairs in development
data, we found the most probable latent class. For
the most part, the 2 classes are separated based on
whether or not the correct output ends in e. This
use of latent classes helped address many errors like
wronging / wronge or owed / ow). Such missing or
surplus final e?s account for 72.5% of the errors for
ngrams and 70.6% of the errors for ngrams+x, but
only 34.0% of the errors for ngrams+x+latent.
The test oracles are between 99.8% ? 99.9%, due
to the pruned alignment alphabet. As on the inflec-
tion task, the insertion limit does not exclude any
gold standard paths.
5 Finite-State Feature Implementation
We used the OpenFST library (Allauzen et al, 2007)
to implement all finite-state computations, using the
expectation semiring (Eisner, 2002) for training.
Our model is defined by the WFSA U?, which is
used to score alignment strings in ?? (section 2.2).
We now sketch how to construct U? from features.
n-gram construction The construction that we
currently use is quite simple. All of our current
features fire on windows of width ? 3. We build
a WFSA with the structure of a 3-gram language
1087
model over ??. Each of the |?|2 states remembers
two previous alignment characters ab of history; for
each c ? ?, it has an outgoing arc that accepts c (and
leads to state bc). The weight of this arc is the total
weight (from ?) of the small set of features that fire
when the trigram window includes abc. By conven-
tion, these also include features on bc and c (which
may be regarded as backoff features ?bc and ??c).
Since each character in ? is actually a 4-tuple, this
trigram machine is fairly large. We build it lazily
(?on the fly?), constructing arcs only as needed to
deal with training or test data.
Feature templates Our experiments use over
50,000 features. How do we specify these features
to the above construction? Rather than writing ordi-
nary code to extract features from a window, we find
it convenient to harness FSTs as a ?little language?
(Bentley, 1986) for specifying entire sets of features.
A feature template T is an nondeterministic FST
that maps the contents of the sliding window, such
as abc, to one or more features, which are also
described as strings.24 The n-gram machine de-
scribed above can compute T [((a?b)?c)?] to find
out what features fire on abc and its suffixes. One
simple feature template performs ?vowel/consonant
backoff?; e.g., it maps abc to the feature named
VCC. Fig. 2 showed the result of applying several
actual feature templates to the window shown in
Fig. 1. The extended regular expression calculus
provides a flexible and concise notation for writ-
ing down these FSTs. As a trivial example, the tri-
gram ?vowel/consonant backoff? transducer can be
described as T = V V V , where V is a transducer
that performs backoff on a single alignment charac-
ter. Feature templates should make it easy to experi-
ment with adding various kinds of linguistic knowl-
edge. We have additional algorithms for compiling
U? from a set of arbitrary feature templates,25 in-
cluding templates whose features consider windows
of variable or even unbounded width. The details are
beyond the scope of this paper, but it is worth point-
ing out that they exploit the fact that feature tem-
plates are FSTs and not arbitrary code.
24Formally, if i is a string naming a feature, then fi(A)
counts the number of positions in A that are immediately pre-
ceded by some string in T?1[i].
25Provided that the total number of features is finite.
6 Conclusions
The modeling framework we have presented here
is, we believe, an attractive solution to most string
transduction problems in NLP. Rather than learn the
topology of an arbitrary WFST, one specifies the
topology using a small set of feature templates, and
simply trains the weights.
We evaluated on two morphology generation
tasks. When inflecting German verbs we, even with
the simplest features, outperform the moses3 base-
line on 3 out of 4 tasks, which uses the same amount
of context as our models. Introducing more sophis-
ticated features that have access to latent classes and
regions improves our results dramatically, even on
small training data sizes. Using these we outper-
form moses9 and moses15, which use long context
windows, reducing error rates by up to 48%. On the
lemmatization task we were able to improve the re-
sults reported in Wicentowski (2002) on three out of
four tested languages and reduce the error rates by
38% to 92%. The model?s errors are often reason-
able misgeneralizations (e.g., assume regular con-
jugation where irregular would have been correct),
and it is able to use even a small number of latent
variables (including the latent alignment) to capture
useful linguistic properties.
In future work, we would like to identify a set of
features, latent variables, and training methods that
port well across languages and string-transduction
tasks. We would like to use features that look at
wide context on the input side, which is inexpen-
sive (Jiampojamarn et al, 2007). Latent variables
we wish to consider are an increased number of
word classes; more flexible regions?see Petrov et
al. (2007) on learning a state transition diagram for
acoustic regions in phone recognition?and phono-
logical features and syllable boundaries. Indeed, our
local log-linear features over several aligned latent
strings closely resemble the soft constraints used by
phonologists (Eisner, 1997). Finally, rather than de-
fine a fixed set of feature templates as in Fig. 2,
we would like to refine empirically useful features
during training, resulting in language-specific back-
off patterns and adaptively sized n-gram windows.
Many of these enhancements will increase the com-
putational burden, and we are interested in strategies
to mitigate this, including approximation methods.
1088
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proc. of CIAA, volume 4783.
Jon Bentley. 1986. Programming pearls [column]. Com-
munications of the ACM, 29(8), August.
Maximilian Bisani and Hermann Ney. 2002. Inves-
tigations on jointmultigram models for grapheme-to-
phoneme conversion.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications, volume 1891.
Francisco Casacuberta. 2000. Inference of finite-
state transducers by using regular grammars and mor-
phisms. In A.L. Oliveira, editor, Grammatical Infer-
ence: Algorithms and Applications, volume 1891.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proc. of Inter-
speech.
Alexander Clark. 2001. Learning morphology with Pair
Hidden Markov Models. In Proc. of the Student Work-
shop at the 39th Annual Meeting of the Association for
Computational Linguistics, Toulouse, France, July.
Sabine Deligne, Francois Yvon, and Fre?de?ric Bimbot.
1995. Variable-length sequence matching for phonetic
transcription using joint multigrams. In Eurospeech.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphological
preprocessing for grapheme-to-phoneme conversion.
In Proc. of ACL, Prague, Czech Republic, June.
Jason Eisner. 1997. Efficient generation in primitive Op-
timality Theory. In Proc. of ACL-EACL, Madrid, July.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. of ACL.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Dayne Freitag and Shahram Khadivi. 2007. A sequence
alignment model based on the averaged perceptron. In
Proc. of EMNLP-CoNLL.
Lucian Galescu and James F. Allen. 2001. Bi-directional
conversion between graphemes and phonemes using a
joint N-gram model.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proc. of NAACL-HLT, Rochester, New
York, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
of ACL, Companion Volume, Prague, Czech Republic,
June.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45(3, (Ser. B)):503?528.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, Ann Arbor, Michigan, June.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2).
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learning
structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5).
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proc. of EMNLP-CoNLL, pages 430?438.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. of ACL, Prague, Czech
Republic, June.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
Columbus, Ohio, June.
Richard Wicentowski. 2002. Modeling and Learning
Multilingual Inflectional Morphology in a Minimally
Supervised Framework. Ph.D. thesis, Johns-Hopkins
University.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient training
methods for maximum entropy language modeling. In
Proc. of ICSLP, volume 3, Beijing, October.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Proc. of NAACL-HLT,
Rochester, New York, April.
1089
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 101?110,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Graphical Models over Multiple Strings
?
Markus Dreyer and Jason Eisner
Department of Computer Science / Johns Hopkins University
Baltimore, MD 21218, USA
{markus,jason}@cs.jhu.edu
Abstract
We study graphical modeling in the case of string-
valued random variables. Whereas a weighted
finite-state transducer can model the probabilis-
tic relationship between two strings, we are inter-
ested in building up joint models of three or more
strings. This is needed for inflectional paradigms
in morphology, cognate modeling or language re-
construction, and multiple-string alignment. We
propose a Markov Random Field in which each
factor (potential function) is a weighted finite-state
machine, typically a transducer that evaluates the
relationship between just two of the strings. The
full joint distribution is then a product of these fac-
tors. Though decoding is actually undecidable in
general, we can still do efficient joint inference
using approximate belief propagation; the nec-
essary computations and messages are all finite-
state. We demonstrate the methods by jointly pre-
dicting morphological forms.
1 Overview
This paper considers what happens if a graphical
model?s variables can range over strings of un-
bounded length, rather than over the typical finite
domains such as booleans, words, or tags. Vari-
ables that are connected in the graphical model are
related by some weighted finite-state transduction.
Graphical models have become popular in ma-
chine learning as a principled way to work with
collections of interrelated random variables. Most
often they are used as follows:
1. Build: Manually specify the n variables of
interest; their domains; and the possible di-
rect interactions among them.
2. Train: Train this model?s parameters ? to
obtain a specific joint probability distribution
p(V
1
, . . . , V
n
) over the n variables.
3. Infer: Use this joint distribution to predict
the values of various unobserved variables
from observed ones.
?
Supported by the Human Language Technology Center
of Excellence at Johns Hopkins University, and by National
Science Foundation grant No. 0347822 to the second author.
Note that 1. requires intuitions about the domain;
2. requires some choice of training procedure; and
3. requires a choice of exact or approximate infer-
ence algorithm.
Our graphical models over strings are natural
objects to investigate. We motivate them with
some natural applications in computational lin-
guistics (section 2). We then give our formalism:
a Markov Random Field whose potential functions
are rational weighted languages and relations (sec-
tion 3). Next, we point out that inference is in gen-
eral undecidable, and explain how to do approxi-
mate inference using message-passing algorithms
such as belief propagation (section 4). The mes-
sages are represented as weighted finite-state ma-
chines.
Finally, we report on some initial experiments
using these methods (section 7). We use incom-
plete data to train a joint model of morphological
paradigms, then use the trained model to complete
the data by predicting unseen forms.
2 Motivation
The problem of mapping between different forms
and representations of strings is ubiquitous in nat-
ural language processing and computational lin-
guistics. This is typically done between string
pairs, where a pronunciation is mapped to its
spelling, an inflected form to its lemma, a spelling
variant to its canonical spelling, or a name is
transliterated from one alphabet into another.
However, many problems involve more than just
two strings:
? in morphology, the inflected forms of a (possi-
bly irregular) verb are naturally considered to-
gether as a whole morphological paradigm in
which different forms reinforce one another;
? mapping an English word to its foreign translit-
eration may be easier when one considers the
orthographic and phonological forms of both
words;
? similar cognates in multiple languages are nat-
urally described together, in orthographic or
phonological representations, or both;
101
? modern and ancestral word forms form a phylo-
genetic tree in historical linguistics;
? in bioinformatics and in system combination,
multiple sequences need to be aligned in order
to identify regions of similarity.
We propose a unified model for multiple strings
that is suitable for all the problems mentioned
above. It is robust and configurable and can
make use of task-specific overlapping features. It
learns from observed and unobserved, or latent, in-
formation, making it useful in supervised, semi-
supervised, and unsupervised settings.
3 Formal Modeling Approach
3.1 Variables
A Markov Random Field (MRF) is a joint model
of a set of random variables, V = {V
1
, . . . , V
n
}.
We assume that all variables are string-valued, i.e.
the value of V
i
may be any string ? ?
?
i
, where ?
i
is some finite alphabet.
We may use meaningful names for the integers
i, such as V
2SA
for the 2nd singular past form of a
verb.
The assumption that all variables are string-
valued is not crucial; it merely simplifies our
presentation. It is, however, sufficient for many
practical purposes, since most other discrete ob-
jects can be easily encoded as strings. For exam-
ple, if V
1
is a part of speech tag, it may be en-
coded as a length-1 string over the finite alphabet
?
1
def
= {Noun,Verb, . . .}.
3.2 Factors
A Markov Random Field defines a probability for
each assignment A of values to the variables in V:
p(A)
def
=
1
Z
m
?
j=1
F
j
(A) (1)
This distribution over assignments is specified by
the collection of factors F
j
: A 7? R
?0
. Each
factor (or potential function) is a function that de-
pends on only a subset of A.
Fig. 1 displays an undirected factor graph, in
which each factor is connected to the variables
that it depends on. F
1
, F
3
, F
5
in this example are
unary factors because each one scores the value
of a single variable, while F
2
, F
4
, F
6
are binary
factors.
F
2
F
6
F
5
F
3
F
1
F
4
V
inf
V
2SA
V
3SE
Figure 1: Example of a factor graph. Black boxes represent
factors, circles represent variables (infinitive, 2nd past, and
3rd present-tense forms of the same verb; different samples
from the MRF correspond to different verbs). Binary factors
evaluate how well one string can be transduced into another,
summing over all transducer paths (i.e., alignments, which
are not observed in training).
In our setting, we will assume that each unary
factor is specified by a weighted finite-state au-
tomaton (WFSA) whose weights fall in the semir-
ing (R
?0
,+,?). Thus the score F
3
(. . . , V
2SA
=
x, . . .) is the total weight of all paths in the F
3
?s
WFSA that accept the string x ? ?
?
2SA
. Each
path?s weight is the product of its component arcs?
weights, which are non-negative.
Similarly, we assume that each binary factor is
specified by a weighted finite-state transducer
(WFST). Such a model is essentially a generaliza-
tion of stochastic edit distance (Ristad and Yian-
ilos, 1996) in which the edit probabilities can be
made sensitive to a finite summary of context.
Formally, a WFST is an automaton that resem-
bles a weighted FSA, but it nondeterministically
reads two strings x, y in parallel from left to right.
The score of (x, y) is given by the total weight of
all accepting paths in the WFST that map x to y.
For example, different paths may consider various
monotonic alignments of x with y, and we sum
over these mutually exclusive possibilities.
1
A factor might depend on k > 2 variables. This
requires a k-tape weighted finite-state machine
(WFSM), an obvious generalization where each
path reads k strings in some alignment.
2
To ensure that Z is finite in equation (1), we can
require each factor to be a ?proper? WFSM, i.e.,
its accepting paths have finite total weight (even if
the WFSM is cyclic, with infinitely many paths).
1
Each string is said to be on a different ?tape,? which has
its own ?read head,? allowing the WFSM to maintain a sep-
arate position in each string. Thus, a path in a WFST may
consume any number of characters from x before consuming
the next character from y.
2
Weighted acceptors and transducers are the cases k = 1
and k = 2, which are said to define rational languages and
rational relations.
102
3.3 Parameters
Our probability model has trainable parameters: a
vector of feature weights ? ? R. Each arc in each
WFSM has a real-valued weight that depends on ?.
Thus, tuning ? during training will change the arc
weights, hence the path weights, the factor func-
tions, and the whole probability distribution p(A).
Designing the probability model includes spec-
ifying the topology and weights of each WFSM.
Eisner (2002) explains how to specify and train
such parameterized WFSMs. Typically, the
weight of an arc is a simple sum like ?
12
+ ?
55
+
?
72
, where ?
12
is included on all arcs that share
feature 12. However, more interesting parameter-
izations arise if the WFSM is constructed by op-
erations such as transducer composition, or from a
weighted regular expression.
3.4 Power of the formalism
Factored finite-state string models (1) were orig-
inally suggested by the second author, in Kempe
et al (2004). That paper showed that even in the
unweighted case, such models could be used to en-
code relations that could not be recognized by any
k-tape FSM. We offer a more linguistic example
as a small puzzle. We invite the reader to spec-
ify a factored model (consisting of three FSTs as
in Fig. 1) that assigns positive probability to just
those triples of character strings (x, y, z) that have
the form (red ball, ball red, red), (white house,
house white, white), etc. This uses the auxiliary
variable Z to help encode a relation between X
and Y that swaps words of unbounded length. By
contrast, no FSM can accomplish such unbounded
swapping, even with 3 or more tapes.
Such extra power might be linguistically useful.
Troublingly, however, Kempe et al (2004) also
observed that the framework is powerful enough to
express computationally undecidable problems.
3
This implies that to work with arbitrary models,
we will need approximate methods.
4
Fortunately,
the graphical models community has already de-
3
Consider a simple model with two variables and two bi-
nary factors: p(V
1
, V
2
)
def
=
1
Z
? F
1
(V
1
, V
2
) ? F
2
(V
1
, V
2
). Sup-
pose F
1
is 1 or 0 according to whether its arguments are
equal. Under this model, p() < 1 iff there exists a string
x 6=  that can be transduced to itself by the unweighted
transducer F
2
. This question can be used to encode any in-
stance of Post?s Correspondence Problem, so is undecidable.
4
Notice that the simplest approximation to cure undecid-
ability would be to impose an arbitrary maximum on string
length, so that the random variables have a finite domain, just
as in most discrete graphical models.
V
F
U
?V?F ?F?U
Figure 2: Illustration of messages being passed from variable
to factor and factor to variable. Each message is represented
by a finite-state acceptor.
veloped many such methods, to deal with the com-
putational intractability (if not undecidability) of
exact inference.
4 Approximate Inference
In this paper, we focus on how belief propagation
(BP)?a simple well-known method for approxi-
mate inference in MRFs (Bishop, 2006)?can be
used in our setting. BP in its general form has
not yet been widely used in the NLP community.
5
However, it is just a generalization to arbitrary
factor graphs of the familiar forward-backward al-
gorithm (which operates only on chain-structured
factor graphs). The algorithm becomes approxi-
mate (and may not even converge) when the factor
graphs have cycles. (In that case it is more prop-
erly called ?loopy belief propagation.?)
4.1 Belief propagation
We first sketch how BP works in general. Each
variable V in the graphical model maintains a be-
lief about its value, in the form of a marginal dis-
tribution p?
V
over the possible values of V . The
final beliefs are the output of the algorithm.
Beliefs arise from messages that are sent be-
tween the variables and factors along the edges of
the factor graph. Variable V sends factor F a mes-
sage ?
V?F
, which is an (unnormalized) probabil-
ity distribution over V ?s values v, computed by
?
V?F
(v) :=
?
F
?
?N (V ),F
?
6=F
?
F
?
?V
(v) (2)
where N is the set of neighbors of V in the graph-
ical model. This message represents a consensus
of V ?s other neighboring factors concerning V ?s
value. It is how V tells F what its belief p?
V
would
be if F were absent. Informally, it communicates
to F : Here is what my value would be if it were up
to my other neighboring factors F
?
to determine.
5
Notable exceptions are Sutton et al (2004) for chunking
and tagging, Sutton and McCallum (2004) for information
extraction, Smith and Eisner (2008) for dependency parsing,
and Cromier`es and Kurohashi (2009) for alignment.
103
The factor F can then collect such incoming
messages from neighboring variables and send its
own message on to another neighbor U . Such a
message ?
F?U
suggests good values for U , in the
form of an (unnormalized) distribution over U ?s
values u, computed by
?
F?U
(u) :=
?
A s.t.A[U ]=u
F (A)
?
U
?
?N (F ),U
?
6=U
?
U
?
?F
(A[U
?
])
(3)
where A is an assignment to all variables, and
A[U ] is the value of variable U in that assign-
ment. This message represents F ?s prediction of
U ?s value based on its other neighboring variables
U
?
. Informally, via this message, F tells U : Here
is what I would like your value to be, based on
the messages that my other neighboring variables
have sent me about their values, and how I would
prefer you to relate to them.
Thus, each edge of the factor graph maintains
two messages ?
V?F
, ?
F?V
. All messages are
updated repeatedly, in some order, using the two
equations above, until some stopping criterion is
reached.
6
The beliefs are then computed:
p?
V
(v)
def
=
?
F?N (V )
?
F?V
(v) (4)
If variable V is observed, then the right-hand
sides of equations (2) and (4) are modified to tell
V that it must have the observed value v. This is
done by multiplying in an extra message ?
obs?V
that puts probability 1 on v
7
and 0 on other val-
ues. That affects other messages and beliefs. The
final belief at each variable estimates its posterior
marginal under the MRF (1), given all observa-
tions.
4.2 Finite-state messages in BP
Both ?
V?F
and ?
F?V
are unnormalized distribu-
tions over the possible values of V ?in our case,
strings. A distribution over strings is naturally
represented by a WFSA. Thus, belief propagation
translates to our setting as follows:
? Each message is a WFSA.
? Messages are typically initialized to a one-state
WFSA that accepts all strings in ?
?
, each with
6
Preferably when the beliefs converge to some fixed point
(a local minimum of the Bethe free energy). However, con-
vergence is not guaranteed.
7
More generally, on all possible observed variables.
weight 1.
8
? Taking a pointwise product of messages to V in
equation (2) corresponds to WFSA intersection.
? If F in equation (3) is binary,
9
then there is only
one U
?
. Then the outgoing message ?
F?U
, a
WFSA, is computed as domain(F ? ?
U
?
?F
).
Here ? composes the factor WFST with the in-
coming message WFSA, yielding a WFST that
gives a joint distribution over (U,U
?
). The
domain operator projects this WFST onto the U
side to obtain a WFSA, which corresponds to
marginalizing to obtain a distribution over U .
? In general, F is a k-tape WFSM. Equation (3)
?composes? k ? 1 of its tapes with k ? 1 in-
coming messages ?
U
?
?F
, to construct a joint
distribution over the k variables in N (F ), then
projects onto the k
th
tape to marginalize over the
k?1 U
?
variables and get a distribution over U .
All this can be accomplished by theWFSM gen-
eralized composition operator  (Kempe et al,
2004).
After projecting, it is desirable to determinize
the WFSA. Otherwise, the summation in (3) is
only implicit?the summands remain as distinct
paths in theWFSA
10
?and thus theWFSAs would
get larger and larger as BP proceeds. Unfortu-
nately, determinizing a WFSA still does not guar-
antee a small result. In fact it can lead to expo-
nential blowup, or even infinite blowup.
11
Thus,
in practice we recommend against determinizing
the messages, which may be inherently complex.
To shrink a message, it is safer to approximate it
with a small deterministic WFSA, as discussed in
the next section.
4.3 Approximation of messages
In our domain, it is possible for the finite-state
messages to grow unboundedly in size as they flow
around a cycle. After all, our messages are not
just multinomial distributions over a fixed finite
8
This is an (improper) uniform distribution over ?
?
. Al-
though is not a proper WFSA (see section 3.2), there is an
upper bound on the weights it assigns to strings. That guar-
antees that all the messages and beliefs computed by (2)?(4)
will be proper FSMs, provided that all the factors are proper
WFSMs.
9
If it is unary, (3) trivially reduces to ?
F?U
= F .
10
The usual implementation of projection does not change
the topology of the WFST, but only deletes the U
?
part of its
arc labels. Thus, multiple paths that accept the same value of
U remain distinct according to the distinct values of U
?
that
they were paired with before projection.
11
If there is no deterministic equivalent (Mohri, 1997).
104
set. They are distributions over the infinite set ?
?
.
A WFSA represents this in finite space, but more
complex distributions require bigger WFSAs, with
more distinct states and arc weights.
Facing the same problem for distributions over
the infinite set R, Sudderth et al (2002) simplified
each message ?
V?F
, approximating a complex
Gaussian mixture by using fewer components.
We could act similarly, variationally approxi-
mating a large WFSA P with a smaller one Q.
Choose a family of message approximations (such
as bigram models) by specifying the topology for
a (small) deterministic WFSA Q. Then choose
Q?s edge weights to minimize the KL divergence
KL(P ?Q). This can be done in closed form.
12
Another possible procedure?used in the ex-
periments of this paper?approximates ?
V?F
by
pruning it back to a finite set of most plausible
strings.
13
Equation (2) requests an intersection
of several WFSAs, e.g., ?
F
1
?V
? ?
F
2
?V
? ? ? ? .
List all strings that appear on any of the 1000-
best paths in any of these WFSAs, removing du-
plicates. Let
?
Q be a uniform distribution over this
combined list of plausible strings, represented as
a determinized, minimized, acyclic WFSA. Now
approximate the intersection of equation (2) as
((
?
Q ? ?
F
1
?V
) ? ?
F
2
?V
) ? ? ? ? . This is efficient
to compute and has the same topology as
?
Q.
5 Training the Model Parameters
Any standard training method for MRFs will
transfer naturally to our setting. In all cases we
draw on Eisner (2002), who showed how to train
the parameters ? of a single WFST, F , to (locally)
maximize the joint or conditional probability of
fully or partially observed training data. This in-
volves computing the gradient of that likelihood
function with respect to ?.
14
12
See Li et al (2009, footnote 9) for a sketch of the con-
struction, which finds locally normalized edge weights. Or
if Q is large but parameterized by some compact parameter
vector ?, so we are only allowed to control its edge weights
via ?, then Li and Eisner (2009, section 6) explain how to
minimize KL(P ?Q) by gradient descent. In both cases Q
must be deterministic.
We remark that if a factor F were specified by a syn-
chronous grammar rather than a WFSM, then its outgoing
messages would be weighted context-free languages. Exact
intersection of these is undecidable, but they too can be ap-
proximated variationally by WFSAs, with the same methods.
13
We are also considering other ways of adaptively choos-
ing the topology of WFSA approximations at runtime, partic-
ularly in conjunction with expectation propagation.
14
The likelihood is usually non-convex; even when the
two strings are observed (supervised training), their accepting
We must generalize this to train a product of
WFSMs. Typically, training data for an MRF (1)
consists of some fully or partially observed IID
samples of the joint distribution p(V
1
, . . . V
n
). It
is well-known how to tune an MRF?s parameters ?
by stochastic gradient descent to locally maximize
the probability of this training set, even though
both the probability and its gradient are in general
intractable to compute in an MRF. The gradient is
a sum of quantities, one for each factor F
j
. While
the summand for F
j
cannot be computed exactly,
it can be estimated using the BP messages to F
j
.
Roughly speaking, the gradient for F
j
is computed
much as in supervised training (see above), but
treating any message ?
V
i
?F
j
as an uncertain ob-
servation of V
i
?a form of noisy supervision.
15
Our concerns about training are the same as
for any MRF. First of all, BP is approximate.
Kulesza and Pereira (2008) warn that its estimates
of the gradient can be misleading. Second, semi-
supervised training (which we will attempt below)
is always difficult and prone to local optima. As
in EM, a small number of supervised examples for
some variable may be drowned out by many nois-
ily reconstructed examples.
Faster and potentially more stable approaches
include the piecewise training methods of Sut-
ton and McCallum (2008), which train the factors
independently or in small groups. In the semi-
supervised case, each factor can be trained on only
the supervised forms available for it. It might be
useful to reweight the trained factors (cf. Smith et
al. (2005)), or train the factors consecutively (cf.
Fahlman and Lebiere (1990)), in a way that mini-
mizes the loss of BP decoding on held-out data.
6 Comparison With Other Approaches
6.1 Multi-tape WFSMs
In principle, one could use a 100-tape WFSM to
jointly model the 100 distinct forms of a typical
Polish verb. In other words, the WFSM would de-
scribe the distribution of a random variable
~
V =
?V
1
, . . . , V
100
?, where each V
i
is a string. One
would train the parameters of the WFSM on a
sample of
~
V , each sample being a fully or partially
observed paradigm for some Polish verb. The re-
sulting distribution could be used to infer missing
forms for these or other verbs.
path through the WFST may be ambiguous and unobserved.
15
See Bishop (2006), or consult Smith and Eisner (2008)
for notation close to that of this paper.
105
As a simple example, either a morphological
generator or a morphological analyzer might need
the probability that krzycza?oby is the neuter third-
person singular conditional imperfective of krzy-
cze?c, despite never having observed it in training.
The model determines this probability based on
other observed and hypothesized forms of krzy-
cze?c, using its knowledge of how neuter third-
person singular conditional imperfectives are re-
lated to these other forms in other verbs.
Unfortunately, such a 100-tape WFSM would
be huge, with an astronomical number of arcs
(each representing a possible 100-way edit opera-
tion). Our approach is to factor the problem into a
number of (e.g.) pairwise relationships among the
verb forms. Using a factored distribution has sev-
eral benefits over the k-tape WFSM: (1) a smaller
representation in memory, (2) a small number
of parameters to learn, (3) efficient approximate
computation that takes advantage of the factored
structure, (4) the ability to reuse WFSAs and WF-
STs previously developed for smaller problems,
(5) additional modeling power.
6.2 Simpler graphical models on strings
Some previous researchers have used factored
joint models of several strings. To our knowledge,
they have all chosen acyclic, directed graphical
models. The acyclicity meant that exact inference
was at least possible for them, if not necessarily ef-
ficient. The factors in these past models have been
WFSTs (though typically simpler than the ones we
will use).
Many papers have used cascades of probabilis-
tic finite-state transducers. Such a cascade may
be regarded as a directed graphical model with a
linear-chain structure. Pereira and Riley (1997)
built a speech recognizer in this way, relating
acoustic to phonetic to lexical strings. Simi-
larly, Knight and Graehl (1997) presented a gen-
erative cascade using 4 variables and 5 factors:
p(w, e, j, k, o)
def
= p(w) ?p(e | w) ?p(j | e) ?p(k | j)
?p(o | k) where e is an English word sequence, w
its pronunciation, j a Japanese version of the pro-
nunciation, k a katakana rendering of the Japanese
pronunciation, and o an OCR-corrupted version of
the katakana. Knight and Graehl used finite-state
operations to perform inference at test time, ob-
serving o and recovering the most likely w, while
marginalizing out e, j, and k.
Bouchard-C?ot?e et al (2009) reconstructed an-
cient word forms given modern equivalents. They
used a directed graphical model, whose tree struc-
ture reflected the evolutionary development of the
modern languages, and which included latent vari-
ables for historical intermediate forms that were
never observed in training data. They used Gibbs
sampling rather than an exact solution (possible on
trees) or a variational approximation (like our BP).
Our work seeks to be general in terms of the
graphical model structures used, as well as effi-
cient through the use of BP with approximate mes-
sages. We also seek to avoid local normalization,
using a globally normalized model.
16
6.3 Unbounded objects in graphical models
We distinguish our work from ?dynamic? graph-
ical models such as Dynamic Bayesian Networks
and Conditional Random Fields, where the string
brechen would be represented by creating 7 letter-
valued variables. Those methods can represent
strings (or paths) of any length?but the length for
each training or test string must be specified in ad-
vance, not inferred. Furthermore, it is awkward
and costly to model unknown alignments, since
the variables are position-specific, and any posi-
tion in brechen could in principle align with any
position in brichst. WFSTs are a much more natu-
ral and flexible model of string pairs.
We also distinguish our work from current non-
parametric Bayesian models, which sometimes
generate unbounded strings, trees, or grammars. If
they generate two unbounded objects, they model
their relationship by a single synchronous genera-
tion process (akin to Section 6.1), rather than by
a globally normalized product of overlapping fac-
tors.
7 Experiments
To study our approach, we conducted initial ex-
periments that reconstruct missing word forms in
morphological paradigms. In inflectional mor-
phology, each uninflected verb form (lemma) is
associated with a vector of forms that are inflected
for tense, person, number, etc. Some inflected
forms may be observed frequently in natural text,
others rarely. Two variables that are usually pre-
dictable from each other may or may not keep this
relationship in the case of an irregular verb.
16
Although we do normalize locally during piecewise
training (see section 7.3).
106
(a) # paradigms 9,393
(b) # finite forms per paradigm 9
(c) # hidden finite forms per paradigm (avg.) 8.3
(d) # paradigms with some finite form(s) observed 2,176
(e) In (d), # of finite forms observed (avg.) 3.4
Table 1: Statistics of our training data.
Our task is to reconstruct (generate) specific un-
observed morphological forms in a paradigm by
learning from observed ones. This is a particu-
larly interesting semisupervised scenario, because
different subsets of the variables are observed on
different examples.
7.1 Experimental data
We used orthographic rather than phonological
forms. We extracted morphological paradigms for
all 9393 German verbs in the CELEX morpholog-
ical database. Each paradigm lists 5 present-tense
and 4 past-tense indicative forms, as well as the
verb?s lemma, for a total of 10 string-valued vari-
ables.
17
In each paradigm, we removed, or hid,
verb forms that occur only rarely in natural text,
i.e, verb forms with a small frequency figure pro-
vided by CELEX.
18
All paradigms other than sein
(?to be?) were now incompletely observed. Table 1
gives some statistics.
7.2 Model factors and parameters
Our current MRF uses only binary factors. Each
factor is aWFST that is trained to relate 2 of the 10
variables (morphological forms). Each WFST can
score an aligned pair using a log-linear model that
counts features in a sliding 3-character window.
To score an unaligned pair, it sums over all pos-
sible alignments. Specifically, our WFST topol-
ogy and parameterization follow the state-of-the-
art approach to supervised morphology in Dreyer
et al (2008), although we dropped some of their
features to speed up these early experiments.
19
We
17
Some pairs of forms are always identical in German,
hence are treated as a single form by CELEX. We likewise
use a single variable?these are the ?1,3? variables in Fig. 3.
Occasionally a form is listed as UNKNOWN. We neither
train nor evaluate on such forms, although the model will still
predict them.
18
The frequency figure for each word form is based on
counts in the Mannheim News corpus. We hide forms with
frequency < 10.
19
We dropped their latent classes and regions as well as
features that detected which characters were orthographic
vowels. Also, we retained their ?target language model fea-
tures? only in the baseline ?U? model, since elsewhere they
implemented and manipulated all WFSMs using
the OpenFST library (Allauzen et al, 2007).
7.3 Training in the experiments
We trained ? on the incompletely observed
paradigms. As suggested in section 5, we used
a variant of piecewise pseudolikelihood training
(Sutton and McCallum, 2008). Suppose there is
a binary factor F attached to forms U and V . For
any value of ?, we can define p
UV
(U | V ) from
the tiny MRF consisting only of U , V , and F .
We can therefore compute the goodness L
UV
def
=
log p
UV
(u
i
| v
i
)+log
V U
(v
i
| u
i
),
20
summed over
all observed (U, V ) pairs in training data. We at-
tempted to tune ? to maximize the total L
UV
over
all U, V pairs,
21
regularized by subtracting ||?||
2
.
Note that different factors thus enjoyed different
amounts of observed training data, but training
was fully supervised (except for the unobserved
alignments between u
i
and v
i
).
7.4 Inference in the experiments
At test time, we are given each lemma (e.g.
brechen) and all its observed (frequent) inflected
forms (e.g., brachen, bricht,. . . ), and are asked to
predict the remaining (rarer) forms (e.g., breche,
brichst, . . . ).
We run approximate joint inference using be-
lief propagation.
22
We extract our output from the
final beliefs: for each unseen variable V , we pre-
seemed to hurt in our current training setup.
We followed Dreyer et al (2008) in slightly pruning the
space of possible alignments. We compensated by replacing
their WFST, F , with the union F ? 10
?12
(0.999? ? ?)
?
.
This ensured that the factor could still map any string to any
other string (though perhaps with very low weight), guaran-
teeing that the intersection at the end of section 4.3 would be
non-empty.
20
The second term is omitted if V is the lemma. We do
not train the model to predict the lemma since it is always
observed in test data.
21
Unfortunately, just before press time we discovered that
this was not quite what we had done. A shortcut in our im-
plementation trained p
UV
(U | V ) and p
V U
(V | U) sepa-
rately. This let them make different use of the (unobserved)
alignments?so that even if each individually liked the pair
(u, v), they might not have been able to agree on the same
accepting path for it at test time. This could have slightly
harmed our joint inference results, though not our baselines.
22
To derive the update order for message passing, we take
an arbitrary spanning tree over the factor graph, and let O be
a list of all factors and variables that is topologically sorted
according to the spanning tree, with the leaves of the tree
coming first. We then discard the spanning tree. A single it-
eration visits all factors and variables in order of O, updating
each one?s messages to later variables and factors, and then
visits all factors and variables in reverse order, updating each
one?s messages to earlier variables and factors.
107
dict its value to be argmax
v
p?
V
(v). This predic-
tion considers the values of all other unseen vari-
ables but sums over their possibilities. This is the
Bayes-optimal decoder for our scoring function,
since that function reports the fraction of individ-
ual forms that were predicted perfectly.
23
7.5 Model selection of MRF topology
It is hard to know a priori what the causal relation-
ships might be in a morphological paradigm. In
principle, one would like to automatically choose
which factors to have in the MRF. Or one could
start with many factors, but use methods such as
those suggested in section 5 to learn that certain
less useful factors should be left weak to avoid
confusing loopy BP.
For our present experiments, we simply com-
pared several fixed model topologies (Fig. 3).
These were variously unconnected (U), chain
graphs (C1,. . . , C4), trees (T1, T2), or loopy
graphs (L1,. . . , L4). We used several factor graphs
that differ only by one or two added factors and
compared the results. The graphs were designed
by hand; they connect some forms with similar
morphological properties more or less densely.
We trained different models using the observed
forms in the 9393 paradigms as training data. The
first 100 paradigms were then used as develop-
ment data for model selection:
24
we were given
the answers to their hidden forms, enabling us to
compare the models. The best model was then
evaluated on the 9293 remaining paradigms.
7.6 Development data results
The models are compared on development data
in Table 2. Among the factor graphs we evalu-
ated, we find that L4 (see Fig. 3) performs best
overall (whole-word accuracy 82.1). Note that the
unconnected graph U does not perform very well
(69.0), but using factor graphs with more connect-
ing factors generally helps overall accuracy (see
C1?C3). Note, however, that in some cases the ad-
ditional structure hurts: The chain model C4 and
the loopy model L1 perform relatively badly. The
23
If we instead wished to maximize the fraction of entire
paradigms that were predicted perfectly, then we would have
approximated full MAP decoding over the paradigm (Viterbi
decoding) by using max-product BP. Other loss functions
(e.g., edit distance) would motivate other decoding methods.
24
Using these paradigms was simply a quick way to avoid
model selection by cross-validation. If data were really as
sparse as our training setup pretends (see Table 2), then 100
complete paradigms would be too valuable to squander as
mere development data.
(U)
1
Pres Past
S
i
n
g
u
l
a
r
P
l
u
r
a
l
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
(C1) (C2)
(C3)
1
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
(C4)
1
2
3
1,3
2
1,3
2
2
1,3
(T1)
1
2
3
1,3
2
1,3
2
2
1,3
(T2) (L1)
1
2
3
1,3
2
1,3
2
2
1,3
(L2)
1
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
(L3)
Pres Past Pres Past Pres Past Pres Past
S
i
n
g
u
l
a
r
P
l
u
r
a
l
1
2
3
1,3
2
1,3
2
2
1,3
(L4)
Figure 3: The graphs that we evaluate on development data.
The nodes represent morphological forms, e.g. the first node
in the left of each graph represents the first person singular
present. Each variable is also connected to the lemma (not
shown). See results in Table 2.
reason for such a performance degradation is that
undertrained factors were used: The factors relat-
ing second-person to second-person forms, for ex-
ample, are trained from only 8 available examples.
Non-loopy models always converge (exactly) in
one iteration (see footnote 22). But even our loopy
models appeared to converge in accuracy within
two iterations. Only L3 and L4 required the sec-
ond iteration, which made tiny improvements.
7.7 Test data results
Based on the development results, we selected
model L4 and tested on the remaining 9293
paradigms.
We regard the unconnected model U as a base-
line to improve upon. We also tried a rather differ-
ent baseline as in (Dreyer et al, 2008). We trained
the machine translation toolkit Moses (Koehn et
al., 2007) to translate groups of letters rather than
groups of words (?phrases?). For each form f
to be predicted, we trained a Moses model on
all supervised form pairs (l, f) available in the
data, to learn a prediction for the form given the
lemma l. The M,3 condition restricted Moses use
?phrases? no longer than 3 letters, comparable to
our own trigram-based factors (see section 7.2).
M,15 could use up to 15 letters.
Again, our novel L4 model far outperformed
the others overall. Breaking the results down by
form, we find that this advantage mainly comes
from the 3 forms with the fewest observed train-
ing examples (Table 3, first 3 rows). The M and
U models are barely able to predict these forms at
all from the lemma, but L4 can predict them bet-
108
Unconn. Chains Trees Loops
U C1 C2 C3 C4 T1 T2 L1 L2 L3 L4
69.0 72.9 73.4 74.8 65.2 78.1 78.7 62.3 79.6 78.9 82.1
Table 2: Whole-word accuracies of the different models in reconstructing the missing forms in morphological paradigms, here
on 100 verbs (development data). The names refer to the graphs in Fig. 3. We selected L4 as final model (Table 3).
Form # obs. M,3 M,15 U L4
2.Sg.Pa. 4 0.0 0.2 0.8 69.7
2.Pl.Pa. 9 0.9 1.1 1.4 45.6
2.Sg.Pr. 166 49.4 62.6 74.7 90.5
1.Sg.Pr. 285 99.6 98.8 99.3 97.2
1,3.Pl.Pa. 673 46.5 78.3 75.0 75.6
1,3.Sg.Pa. 1124 65.0 88.8 84.0 74.8
2.Pl.Pr. 1274 98.3 99.2 99.0 96.4
3.Sg.Pr. 1410 91.0 95.9 95.2 88.2
1,3.Pl.Pr. 1688 99.8 98.9 99.8 98.0
All 6633 59.2 67.3 68.0 81.2
Table 3: Whole-word accuracies on the missing forms from
9293 test paradigms. The Moses baselines and our un-
connected model (U) predict each form separately from the
lemma, which is always observed. L4 uses all observations
jointly, running belief propagation for decoding. Moses,15
memorizes phrases of length up to 15, all other models use
max length 3. The table is sorted by the column ?# obs.?,
which reports the numbers of observations for a given form.
ter by exploiting other observed or latent forms.
By contrast, well-trained forms were already easy
enough for the M and U models that L4 had little
new to offer and in fact suffered from its approxi-
mate training and/or inference.
Leaving aside the comparisons, it was useful to
confirm that loopy BP could be used in this set-
ting at all. 8014 of the 9293 test paradigms had
? 2 observed forms (in addition to the lemma)
but ? 7 missing forms. One might have expected
that loopy BP would have failed to converge, or
converged to the wrong thing. Nonetheless, it
achieved quite respectable success at exactly pre-
dicting various inflected forms.
For the curious, Table 4 shows accuracies
grouped by different categories of paradigms,
where the category is determined by the number
of missing forms to predict. Most paradigms fall
in the category where 7 to 9 forms are missing, so
the accuracies in that line are similar to the overall
accuracies in Table 3.
8 Conclusions
We have proposed that one can jointly model sev-
eral multiple strings by using Markov Random
Fields. We described this formally as an undi-
# missing # paradig. M,3 M,15 U L4
1?3 205 20.3 20.8 26.8 74.4
4?6 1037 44.2 50.5 52.7 82.8
7?9 8014 60.6 68.8 69.4 81.1
Table 4: Accuracy on test data, reported separately for
paradigms in which 1?3, 4?6, or 7?9 forms are missing.
Missing words have CELEX frequency count< 10; these are
the ones to predict. (The numbers in col. 2 add up to 9256,
not 9293, since some paradigms are incomplete in CELEX to
begin with, with no forms to be removed or evaluated.)
rected graphical model with string-valued vari-
ables and whose factors (potential functions) are
defined by weighted finite-state transducers. Each
factor evaluates some subset of the strings.
Approximate inference can be done by loopy
belief propagation. The messages take the form
of weighted finite-state acceptors, and are con-
structed by standard operations. We explained
why the messages might become large, and gave
methods for approximating them with smaller
messages. We also discussed training methods.
We presented some pilot experiments on the
task of jointly predicting multiple missing verb
forms in morphological paradigms. The factors
were simplified versions of statistical finite-state
models for supervised morphology. Our MRF
for this task might be used not only to conjugate
verbs (e.g., in MT), but to guide further learning
of morphology?either active learning from a hu-
man or semi-supervised learning from the distri-
butional properties of a raw text corpus.
Our modeling approach is potentially applicable
to a wide range of other tasks, including translit-
eration, phonology, cognate modeling, multiple-
sequence alignment and system combination.
Our work ties into a broader vision of using al-
gorithms like belief propagation to coordinate the
work of several NLP models and algorithms. Each
individual factor considers some portion of a joint
problem, using classical statistical NLP methods
(weighted grammars, transducers, dynamic pro-
gramming). The factors coordinate their work by
passing marginal probabilities. Smith and Eisner
(2008) reported complementary work in this vein.
109
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proc. of CIAA, volume 4783 of Lecture
Notes in Computer Science, pages 11?23.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Alexandre Bouchard-C?ot?e, Thomas L. Griffiths, and
Dan Klein. 2009. Improved reconstruction of pro-
tolanguage word forms. In Proc. of HLT-NAACL,
pages 65?73, Boulder, Colorado, June. Association
for Computational Linguistics.
Fabien Cromier`es and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proc. of EACL,
pages 166?174, Athens, Greece, March. Association
for Computational Linguistics.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Proc. of EMNLP, Hon-
olulu, Hawaii, October.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proc. of ACL,
pages 1?8, Philadelphia, July.
Scott E. Fahlman and Christian Lebiere. 1990. The
cascade-correlation learning architecture. Technical
Report CMU-CS-90-100, School of Computer Sci-
ence, Carnegie Mellon University.
Andr?e Kempe, Jean-Marc Champarnaud, and Jason
Eisner. 2004. A note on join and auto-intersection
of n-ary rational relations. In Loek Cleophas and
Bruce Watson, editors, Proceedings of the Eind-
hoven FASTAR Days (Computer Science Techni-
cal Report 04-40). Department of Mathematics and
Computer Science, Technische Universiteit Eind-
hoven, Netherlands.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proc. of ACL, pages 128?135.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Kulesza and Fernando Pereira. 2008. Structured
learning with approximate inference. In Proc. of
NIPS.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proc. of EMNLP.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2).
Fernando C. N. Pereira and Michael Riley. 1997.
Speech recognition by composition of weighted fi-
nite automata. In Emmanuel Roche and Yves
Schabes, editors, Finite-State Language Processing.
MIT Press, Cambridge, MA.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learn-
ing string edit distance. Technical Report CS-TR-
532-96, Princeton University, Department of Com-
puter Science, October.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional
random fields. In Proc. of ACL, pages 18?25, June.
Erik B. Sudderth, Alexander T. Ihler, Er T. Ihler,
William T. Freeman, and Alan S. Willsky. 2002.
Nonparametric belief propagation. In Proc. of
CVPR, pages 605?612.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. In ICML Workshop on Sta-
tistical Relational Learning and Its Connections to
Other Fields.
Charles Sutton and Andrew McCallum. 2008. Piece-
wise training for structured prediction. Machine
Learning. In submission.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proc. of
ICML.
110
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 81?84,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Machine Translation System Combination
using ITG-based Alignments?
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, Markus Dreyer
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218
{damianos,eisner,khudanpur,dreyer}@jhu.edu
Abstract
Given several systems? automatic translations
of the same sentence, we show how to com-
bine them into a confusion network, whose
various paths represent composite translations
that could be considered in a subsequent
rescoring step. We build our confusion net-
works using the method of Rosti et al (2007),
but, instead of forming alignments using the
tercom script (Snover et al, 2006), we create
alignments that minimize invWER (Leusch
et al, 2003), a form of edit distance that
permits properly nested block movements of
substrings. Oracle experiments with Chinese
newswire and weblog translations show that
our confusion networks contain paths which
are significantly better (in terms of BLEU and
TER) than those in tercom-based confusion
networks.
1 Introduction
Large improvements in machine translation (MT)
may result from combining different approaches
to MT with mutually complementary strengths.
System-level combination of translation outputs is
a promising path towards such improvements. Yet
there are some significant hurdles in this path. One
must somehow align the multiple outputs?to iden-
tify where different hypotheses reinforce each other
and where they offer alternatives. One must then
?This work was partially supported by the DARPA GALE
program (Contract No HR0011-06-2-0001). Also, we would
like to thank the IBM Rosetta team for the availability of several
MT system outputs.
use this alignment to hypothesize a set of new, com-
posite translations, and select the best composite hy-
pothesis from this set. The alignment step is difficult
because different MT approaches usually reorder the
translated words differently. Training the selection
step is difficult because identifying the best hypothe-
sis (relative to a known reference translation) means
scoring all the composite hypotheses, of which there
may be exponentially many.
Most MT combination methods do create an ex-
ponentially large hypothesis set, representing it as a
confusion network of strings in the target language
(e.g., English). (A confusion network is a lattice
where every node is on every path; i.e., each time
step presents an independent choice among several
phrases. Note that our contributions in this paper
could be applied to arbitrary lattice topologies.) For
example, Bangalore et al (2001) show how to build
a confusion network following a multistring align-
ment procedure of several MT outputs. The proce-
dure (used primarily in biology, (Thompson et al,
1994)) yields monotone alignments that minimize
the number of insertions, deletions, and substitu-
tions. Unfortunately, monotone alignments are often
poor, since machine translations (particularly from
different models) can vary significantly in their word
order. Thus, when Matusov et al (2006) use this
procedure, they deterministically reorder each trans-
lation prior to the monotone alignment.
The procedure described by Rosti et al (2007)
has been shown to yield significant improvements in
translation quality, and uses an estimate of Trans-
lation Error Rate (TER) to guide the alignment.
(TER is defined as the minimum number of inser-
81
tions, deletions, substitutions and block shifts be-
tween two strings.) A remarkable feature of that
procedure is that it performs the alignment of the
output translations (i) without any knowledge of the
translation model used to generate the translations,
and (ii) without any knowledge of how the target
words in each translation align back to the source
words. In fact, it only requires a procedure for cre-
ating pairwise alignments of translations that allow
appropriate re-orderings. For this, Rosti et al (2007)
use the tercom script (Snover et al, 2006), which
uses a number of heuristics (as well as dynamic pro-
gramming) for finding a sequence of edits (inser-
tions, deletions, substitutions and block shifts) that
convert an input string to another. In this paper, we
show that one can build better confusion networks
(in terms of the best translation possible from the
confusion network) when the pairwise alignments
are computed not by tercom, which approximately
minimizes TER, but instead by an exact minimiza-
tion of invWER (Leusch et al, 2003), which is a re-
stricted version of TER that permits only properly
nested sets of block shifts, and can be computed in
polynomial time.
The paper is organized as follows: a summary of
TER, tercom, and invWER, is presented in Section
2. The system combination procedure is summa-
rized in Section 3, while experimental (oracle) re-
sults are presented in Section 4. Conclusions are
given in Section 5.
2 Comparing tercom and invWER
The tercom script was created mainly in order to
measure translation quality based on TER. As is
proved by Shapira and Storer (2002), computation
of TER is an NP-complete problem. For this reason,
tercom uses some heuristics in order to compute an
approximation to TER in polynomial time. In the
rest of the paper, we will denote this approximation
as tercomTER, to distinguish it from (the intractable)
TER. The block shifts which are allowed in tercom
have to adhere to the following constraints: (i) A
block that has an exact match cannot be moved, and
(ii) for a block to be moved, it should have an exact
match in its new position. However, this sometimes
leads to counter-intuitive sequences of edits; for in-
stance, for the sentence pair
?thomas jefferson says eat your vegetables?
?eat your cereal thomas edison says?,
tercom finds an edit sequence of cost 5, instead of
the optimum 3. Furthermore, the block selection is
done in a greedy manner, and the final outcome is
dependent on the shift order, even when the above
constraints are imposed.
An alternative to tercom, considered in this pa-
per, is to use the Inversion Transduction Grammar
(ITG) formalism (Wu, 1997) which allows one to
view the problem of alignment as a problem of bilin-
gual parsing. Specifically, ITGs can be used to find
the optimal edit sequence under the restriction that
block moves must be properly nested, like paren-
theses. That is, if an edit sequence swaps adjacent
substrings A and B of the original string, then any
other block move that affects A (or B) must stay
completely within A (or B). An edit sequence with
this restriction corresponds to a synchronous parse
tree under a simple ITG that has one nonterminal
and whose terminal symbols allow insertion, dele-
tion, and substitution.
The minimum-cost ITG tree can be found by dy-
namic programming. This leads to invWER (Leusch
et al, 2003), which is defined as the minimum num-
ber of edits (insertions, deletions, substitutions and
block shifts allowed by the ITG) needed to convert
one string to another. In this paper, the minimum-
invWER alignments are used for generating confu-
sion networks. The alignments are found with a 11-
rule Dyna program (Dyna is an environment that fa-
cilitates the development of dynamic programs?see
(Eisner et al, 2005) for more details). This pro-
gram was further sped up (by about a factor of 2)
with an A? search heuristic computed by additional
code. Specifically, our admissible outside heuris-
tic for aligning two substrings estimated the cost of
aligning the words outside those substrings as if re-
ordering those words were free. This was compli-
cated somewhat by type/token issues and by the fact
that we were aligning (possibly weighted) lattices.
Moreover, the same Dyna program was used for the
computation of the minimum invWER path in these
confusion networks (oracle path), without having to
invoke tercom numerous times to compute the best
sentence in an N -best list.
The two competing alignment procedures were
82
Lang. / Genre tercomTER invWER
Arabic NW 15.1% 14.9%
Arabic WB 26.0% 25.8%
Chinese NW 26.1% 25.6%
Chinese WB 30.9% 30.4%
Table 1: Comparison of average per-document ter-
comTER with invWER on the EVAL07 GALE Newswire
(?NW?) and Weblogs (?WB?) data sets.
used to estimate the TER between machine transla-
tion system outputs and reference translations. Ta-
ble 1 shows the TER estimates using tercom and
invWER. These were computed on the translations
submitted by a system to NIST for the GALE eval-
uation in June 2007. The references used are the
post-edited translations for that system (i.e., these
are ?HTER? approximations). As can be seen from
the table, in all language and genre conditions, in-
vWER gives a better approximation to TER than
tercomTER. In fact, out of the roughly 2000 total
segments in all languages/genres, tercomTER gives
a lower number of edits in only 8 cases! This is a
clear indication that ITGs can explore the space of
string permutations more effectively than tercom.
3 The System Combination Approach
ITG-based alignments and tercom-based alignments
were also compared in oracle experiments involving
confusion networks created through the algorithm of
Rosti et al (2007). The algorithm entails the follow-
ing steps:
? Computation of all pairwise alignments be-
tween system hypotheses (either using ITGs or
tercom); for each pair, one of the hypotheses
plays the role of the ?reference?.
? Selection of a system output as the ?skele-
ton? of the confusion network, whose words
are used as anchors for aligning all other ma-
chine translation outputs together. Each arc has
a translation output word as its label, with the
special token ?NULL? used to denote an inser-
tion/deletion between the skeleton and another
system output.
? Multiple consecutive words which are inserted
relative to the skeleton form a phrase that gets
Genre CNs with tercom CNs with ITG
NW 50.1% (27.7%) 48.8% (28.3%)
WB 51.0% (25.5%) 50.5% (26.0%)
Table 2: TercomTERs of invWER-oracles and (in paren-
theses) oracle BLEU scores of confusion networks gen-
erated with tercom and ITG alignments. The best results
per row are shown in bold.
aligned with an epsilon arc of the confusion
network.
? Setting the weight of each arc equal to the
negative log (posterior) probability of its la-
bel; this probability is proportional to the num-
ber of systems which output the word that gets
aligned in that location. Note that the algo-
rithm of Rosti et al (2007) used N -best lists in
the combination. Instead, we used the single-
best output of each system; this was done be-
cause not all systems were providing N -best
lists, and an unbalanced inclusion would favor
some systems much more than others. Further-
more, for each genre, one of our MT systems
was significantly better than the others in terms
of word order, and it was chosen as the skele-
ton.
4 Experimental Results
Table 2 shows tercomTERs of invWER-oracles (as
computed by the aforementioned Dyna program)
and oracle BLEU scores of the confusion networks.
The confusion networks were generated using 9
MT systems applied to the Chinese GALE 2007
Dev set, which consists of roughly 550 Newswire
segments, and 650 Weblog segments. The confu-
sion networks which were generated with the ITG-
based alignments gave significantly better oracle ter-
comTERs (significance tested with a Fisher sign
test, p ? 0.02) and better oracle BLEU scores.
The BLEU oracle sentences were found using the
dynamic-programming algorithm given in Dreyer et
al. (2007) and measured using Philipp Koehn?s eval-
uation script. On the other hand, a comparison be-
tween the 1-best paths did not reveal significant dif-
ferences that would favor one approach or the other
(either in terms of tercomTER or BLEU).
83
We also tried to understand which alignment
method gives higher probability to paths ?close?
to the corresponding oracle. To do that, we com-
puted the probability that a random path from a
confusion network is within x edits from its ora-
cle. This computation was done efficiently using
finite-state-machine operations, and did not involve
any randomization. Preliminary experiments with
the invWER-oracles show that the probability of all
paths which are within x = 3 edits from the oracle
is roughly the same for ITG-based and tercom-based
confusion networks. We plan to report our findings
for a whole range of x-values in future work. Fi-
nally, a runtime comparison of the two techniques
shows that ITGs are much more computationally
intensive: on average, ITG-based alignments took
1.5 hours/sentence (owing to their O(n6) complex-
ity), while tercom-based alignments only took 0.4
sec/sentence.
5 Concluding Remarks
We compared alignments obtained using the widely
used program tercom with alignments obtained with
ITGs and we established that the ITG alignments are
superior in two ways. Specifically: (a) we showed
that invWER (computed using the ITG alignments)
gives a better approximation to TER between ma-
chine translation outputs and human references than
tercom; and (b) in an oracle system combination ex-
periment, we found that confusion networks gen-
erated with ITG alignments contain better oracles,
both in terms of tercomTER and in terms of BLEU.
Future work will include rescoring results with a
language model, as well as exploration of heuristics
(e.g., allowing only ?short? block moves) that can
reduce the ITG alignment complexity to O(n4).
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proceedings of ASRU, pages
351?354.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing reordering constraints for smt using efficient bleu
oracle computation. In Proceedings of SSST, NAACL-
HLT 2007 / AMTA Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Rochester,
New York, April. Association for Computational Lin-
guistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of HLT-
EMNLP, pages 281?290. Association for Computa-
tional Linguistics, October.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proceedings of the
Machine Translation Summit 2003, pages 240?247,
September.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proceedings of EACL, pages 33?40.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proceedings of the ACL, pages 312?
319, June.
D. Shapira and J. A. Storer. 2002. Edit distance with
move operations. In Proceedings of the 13th Annual
Symposium on Combinatorial Pattern Matching, vol-
ume 2373/2002, pages 85?98, Fukuoka, Japan, July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, Cam-
bridge, MA, August.
J. D. Thompson, D. G. Higgins, and T. J. Gibson.
1994. Clustalw: Improving the sensitivity of progres-
sive multiple sequence alignment through sequence
weighting, position-specific gap penalties and weight
matrix choice. Nucleic Acids Research, 22(22):4673?
4680.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
84
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317?326,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Better Informed Training of Latent Syntactic Features
Markus Dreyer and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street, Baltimore, MD 21218 USA
{markus,jason}@clsp.jhu.edu
Abstract
We study unsupervised methods for learn-
ing refinements of the nonterminals in
a treebank. Following Matsuzaki et al
(2005) and Prescher (2005), we may for
example split NP without supervision into
NP[0] and NP[1], which behave differently.
We first propose to learn a PCFG that adds
such features to nonterminals in such a
way that they respect patterns of linguis-
tic feature passing: each node?s nontermi-
nal features are either identical to, or inde-
pendent of, those of its parent. This lin-
guistic constraint reduces runtime and the
number of parameters to be learned. How-
ever, it did not yield improvements when
training on the Penn Treebank. An orthog-
onal strategy was more successful: to im-
prove the performance of the EM learner
by treebank preprocessing and by anneal-
ing methods that split nonterminals selec-
tively. Using these methods, we can main-
tain high parsing accuracy while dramati-
cally reducing the model size.
1 Introduction
Treebanks never contain enough information; thus
PCFGs estimated straightforwardly from the Penn
Treebank (Bies et al, 1995) work only moderately
well (Charniak, 1996). To address this problem,
researchers have used heuristics to add more infor-
mation. Eisner (1996), Charniak (1997), Collins
(1997), and many subsequent researchers1 anno-
tated every node with lexical features passed up
from its ?head child,? in order to more precisely re-
flect the node?s ?inside? contents. Charniak (1997)
and Johnson (1998) annotated each node with its
parent and grandparent nonterminals, to more pre-
cisely reflect its ?outside? context. Collins (1996)
split the sentence label S into two versions, repre-
senting sentences with and without subjects. He
1Not to mention earlier non-PCFG lexicalized statistical
parsers, notably Magerman (1995) for the Penn Treebank.
also modified the treebank to contain different la-
bels for standard and for base noun phrases. Klein
and Manning (2003) identified nonterminals that
could valuably be split into fine-grained ones us-
ing hand-written linguistic rules. Their unlexical-
ized parser combined several such heuristics with
rule markovization and reached a performance
similar to early lexicalized parsers.
In all these cases, choosing which nonterminals
to split, and how, was a matter of art. Ideally
such splits would be learned automatically from
the given treebank itself. This would be less costly
and more portable to treebanks for new domains
and languages. One might also hope that the auto-
matically learned splits would be more effective.
Matsuzaki et al (2005) introduced a model for
such learning: PCFG-LA.2 They used EM to in-
duce fine-grained versions of a given treebank?s
nonterminals and rules. We present models that
similarly learn to propagate fine-grained features
through the tree, but only in certain linguistically
motivated ways. Our models therefore allocate
a supply of free parameters differently, allow-
ing more fine-grained nonterminals but less fine-
grained control over the probabilities of rewriting
them. We also present simple methods for decid-
ing selectively (during training) which nontermi-
nals to split and how.
Section 2 describes previous work in finding
hidden information in treebanks. Section 3 de-
scribes automatically induced feature grammars.
We start by describing the PCFG-LA model, then
introduce new models that use specific agreement
patterns to propagate features through the tree.
Section 4 describes annealing-like procedures for
training latent-annotation models. Section 5 de-
scribes the motivation and results of our experi-
ments. We finish by discussing future work and
conclusions in sections 6?7.
2Probabilistic context-free grammar with latent annota-
tions.
317
Citation Observed data Hidden data
Collins (1997) Treebank tree with head child an-
notated on each nonterminal
No hidden data. Degenerate EM
case.
Lari and Young (1990) Words Parse tree
Pereira and Schabes (1992) Words and partial brackets Parse tree
Klein and Manning (2001) Part-of-speech tags Parse tree
Chiang and Bikel (2002) Treebank tree Head child on each nonterminal
Matsuzaki et al (2005) Treebank tree Integer feature on each nontermi-
nal
INHERIT model (this paper) Treebank tree and head child
heuristics
Integer feature on each nontermi-
nal
Table 1: Observed and hidden data in PCFG grammar learning.
2 Partially supervised EM learning
The parameters of a PCFG can be learned with
or without supervision. In the supervised case,
the complete tree is observed, and the rewrite rule
probabilities can be estimated directly from the
observed rule counts. In the unsupervised case,
only the words are observed, and the learning
method must induce the whole structure above
them. (See Table 1.)
In the partially supervised case we will con-
sider, some part of the tree is observed, and
the remaining information has to be induced.
Pereira and Schabes (1992) estimate PCFG pa-
rameters from partially bracketed sentences, using
the inside-outside algorithm to induce the miss-
ing brackets and the missing node labels. Some
authors define a complete tree as one that speci-
fies not only a label but also a ?head child? for
each node. Chiang and Bikel (2002) induces the
missing head-child information; Prescher (2005)
induces both the head-child information and the
latent annotations we will now discuss.
3 Feature Grammars
3.1 The PCFG-LA Model
Staying in the partially supervised paradigm, the
PCFG-LA model described in Matsuzaki et al
(2005) observe whole treebank trees, but learn
an ?annotation? on each nonterminal token?an
unspecified and uninterpreted integer that distin-
guishes otherwise identical nonterminals. Just as
Collins manually split the S nonterminal label into
S and SG for sentences with and without subjects,
Matsuzaki et al (2005) split S into S[1], S[2], . . . ,
S[L] where L is a predefined number?but they do
it automatically and systematically, and not only
for S but for every nonterminal. Their partially
supervised learning procedure observes trees that
are fully bracketed and fully labeled, except for
the integer subscript used to annotate each node.
After automatically inducing the annotations with
EM, their resulting parser performs just as well as
one learned from a treebank whose nonterminals
were manually refined through linguistic and error
analysis (Klein and Manning, 2003).
In Matsuzaki?s PCFG-LA model, rewrite rules
take the form
X[?] ? Y [?] Z[?] (1)
in the binary case, and
X[?] ? w (2)
in the lexical case. The probability of a tree con-
sisting of rules r1, r2, . . . is given by the probabil-
ity of its root symbol times the conditional prob-
abilities of the rules. The annotated tree T1 in
Fig. 1, for example, has the following probability:
P (T1) = P (ROOT ? S[2])
?P (S[2] ? NP[1] VP[3])
?P (NP[1] ?? He)
?P (VP[3] ?? loves cookies)
where, to simplify the notation, we use
P (X ? Y Z) to denote the conditional probabil-
ity P (Y Z | X) that a given node with label X
will have children Y Z.
Degrees of freedom. We will want to compare
models that have about the same size. Models with
more free parameters have an inherent advantage
on modeling copious data because of their greater
318
Figure 1: Treebank tree with annotations.
expressiveness. Models with fewer free parame-
ters are easier to train accurately on sparse data,
as well as being more efficient in space and often
in time. Our question is therefore what can be ac-
complished with a given number of parameters.
How many free parameters in a PCFG-LA
model? Such a model is created by annotating
the nonterminals of a standard PCFG (extracted
from the given treebank) with the various integers
from 1 to L. If the original, ?backbone? grammar
has R3 binary rules of the form X ? Y Z, then
the resulting PCFG-LA model has L3 ? R3 such
rules: X[1] ? Y [1] Z[1], X[1] ? Y [1] Z[2],
X[1] ? Y [2] Z[1], . . . , X[L] ? Y [L] Z[L]. Sim-
ilarly, if the backbone grammar has R2 rules of
the form X ? Y the PCFG-LA model has L2 ?
R2 such rules.3 The number of R1 terminal rules
X ? w is just multiplied by L.
The PCFG-LA has as many parameters to learn
as rules: one probability per rule. However, not
all these parameters are free, as there are L ? N
sum-to-one constraints, where N is the number of
backbone nonterminals. Thus we have
L3R3 + L
2R2 + LR1 ? LN (3)
degrees of freedom.
We note that Goodman (1997) mentioned possi-
ble ways to factor the probability 1, making inde-
pendence assumptions in order to reduce the num-
ber of parameters.
Runtime. Assuming there are no unary rule cy-
cles in the backbone grammar, bottom-up chart
parsing of a length-n sentence at test time takes
time proportional to n3L3R3 + n2L2R2 + nLR1,
by attempting to apply each rule everywhere in the
sentence. (The dominating term comes from equa-
tion (4) of Table 2: we must loop over all n3 triples
i, j, k and all R3 backbone rules X ? Y Z and all
3We use unary rules of this form (e.g. the Treebank?s S?
NP) in our reimplementation of Matsuzaki?s algorithm.
L3 triples ?, ?, ?.) As a function of n and L only,
this is O(n3L3).
At training time, to induce the annotations on
a given backbone tree with n nodes, one can run
a constrained version of this algorithm that loops
over only the n triples i, j, k that are consistent
with the given tree (and considers only the single
consistent backbone rule for each one). This takes
time O(nL3), as does the inside-outside version
we actually use to collect expected PCFG-LA rule
counts for EM training.
We now introduce a model that is smaller, and
has a lower runtime complexity, because it adheres
to specified ways of propagating features through
the tree.
3.2 Feature Passing: The INHERIT Model
Many linguistic theories assume that features get
passed from the mother node to their children or
some of their children. In many cases it is the
head child that gets passed its feature value from
its mother (e.g., Kaplan and Bresnan (1982), Pol-
lard and Sag (1994)). In some cases the feature is
passed to both the head and the non-head child, or
perhaps even to the non-head alone.
Figure 2: Features are passed to different children
at different positions in the tree.
In the example in Fig. 2, the tense feature (pres)
is always passed to the head child (underlined).
How the number feature (sg/pl) is passed depends
on the rewrite rule: S ? NP VP passes it to both
children, to enforce subject-verb agreement, while
VP ? V NP only passes it to the head child, since
the object NP is free not to agree with the verb.
A feature grammar can incorporate such pat-
terns of feature passing. We introduce additional
parameters that define the probability of passing a
feature to certain children. The head child of each
node is given deterministically by the head rules
of (Collins, 1996).
Under the INHERIT model that we propose, the
319
Model Runtime and d.f. Simplified equation for inside probabilities (ignores unary rules)
Matsuzaki
et al (2005)
test: O(n3L3)
train: O(nL3)
d.f.: L3R3 +
L2R2 +LR1?LN
BX[?](i, k) =
X
Y,?,Z,?,j
P (X[?] ? Y [?] Z[?]) (4)
?BY [?](i, j)?BZ[?](j, k)
INHERIT
model
(this paper)
test: O(n3L)
train: O(nL)
d.f.: L(R3 + R2 +
R1) + 3R3 ?N
BX[?](i, k) =
X
Y,Z,j
P (X[?] ? Y Z) (5)
?
0
B
@
P (neither | X,Y, Z) ? BY (i, j) ? BZ(j, k))
+ P (left | X,Y, Z) ? BY [?](i, j) ? BZ(j, k))
+ P (right | X,Y, Z) ? BY (i, j) ? BZ[?](j, k))
+ P (both | X,Y, Z) ? BY [?]Y (i, j) ? BZ[?](j, k))
1
C
A
BX(i, j) =
X
?
Pann(? | X)?BX[?](i, j) (6)
P (left | X,Y, Z) =
?
P (head | X,Y, Z) if Y heads X ? Y Z
P (nonhead | X,Y, Z) otherwise (7)
P (right | X,Y, Z) =
?
P (head | X,Y, Z) if Z heads X ? Y Z
P (nonhead | X,Y, Z) otherwise (8)
Table 2: Comparison of the PCFG-LA model with the INHERIT model proposed in this paper. ?d.f.?
stands for ?degrees of freedom? (i.e., free parameters). The B terms are inside probabilities; to compute
Viterbi parse probabilities instead, replace summation by maximization. Note the use of the intermediate
quantity BX(i, j) to improve runtime complexity by moving some summations out of the inner loop;
this is an instance of a ?folding transformation? (Blatz and Eisner, 2006).
Figure 3: Two passpatterns. Left: T2. The feature
is passed to the head child (underlined). Right: T3.
The feature is passed to both children.
probabilities of tree T2 in Fig. 3 are calculated as
follows, with Pann(1 | NP ) being the probability
of annotating an NP with feature 1 if it does not
inherit its parent?s feature. The VP is boldfaced to
indicate that it is the head child of this rule.
P (T2) = P (ROOT ? S[2])
?P (S[2] ? NP VP)
?P (pass to head | S ? NP VP)
?Pann(1 | NP) ? P (NP[1] ?? He)
?P (VP[2] ?? loves cookies)
Tree T3 in Fig. 3 has the following probability:
P (T3) = P (ROOT ? S[2])
?P (S[2] ? NP VP)
?P (pass to both | S ? NP VP)
?P (NP[2] ?? He)
?P (VP[2] ?? loves cookies)
In T2, the subject NP chose feature 1 or 2 indepen-
dent of its parent S, according to the distribution
Pann(? | NP). In T3, it was constrained to inherit
its parent?s feature 2.
Degrees of freedom. The INHERIT model may
be regarded as containing all the same rules
(see (1)) as the PCFG-LA model. However, these
rules? probabilities are now collectively deter-
mined by a smaller set of shared parameters.4 That
is because the distribution of the child features ?
and ? no longer depends arbitrarily on the rest of
the rule. ? is either equal to ?, or chosen indepen-
dently of everything but Y .
The model needs probabilities for L ? R3
binary-rule parameters like P (S[2] ? NP VP)
above, as well as L ? R2 unary-rule and L ? R1
lexical-rule parameters. None of these consider
the annotations on the children. They are subject
to L?N sum-to-one constraints.
The model also needs 4?R3 passpattern prob-
abilities like P (pass to head | X ? Y Z) above,
with R3 sum-to-one constraints, and L ? N non-
inherited annotation parameters Pann(?|X), with
N sum-to-one constraints.
Adding these up and canceling the two L ? N
4The reader may find it useful to write out the probability
P (X[?] ? Y [?] Z[?]) in terms of the parameters described
below. Like equation (5), it is P (X[?] ? Y Z) times a sum
of up to 4 products, corresponding to the 4 passpattern cases.
320
terms, the INHERIT model has
L(R3 +R2 +R1) + 3R3 ?N (9)
degrees of freedom. Thus for a typical grammar
where R3 dominates, we have reduced the number
of free parameters from about L3R3 to only about
LR3.
Runtime. We may likewise reduce an L3 factor
to L in the runtime. Table 2 shows dynamic pro-
gramming equations for the INHERIT model. By
exercising care, they are able to avoid summing
over all possible values of ? and ? within the in-
ner loop. This is possible because when they are
not inherited, they do not depend on X,Y, Z, or ?.
3.3 Multiple Features
The INHERIT model described above is linguisti-
cally naive in several ways. One problem (see sec-
tion 6 for others) is that each nonterminal has only
a single feature to pass. Linguists, however, usu-
ally annotate each phrase with multiple features.
Our example tree in Fig. 2 was annotated with both
tense and number features, with different inheri-
tance patterns.
As a step up from INHERIT, we propose an
INHERIT2 model where each nonterminal carries
two features. Thus, we will have L6R3 binary
rules instead of L3R3. However, we assume that
the two features choose their passpatterns inde-
pendently, and that when a feature is not inher-
ited, it is chosen independently of the other fea-
ture. This keeps the number of parameters down.
In effect, we are defining
P (X[?][?] ? Y [?][?] Z[?][? ])
= P (X[?][?] ? Y Z)
?P1(?, ? | X[?] ? Y Z)
?P2(?, ? | X[?] ? Y Z)
where P1 and P2 choose child features as if they
were separate single-feature INHERIT models.
We omit discussion of dynamic programming
speedups for INHERIT2. Empirically, the hope is
that the two features when learned with the EM
algorithm will pick out different linguistic proper-
ties of the constituents in the treebank tree.
4 Annealing-Like Training Approaches
Training latent PCFG models, like training most
other unsupervised models, requires non-convex
optimization. To find good parameter values, it
is often helpful to train a simpler model first and
use its parameters to derive a starting guess for the
harder optimization problem. A well-known ex-
ample is the training of the IBM models for statis-
tical machine translation (Berger et al, 1994).
In this vein, we did an experiment in which we
gradually increased L during EM training of the
PCFG-LA and INHERIT models. Whenever the
training likelihood began to converge, we man-
ually and globally increased L, simply doubling
or tripling it (see ?clone all? in Table 3 and
Fig. 5). The probability of X[?] ? Y [?]Z[?]
under the new model was initialized to be pro-
portional to the probability of X[? mod L] ?
Y [? mod L]Z[? mod L] (where L refers to the
old L),5 times a random ?jitter? to break symme-
try.
In a second annealing experiment (?clone
some?) we addressed a weakness of the PCFG-
LA and INHERIT models: They give every non-
terminal the same number of latent annotations.
It would seem that different coarse-grained non-
terminals in the original Penn Treebank have dif-
ferent degrees of impurity (Klein and Manning,
2003). There are linguistically many kinds of
NP, which are differentially selected for by vari-
ous contexts and hence are worth distinguishing.
By contrast, -LRB- is almost always realized as
a left parenthesis and may not need further refine-
ment. Our ?clone some? annealing starts by train-
ing a model with L=2 to convergence. Then, in-
stead of cloning all nonterminals as in the previ-
ous annealing experiments, we clone only those
that have seemed to benefit most from their previ-
ous refinement. This benefit is measured by the
Jensen-Shannon divergence of the two distribu-
tions P (X[0] ? ? ? ? ) and P (X[1] ? ? ? ? ). The
5Notice that as well as cloning X[?], this procedure mul-
tiplies by 4, 2, and 1 the number of binary, unary, and lex-
ical rules that rewrite X[?]. To leave the backbone gram-
mar unchanged, we should have scaled down the probabili-
ties of such rules by 1/4, 1/2, and 1 respectively. Instead, we
simply scaled them all down by the same proportion. While
this temporarily changes the balance of probability among the
three kinds of rules, EM immediately corrects this balance on
the next training iteration to match the observed balance on
the treebank trees?hence the one-iteration downtick in Fig-
ure 5).
321
Jensen-Shannon divergence is defined as
D(q, r) =
1
2
(
D
(
q ||
q + r
2
)
+D
(
r ||
q + r
2
))
These experiments are a kind of ?poor man?s
version? of the deterministic annealing cluster-
ing algorithm (Pereira et al, 1993; Rose, 1998),
which gradually increases the number of clus-
ters during the clustering process. In determinis-
tic annealing, one starts in principle with a very
large number of clusters, but maximizes likeli-
hood only under a constraint that the joint distri-
bution p(point , cluster) must have very high en-
tropy. This drives all of the cluster centroids to co-
incide exactly, redundantly representing just one
effective cluster. As the entropy is permitted to de-
crease, some of the cluster centroids find it worth-
while to drift apart.6 In future work, we would
like to apply this technique to split nonterminals
gradually, by initially requiring high-entropy parse
forests on the training data and slowly relaxing this
constraint.
5 Experiments
5.1 Setup
We ran several experiments to compare the IN-
HERIT with the PCFG-LA model and look into the
effect of different Treebank preprocessing and the
annealing-like procedures.
We used sections 2?20 of the Penn Treebank 2
Wall Street Journal corpus (Marcus et al, 1993)
for training, section 22 as development set and
section 23 for testing. Following Matsuzaki et al
(2005), words occurring fewer than 4 times in the
training corpus were replaced by unknown-word
symbols that encoded certain suffix and capitaliza-
tion information.
All experiments used simple add-lambda
smoothing (?=0.1) during the reestimation step
(M step) of training.
Binarization and Markovization. Before ex-
tracting the backbone PCFG and running the con-
strained inside-outside (EM) training algorithm,
we preprocessed the Treebank using center-parent
binarization Matsuzaki et al (2005). Besides mak-
ing the rules at most binary, this preprocessing also
helpfully enriched the backbone nonterminals. For
6In practice, each very large group of centroids (effective
cluster) is represented by just two, until such time as those
two drift apart to represent separate effective clusters?then
each is cloned.
all but the first (?Basic?) experiments, we also
enriched the nonterminals with order-1 horizon-
tal and order-2 vertical markovization (Klein and
Manning, 2003).7 Figure 4 shows what a multiple-
child structure X ? A B H C D looks like
after binarization and markovization. The bina-
rization process starts at the head of the sentence
and moves to the right, inserting an auxiliary node
for each picked up child, then moving to the left.
Each auxiliary node consists of the parent label,
the direction (L or R) and the label of the child
just picked up.
Figure 4: Horizontal and vertical markovization
and center-parent binarization of the rule X ?
A B H C D where H is the head child.
Initialization. The backbone PCFG grammar
was read off the altered Treebank, and the initial
annotated grammar was created by creating sev-
eral versions of every rewrite rule. The proba-
bilities of these newly created rules are uniform
and proportional to the original rule, multiplied by
a random epsilon factor uniformly sampled from
[.9999,1.0001] to break symmetry.
5.2 Decoding
To test the PCFG learned by a given method,
we attempted to recover the unannotated parse
of each sentence in the development set. We
then scored these parses by debinarizing or de-
markovizing them, then measuring their precision
and recall of the labeled constituents from the
gold-standard Treebank parses.
7The vertical markovization was applied before binariza-
tion. ? Matsuzaki et al (2005) used a markovized grammar
to get a better unannotated parse forest during decoding, but
they did not markovize the training data.
322
Figure 5: Loge-likelihood during training. The
two ?anneal? curves use the ?clone all? method.
We increased L after iteration 50 and, for the IN-
HERIT model, iteration 110. The downward spikes
in the two annealed cases are due to perturbation
of the model parameters (footnote 5).
An unannotated parse?s probability is the total
probability, under our learned PCFG, of all of its
annotated refinements. This total can be efficiently
computed by the constrained version of the inside
algorithm in Table 2.
How do we obtain the unannotated parse whose
total probability is greatest? It does not suffice to
find the single best annotated parse and then strip
off the annotations. Matsuzaki et al (2005) note
that the best annotated parse is in fact NP-hard to
find. We use their reranking approximation. A
1000-best list for each sentence in the decoding
set was created by parsing with our markovized
unannotated grammar and extracting the 1000 best
parses using the k-best algorithm 3 described in
Huang and Chiang (2005). Then we chose the
most probable of these 1000 unannotated parses
under our PCFG, first finding the total probability
of each by using the the constrained inside algo-
rithm as explained above.8
5.3 Results and Discussion
Table 3 summarizes the results on development
and test data. 9 Figure 5 shows the training log-
likelihoods.
First, markovization of the Treebank leads to
8For the first set of experiments, in which the models were
trained on a simple non-markovized grammar, the 1000-best
trees had to be ?demarkovized? before our PCFG was able to
rescore them.
9All results are reported on sentences of 40 words or less.
striking improvements. The ?Basic? block of ex-
periments in Table 3 used non-markovized gram-
mars, as in Matsuzaki et al (2005). The next block
of experiments, introducing markovized gram-
mars, shows a considerable improvement. This
is not simply because markovization increases the
number of parameters: markovization with L = 2
already beats basic models that have much higher
L and far more parameters.
Evidently, markovization pre-splits the labels
in the trees in a reasonable way, so EM has less
work to do. This is not to say that markovization
eliminates the need for hidden annotations: with
markovization, going from L=1 to L=2 increases
the parsing accuracy even more than without it.
Second, our ?clone all? training technique
(shown in the next block of Table 3) did not
help performance and may even have hurt slightly.
Here we initialized the L=2x2 model with the
trained L=2 model for PCFG-LA, and the L=3x3
model with the L=3 and the L=3x3x3 model with
the L=3x3 model.
Third, our ?clone some? training technique ap-
peared to work. On PCFG-LA, the L<2x2 con-
dition (i.e., train with L=2 and then clone some)
matched the performance of L=4 with 30% fewer
parameters. On INHERIT, L<2x2 beat L=4 with
8% fewer parameters. In these experiments, we
used the average divergence as a threshold: X[0]
and X[1] are split again if the divergence of their
rewrite distributions is higher than average.
Fourth, our INHERIT model was a disappoint-
ment. It generally performed slightly worse than
PCFG-LA when given about as many degrees
of freedom. This was also the case on some
cursory experiments on smaller training corpora.
It is tempting to conclude that INHERIT simply
adopted overly strong linguistic constraints, but
relaxing those constraints by moving to the IN-
HERIT2 model did not seem to help. In our
one experiment with INHERIT2 (not shown in Ta-
ble 3), using 2 features that can each take L=2
values (d.f.: 212,707) obtains an F1 score of only
83.67?worse than 1 feature taking L=4 values.
5.4 Analysis: What was learned by INHERIT?
INHERIT did seem to discover ?linguistic? fea-
tures, as intended, even though this did not im-
prove parse accuracy. We trained INHERIT and
PCFG-LA models (both L=2, non-markovized)
and noticed the following.
323
PCFG-LA INHERIT
L d.f. LP LR F1 L d.f. LP LR F1
B
as
ic 1 24,226 76.99 74.51 75.73 1 35,956 76.99 74.51 75.73
2 72,392 81.22 80.67 80.94 2 60,902 79.42 77.58 78.49
4 334,384 83.53 83.39 83.46 12 303,162 82.41 81.55 81.98
8 2,177,888 85.43 85.05 85.24 80 1,959,053 83.99 83.02 83.50
M
ar
ko
v. 1 41,027 79.95 78.43 79.18 1 88,385 79.95 78.43 79.18
2 132,371 83.85 82.23 83.03
2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31
4 220,343 85.30 84.06 84.68
3 506,427 86.44 85.19 85.81 9 440,273 86.16 85.12 85.64
4 1,120,232 87.09 85.71 86.39 26 1,188,035 86.55 85.55 86.05
Cl
on
e
al
l 2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31
3x3 440,273 85.99 84.88 85.43
2x2 1,120,232 87.06 85.49 86.27 3x3x3 1,232,021 86.65 85.70 86.17
Cl
.so
m
e 2 178,264 85.70 84.37 85.03 2 132,371 83.85 82.23 83.03
<2x2 789,279 87.17 85.71 86.43 <2x2 203,673 85.49 84.45 84.97
<2x2x2 314,999 85.57 84.60 85.08
Table 3: Results on the development set: labeled precision (LP), labeled recall (LR), and their harmonic
mean (F1). ?Basic? models are trained on a non-markovized treebank (as in Matsuzaki et al (2005)); all
others are trained on a markovized treebank. The best model (PCFG-LA with ?clone some? annealing,
F1=86.43) has also been decoded on the final test set, reaching P/R=86.94/85.40 (F1=86.17).
We used both models to assign the most-
probable annotations to the gold parses of the de-
velopment set. Under the INHERIT model, NP[0]
vs. NP[1] constituents were 21% plural vs. 41%
plural. Under PCFG-LA this effect was weaker
(30% vs. 39%), although it was significant in both
(Fisher?s exact test, p < 0.001). Strikingly, un-
der the INHERIT model, the NP?s were 10 times
more likely to pass this feature to both children
(Fisher?s, p < 0.001)?just as we would expect
for a number feature, since the determiner and
head noun of an NP must agree.
The INHERIT model also learned to use feature
value 1 for ?tensed auxiliary.? The VP[1] nonter-
minal was far more likely than VP[0] to expand as
V VP, where V represents any of the tensed verb
preterminals VBZ, VBG, VBN, VBD, VBP. Further-
more, these expansion rules had a very strong pref-
erence for ?pass to head,? so that the left child
would also be annotated as a tensed auxiliary, typ-
ically causing it to expand as a form of be, have,
or do. In short, the feature ensured that it was gen-
uine auxiliary verbs that subcategorized for VP?s.
(The PCFG-LA model actually arranged the
same behavior, e.g. similarly preferring VBZ[1] in
the auxiliary expansion rule VP ? VBZ VP. The
difference is that the PCFG-LA model was able
to express this preference directly without prop-
agating the [1] up to the VP parent. Hence neither
VP[0] nor VP[1] became strongly associated with
the auxiliary rule.)
Many things are equally learned by both mod-
els: They learn the difference between subordinat-
ing conjunctions (while, if ) and prepositions (un-
der, after), putting them in distinct groups of the
original IN tag, which typically combine with sen-
tences and noun phrases, respectively. Both mod-
els also split the conjunction CC into two distinct
groups: a group of conjunctions starting with an
upper-case letter at the beginning of the sentence
and a group containing all other conjunctions.
6 Future Work: Log-Linear Modeling
Our approach in the INHERIT model made certain
strict independence assumptions, with no backoff.
The choice of a particular passpattern, for exam-
ple, depends on all and only the three nontermi-
nals X,Y, Z. However, given sparse training data,
sometimes it is advantageous to back off to smaller
amounts of contextual information; the nontermi-
nal X or Y might alone be sufficient to predict the
passpattern.
324
A very reasonable framework for handling this
issue is to model P (X[?] ? Y [?] Z[?]) with
a log-linear model.10 Feature functions would
consider the values of variously sized, over-
lapping subsets of X,Y, Z, ?, ?, ?. For exam-
ple, a certain feature might fire when X[?] =
NP[1] and Z[?] = N[2]. This approach can be ex-
tended to the multi-feature case, as in INHERIT2.
Inheritance as in the INHERIT model can then
be expressed by features like ? = ?, or ? =
? and X = VP. During early iterations, we could
use a prior to encourage a strong positive weight
on these inheritance features, and gradually re-
lax this bias?akin to the ?structural annealing? of
(Smith and Eisner, 2006).
When modeling the lexical rule P (X[?] ? w),
we could use features that consider the spelling
of the word w in conjunction with the value of
?. Thus, we might learn that V [1] is particularly
likely to rewrite as a word ending in -s. Spelling
features that are predictable from string context
are important clues to the existence and behavior
of the hidden annotations we wish to induce.
A final remark is that ?inheritance? does not
necessarily have to mean that ? = ?. It is enough
that ? and ? should have high mutual informa-
tion, so that one can be predicted from the other;
they do not actually have to be represented by the
same integer. More broadly, we might like ? to
have high mutual information with the pair (?, ?).
One might try using this sort of intuition directly
in an unsupervised learning procedure (Elidan and
Friedman, 2003).
7 Conclusions
We have discussed ?informed? techniques for in-
ducing latent syntactic features. Our INHERIT
model tries to constrain the way in which features
are passed through the tree. The motivation for
this approach is twofold: First, we wanted to cap-
ture the linguistic insight that features follow cer-
tain patterns in propagating through the tree. Sec-
ond, we wanted to make it statistically feasible and
computationally tractable to increase L to higher
values than in the PCFG-LA model. The hope was
that the learning process could then make finer dis-
tinctions and learn more fine-grained information.
However, it turned out that the higher values of
L did not compensate for the perhaps overly con-
10This affects EM training only by requiring a convex op-
timization at the M step (Riezler, 1998).
strained model. The results on English parsing
rather suggest that it is the similarity in degrees of
freedom (e.g., INHERIT with L=3x3x3 and PCFG-
LA with L=2x2) that produces comparable results.
Substantial gains were achieved by using
markovization and splitting only selected nonter-
minals. With these techniques we reach a pars-
ing accuracy similar to Matsuzaki et al (2005),
but with an order of magnitude less parameters,
resulting in more efficient parsing. We hope to
get more wins in future by using more sophisti-
cated annealing techniques and log-linear model-
ing techniques.
Acknowledgments
This paper is based upon work supported by the
National Science Foundation under Grant No.
0313193. We are grateful to Takuya Matsuzaki
for providing details about his implementation of
PCFG-LA, and to Noah Smith and the anonymous
reviewers for helpful comments.
References
A. Berger, P. Brown, S. Pietra, V. Pietra, J. Lafferty,
H. Printz, and L. Ures. 1994. The CANDIDE sys-
tem for machine translation.
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for Treebank II style: Penn
Treebank project. Technical Report MS-CIS-95-06,
University of Pennsylvania, January.
John Blatz and Jason Eisner. 2006. Transforming pars-
ing algorithms and other weighted logic programs.
In Proceedings of the 11th Conference on Formal
Grammar.
Eugene Charniak. 1996. Tree-bank grammars. In Pro-
ceedings of the 13th National Conference on Artifi-
cial Intelligence.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference on
Artificial Intelligence, pages 598?603.
David Chiang and Daniel M. Bikel. 2002. Recov-
ering latent information in treebanks. In COLING
2002: The 17th International Conference on Com-
putational Linguistics, Taipei.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In ACL-96,
pages 184?191, Santa Cruz, CA. ACL.
325
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 16?23, Madrid. Association for
Computational Linguistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 281?290, Vancouver, October.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING96: Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 340?
345, Copenhagen. Center for Sprogteknologi.
Gal Elidan and Nir Friedman. 2003. The information
bottleneck EM algorithm. In Proceedings of UAI.
Joshua Goodman. 1997. Probabilistic feature gram-
mars. In Proceedings of the 5th International Work-
shop on Parsing Technologies, pages 89?100, MIT,
Cambridge, MA, September.
L. Huang and D. Chiang. 2005. Parsing and k-best
algorithms. In Proc. of IWPT.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. MIT Press, Cambridge, MA.
Dan Klein and Christopher D. Manning. 2001. Dis-
tributional phrase structure induction. In The Fifth
Conference on Natural Language Learning.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423?430, Sapporo, Japan.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside
algorithm. Computer Speech and Language, 4:35?
56.
David M. Magerman. 1995. Statistical Decision-Tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 276?283, Cambridge, Mass.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
University of Michigan.
Fernando Pereira and Yves Schabes. 1992. Inside-
Outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark. University of Delaware.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of ACL, Ohio State University.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago.
Detlef Prescher. 2005. Head-driven PCFGs with
latent-head statistics. In Proceedings of the 9th
International Workshop on Parsing Technologies,
pages 115?124, Vancouver, BC, Canada, October.
Stefan Riezler. 1998. Statistical inference and prob-
abilistic modeling for constraint-based NLP. In
B. Schro?der, W. Lenders, W. Hess, and T. Portele,
editors, Computers, Linguistics, and Phonetics be-
tween Language and Speech: Proceedings of the 4th
Conference on Natural Language Processing (KON-
VENS?98), pages 111?124, Bonn. Lang.
Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and
related optimization problems. Proceedings of the
IEEE, 80:2210?2239, November.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of ACL.
326
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 201?205, New York City, June 2006. c?2006 Association for Computational Linguistics
Vine Parsing and Minimum Risk Reranking for Speed and Precision?
Markus Dreyer, David A. Smith, and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{markus,{d,n}asmith}@cs.jhu.edu
Abstract
We describe our entry in the CoNLL-X shared task.
The system consists of three phases: a probabilistic
vine parser (Eisner and N. Smith, 2005) that pro-
duces unlabeled dependency trees, a probabilistic
relation-labeling model, and a discriminative mini-
mum risk reranker (D. Smith and Eisner, 2006). The
system is designed for fast training and decoding and
for high precision. We describe sources of cross-
lingual error and ways to ameliorate them. We then
provide a detailed error analysis of parses produced
for sentences in German (much training data) and
Arabic (little training data).
1 Introduction
Standard state-of-the-art parsing systems (e.g.,
Charniak and Johnson, 2005) typically involve two
passes. First, a parser produces a list of the most
likely n parse trees under a generative, probabilistic
model (usually some flavor of PCFG). A discrim-
inative reranker then chooses among trees in this
list by using an extended feature set (Collins, 2000).
This paradigm has many advantages: PCFGs are
fast to train, can be very robust, and perform bet-
ter as more data is made available; and rerankers
train quickly (compared to discriminative models),
require few parameters, and permit arbitrary fea-
tures.
We describe such a system for dependency pars-
ing. Our shared task entry is a preliminary system
developed in only 3 person-weeks, and its accuracy
is typically one s.d. below the average across sys-
tems and 10?20 points below the best system. On
?This work was supported by NSF ITR grant IIS-0313193,
an NSF fellowship to the second author, and a Fannie and John
Hertz Foundation fellowship to the third author. The views ex-
pressed are not necessarily endorsed by the sponsors. We thank
Charles Schafer, Keith Hall, Jason Eisner, and Sanjeev Khudan-
pur for helpful conversations.
the positive side, its decoding algorithms have guar-
anteed O(n) runtime, and training takes only a cou-
ple of hours. Having designed primarily for speed
and robustness, we sacrifice accuracy. Better esti-
mation, reranking on larger datasets, and more fine-
grained parsing constraints are expected to boost ac-
curacy while maintaining speed.
2 Notation
Let a sentence x = ?x1, x2, ..., xn?, where each xi is
a tuple containing a part-of-speech tag ti and a word
wi, and possibly more information.1 x0 is a special
wall symbol, $, on the left. A dependency tree y
is defined by three functions: yleft and yright (both
{0, 1, 2, ..., n} ? 2{1,2,...,n}) that map each word to
its sets of left and right dependents, respectively, and
ylabel : {1, 2, ..., n} ? D, which labels the relation-
ship between word i and its parent from label set D.
In this work, the graph is constrained to be a pro-
jective tree rooted at $: each word except $ has a sin-
gle parent, and there are no cycles or crossing depen-
dencies. Using a simple dynamic program to find the
minimum-error projective parse, we find that assum-
ing projectivity need not harm accuracy very much
(Tab. 1, col. 3).
3 Unlabeled Parsing
The first component of our system is an unlabeled
parser that, given a sentence, finds the U best un-
labeled trees under a probabilistic model using a
bottom-up dynamic programming algorithm.2 The
model is a probabilistic head automaton grammar
(Alshawi, 1996) that assumes conditional indepen-
1We used words and fine tags in our parser and labeler, with
coarse tags in one backoff model. Other features are used in
reranking; we never used the given morphological features or
the ?projective? annotations offered in the training data.
2The execution model we use is best-first, exhaustive search,
as described in Eisner et al (2004). All of our dynamic pro-
gramming algorithms are implemented concisely in the Dyna
language.
201
B` Br
projective oracle
(B
` , B
r )-vine oracle
20-best unlabeled oracle
1-best unlabeled
unlabeled, reranked
20?50-best labeled oracle
1?1-best labeled
reranked (labeled)
(unlabeled)
(non-$ unl. recall)
(non-$ unl. precision)
Arabic 10 4 99.8 90.7 71.5 68.1 68.7 59.7 52.0 53.4 68.5 63.4 76.0
Bulgarian 5 4 99.6 90.7 86.4 80.1 80.5 85.1 73.0 74.8 82.0 74.3 86.3
Chinese 4 4 100.0 93.1 89.9 79.4 77.7 88.6 72.6 71.6 77.6 61.4 80.8
Czech 6 4 97.8 90.5 79.2 70.3 71.5 72.8 58.1 60.5 70.7 64.8 75.7
Danish 5 4 99.2 91.4 84.6 77.7 78.6 79.3 65.5 66.6 77.5 71.4 83.4
Dutch 6 5 94.6 88.3 77.5 67.9 68.8 73.6 59.4 61.6 68.3 60.4 73.0
German 8 7 98.8 90.9 83.4 75.5 76.2 82.3 70.1 71.0 77.0 70.2 82.9
Japanese 4 1 99.2 92.2 90.7 86.3 85.1 89.4 81.6 82.9 86.0 68.5 91.5
Portuguese 5 5 98.8 91.5 85.9 81.4 82.5 83.7 73.4 75.3 82.4 76.2 87.0
Slovene 6 4 98.5 91.7 80.5 72.0 73.3 72.8 57.5 58.7 72.9 66.3 78.5
Spanish 5 6 100.0 91.2 77.3 71.5 72.6 74.9 66.2 67.6 72.9 69.3 80.7
Swedish 4 5 99.7 94.0 87.5 79.3 79.6 81.0 65.5 67.6 79.5 72.6 83.3
Turkish 6 1 98.6 89.5 73.0 61.0 61.8 64.4 44.9 46.1 60.5 48.5 61.6
parser reranker labeler reranker
1 2 3 4 5 6 7 8 9 10 11 12 13
Table 1: Parameters and performance on test data. B` and Br were chosen to retain 90% of dependencies
in training data. We show oracle, 1-best, and reranked performance on the test set at different stages of the
system. Boldface marks oracle performance that, given perfect downstream modules, would supercede the
best system. Italics mark the few cases where the reranker increased error rate. Columns 8?10 show labeled
accuracy; column 10 gives the final shared task evaluation scores.
dence between the left yield and the right yield of
a given head, given the head (Eisner, 1997).3 The
best known parsing algorithm for such a model is
O(n3) (Eisner and Satta, 1999). The U -best list is
generated using Algorithm 3 of Huang and Chiang
(2005).
3.1 Vine parsing (dependency length bounds)
Following Eisner and N. Smith (2005), we also im-
pose a bound on the string distance between every
3To empirically test this assumption across languages, we
measured the mutual information between different features of
yleft(j) and yright(j), given xj . (Mutual information is a statis-
tic that equals zero iff conditional independence holds.) A de-
tailed discussion, while interesting, is omitted for space, but we
highlight some of our findings. First, unsurprisingly, the split-
head assumption appears to be less valid for languages with
freer word order (Czech, Slovene, German) and more valid for
more fixed-order languages (Chinese, Turkish, Arabic) or cor-
pora (Japanese). The children of verbs and conjunctions are the
most frequent violators. The mutual information between the
sequence of dependency labels on the left and on the right, given
the head?s (coarse) tag, only once exceeded 1 bit (Slovene).
child and its parent, with the exception of nodes at-
taching to $. Bounds of this kind are intended to im-
prove precision of non-$ attachments, perhaps sac-
rificing recall. Fixing bound B`, no left dependency
may exist between child xi and parent xj such that
j?i > B` (similarly for right dependencies and Br).
As a result, edge-factored parsing runtime is reduced
from O(n3) to O(n(B2` +B2r )). For each language,
we choose B` (Br) to be the minimum value that
will allow recovery of 90% of the left (right) depen-
dencies in the training corpus (Tab. 1, cols. 1, 2, and
4). In order to match the training data to the parsing
model, we re-attach disallowed long dependencies
to $ during training.
3.2 Estimation
The probability model predicts, for each parent word
xj , {xi}i?yleft (j) and {xi}i?yright (j). An advantage
of head automaton grammars is that, for a given par-
ent node xj , the children on the same side, yleft(j),
202
for example, can depend on each other (cf. McDon-
ald et al, 2005). Child nodes in our model are gener-
ated outward, conditional on the parent and the most
recent same-side sibling (MRSSS). This increases
our parser?s theoretical runtime to O(n(B3` + B3r )),
which we found was quite manageable.
Let pary : {1, 2, ..., n} ? {0, 1, ..., n} map each
node to its parent in y. Let predy : {1, 2, ..., n} ?
{?, 1, 2, ..., n} map each node to the MRSSS in y if
it exists and ? otherwise. Let ?i = |i ? j| if j is i?s
parent. Our (probability-deficient) model defines
p(y) =
n?
j=1
?
?
?
i?yleft (j)
p(xi,?i | xj , xpredy(i), left)
?
?
?p(STOP | xj , xminyleft (j) j , left)
?
?
?
?
i?yright (j)
p(xi,?i | xj ,predy(i), right)
?
?
?p(STOP | xj , xmaxyright (j) j , right) (1)
Due to the familiar sparse data problem, a maxi-
mum likelihood estimate for the ps in Eq. 1 performs
very badly (2?23% unlabeled accuracy). Good sta-
tistical parsers smooth those distributions by mak-
ing conditional independence assumptions among
variables, including backoff and factorization. Ar-
guably the choice of assumptions made (or interpo-
lated among) is central to the success of many exist-
ing parsers.
Noting that (a) there are exponentially many such
options, and (b) the best-performing independence
assumptions will almost certainly vary by language,
we use a mixture among 8 such models. The same
mixture is used for all languages. The models were
not chosen with particular care,4 and the mixture is
not trained?the coefficients are fixed at uniform,
with a unigram coarse-tag model for backoff. In
principle, this mixture should be trained (e.g., to
maximize likelihood or minimize error on a devel-
opment dataset).
The performance of our unlabeled model?s top
choice and the top-20 oracle are shown in Tab. 1,
cols. 5?6. In 5 languages (boldface), perfect label-
ing and reranking at this stage would have resulted in
performance superior to the language?s best labeled
4Our infrastructure provides a concise, interpreted language
for expressing the models to be mixed, so large-scale combina-
tion and comparison are possible.
system, although the oracle is never on par with the
best unlabeled performance.
4 Labeling
The second component of our system is a labeling
model that independently selects a label from D for
each parent/child pair in a tree. Given the U best
unlabeled trees for a sentence, the labeler produces
the L best labeled trees for each unlabeled one.
The computation involves an O(|D|n) dynamic pro-
gramming algorithm, the output of which is passed
to Huang and Chiang?s (2005) algorithm to generate
the L-best list.
We separate the labeler from the parser for two
reasons: speed and candidate diversity. In prin-
ciple the vine parser could jointly predict depen-
dency labels along with structures, but parsing run-
time would increase by at least a factor of |D|. The
two stage process also forces diversity in the candi-
date list (20 structures with 50 labelings each); the
1,000-best list of jointly-decoded parses often con-
tained many (bad) relabelings of the same tree.
In retrospect, assuming independence among de-
pendency labels damages performance substantially
for some languages (Turkish, Czech, Swedish, Dan-
ish, Slovene, and Arabic); note the often large drop
in oracle performance between Tab. 1, cols. 5 and
8. This assumption is necessary in our framework,
because the O(|D|M+1n) runtime of decoding with
an M th-order Markov model of labels5 is in general
prohibitive?in some cases |D| > 80. Pruning and
search heuristics might ameliorate runtime.
If xi is a child of xj in direction D, and xpred is
the MRSSS (possibly ?), where ?i = |i? j|, we es-
timate p(`, xi, xj , xpred ,?i | D) by a mixture (un-
trained, as in the parser) of four backed-off, factored
estimates.
After parsing and labeling, we have for each sen-
tence a list of U ? L candidates. Both the oracle
performance of the best candidate in the (20 ? 50)-
best list and the performance of the top candidate are
shown in Tab. 1, cols. 8?9. It should be clear from
the drop in both oracle and 1-best accuracy that our
labeling model is a major source of error.
5We tested first-order Markov models that conditioned on
parent or MRSSS dependency labels.
203
5 Reranking
We train a log-linear model combining many feature
scores (see below), including the log-probabilities
from the parser and labeler. Training minimizes
the expected error under the model; we use deter-
ministic annealing to smooth the error surface and
avoid local minima (Rose, 1998; D. Smith and Eis-
ner, 2006).
We reserved 200 sentences in each language for
training the reranker, plus 200 for choosing among
rerankers trained on different feature sets and differ-
ent (U ? L)-best lists.6
Features Our reranking features predict tags, la-
bels, lemmata, suffixes and other information given
all or some of the following non-local conditioning
context: bigrams and trigrams of tags or dependency
labels; parent and grandparent dependency labels;
subcategorization frames (in terms of tags or depen-
dency labels); the occurrence of certain tags between
head and child; surface features like the lemma7 and
the 3-character suffix. In some cases the children of
a node are considered all together, and in other cases
left and right are separated.
The highest-ranked features during training, for
all languages, are the parser and labeler probabil-
ities, followed by p(?i | tparent), p(direction |
tparent), p(label | labelpred , label succ , subcat), and
p(coarse(t) | D, coarse(tparent),Betw), where
Betw is TRUE iff an instance of the coarse tag type
with the highest mutual information between its left
and right children (usually verb) is between the child
and its head.
Feature and Model Selection For training speed
and to avoid overfitting, only a subset of the above
features are used in reranking. Subsets of differ-
ent sizes (10, 20, and 40, plus ?all?) are identified
for each language using two na??ve feature-selection
heuristics based on independent performance of fea-
tures. The feature subset with the highest accuracy
on the 200 heldout sentences is selected.
6In training our system, we made a serious mistake in train-
ing the reranker on only 200 sentences. As a result, our pre-
testing estimates of performance (on data reserved for model
selection) were very bad. The reranker, depending on condition,
had only 2?20 times as many examples as it had parameters to
estimate, with overfitting as the result.
7The first 4 characters of a word are used where the lemma
is not available.
Performance Accuracy of the top parses after
reranking is shown in Tab. 1, cols. 10?11. Reranking
almost always gave some improvement over 1-best
parsing.8 Because of the vine assumption and the
preprocessing step that re-attaches all distant chil-
dren to $, our parser learns to over-attach to $, treat-
ing $-attachment as a default/agnostic choice. For
many applications a local, incomplete parse may be
sufficiently useful, so we also measured non-$ unla-
beled precision and recall (Tab. 1, cols. 12?13); our
parser has > 80% precision on 8 of the languages.
We also applied reranking (with unlabeled features)
to the 20-best unlabeled parse lists (col. 7).
6 Error Analysis: German
The plurality of errors (38%) in German were er-
roneous $ attachments. For ROOT dependency la-
bels, we have a high recall (92.7%), but low pre-
cision (72.4%), due most likely to the dependency
length bounds. Among the most frequent tags, our
system has most trouble finding the correct heads of
prepositions (APPR), adverbs (ADV), finite auxil-
iary verbs (VAFIN), and conjunctions (KON), and
finding the correct dependency labels for preposi-
tions, nouns, and finite auxiliary verbs.
The German conjunction und is the single word
with the most frequent head attachment errors. In
many of these cases, our system does not learn
the subtle difference between enumerations that are
headed by A in A und B, with two children und and
B on the right, and those headed by B, with und and
A as children on its left.
Unlike in some languages, our labeled oracle ac-
curacy is nearly as good as our unlabeled oracle ac-
curacy (Tab. 1, cols. 8, 5). Among the ten most fre-
quent dependency labels, our system has the most
difficulty with accusative objects (OA), genitive at-
tributes (AG), and postnominal modifiers (MNR).
Accusative objects are often mistagged as subject
(SB), noun kernel modifiers (NK), or AG. About
32% of the postnominal modifier relations (ein Platz
in der Geschichte, ?a place in history?) are labeled
as modifiers (in die Stadt fliegen, ?fly into the city?).
Genitive attributes are often tagged as NK since both
are frequently realized as nouns.
8The exception is Chinese, where the training set for rerank-
ing is especially small (see fn. 6).
204
7 Error Analysis: Arabic
As with German, the greatest portion of Arabic er-
rors (40%) involved attachments to $. Prepositions
are consistently attached too low and accounted for
26% of errors. For example, if a form in construct
(idafa) governed both a following noun phrase and
a prepositional phrase, the preposition usually at-
taches to the lower noun phrase. Similarly, prepo-
sitions usually attach to nearby noun phrases when
they should attach to verbs farther to the left.
We see a more serious casualty of the dependency
length bounds with conjunctions. In ground truth
test data, 23 conjunctions are attached to $ and 141
to non-$ to using the COORD relation, whereas 100
conjunctions are attached to $ and 67 to non-$ us-
ing the AUXY relation. Our system overgeneralizes
and attaches 84% of COORD and 71% of AUXY
relations to $. Overall, conjunctions account for
15% of our errors. The AUXY relation is defined
as ?auxiliary (in compound expressions of various
kinds)?; in the data, it seems to be often used for
waw-consecutive or paratactic chaining of narrative
clauses. If the conjunction wa (?and?) begins a sen-
tence, then that conjunction is tagged in ground truth
as attaching to $; if the conjunction appears in the
middle of the sentence, it may or may not be at-
tached to $.
Noun attachments exhibit a more subtle problem.
The direction of system attachments is biased more
strongly to the left than is the case for the true data.
In canonical order, Arabic nouns do generally attach
on the right: subjects and objects follow the verb; in
construct, the governed noun follows its governor.
When the data deviate from this canonical order?
when, e.g, a subject precedes its verb?the system
prefers to find some other attachment point to the
left. Similarly, a noun to the left of a conjunction
often erroneously attaches to its left. Such ATR re-
lations account for 35% of noun-attachment errors.
8 Conclusion
The tradeoff between speed and accuracy is famil-
iar to any parsing researcher. Rather than starting
with an accurate system and then applying corpus-
specific speedups, we start by imposing carefully-
chosen constraints (projectivity and length bounds)
for speed, leaving accuracy to the parsing and
reranking models. As it stands, our system performs
poorly, largely because the estimation is not state-
of-the-art, but also in part due to dependency length
bounds, which are rather coarse at present. Better re-
sults are achievable by picking different bounds for
different head tags (Eisner and N. Smith, 2005). Ac-
curacy should not be difficult to improve using bet-
ter learning methods, especially given our models?
linear-time inference and decoding.
References
H. Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations.
In Proc. of ACL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL.
M. Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
J. Eisner and G. Satta. 1999. Efficient parsing
for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In
Proc. of IWPT.
J. Eisner, E. Goldlust, and N. A. Smith. 2004.
Dyna: A declarative language for implementing
dynamic programs. In Proc. of ACL (companion
volume).
J. Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proc. of IWPT.
L. Huang and D. Chiang. 2005. Better k-best pars-
ing. In Proc. of IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proc. of HLT-
EMNLP.
K. Rose. 1998. Deterministic annealing for cluster-
ing, compression, classification, regression, and
related optimization problems. Proc. of the IEEE,
86(11):2210?2239.
D. A. Smith and J. Eisner. 2006. Minimum risk an-
nealing for training log-linear models. To appear
in Proc. of COLING-ACL.
205
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 103?110,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street, Baltimore, MD 21218 USA
{dreyer,keith hall,khudanpur}@jhu.edu
Abstract
This paper describes a new method to
compare reordering constraints for Statis-
tical Machine Translation. We investi-
gate the best possible (oracle) BLEU score
achievable under different reordering con-
straints. Using dynamic programming, we
efficiently find a reordering that approxi-
mates the highest attainable BLEU score
given a reference and a set of reordering
constraints. We present an empirical eval-
uation of popular reordering constraints:
local constraints, the IBM constraints,
and the Inversion Transduction Grammar
(ITG) constraints. We present results for a
German-English translation task and show
that reordering under the ITG constraints
can improve over the baseline by more
than 7.5 BLEU points.
1 Introduction
Reordering the words and phrases of a foreign sen-
tence to obtain the target word order is a fundamen-
tal, and potentially the hardest, problem in machine
translation. The search space for all possible per-
mutations of a sentence is factorial in the number
of words/phrases; therefore a variety of models have
been proposed that constrain the set of possible per-
mutations by allowing certain reorderings while dis-
allowing others. Some models (Brown et al (1996),
Kumar and Byrne (2005)) allow words to change
place with their local neighbors, but disallow global
reorderings. Other models (Wu (1997), Xiong et al
(2006)) explicitly allow global reorderings, but do
not allow all possible permutations, including some
local permutations.
We present a novel technique to compare achiev-
able translation accuracies under different reorder-
ing constraints. While earlier work has trained and
tested instantiations of different reordering models
and then compared the translation results (Zens and
Ney, 2003) we provide a more general mechanism
to evaluate the potential efficacy of reordering con-
straints, independent of specific training paradigms.
Our technique attempts to answer the question:
What is the highest BLEU score that a given trans-
lation system could reach when using reordering
constraints X? Using this oracle approach, we ab-
stract away from issues that are not inherent in the
reordering constraints, but may nevertheless influ-
ence the comparison results, such as model and fea-
ture design, feature selection, or parameter estima-
tion. In fact, we compare several sets of reorder-
ing constraints empirically, but do not train them as
models. We merely decode by efficiently search-
ing over possible translations allowed by each model
and choosing the reordering that achieves the high-
est BLEU score.
We start by introducing popular reordering con-
straints (Section 2). Then, we present dynamic-
programming algorithms that find the highest-
scoring permutations of sentences under given re-
ordering constraints (Section 3). We use this tech-
nique to compare several reordering constraints em-
pirically. We combine a basic translation framework
with different reordering constraints (Section 4) and
103
present results on a German-English translation task
(Section 5). Finally, we offer an analysis of the
results and provide a review of related work (Sec-
tions 6?8).
2 Reordering Constraints
Reordering constraints restrict the movement of
words or phrases in order to reach or approximate
the word order of the target language. Some of
the constraints considered in this paper were origi-
nally proposed for reordering words, but we will de-
scribe all constraints in terms of reordering phrases.
Phrases are units of consecutive words read off a
phrase translation table.
2.1 Local Constraints
Local constraints allow phrases to swap with one
another only if they are adjacent or very close to
each other. Kumar and Byrne (2005) define two
local reordering models for their Translation Tem-
plate Model (TTM): In the first one, called MJ-1,
only adjacent phrases are allowed to swap, and the
movement has to be done within a window of 2. A
sequence consisting of three phrases abc can there-
fore become acb or bac, but not cba. One phrase
can jump at most one phrase ahead and cannot take
part in more than one swap. In their second strategy,
called MJ-2, phrases are allowed to swap with their
immediate neighbor or with the phrase next to the
immediate neighbor; the maximum jump length is 2.
This allows for all six possible permutations of abc.
The movement here has to take place within a win-
dow of 3 phrases. Therefore, a four-phrase sequence
abcd cannot be reordered to cadb, for example. MJ-
1 and MJ-2 are shown in Figure 1.
2.2 IBM Constraints
First introduced by Brown et al (1996), the IBM
constraints are among the most well-known and
most widely used reordering paradigms. Transla-
tion is done from the beginning of the sentence to
the end, phrase by phrase; at each point in time, the
constraints allow one of the first k still untranslated
phrases to be selected for translation (see Figure 1d,
for k=2). The IBM constraints are much less restric-
tive than local constraints. The first word of the in-
put, for example, can move all the way to the end,
independent of the value of k. Typically, k is set to
4 (Zens and Ney, 2003). We write IBM with k=4 as
IBM(4). The IBM constraints are supersets of the
local constraints.
0
1
if
2
you
3
to-m
e
4
that
5
expla
in
6
cou
ld
(a) The sentence in foreign word order.
0
3
you
1
if
4
if you
2
to-m
e
5
to-m
e
7
that you
8
that
6
expla
in
to-m
e
9
expla
in
10
cou
ld that
11
cou
ld
expla
in
(b) MJ-1
0
8
you
6
to-m
e
1
if
9
if
60
to-m
e you
7
ifyou
3
that
2
to-m
e
15
to-m
e
12
that
10
expla
in
4
to-m
e
5
you youthat
17
that
19
cou
ld
16
expla
inyouto-me
18
expla
in
21
cou
ld
if you
to-m
e
13
expla
inthat
11
to-m
e
to-m
ethat
22
cou
ld
20
expla
inthatthat could
thatexpla
in
(c) MJ-2
0
6
you
1
if
7
if
11
to-m
e you
2
to-m
e
12
to-m
e
8
that you
3
that
16
that
13
expla
in you
4
expla
in
19
expla
in
17
cou
ld you
5
cou
ld
21
cou
ld you
if
15
that
to-m
e
9
expla
in
to-m
e
10
cou
ld
to-m
e
if
18
expla
in
that
60
cou
ld
that
if
20
cou
ld
expla
inif
(d) IBM(2)
Figure 1: The German word order if you to-me that explain
could (?wenn Sie mir das erkla?ren ko?nnten?) and all possible
reorderings under different constraints, represented as lattices.
None of these lattices contains the correct English order if you
could explain that to-me. See also Table 1.
2.3 ITG Constraints
The Inversion Transduction Grammar (ITG) (Wu,
1997), a derivative of the Syntax Directed Trans-
duction Grammars (Aho and Ullman, 1972), con-
strains the possible permutations of the input string
by defining rewrite rules that indicate permutations
of the string. In particular, the ITG allows all per-
mutations defined by all binary branching struc-
tures where the children of any constituent may be
swapped in order. The ITG constraint is different
from the other reordering constraints presented in
that it is not based on finite-state operations. An
104
Model # perm. ?Best? sentence n-gram precisions BLEU
MJ-1 13 if you that to-me could explain 100.0/66.7/20.0/0.0 0.0
MJ-2 52 to-me if you could explain that 100.0/83.3/60.0/50.0 70.71
IBM(2) 32 if to-me that you could explain 100.0/50.0/20.0/0.0 0.0
IBM(4) 384 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
IBM(4) (prune) 42 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
ITG 394 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
ITG (prune) 78 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
Table 1: Illustrating example: The number of permutations (# perm.) that different reordering paradigms consider for the input
sequence if you to-me that explain could, and the permutation with highest BLEU score. The sentence length is 7, but there are
only 6! possible permutations, since the phrase to-me counts as one word during reordering. ITG (prune) is the ITG BLEU decoder
with the pruning settings we used in our experiments (beam threshold 10?4). For comparison, IBM(4) (prune) is the lattice
BLEU decoder with the same pruning settings, but we use pruning only for ITG permutations in our experiments.
Figure 2: The example if
you to-me that explain could
and its reordering to if you
could explain that to-me us-
ing an ITG. The alignments
are added below the tree, and
the horizontal bars in the tree
indicate a swap.
ITG decoder runs in polynomial time and allows for
long-distance phrasal reordering. A phrase can, for
example, move from the first position in the input
to the last position in the output and vice versa, by
swapping the topmost node in the constructed bi-
nary tree. However, due to the binary bracketing
constraint, some permutations are not modeled. A
four-phrase sequence abcd cannot be permuted into
cadb or bdac. Therefore, the ITG constraints are not
supersets of the IBM constraints. IBM(4), for exam-
ple, allows abcd to be permuted into cadb and bdac.
3 Factored BLEU Computation
The different reordering strategies described allow
for different permutations and restrict the search
space in different ways. We are concerned with
the maximal achievable accuracy under given con-
straints, independent of feature design or parameter
estimation. This is what we call the oracle accuracy
under the reordering constraints and it is computed
on a dataset with reference translations.
We now describe algorithms that can be used
to find such oracle translations among unreordered
translation candidates. There are two equivalent
strategies: The reordering constraints that are be-
ing tested can be expressed as a special dynamic-
programming decoder which, when applied to an
unreordered hypothesis, searches the space of per-
mutations defined by the reordering constraints and
returns the highest-scoring permutation. We employ
this strategy for the ITG reorderings (Section 3.2).
For the other reordering constraints, we employ a
more generic strategy: Given the set of reorder-
ing constraints, all permutations of an unreordered
translation candidate are precomputed and explicitly
represented as a lattice. This lattice is passed as in-
put to a Dijkstra-style decoder (Section 3.1) which
traverses it and finds the solution that reachest the
highest BLEU score.1
3.1 Dijkstra BLEU Decoder
The Dijkstra-style decoder takes as input a lattice in
which each path represents one possible permutation
of an unreordered hypothesis under a given reorder-
ing paradigm, as in Figure 1. It traverses the lat-
tice and finds the solution that has the highest ap-
proximate BLEU score, given the reference. The
dynamic-programming algorithm divides the prob-
lem into subproblems that are solved independently,
the solutions of which contribute to the solutions
of other subproblems. The general procedure is
sketched in Figure 3: for each subpath of the lat-
tice containing the precomputed permutations, we
store the three most recently attached words (Fig-
1For both strategies, several unreordered translation candi-
dates do not have to be regarded separately, but can be repre-
sented as a weighted lattice and be used as input to the special
dynamic program or to the process that precomputes possible
permutations.
105
?([0, k, len + 1, w2, w3, wnew]) = max
w1
( get bleu ( [0, j, len, w1, w2, w3], [j, k, wnew] ) ) (1)
function get bleu ( [0, j, len, w1, w2, w3], [j, k, wnew] ) :=
update ngrams (0, j, k, len, w1, w2, w3, wnew) ;
return exp
(
1
4
4?
n=1
log
(
ngramsi([0, k, len + 1, w2, w3, wnew])
len ? n + 1
))
;
(2)
Figure 3: Top: The BLEU score is used as inside score for a subpath from 0 to k with the rightmost words w2, w3, wnew in the
Dijkstra decoder. Bottom: Pseudo code for a function get bleu which updates the n-gram matches ngrams1(. . . ), ngrams2(. . . ),
ngrams3(. . . ), ngrams4(. . . ) for the resulting subpath in a hash table [0, k, len + 1, w2, w3, wnew] and returns its approximate
BLEU score.
("",
"","
")
0/0
/0/
0
("",
"to"
,"m
e")
2/1
/0/
0
("to
","m
e","
if")
3/1
/0/
0
("m
e","
if",
"yo
u")
4/2
/0/
0
("if
","y
ou"
,"co
uld
")
5/3
/1/
0
("y
ou"
,"co
uld
","e
xpl
ain
")
6/4
/2/
1
("co
uld
","e
xpl
ain
","t
hat
")
7/5
/3/
2
0
6
to-
me if yo
u
7
if yo
u
15
yo
u
19
co
uld tha
t
ex
pla
in
20
ex
pla
in
tha
t
22
tha
t
Figure 4: Three right-most words and n-gram matches: This shows the best path for the MJ-2 reordering of if you to-me that
explain could, along with the words stored at each state and the progressively updated n-gram matches. The full path to-me if you
could explain that has 7 unigram matches, 5 bigram, 3 trigram, and 2 fourgram matches. See the full MJ-2 lattice in Figure 1c.
ure 4). A context of three words is needed to com-
pute fourgram precisions used in the BLEU score.
Starting from the start state, we recursively extend
a subpath word by word, following the paths in
the lattice. Whenever we extend the path by a
word to the right we incorporate that word and use
update ngrams to update the four n-gram counts
for the subpath. The function update ngrams has
access to the reference string2 and stores the updated
n-gram counts for the resulting path in a hash table.3
The inside score of each subpath is the approximate
BLEU score, calculated as the average of the four
n-gram log precisions. An n-gram precision is al-
ways the number of n-gram matches divided by the
length len of the path minus (n ? 1). A path of
length 4 with 2 bigram matches, for example, has
a bigram precision of 2/3. This method is similar to
Dijkstra?s algorithm (Dijkstra, 1959) composed with
a fourgram finite-state language model, where the
scoring is done using n-gram counts and precision
2Multiple reference strings can be used if available.
3An epsilon value of 1?10 is used for zero precisions.
scores. We call this the Dijkstra BLEU decoder.
3.2 ITG BLEU Decoder
For the ITG reordering constraints, we use a dy-
namic program that computes the permutations im-
plicitly. It takes only the unreordered hypothesis
as input and creates the possible reorderings under
the ITG constraints during decoding, as it creates
a parse chart. The algorithm is similar to a CKY
parsing algorithm in that it proceeds bottom-up and
combines smaller constituents into larger ones re-
cursively. Figure 5 contains details of the algo-
rithm. The ITG BLEU decoder stores the three left-
most and the three rightmost words in each con-
stituent. A constituent from position i to posi-
tion k, with wa, wb, and wc as leftmost words,
and wx, wy, wz as rightmost words is written as
[i, k, (wa, wb, wc), (wx, wy, wz)]. Such a constituent
can be built by straight or inverted rules. Using an
inverted rule means swapping the order of the chil-
dren in the built constituent. The successive bottom-
up combinations of adjacent constituents result in hi-
erarchical binary bracketing with swapped and non-
106
? ([i, k, (wa, wb, wc), (wx, wy, wz)]) = max
(
?() ([i, k, (wa, wb, wc), (wx, wy, wz)]) ,
?<> ([i, k, (wa, wb, wc), (wx, wy, wz)])
)
(3)
?<>([i, k, (wa, wb, wc), (wx, wy, wz)]) =
max
j,wa? ,wb? ,wc? ,wx? ,wy? ,wz?
(
get bleu
( [
j, k, (wa, wb, wc), (wx? , wy? , wz?)
]
,
[i, j, (wa? , wb? , wc?), (wx, wy, wz)]
) ) (4)
Figure 5: Equations for the ITG oracle BLEU decoder. [i, k, (wa, wb, wc), (wx, wy, wz)] is a constituent from i to k with leftmost
words wa,wb,wc and rightmost words wx,wy ,wz . Top: A constituent can be built with a straight or a swapped rule. Bottom: A
swapped rule. The get bleu function can be adapted from Figure 3
swapped constituents. Our ITG BLEU decoder uses
standard beam search pruning. As in Zens and Ney
(2003), phrases are not broken up, but every phrase
is, at the beginning of reordering, stored in the chart
as one lexical token together with the precomputed
n-gram matches and the n-gram precision score.
In addition to standard ITG we run experiments
with a constrained ITG, in which we impose a bound
? on the maximum length of reordered constituents,
measured in phrases. If the combined length of two
constituents exceeds this bound they can only be
combined in the given monotone order. Experiments
with this ITG variant give insight into the effect that
various long-distance reorderings have on the final
BLEU scores (see Table 3). Such bounds are also
effective speedup techniques(Eisner and Tromble,
2006).
3.3 BLEU Approximations
BLEU is defined to use the modified n-gram preci-
sion, which means that a correct n-gram that oc-
curs once in the reference, but several times in the
system translation will be counted only once as
correct. The other occurrences are clipped. We
do not include this global feature since we want
a dynamic-programming solution with polynomial
size and runtime. The decoder processes subprob-
lems independently; words are attached locally and
stored only as boundary words of covered paths/
constituents. Therefore we cannot discount a locally
attached word that has already been attached else-
where to an alternative path/constituent. However,
clipping affects most heavily the unigram scores
which are constant, like the length of the sentence.4
4Since the sentence lengths are constant for all reorderings
of a given sentence we can in our experiments also ignore the
brevity penalty which cancels out. If the input consists of sev-
We also adopt the approximation that treats every
sentence with its reference as a separate corpus (Till-
mann and Zhang, 2006) so that ngram counts are not
accumulated, and parallel processing of sentences
becomes possible. Due to these two approximations,
our method is not guaranteed to find the best reorder-
ing defined by the reordering constraints. However,
we have found on our heldout data that an oracle
that does not accumulate n-gram counts is only min-
imally worse than an oracle that does accumulate
them (up to 0.25 BLEU points).5 If, in addition,
clipping is ignored, the resulting oracle stays virtu-
ally the same, at most 0.02 BLEU points worse than
the oracle found otherwise. All results in this paper
are computed with the original BLEU formula on the
sentences found by the oracle algorithms.
4 Creating a Monotone Translation
Baseline
To compare the reordering constraints under ora-
cle conditions we first obtain unreordered candi-
date translations from a simple baseline translation
model. For each reordering paradigm, we take the
candidate translations, get the best oracle reorder-
ings under the given reordering constraints and pick
the best sentence according to the BLEU score.
The baseline translation system is created using
probabilistic word-to-word and phrase-to-phrase ta-
eral sentences of different lengths (see fn. 1) then the brevity
penalty can be built in by keeping track of length ratios of at-
tached phrases.
5The accumulating oracle algorithm makes a greedy deci-
sion for every sentence given the ngram counts so far accumu-
lated (Zens and Ney, 2005). The result of such a greedy or-
acle method may depend on the order of the input sentences.
We tried 100 shuffles of these and received 100 very simi-
lar results, with a variance of under 0.006 BLEU points. The
non-accumulating oracles use an epsilon value (1?10) for zero
counts.
107
bles. Using the translation probabilities, we create
a lattice that contains word and phrase translations
for every substring of the source sentence. The re-
sulting lattice is made of English words and phrases
of different lengths. Every word or phrase transla-
tion probability p is a mixture of p(f |e) and p(e|f).
We discard short phrase translations exponentially
by a parameter that is trained on heldout data. Inser-
tions and deletions are handled exclusively by the
use of a phrase table: an insertion takes place wher-
ever the English side of a phrase translation is longer
than the foreign side (e.g. English presidential can-
didate for German Pra?sidentschaftskandidat), and
vice versa for deletions (e.g. we discussed for wir
haben diskutiert). Gaps or discontinuous phrases
are not handled. The baseline decoder outputs the
n-best paths through the lattice according to the lat-
tice scores6, marking consecutive phrases so that the
oracle reordering algorithms can recognize them and
keep them together. Note that the baseline system is
trained on real data, while the reordering constraints
that we want to test are not trained.
5 Empirical Comparison of Reordering
Constraints
We use the monotone translation baseline model and
the oracle BLEU computation to evaluate different
popular reordering strategies. We now describe the
experimental settings. The word and phrase transla-
tion probabilities of the baseline model are trained
on the Europarl German-English training set, using
GIZA++ and the Pharaoh phrase extraction algo-
rithm. For testing we use the NAACL 2006 SMT
Shared Task test data. For each sentence of the test
set, a lattice is created in the way described in Sec-
tion 4, with parameters optimized on a small heldout
set.7 For each sentence, the 1000-best candidates ac-
cording to the lattice scores are extracted. We take
the 10-best oracle candidates, according to the ref-
erence, and use a BLEU decoder to create the best
permutation of each of them and pick the best one.
Using this procedure, we make sure that we get the
highest-scoring unreordered candidates and choose
the best one among their oracle reorderings. Table 2
6We use a straightforward adaption of Algorithm 3 in Huang
and Chiang (2005)
7We fill the initial phrase and word lattice with the 20 best
candidates, using phrases of 3 or less words.
and Figure 6 show the resulting BLEU scores for dif-
ferent sentence lengths. Table 3 shows results of the
ITG runs with different length bounds ?. The aver-
age phrase length in the candidate translations of the
test set is 1.42 words.
Oracle decodings under the ITG and under
IBM(4) constraints were up to 1000 times slower
than under the other tested oracle reordering meth-
ods in our implementations. Among the faster meth-
ods, decoding under MJ-2 constraints was up to 40%
faster than under IBM(2) constraints in our imple-
mentation.
 
20
 
25
 
30
 
35
 
40
 
45  5
 
10
 
15
 
20
 
25
 
30
 
35
 
40
BLEU
Senten
ce leng
thI
TG
IBM, 
k=4
IBM, 
k=2 MJ-2 MJ-1 Baseli
ne
Figure 6: Reordering oracle scores for different sentence
lengths. See also Table 2.
6 Discussion
The empirical results show that reordering un-
der sufficiently permissive constraints can improve
a monotone baseline oracle by more than 7.5
BLEU points. This gap between choosing the best
unreordered sentences versus choosing the best op-
timally reordered sentences is small for short sen-
tences and widens dramatically (more than nine
BLEU points) for longer sentences.
The ITG constraints and the IBM(4) constraints
both give very high oracle translation accuracies on
the German-English translation task. Overall, their
BLEU scores are about 2 to more than 4 points bet-
ter than the BLEU scores of the best other meth-
ods. This gap between the two highest-scoring con-
straints and the other methods becomes bigger as
the sentence lengths grow and is greater than 4
108
Sen
tenc
e le
ngth
# of
test
sen
ten
ces
BLEU (NIST) scores
ITG (prune) IBM, k=4 IBM, k=2 MJ-2 MJ-1 No reordering
1?5 61 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.17 (5.68)
6?10 230 43.83 (6.75) 43.71 (6.74) 41.94 (6.68) 42.50 (6.71) 40.85 (6.66) 39.21 (6.99)
11?15 440 33.66 (6.71) 33.37 (6.71) 31.23 (6.62) 31.49 (6.64) 29.67 (6.56) 28.21 (6.76)
16?20 447 30.47 (6.66) 29.99 (6.65) 27.00 (6.52) 27.06 (6.50) 25.15 (6.45) 23.34 (6.52)
21?25 454 30.13 (6.80) 29.83 (6.79) 27.21 (6.67) 27.22 (6.65) 25.46 (6.58) 23.32 (6.63)
26?30 399 26.85 (6.42) 26.36 (6.42) 22.79 (6.25) 22.47 (6.22) 20.38 (6.12) 18.31 (6.11)
31?35 298 28.11 (6.45) 27.47 (6.43) 23.79 (6.25) 23.28 (6.21) 21.09 (6.12) 18.94 (6.06)
36?40 242 27.65 (6.37) 26.97 (6.35) 23.31 (6.19) 22.73 (6.16) 20.70 (6.06) 18.22 (5.94)
1?40 2571 29.63 (7.48) 29.17 (7.46) 26.07 (7.24) 25.89 (7.22) 23.95 (7.08) 21.89 (7.07)
Table 2: BLEU and NIST results for different reordering methods on binned sentence lengths. The ITG results are, unlike the
other results, with pruning (beam 10?4). The BLEU results are plotted in Figure 6. All results are computed with the original
BLEU formula on the sentences found by the oracle algorithms.
BLEU scores for sentences longer than 30 sentences.
This advantage in translation accuracy comes with
high computational cost, as mentioned above.
Among the computationally more lightweight re-
ordering methods tested, IBM(2) and MJ-2 are very
close to each other in translation accuracy, with
IBM(2) obtaining slightly better scores on longer
sentences, while MJ-2 is more efficient. MJ-1 is
less successful in reordering, improving the mono-
tone baseline by only about 2.5 BLEU points at best,
but is the best choice if speed is an issue.
As described above, the reorderings defined by
the local constraints MJ-1 and MJ-2 are subsets of
IBM(2) and IBM(3). We did not test IBM(3), but
the values can be interpolated between IBM(2) and
IBM(4). The ITG constraints do not belong in this
family of finite-state contraints; they allow reorder-
ings that none of the other methods allow, and vice
versa. The fact that ITG constraints can reach such
high translation accuracies supports the findings in
Zens et al (2004) and is an empirical validation of
the ITG hypothesis.
The experiments with the constrained ITG show
the effect of reorderings spanning different lengths
(see Table 3). While most reorderings are short-
distance (<5 phrases) a lot of improvements can still
be obtained when ? is increased from length 5 to 10
and even from 10 to 20 phrases.
7 Related Work
There exist related algorithms that search the space
of reorderings and compute BLEU oracle approxi-
Len. ?=0 ?=5 ?=10 ?=20 ?=30 ?=40
26?30 18.31 24.07 26.40 26.79 26.85 26.85
31?35 18.94 25.10 27.21 28.00 28.09 28.11
36?40 18.22 24.46 26.66 27.53 27.64 27.65
26?40 18.49 24.74 26.74 27.41 27.50 27.51
Table 3: BLEU results of ITGs that are constrained to reorder-
ings not exceeding a certain span length ?. Results shown for
different sentence lengths.
mations. Zens and Ney (2005) describe a dynamic-
programming algorithm in which at every state the
number of n-gram matches is stored, along with a
multiset that contains all words from the reference
that have not yet been matched. This makes it pos-
sible to compute the modified ngram precision, but
the search space is exponential. Tillmann and Zhang
(2006) use a BLEU oracle decoder for discrimina-
tive training of a local reordering model. No de-
tails about the algorithm are given. Zens and Ney
(2003) perform a comparison of different reorder-
ing strategies. Their study differs from ours in that
they use reordering models trained on real data and
may therefore be influenced by feature selection,
parameter estimation and other training-specific is-
sues. In our study, only the baseline translation
model is trained on data. Zens et al (2004) con-
duct a study similar to Zens and Ney (2003) and note
that the results for the ITG reordering constraints
were quite dependent on the very simple probability
model used. Our study avoids this issue by using the
109
BLEU oracle approach. In Wellington et al (2006),
hand-aligned data are used to compare the standard
ITG constraints to ITGs that allow gaps.
8 Conclusions
We have presented a training-independent method
to compare different reordering constraints for ma-
chine translation. Given a sentence in foreign word
order, its reference translation(s) and reordering
constraints, our dynamic-programming algorithms
efficiently find the oracle reordering that has the ap-
proximately highest BLEU score. This allows eval-
uating different reordering constraints experimen-
tally, but abstracting away from specific features,
the probability model or training methods of the re-
ordering strategies. The presented method evaluates
the theoretical capabilities of reordering constraints,
as opposed to more arbitrary accuracies of specifi-
cally trained instances of reordering models.
Using our oracle method, we presented an em-
pirical evaluation of different reordering constraints
for a German-English translation task. The results
show that a good reordering of a given monotone
translation can improve the translation quality dra-
matically. Both short- and long-distance reorderings
contribute to the BLEU score improvements, which
are generally greater for longer sentences. Reorder-
ing constraints that allow global reorderings tend
to reach better oracles scores than ones that search
more locally. The ITG constraints and the IBM(4)
constraints both give the highest oracle scores.
The presented BLEU decoder algorithms can be
useful in many ways: They can generally help de-
cide what reordering constraints to choose for a
given translation system. They can be used for
discriminative training of reordering models (Till-
mann and Zhang, 2006). Furthermore, they can help
detecting insufficient parameterization or incapable
training algorithms: If two trained reordering model
instances show similar performances on a given task,
but the oracle scores differ greatly then the training
methods might not be optimal.
Acknowledgments
This work was partially supported by the National
Science Foundation via an ITR grant (No 0121285),
the Defense Advanced Research Projects Agency
via a GALE contract (No HR0011-06-2-0001), and
the Office of Naval Research via a MURI grant (No
N00014-01-1-0685). We thank Jason Eisner, David
Smith, Roy Tromble and the anonymous reviewers
for helpful comments and suggestions.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing,
Translation, and Compiling. Prentice Hall.
A.L. Berger P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
J. R. Gillett, J. D. Lafferty, R. L. Mercer, H. Printz, and
L. Ures. 1996. Language translation apparatus and method
using context-based translation models. United States Patent
No. 5,510,981.
E.W. Dijkstra. 1959. A note on two problems in connexion
with graphs. Numerische Mathematik., 1:269?271.
J. Eisner and R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations in Ma-
chine Translation. In Proc. of the Workshop on Computa-
tionally Hard Problems and Joint Inference, New York.
L. Huang and D. Chiang. 2005. Better k-best parsing. In Proc.
of IWPT, Vancouver, B.C., Canada.
S. Kumar and W. Byrne. 2005. Local phrase reordering
models for Statistical Machine Translation. In Proc. of
HLT/EMNLP, pages 161?168, Vancouver, B.C., Canada.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for Statistical MT. In Proc. of ACL, pages
721?728, Sydney, Australia.
B. Wellington, S. Waxmonsky, and D. Melamed. 2006. Empir-
ical lower bounds on the complexity of translational equiv-
alence. In Proc. of COLING-ACL, pages 977?984, Sydney,
Australia.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?404.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based
phrase reordering model for Statistical Machine Translation.
In Proc. of COLING-ACL, pages 521?528, Sydney, Aus-
tralia.
R. Zens and H. Ney. 2003. A comparative study on reordering
constraints in Statistical Machine Translation. In Proc. of
ACL, pages 144?151, Sapporo, Japan.
R. Zens and H. Ney. 2005. Word graphs for Statistical Machine
Translation. In Proc. of the ACL Workshop on Building and
Using Parallel Texts, pages 191?198, Ann Arbor, MI.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Reorder-
ing constraints for phrase-based Statistical Machine Transla-
tion. In Proc. of CoLing, pages 205?211, Geneva.
110
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 616?627,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Discovering Morphological Paradigms from Plain Text
Using a Dirichlet Process Mixture Model
Markus Dreyer?
SDL Language Weaver
Los Angeles, CA 90045, USA
mdreyer@sdl.com
Jason Eisner
Computer Science Dept., Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Abstract
We present an inference algorithm that orga-
nizes observed words (tokens) into structured
inflectional paradigms (types). It also natu-
rally predicts the spelling of unobserved forms
that are missing from these paradigms, and dis-
covers inflectional principles (grammar) that
generalize to wholly unobserved words.
Our Bayesian generative model of the data ex-
plicitly represents tokens, types, inflections,
paradigms, and locally conditioned string edits.
It assumes that inflected word tokens are gen-
erated from an infinite mixture of inflectional
paradigms (string tuples). Each paradigm is
sampled all at once from a graphical model,
whose potential functions are weighted finite-
state transducers with language-specific param-
eters to be learned. These assumptions natu-
rally lead to an elegant empirical Bayes infer-
ence procedure that exploits Monte Carlo EM,
belief propagation, and dynamic programming.
Given 50?100 seed paradigms, adding a 10-
million-word corpus reduces prediction error
for morphological inflections by up to 10%.
1 Introduction
1.1 Motivation
Statistical NLP can be difficult for morphologically
rich languages. Morphological transformations on
words increase the size of the observed vocabulary,
which unfortunately masks important generalizations.
In Polish, for example, each lexical verb has literally
100 inflected forms (Janecki, 2000). That is, a single
lexeme may be realized in a corpus as many different
word types, which are differently inflected for person,
number, gender, tense, mood, etc.
? This research was done at Johns Hopkins University as
part of the first author?s dissertation work. It was supported by
the Human Language Technology Center of Excellence and by
the National Science Foundation under Grant No. 0347822.
All this makes lexical features even sparser than
they would be otherwise. In machine translation
or text generation, it is difficult to learn separately
how to translate, or when to generate, each of these
many word types. In text analysis, it is difficult to
learn lexical features (as cues to predict topic, syntax,
semantics, or the next word), because one must learn
a separate feature for each word form, rather than
generalizing across inflections.
Our engineering goal is to address these problems
by mostly-unsupervised learning of morphology. Our
linguistic goal is to build a generative probabilistic
model that directly captures the basic representations
and relationships assumed by morphologists. This
model suffices to define a posterior distribution over
analyses of any given collection of type and/or token
data. Thus we obtain scientific data interpretation as
probabilistic inference (Jaynes, 2003). Our computa-
tional goal is to estimate this posterior distribution.
1.2 What is Estimated
Our inference algorithm jointly reconstructs token,
type, and grammar information about a language?s
morphology. This has not previously been attempted.
Tokens: We will tag each word token in a corpus
with (1) a part-of-speech (POS) tag,1 (2) an inflection,
and (3) a lexeme. A token of broken might be tagged
as (1) a VERB and more specifically as (2) the past
participle inflection of (3) the abstract lexeme b&r?a?k.2
Reconstructing the latent lexemes and inflections
allows the features of other statistical models to con-
sider them. A parser may care that broken is a
past participle; a search engine or question answer-
ing system may care that it is a form of b&r?a?k; and a
translation system may care about both facts.
1POS tagging may be done as part of our Bayesian model or
beforehand, as a preprocessing step. Our experiments chose the
latter option, and then analyzed only the verbs (see section 8).
2We use cursive font for abstract lexemes to emphasize that
they are atomic objects that do not decompose into letters.
616
singular plural
present 1st-person breche brechen
2nd-person brichst brecht
3rd-person bricht brechen
past 1st-person brach brachen
2nd-person brachst bracht
3rd-person brach brachen
Table 1: Part of a morphological paradigm in German,
showing the spellings of some inflections of the lexeme
b&r?a?k (whose lemma is brechen), organized in a grid.
Types: In carrying out the above, we will recon-
struct specific morphological paradigms of the lan-
guage. A paradigm is a grid of all the inflected forms
of some lexeme, as illustrated in Table 1. Our recon-
structed paradigms will include our predictions of
inflected forms that were never observed in the cor-
pus. This tabular information about the types (rather
than the tokens) of the language may be separately
useful, for example in translation and other genera-
tion tasks, and we will evaluate its accuracy.
Grammar: We estimate parameters ~? that de-
scribe general patterns in the language. We learn
a prior distribution over inflectional paradigms by
learning (e.g.) how a verb?s suffix or stem vowel
tends to change when it is pluralized. We also learn
(e.g.) whether singular or plural forms are more com-
mon. Our basic strategy is Monte Carlo EM, so these
parameters tell us how to guess the paradigms (Monte
Carlo E step), then these reconstructed paradigms tell
us how to reestimate the parameters (M step), and so
on iteratively. We use a few supervised paradigms to
initialize the parameters and help reestimate them.
2 Overview of the Model
We begin by sketching the main ideas of our model,
first reviewing components that we introduced in
earlier papers. Sections 5?7 will give more formal
details. Full details and more discussion can be found
in the first author?s dissertation (Dreyer, 2011).
2.1 Modeling Morphological Alternations
We begin with a family of joint distributions p(x, y)
over string pairs, parameterized by ~?. For example,
to model just the semi-systematic relation between a
German lemma and its 3rd-person singular present
form, one could train ~? to maximize the likelihood
of (x, y) pairs such as (brechen, bricht). Then,
given a lemma x, one could predict its inflected form
y via p(y | x), and vice-versa.
Dreyer et al (2008) define such a family via a
log-linear model with latent alignments,
p(x, y) =
?
a
p(x, y, a) ?
?
a
exp(~? ? ~f(x, y, a))
Here a ranges over monotonic 1-to-1 character align-
ments between x and y. ?means ?proportional to? (p
is normalized to sum to 1). ~f extracts a vector of local
features from the aligned pair by examining trigram
windows. Thus ~? can reward or penalize specific
features?e.g., insertions, deletions, or substitutions
in specific contexts, as well as trigram features of x
and y separately.3 Inference and training are done by
dynamic programming on finite-state transducers.
2.2 Modeling Morphological Paradigms
A paradigm such as Table 1 describes how some ab-
stract lexeme (b&r?a?k) is expressed in German.4 We
evaluate whole paradigms as linguistic objects, fol-
lowing word-and-paradigm or realizational morphol-
ogy (Matthews, 1972; Stump, 2001). That is, we pre-
sume that some language-specific distribution p(pi)
defines whether a paradigm pi is a grammatical?and
a priori likely?way for a lexeme to express itself
in the language. Learning p(pi) helps us reconstruct
paradigms, as described at the end of section 1.2.
Let pi = (x1, x2, . . .). In Dreyer and Eisner (2009),
we showed how to model p(pi) as a renormalized
product of many pairwise distributions prs(xr, xs),
each having the log-linear form of section 2.1:
p(pi) ?
?
r,s
prs(xr, xs) ? exp(
?
r,s
~????frs(xr, xs, ars))
This is an undirected graphical model (MRF) over
string-valued random variables xs; each factor prs
evaluates the relationship between some pair of
strings. Note that it is still a log-linear model, and pa-
rameters in ~? can be reused across different rs pairs.
To guess at unknown strings in the paradigm,
Dreyer and Eisner (2009) show how to perform ap-
proximate inference on such an MRF by loopy belief
3Dreyer et al (2008) devise additional helpful features based
on enriching the aligned pair with additional latent information,
but our present experiments drop those for speed.
4Our present experiments focus on orthographic forms, be-
cause we are learning from a written corpus. But it would be
natural to use phonological forms instead, or to include both in
the paradigm so as to model their interrelationships.
617
X1pl
X
2pl
X
3pl
X
Lem
X
1sg
X
2sg
X
3sg
brichen
brechen
 ...
?
brichen
brechen
 ...
?
bricht
brecht
 ...
?
briche
breche
 ...
?
brichst
brechst
 ...
?
bricht
brechen
Figure 1: A distribution over paradigms modeled as an
MRF over 7 strings. Random variables XLem, X1st, etc.,
are the lemma, the 1st person form, etc. Suppose two
forms are observed (denoted by the ?lock? icon). Given
these observations, belief propagation estimates the poste-
rior marginals over the other variables (denoted by ???).
propagation, using finite-state operations. It is not
necessary to include all rs pairs. For example, Fig. 1
illustrates the result of belief propagation on a simple
MRF whose factors relate all inflected forms to a
common (possibly unobserved) lemma, but not di-
rectly to one another.5
Our method could be used with any p(pi). To speed
up inference (see footnote 7), our present experiments
actually use the directed graphical model variant of
Fig. 1?that is, p(pi) = p1(x1) ??s>1 p1s(xs | x1),
where x1 denotes the lemma.
2.3 Modeling the Lexicon (types)
Dreyer and Eisner (2009) learned ~? by partially ob-
serving some paradigms (type data). That work,
while rather accurate at predicting inflected forms,
sometimes erred: it predicted spellings that never oc-
curred in text, even for forms that ?should? be com-
mon. To fix this, we shall incorporate an unlabeled
or POS-tagged corpus (token data) into learning.
We therefore need a model for generating tokens?
a probabilistic lexicon that specifies which inflections
of which lexemes are common, and how they are
spelled. We do not know our language?s probabilistic
lexicon, but we assume it was generated as follows:
1. Choose parameters ~? of the MRF. This defines
p(pi): which paradigms are likely a priori.
2. Choose a distribution over the abstract lexemes.
5This view is adopted by some morphological theorists (Al-
bright, 2002; Chan, 2006), although see Appendix E.2 for a
caution about syncretism. Note that when the lemma is unob-
served, the other forms do still influence one another indirectly.
3. For each lexeme, choose a distribution over its
inflections.
4. For each lexeme, choose a paradigm that will
be used to express the lexeme orthographically.
Details are given later. Briefly, step 1 samples ~?
from a Gaussian prior. Step 2 samples a distribution
from a Dirichlet process. This chooses a countable
number of lexemes to have positive probability in the
language, and decides which ones are most common.
Step 3 samples a distribution from a Dirichlet. For
the lexeme t?h?i?n?k, this might choose to make 1st-
person singular more common than for typical verbs.
Step 4 just samples IID from p(pi).
In our model, each part of speech generates its own
lexicon: VERBs are inflected differently from NOUNs
(different parameters and number of inflections). The
size and layout of (e.g.) VERB paradigms is language-
specific; we currently assume it is given by a linguist,
along with a few supervised VERB paradigms.
2.4 Modeling the Corpus (tokens)
At present, we use only a very simple exchangeable
model of the corpus. We assume that each word was
independently sampled from the lexicon given its
part of speech, with no other attention to context.
For example, a token of brechen may have been
chosen by choosing frequent lexeme b&r?a?k from the
VERB lexicon; then choosing 1st-person plural given
b&r?a?k; and finally looking up that inflection?s spelling
in b&r?a?k?s paradigm. This final lookup is determinis-
tic since the lexicon has already been generated.
3 A Sketch of Inference and Learning
3.1 Gibbs Sampling Over the Corpus
Our job in inference is to reconstruct the lexicon that
was used and how each token was generated from it
(i.e., which lexeme and inflection?). We use collapsed
Gibbs sampling, repeatedly guessing a reanalysis of
each token in the context of all others. Gradually, sim-
ilar tokens get ?clustered? into paradigms (section 4).
The state of the sampler is illustrated in Fig. 2.
The bottom half shows the current analyses of the
verb tokens. Each is associated with a particular slot
in some paradigm. We are now trying to reanalyze
brechen at position ?. The dashed arrows show
some possible analyses.
618
singular
plural
1st
2nd
3rd
bricht
brichst
brechst
...
?
briche
breche
...
?
brechen
bricht
brecht
...
?
brichen
brechen
...
?
springst
sprengst
...
?
springe
sprenge
...
?
springt
sprengt
...
?
springen
sprengen
...
springen
sprengen
...
1
3
Index i
2
PRON
4
NOUN
6
PREP
POS ti
Infl si
Spell wi
Lex ?i
1
VERB
3rd sg.
bricht
5
VERB
2nd pl.
springt
3
VERB
3rd pl.
brechen
?
?
5
Lexicon
Corpus
?
...
?
?
springt
7
VERB
1st pl.
...
brechen... ... ...
3rd pl.
Figure 2: A state of the Gibbs sampler (note that the
assumed generative process runs roughly top-to-bottom).
Each corpus token i has been tagged with part of speech ti,
lexeme `i and inflection si. Token ? has been tagged as
b&r?a?k and 3rd sg., which locked the corresponding type
spelling in the paradigm to the spelling w1 = bricht;
similarly for ? and ?. Now w7 is about to be reanalyzed.
The key intuition is that the current analyses of the
other verb tokens imply a posterior distribution over
the VERB lexicon, shown in the top half of the figure.
First, because of the current analyses of ? and ?,
the 3rd-person spellings of b&r?a?k are already con-
strained to match w1 and w3 (the ?lock? icon).
Second, belief propagation as in Fig. 1 tells us
which other inflections of b&r?a?k (the ??? icon) are
plausibly spelled as brechen, and how likely they
are to be spelled that way.
Finally, the fact that other tokens are associated
with b&r?a?k suggest that this is a popular lexeme, mak-
ing it a plausible explanation of ? as well. (This is
the ?rich get richer? property of the Chinese restau-
rant process; see section 6.6.) Furthermore, certain
inflections of b&r?a?k appear to be especially popular.
In short, given the other analyses, we know which
inflected lexemes in the lexicon are likely, and how
likely each one is to be spelled as brechen. This lets
us compute the relative probabilities of the possible
analyses of token ?, so that the Gibbs sampler can
accordingly choose one of these analyses at random.
3.2 Monte Carlo EM Training of ~?
For a given ~?, this Gibbs sampler converges to the
posterior distribution over analyses of the full corpus.
To improve our ~? estimate, we periodically adjust ~?
to maximize or increase the probability of the most
recent sample(s). For example, having tagged w5 =
springt as s5 = 2nd-person plural may strengthen
our estimated probability that 2nd-person spellings
tend to end in -t. That revision to ~?, in turn, will
influence future moves of the sampler.
If the sampler is run long enough between calls to
the ~? optimizer, this is a Monte Carlo EM procedure
(see end of section 1.2). It uses the data to optimize a
language-specific prior p(pi) over paradigms?an em-
pirical Bayes approach. (A fully Bayesian approach
would resample ~? as part of the Gibbs sampler.)
3.3 Collapsed Representation of the Lexicon
The lexicon is collapsed out of our sampler, in the
sense that we do not represent a single guess about the
infinitely many lexeme probabilities and paradigms.
What we store about the lexicon is information about
its full posterior distribution: the top half of Fig. 2.
Fig. 2 names its lexemes as b&r?a?k and ?j?u?m?p for ex-
pository purposes, but of course the sampler cannot
reconstruct such labels. Formally, these labels are col-
lapsed out, and we represent lexemes as anonymous
objects. Tokens ? and ? are tagged with the same
anonymous lexeme (which will correspond to sitting
at the same table in a Chinese restaurant process).
For each lexeme ` and inflection s, we maintain
pointers to any tokens currently tagged with the slot
(`, s). We also maintain an approximate marginal
distribution over the spelling of that slot:6
1. If (`, s) points to at least one token i, then we
know (`, s) is spelled as wi (with probability 1).
2. Otherwise, the spelling of (`, s) is not known.
But if some spellings in `?s paradigm are known,
store a truncated distribution that enumerates the
25 most likely spellings for (`, s), according to
loopy belief propagation within the paradigm.
3. Otherwise, we have observed nothing about `:
it is currently unused. All such ` share the same
marginal distribution over spellings of (`, s):
the marginal of the prior p(pi). Here a 25-best
list could not cover all plausible spellings. In-
stead we store a probabilistic finite-state lan-
guage model that approximates this marginal.7
6Cases 1 and 2 below must in general be updated whenever
a slot switches between having 0 and more than 0 tokens. Cases
2 and 3 must be updated when the parameters ~? change.
7This character trigram model is fast to build if p(pi) is de-
619
A hash table based on cases 1 and 2 can now be
used to rapidly map any word w to a list of slots of
existing lexemes that might plausibly have generated
w. To ask whether w might instead be an inflection s
of a novel lexeme, we score w using the probabilistic
finite-state automata from case 3, one for each s.
The Gibbs sampler randomly chooses one of these
analyses. If it chooses the ?novel lexeme? option,
we create an arbitrary new lexeme object in mem-
ory. The number of explicitly represented lexemes is
always finite (at most the number of corpus tokens).
4 Interpretation as a Mixture Model
It is common to cluster points in Rn by assuming
that they were generated from a mixture of Gaussians,
and trying to reconstruct which points were generated
from the same Gaussian.
We are similarly clustering word tokens by assum-
ing that they are generated from a mixture of weighted
paradigms. After all, each word token was obtained
by randomly sampling a weighted paradigm (i.e., a
cluster) and then randomly sampling a word from it.
Just as each Gaussian in a Gaussian mixture is
a distribution over all points Rn, each weighted
paradigm is a distribution over all spellings ?? (but
assigns probability > 0 to only a finite subset of ??).
Inference under our model clusters words together
by tagging them with the same lexeme. It tends to
group words that are ?similar? in the sense that the
base distribution p(pi) predicts that they would tend
to co-occur within a paradigm. Suppose a corpus
contains several unlikely but similar tokens, such
as discombobulated and discombobulating.
A language might have one probable lexeme from
whose paradigm all these words were sampled. It is
much less likely to have several probable lexemes that
all coincidentally chose spellings that started with
discombobulat-. Generating discombobulat-
only once is cheaper (especially for such a long pre-
fix), so the former explanation has higher probability.
This is like explaining nearby points in Rn as sam-
ples from the same Gaussian. Of course, our model
is sensitive to more than shared prefixes, and it does
not merely cluster words into a paradigm but assigns
them to particular inflectional slots in the paradigm.
fined as at the end of section 2.2. If not, one could still try belief
propagation; or one could approximate by estimating a language
model from the spellings associated with slot s by cases 1 and 2.
4.1 The Dirichlet Process Mixture Model
Our mixture model uses an infinite number of mix-
ture components. This avoids placing a prior bound
on the number of lexemes or paradigms in the lan-
guage. We assume that a natural language has an
infinite lexicon, although most lexemes have suffi-
ciently low probability that they have not been used
in our training corpus or even in human history (yet).
Our specific approach corresponds to a Bayesian
technique, the Dirichlet process mixture model. Ap-
pendix A (supplementary material) explains the
DPMM and discusses it in our context.
The DPMM would standardly be presented as gen-
erating a distribution over countably many Gaussians
or paradigms. Our variant in section 2.3 instead broke
this into two steps: it first generated a distribution
over countably many lexemes (step 2), and then gen-
erated a weighted paradigm for each lexeme (steps
3?4). This construction keeps distinct lexemes sepa-
rate even if they happen to have identical paradigms
(polysemy). See Appendix A for a full discussion.
5 Formal Notation
5.1 Value Types
We now describe our probability model in more for-
mal detail. It considers the following types of mathe-
matical objects. (We use consistent lowercase letters
for values of these types, and consistent fonts for
constants of these types.)
A word w, such as broken, is a finite string of
any length, over some finite, given alphabet ?.
A part-of-speech tag t, such as VERB, is an ele-
ment of a certain finite set T , which in this paper we
assume to be given.
An inflection s,8 such as past participle, is an ele-
ment of a finite set St. A token?s part-of-speech tag
t ? T determines its set St of possible inflections.
For tags that do not inflect, |St| = 1. The sets St
are language-specific, and we assume in this paper
that they are given by a linguist rather than learned.
A linguist also specifies features of the inflections:
the grid layout in Table 1 shows that 4 of the 12
inflections in SVERB share the ?2nd-person? feature.
8We denote inflections by s because they represent ?slots? in
paradigms (or, in the metaphor of section 6.7, ?seats? at tables in
a Chinese restaurant). These slots (or seats) are filled by words.
620
A paradigm for t ? T is a mapping pi : St ? ??,
specifying a spelling for each inflection in St. Table 1
shows one VERB paradigm.
A lexeme ` is an abstract element of some lexical
space L. Lexemes have no internal semantic struc-
ture: the only question we can ask about a lexeme is
whether it is equal to some other lexeme. There is no
upper bound on how many lexemes can be discovered
in a text corpus; L is infinite.
5.2 Random Quantities
Our generative model of the corpus is a joint probabil-
ity distribution over a collection of random variables.
We describe them in the same order as section 1.2.
Tokens: The corpus is represented by token vari-
ables. In our setting the sequence of words ~w =
w1, . . . , wn ? ?? is observed, along with n. We
must recover the corresponding part-of-speech tags
~t = t1, . . . , tn ? T , lexemes ~` = `1, . . . , `n ? L,
and inflections ~s = s1, . . . , sn, where (?i)si ? Sti .
Types: The lexicon is represented by type
variables. For each of the infinitely many lex-
emes ` ? L, and each t ? T , the paradigm
pit,` is a function St ? ??. For example,
Table 1 shows a possible value piVERB,b&r?a?k.The various spellings in the paradigm, such as
piVERB,b&r?a?k(1st-person sing. pres.)=breche, arestring-valued random variables that are correlated
with one another.
Since the lexicon is to be probabilistic (section 2.3),
Gt(`) denotes tag t?s distribution over lexemes ` ?
L, while Ht,`(s) denotes the tagged lexeme (t, `)?s
distribution over inflections s ? St.
Grammar: Global properties of the language are
captured by grammar variables that cut across lex-
ical entries: our parameters ~? that describe typical
inflectional alternations, plus parameters ~?t, ?t, ??t, ~?
(explained below). Their values control the overall
shape of the probabilistic lexicon that is generated.
6 The Formal Generative Model
We now fully describe the generative process that
was sketched in section 2. Step by step, it randomly
chooses an assignment to all the random variables of
section 5.2. Thus, a given assignment?s probability?
which section 3?s algorithms consult in order to re-
sample or improve the current assignment?is the
product of the probabilities of the individual choices,
as described in the sections below. (Appendix B
provides a drawing of this as a graphical model.)
6.1 Grammar Variables p(~?), p(???t), p(?t), p(??t)
First select the grammar variables from a prior. (We
will see below how these variables get used.) Our
experiments used fairly flat priors. Each weight in ~?
or ???t is drawn IID from N (0, 10), and each ?t or ??t
from a Gamma with mode 10 and variance 1000.
6.2 Paradigms p(pit,` | ~?)
For each t ? T , let Dt(pi) denote the distribution
over paradigms that was presented in section 2.2
(where it was called p(pi)). Dt is fully specified by
our graphical model for paradigms of part of speech
t, together with its parameters ~? as generated above.
This is the linguistic core of our model. It consid-
ers spellings: DVERB describes what verb paradigms
typically look like in the language (e.g., Table 1).
Parameters in ~? may be shared across parts of
speech t. These ?backoff? parameters capture gen-
eral phonotactics of the language, such as prohibited
letter bigrams or plausible vowel changes.
For each possible tagged lexeme (t, `), we now
draw a paradigm pit,` fromDt. Most of these lexemes
will end up having probability 0 in the language.
6.3 Lexical Distributions p(Gt | ?t)
We now formalize section 2.3. For each t ? T , the
language has a distribution Gt(`) over lexemes. We
draw Gt from a Dirichlet process DP(G,?t), where
G is the base distribution over L, and ?t > 0 is
a concentration parameter generated above. If ?t
is small, then Gt will tend to have the property that
most of its probability mass falls on relatively few
of the lexemes in Lt def= {` ? L : Gt(`) > 0}. A
closed-class tag is one whose ?t is especially small.
For G to be a uniform distribution over an infinite
lexeme set L, we need L to be uncountable.9 How-
ever, it turns out10 that with probability 1, each Lt
is countably infinite, and all the Lt are disjoint. So
each lexeme ` ? L is selected by at most one tag t.
9For example, L def= [0, 1], so that b&r?a?k is merely a sugges-
tive nickname for a lexeme such as 0.2538159.
10This can be seen by considering the stick-breaking construc-
tion of the Dirichlet process that (Sethuraman, 1994; Teh et al,
2006). A separate stick is broken for each Gt. See Appendix A.
621
6.4 Inflectional Distributions p(Ht,` | ???t, ??t)
For each tagged lexeme (t, `), the language specifies
some distribution Ht,` over its inflections.
First we construct backoff distributions Ht that are
independent of `. For each tag t ? T , let Ht be some
base distribution over St. As St could be large in
some languages, we exploit its grid structure (Table 1)
to reduce the number of parameters of Ht. We take
Ht to be a log-linear distribution with parameters ???t
that refer to features of inflections. E.g., the 2nd-
person inflections might be systematically rare.
Now we model each Ht,` as an independent draw
from a finite-dimensional Dirichlet distribution with
meanHt and concentration parameter ??t. E.g., t?h?i?n?k
might be biased toward 1st-person sing. present.
6.5 Part-of-Speech Tag Sequence p(~t | ~?)
In our current experiments, ~t is given. But in general,
to discover tags and inflections simultaneously, we
can suppose that the tag sequence ~t (and its length n)
are generated by a Markov model, with tag bigram or
trigram probabilities specified by some parameters ~? .
6.6 Lexemes p(`i | Gti)
We turn to section 2.4. A lexeme token depends on
its tag: draw `i from Gti , so p(`i | Gti) = Gti(`i).
6.7 Inflections p(si | Hti,`i)
An inflection slot depends on its tagged lexeme: we
draw si from Hti,`i , so p(si | Hti,`i) = Hti,`i(si).
6.8 Spell-out p(wi | piti,`i(si))
Finally, we generate the word wi through a determin-
istic spell-out step.11 Given the tag, lexeme, and in-
flection at position i, we generate the word wi simply
by looking up its spelling in the appropriate paradigm.
So p(wi | piti,`i(si)) is 1 if wi = piti,`i(si), else 0.
6.9 Collapsing the Assignment
Again, a full assignment?s probability is the product
of all the above factors (see drawing in Appendix B).
11To account for typographical errors in the corpus, the spell-
out process could easily be made nondeterministic, with the
observed word wi derived from the correct spelling piti,`i(si)
by a noisy channel model (e.g., (Toutanova and Moore, 2002))
represented as a WFST. This would make it possible to analyze
brkoen as a misspelling of a common or contextually likely
word, rather than treating it as an unpronounceable, irregularly
inflected neologism, which is presumably less likely.
But computationally, our sampler?s state leaves the
Gt unspecified. So its probability is the integral of
p(assignment) over all possible Gt. As Gt appears
only in the factors from headings 6.3 and 6.6, we can
just integrate it out of their product, to get a collapsed
sub-model that generates p(~` | ~t, ~?) directly:
?
GADJ
? ? ?
?
GVERB
dG
(?
t?T
p(Gt | ?t)
)( n?
i=1
p(`i | Gti)
)
= p(~` | ~t, ~?) =
n?
i=1
p(`i | `1, . . . `i?1 ~t, ~?)
where it turns out that the factor that generates `i is
proportional to |{j < i : `j = `i and tj = ti}| if that
integer is positive, else proportional to ?tiG(`i).
Metaphorically, each tag t is a Chinese restaurant
whose tables are labeled with lexemes. The tokens
are hungry customers. Each customer i = 1, 2, . . . , n
enters restaurant ti in turn, and `i denotes the label
of the table she joins. She picks an occupied table
with probability proportional to the number of pre-
vious customers already there, or with probability
proportional to ?ti she starts a new table whose label
is drawn from G (it is novel with probability 1, since
G gives infinitesimal probability to each old label).
Similarly, we integrate out the infinitely many
lexeme-specific distributionsHt,` from the product of
6.4 and 6.7, replacing it by the collapsed distribution
p(~s | ~`,~t,???t,
??
??) [recall that ???t determines Ht]
=
n?
i=1
p(si | s1, . . . si?1, ~`,~t,
???t,
??
??)
where the factor for si is proportional to |{j < i :
sj = si and (tj , `j) = (ti, `i)}|+ ??tiHti(si).Metaphorically, each table ` in Chinese restaurant
t has a fixed, finite set of seats corresponding to the
inflections s ? St. Each seat is really a bench that
can hold any number of customers (tokens). When
customer i chooses to sit at table `i, she also chooses
a seat si at that table (see Fig. 2), choosing either an
already occupied seat with probability proportional to
the number of customers already in that seat, or else
a random seat (sampled from Hti and not necessarily
empty) with probability proportional to ??ti .
7 Inference and Learning
As section 3 explained, the learner alternates between
a Monte Carlo E step that uses Gibbs sampling to
622
sample from the posterior of (~s, ~`,~t) given ~w and the
grammar variables, and an M step that adjusts the
grammar variables to maximize the probability of the
(~w,~s, ~`,~t) samples given those variables.
7.1 Block Gibbs Sampling
As in Gibbs sampling for the DPMM, our sampler?s
basic move is to reanalyze token i (see section 3).
This corresponds to making customer i invisible and
then guessing where she is probably sitting?which
restaurant t, table `, and seat s??given knowledge
of wi and the locations of all other customers.12
Concretely, the sampler guesses location (ti, `i, si)
with probability proportional to the product of
? p(ti | ti?1, ti+1, ~?) (from section 6.5)
? the probability (from section 6.9) that a new cus-
tomer in restaurant ti chooses table `i, given the
other customers in that restaurant (and ?ti)13
? the probability (from section 6.9) that a new
customer at table `i chooses seat si, given the
other customers at that table (and ???ti and ??ti)13
? the probability (from section 3.3?s belief propa-
gation) that piti,`i(si) = wi (given ~?).
We sample only from the (ti, `i, si) candidates for
which the last factor is non-negligible. These are
found with the hash tables and FSAs of section 3.3.
7.2 Semi-Supervised Sampling
Our experiments also consider the semi-supervised
case where a few seed paradigms?type data?are
fully or partially observed. Our samples should also
be conditioned on these observations. We assume
that our supervised list of observed paradigms was
generated by sampling from Gt.14 We can modify
our setup for this case: certain tables have a host
who dictates the spelling of some seats and attracts
appropriate customers to the table. See Appendix C.
7.3 Parameter Gradients
Appendix D gives formulas for the M step gradients.
12Actually, to improve mixing time, we choose a currently
active lexeme ` uniformly at random, make all customers {i :
`i = `} invisible, and sequentially guess where they are sitting.
13This is simple to find thanks to the exchangeability of the
CRP, which lets us pretend that i entered the restaurant last.
14Implying that they are assigned to lexemes with non-
negligible probability. We would learn nothing from a list of
merely possible paradigms, since Lt is infinite and every con-
ceivable paradigm is assigned to some ` ? Lt (in fact many!).
50 seed paradigms 100 seed paradigms
Corpus size 0 106 107 0 106 107
Accuracy 89.9 90.6 90.9 91.5 92.0 92.2
Edit dist. 0.20 0.19 0.18 0.18 0.17 0.17
Table 2: Whole-word accuracy and edit distance of pre-
dicted inflection forms given the lemma. Edit distance to
the correct form is measured in characters. Best numbers
per set of seed paradigms in bold (statistically signifi-
cant on our large test set under a paired permutation test,
p < 0.05). Appendix E breaks down these results per
inflection and gives an error analysis and other statistics.
8 Experiments
8.1 Experimental Design
We evaluated how well our model learns German
verbal morphology. As corpus we used the first 1
million or 10 million words from WaCky (Baroni
et al, 2009). For seed and test paradigms we used
verbal inflectional paradigms from the CELEX mor-
phological database (Baayen et al, 1995). We fully
observed the seed paradigms. For each test paradigm,
we observed the lemma type (Appendix C) and eval-
uated how well the system completed the other 21
forms (see Appendix E.2) in the paradigm.
We simplified inference by fixing the POS tag
sequence to the automatic tags delivered with the
WaCky corpus. The result that we evaluated for each
variable was the value whose probability, averaged
over the entire Monte Carlo EM run,15 was highest.
For more details, see (Dreyer, 2011).
All results are averaged over 10 different train-
ing/test splits of the CELEX data. Each split sampled
100 paradigms as seed data and used the remain-
ing 5,415 paradigms for evaluation.16 From the 100
paradigms, we also sampled 50 to obtain results with
smaller seed data.17
8.2 Results
Type-based Evaluation. Table 2 shows the results
of predicting verb inflections, when running with no
corpus, versus with an unannotated corpus of size 106
and 107 words. Just using 50 seed paradigms, but
15This includes samples from before ~? has converged, some-
what like the voted perceptron (Freund and Schapire, 1999).
16100 further paradigms were held out for future use.
17Since these seed paradigms are sampled uniformly from a
set of CELEX paradigms, most of them are regular. We actually
only used 90 and 40 for training, reserving 10 as development
data for sanity checks and for deciding when to stop.
623
Bin Frequency # Verb Forms
1 0?9 116,776
2 10?99 4,623
3 100?999 1,048
4 1,000?9,999 95
5 10,000? 10
all any 122,552
Table 3: The inflected verb forms from 5,615 inflectional
paradigms, split into 5 token frequency bins. The frequen-
cies are based on the 10-million word corpus.
no corpus, gives an accuracy of 89.9%. By adding
a corpus of 10 million words we reduce the error
rate by 10%, corresponding to a one-point increase
in absolute accuracy to 90.9%. A similar trend can
be seen when we use more seed paradigms. Sim-
ply training on 100 seed paradigms, but not using a
corpus, results in an accuracy of 91.5%. Adding a
corpus of 10 million words to these 100 paradigms re-
duces the error rate by 8.3%, increasing the absolute
accuracy to 92.2%. Compared to the large corpus,
the smaller corpus of 1 million words goes more than
half the way; it results in error reductions of 6.9%
(50 seed paradigms) and 5.8% (100 seed paradigms).
Larger unsupervised corpora should help by increas-
ing coverage even more, although Zipf?s law implies
a diminishing rate of return.18
We also tested a baseline that simply inflects each
morphological form according to the basic regular
German inflection pattern; this reaches an accuracy
of only 84.5%.
Token-based Evaluation. We now split our results
into different bins: how well do we predict the
spellings of frequently expressed (lexeme, inflection)
pairs as opposed to rare ones? For example, the third
person singular indicative of ?g?i?v (geben) is used
significantly more often than the second person plural
subjunctive of b$a?s?k (aalen);19 they are in different
frequency bins (Table 3). The more frequent a form
is in text, the more likely it is to be irregular (Jurafsky
et al, 2000, p. 49).
The results in Table 4 show: Adding a corpus of
either 1 or 10 million words increases our prediction
accuracy across all frequency bins, often dramati-
cally. All methods do best on the huge number of
18Considering the 63,778 distinct spellings from all of our
5,615 CELEX paradigms, we find that the smaller corpus con-
tains 7,376 spellings and the 10? larger corpus contains 13,572.
19See Appendix F for how this was estimated from text.
50 seed paradigms 100 seed paradigms
Bin 0 106 107 0 106 107
1 90.5 91.0 91.3 92.1 92.4 92.6
2 78.1 84.5 84.4 80.2 85.5 85.1
3 71.6 79.3 78.1 73.3 80.2 79.1
4 57.4 61.4 61.8 57.4 62.0 59.9
5 20.7 25.0 25.0 20.7 25.0 25.0
all 52.6 57.5 57.8 53.4 58.5 57.8
all (e.d.) 1.18 1.07 1.03 1.16 1.02 1.01
Table 4: Token-based analysis: Whole-word accuracy re-
sults split into different frequency bins. In the last two
rows, all predictions are included, weighted by the fre-
quency of the form to predict. Last row is edit distance.
rare forms (Bin 1), which are mostly regular, and
worst on on the 10 most frequent forms of the lan-
guage (Bin 5). However, adding a corpus helps most
in fixing the errors in bins with more frequent and
hence more irregular verbs: in Bins 2?5 we observe
improvements of up to almost 8% absolute percent-
age points. In Bin 1, the no-corpus baseline is already
relatively strong.
Surprisingly, while we always observe gains from
using a corpus, the gains from the 10-million-word
corpus are sometimes smaller than the gains from the
1-million-word corpus, except in edit distance. Why?
The larger corpus mostly adds new infrequent types,
biasing ~? toward regular morphology at the expense
of irregular types. A solution might be to model irreg-
ular classes with separate parameters, using the latent
conjugation-class model of Dreyer et al (2008).
Note that, by using a corpus, we even improve
our prediction accuracy for forms and spellings that
are not found in the corpus, i.e., novel words. This
is thanks to improved grammar parameters. In the
token-based analysis above we have already seen that
prediction accuracy increases for rare forms (Bin 1).
We add two more analyses that more explicitly show
our performance on novel words. (a) We find all
paradigms that consist of novel spellings only, i.e.
none of the correct spellings can be found in the
corpus.20 The whole-word prediction accuracies for
the models that use corpus size 0, 1 million, and
10 million words are, respectively, 94.0%, 94.2%,
94.4% using 50 seed paradigms, and 95.1%, 95.3%,
95.2% using 100 seed paradigms. (b) Another, sim-
20This is measured on the largest corpus used in inference, the
10-million-word corpus, so that we can evaluate all models on
the same set of paradigms.
624
pler measure is the prediction accuracy on all forms
whose correct spelling cannot be found in the 10-
million-word corpus. Here we measure accuracies
of 91.6%, 91.8% and 91.8%, respectively, using 50
seed paradigms. With 100 seed paradigms, we have
93.0%, 93.4% and 93.1%. The accuracies for the
models that use a corpus are higher, but do not al-
ways steadily increase as we increase the corpus size.
The token-based analysis we have conducted here
shows the strength of the corpus-based approach pre-
sented in this paper. While the integrated graphi-
cal models over strings (Dreyer and Eisner, 2009)
can learn some basic morphology from the seed
paradigms, the added corpus plays an important role
in correcting its mistakes, especially for the more fre-
quent, irregular verb forms. For examples of specific
errors that the models make, see Appendix E.3.
9 Related Work
Our word-and-paradigm model seamlessly handles
nonconcatenative and concatenative morphology
alike, whereas most previous work in morphological
knowledge discovery has modeled concatenative mor-
phology only, assuming that the orthographic form
of a word can be split neatly into stem and affixes?a
simplifying asssumption that is convenient but often
not entirely appropriate (Kay, 1987) (how should one
segment English stopping, hoping, or knives?).
In concatenative work, Harris (1955) finds mor-
pheme boundaries and segments words accordingly,
an approach that was later refined by Hafer and
Weiss (1974), D?jean (1998), and many others. The
unsupervised segmentation task is tackled in the
annual Morpho Challenge (Kurimo et al, 2010),
where ParaMor (Monson et al, 2007) and Morfessor
(Creutz and Lagus, 2005) are influential contenders.
The Bayesian methods that Goldwater et al (2006b,
et seq.) use to segment between words might also be
applied to segment within words, but have no notion
of paradigms. Goldsmith (2001) finds what he calls
signatures?sets of affixes that are used with a given
set of stems, for example (NULL, -er, -ing, -s).
Chan (2006) learns sets of morphologically related
words; he calls these sets paradigms but notes that
they are not substructured entities, in contrast to the
paradigms we model in this paper. His models are
restricted to concatenative and regular morphology.
Morphology discovery approaches that han-
dle nonconcatenative and irregular phenomena
are more closely related to our work; they are
rarer. Yarowsky and Wicentowski (2000) identify
inflection-root pairs in large corpora without supervi-
sion. Using similarity as well as distributional clues,
they identify even irregular pairs like take/took.
Schone and Jurafsky (2001) and Baroni et al (2002)
extract whole conflation sets, like ?abuse, abused,
abuses, abusive, abusively, . . . ,? which may
also be irregular. We advance this work by not only
extracting pairs or sets of related observed words,
but whole structured inflectional paradigms, in which
we can also predict forms that have never been ob-
served. On the other hand, our present model does
not yet use contextual information; we regard this as
a future opportunity (see Appendix G). Naradowsky
and Goldwater (2009) add simple spelling rules to
the Bayesian model of (Goldwater et al, 2006a), en-
abling it to handle some systematically nonconcate-
native cases. Our finite-state transducers can handle
more diverse morphological phenomena.
10 Conclusions and Future Work
We have formulated a principled framework for si-
multaneously obtaining morphological annotation,
an unbounded morphological lexicon that fills com-
plete structured morphological paradigms with ob-
served and predicted words, and parameters of a non-
concatenative generative morphology model.
We ran our sampler over a large corpus (10 million
words), inferring everything jointly and reducing the
prediction error for morphological inflections by up
to 10%. We observed that adding a corpus increases
the absolute prediction accuracy on frequently occur-
ring morphological forms by up to almost 8%. Future
extensions to the model could leverage token context
for further improvements (Appendix G).
We believe that a major goal of our field should be
to build full-scale explanatory probabilistic models
of language. While we focus here on inflectional
morphology and evaluate the results in isolation, we
regard the present work as a significant step toward
a larger generative model under which Bayesian
inference would reconstruct other relationships as
well (e.g., inflectional, derivational, and evolution-
ary) among the words in a family of languages.
625
References
A. C. Albright. 2002. The Identification of Bases in
Morphological Paradigms. Ph.D. thesis, University of
California, Los Angeles.
D. Aldous. 1985. Exchangeability and related topics.
?cole d??t? de probabilit?s de Saint-Flour XIII, pages
1?198.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric problems.
Annals of Statistics, 2(6):1152?1174.
R. H Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database (release 2)[cd-rom]. Philadel-
phia, PA: Linguistic Data Consortium, University of
Pennsylvania [Distributor].
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsupervised
discovery of morphologically related words based on
orthographic and semantic similarity. In Proc. of the
ACL-02 Workshop on Morphological and Phonological
Learning, pages 48?57.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A collection of very
large linguistically processed web-crawled corpora.
Language Resources and Evaluation, 43(3):209?226.
David Blackwell and James B. MacQueen. 1973. Fergu-
son distributions via P?lya urn schemes. The Annals of
Statistics, 1(2):353?355, March.
David M. Blei and Peter I. Frazier. 2010. Distance-
dependent Chinese restaurant processes. In Proc. of
ICML, pages 87?94.
E. Chan. 2006. Learning probabilistic paradigms for
morphology in a latent class model. In Proceedings of
the Eighth Meeting of the ACL Special Interest Group
on Computational Phonology at HLT-NAACL, pages
69?78.
M. Creutz and K. Lagus. 2005. Unsupervised morpheme
segmentation and morphology induction from text cor-
pora using Morfessor 1.0. Computer and Information
Science, Report A, 81.
H. D?jean. 1998. Morphemes as necessary concept
for structures discovery from untagged corpora. In
Proc. of the Joint Conferences on New Methods in
Language Processing and Computational Natural Lan-
guage Learning, pages 295?298.
Markus Dreyer and Jason Eisner. 2009. Graphical models
over multiple strings. In Proc. of EMNLP, Singapore,
August.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proc. of EMNLP, Honolulu,
Hawaii, October.
Markus Dreyer. 2011. A Non-Parametric Model for the
Discovery of Inflectional Paradigms from Plain Text
Using Graphical Models over Strings. Ph.D. thesis,
Johns Hopkins University.
T.S. Ferguson. 1973. A Bayesian analysis of some non-
parametric problems. The annals of statistics, 1(2):209?
230.
Y. Freund and R. Schapire. 1999. Large margin classifica-
tion using the perceptron algorithm. Machine Learning,
37(3):277?296.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguis-
tics, 27(2):153?198.
S. Goldwater, T. Griffiths, and M. Johnson. 2006a. In-
terpolating between types and tokens by estimating
power-law generators. In Proc. of NIPS, volume 18,
pages 459?466.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006b.
Contextual dependencies in unsupervised word seg-
mentation. In Proc. of COLING-ACL.
P.J. Green. 1995. Reversible jump Markov chain Monte
Carlo computation and Bayesian model determination.
Biometrika, 82(4):711.
M. A Hafer and S. F Weiss. 1974. Word segmentation
by letter successor varieties. Information Storage and
Retrieval, 10:371?385.
Z. S. Harris. 1955. From phoneme to morpheme. Lan-
guage, 31(2):190?222.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771?1800.
Klara Janecki. 2000. 300 Polish Verbs. Barron?s Educa-
tional Series.
E. T. Jaynes. 2003. Probability Theory: The Logic of
Science. Cambridge Univ Press. Edited by Larry Bret-
thorst.
D. Jurafsky, J. H. Martin, A. Kehler, K. Vander Linden,
and N. Ward. 2000. Speech and Language Processing:
An Introduction to Natural Language Processing, Com-
putational Linguistics, and Speech Recognition. MIT
Press.
M. Kay. 1987. Nonconcatenative finite-state morphology.
In Proc. of EACL, pages 2?10.
M. Kurimo, S. Virpioja, V. Turunen, and K. Lagus. 2010.
Morpho Challenge competition 2005?2010: Evalua-
tions and results. In Proc. of ACL SIGMORPHON,
pages 87?95.
P. H. Matthews. 1972. Inflectional Morphology: A Theo-
retical Study Based on Aspects of Latin Verb Conjuga-
tion. Cambridge University Press.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2007. ParaMor: Minimally supervised induc-
tion of paradigm structure and morphological analysis.
In Proc. of ACL SIGMORPHON, pages 117?125, June.
626
J. Naradowsky and S. Goldwater. 2009. Improving mor-
phology induction by learning spelling rules. In Proc.
of IJCAI, pages 1531?1536.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordinator.
Annals of Probability, 25:855?900.
P. Schone and D. Jurafsky. 2001. Knowledge-free induc-
tion of inflectional morphologies. In Proc. of NAACL,
volume 183, pages 183?191.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4(2):639?650.
N. A. Smith, D. A. Smith, and R. W. Tromble. 2005.
Context-based morphological disambiguation with ran-
dom fields. In Proceedings of HLT-EMNLP, pages
475?482, October.
G. T. Stump. 2001. Inflectional Morphology: A Theory of
Paradigm Structure. Cambridge University Press.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101(476):1566?1581.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL.
K. Toutanova and R.C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Proc. of
ACL, pages 144?151.
D. Yarowsky and R. Wicentowski. 2000. Minimally su-
pervised morphological analysis by multimodal align-
ment. In Proc. of ACL, pages 207?216, October.
627
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 757?766,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Generation of landmark-based navigation instructions
from open-source data
Markus Dra?ger
Dept. of Computational Linguistics
Saarland University
mdraeger@coli.uni-saarland.de
Alexander Koller
Dept. of Linguistics
University of Potsdam
koller@ling.uni-potsdam.de
Abstract
We present a system for the real-time gen-
eration of car navigation instructions with
landmarks. Our system relies exclusively
on freely available map data from Open-
StreetMap, organizes its output to fit into
the available time until the next driving ma-
neuver, and reacts in real time to driving er-
rors. We show that female users spend sig-
nificantly less time looking away from the
road when using our system compared to a
baseline system.
1 Introduction
Systems that generate route instructions are be-
coming an increasingly interesting application
area for natural language generation (NLG) sys-
tems. Car navigation systems are ubiquitous
already, and with the increased availability of
powerful mobile devices, the wide-spread use of
pedestrian navigation systems is on the horizon.
One area in which NLG systems could improve
existing navigation systems is in the use of land-
marks, which would enable them to generate in-
structions such as ?turn right after the church? in-
stead of ?after 300 meters?. It has been shown in
human-human studies that landmark-based route
instructions are easier to understand (Lovelace
et al 1999) than distance-based ones and re-
duce driver distraction in in-car settings (Bur-
nett, 2000), which is crucial for improved traffic
safety (Stutts et al 2001). From an NLG per-
spective, navigation systems are an obvious ap-
plication area for situated generation, for which
there has recently been increasing interest (see
e.g. (Lessmann et al 2006; Koller et al 2010;
Striegnitz and Majda, 2009)).
Current commercial navigation systems use
only trivial NLG technology, and in particular are
limited to distance-based route instructions. Even
in academic research, there has been remarkably
little work on NLG for landmark-based naviga-
tion systems. Some of these systems rely on map
resources that have been hand-crafted for a par-
ticular city (Malaka et al 2004), or on a com-
bination of multiple complex resources (Raubal
and Winter, 2002), which effectively limits their
coverage. Others, such as Dale et al(2003), fo-
cus on non-interactive one-shot instruction dis-
courses. However, commercially successful car
navigation systems continuously monitor whether
the driver is following the instructions and pro-
vide modified instructions in real time when nec-
essary. That is, two key problems in designing
NLG systems for car navigation instructions are
the availability of suitable map resources and the
ability of the NLG system to generate instructions
and react to driving errors in real time.
In this paper, we explore solutions to both of
these points. We present the Virtual Co-Pilot,
a system which generates route instructions for
car navigation using landmarks that are extracted
from the open-source OpenStreetMap resource.1
The system computes a route plan and splits it
into episodes that end in driving maneuvers. It
then selects landmarks that describe the locations
of these driving maneuvers, and aggregates in-
structions such that they can be presented (via
a TTS system) in the time available within the
episode. The system monitors the user?s position
and computes new, corrective instructions when
the user leaves the intended path. We evaluate
our system using a driving simulator, and com-
pare it to a baseline that is designed to replicate
a typical commercial navigation system. The Vir-
tual Co-Pilot performs comparably to the baseline
1http://www.openstreetmap.org/
757
on the number of driving errors and on user sat-
isfaction, and outperforms it significantly on the
time female users spend looking away from the
road. To our knowledge, this is the first time that
the generation of landmarks has been shown to
significantly improve the instructions of a wide-
coverage navigation system.
Plan of the paper. We start by reviewing ear-
lier literature on landmarks, route instructions,
and the use of NLG for route instructions in Sec-
tion 2. We then present the way in which we
extract information on potential landmarks from
OpenStreetMap in Section 3. Section 4 shows
how we generate route instructions, and Section 5
presents the evaluation. Section 6 concludes.
2 Related Work
What makes an object in the environment a good
landmark has been the topic of research in vari-
ous disciplines, including cognitive science, com-
puter science, and urban planning. Lynch (1960)
defines landmarks as physical entities that serve
as external points of reference that stand out from
their surroundings. Kaplan (1976) specified a
landmark as ?a known place for which the in-
dividual has a well-formed representation?. Al-
though there are different definitions of land-
marks, a common theme is that objects are con-
sidered landmarks if they have some kind of cog-
nitive salience (both in terms of visual distinctive-
ness and frequeny of interaction).
The usefulness of landmarks in route instruc-
tions has been shown in a number of different
human-human studies. Experimental results from
Lovelace et al(1999) show that people not only
use landmarks intuitively when giving directions,
but they also perceive instructions that are given to
them to be of higher quality when those instruc-
tions contain landmark information. Similar find-
ings have also been reported by Michon and Denis
(2001) and Tom and Denis (2003).
Regarding car navigation systems specifically,
Burnett (2000) reports on a road-based user study
which compared a landmark-based navigation
system to a conventional car navigation system.
Here the provision of landmark information in
route directions led to a decrease of navigational
errors. Furthermore, glances at the navigation
display were shorter and fewer, which indicates
less driver distraction in this particular experi-
mental condition. Minimizing driver distraction
is a crucial goal of improved navigation systems,
as driver inattention of various kinds is a lead-
ing cause of traffic accidents (25% of all police-
reported car crashes in the US in 2000, according
to Stutts et al(2001)). Another road-based study
conducted by May and Ross (2006) yielded simi-
lar results.
One recurring finding in studies on landmarks
in navigation is that some user groups are able
to benefit more from their inclusion than oth-
ers. This is particularly the case for female users.
While men tend to outperform women in wayfind-
ing tasks, completing them faster and with fewer
navigation errors (c.f. Allen (2000)), women are
likely to show improved wayfinding performance
when landmark information is given (e.g. Saucier
et al(2002)).
Despite all of this evidence from human-human
studies, there has been remarkably little research
on implemented navigation systems that use land-
marks. Commercial systems make virtually no
use of landmark information when giving direc-
tions, relying on metric representations instead
(e.g. ?Turn right in one hundred meters?). In aca-
demic research, there have only been a handful of
relevant systems. A notable example is the DEEP
MAP system, which was created in the SmartKom
project as a mobile tourist information system for
the city of Heidelberg (Malaka and Zipf, 2000;
Malaka et al 2004). DEEP MAP uses landmarks
as waypoints for the planning of touristic routes
for car drivers and pedestrians, while also making
use of landmark information in the generation of
route directions. Raubal and Winter (2002) com-
bine data from digital city maps, facade images,
cultural heritage information, and other sources
to compute landmark descriptions that could be
used in a pedestrian navigation system for the city
of Vienna.
The key to the richness of these systems is a
set of extensive, manually curated geographic and
landmark databases. However, creation and main-
tenance of such databases is expensive, which
makes it impractical to use these systems outside
of the limited environments for which they were
created. There have been a number of suggestions
for automatically acquiring landmark data from
existing electronic databases, for instance cadas-
tral data (Elias, 2003) and airborne laser scans
(Brenner and Elias, 2003). But the raw data for
these approaches is still hard to obtain; informa-
758
tion about landmarks is mostly limited to geomet-
ric data and does not specify the semantic type
of a landmark (such as ?church?); and updating
the landmark database frequently when the real
world changes (e.g., a shop closes down) remains
an open issue.
The closest system in the literature to the re-
search we present here is the CORAL system
(Dale et al 2003). CORAL generates a text of
driving instructions with landmarks out of the out-
put of a commercial web-based route planner. Un-
like CORAL, our system relies purely on open-
source map data. Also, our system generates driv-
ing instructions in real time (as opposed to a sin-
gle discourse before the user starts driving) and
reacts in real time to driving errors. Finally, we
evaluate our system thoroughly for driving errors,
user satisfaction, and driver distraction on an ac-
tual driving task, and find a significant improve-
ment over the baseline.
3 OpenStreetMap
A system that generates landmark-based route di-
rections requires two kinds of data. First, it must
plan routes between points in space, and therefore
needs data on the road network, i.e. the road seg-
ments that make up streets along with their con-
nections. Second, the system needs information
about the landmarks that are present in the envi-
ronment. This includes geographic information
such as position, but also semantic information
such as the landmark type.
We have argued above that the availability of
such data has been a major bottleneck in the
development of landmark-based navigation sys-
tems. In the Virtual Co-Pilot system, which
we present below, we solve this problem by us-
ing data from OpenStreetMap, an on-line map
resource that provides both types of informa-
tion mentioned above, in a unified data struc-
ture. The OpenStreetMap project is to maps what
Wikipedia is to encyclopedias: It is a map of
the entire world which can be edited by anyone
wishing to participate. New map data is usually
added by volunteers who measure streets using
GPS devices and annotate them via a Web inter-
face. The decentralized nature of the data entry
process means that when the world changes, the
map will be updated quickly. Existing map data
can be viewed as a zoomable map on the Open-
StreetMap website, or it can be downloaded in an
Figure 1: A graphical representation of some nodes
and ways in OpenStreetMap.
Landmark Type
Street Furniture stop sign
traffic lights
pedestrian crossing
Visual Landmarks church
certain video stores
certain supermarkets
gas station
pubs and bars
Figure 2: Landmarks used by the Virtual Co-Pilot.
XML format for offline use.
Geographical data in OpenStreetMap is repre-
sented in terms of nodes and ways. Nodes rep-
resent points in space, defined by their latitude
and longitude. Ways consist of sequences of
edges between adjacent nodes; we call the in-
dividual edges segments below. They are used
to represent streets (with curved streets consist-
ing of multiple straight segments approximating
their shape), but also a variety of other real-world
entities: buildings, rivers, trees, etc. Nodes and
ways can both be enriched with further infor-
mation by attaching tags. Tags encode a wide
range of additional information using a predefined
type ontology. Among other things, they specify
the types of buildings (church, cafe, supermarket,
etc.); where a shop or restaurant has a name, it too
is specified in a tag. Fig. 1 is a graphical represen-
tation of some OpenStreetMap data, consisting of
nodes and ways for two streets (with two and five
segments) and a building which has been tagged
as a gas station.
For the Virtual Co-Pilot system, we have cho-
sen a set of concrete landmark types that we con-
sider useful (Fig. 2). We operationalize the crite-
ria for good landmarks sketched in Section 2 by
requiring that a landmark should be easily visible,
and that it should be generic in that it is appli-
759
cable not just for one particular city, but for any
place for which OpenStreetMap data is available.
We end up with two classes of landmark types:
street furniture and visual landmarks. Street fur-
niture is a generic term for objects that are in-
stalled on streets. In this subset, we include stop
signs, traffic lights, and pedestrian crossings. Our
assumption is that these objects inherently pos-
sess a high salience, since they already require
particular attention from the driver. ?Visual land-
marks? encompass roadside buildings that are not
directly connected to the road infrastructure, but
draw the driver?s attention due to visual salience.
Churches are an obvious member of this group; in
addition, we include gas stations, pubs, and bars,
as well as certain supermarket and video store
chains (selected for wide distribution over differ-
ent cities and recognizable, colorful signs).
Given a certain location at which the Virtual
Co-Pilot is to be used, we automatically extract
suitable landmarks along with their types and lo-
cations from OpenStreetMap. We also gather
the road network information that is required
for route planning, and collect informations on
streets, such as their names, from the tags. We
then transform this information into a directed
street graph. The nodes of this graph are the
OpenStreetMap nodes that are part of streets; two
adjacent nodes are connected by a single directed
edge for segments of one-way streets and a di-
rected edge in each direction for ordinary street
segments. Each edge is weighted with the Eu-
clidean distance between the two nodes.
4 Generation of route directions
We will now describe how the Virtual Co-Pilot
generates route directions from OpenStreetMap
data. The system generates three types of mes-
sages (see Fig. 3). First, at every decision point,
i.e. at the intersection where a driving maneu-
ver such as turning left or right is required, the
user is told to turn immediately in the given di-
rection (?now turn right?). Second, if the driver
has followed an instruction correctly, we gener-
ate a confirmation message after the driver has
made the turn, letting them know they are still
on the right track. Finally, we generate preview
messages on the street leading up to the decision
point. These preview messages describe the loca-
tion of the next driving maneuver.
Of the three types, preview messages are the
Figure 3: Schematic representation of an episode
(dashed red line), with sample trigger positions of pre-
view, turn instruction, and confirmation messages.
most interesting. Our system avoids the genera-
tion of metric distance indicators, as in ?turn left
in 100 meters?. Instead, it tries to find landmarks
that describe the position of the decision point:
?Prepare to turn left after the church.? When no
landmark is available, the system tries to use street
intersections as secondary landmarks, as in ?Turn
right at the next/second/third intersection.? Metric
distances are only used when both of these strate-
gies fail.
In-car NLG takes place in a heavily real-time
setting, in which an utterance becomes uninter-
pretable or even misleading if it is given too late.
This problem is exacerbated for NLG of speech
because simply speaking the utterance takes time
as well. One consequence that our system ad-
dresses is the problem of planning preview mes-
sages in such a way that they can be spoken be-
fore the decision point without overlapping each
other. We handle this problem in the sentence
planner, which may aggregate utterances to fit
into the available time. A second problem is that
the user?s reactions to the generated utterances are
unpredictable; if the driver takes a wrong turn, the
system must generate updated instructions in real
time.
Below, we describe the individual components
of the system. We mostly follow a standard NLG
pipeline (Reiter and Dale, 2000), with a focus on
the sentence planner and an extension to interac-
tive real-time NLG.
760
Segment123
From: Node1
To: Node2
On: ?Main Street?
Segment124
From: Node2
To: Node3
On: ?Main Street?
Segment125
From: Node3
To: Node4
On: ?Park Street?
Segment126
From: Node4
To: Node5
On: ?Park Street?
Figure 4: A simple example of a route plan consisting
of four street segments.
4.1 Content determination and text planning
The first step in our system is to obtain a plan for
reaching the destination. To this end, we com-
pute a shortest path on the directed street graph
described in Section 3. The result is an ordered
list of street segments that need to be traversed in
the given order to successfully reach the destina-
tion; see Fig. 4 for an example.
To be suitable as the input for an NLG system,
this flat list of OpenStreetMap nodes needs to be
subdivided into smaller message chunks. In turn-
by-turn navigation, the general delimiter between
such chunks are the driving maneuvers that the
driver must execute at each decision point. We
call each span between two decision points an
episode. Episodes are not explicitly represented
in the original route plan: although every segment
has a street name associated with it, the name of
a street sometimes changes as we go along, and
because chains of segments are used to model
curved streets in OpenStreetMap, even segments
that are joined at an angle may be parts of the
same street. Thus, in Fig. 4 it is not apparent
which segment traversals require any navigational
maneuvers.
We identify episode boundaries with the fol-
lowing heuristic. We first assume that episode
boundaries occur when the street name changes
from one segment to the next. However, stay-
ing on the road may involve a driving maneu-
ver (and therefore a decision point) as well, e.g.
when the road makes a sharp turn where a minor
street forks off. To handle this case, we introduce
decision points at nodes with multiple adjacent
segments if the angle between the incoming and
outgoing segment of the street exceeds a certain
threshold. Conversely, our heuristic will some-
times end an episode where no driving maneuver
is necessary, e.g. when an ongoing street changes
its name. This is unproblematic in practice; the
system will simply generate an instruction to keep
driving straight ahead. Fig. 3 shows a graphical
representation of an episode, with the street seg-
ments belonging to it drawn as red dashed lines.
4.2 Aggregation
Because we generate spoken instructions that are
given to the user while they are driving, the timing
of the instructions becomes a crucial issue, espe-
cially because a driver moves faster than the user
of a pedestrian navigation system. It is undesir-
able for a second instruction to interrupt an ear-
lier one. On the other hand, the second instruc-
tion cannot be delayed because this might make
the user miss a turn or interpret the instruction in-
correctly.
We must therefore control at which points in-
structions are given and make sure that they do
not overlap. We do this by always presenting pre-
view messages at trigger positions at certain fixed
distances from the decision point. The sentence
planner calculates where these trigger positions
are located for each episode. In this way, we cre-
ate time frames during which there is enough time
for instructions to be presented.
However, some episodes are too short to ac-
commodate the three trigger positions for the con-
firmation message and the two preview messages.
In such episodes, we aggregate different mes-
sages. We remove the trigger positions for the two
preview messages from the episode, and instead
add the first preview message to the turn instruc-
tion message of the previous episode. This allows
our system to generate instructions like ?Now turn
right, and then turn left after the church.?
4.3 Generation of landmark descriptions
The Virtual Co-Pilot computes referring expres-
sions to decision points by selecting appropriate
landmarks. To this end, it first looks up landmark
candidates within a given range of the decision
point from the database created in Section 3. This
761
yields an initial list of landmark candidates.
Some of these landmark candidates may be un-
suitable for the given situation because of lack of
uniqueness. If there are several visual landmarks
of the same type along the course of an episode,
all of these landmark candidates are removed. For
episodes which contain multiple street furniture
landmarks of the same type, the first three in each
episode are retained; a referring expression for the
decision point might then be ?at the second traf-
fic light?. If the decision point is no more than
three intersections away, we also add a landmark
description of the form ?at the third intersection?.
Furthermore, a landmark must be visible from the
last segment of the current episode; we only retain
a candidate if it is either adjacent to a segment of
the current episode or if it is close to the end point
of the very last segment of the episode. Among
the landmarks that are left over, the system prefers
visual landmarks over street furniture, and street
furniture over intersections. If no landmark candi-
dates are left over, the system falls back to metric
distances.
Second, the Virtual Co-Pilot determines the
spatial relationship between the landmark and the
decision point so that an appropriate preposition
can be used in the referring expression. If the de-
cision point occurs before the landmark along the
course of the episode, we use the preposition ?in
front of?, otherwise, we use ?after?. Intersections
are always used with ?at? and metric distances
with ?in?.
Finally, the system decides how to refer to the
landmark objects themselves. Although it has ac-
cess to the names of all objects from the Open-
StreetMap data, the user may not know these
names. We therefore refer to churches, gas sta-
tions, and any street furniture simply as ?the
church?, ?the gas station?, etc. For supermar-
kets and bars, we assume that these buildings are
more saliently referred to by their names, which
are used in everyday language, and therefore use
the names to refer to them.
The result of the sentence planning stage is
a list of semantic representations, specifying the
individual instructions that are to be uttered in
each episode; an example is shown in Fig. 5.
For each type of instruction, we then use a sen-
tence template to generate linguistic surface forms
by inserting the information contained in those
plans into the slots provided by the templates (e.g.
Preview message p1:
Trigger position: Node3 ? 50m
Turn direction: right
Landmark: church
Preposition: after
Preview message p2 = p1, except:
Trigger position: Node3 ? 100m
Turn instruction t1:
Trigger position: Node3
Turn direction: right
Confirmation message c1:
Trigger position: Node3 + 50m
Figure 5: Semantic representations of the different
types of instructions in one episode.
?Turn direction preposition landmark?).
4.4 Interactive generation
As a final point, the NLG process of a car naviga-
tion system takes place in an interactive setting:
as the system generates and utters instructions, the
user may either follow them correctly, or they may
miss a turn or turn incorrectly because they mis-
understood the instruction or were forced to disre-
gard it by the traffic situation. The system must be
able to detect such problems, recover from them,
and generate new instructions in real time.
Our system receives a continuous stream of in-
formation about the position and direction of the
user. It performs execution monitoring to check
whether the user is still following the intended
route. If a trigger position is reached, we present
the instruction that we have generated for this po-
sition. If the user has left the route, the system
reacts by planning a new route starting from the
user?s current position and generating a new set of
instructions. We check whether the user is follow-
ing the intended route in the following way. The
system keeps track of the current episode of the
route plan, and monitors the distance of the car
to the final node of the episode. While the user
is following the route correctly, the distance be-
tween the car and the final node should decrease
or at least stay the same between two measure-
ments. To accommodate for occasional deviations
from the middle of the road, we allow five subse-
quent measurements to increase the distance; the
sixth increase of the distance triggers a recompu-
tation of the route plan and a freshly generated
instruction. On the other hand, when the distance
762
of the car to the final node falls below a certain
threshold, we assume that the end of the episode
has been reached, and activate the next episode.
By monitoring whether the user is now approach-
ing the final node of this new episode, we can in
particular detect wrong turns at intersections.
Because each instruction carries the risk that it
may not be followed correctly, there is a question
as to whether it is worth planning out all remain-
ing instructions for the complete route plan. After
all, if the user does not follow the first instruc-
tion, the computation of all remaining instructions
was a waste of time. We decided to compute all
future instructions anyway because the aggrega-
tion procedure described above requires them. In
practice, the NLG process is so efficient that all
instructions can be done in real time, but this de-
cision would have to be revisited for a slower sys-
tem.
5 Evaluation
We will now report on an experiment in which we
evaluated the performance of the Virtual Co-Pilot.
5.1 Experimental Method
5.1.1 Subjects
In total, 12 participants were recruited through
printed ads and mailing lists. All of them were
university students aged between 21 and 27 years.
Our experiment was balanced for gender, hence
we recruited 6 male and 6 female participants. All
participants were compensated for their effort.
5.1.2 Design
The driving simulator used in the experiment
replicates a real-world city center using a 3D
model that contains buildings and streets as they
can be perceived in reality. The street layout 3D
model used by the driving simulator is based on
OpenStreetMap data, and buildings were added to
the virtual environment based on cadastral data.
To increase the perceived realism of the model,
some buildings were manually enhanced with
photographic images of their real-world counter-
parts (see Fig. 7).
Figure 6 shows the set-up of the evaluation ex-
periment. The virtual driving simulator environ-
ment (main picture in Fig. 7) was presented to the
participants on a 20? computer screen (A). In ad-
dition, graphical navigation instructions (shown
in the lower right of Fig. 7) were displayed on
Figure 6: Experiment setup. A) Main screen B) Navi-
gation screen C) steering wheel D) eye tracker
a separate 7? monitor (B). The driving simula-
tor was controlled by means of a steering wheel
(C), along with a pair of brake and acceleration
pedals. We recorded user eye movements using
a Tobii IS-Z1 table-mounted eye tracker (D). The
generated instructions were converted to speech
using MARY, an open-source text-to-speech sys-
tem (Schro?der and Trouvain, 2003), and played
back on loudspeakers.
The task of the user was to drive the car in
the virtual environment towards a given destina-
tion; spoken instructions were presented to them
as they were driving, in real time. Using the
steering wheel and the pedals, users had full con-
trol over steering angles, acceleration and brak-
ing. The driving speed was limited to 30 km/h, but
there were no restrictions otherwise. The driving
simulator sent the NLG system a message with the
current position of the car (as GPS coordinates)
once per second.
Each user was asked to drive three short routes
in the driving simulator. Each route took about
four minutes to complete, and the travelled dis-
tance was about 1 km. The number of episodes
per route ranged from three to five. Landmark
candidates were sufficiently dense that the Virtual
Co-Pilot used landmarks to refer to all decision
points and never had to fall back to the metric dis-
tance strategy.
There were three experimental conditions,
which differed with respect to the spoken route
instructions and the use of the navigation screen.
In the baseline condition, designed to replicate the
behavior of an off-the-shelf commercial car nav-
763
All Users Males Females
B VCP B VCP B VCP
Total Fixation Duration (seconds) 4.9 3.5 2.7 4.1 7.0 2.9*
Total Fixation Count (N) 21.8 15.4 13.5 16.5 30.0 14.3*
?The system provided the right amount
of information at any time?
3.9 2.9 4.2* 3.3 3.5 2.5
?I was insecure at times about still be-
ing on the right track.?
2.3 3.2 1.9* 2.8 2.6 3.5
?It was important to have a visual rep-
resentation of route directions?
4.3 4.0 4.2 4.2 4.3 3.7
?I could trust the navigation system? 3.6 3.7 4.1 3.7 3.0 3.7
Figure 8: Mean values for gaze behavior and subjective evaluation, separated by user group and condition (B =
baseline, VCP = our system). Significant differences are indicated by *; better values are printed in boldface.
Figure 7: Screenshot of a scene in the driving simula-
tor. Lower right corner: matching screenshot of navi-
gation display.
igation system, participants were provided with
spoken metric distance-to-turn navigation instruc-
tions. The navigation screen showed arrows de-
picting the direction of the next turn, along with
the distance to the decision point (cf. Fig. 7). The
second condition replaced the spoken route in-
structions by those generated by the Virtual Co-
Pilot. In a third condition, the output of the nav-
igation screen was further changed to display an
icon for the next landmark along with the arrow
and distance indicator. The three routes were pre-
sented to the users in different orders, and com-
bined with the conditions in a Latin Squares de-
sign. In this paper, we focus on the first and sec-
ond condition, in order to contrast the two styles
of spoken instruction.
Participants were asked to answer two ques-
tionnaires after each trial run. The first was the
DALI questionnaire (Pauzie?, 2008), which asks
subjects to report how they perceived different
aspects of their cognitive workload (general, vi-
sual, auditive and temporal workload, as well as
perceived stress level). In the second question-
naire, participants were state to rate their agree-
ment with a number of statements about their sub-
jective impression of the system on a 5-point un-
labelled Likert scale, e.g. whether they had re-
ceived instructions at the right time or whether
they trusted the navigation system to give them
the right instructions during trials.
5.2 Results
There were no significant differences between the
Virtual Co-Pilot and the baseline system on task
completion time, rate of driving errors, or any of
the questions of the DALI questionnaire. Driv-
ing errors in particular were very rare: there were
only four driving errors in total, two of which
were due to problems with left/right coordination.
We then analyzed the gaze data collected by the
table-mounted eye tracker, which we set up such
that it recognized glances at the navigation screen.
In particular, we looked at the total fixation dura-
tion (TFD), i.e. the total amount of time that a user
spent looking at the navigation screen during a
given trial run. We also looked at the total fixation
count (TFC), i.e. the total number of times that a
user looked at the navigation screen in each run.
Mean values for both metrics are given in Fig. 8,
averaged over all subjects and only male and fe-
male subjects, respectively; the ?VCP? column is
for the Virtual Co-Pilot, whereas ?B? stands for
the baseline. We found that male users tended
to look more at the navigation screen in the VCP
condition than in B, although the difference is not
statistically significant. However, female users
looked at the navigation screen significantly fewer
764
times (t(5) = 3.2, p < 0.05, t-test for dependent
samples) and for significantly shorter amounts of
time (t(5) = 3.2, p < 0.05) in the VCP condition
than in B.
On the subjective questionnaire, most questions
yielded no significant differences (and are not re-
ported here). However, we found that female
users tended to rate the Virtual Co-Pilot more pos-
itively than the baseline on questions concerning
trust in the system and the need for the navigation
screen (but not significantly). Male users found
that the baseline significantly outperformed the
Virtual Co-Pilot on presenting instructions at the
right time (t(5) = 2.7, p < 0.05) and on giving
them a sense of security in still being on the right
track (t(5) = ?2.7, p < 0.05).
5.3 Discussion
The most striking result of the evaluation is that
there was a significant reduction of looks to the
navigation display, even if only for one group
of users. Female users looked at the navigation
screen less and more rarely with the Virtual Co-
Pilot compared to the baseline system. In a real
car navigation system, this translates into a driver
who spends less time looking away from the road,
i.e. a reduction in driver distraction and an in-
crease in traffic safety. This suggests that female
users learned to trust the landmark-based instruc-
tions, an interpretation that is further supported
by the trends we found in the subjective question-
naire.
We did not find these differences in the male
user group. Part of the reason may be the known
gender differences in landmark use we mentioned
in Section 2. But interestingly, the two signifi-
cantly worse ratings by male users concerned the
correct timing of instructions and the feedback for
driving errors, i.e. issues regarding the system?s
real-time capabilities. Although our system does
not yet perform ideally on these measures, this
confirms our initial hypothesis that the NLG sys-
tem must track the user?s behavior and schedule
its utterances appropriately. This means that ear-
lier systems such as CORAL, which only com-
pute a one-shot discourse of route instructions
without regard to the timing of the presentation,
miss a crucial part of the problem.
Apart from the exceptions we just discussed,
the landmark-based system tended to score com-
parably or a bit worse than the baseline on the
other subjective questions. This may partly be due
to the fact that the subjects were familiar with ex-
isting commercial car navigation systems and not
used to landmark-based instructions. On the other
hand, this finding is also consistent with results
of other evaluations of NLG systems, in which
an improvement in the objective task usefulness
of the system does not necessarily correlate with
improved scores from subjective questionnaires
(Gatt et al 2009).
6 Conclusion
In this paper, we have described a system for gen-
erating real-time car navigation instructions with
landmarks. Our system is distinguished from ear-
lier work in its reliance on open-source map data
from OpenStreetMap, from which we extract both
the street graph and the potential landmarks. This
demonstrates that open resources are now infor-
mative enough for use in wide-coverage naviga-
tion NLG systems. The system then chooses ap-
propriate landmarks at decision points, and con-
tinuously monitors the driver?s behavior to pro-
vide modified instructions in real time when driv-
ing errors occur.
We evaluated our system using a driving simu-
lator with respect to driving errors, user satisfac-
tion, and driver distraction. To our knowledge,
we have shown for the first time that a landmark-
based car navigation system outperforms a base-
line significantly; namely, in the amount of time
female users spend looking away from the road.
In many ways, the Virtual Co-Pilot is a very
simple system, which we see primarily as a start-
ing point for future research. The evaluation
confirmed the importance of interactive real-time
NLG for navigation, and we therefore see this as
a key direction of future work. On the other hand,
it would be desirable to generate more complex
referring expressions (?the tall church?). This
would require more informative map data, as well
as a formal model of visual salience (Kelleher and
van Genabith, 2004; Raubal and Winter, 2002).
Acknowledgments. We would like to thank the
DFKI CARMINA group for providing the driv-
ing simulator, as well as their support. We would
furthermore like to thank the DFKI Agents and
Simulated Reality group for providing the 3D city
model.
765
References
G. L. Allen. 2000. Principles and practices for com-
municating route knowledge. Applied Cognitive
Psychology, 14(4):333?359.
C. Brenner and B. Elias. 2003. Extracting land-
marks for car navigation systems using existing
gis databases and laser scanning. International
archives of photogrammetry remote sensing and
spatial information sciences, 34(3/W8):131?138.
G. Burnett. 2000. ?Turn right at the Traffic Lights?:
The Requirement for Landmarks in Vehicle Nav-
igation Systems. The Journal of Navigation,
53(03):499?510.
R. Dale, S. Geldof, and J. P. Prost. 2003. Using natural
language generation for navigational assistance. In
ACSC, pages 35?44.
B. Elias. 2003. Extracting landmarks with data min-
ing methods. Spatial information theory, pages
375?389.
A. Gatt, F. Portet, E. Reiter, J. Hunter, S. Mahamood,
W. Moncur, and S. Sripada. 2009. From data to text
in the neonatal intensive care unit: Using NLG tech-
nology for decision support and information man-
agement. AI Communications, 22:153?186.
S. Kaplan. 1976. Adaption, structure and knowledge.
In G. Moore and R. Golledge, editors, Environmen-
tal knowing: Theories, research and methods, pages
32?45. Dowden, Hutchinson and Ross.
J. D. Kelleher and J. van Genabith. 2004. Visual
salience and reference resolution in simulated 3-D
environments. Artificial Intelligence Review, 21(3).
A. Koller, K. Striegnitz, D. Byron, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2010. The First Chal-
lenge on Generating Instructions in Virtual Environ-
ments. In E. Krahmer and M. Theune, editors, Em-
pirical Methods in Natural Language Generation.
Springer.
N. Lessmann, S. Kopp, and I. Wachsmuth. 2006. Sit-
uated interaction with a virtual human ? percep-
tion, action, and cognition. In G. Rickheit and
I. Wachsmuth, editors, Situated Communication,
pages 287?323. Mouton de Gruyter.
K. Lovelace, M. Hegarty, and D. Montello. 1999. El-
ements of good route directions in familiar and un-
familiar environments. Spatial information theory.
Cognitive and computational foundations of geo-
graphic information science, pages 751?751.
K. Lynch. 1960. The image of the city. MIT Press.
R. Malaka and A. Zipf. 2000. DEEP MAP ? Chal-
lenging IT research in the framework of a tourist in-
formation system. Information and communication
technologies in tourism, 7:15?27.
R. Malaka, J. Haeussler, and H. Aras. 2004.
SmartKom mobile: intelligent ubiquitous user in-
teraction. In Proceedings of the 9th International
Conference on Intelligent User Interfaces.
A. J. May and T. Ross. 2006. Presence and quality
of navigational landmarks: effect on driver perfor-
mance and implications for design. Human Fac-
tors: The Journal of the Human Factors and Er-
gonomics Society, 48(2):346.
P. E. Michon and M. Denis. 2001. When and why are
visual landmarks used in giving directions? Spatial
information theory, pages 292?305.
A. Pauzie?. 2008. Evaluating driver mental workload
using the driving activity load index (DALI). In
Proc. of European Conference on Human Interface
Design for Intelligent Transport Systems, pages 67?
77.
M. Raubal and S. Winter. 2002. Enriching wayfind-
ing instructions with local landmarks. Geographic
information science, pages 243?259.
E. Reiter and R. Dale. 2000. Building natural lan-
guage generation systems. Studies in natural lan-
guage processing. Cambridge University Press.
D. M. Saucier, S. M. Green, J. Leason, A. MacFadden,
S. Bell, and L. J. Elias. 2002. Are sex differences in
navigation caused by sexually dimorphic strategies
or by differences in the ability to use the strategies?.
Behavioral Neuroscience, 116(3):403.
M. Schro?der and J. Trouvain. 2003. The German
text-to-speech synthesis system MARY: A tool for
research, development and teaching. International
Journal of Speech Technology, 6(4):365?377.
K. Striegnitz and F. Majda. 2009. Landmarks in
navigation instructions for a virtual environment.
Online Proceedings of the First NLG Challenge
on Generating Instructions in Virtual Environments
(GIVE-1).
J. C. Stutts, D. W. Reinfurt, L. Staplin, and E. A. Rodg-
man. 2001. The role of driver distraction in traf-
fic crashes. Washington, DC: AAA Foundation for
Traffic Safety.
A. Tom and M. Denis. 2003. Referring to landmark
or street information in route directions: What dif-
ference does it make? Spatial information theory,
pages 362?374.
766
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 162?171,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HyTER: Meaning-Equivalent Semantics for Translation Evaluation
Markus Dreyer
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA
mdreyer@sdl.com
Daniel Marcu
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA
dmarcu@sdl.com
Abstract
It is common knowledge that translation is
an ambiguous, 1-to-n mapping process, but
to date, our community has produced no em-
pirical estimates of this ambiguity. We have
developed an annotation tool that enables us
to create representations that compactly en-
code an exponential number of correct trans-
lations for a sentence. Our findings show that
naturally occurring sentences have billions of
translations. Having access to such large sets
of meaning-equivalent translations enables us
to develop a new metric, HyTER, for transla-
tion accuracy. We show that our metric pro-
vides better estimates of machine and human
translation accuracy than alternative evalua-
tion metrics.
1 Motivation
During the last decade, automatic evaluation met-
rics (Papineni et al, 2002; Snover et al, 2006; Lavie
and Denkowski, 2009) have helped researchers ac-
celerate the pace at which they improve machine
translation (MT) systems. And human-assisted met-
rics (Snover et al, 2006) have enabled and sup-
ported large-scale U.S. government sponsored pro-
grams, such as DARPA GALE (Olive et al, 2011).
However, these metrics have started to show signs of
wear and tear.
Automatic metrics are often criticized for provid-
ing non-intuitive scores ? few researchers can ex-
plain to casual users what a BLEU score of 27.9
means. And researchers have grown increasingly
concerned that automatic metrics have a strong bias
towards preferring statistical translation outputs; the
NIST (2008, 2010), MATR (Gao et al, 2010) and
WMT (Callison-Burch et al, 2011) evaluations held
during the last five years have provided ample ev-
idence that automatic metrics yield results that are
inconsistent with human evaluations when compar-
ing statistical, rule-based, and human outputs.
In contrast, human-informed metrics have other
deficiencies: they have large variance across human
judges (Bojar et al, 2011) and produce unstable re-
sults from one evaluation to another (Przybocki et
al., 2011). Because evaluation scores are not com-
puted automatically, systems developers cannot au-
tomatically tune to human-based metrics.
Table 1 summarizes the dimensions along which
evaluation metrics should do well and the strengths
and weaknesses of the automatic and human-
informed metrics proposed to date. Our goal is
to develop metrics that do well along all these di-
mensions. The fundamental insight on which our
research relies is that the failures of current auto-
matic metrics are not algorithmic: BLEU, Meteor,
TER (Translation Edit Rate), and other metrics ef-
ficiently and correctly compute informative distance
functions between a translation and one or more hu-
man references. We believe that these metrics fail
simply because they have access to sets of human
references that are too small. If we had access to
the set of all correct translations of a given sentence,
we could measure the minimum distance between a
translation and the set. When a translation is perfect,
it can be found in the set, so it requires no editing to
produce a perfect translation. Therefore, its score
should be zero. If the translation has errors, we can
162
Desiderata Auto. Manu. HyTER
Metric is intuitive N Y Y
Metric is computed automatically Y N Y
Metric is stable and reproducible from one evaluation to another Y N Y
Metric works equally well when comparing human and automatic outputs
and when comparing rule-based, statistical-based, and hybrid engines
N Y Y
System developers can tune to the metric Y N Y
Metric helps developers identify deficiencies of MT engines N N Y
Table 1: Desiderata of evaluation metrics: Current automatic and human metrics, proposed metric.
efficiently compute the minimum number of edits
(substitutions, deletions, insertions, moves) needed
to rewrite the translation into the ?closest? reference
in the set. Current automatic evaluation metrics do
not assign their best scores to most perfect transla-
tions because the set of references they use is too
small; their scores can therefore be perceived as less
intuitive.
Following these considerations, we developed an
annotation tool that enables one to efficiently create
an exponential number of correct translations for a
given sentence, and present a new evaluation met-
ric, HyTER, which efficiently exploits these mas-
sive reference networks. In the rest of the paper, we
first describe our annotation environment, process,
and meaning-equivalent representations that we cre-
ate (Section 2). We then present the HyTER met-
ric (Section 3). We show that this new metric pro-
vides better support than current metrics for machine
translation evaluation (Section 4) and human trans-
lation proficiency assessment (Section 5).
2 Annotating sentences with exponential
numbers of meaning equivalents
2.1 Annotation tool
We have developed a web-based annotation tool
that can be used to create a representation encoding
an exponential number of meaning equivalents
for a given sentence. The meaning equivalents
are constructed in a bottom-up fashion by typing
translation equivalents for larger and larger phrases.
For example, when building the meaning equiv-
alents for the Spanish phrase ?el primer ministro
italiano Silvio Berlusconi?, the annotator first types
in the meaning equivalents for ?primer ministro?
? ?prime-minister; PM; prime minister; head of
government; premier; etc.?; ?italiano? ? ?Italian?;
and ?Silvio Berlusconi? ? ?Silvio Berlusconi;
Berlusconi?. The tool creates a card that stores
all the alternative meanings for a phrase as a
determinized FSA and gives it a name in the target
language that is representative of the underly-
ing meaning-equivalent set: [PRIME-MINISTER],
[ITALIAN], and [SILVIO-BERLUSCONI]. Each base
card can be thought of expressing a semantic con-
cept. A combination of existing cards and additional
words can be subsequently used to create larger
meaning equivalents that cover increasingly larger
source sentence segments. For example, to create
the meaning equivalents for ?el primer ministro ital-
iano? one can drag-and-drop existing cards or type
in new words: ?the [ITALIAN] [PRIME-MINISTER];
the [PRIME-MINISTER] of Italy?; to create the
meaning equivalents for ?el primer ministro italiano
Silvio Berlusconi?, one can drag-and-drop and type:
?[SILVIO-BERLUSCONI] , [THE-ITALIAN-PRIME-
MINISTER]; [THE-ITALIAN-PRIME-MINISTER] ,
[SILVIO-BERLUSCONI]; [THE-ITALIAN-PRIME-
MINISTER] [SILVIO-BERLUSCONI] ?. All meaning
equivalents associated with a given card are ex-
panded and used when that card is re-used to create
larger meaning-equivalent sets.
The annotation tool supports, but does not en-
force, re-use of annotations created by other anno-
tators. The resulting meaning equivalents are stored
as recursive transition networks (RTNs), where each
card is a subnetwork; if needed, these non-cyclic
RTNs can be automatically expanded into finite-
state acceptors (FSAs, see Section 3).
2.2 Data and Annotation Protocols
Using the annotation tool, we have created meaning-
equivalent annotations for 102 Arabic and 102 Chi-
nese sentences ? a subset of the ?progress set? used
in the 2010 Open MT NIST evaluation (the average
163
sentence length was 24 words). For each sentence,
we had access to four human reference translations
produced by LDC and five MT system outputs,
which were selected by NIST to cover a variety of
system architectures (statistical, rule-based, hybrid)
and performances. For each MT output, we also had
access to sentence-level HTER scores (Snover et al,
2006), which were produced by experienced LDC
annotators.
We have experimented with three annotation pro-
tocols:
? Ara-A2E and Chi-C2E: Foreign language natives
built English networks starting from foreign lan-
guage sentences.
? Eng-A2E and Eng-C2E: English natives built En-
glish networks starting from ?the best translation?
of a foreign language sentence, as identified by
NIST.
? Eng*-A2E and Eng*-C2E: English natives built
English networks starting from ?the best transla-
tion?, but had access to three additional, indepen-
dently produced human translations to boost their
creativity.
Each protocol was implemented independently by
at least three annotators. In general, annotators need
to be fluent in the target language, familiar with the
annotation tool we provide and careful not to gen-
erate incorrect paths, but they do not need to be lin-
guists.
2.3 Exploiting multiple annotations
For each sentence, we combine all networks that
were created by the different annotators. We eval-
uate two different combination methods, each of
which combines networks N1 and N2 of two anno-
tators (see an example in Figure 1):
(a) Standard union U(N1, N2): The standard
finite-state union operation combines N1 and N2
on the whole-network level. When traversing
U(N1, N2), one can follow a path that comes from
either N1 or N2.
(b) Source-phrase-level union SPU(N1, N2): As
an alternative, we introduce SPU, a more fine-
grained union which operates on sub-sentence seg-
ments. Here we exploit the fact that each annotator
explicitly aligned each of her various subnetworks
N1
the level of approval
was
close to
zero
the approval rate
practically
the approval level
was
close to
zero
the approval rate
about equal to
(a)
was
zero
the approval rate
the level of approval
the approval level
close to
practically
about equal to
(b)
N2
Figure 1: (a) Finite-state union versus (b) source-phrase-
level union (SPU). The former does not contain the path
?the approval level was practically zero?.
for a given sentence to a source span of that sen-
tence. Now for each pair of subnetworks (S1, S2)
from N1 and N2, we build their union if they are
compatible; two subnetworks S1, S2 are defined to
be compatible if they are aligned to the same source
span and have at least one path in common.
The purpose of SPU is to create new paths by mix-
ing paths from N1 and N2. In Figure 1, for example,
the path ?the approval level was practically zero? is
contained in the SPU, but not in the standard union.
We build SPUs using a dynamic programming al-
gorithm that builds subnetworks bottom-up, build-
ing unions of intermediate results. Two larger sub-
networks can be compatible only if their recursive
smaller subnetworks are compatible. Each SPU con-
tains at least all paths from the standard union.
2.4 Empirical findings
Now that we have described how we created partic-
ular networks for a given dataset, we describe some
empirical findings that characterize our annotation
process and the created networks.
Meaning-equivalent productivity. When we
compare the productivity of the three annotation
protocols in terms of the number of reference trans-
lations that they enable, we observe that target lan-
guage natives that have access to multiple human
references produce the largest networks. The me-
dian number of paths produced by one annotator un-
der the three protocols varies from 7.7 ? 105 paths
for Ara-A2E, to 1.4 ? 108 paths for Eng-A2E, to
5.9 ? 108 paths for Eng*-A2E; in Chinese, the me-
164
dians vary from 1.0? 105 for Chi-C2E, to 1.7? 108
for Eng-C2E, to 7.8? 109 for Eng*-C2E.
Protocol productivity. When we compare the
annotator time required by the three protocols, we
find that foreign language natives work faster ? they
need about 2 hours per sentence ? while target lan-
guage natives need 2.5 hours per sentence. Given
that target language natives build significantly larger
networks and that bilingual speakers are in shorter
supply than monolingual ones, we conclude that us-
ing target language annotators is more cost-effective
overall.
Exploiting multiple annotations. Overall, the me-
dian number of paths produced by a single annota-
tor for A2E is 1.5 ? 106, two annotators (randomly
picked per sentence) produce a median number of
4.7 ? 107 (Union), for all annotators together it is
2.1? 1010 (Union) and 2.1? 1011 (SPU). For C2E,
these numbers are 5.2? 106 (one), 1.1? 108 (two),
and 2.6?1010 (all, Union) and 8.5?1011 (all, SPU).
Number of annotators and annotation time. We
compute the minimum number of edits and length-
normalized distance scores required to rewrite ma-
chine and human translations into translations found
in the networks produced by one, two, and three
annotators. We find that the length-normalized dis-
tances do not vary by more than 1% when adding the
meaning equivalents produced by a third annotator.
We conclude that 2-3 annotators per sentence pro-
duce a sufficiently large set of alternative meaning
equivalents, which takes 4-7.5 hours. We are cur-
rently investigating alternative ways to create net-
works more efficiently.
Grammaticality. For each of the four human trans-
lation references and each of the five machine trans-
lation outputs (see Section 2.2), we algorithmically
find the closest path in the annotated networks of
meaning equivalents (see Section 3). We presented
the resulting 1836 closest paths extracted from the
networks (2 language pairs ?102 sentences ?9 hu-
man/machine translations) to three independent En-
glish speakers. We asked each English path to be
labeled as grammatical, grammatical-but-slightly-
odd, or non-grammatical. The metric is harsh: paths
such as ?he said that withdrawing US force with-
out promoting security would be cataclysmic? are
judged as non-grammatical by all three judges al-
though a simple rewrite of ?force? into ?forces?
would make this path grammatical. We found that
90% of the paths are judged as grammatical and
96% as grammatical or grammatical-but-slightly-
odd, by at least one annotator. We interpret these
results as positive: the annotation process leads to
some ungrammatical paths being created, but most
of the closest paths to human and machine outputs,
those that matter from an evaluation perspective, are
judged as correct by at least one judge.
Coverage. We found it somewhat disappoint-
ing that networks that encode billions of meaning-
equivalent translations for a given sentence do not
contain every independently produced human ref-
erence translation. The average length-normalized
edit distance (computed as described in Section 3)
between an independently produced human refer-
ence and the corresponding network is 19% for
Arabic-English and 34% for Chinese-English across
the entire corpus. Our analysis shows that about
half of the edits are explained by several non-
content words (?the?, ?of?, ?for?, ?their?, ?,?) be-
ing optional in certain contexts; several ?obvious?
equivalents not being part of the networks (?that??
?this?; ?so???accordingly?); and spelling alterna-
tives/errors (?rockstrom???rockstroem?). We hy-
pothesize that most of these ommissions/edits can be
detected automatically and dealt with in an appropri-
ate fashion. The rest of the edits would require more
sophisticated machinery, to figure out, for example,
that in a particular context pairs like ?with???and?
or ?that???therefore? are interchangeable.
Given that Chinese is significantly more under-
specified compared to Arabic and English, it is con-
sistent with our intuition to see that the average mini-
mal distance is higher between Chinese-English ref-
erences and their respective networks (34%) than
between Arabic-English references and their respec-
tive networks (19%).
3 Measuring translation quality with large
networks of meaning equivalents
In this section, we present HyTER (Hybrid Trans-
lation Edit Rate), a novel metric that makes use of
large reference networks.
165


	


	


	


	





	


	


	
 

	



	
	




	

	

  	
		
 		

<ts>
 
	

	

	





 


 