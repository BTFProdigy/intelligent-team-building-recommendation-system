Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Cognitive-based Annotation System for Emotion Computing 
 
 
Ying Chen, Sophia Y. M. Lee and Chu-Ren Huang 
Department of Chinese & Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,churen.huang}@gmail.com 
 
  
 
Abstract 
Emotion computing is very important for 
expressive information extraction. In this 
paper, we provide a robust and versatile 
emotion annotation scheme based on cog-
nitive emotion theories, which not only 
can annotate both explicit and implicit 
emotion expressions, but also can encode 
different levels of emotion information for 
the given emotion content. In addition, 
motivated by a cognitive framework, an 
automatic emotion annotation system is 
developed, and large and comparatively 
high-quality emotion corpora are created 
for emotion computing, one in Chinese 
and the other in English. Such an annota-
tion system can be easily adapted for dif-
ferent kinds of emotion applications and 
be extended to other languages. 
1 Introduction 
Affective information is important for human 
language technology, and sentiment analysis, a 
coarse-grained affective computing (Shanahan et 
al., 2006), which is attitude assessment, has be-
come the most salient trend. The polarity-driven 
approach in sentiment analysis is, however, often 
criticized as too general to satisfy some applica-
tions, such as advertisement design and robot 
design, and one way to capture more fine-grained 
affective information is to detect emotion expres-
sions. Unlike sentiment, emotions are cognitive-
based, which consistently occur across domains 
because of its human psychological activities. 
We believe that emotion computing, which is a 
fine-grained and cognitive-based framework of 
affective computing, will provide a more robust 
and versatile model for human language technol-
ogy. 
Since the concept of emotion is very compli-
cated and subjective, comparing to some annota-
tions such as POS annotation and Chinese word 
segmentation annotation, emotion annotation is 
highly labor intensive as it requires careful hu-
man judgment. Both explicit and implicit emo-
tions must be recognized and tagged during emo-
tion annotation, therefore, emotion annotation is 
not a simple assignment exercise as in POS an-
notation. Technically, emotion annotation can be 
divided into two subtasks: emotion detection (i.e. 
differentiate emotional content from neutral con-
tent), which is a very important task for affective 
information extraction, and emotion classifica-
tion (i.e. assign emotion tags to emotional con-
tent.)  
Emotion computing often requires a large and 
high-quality annotated data, however, there is a 
lack of this kind of corpus. This is not only be-
cause of the enormous human involvement, but 
also because of the unavailability of emotion an-
notation scheme, which is robust and versatile 
for both emotion annotation and emotion com-
puting. Tokuhisa et al (2008) is the only work 
that explores the issue of emotion detection 
while most of the previous studies concentrate on 
the emotion classification given a known emo-
tion context (Mihalcea and Liu, 2006; Kozareva 
et al, 2007.) Even for emotion classification, 
some issues remain unresolved, such as the com-
plicated relationships among different emotion 
types, emotion type selection, and so on. Thus, it 
is still far from solving the emotion problem if 
emotion annotation is just considered as emo-
tion-tag assignment.  
In this paper, we first explore the relationships 
among different emotion types with the support 
of a proposed emotion taxonomy, which com-
bines some psychological theories and linguistic 
semantics. Based on the emotion taxonomy, a 
robust and versatile emotion annotation scheme 
is designed and used in both Chinese and English 
1
emotion corpora. Our emotion annotation 
scheme is very flexible, which is only a layer 
added to a sentence, although it can easily be 
extended to a higher level of a text. Our annota-
tion scheme not only can provide the emotion 
type information, but also can encode the infor-
mation regarding the relationship between emo-
tions. With this versatile annotated emotion in-
formation, different NLP users can extract dif-
ferent emotion information from a given anno-
tated corpus according to their applications.  
With such an emotion annotation scheme, a 
large and comparatively high-quality annotated 
emotion corpus is built for emotion computing 
through an unsupervised approach. Tokuhisa et 
al. (2008) pointed out that besides emotion cor-
pus, neutral corpus (i.e. sentences containing no 
emotion) is also very important for emotion 
computing. Therefore, a high-quality neutral 
corpus is also automatically collected using con-
textual information. These two corpora are com-
bined to form a complete emotion-driven corpus 
for emotion computing. Although the unsuper-
vised method cannot provide a perfectly-
annotated corpus, it can easily adapt for different 
emotion computing.  
The remainder of this paper is organized as 
follows. In Section 2, we give an overview of the 
previous work on emotion annotation and some 
related psychological and linguistic theories. In 
Section 3, we describe our emotion taxonomy 
and emotion annotation scheme. Section 4 dis-
cusses how the unsupervised corpus is created.  
Section 5 presents the pilot experiments for emo-
tion computing with our corpus, which suggests 
that the unsupervised approach of our corpus 
creation is effective. Finally, a conclusion is 
made in Section 5. 
2 Related work 
There is no clear consensus among many psy-
chological and linguistic theories on the concept 
of emotions. Here, we limit our work by the clas-
sic definition of ?emotions? (Cannon, 1927): 
Emotion is the felt awareness of bodily reactions 
to something perceived or thought. 
Emotion is a complicated concept, and there 
are complicated relationships among different 
emotions. For example, the relationship between 
?discouraged? and ?sad? is different with the one 
between ?remorse? and ?sad.? Hobbs & Gordon 
(2008) and Mathieu (2005) explore emotions 
mainly from a lexical semantics perspective, and 
Schr?der et al (2006) designed an annotation 
scheme, EARL, mainly for speech processing. 
Because of the disagreements in emotion theories, 
EARL did not explore the relationships among 
emotion types. In this paper, we focus on emo-
tions in written data, which is very different from 
that of in spoken data in terms of expressions. 
Here, we first adopt psychological theories 
(Plutchik, 1980; Turner, 2000) to create an emo-
tion taxonomy, and then design an emotion anno-
tation scheme based on the taxonomy. 
Since most of the previous emotion corpora 
are either too small (Xu et al, 2008) or compara-
tively ineffective in terms of accuracy (Tokuhisa 
et al, 2008), they cannot satisfy the requirements 
of emotion computing. In this paper, based on 
Natural Semantic Metalanguage (NSM), a cogni-
tive approach to human emotions (which will be 
discussed in the later section), we create an au-
tomatic emotion annotation system. While this 
annotation system needs only a little training da-
ta and does not require human supervision, the 
corpus still maintains a comparatively high qual-
ity. Another significant advantage of our auto-
matic annotation system is that it can easily adapt 
to different emotion applications by simply sup-
plying different training data. 
Most of the existing emotion theories study 
emotions from the biological and psychological 
perspectives, hence they cannot easily apply to 
NLP. Fortunately, NSM, one of the prominent 
cognitive models exploring human emotions, 
offers a comprehensive and practical approach to 
emotions (Wierbicka 1996.) NSM describes 
complex and abstract concepts, such as emotions, 
in terms of simpler and concrete ones. In such a 
way, emotions are decomposed as complex 
events involving a cause and a mental state, 
which can be further described with a set of uni-
versal, irreducible cores called semantic primi-
tives. This approach identifies the exact differ-
ences and connections between emotion concepts 
in terms of the causes, which provide an imme-
diate cue for emotion detection and classification. 
We believe that the NSM model offers a plausi-
ble framework to be implemented for automatic 
emotion computing.  
3 Emotion annotation scheme 
3.1 The emotion taxonomy 
Although there are many emotion theories devel-
oped in different fields, such as biology, psy-
chology, and linguistics, most of them agree that 
emotion can be divided into primary emotions 
and complex emotions (i.e. the combinations of 
2
primary emotions.) There is still controversy 
over the selection of primary emotions, nonethe-
less, ?happiness?, ?sadness?, ?anger?, and ?fear? 
are considered as primary emotions by most of 
emotion theories.  
Plutchik?s emotion taxonomy (Plutchik 1980), 
one of the classic emotion taxonomies, also fol-
lows the division of primary emotions and com-
plex emotions, and Turner's taxonomy (Turner 
2000), which is based on Plutchik?s work, allows  
more flexible combinations of primary emotions. 
In this paper, we adopt Turner?s taxonomy, with 
the two main points emphasized: 
1) For each primary emotion, it is divided into 
three levels according to its intensity: high, mod-
erate, and low. Besides ?happiness,? ?sadness,? 
?anger? and ?fear,? Turner also suggests that 
?disgust? and ?surprise? can be primary emo-
tions (Turner 1996; Turner 2007). In Chinese, 
the character ??? (?surprise?) has a strong abil-
ity to form many emotion words, such as ?? 
(surprise and happiness), and ?? (surprise and 
fear), which is consistent with the explanation of 
?surprise? emotion by Plutchik (1991): ?when 
the stimulus has been evaluated, the surprise may 
quickly change to any other emotion.? Therefore, 
in our annotation scheme, we consider ?happi-
ness,? ?sadness,? ?anger,? ?fear,? and ?surprise? 
as primary emotions. 
2) Complex emotion can be divided into first-
order complex emotions (consisting of two pri-
mary emotions), second-order complex emotions 
(consisting of three primary emotions), and so on, 
according to the number of primary emotions 
that involves in the complex emotion. For exam-
ple, ?pride? (happiness + fear) is a first-order 
complex emotion, which contains a greater 
amount of ?happiness? with a lesser amount of 
?fear.? 
Tables 1 and 2 show some keywords in Turn-
er?s taxonomy, and the symbol ?//? is to separate 
different emotion types. Table 1 lists the five 
most common English keywords and their cor-
responding primary emotions, and Table 2 lists 
the English keywords and their corresponding 
complex emotions. In Table 2, several emotion 
keywords, which express similar emotion 
meaning, are grouped into an emotion type. For 
example, the emotion keywords ?awe, reverence, 
veneration? are grouped into emotion type 
?awe.? For a complex emotion, the order of pri-
mary emotions indicates the importance of those 
primary emotions for that complex emotion. For 
examples, ?envy? is ?fear + anger,? which con-
tains a greater amount of ?fear? with a lesser 
amount of ?anger? whereas ?awe? is ?fear + 
happiness,? which contains a greater amount of 
?fear? with a lesser amount of ?happiness.?  
For English emotion keywords, as Turner?s 
taxonomy missed some common emotion key-
words, we add the emotion keywords from 
Plutchik's taxonomy. Besides, unlike Chinese, 
English words have morphological variations, for 
example, the emotion keyword ?pride? can occur 
in text with the various formats: ?pride,? 
?prides,? ?prided,? ?proud,? ?proudly.? As 
shown in Tables 1 and 2, there are 188 English 
lemmas in our taxonomy. In total, there are 720 
emotion keywords if morphology is taken into 
account.  
Since Turner?s emotion taxonomy is cogni-
tive-based, it is versatile for different languages 
although there is no one-to-one mapping. We 
also explore Chinese emotion taxonomy in our 
previous work (Chen at el., 2009). We first select 
emotion keywords from the cognitive-based feel-
ing words listed in Xu and Tao (2003), and then 
map those emotion keywords to Turner?s taxon-
omy with adaptation for some cases. Lastly, 
some polysemous emotion keywords are re-
moved to reduce ambiguity, and 226 Chinese 
emotion keywords remain. 
Moreover, Turner?s taxonomy is a compara-
tively flexible structure, and more extensions can 
be done for different applications. For example, 
for a complex emotion, not only its primary emo-
tions are listed, but also the intensity of the pri-
mary emotions can be given. For instance, three 
emotion types, which belong to ?anger + fear,? 
are extended as follows: 
Jealousy:      Anger (Moderate) + Fear (Moderate) 
Suspicion:    Anger (Low) + Fear (Low) 
Abhorrence: Anger (High) + Fear (Low) 
Finally, we should admit that the emotion tax-
onomy is still an on-going research topic and 
needs further exploration, such as the position of 
a given emotion keyword in the emotion taxon-
omy, whether and how to group similar emotion 
keywords, and how to decompose a complex 
emotion into primary emotions. 
3.2 The emotion annotation scheme 
Given Turner?s taxonomy, we design our annota-
tion scheme to encode this kind of emotion in-
formation. Our emotion annotation scheme is 
XML scheme, and conforms with the Text En-
coding Initiative (TEI) scheme with some modi-
fications. The emotion scheme is a layer just
3
Primary Emotions Keywords 
Happiness High: ecstatic, eager, joy, enthusiastic, happy//Moderate: cheerful, satisfy, pleased, enjoy, interest//Low: 
sanguine, serene, content, grateful 
Fear High: horror, terror//Moderate: misgivings, self-conscious, scare, panic, anxious//Low: bewilder, reluct, 
shy, puzzles, confuse 
Anger High: dislike, disgust, outrage, furious, hate//Moderate: contentious, offend, frustrate, hostile, an-
gry//Low: contemptuous, agitate, irritate, annoy, impatient 
Sadness High: deject, despondent, sorrow, anguish, despair//Moderate: gloomy, dismay, sad, unhappy, disap-
point//Low: dispirit, downcast, discourage 
Surprise High: astonish//Moderate: startled, amaze, surprise 
Table1: Primary emotions and some corresponding keywords 
Combinations Keywords 
Happiness + Fear Wonder: wonder, wondering, hopeful//Pride: pride, boastful 
Happiness + Anger Vengeance: vengeance, vengeful//Calm: appeased, calmed, calm, soothed//Bemused: bemused 
Happiness + Sadness Yearning: nostalgia, yearning 
Fear + Happiness Awe: awe, reverence, veneration 
Fear + Anger Antagonism: antagonism, revulsed//Envy: envy 
Fear + Sadness Worried: dread, wariness, pensive, helpless, apprehension, worried 
Anger +Happiness Unfriendly: snubbing, mollified, rudeness, placated, apathetic, unsympathetic, unfriendly, unaffection-
ate//Sarcastic: sarcastic 
Anger + Fear Jealousy: jealous//Suspicion: suspicion, distrustful//Abhorrence: abhorrence 
Anger + Sadness Depressed: bitter, depression//Intolerant: intolerant  
Sadness +Happiness Acceptance: acceptance, tolerant//Solace: moroseness, solace, melancholy 
Sadness+ Fear Hopeless: forlorn, lonely, hopeless, miserable//Remorseful: remorseful, ashamed, humiliated 
Sadness+ Anger Discontent: aggrieved, discontent, dissatisfied, unfulfilled//Boredom: boredom//Grief: grief, sullenness 
Surprise + Happiness Delight: delight 
Surprise + Sadness Embarrassed: embarrassed 
Table 2:  First-order complex emotions and some corresponding keywords 
 
beyond a sentence, and encodes emotion infor-
mation for a sentence. This annotation scheme 
can be compatible for any TEI-based annotated 
corpora as long as sentences are clearly marked. 
The emotion-related elements (tags) in our 
annotation scheme are described as follows. For 
easy demonstration, our elements are defined 
with the format of British National Corpus 
(BNC) annotation scheme1 , and our examples 
are also based on BNC annotated text. Figure 1 
gives the definition of each element, and Figure 
2 shows several examples using our annotation 
scheme. Note that <s> element is a tag for a sen-
tence-like division of a text, and its attribute ?n? 
gives the sentence index. In Figure 2, Sentence 1, 
which expresses emotions by emotion keywords, 
contains two types of emotions: ?surprise? (pri-
mary emotion) and ?jealousy? (complex emo-
tion); Sentence 2 is a neutral sentence. 
<emotion> element 
It is used only when the sentence expresses 
emotions. It contains a list of <emotionType> 
elements and a <s> element. As a sentence may 
                                                 
1 http://www.natcorp.ox.ac.uk/XMLedition/U
RG/ 
express several emotions, an <emotion> element 
can contain several <emotionType> elements, 
and each <emotionType> element describes an 
emotion occurring in that sentence separately. 
<neutral> element 
It is used only when the sentence does not 
contain any emotion expression. It contains only 
a <s> element. 
<emotionType> element 
It describes a type of emotion in that sentence.  
It contains an ordered sequence of <pri-
maryEmotion> elements. Attribute ?name? pro-
vides the name of the emotion type, such as 
?surprise?, ?jealousy,? and so on, and it is op-
tional. If the emotion type is a primary emotion, 
the <emotionType> element will have only one 
<primaryEmotion> element, which encodes the 
information of this primary emotion. If the emo-
tion is a complex emotion, the <emotionType> 
element will have several <primaryEmotion> 
elements (each of them describes the primary 
emotion involved in that complex emotion.) At-
tribute ?keyword? is an optional attribution if 
annotators want to provide the indicator of a text 
for that emotion. 
4
   
<primaryEmtion> element 
It describes the property of a primary emotion 
involved in the emotion type. There are three 
attributes: ?order,? ?name,? and ?intensity.?  
?Order? gives the weight of this primary emo-
tion in the emotion type, and the weight value 
decreases with the ascending ?order? value. 
?Name? and ?intensity? provide the name and 
intensity of a primary emotion. To encode the 
information in our emotion taxonomy, the value 
of ?order? is {1,2,3,4,5}, the value of ?name? is 
{?happiness,? ?sadness,? ?anger,?  ?fear?, ?sur-
prise? }, and  the value of ?intensity? is {?high?, 
?moderate?, ?low?.} 
The <primaryEmotion> element seems to be 
redundant because its encoded information can 
be obtained from the given emotion taxonomy if 
the name of the emotion type is available, but 
the presence of this element can make our anno-
tation scheme more robust. Sometimes emotion 
is so complicated (especially for those emotion 
expressions without any explicit emotion key-
word) that an annotator may not be able to find 
an exact emotion type to match this emotion, or 
to list all involved primary emotions. For those 
subtle cases, emotion annotation can be simpli-
fied to list the involved primary emotions as 
many as possible through <primaryEmotion> 
elements. For example, in Sentence 3 in Figure 2, 
although there is no emotion keyword occurring, 
the word ?hurt? indicates the presence of an 
emotion, which at least involves ?sadness.? 
However, because it is hard to explicitly list oth-
er primary emotions, therefore, we give only the 
annotation of ?sadness.?  
Our annotation scheme has the versatility to 
provide emotion data for different applications. 
For example, if textual information input anno-
tated with our scheme is provided for the Japa-
nese robot Saya (Hashimoto et al 2006) to con-
trol her facial emotion expression, a simple 
mapping from our 24 emotion types can be done 
automatically to Saya?s six emotion types, i.e. 
surprise, fear, disgust, anger, happiness, and 
sadness. As four of her emotion types are also 
unique primary emotions, using information en-
coded in <emotionType> element and <pri-
maryEmotion> element will ensure unique 
many-to-one mapping and the correct robotic 
expressions. A trickier case involves her ?anger? 
and ?disgust? emotions. The emotion type ?an-
ger? in our taxonomy includes emotion words 
?anger? and ?disgust?. However, with the ?key-
word? information provided in <emotionType> 
element, a small subset of ?anger? emotion in 
our taxonomy can be mapped to ?disgust? in 
Saya?s system. For example, we could map 
keywords ?dislike, disgust, hate? to ?disgust?, 
element emotion 
{ 
(emotionType)+, 
<s> 
} 
element emotionType 
{ 
attribute name (optional), 
attribute keyword (optional), 
(primaryEmotion)+ 
} 
element primaryEmotion 
{ 
attribute order (optional), 
attribute name (necessary), 
attribute intensity (optional) 
} 
element neutral 
{  
<s> 
} 
Figure 1: The definition of emotion-related elements 
<emotion> 
<emotionType name =  "surprise"  keyword ="surprised"> 
<primaryEmotion  order =  "1" name =  "surprise"  intensity = "moderate"></primaryEmotion> 
</emotionType>   
<emotionType name = "jealousy"  keyword = ?jealousy?> 
<primaryEmotion  order =  "1"  name = "anger" intensity =  "moderate"></primaryEmotion> 
<primaryEmotion  order =  "2"  name =  "fear"   intensity =  "moderate"></primaryEmotion> 
</emotionType> 
<s n = "1"> Hari was surprised at the rush of pure jealousy that swept over her at the mention of Emily Grenfell .</s> 
</emotion> 
<neutral> 
<s n = "2"> By law no attempts may be made to hasten death or prolong the life of the sufferer . </s> 
</neutral> 
<emotion> 
<emotionType> 
<primaryEmotion name =  "sadness"></primaryEmotion> 
</emotionType>    
<s n = "3">He looked hurt when she did n't join him , his emotions transparent as a child 's . </s> 
</emotion> 
Figure 2: The example of sentence annotation 
 
5
and all the remaining ones, such as ?outrage, 
furious,? to ?anger.? 
4 Emotion-driven corpus creation 
Similar to most corpora, our corpus creation is 
designed to satisfy the requirements of real emo-
tion computing. Emotions can be expressed with 
or without emotion vocabulary in the text. It 
seems to be intuitive that emotion computing for 
a context with emotion keywords can be satis-
factory when the collection of emotion vocabu-
lary is comprehensive, such as ?joyful? indicates 
the presence of ?happiness? emotion. However, 
this intuitive approach cannot work well because 
of the ambiguity of some emotion keywords and 
the emotion context shift as the sentiment shift 
(Polanyi and Zaenen, 2004). Moreover, the de-
tection of emotions in a context without emotion 
keywords is very challenging. To deal with these 
problems, we build the emotion corpus, which is 
motivated by the NSM theory. 
According to the NSM theory, an emotion is 
provoked by a stimulus. This indicates one pos-
sible way to detect emotions in text, i.e. the de-
tection of emotional stimulus, which is often 
provided in the text. In other words, emotion 
corpus is a collection of emotion stimuli. Since 
emotion is subjective, the stimulus-based ap-
proach works only when its context is provided. 
For example, the stimulus ? ?build a gym for 
this community? ? may cause different emotions, 
such as ?surprise?, ?happy? and so on, depend-
ing on its context. We also notice that the text 
containing an emotion keyword may contain 
emotional stimulus and its context. Thus, a natu-
ral corpus creation approach comes out. 
 In our system, a pattern-based approach is 
used to collect the emotion corpus, which is sim-
ilar to the one used in Tokuhisa et al (2008), but 
we do not limit to event-driven emotions 
(Kozareva et al, 2008), and adjust our rules to 
improve the quality of emotion annotation. 
There are five steps in our emotion sentence an-
notation as given below, and Steps (2) and (3) 
are to improve the annotation quality. 
1) Extract emotion sentences: sentences con-
taining emotion keywords are extracted by 
keyword matching.  
2) Delete ambiguous structures: some ambigu-
ous sentences, which contain structures such 
as negation and modal, are filtered out.  
3) Delete ambiguous emotion keywords: if an 
emotion keyword is very ambiguous, all sen-
tences containing this ambiguous emotion 
keyword are filtered out. 
4) Give emotion tags: each remaining sentence 
is marked with its emotion tag according to the 
emotion type which the focus emotion word 
belongs to (refer to Tables 1 and 2.) 
5) Ignore the focus emotion keywords: for 
emotion computing, the emotion word is re-
moved from each sentence.  
 Polanyi and Zaenen (2004) addressed the is-
sue of polarity-based sentiment context shift, 
and the similar phenomenon also exists in emo-
tion expressions. In our corpus creation, two 
kinds of contextual structures are handled with: 
the negation structure and the modal structure. 
In both English and Chinese, a negated emotion 
expression can be interpreted as one of the three 
possible meanings (as shown in Figure 3): oppo-
site to the target emotion (S1), deny the exis-
tence of the target emotion (S2), or confirm the 
existence of the target emotion (S3). The modal 
structure often indicates that the emotion expres-
sion is based on the counter-factual assumption, 
hence the emotion does not exist at all (S4 and 
S5 in Figure 3). Although Chinese and English 
have different interpretations about the modal 
structure, for emotion analysis, those sentences 
often do not express an emotion. Therefore, to 
ensure the quality of the emotion corpus, all sen-
tences containing a negation structure or a modal 
structure, which are detected by some rules plus 
a list of keywords (negation polarity words for 
the negation structure, and modal words for the 
modal structure), are removed. 
 
To overcome the high ambiguity of some 
emotion keywords, after Step (2), for each emo-
tion keyword, five sentences are randomly se-
lected and annotated by two annotators. If the 
accuracy of five sentences is lower than 40%, 
this emotion keyword is removed from our emo-
tion taxonomy. Finally, 191 Chinese keywords 
and 645 English keywords are remained.  
Tokuhisa et al found that a big challenge for 
emotion computing, especially for emotion de-
tection, is to collect neutral sentences. Since 
neutral sentences are unmarked and hard to de-
tect, we develop a na?ve yet effective algorithm 
S1  (Neg_Happiness): I am not happy about that. 
S2 (Netural): Though the palazzo is our family home, my 
father had never been very happy there. 
S3  (Pos_Happiness): I 've never been so happy. 
S4  (Netural): I can die happy if you will look after them when 
I have gone.  
S5  (Netural): Then you could move over there and we'd all be 
happy. 
Figure 3: Structures for emotion shift 
6
to create a neutral corpus. A sentence is consid-
ered as neutral only when the sentence itself and 
its context (i.e. the previous sentence and the 
following sentence) do not contain any of the 
given emotion keywords. 
We run our emotion sentence extraction and 
neutral sentence extraction on three corpora: the 
Sinica Corpus (Chinese), the Chinese Gigaword 
Corpus, and the British National Corpus (BNC, 
English), and create three emotion corpora and 
three neutral corpora separately. The Sinica 
Corpus is a balanced Chinese corpus, which in-
cludes documents in 15 kinds of genres; The 
Chinese Gigaword Corpus is a huge collection 
of news reports; The BNC is also a balanced 
corpus, which collects documents from different 
domains.  
To estimate the accuracy of our emotion sen-
tence extraction, we randomly select about 1000 
sentences from the three emotion corpora, and 
have two annotators to check it. Table 3 lists the 
accuracy of those emotions sentences (emotion 
corpus.) To test how good this straightforward 
neutral sentence extraction strategy is, about 
1000 sentences are randomly selected from each 
of the three neutral corpora and are checked by 
two annotators. Table 3 lists the accuracy of 
those neutral sentences (neutral corpus.)  
 Emotion corpus Neutral corpus 
Gigaword 82.17 98.61 
Sinica 77.56 98.39 
BNC 69.36 99.50 
Table 3: The accuracy of the emotion-driven corpora 
From Table 3, the high accuracy of neutral 
corpus proves that our approach is effective in 
extracting neutral sentences from the document-
based corpus which contains contextual informa-
tion. Although the accuracy of emotion corpus is 
lower, it is still much higher than the one re-
ported by Kozareva et al (2008), i.e. 49.4. The 
accuracy is significantly increased by deleting 
ambiguous emotion keywords in Step (3). For 
the 2,474 randomly selected Chinese sentences, 
the overall accuracy of the remaining 1,751 sen-
tence is increased by about 14% after Step (3). 
For the 803 randomly selected English sentences, 
the accuracy of the remaining 473 sentence is 
increased about 21% after Step (3). Whether or 
how the ambiguous emotion keywords in Step 3 
are removed is a tradeoff between the coverage 
and the accuracy of the emotion corpus.  
From Table 3, we also find that the accuracy 
of English emotion corpus is much lower than 
Chinese emotion corpus, which indicates Eng-
lish emotion sentences expressed by emotion 
keywords are more ambiguous than that of Chi-
nese. Moreover, during our emotion corpus 
building, 20.2% of Sinica sentences and 22.4% 
of Gigaword sentences are removed in Step (2) 
and (3), on the contrary, 41.2% of BNC sen-
tences are deleted. Although it is more difficult 
to develop the rules in Step (2) and (3) for Chi-
nese than for English, it also confirms the higher 
ambiguity of emotion expressions in English due 
to the ambiguity of emotion keyword. Finally, 
because of the comparatively-high percentage of 
the sentences removed in Step (2) and (3), more 
exploration about those sentences is needed, 
such as the emotion distribution, the expression 
patterns and so on, and how to re-incorporate 
them into the emotion corpus without hurting the 
whole quality is also our future work.  
We also explore emotions through the sen-
tences (no-emotion-keyword sentences) that do 
not contain any given emotion keyword, because 
our approach extracts only partial neutral sen-
tences and partial emotion sentences in reality. 
For each corpus, about 1000 no-emotion-
keyword sentences are randomly selected and 
checked by two annotators. It is surprising that 
only about 1% of those sentences express emo-
tions. This indicates that it is important for real 
emotion computing, which mainly works on 
formal written text, to deal with the emotion ex-
pressions which contain emotion keywords and 
however are ambiguous, such as the sentences 
deleted in Steps (2) and (3). More exploration is 
needed for the emotion and neutral sentence dis-
tribution on other kinds of written text, such as 
blogs, and on spoken text. 
The unsupervised corpus creation approach 
can easily be adapted for different languages and 
different emotion applications, provided that the 
keyword collection and patterns in Step (2) and 
(3) need some changes.  Moreover, another big 
advantage of our approach is that it can avoid 
the controversy during emotion annotation. 
Emotion is subjective, and therefore disagree-
ment for emotion types often arises if the emo-
tion is not expressed through an explicit emotion 
keyword.  
Overall, the annotated corpus created by the 
unsupervised approach has a comparatively high 
quality, and is suitable for the emotion comput-
ing. As the size of the neutral corpus is much 
bigger than its corresponding emotion corpus, to 
avoid model bias, we randomly select some neu-
tral sentences from the neutral corpus, combin-
7
ing with its corresponding emotion sentences to 
form a complete emotion-driven corpus. 
5 Emotion computing system 
In this paper, we present some pilot work to 
prove that our emotion-driven corpus is useful 
for emotion computing. With the inclusion of 
neutral sentences, emotion detection and classi-
fication is simplified into a general classification 
problem, and a supervised machine learning 
method can be directly applied if enough anno-
tated data are obtained. Here, we choose the 
MaxEnt learning in Mallet as a classifier. 
 Both the Sinica Corpus and the Chinese Gi-
gaword Corpus are segmented, and POS-tagged. 
This allows us to implement the bag-of-words 
approach in the focus sentences in both Chinese 
and English. However, emotions are mostly hu-
man attitudes or expectations arising from situa-
tions, where situations are often expressed in 
more than a single word. Such kind of situations 
tends to be more easily extracted by word bi-
grams (2-gram word) than by word unigram (1-
gram word.) To take this into account, besides 1-
gram words, we also extract word bi-grams from 
the focus sentences.  
There are too many emotion types in our cor-
pus, which can cause data sparse; therefore, we 
choose the most frequent emotions to do explo-
ration. Besides the five primary emotions, for 
Chinese, we select another nine complex emo-
tions, and for English, we select another four 
complex emotions. Other emotion types are re-
named as ?Other Emotions.? 
Since Chinese emotion-driven corpus is much 
larger than the English one, to fairly compare the 
performance, we reduce the size of Chinese cor-
pus in our experiments. Then, for each corpus, 
we reserve 80% as the training data, 10% as the 
development data, and 10% as the test data 
(there are two sets of test data as follows.) In the 
evaluation, for each emotion sentence, if our 
system detects one of its emotion tags, we con-
sider this sentence is correctly tagged. 
Test data set 1 (TDS 1): contains about 10% 
of the sentences from the complete emotion-
driven corpus, and emotion tags are automati-
cally given during the corpus creation.  
Test data set 2 (TDS 2): contains the sen-
tences used in Table 3, which is checked by two 
annotators. If more than one emotion tags co-
exist in a sentence, all of them are chosen to la-
bel the sentence. If there exists an emotion that 
does not belong to any of the emotion types, it is 
labeled as ?Other Emotions.? 
Table 4 shows the performance (accuracy) of 
our system for Test data set 1 and 2 for both 
Chinese and English. We notice that our corpus 
creation approach is effective for emotion com-
puting. As we expect, the 2-gram words can par-
tially catch the emotion stimulus, and improves 
the performances. However, the overall per-
formance is still very low, which indicates that 
emotion computing is a difficult task. From the 
error analysis, it is surprised that for Chinese, 
the mislabeling of emotion sentences as neutral 
sentences (?emotion? vs. ?neutral?) is a common 
error, and whereas, for English, two kinds of 
errors: ?emotion? vs. ?neutral? and ?focus emo-
tions? vs. ?Other emotions? (the mislabeling of a 
sentence with a focus emotion as ?Other emo-
tions,?) occupy at least 50%. The error distribu-
tion confirms the importance of emotion detec-
tion during emotion computing. The high fre-
quency of the error of ?focus emotions? vs. 
?Other Emotions? in English may be because 
there are fewer focus emotion types for English.  
 1-gram words  {1,2}-gram words 
Chinese TDS 1 53.92 58.75 
English TDS 1 44.02 48.20 
Chinese TDS 2 37.18 39.95 
English TDS 2 33.24 36.31 
Table 4: The performances of our system for the test data  
6 Conclusion 
Emotion, no matter its annotation or computing, 
is still a new and difficult topic. In this paper, we 
apply emotion theories to design a cognitive-
based emotion annotation scheme, which are 
robust and versatile so that it can encode differ-
ent levels of emotion information for different 
emotion computing. Moreover, motivated from 
NSM, we develop an unsupervised approach to 
create a large and comparatively high-quality 
corpus for emotion computing, which is proven 
in our pilot experiments to be useful. Moreover, 
this approach makes emotion computing for dif-
ferent applications possible through a little mod-
ification. 
Certainly, there are some issues remaining un-
solved. For corpus construction, we will explore 
emotion distribution in other kinds of corpora, 
such as blog and dialog, and make analysis of 
ambiguous emotion sentences, such as negation 
structure and modal structure. For emotion com-
puting, we did only pilot experiments and more 
work needs to be done, such as feature extrac-
tion. 
8
References  
W. B. Cannon. 1927. The James-Lange theory of 
emotions: A Critical Examination and an Alterna-
tive Theory. American Journal of Psychology, 39, 
106-124. 
Y. Chen, S. Y. M. Lee and C. R. Huang, 2009. Con-
struction of Chinese Emotion Corpus with an Un-
supervised Approach. In CNCCL-2009, 2009. (in 
Chinese) 
T. Hashimoto, S. Hiramatsu, T. Tsuji and H. Kobaya-
shi. 2006. Development of the Face Robot SAYA 
for Rich Facial Expressions. SICE-ICASE Inter-
national Joint Conference, Busan,Korea. 
J. Hobbs and A. Gordon. 2008. The Deep Lexical 
Semantics of Emotions. Workshop on Sentiment 
Analysis: Emotion, Metaphor, Ontology and 
Terminology (EMOT-08), 6th International con-
ference on Language Resources and Evaluation 
(LREC-08), Marrakech, Morocco, May 27, 2008. 
P. Livia, A. Zaenen. 2004. Contextual Valence Shift-
ers. In Shanahan, J. G., Y. Qu, and J. Wiebe 
(Eds.), Computing Attitude and Affect in Text: 
Theory and Applications, pp. 1-10. 
Z. Kozareva, Borja Navarro, Sonia Vazquez, and 
Andres Nibtoyo. 2007. UA-ZBSA: A Headline 
Emotion Classification through Web Information. 
In Proceedings of the 4th International Workshop 
on Semantic Evaluations.  
Y. Y. Mathieu. 2005.  Annotations of Emotions and 
Feelings in Texts. In Conference on Affective 
Computing and intelligent Interaction (ACII2005), 
Beijing, Springer Lecture Notes in Computer Sci-
ence, pp. 350-357. 
R. Mihalcefa, and Hugo Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of AAAI.  
R. Plutchik. 1991. The Emotions. University Press of 
America, Inc. 
R. Plutchik. 1980. Emotions: A psychoevolutionary 
synthesis. New York:Harper & Row. 
M. Schr?der, H. Pirker and M. Lamolle. 2006. First 
suggestions for an emotion annotation and repre-
sentation language. In L. Deviller et al (Ed.), 
Proceedings of LREC'06 Workshop on Corpora 
for Research on Emotion and Affect (pp. 88-92). 
Genoa, Italy. 
J. G. Shanahan, Y. Qu and J. Wiebe. 2006. Comput-
ing attitude and affect in text: theory and applica-
tions, Springer. 
R. Tokuhisa, K. Inui, and Y. Matsumoto (Eds.) 2008. 
Emotion Classification Using Massive Examples 
Extracted from the Web. COLING.   
J. H. Turner. 2007. Human Emotions: A sociological 
theory. New York : Routledge, 2007. 
J. H. Turner. 2000. On the origins of human emotions: 
A sociological inquiry into the evolution of hu-
man affect. Stanford, CA: Stanford University 
Press.  
J. H. Turner. 1996. The Evolution of Emotions in 
Humans: A Darwinian?Durkheimian Analysis. 
Journal for the theory of social behaviour26:1-34 
L. Xu, H. Lin, J. ZHAO.2008. Construction and 
Analysis of Emotional Corpus. JOURNAL OF 
CHINESE INFORMA TION PROCESSIN. 
X. Y. Xu, and J. H. Tao. 2003. The study of affective 
categorization in Chinese. The 1st Chinese Con-
ference on Affective Computing and Intelligent 
Interaction. Beijing, China. 
A. Wierzbicka, 1996. Semantics: Primes and Univer-
sals. Oxford: Oxford University Press. 
 
9
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179?187,
Beijing, August 2010
Emotion Cause Detection with Linguistic Constructions 
 Ying Chen*?, Sophia Yat Mei Lee?, Shoushan Li?, Chu-Ren Huang?  
*
 Dep. of Computer Engineering 
China Agricultural University 
?Dep. of Chinese and Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,shoushan.li,churenhuang}@gmail.com 
 
 
Abstract 
This paper proposes a multi-label ap-
proach to detect emotion causes. The 
multi-label model not only detects mul-
ti-clause causes, but also captures the 
long-distance information to facilitate 
emotion cause detection. In addition, 
based on the linguistic analysis, we cre-
ate two sets of linguistic patterns during 
feature extraction. Both manually gener-
alized patterns and automatically gener-
alized patterns are designed to extract 
general cause expressions or specific 
constructions for emotion causes. Ex-
periments show that our system 
achieves a performance much higher 
than a baseline model.   
1 Introduction 
Text-based emotion processing has been a cen-
ter of attention in the NLP field in the past few 
years. Most previous researches have focused 
on detecting the surface information of emo-
tions, especially emotion classes, e.g., ?happi-
ness? and ?anger? (Mihalcea and Liu 2006, 
Strapparava and Mihalcea 2008, Abbasi et al 
2008, Tokuhisa et al 2008). Although most 
emotion theories recognize the important role of 
causes in emotion analysis (Descartes, 1649; 
James, 1884; Plutchik 1980, Wierzbicka 1999), 
very few studies explore the interactions be-
tween emotion and causes. Emotion-cause in-
teraction is the eventive relation which poten-
tially yields the most crucial information in 
terms of information extraction. For instance, 
knowing the existence of an emotion is often 
insufficient to predict future events or decide on 
the best reaction. However, if the emotion cause 
is known in addition to the type of emotion, 
prediction of future events or assessment of po-
tential implications can be done more reliably. 
In other words, when emotion is treated as an 
event, causal relation is the pivotal relation to 
discover. In this paper, we explore one of the 
crucial deep level types of information of emo-
tion, i.e. cause events.  
Our study focuses on explicit emotions in 
which emotions are often presented by emotion 
keywords such as ?shocked? in ?He was 
shocked after hearing the news?. Emotion caus-
es are the explicitly expressed propositions that 
evoke the presence of the corresponding emo-
tions. They can be expressed by verbs, nomi-
nalizations, and nominals. Lee et al (2010a) 
explore the causes of explicit emotions by con-
structing a Chinese emotion cause corpus. 
Based on this corpus, we formalize the emotion 
cause detection problem through extensive data 
analysis. We find that ~14% emotion causes are 
complicated events containing multi-clauses, to 
which previous cause detection systems can 
hardly be applied directly. Most previous cause 
detection systems focus on the causal relation 
between a pair of small-size text units, such as 
clauses or phrases. They are thus not able to 
detect emotion causes that are multi-clauses. In 
this paper, we formalize emotion cause detec-
tion as a multi-label classification task (i.e. each 
instance may contain more than one label), 
which allows us to capture long-distance infor-
mation for emotion cause detection. 
In term of feature extraction, as emotion 
cause detection is a case of cause detection, 
some typical patterns used in existing cause de-
tection systems, e.g., ?because? and ?thus?, can 
be adopted. In addition, various linguistic cues 
are examined which potentially indicate emo-
tion causes, such as causative verbs and epis-
temic markers (Lee at al. 2010a). Then some 
linguistic patterns of emotion causes are manu-
179
ally generalized by examining the linguistic 
context of the empirical data (Lee et al, 2010b). 
It is expected that these manually generalized 
patterns often yield a low-coverage problem. 
Thus, we extracted features which enable us to 
automatically capture more emotion-specific 
constructions. Experiments show that such an 
integrated system with various linguistic fea-
tures performs promisingly well. We believe 
that the present study should provide the foun-
dation for future research on emotion analysis, 
such as the detection of implicit emotion or 
cause.  
The paper is organized as follows. Section 2 
discusses the related work on cause-effect de-
tection. Section 3 briefly describes the emotion 
cause corpus, and then presents our data analy-
sis. Section 4 introduces the multi-label classifi-
cation system for emotion cause detection. Sec-
tion 5 describes the two kinds of features for our 
system, one is based on hand-coded patterns and 
the other is the generalized features. Section 6 
presents the evaluation and performance of our 
system. Section 7 highlights our main contribu-
tions and the possible future work. 
2 Related Work 
Most previous studies on textual emotion proc-
essing focus on emotion recognition or classifi-
cation given a known emotion context (Mihal-
cea and Liu 2006, Strapparava and Mihalcea 
2008, Abbasi et al 2008, Tokuhisa et al 2008). 
However, the performance is far from satisfac-
tory. One crucial problem in these works is that 
they limit the emotion analysis to a simple clas-
sification and do not explore the underlying in-
formation regarding emotions. Most theories 
conclude that emotions are often invoked by the 
perception of external events. An effective emo-
tion recognition model should thus take this into 
account.  
To the best of our knowledge, little research 
has been done with respect to emotion cause 
detection. Lee et al (2010a) first investigate the 
interactions between emotions and the corre-
sponding causes from a linguistic perspective. 
They annotate a small-scale emotion cause cor-
pus, and identify six groups of linguistic cues 
facilitating emotion cause detection. Based on 
these findings, they develop a rule-based system 
for automatic emotion cause detection (Lee et 
al., 2010b).  
Emotion cause detection can be considered as 
a kind of causal relation detection, which has 
been intensively studied for years. Most previ-
ous cause detection studies focus on a specific 
domain, such as aviation (Persing and Ng, 2009) 
and finance (Low, et al, 2001). Few works 
(Marcu and Echihabi, 2002; Girju, 2003; Chang 
and Choi, 2005) examine causal relation for 
open domains. 
In recognizing causal relations, most existing 
systems involve two steps: 1) cause candidate 
identification; 2) causal relation detection. To 
simplify the task, most systems omit the step of 
identifying cause candidates. Instead, they often 
predefine or filter out possible causes based on 
domain knowledge, e.g., 14 kinds of cause types 
are identified for aviation incidents (Persing and 
Ng, 2009). For events without specific domain 
information, open-domain systems choose to 
limit their cause candidate. For example, the 
cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with fixed conjunction 
words (Marcu and Echihabi, 2002). 
Given pairs of cause-effect candidates, causal 
relation detection is considered as a binary clas-
sification problem, i.e. ?causal? vs. ?non-
causal?. In general, there are two kinds of in-
formation extracted to identify the causal rela-
tion. One is patterns or constructions expressing 
a cause-effect relation (Chang and Choi, 2005; 
Girju, 2003), and the other is semantic informa-
tion underlying in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of in-
formation usually interact with each other in a 
real cause detection system. 
In the literature, the three common classifica-
tion methods, i.e. unsupervised, semi-supervised, 
and supervised, have all been used for cause 
detection systems. Marcu and Echihabi (2002) 
first collected a cause corpus using an unsuper-
vised approach with the help of several conjunc-
tion words, such as ?because? and ?thus?, and 
determined the causal relation for a clause pair 
using the word pair probability. Chang and Choi 
(2005) used a semi-supervised method to recur-
sively learn lexical patterns for cause recogni-
tion based on syntactic trees. Bethard and Mar-
tin (2008) put various causal information in a 
180
supervised classifier, such as the temporal in-
formation and syntactic information.  
For our emotion cause detection, several 
practical issues need to be investigated and re-
solved. First, for the identification of cause can-
didates, we need to define a reasonable span of 
a cause. Based on our data analysis, we find that 
emotion causes often appear across phrases or 
even clauses. Second, although in emotion 
cause detection the effect is fixed, the cause is 
open-domain. We also notice that besides the 
common patterns, emotion causes have their 
own expression patterns. An effective emotion 
cause detection system should take them into 
account. 
3 Corpus Analysis  
In this section, we briefly introduce the Chinese 
emotion cause corpus (Lee et al, 2010a), and 
discuss emotion cause distribution. 
3.1 Emotion Cause corpus 
Lee at al. (2010a) made the first attempt to ex-
plore the correlation between emotions and 
causes, and annotate a Chinese emotion cause 
corpus. The emotion cause corpus focuses on 
five primary emotions, namely ?happiness?, 
?sadness?, ?fear?, ?anger?, and ?surprise?. The 
emotions are explicitly expressed by emotion 
keywords, e.g., gao1xing4 ?happy?, shang1xin1 
?sad?, etc. The corpus is created as follows. 
1. 6,058 entries of Chinese sentences are ex-
tracted from the Academia Sinica Balanced 
Corpus of Mandarin Chinese (Sinica Cor-
pus) with the pattern-match method as well 
as the list of 91 Chinese primary emotion 
keywords (Chen et al, 2009). Each entry 
contains the focus sentence with the emo-
tion keyword ?<FocusSentence>? plus the 
sentence before ?<PrefixSentence>? and 
after ?<SuffixSentence>? it. For each entry, 
the emotion keywords are indexed since 
more than one emotion may be presented in 
an entry;  
2. Some preprocessing, such as balancing the 
number of entry among emotions, is done 
to remove some entries. Finally, 5,629 en-
tries remain; 
3. Each emotion keyword is annotated with 
its corresponding causes if existing. An 
emotion keyword can sometimes be associ-
ated with more than one cause, in such a 
case, both causes are marked. Moreover, 
the cause type is also identified, which is 
either a nominal event or a verbal event (a 
verb or a nominalization).  
Lee at al. (2010a) notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause. 
3.2 The Analysis of Emotion Causes 
To have a deeper understanding of emotion 
cause detection, we take a closer look at the 
emotion cause distribution, including the distri-
bution of emotion cause occurrence and the dis-
tribution of emotion cause text. 
 
The occurrence of emotion causes: According 
to most emotion theories, an emotion is gener-
ally invoked by an external event. The corpus 
shows that, however, 20% of the emotional en-
tries have no cause. Entries without causes ex-
plicitly expressed are mainly due to the follow-
ing reasons: 
i) There is not enough contextual information, 
for instance the previous or the suffix sentence 
is interjections, e.g., en heng ?aha?;  
ii) When the focus sentence is the beginning 
or the ending of a paragraph, no prefix sentence 
or suffix sentence can be extracted as the con-
text. In this case, the cause may be beyond the 
context;  
iii) The cause is obscure, which can be very 
abstract or even unknown reasons.  
 
The emotion cause text: A cause is considered 
as a proposition. It is generally assumed that a 
proposition has a verb which optionally takes a 
noun occurring before it as the subject and a 
noun after it as the object. However, a cause can 
also be expressed as a nominal. In other words, 
both the predicate and the two arguments are 
optional provided that at least one of them is 
present. Thus, the fundamental issue in design-
ing a cause detection system is the definition of 
the span of a cause text. As mentioned, most 
previous studies on causal relations choose to 
ignore the identification of cause candidates. In 
this paper, we first analyze the distribution of 
cause text and then determine the cause candi-
dates for an emotion. 
Based on the emotion cause corpus, we find 
that emotion causes are more likely to be ex-
181
pressed by verbal events than nominal events 
(85% vs. 15%). Although a nominalization (a 
kind of verbal events) is usually a noun phrase, 
a proposition containing a verb plays a salient 
role in the expressions of emotion causes, and 
thus a cause candidate are more likely to be a 
clause-based unit. 
In addition, the actual cause can sometimes 
be too long and complicated, which involves 
several events. In order to explore the span of a 
cause text, we do the following analysis. 
 
Table 1: The clause distribution of cause texts 
Position Cause (%) Position Cause (%) 
Left_0 12.90 Right _0 15.54 
Left_1 31.37 Right _1  9.55 
Left_2 13.31 Right_n  
(n>1) 
9.18 
Left_n 
(n>2) 
10.15   
Total  67.73  32.27 
 
Table 2: The multi-clause distribution of cause 
text 
Same clause % Cross-clauses % 
Left_0 16.80 Left_2_1_0 0.25 
Left_1 31.82 Left_2_1 10.84 
Left_2 7.33 Left_1_0 0.62 
Right _0 18.97 Right_0_1 2.55 
Right _1  10.59   
Total 85.75  14.25 
 
Firstly, for each emotion keyword, an entry is 
segmented into clauses with four punctuations 
(i.e. commas, periods, question marks and ex-
clamation marks), and thus an entry becomes a 
list of cause candidates. For example, when an 
entry has four clauses, its corresponding list of 
cause candidates contains five text units, i.e. 
<left_2, left_1, left_0, right_0, right_1>. If we 
assume the clause where emotion keyword lo-
cates is a focus clause, ?left_2? and ?left_1? are 
previous two clauses, and ?right_1? is the fol-
lowing one. ?left_0? and ?right_0? are the partial 
texts of the focus clause, which locate in the left 
side of and the right side of the emotion key-
word, respectively. Moreover, a cause candidate 
must contain either a noun or a verb because a 
cause is either a verbal event or a nominal event; 
otherwise, it will be removed from the list. 
Secondly, we calculate whether a cause can-
didate overlaps with the real cause, as shown in 
Table 1. We find that emotion causes are more 
likely to occur in the left of emotion keyword. 
This observation is consistent with the fact that 
an emotion is often trigged by an external hap-
pened event. Thirdly, for all causes occurring 
between ?left_2? and ?right_1?, we calculate 
whether a cause occurs across clauses, as in Ta-
ble 2. We observe that most causes locate 
within the same clause of the representation of 
the emotion (85.57%). This suggests that a 
clause may be the most appropriate unit to de-
tect a cause. 
 
4 Emotion Cause Detection Based on 
Multi-label Classification 
A cause detection system is to identify the caus-
al relation between a pair of two text units. For 
emotion cause detection, one of the two text 
units is fixed (i.e. the emotion keyword), and 
therefore the remaining two unresolved issues 
are the identification of the other text unit and 
the causal relation. 
From the above data analysis, there are two 
observations. First, most emotion causes are 
verbal events, which are often expressed by a 
proposition (or a clause). Thus, we define an-
other text unit as a clause, namely a cause can-
didate. Second, as most emotion causes occur 
between ?left_2? and ?right_1? (~80%), we de-
fine the cause candidates for an emotion as 
<left_2, left_1, left_0, right_0, right_1>.  
Differing from the existing cause systems, we 
formalize emotion cause detection as a multi-
label problem. In other words, given an emotion 
keyword and its context, its label is the loca-
tions of its causes, such as ?left_1, left_0?. This 
multi-label-based formalization of the cause 
detection task has two advantages. First, it is an 
integrated system detecting causes for an emo-
tion from the contextual information. In most 
previous cause detection systems, a causal rela-
tion is identified based on the information be-
tween two small text units, i.e. a pair of clauses 
or noun phrases, and therefore it is often the 
case that long-distance information is missed. 
Second, the multi-label-based tagging is able to 
182
capture the relationship between two cause can-
didates. For example, ?left_2? and ?left_1? are 
often combined as a complicated event as a 
cause.   
As a multi-label classification task, every 
multi-label classifier is applicable. In this study, 
we use a simple strategy: we treat each possible 
combination of labels appearing in the training 
data as a unique label. Note that an emotion 
without causes is labeled as ?None?. This con-
verts multi-label classification to single-label 
classification, which is suitable for any multi-
class classification technologies. In particular, 
we choose a Max Entropy tool, Mallet1, to per-
form the classification.  
5 Linguistic Features  
As explained, there are basically two kinds of 
features for cause detection, namely pattern-
based features and semantic-based features. In 
this study, we develop two sets of patterns 
based on linguistic analysis: one is a set of ma-
nually generalized patterns, and the other con-
tains automatically generalized patterns. All of 
these patterns explore causal constructions ei-
ther for general causal relations or for specific 
emotion cause relations. 
5.1 Linguistic Cues  
Based on the linguistic analysis, Lee et al 
(2010a) identify six groups of linguistic cue 
words that are highly collocated with emotion 
causes, as shown in Table 3. Each group of the 
linguistic cues serves as an indicator marking 
the causes in different emotional constructions. 
In this paper, these groups of linguistic cues are 
reinterpreted from the computational perspec-
tive, and are used to develop pattern-based fea-
tures for the emotion cause detection system.  
 
Table 3:  Linguistic cue words for emotion 
cause detection (Lee et al 2010a) 
Group Cue Words 
I: 
Prepositions 
?for? as in ?I will do this for you?: wei4, 
wei4le 
?for? as in ?He is too old for the job?: 
dui4, dui4yu2 
?as?: yi3 
                                                 
1
 http://mallet.cs.umass.edu/ 
II: 
Conjunctions 
?because?: yin1, yin1wei4, you2yu2 
?so?: yu1shi4, suo3yi3, yin1er2 
?but?: ke3shi4 
III:  
Light Verbs ?to make?: rang4, ling4, shi3 
IV: 
Reported 
Verbs 
?to think about?: xiang3dao4, 
xiang3qi3, yi1xiang3, xiang3 lai2 
?to talk about?: shuo1dao4, shuo1qi3, 
yi1shuo1, jiang3dao4, jiang3qi3, 
yi1jiang3, tan2dao4, tan2qi3, yi1tan2, 
ti2dao4, ti2qi3, yi1ti2 
V: 
Epistemic 
Markers 
?to hear?: ting1, ting1dao4, ting1shuo1 
?to see?: kan4, kan4dao4, kan4jian4, 
jian4dao4, jian4, yan3kan4, qiao2jian4 
?to know?: zhi1dao4, de2zhi1, de2xi1, 
huo4zhi1, huo4xi1, fa1xian4, fa1jue2 
?to exist?: you3 
VI: 
Others 
?is?: deshi4 
?say?: deshuo1 
?at?: yu2 
?can?: neng2  
 
For emotion cause processing, Group I and II 
contain cues which are for general cause detec-
tion, and while Group III, IV and V include 
cues specifically for emotion cause detection. 
Group VI includes other linguistic cues that do 
not fall into any of the five groups.  
Group I covers some prepositions which all 
roughly mean ?for?, and Group II contains the 
conjunctions that explicitly mark the emotion 
cause. Group I is expected to capture the prepo-
sitions constructions in the focus clause where 
the emotion keyword locates. Group II tends to 
capture the rhetorical relation expressed by con-
junction words so as to infer causal relation 
among multi-clauses. These two groups are typ-
ical features for general cause detection. 
Group III includes three common light verbs 
which correspond to the English equivalents ?to 
make? or ?to cause?. Although these light verbs 
themselves do not convey any concrete meaning, 
they are often associated with several construc-
tions to express emotions and at the same time 
indicate the position of emotion causes. For ex-
ample, ?The birthday party made her happy?.  
One apparent difference between emotion 
causes and general causes is that emotions are 
often triggered by human activities or the per-
ception of such activities, e.g., ?glad to say? or 
?glad to hear?. Those human activities are often 
strong indicators for the location of emotion 
183
causes. Group IV and V are used to capture this 
kind of information. Group IV is a list of verbs 
of thinking and talking, and Group V includes 
four types of epistemic markers which are usu-
ally verbs marking the cognitive awareness of 
emotions in the complement position. The epis-
temic markers include verbs of seeing, hearing, 
knowing, and existing. 
  
5.2 Linguistic Patterns  
With the six groups of linguistic cues, we gen-
eralize 14 rules used in Lee et al (2010b) to 
locate the clause positions of an emotion cause, 
as shown in Table 4. The abbreviations used in 
the rules are given as follows:  
 
C = Cause 
K = Emotion keyword 
B = Clauses before the focus clause 
F = Focus clause/the clause containing the emotion 
verb 
A = Clauses after the focus clause 
 
Table 4: Linguistic rules for emotion cause de-
tection (Lee et al 2010b) 
No. Rules 
1 i) C(B/F) + III(F)  + K(F)  
ii) C = the nearest N/V before I in F/B 
2 i)  IV/V/I/II(B/F) + C(B/F) + K(F)  
ii) C = the nearest N/V before K in F 
3 i) I/II/IV/V (B) + C(B)  + K(F)  
ii) C = the nearest N/V after I/II/IV/V in B 
4 i) K(F) + V/VI(F) + C(F/A)  
ii) C = the nearest N/V after V/VI in F/A 
5 i) K(F)+II(A)+C(A)  
ii) C = the nearest N/V after II in A 
6 i) III(F) + K(F) + C(F/A)  
ii) C = the nearest N/V after K in F or A 
7 i) yue4 C yue4 K ?the more C the more K? (F)   
ii) C = the V in between the two yue4?s in F 
8 i) K(F) + C(F)  
ii) C = the nearest N/V after K in F 
9 i) V(F) + K(F)  
ii) C = V+(an aspectual marker) in F 
10 i) K(F)  + de ?possession?(F) + C(F)  
ii) C = the nearest N/V +?+N after de in F 
12 i) K(B) + IV (B) + C(F)   
ii) C = the nearest N/V after IV in F 
13 i) IV(B) + C(B) + K(F)  
ii) C = the nearest N/V after IV in B 
14 i) C(B) +  K(F)  
ii) C = the nearest N/V before K in B  
 
For illustration, an example of the rule descrip-
tion is given in Rule 1. 
Rule 1: 
i) C(B/F) + III(F) + K(F)  
ii) C = the nearest N/V before III in F/B  
 
Rule 1 indicates that the cause (C) comes before 
Group III cue words. Theoretically, in identify-
ing C, we look for the nearest verb/noun occur-
ring before Group III cue words in the focus 
clause (F) or the clauses before the focus clause 
(B), and consider the clause containing this 
verb/noun as a cause. Practically, for each cause 
candidate, i.e. ?left_1?, if it contains this 
verb/noun, we create a feature with 
?left_1_rule_1=1?. 
5.3 Generalized Patterns  
Rule-based patterns usually achieve a rather 
high accuracy, but suffer from low coverage. To 
avoid this shortcoming, we extract a generalized 
feature automatically according to the rules in 
Table 4. The features are able to detect two 
kinds of constructions, namely functional con-
structions, i.e. rhetorical constructions, and spe-
cific constructions for emotion causes.  
Local functional constructions: a cause occur-
ring in the focus clause is often expressed with 
certain functional words, such as ?because of?, 
?due to?. In order to capture the various expres-
sions of these functional constructions, we iden-
tify all functional words around the given emo-
tion keyword. For an emotion keyword, we 
search ?left_0? from the right until a noun or a 
verb is found. Next, all unigrams and bigrams 
between the noun or the verb and the emotion 
keyword are extracted. The same applies to 
?right_0?. 
Long-distance conjunction constructions: 
Group II enumerates only some typical conjunc-
tion words. To capture more general rhetorical 
relations, according to the given POS tags, the 
conjunction word is extracted for each cause 
candidate, if it occurs at the beginning of the 
candidate. 
Generalized action and epistemic verbs: 
Group IV and V cover only partial action and 
epistemic verbs. To capture possible related ex-
pressions, we take the advantage of Chinese 
characters. In Chinese, each character itself usu-
ally has a meaning and some characters have a 
strong capability to create words with extended 
meaning. For example, the character ?ting1-
listen? combines with other characters to create 
184
words expressing ?listening?, such as ting1jian4, 
ting1wen5. With the selected characters regard-
ing reported verbs and epistemic markers, each 
cause candidate is checked to see whether it 
contains the predefined characters.  
6 Experiments 
For the emotion cause corpus, we reserve 80% 
as the training data, 10% as the development 
data, and 10% as the test data. During evalua-
tion, we first convert the multi-label tag output-
ted from our system into a binary tag (?Y? 
means the presence of a causal relation; ?N? in-
dicates the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. Thus, the 
evaluation scores for binary classification based 
on three common measures, i.e. precision, recall 
and F-score, are chosen. 
6.1 Linguistic Feature Analysis 
According to the distribution in Table 1, we de-
sign a naive baseline to allow feature analysis. 
The baseline searches for the cause candidates 
in the order of <left_1, right_0, left_2, left_0, 
right_1>. If the candidate contains a noun or 
verb, consider this clause as a cause and stop. 
We run the multi-label system with different 
groups of features and the performances are 
shown in Table 5. The feature set begins with 
linguistic patterns (LP), and is then incorporated 
with local functional constructions (LFC), long-
distance conjunction constructions (LCC), and 
generalized action and epistemic verbs (GAE), 
one by one. Since the ?N? tag is overwhelming, 
we report only the Mac average scores for both 
?Y? and ?N? tags.  
In Table 5, we first notice that the perform-
ances achieve significant improvement from the 
baseline to the final system (~17%). This indi-
cates that our linguistic features are effective for 
emotion cause detection. In addition, we ob-
serve that LP and LFC are the best two effective 
features, whereas LCC and GAE have slight 
contributions. This shows that our feature ex-
traction has a strong capability to detect local 
causal constructions, and is yet unable to detect 
the long-distance or semantic causal informa-
tion. Here, ?local? refers to the information in 
the focus clause. We also find that incorporating 
LFC, which is a pure local feature, generally 
improves the performances of all cause candi-
dates, i.e. ~5% improvement for ?left_1?. This 
indicates that our multi-label integrated system 
is able to convey information among cause can-
didates.  
 
Table 5: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ LFC 72.80 71.94 72.35 
+ LCC 73.60 72.50 73.02 
+ GAE 73.90 72.70 73.26 
 
Table 6: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ LFC 55.45 89.24 
+ LCC 56.48 89.57 
+ GPE 56.84 89.68 
 
Table 6 shows the performances (F-scores) 
for ?Y? and ?N? tags separately. First, we notice 
that the performances of the ?N? tag are much 
better than the ones of ?Y? tag. Second, it is sur-
prising that incorporating the linguistic features 
significantly improves only the ?Y? tag (from 
33% to 56%), but does not affect ?N? tag. This 
suggests that our linguistic features are effective 
to detect the presence of causal relation, and yet 
do not hurt the detection of ?non_causal? rela-
tion. For the ?Y? tag, the features LP and LFC 
achieve ~15% and ~7% improvements respec-
tively. LCC and GPE, on the other hand, show 
slight improvements only. 
Finally, Table 7 shows the detailed perform-
ances of our multi-label system with all features. 
The last row shows the overall performances of 
?Y? and ?N? tags. For the ?Y? tag, the closer the 
cause candidates are to the emotion keyword, 
the better performances the system achieves. 
This proves that the features we propose effec-
tively detect local emotion causes, more effort, 
185
Table 7: The detailed performance for the multi-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 68.92 68.92 68.92 Left_0 93.72 93.72 93.72 
Left_1 57.63 63.35 60.36 Left_1 82.90 79.22 81.02 
Left_2 29.27 20.69 24.24 Left_2 89.23 92.93 91.04 
Right_0 67.78 64.89 66.30 Right_0 82.63 84.41 83.51 
Right_1 54.84 30.91 39.54 Right_1 92.00 96.90 94.38 
Total 58.84 54.98 56.84 Total 88.96 90.42 89.68 
 
Table 8: The detailed performance for the single-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 65.39  68.92 67.11 Left_0 93.65  92.62 93.13 
Left_1 61.19  50.93 55.59 Left_1 79.64   85.60 82.51 
Left_2 28.57   20.69 24.00 Left_2 89.20   92.68 90.91 
Right_0 70.13   57.45 63.16 Right_0 80.30  87.63 83.81 
Right_1 33.33   40.00 36.36 Right_1 92.50   90.24 91.36 
Total 55.67   50.00 52.68 Total 87.85  90.08 88.95 
 
however, should be put on the detection of 
long-distance causes. In addition, we find that 
the detection of long-distance causes usually 
relies on two kinds of information for inference: 
rhetorical relation and deep semantic informa-
tion. 
6.2 Modeling Analysis 
To compare our multi-label model with single-
label models, we create a single-label system as 
follows. The single-label model is a binary 
classification for a pair comprising the emotion 
keyword and a candidate in its corresponding 
cause candidates. For each pair, all linguistic 
features are extracted only from the focus 
clause and its corresponding cause candidate. 
Note that we only use the features in the focus 
clause for ?left_0? and ?right_0?. The perform-
ances are shown in Table 8. 
Comparing Tables 7 and 8, all F-scores of 
the ?Y? tag increase and the performances of 
the ?N? tag remain almost the same for both the 
single-label model and our multi-label model. 
We also find that the multi-label model takes 
more advantage of local information, and im-
proves the performances, particularly for 
?left_1?.  
To take an in-depth analysis of the cause de-
tection capability of the multi-label model, an 
evaluation is designed that the label is treated 
as a tag from the multi-label classifier. Due to 
the tag sparseness problem (as in Table 2), only 
the ?left_2, left_1? tag is detected in the test 
data, and its performance is 21% precision, 
26% recall and 23% F-score. Furthermore, we 
notice that ~18% of the ?left_1? tags are de-
tected through this combination tag. This 
shows that some causes need to take into ac-
count the mutual information between clauses. 
Although the scores are low, it still shows that 
our multi-label model provides an effective 
way of detecting some of the multi-clauses 
causes. 
7 Conclusion 
We treat emotion cause detection as a multi-
label task, and develop two sets of linguistic 
features for emotion cause detection based on 
linguistic cues. The experiments on the small-
scale corpus show that both the multi-label 
model and the linguistic features are able to 
effectively detect emotion causes. The auto-
matic detection of emotion cause will in turn 
allow us to extract directly relevant information 
for public opinion mining and event prediction. 
It can also be used to improve emotion detec-
tion and classification. In the future, we will 
attempt to improve our system from two as-
pects. On the one hand, we will explore more 
powerful multi-label classification models for 
our system. On the other hand, we will investi-
gate more linguistic patterns or semantic in-
formation to further help emotion cause detec-
tion. 
186
References 
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. 
Affect Analysis of Web Forums and Blogs using 
Correlation Ensembles?. In IEEE Tran. Knowl-
edge and Data Engineering, vol. 20(9), pp. 1168-
1180. 
Bethard, S. and J. Martin. 2008. Learning Semantic 
Links from a Corpus of Parallel Temporal and 
Causal Relations. In Proceedings of ACL. 
Descartes, R. 1649. The Passions of the Soul. In J. 
Cottingham et al (Eds), The Philosophical Writ-
ings of Descartes. Vol. 1: 325-404. 
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for 
causality extraction using cue phrase and word 
pair probabilities. Information Processing and 
Management. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And 
Its Implications for Emotion Processing. In Pro-
ceedings of the 23rd Pacific Asia Conference on 
Language, Information and Computation. 
Girju, R. 2003. Automatic Detection of Causal Re-
lations for Question Answering. In the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics, Workshop on Multilingual 
Summarization and Question Answering - Ma-
chine Learning and Beyond, Sapporo, Japan. 
James, W. 1884. What is an Emotion? Mind, 
9(34):188?205. 
Lee, S. Y. M., Y. Chen and C.-R. Huang. 2010a. A 
Text-driven Rule-based System for Emotion 
Cause Detection. In Proceedings of NAACL-HLT 
2010 Workshop on Computational Approaches to 
Analysis and Generation of Emotion in Text. 
Lee, S. Y. M., Y. Chen, S. Li and C.-R. Huang. 
2010b. Emotion Cause Events: Corpus Construc-
tion and Analysis. In Proceedings of LREC 2010. 
Low, B. T., K. Chan , L. L. Choi , M. Y. Chin , S. L. 
Lay. 2001. Semantic Expectation-Based Causa-
tion Knowledge Extraction: A Study on Hong 
Kong Stock Movement Analysis, In Proceedings 
of the 5th Pacific-Asia Conference on Knowledge 
Discovery and Data Mining, p.114-123, April 
16-18.  
Marcu, D., and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of the AAAI Spring Symposium on Computational 
Approaches to Weblogs.  
Persing, I. and V. Ng. 2009. Semi-Supervised Cause 
Identification from Aviation Safety Reports. In 
Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-
SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. 
Emotion recognition Using Massive Examples 
Extracted from the Web. In Proceedings of COL-
ING. 
Wierzbicka, A. 1999. Emotions across Languages 
and Cultures: Diversity and Universals. Cam-
bridge: Cambridge University Press. 
 
 
 
187
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 635?643,
Beijing, August 2010
Sentiment Classification and Polarity Shifting 
 
Shoushan Li??  Sophia Yat Mei Lee?  Ying Chen?  Chu-Ren Huang?  Guodong Zhou?  
 
?Department of CBS 
The Hong Kong Polytechnic University 
{shoushan.li, sophiaym, 
chenying3176, churenhuang} 
@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and 
Technology 
      Soochow University
gdzhou@suda.edu.cn 
 
  
Abstract 
Polarity shifting marked by various 
linguistic structures has been a challenge 
to automatic sentiment classification. In 
this paper, we propose a machine learning 
approach to incorporate polarity shifting 
information into a document-level 
sentiment classification system. First, a 
feature selection method is adopted to 
automatically generate the training data 
for a binary classifier on polarity shifting 
detection of sentences. Then, by using the 
obtained binary classifier, each document 
in the original polarity classification 
training data is split into two partitions, 
polarity-shifted and polarity-unshifted, 
which are used to train two base 
classifiers respectively for further 
classifier combination. The experimental 
results across four different domains 
demonstrate the effectiveness of our 
approach. 
1 Introduction 
Sentiment classification is a special task of text 
classification whose objective is to classify a text 
according to the sentimental polarities of 
opinions it contains (Pang et al, 2002), e.g., 
favorable or unfavorable, positive or negative. 
This task has received considerable interests in 
the computational linguistic community due to its 
potential applications.  
In the literature, machine learning approaches 
have dominated the research in sentiment 
classification and achieved the state-of-the-art 
performance (e.g., Kennedy and Inkpen, 2006; 
Pang et al, 2002). In a typical machine learning 
approach, a document (text) is modeled as a 
bag-of-words, i.e. a set of content words without 
any word order or syntactic relation information. 
In other words, the underlying assumption is that 
the sentimental orientation of the whole text 
depends on the sum of the sentimental polarities 
of content words. Although this assumption is 
reasonable and has led to initial success, it is 
linguistically unsound since many function 
words and constructions can shift the 
sentimental polarities of a text. For example, in 
the sentence ?The chair is not comfortable?, the 
polarity of the word ?comfortable? is positive 
while the polarity of the whole sentence is 
reversed because of the negation word ?not?. 
Therefore, the overall sentiment of a document is 
not necessarily the sum of the content parts 
(Turney, 2002). This phenomenon is one main 
reason why machine learning approaches fail 
under some circumstances. 
As a typical case of polarity shifting, negation 
has been paid close attention and widely studied 
in the literature (Na et al, 2004; Wilson et al, 
2009; Kennedy and Inkpen, 2006). Generally, 
there are two steps to incorporate negation 
information into a system: negation detection 
and negation classification. For negation 
detection, some negation trigger words, such as 
?no?, ?not?, and ?never?, are usually applied to 
recognize negation phrases or sentences. As for 
negation classification, one way to import 
negation information is to directly reverse the 
polarity of the words which contain negation 
trigger words as far as term-counting approaches 
are considered (Kennedy and Inkpen, 2006). An 
alternative way is to add some negation features 
(e.g., negation bigrams or negation phrases) into 
635
machine learning approaches (Na et al, 2004). 
Such approaches have achieved certain success.  
There are, however, some shortcomings with 
current approaches in incorporating negation 
information. In terms of negation detection, 
firstly, the negation trigger word dictionary is 
either manually constructed or relies on existing 
resources. This leads to certain limitations 
concerning the quality and coverage of the 
dictionary. Secondly, it is difficult to adapt 
negation detection to other languages due to its 
language dependence nature of negation 
constructions and words. Thirdly, apart from 
negation, many other phenomena, e.g., contrast 
transition with trigger words like ?but?, 
?however?, and ?nevertheless?, can shift the 
sentimental polarity of a phrase or sentence. 
Therefore, considering negation alone is 
inadequate to deal with the polarity shifting 
problem, especially for document-level 
sentiment classification. 
In terms of negation classification, although it 
is easy for term-counting approaches to integrate 
negation information, they rarely outperform a 
machine learning baseline (Kennedy and Inkpen, 
2006). Even for machine learning approaches, 
although negation information is sometimes 
effective for local cases (e.g., not good), it fails 
on long-distance cases (e.g., I don?t think it is 
good). 
In this paper, we first propose a feature 
selection method to automatically generate a 
large scale polarity shifting training data for 
polarity shifting detection of sentences. Then, a 
classifier combination method is presented for 
incorporating polarity shifting information. 
Compared with previous ones, our approach 
highlights the following advantages?First of all, 
we apply a binary classifier to detect polarity 
shifting rather than merely relying on trigger 
words or phrases. This enables our approach to 
handle different kinds of polarity shifting 
phenomena. More importantly, a feature 
selection method is presented to automatically 
generate the labeled training data for polarity 
shifting detection of sentences. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
approach in details. Experimental results are 
presented and analyzed in Section 4. Finally, 
Section 5 draws the conclusion and outlines the 
future work. 
2 Related Work 
Generally, sentiment classification can be 
performed at four different levels: word level 
(Wiebe, 2000), phrase level (Wilson et al, 2009), 
sentence level (Kim and Hovy, 2004; Liu et al, 
2005), and document level (Turney, 2002; Pang 
et al, 2002; Pang and Lee, 2004; Riloff et al, 
2006). This paper focuses on document-level 
sentiment classification. 
In the literature, there are mainly two kinds of 
approaches on document-level sentiment 
classification: term-counting approaches 
(lexicon-based) and machine learning 
approaches (corpus-based). Term-counting 
approaches usually involve deriving a sentiment 
measure by calculating the total number of 
negative and positive terms (Turney, 2002; Kim 
and Hovy, 2004; Kennedy and Inkpen, 2006). 
Machine learning approaches recast the 
sentiment classification problem as a statistical 
classification task (Pang and Lee, 2004). 
Compared to term-counting approaches, 
machine learning approaches usually achieve 
much better performance (Pang et al, 2002; 
Kennedy and Inkpen, 2006), and have been 
adopted to more complicated scenarios, such as 
domain adaptation (Blitzer et al, 2007), 
multi-domain learning (Li and Zong, 2008) and 
semi-supervised learning (Wan, 2009; Dasgupta 
and Ng, 2009) for sentiment classification. 
Polarity shifting plays a crucial role in 
phrase-level, sentence-level, and document-level 
sentiment classification. However, most of 
previous studies merely focus on negation 
shifting (polarity shifting caused by the negation 
structure). As one pioneer research on sentiment 
classification, Pang et al (2002) propose a 
machine learning approach to tackle negation 
shifting by adding the tag ?not? to every word 
between a negation trigger word/phrase (e.g., not, 
isn't, didn't, etc.) and the first punctuation mark 
following the negation trigger word/phrase. To 
their disappointment, considering negation 
shifting has a negligible effect and even slightly 
harms the overall performance. Kennedy and 
Inkpen (2006) explore negation shifting by 
incorporating negation bigrams as additional 
features into machine learning approaches. The 
636
experimental results show that considering 
sentiment shifting greatly improves the 
performance of term-counting approaches but 
only slightly improves the performance of 
machine learning approaches. Other studies such 
as Na et al (2004), Ding et al (2008), and Wilson 
et al (2009) also explore negation shifting and 
achieve some improvements1. Nonetheless, as far 
as machine learning approaches are concerned, 
the improvement is rather insignificant (normally 
less than 1%). More recently, Ikeda et al (2008) 
first propose a machine learning approach to 
detect polarity shifting for sentence-level 
sentiment classification, based on a 
manually-constructed dictionary containing 
thousands of positive and negative sentimental 
words, and then adopt a term-counting approach 
to incorporate polarity shifting information. 
3 Sentiment Classification with Polarity 
Shifting Detection 
 
 
Figure 1: General framework of our approach 
 
The motivation of our approach is to improve the 
performance of sentiment classification by robust 
treatment of sentiment polarity shifting between 
sentences. With the help of a binary classifier, the 
sentences in a document are divided into two 
parts: sentences which contain polarity shifting 
structures and sentences without any polarity 
shifting structure. Figure 1 illustrates the general 
framework of our approach. Note that this 
framework is a general one, that is, different 
polarity shifting detection methods can be applied 
to differentiate polarity-shifted sentences from 
those polarity-unshifted sentences and different 
                                                      
1
 Note that Ding et al (2006) also consider but-clause, another 
important structure for sentiment shifting. Wilson et al (2009) use 
conjunctive and dependency relations among polarity words. 
polarity classification methods can be adopted to 
incorporate sentiment shifting information. For 
clarification, the training data used for polarity 
shifting detection and polarity classification are 
referred to as the polarity shifting training data 
and the polarity classification training data, 
respectively. 
3.1 Polarity Shifting Detection 
In this paper, polarity shifting means that the 
polarity of a sentence is different from the 
polarity expressed by the sum of the content 
words in the sentence. For example, in the 
sentence ?I am not disappointed?, the negation 
structure makes the polarity of the word 
'disappointed' different from that of the whole 
sentence (negative vs. positive). Apart from the 
negation structure, many other linguistic 
structures allow polarity shifting, such as 
contrast transition, modals, and 
pre-suppositional items (Polanyi and Zaenen, 
2006). We refer these structures as polarity 
shifting structures. 
One of the great challenges in building a 
polarity shifting detector lies on the lack of 
relevant training data since manually creating a 
large scale corpus of polarity shifting sentences 
is time-consuming and labor-intensive. Ikeda et 
al. (2008) propose an automatic way for 
collecting the polarity shifting training data 
based on a manually-constructed large-scale 
dictionary. Instead, we adopt a feature selection 
method to build a large scale training corpus of 
polarity shifting sentences, given only the 
already available document-level polarity 
classification training data. With the help of the 
feature selection method, the top-ranked word 
features with strong sentimental polarity 
orientation, e.g., ?great?, ?love?, ?worst? are first 
chosen as the polarity trigger words. Then, those 
sentences with the top-ranked polarity trigger 
words in both categories of positive and negative 
documents are selected. Finally, those candidate 
sentences taking opposite-polarity compared to 
the containing trigger word are deemed as 
polarity-shifted. 
The basic idea of automatically generating the 
polarity shifting training data is based on the 
assumption that the real polarity of a word or 
phrase is decided by the major polarity category 
where the word or phrase appears more often. As 
a result, the sentences in the 
Polarity Shifting 
Detector 
Documents 
 
Polarity-shifted 
Sentences 
Polarity-unshifted 
Sentences 
Polarity Classifier Positive/Negative 
637
frequently-occurring category would be seen as 
polarity-unshifted while the sentences in the 
infrequently-occurring category would be seen 
as polarity-shifted. 
In the literature, various feature selection 
methods, such as Mutual Information (MI), 
Information Gain (IG) and Bi-Normal Separation 
(BNS) (Yang and Pedersen, 1997; Forman 2003), 
have been employed to cope with the problem of 
the high-dimensional feature space which is 
normal in sentiment classification.  
In this paper, we employ the theoretical 
framework, proposed by Li et al (2009), 
including two basic measurements, i.e. frequency 
measurement and ratio measurement, where the 
first measures, the document frequency of a term 
in one category, and the second measures, the 
ratio between the document frequency in one 
category and other categories. In particular, a 
novel method called Weighed Frequency and 
Odds (WFO) is proposed to incorporate both 
basic measurements: 
1( | )( , ) ( | ) {max(0, log )}( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
where ( | )iP t c  denotes the probability that a 
document x contains the term t with the 
condition that x belongs to category ic ; 
( | )iP t c  denotes the probability that a document 
x contains the term t with the condition that x 
does not belong to category ic . The left part of 
the formula ( | )iP t c  implies the first basic 
measurement and the right part 
log( ( | ) / ( | ))i iP t c P t c  implies the second one. 
The parameter ?  0 1?? ?? ?is thus to tune the 
weight between the two basic measurements. 
Especially, when ?  equals 0, the WFO method 
fades to the MI method which fully prefers the 
second basic measurement. 
Figure 2 illustrates our algorithm for 
automatically generating the polarity shifting 
training data where 1c and 2c denote the two 
sentimental orientation categories, i.e. negative 
and positive. Step A segments a document into 
sentences with punctuations. Besides, two 
special words, ?but? and ?and?, are used to 
further segment some contrast transition 
structures and compound sentences. Step B 
employs the WFO method to rank all features 
including the words. Step D extracts those 
polarity-shifted and polarity-unshifted sentences 
containing top it ?  where maxN denotes the 
upper-limit number of sentences in each 
category of the polarity shifting training data and 
#(x) denotes the total number of the elements in 
x. Apart from that, the first word in the following 
sentence is also included to capture a common 
kind of long-distance polarity shifting structure: 
contrast transition. Thus, important trigger words 
like ?however? and ?but? may be considered. 
Finally, Step E guarantees the balance between 
the two categories of the polarity shifting 
training data. 
Given the polarity shifting training data, we 
apply SVM classification algorithm to train a 
polarity-shifting detector with word unigram 
features. 
Input: 
The polarity classification training data: the negative 
sentimental document set 
1c
D and the positive sentimental 
document set
 2c
D . 
Output: 
    The polarity shifting training data: the 
polarity-unshifted sentence set unshiftS  and the polarity- 
shifted sentence set
 
shiftS . 
Procedure: 
A. Segment documents 
1c
D  and  
2c
D  to single 
sentences  
1c
S  and  
2c
S . 
B. Apply feature selection on the polarity classification  
training data and get the ranked features, 
1( ,..., ,..., )top top i top Nt t t? ? ?  
C. shiftS  = {}, unshiftS  = {} 
D. For  top it ?  in  1( ,..., ,..., )top top i top Nt t t? ? ? : 
D1) if #( shiftS )> maxN : break 
D2) Collect all sentences  
1,top i c
S
?
 and  
2,top i c
S
?
 
which contain  top it ?  from  1cS  and  2cS  
respectively 
D3)  if #(
1,top i c
S
?
)>#(
2,top i c
S
?
): 
put  
2,top i c
S
?
 into  shiftS  
put  
1,top i c
S
?
 into  unshiftS  
else: 
put  
1,top i c
S
?
 into  shiftS  
put  
2,top i c
S
?
 into  unshiftS  
E. Randomly select 
maxN sentences from unshiftS as the 
output of 
unshiftS  
 
Figure 2: The algorithm for automatically 
generating the polarity shifting training data 
 
638
3.2 Polarity Classification with Classifier 
Combination  
After polarity shifting detection, each document 
in the polarity classification training data is 
divided into two parts, one containing 
polarity-shifted sentences and the other 
containing polarity-unshifted sentences, which 
are used to form the polarity-shifted training data 
and the polarity-unshifted training data. In this 
way, two different polarity classifiers, If  and 
2f , can be trained on the polarity-shifted 
training data and the polarity-unshifted training 
data respectively. Along with classifier 3f , 
trained on all original polarity classification 
training data, we now have three base classifiers 
in hand for possible classifier combination via a 
multiple classifier system. 
The key issue in constructing a multiple 
classifier system (MCS) is to find a suitable way 
to combine the outputs of the base classifiers. In 
MCS literature, various methods are available 
for combining the outputs, such as fixed rules 
including the voting rule, the product rule and 
the sum rule (Kittler et al, 1998) and trained 
rules including the weighted sum rule (Fumera 
and Roli, 2005) and the meta-learning 
approaches (Vilalta and Drissi, 2002). In this 
study, we employ the product rule, a popular 
fixed rule, and stacking (D?eroski and ?enko, 
2004), a well-known trained rule, to combine the 
outputs. 
Formally, each base classifier provides some 
kind of confidence measurements, e.g., posterior 
probabilities of the test sample belonging to each 
class. Formally, each base classifier 
 ( 1,2,3)lf l =  assigns a test sample (denoted as 
lx ) a posterior probability vector ( )lP x

:  
1 2( ) ( | ), ( | ))tl l lP x p c x p c x= (

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging 1c . 
The product rule combines the base classifiers 
by multiplying the posterior possibilities and 
using the multiplied possibility for decision, i.e. 
3
1
      arg max ( | )j i l
i l
assign y c when j p c x
=
? = ?  
Stacking belongs to well-known 
meta-learning (Vilalta and Drissi, 2002). The 
key idea behind meta-learning is to train a 
meta-classifier with input attributes that are the 
outputs of the base classifiers. Hence, 
meta-learning usually needs some development 
data for generating the meta-training data. Let 
'x  denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( ( | ' ), ( | ' ))l l l lP x p c x p c x=

 
A meta-classifier can be trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ( ' ), ( ' ), ( ' ))meta l l lx P x P x P x= = ==
  
 
Stacking is a specific meta-learning rule, in 
which a leave-one-out or a cross-validation 
procedure on the training data is applied to 
generate the meta-training data instead of using 
extra development data. In our experiments, we 
perform stacking with 10-fold cross-validation to 
generate the meta-training data. 
4 Experimentation 
4.1 Experimental Setting 
The experiments are carried out on product 
reviews from four domains: books, DVDs, 
electronics, and kitchen appliances (Blitzer et al, 
2007)2. Each domain contains 1000 positive and 
1000 negative reviews. 
For sentiment classification, all classifiers 
including the polarity shifting detector, three 
base classifiers and the meta-classifier in 
stacking are trained by SVM using the 
SVM-light tool 3  with Logistic Regression 
method for probability measuring (Platt, 1999). 
In all the experiments, each dataset is 
randomly and evenly split into two subsets: 50% 
documents as the training data and the remaining 
50% as the test data. The features include word 
unigrams and bigrams with Boolean weights. 
4.2 Experimental Results on Polarity 
Shifting Data 
To better understand the polarity shifting 
phenomena in document-level sentiment 
classification, we randomly investigate 200 
                                                      
2
 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 It is available at: http://svmlight.joachims.org/ 
639
polarity-shifted sentences, together with their 
contexts (i.e. the sentences before and after it), 
automatically generated by the WFO ( 0? = ) 
feature selection method. We find that nearly 
half of the automatically generated polarity- 
shifted sentences are actually polarity-unshifted 
sentences or difficult to decide. That is to say, 
the polarity shifting training data is noisy to 
some extent. One main reason is that some 
automatically selected trigger words do not 
really contain sentiment information, e.g., ?hear?, 
?information? etc. Another reason is that some 
reversed opinion is given in a review without 
any explicit polarity shifting structures.  
To gain more insights, we manually checked 
100 sentences which are explicitly 
polarity-shifted and can also be judged by 
human according to their contexts. Table 1 
presents some typical structures causing polarity 
shifting. It shows that the most common polarity 
shifting type is Explicit Negation (37%), usually 
expressed by trigger words such as ?not?, ?no?, or 
?without?, e.g., in the sentence ?I am not happy 
with this flashcard at all?. Another common type 
of polarity shifting is Contrast Transition (20%), 
expressed by trigger words such as ?however?, 
e.g., in the sentence ?It is large and stylish, 
however, I cannot recommend it because of the 
lid?. Other less common yet productive polarity 
shifting types include Exception and Until. 
Exception structure is usually expressed by the 
trigger phrase ?the only? to indicate the one and 
only advantage of the product, e.g., in the 
sentence ?The only thing that I like about it is 
that bamboo is a renewable resource?. Until 
structure is often expressed by the trigger word 
?until? to show the reversed polarity, e.g. in the 
sentence ?This unit was a great addition until the 
probe went bad after only a few months?. 
 
Polarity Shifting 
Structures 
Trigger 
Words/Phrases 
Distribution 
(%) 
Explicit Negation not, no, without 37 
Contrast Transition but, however, 
unfortunately 
20 
Implicit Negation avoid, hardly,  7 
False Impression look, seem 6 
Likelihood probably, perhaps 5 
Counter-factual should, would 5 
Exception the only 5 
Until until 3 
Table 1: Statistics on various polarity shifting 
structures 
4.3 Experimental Results on Polarity 
Classification 
For comparison, several classifiers with different 
classification methods are developed.  
1) Baseline classifier, which applies SVM with 
all unigrams and bigrams. Note that it also 
serves as a base classifier in the following 
combined classifiers. 
2) Base classifier 1, a base classifier for the 
classifier combination method. It works on the 
polarity-unshifted data.  
3) Base classifier 2, another base classifier for 
the classifier combination method. It works on 
the polarity-shifted data. 
4) Negation classifier, which applies SVM with 
all unigrams and bigrams plus negation bigrams. 
It is a natural extension of the baseline classifier 
with the consideration of negation bigrams. In 
this study, the negation bigrams are collected 
using some negation trigger words, such as ?not? 
and ?never?. If a negation trigger word is found 
in a sentence, each word in the sentence is 
attached with the word ?_not? to form a negation 
bigram. 
5) Product classifier, which combines the 
baseline classifier, the base classifier 1 and the 
base classifier 2 using the product rule. 
6) Stacking classifier, a combined classifier 
similar to the Product classifier. It uses the 
stacking classifier combination method instead 
of the product rule.  
Please note that we do not compare our approach 
with the one as proposed in Ikeda et al (2008) 
due to the absence of a manually-collected 
sentiment dictionary. Besides, it is well known 
that a combination strategy itself is capable of 
improving the classification performance. To 
justify whether the improvement is due to the 
combination strategy or our polarity shifting 
detection or both, we first randomly split the 
training data into two portions and train two base 
classifiers on each portion, then apply the 
stacking method to combine them along with the 
baseline classifier. The corresponding results are 
shown as ?Random+Stacking? in Table 2. Finally, 
in our experiments, t-test is performed to 
evaluate the significance of the performance 
improvement between two systems employing 
different methods (Yang and Liu, 1999). 
 
640
Domain Baseline Base  
Classifier 
1 
Base  
Classifier 
2 
Negation 
Classifier 
Random 
+ 
Stacking 
Shifting 
+ 
Product 
Shifting 
+ 
Stacking 
Book 0.755 0.756 0.670 0.759 0.764 0.772 0.785 
DVD 0.750 0.743 0.667 0.748 0.759 0.768 0.770 
Electronic 0.779 0.786 0.711 0.785 0.789 0.820 0.830 
Kitchen 0.818 0.814 0.683 0.826 0.835 0.840 0.849 
Table 2: Performance comparison of different classifiers with equally-splitting between training and test data 
 
Performance comparison of different 
classifiers 
Table 2 shows the accuracy results of different 
methods using 2000 polarity shifted sentences 
and 2000 polarity-unshifted sentences to train the 
polarity shifting detector (Nmax=2000). Compared 
to the baseline classifier, it shows that: 1) The 
base classifier 1, which only uses the 
polarity-unshifted sentences as the training data, 
achieves similar performance. 2)  The base 
classifier 2 achieves much lower performance 
due to much fewer sentences involved. 3) 
Including negation bigrams usually allows 
insignificant improvements (p-value>0.1), which 
is consistent with most of previous works (Pang 
et al, 2002; Kennedy and Inkpen, 2006). 4) Both 
the product and stacking classifiers with polarity 
shifting detection significantly improve the 
performance (p-value<0.05). Compared to the 
product rule, the stacking classifier is preferable, 
probably due to the performance unbalance 
among the individual classifiers, e.g., the 
performance of the base classifier 2 is much 
lower than the other two. Although stacking with 
two randomly generated base classifiers, i.e. 
?Random + Stacking?, also consistently 
outperforms the baseline classifier, the 
improvements are much lower than what has 
been achieved by our approach. This suggests 
that both the classifier combination strategy and 
polarity shifting detection contribute to the 
overall performance improvement. 
Effect of WFO feature selection method 
Figure 3 presents the accuracy curve of the 
stacking classifier when using different Lambda 
( ? ) values in the WFO feature selection method. 
It shows that those feature selection methods 
which prefer frequency information, e.g., MI and 
BNS, are better in automatically generating the 
polarity shifting training data. This is reasonable 
since high frequency terms, e.g., ?is?, ?it?, ?a?, 
etc., tend to obey our assumption that the real 
polarity of one top term should belong to the 
polarity category where the term appears 
frequently. 
Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Lambda=0 0.25 0.5 0.75 1
Ac
cu
ra
cy
Book DVD Electronic Kitchen
Figure 3: Performance of the stacking classifier using 
WFO with different Lambda ( ? ) values 
 Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
200 500 1000 1500 2000 3000 4000 6000 8000
Ac
cu
ra
cy
Book DVD Electronic Kitchen
 Figure 4: Performance of the stacking classifier over 
different sizes of the polarity shifting training data 
(with Nmax sentences in each category) 
Effect of a classifier over different sizes of the 
polarity shifting training data 
Another factor which might influence the 
overall performance is the size of the polarity 
shifting training data. Figure 4 presents the 
overall performance on different numbers of the 
polarity shifting sentences when using the 
stacking classifier. It shows that 1000 to 4000 
sentences are enough for the performance 
improvement. When the number is too large, the 
noisy training data may harm polarity shifting 
detection. When the number is too small, it is not 
enough for the automatically generated polarity 
shifting training data to capture various polarity 
shifting structures. 
641
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: Book
The traning data sizes
Ac
c
ur
ac
y
 
 
Baseline BaseClassifier 1 BaseClassifier 2 Stacking
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: DVD
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Electronic
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Kitchen
The traning data sizes
Ac
c
ur
ac
y
 
 
Figure 5: Performance of different classifiers over different sizes of the polarity classification training data 
 
Effect of different classifiers over different 
sizes of the polarity classification training data 
Figure 5 shows the classification results of 
different classifiers with varying sizes of the 
polarity classification training data. It shows that 
our approach is able to improve the overall 
performance robustly. We also notice the big 
difference between the performance of the 
baseline classifier and that of the base classifier 
1 when using 30% training data in Book domain 
and 90% training data in DVD domain. Detailed 
exploration of the polarity shifting sentences in 
the training data shows that this difference is 
mainly attributed to the poor performance of the 
polarity shifting detector. Even so, the stacking 
classifier guarantees no worse performance than 
the baseline classifier. 
5 Conclusion and Future Work 
In this paper, we propose a novel approach to 
incorporate polarity shifting information into 
document-level sentiment classification. In our 
approach, we first propose a 
machine-learning-based classifier to detect 
polarity shifting and then apply two classifier 
combination methods to perform polarity 
classification. Particularly, the polarity shifting 
training data is automatically generated through 
a feature selection method. As shown in our 
experimental results, our approach is able to 
consistently improve the overall performance 
across different domains and training data sizes, 
although the automatically generated polarity 
shifting training data is prone to noise. 
Furthermore, we conclude that those feature 
selection methods, which prefer frequency 
information, e.g., MI and BNS, are good choices 
for generating the polarity shifting training data. 
In our future work, we will explore better 
ways in generating less-noisy polarity shifting 
training data. In addition, since our approach is 
language-independent, it is readily applicable to 
sentiment classification tasks in other languages. 
For availability of the automatically generated 
polarity shifting training data, please contact the 
first author (for research purpose only). 
Acknowledgments 
This research work has been partially supported 
by Start-up Grant for Newly Appointed 
Professors, No. 1-BBZM in the Hong Kong 
Polytechnic University and two NSFC grants, 
No. 60873150 and No. 90920004. We also thank 
the three anonymous reviewers for their helpful 
comments. 
642
References 
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Ding X., B. Liu, and P. Yu. 2008. A Holistic 
Lexicon-based Approach to Opinion Mining. In 
Proceedings of the International Conference on 
Web Search and Web Data Mining, WSDM-08. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification. 
The Journal of Machine Learning Research, 3(1), 
pp.1289-1305. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Ikeda D., H. Takamura, L. Ratinov, and M. Okumura. 
2008. Learning to Shift the Polarity of Words for 
Sentiment Classification. In Proceedings of 
IJCNLP-08. 
Kennedy, A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Li S., R. Xia, C. Zong, and C. Huang. 2009. A 
Framework of Feature Selection Methods for Text 
Categorization. In Proceedings of 
ACL-IJCNLP-09. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08: HLT, 
short paper. 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
Na J., H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. 
Effectiveness of Simple Linguistic Processing in 
Automatic Sentiment Classification of Product 
Reviews. In Conference of the International 
Society for Knowledge Organization (ISKO-04). 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisons to Regularized 
Likelihood Methods. In: A. Smola, P. Bartlett, B. 
Schoelkopf and D. Schuurmans (Eds.): Advances 
in Large Margin Classiers. MIT Press, Cambridge, 
61?74. 
Polanyi L. and A. Zaenen. 2006. Contextual Valence 
Shifters. Computing attitude and affect in text: 
Theory and application. Springer Verlag. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In 
Proceedings of EMNLP-06. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial 
Intelligence Review, 18(2), pp. 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wiebe J. 2000. Learning Subjective Adjectives from 
Corpora. In Proceedings of AAAI-2000. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu, X. 1999. A Re-Examination of 
Text Categorization methods. In Proceedings of 
SIGIR-99. 
Yang Y. and J. Pedersen. 1997. A Comparative Study 
on Feature Selection in Text Categorization. In 
Proceedings of ICML-97. 
643
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 414?423,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Employing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification 
 
Shoushan Li??  Chu-Ren Huang?  Guodong Zhou?  Sophia Yat Mei Lee? 
 
?Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang, 
sophiaym}@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, China 
gdzhou@suda.edu.cn 
 
 
Abstract 
In this paper, we adopt two views, personal 
and impersonal views, and systematically 
employ them in both supervised and 
semi-supervised sentiment classification. Here, 
personal views consist of those sentences 
which directly express speaker?s feeling and 
preference towards a target object while 
impersonal views focus on statements towards 
a target object for evaluation. To obtain them, 
an unsupervised mining approach is proposed. 
On this basis, an ensemble method and a 
co-training algorithm are explored to employ 
the two views in supervised and 
semi-supervised sentiment classification 
respectively. Experimental results across eight 
domains demonstrate the effectiveness of our 
proposed approach. 
1 Introduction 
As a special task of text classification, sentiment 
classification aims to classify a text according to 
the expressed sentimental polarities of opinions 
such as ?thumb up? or ?thumb down? on the 
movies (Pang et al, 2002). This task has recently 
received considerable interests in the Natural 
Language Processing (NLP) community due to its 
wide applications. 
In general, the objective of sentiment 
classification can be represented as a kind of 
binary relation R, defined as an ordered triple (X, 
Y, G), where X is an object set including different 
kinds of people (e.g. writers, reviewers, or users), 
Y is another object set including the target 
objects (e.g. products, events, or even some 
people), and G is a subset of the Cartesian 
product X Y? . The concerned relation in 
sentiment classification is X ?s evaluation on Y, 
such as ?thumb up?, ?thumb down?, ?favorable?, 
and ?unfavorable?. Such relation is usually 
expressed in text by stating the information 
involving either a person (one element in X ) or a 
target object itself (one element in Y ). The first 
type of statement called personal view, e.g. ?I am 
so happy with this book?, contains X ?s 
?subjective? feeling and preference towards a 
target object, which directly expresses 
sentimental evaluation. This kind of information 
is normally domain-independent and serves as 
highly relevant clues to sentiment classification. 
The latter type of statement called impersonal 
view, e.g. ?it is too small?, contains Y ?s 
?objective? (i.e. or at least criteria-based) 
evaluation of the target object. This kind of 
information tends to contain much 
domain-specific classification knowledge. 
Although such information is sometimes not as 
explicit as personal views in classifying the 
sentiment of a text, speaker?s sentiment is 
usually implied by the evaluation result.  
It is well-known that sentiment classification 
is very domain-specific (Blitzer et al, 2007), so 
it is critical to eliminate its dependence on a 
large-scale labeled data for its wide applications. 
Since the unlabeled data is ample and easy to 
collect, a successful semi-supervised sentiment 
classification system would significantly 
minimize the involvement of labor and time. 
Therefore, given the two different views 
mentioned above, one promising application is to 
adopt them in co-training algorithms, which has 
been proven to be an effective semi-supervised 
learning strategy of incorporating unlabeled data 
to further improve the classification performance 
(Zhu, 2005). In addition, we would show that 
personal/impersonal views are linguistically 
marked and mining them in text can be easily 
performed without special annotation.  
414
In this paper, we systematically employ 
personal/impersonal views in supervised and 
semi-supervised sentiment classification. First, 
an unsupervised bootstrapping method is adopted 
to automatically separate one document into 
personal and impersonal views. Then, both views 
are employed in supervised sentiment 
classification via an ensemble of individual 
classifiers generated by each view. Finally, a 
co-training algorithm is proposed to incorporate 
unlabeled data for semi-supervised sentiment 
classification. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
unsupervised approach for mining personal and 
impersonal views. Section 4 and Section 5 
propose our supervised and semi-supervised 
methods on sentiment classification respectively. 
Experimental results are presented and analyzed 
in Section 6. Section 7 discusses on the 
differences between personal/impersonal and 
subjective/objective. Finally, Section 8 draws our 
conclusions and outlines the future work. 
2 Related Work 
Recently, a variety of studies have been reported 
on sentiment classification at different levels: 
word level (Esuli and Sebastiani, 2005), phrase 
level (Wilson et al, 2009), sentence level (Kim 
and Hovy, 2004; Liu et al, 2005), and document 
level (Turney, 2002; Pang et al, 2002). This 
paper focuses on the document-level sentiment 
classification. Generally, document-level 
sentiment classification methods can be 
categorized into three types: unsupervised, 
supervised, and semi-supervised. 
Unsupervised methods involve deriving a 
sentiment classifier without any labeled 
documents. Most of previous work use a set of 
labeled sentiment words called seed words to 
perform unsupervised classification. Turney 
(2002) determines the sentiment orientation of a 
document by calculating point-wise mutual 
information between the words in the document 
and the seed words of ?excellent? and ?poor?. 
Kennedy and Inkpen (2006) use a term-counting 
method with a set of seed words to determine the 
sentiment. Zagibalov and Carroll (2008) first 
propose a seed word selection approach and then 
apply the same term-counting method for Chinese 
sentiment classifications. These unsupervised 
approaches are believed to be 
domain-independent for sentiment classification. 
Supervised methods consider sentiment 
classification as a standard classification problem 
in which labeled data in a domain are used to 
train a domain-specific classifier. Pang et al 
(2002) are the first to apply supervised machine 
learning methods to sentiment classification. 
Subsequently, many other studies make efforts to 
improve the performance of machine 
learning-based classifiers by various means, such 
as using subjectivity summarization (Pang and 
Lee, 2004), seeking new superior textual features 
(Riloff et al, 2006), and employing document 
subcomponent information (McDonald et al, 
2007). As far as the challenge of 
domain-dependency is concerned, Blitzer et al 
(2007) present a domain adaptation approach for 
sentiment classification. 
Semi-supervised methods combine unlabeled 
data with labeled training data (often 
small-scaled) to improve the models. Compared 
to the supervised and unsupervised methods, 
semi-supervised methods for sentiment 
classification are relatively new and have much 
less related studies. Dasgupta and Ng (2009) 
integrate various methods in semi-supervised 
sentiment classification including spectral 
clustering, active learning, transductive learning, 
and ensemble learning. They achieve a very 
impressive improvement across five domains. 
Wan (2009) applies a co-training method to 
semi-supervised learning with labeled English 
corpus and unlabeled Chinese corpus for Chinese 
sentiment classification. 
3 Unsupervised Mining of Personal and 
Impersonal Views 
As mentioned in Section 1, the objective of 
sentiment classification is to classify a specific 
binary relation: X ?s evaluation on Y, where X is 
an object set including different kinds of persons 
and Y is another object set including the target 
objects to be evaluated. First of all, we focus on 
an analysis on sentences in product reviews 
regarding the two views: personal and 
impersonal views.  
The personal view consists of personal 
sentences (i.e. X ?s sentences) exemplified 
below: 
I. Personal preference: 
E1: I love this breadmaker! 
E2: I disliked it from the beginning. 
II. Personal emotion description: 
E3: Very disappointed! 
E4: I am happy with the product. 
III. Personal actions: 
415
E5: Do not waste your money. 
E6: I have recommended this machine to all my 
friends. 
The impersonal view consists of impersonal 
sentences (i.e.Y ?s sentences) exemplified below: 
I. Impersonal feature description: 
E7: They are too thin to start with. 
E8: This product is extremely quiet. 
II. Impersonal evaluation: 
E9: It's great. 
E10: The product is a waste of time and money. 
III. Impersonal actions: 
E11: This product not even worth a penny. 
E12: It broke down again and again. 
We find that the subject of a sentence presents 
important cues for personal/impersonal views, 
even though a formal and computable definition 
of this contrast cannot be found. Here, subject 
refers to one of the two main constituents in the 
traditional English grammar (the other 
constituent being the predicate) (Crystal, 2003)1. 
For example, the subjects in the above examples 
of E1, E7 and E11 are ?I?, ?they?, and ?this 
product? respectively. For automatic mining the 
two views, personal/impersonal sentences can be 
defined according to their subjects: 
Personal sentence: the sentence whose 
subject is (or represents) a person. 
Impersonal sentence: the sentence whose 
subject is not (does not represent) a person. 
In this study, we mainly focus on product 
review classification where the target object in 
the set Y  is not a person. The definitions need 
to be adjusted when the evaluation target itself is 
a person, e.g. the political sentiment 
classification by Durant and Smith (2007). 
Our unsupervised mining approach for mining 
personal and impersonal sentences consists of 
two main steps. First, we extract an initial set of 
personal and impersonal sentences with some 
heuristic rules: If the first word of one sentence 
is (or implies) a personal pronoun including ?I?, 
?we?, and ?do?, then the sentence is extracted as a 
personal sentence; If the first word of one 
sentence is an impersonal pronoun including 'it', 
'they', 'this', and 'these', then the sentence is 
extracted as an impersonal sentence. Second, we 
apply the classifier which is trained with the 
initial set of personal and impersonal sentences 
to classify the remaining sentences. This step 
aims to classify the sentences without pronouns 
                                                      
1
 The subject has the grammatical function in a sentence of 
relating its constituent (a noun phrase) by means of the verb to any 
other elements present in the sentence, i.e. objects, complements, 
and adverbials. 
(e.g. E3). Figure 1 shows the unsupervised 
mining algorithm. 
Input: 
The training data D
 
 
Output: 
    All personal and impersonal sentences, i.e. 
sentence sets personalS  and impersonalS . 
Procedure: 
(1). Segment all documents in D to sentences 
S using punctuations (such as periods and 
interrogation marks) 
(2). Apply the heuristic rules to classify the 
sentences S  with proper pronouns into, 1pS  
and  1iS  
(3). Train a binary classifier p if ?  with  1pS  and  
1iS  
(4). Use  p if ?  to classify the remaining sentences 
into  2pS  and  2iS  
(5). 1 2personal p pS S S= ? ,  1 2impersonal i iS S S= ?  
 
Figure 1: The algorithm for unsupervised mining 
personal and impersonal sentences from a training 
data 
4 Employing Personal/Impersonal 
Views in Supervised Sentiment 
Classification 
After unsupervised mining of personal and 
impersonal sentences, the training data is divided 
into two views: the personal view, which 
contains personal sentences, and the impersonal 
view, which contains impersonal sentences. 
Obviously, these two views can be used to train 
two different classifiers, 1f  and 2f , for 
sentiment classification respectively.  
Since our mining approach is unsupervised, 
there inevitably exist some noises. In addition, 
the sentences of different views may share the 
same information for sentiment classification. 
For example, consider the following two 
sentences: ?It is a waste of money.? and ?Do not 
waste your money.? Apparently, the first one 
belongs to the impersonal view while the second 
one belongs to personal view, according to our 
heuristic rules. However, these two sentences 
share the same word, ?waste?, which conveys 
strong negative sentiment information. This 
suggests that training a single-view classifier 3f  
with all sentences should help. Therefore, three 
base classifiers, 1f , 2f , and 3f , are eventually 
derived from the personal view, the impersonal 
416
view and the single view, respectively. Each base 
classifier provides not only the class label 
outputs but also some kinds of confidence 
measurements, e.g. posterior probabilities of the 
testing sample belonging to each class.  
Formally, each base classifier  ( 1,2,3)lf l =  
assigns a test sample (denoted as lx ) a posterior 
probability vector ( )lP x

:  
1 2( ) ( | ), ( | ) tl l lP x p c x p c x= < >

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging to 1c . 
In the ensemble learning literature, various 
methods have been presented for combining base 
classifiers. The combining methods are 
categorized into two groups (Duin, 2002): fixed 
rules such as voting rule, product rule, and sum 
rule (Kittler et al, 1998), and trained rules such 
as weighted sum rule (Fumera and Roli, 2005) 
and meta-learning approaches (Vilalta and Drissi, 
2002). In this study, we choose a fixed rule and a 
trained rule to combine the three base classifiers 
1f , 2f , and 3f .  
The chosen fixed rule is product rule which 
combine base classifiers by multiplying the 
posterior possibilities and using the multiplied 
possibility for decision, i.e. 
3
1
                 
  arg max ( | )
j
i l
i l
assign y c
where j p c x
=
?
= ?  
The chosen trained rule is stacking (Vilalta and 
Drissi, 2002; D?eroski and ?enko, 2004) where a 
meta-classifier is trained with the output of the 
base classifiers as the input. Formally, let 'x  
denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( | ' ), ( | ' )l l l lP x p c x p c x=< >

 
Then, a meta-classifier is trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ' ), ( ' ), ( ' )meta l l lx P x P x P x= = ==< >
  
 
In our experiments, we perform stacking with 
4-fold cross validation to generate meta-training 
data where each fold is used as the development 
data and the other three folds are used to train the 
base classifiers in the training phase. 
5 Employing Personal/Impersonal 
Views in Semi-Supervised Sentiment 
Classification 
Semi-supervised learning is a strategy which 
combines unlabeled data with labeled training 
data to improve the models. Given the two-view 
classifiers 1f  and 2f  along with the single-view 
classifier 3f , we perform a co-training algorithm 
for semi-supervised sentiment classification. The 
co-training algorithm is a specific 
semi-supervised learning approach which starts 
with a set of labeled data and increases the 
amount of labeled data using the unlabeled data 
by bootstrapping (Blum and Mitchell, 1998). 
Figure 2 shows the co-training algorithm in our 
semi-supervised sentiment classification. 
Input: 
The labeled data L
 
containing personal 
sentence set L personalS ?  and impersonal sentence set 
L impersonalS ?  
The unlabeled data U  containing personal 
sentence set
 
U personalS ?  and impersonal sentence set 
U impersonalS ?  
Output: 
    New labeled data L  
Procedure: 
Loop for N iterations untilU ?=  
(1). Learn the first classifier 1f  with L personalS ?  
(2). Use 1f  to label samples from U with 
U personalS ?  
(3). Choose 1n  positive and 1n negative most 
confidently predicted samples 1A  
(4). Learn the second classifier 2f  with L impersonalS ?  
(5). Use 2f to label samples from U with 
U impersonalS ?   
(6). Choose 2n  positive and 2n negative most 
confidently predicted samples 2A   
(7). Learn the third classifier 3f  with L  
(8). Use 3f  to label samples from U  
(9). Choose 3n  positive and 3n  negative most 
confidently predicted samples 3A  
(10). Add samples 1 2 3A A A? ?  with the 
corresponding labels into L  
(11). Update L personalS ?  and L impersonalS ?  
 
Figure 2: Our co-training algorithm for 
semi-supervised sentiment classification 
417
After obtaining the new labeled data, we can 
either adopt one classifier (i.e. 3f ) or a 
combined classifier (i.e. 1 2 3f f f+ + ) in further 
training and testing. In our experimentation, we 
explore both of them with the former referred to 
as co-training and single classifier and the latter 
referred to as co-training and combined 
classifier. 
6 Experimental Studies 
We have systematically explored our method on 
product reviews from eight domains: book, DVD, 
electronic appliances, kitchen appliances, health, 
network, pet and software. 
6.1 Experimental Setting 
The product reviews on the first four domains 
(book, DVD, electronic, and kitchen appliances) 
come from the multi-domain sentiment 
classification corpus, collected from 
http://www.amazon.com/ by Blitzer et al (2007)2. 
Besides, we also collect the product views from 
http://www.amazon.com/ on other four domains 
(health, network, pet and software)3. Each of the 
eight domains contains 1000 positive and 1000 
negative reviews. Figure 3 gives the distribution 
of personal and impersonal sentences in the 
training data (75% labeled data of all data). It 
shows that there are more impersonal sentences 
than personal ones in each domain, in particular 
in the DVD domain, where the number of 
impersonal sentences is at least twice as many as 
that of personal sentences. This unusual 
phenomenon is mainly attributed to the fact that 
many objective descriptions, e.g. the movie plot 
introductions, are expressed in the DVD domain 
which makes the extracted personal and 
impersonal sentences rather unbalanced. 
We apply both support vector machine (SVM) 
and Maximum Entropy (ME) algorithms with the 
help of the SVM-light4 and Mallet5 tools. All 
parameters are set to their default values. We 
find that ME performs slightly better than SVM 
on the average. Furthermore, ME offers posterior 
probability information which is required for 
                                                      
2
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 Note that the second version of multi-domain sentiment 
classification corpus does contain data from many other domains. 
However, we find that the reviews in the other domains contain 
many duplicated samples. Therefore, we re-collect the reviews from 
http://www.amazon.com/ and filter those duplicated ones. The new 
collection is here:  
http://llt.cbs.polyu.edu.hk/~lss/ACL2010_Data_SSLi.zip 
4
 http://svmlight.joachims.org/  
5
 http://mallet.cs.umass.edu/  
combination methods. Thus we apply the ME 
classification algorithm for further combination 
and co-training. In particular, we only employ 
Boolean features, representing the presence or 
absence of a word in a document. Finally, we 
perform t-test to evaluate the significance of the 
performance difference between two systems 
with different methods (Yang and Liu, 1999). 
Sentence Number in the Training Data
16134
8477 8337 8843
13097
29290
1485214414
12691 11941
1381814265 16441
1475315573
27714
0
10000
20000
30000
40000
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Nu
mb
er
Number of personal sentences
Number of impersonal sentences
Figure 3: Distribution of personal and impersonal 
sentences in the training data of each domain 
6.2 Experimental Results on Supervised 
Sentiment Classification 
4-fold cross validation is performed for 
supervised sentiment classification. For 
comparison, we generate two random views by 
randomly splitting the whole feature space into 
two parts. Each part is seen as a view and used to 
train a classifier. The combination (two random 
view classifiers along with the single-view 
classifier 3f ) results are shown in the last column 
of Table 1. The comparison between random two 
views and our proposed two views will clarify 
whether the performance gain comes truly from 
our proposed two-view mining, or simply from 
using the classifier combination strategy. 
Table 1 shows the performances of different 
classifiers, where the single-view classifier 3f  
which uses all sentences for training and testing, 
is considered as our baseline. Note that the 
baseline performances of the first four domains 
are worse than the ones reported in Blitzer et al 
(2007). But their experiment is performed with 
only one split on the data with 80% as the 
training data and 20% as the testing data, which 
means the size of their training data is larger than 
ours. Also, we find that our performances are 
similar to the ones (described as fully supervised 
results) reported in Dasgupta and Ng (2009) 
where the same data in the four domains are used 
and 10-fold cross validation is performed.  
418
Domain Personal 
View 
Classifier 
1f  
Impersonal 
View 
Classifier 
2f  
Single View 
Classifier 
(baseline) 
3f
 
Combination  
(Stacking) 
1 2 3f f f+ +  
Combination 
(Product rule) 
1 2 3f f f+ +  
Combination 
with two 
random views 
(Product rule) 
Book 0.7004 0.7474 0.7654 0.7919 0.7949 0.7546 
DVD 0.6931 0.7663 0.7884 0.8079 0.8165 0.8054 
Electronic 0.7414 0.7844 0.8074 0.8304 0.8364 0.8210 
Kitchen 0.7430 0.8030 0.8290 0.8555 0.8565 0.8152 
Health 0.7000 0.7370 0.7559 0.7780 0.7815 0.7548 
Network 0.7655 0.7710 0.8265 0.8360 0.8435 0.8312 
Pet 0.6940 0.7145 0.7390 0.7565 0.7665 0.7423 
Software 0.7035 0.7205 0.7470 0.7730 0.7715 0.7615 
AVERAGE 0.7176 0.7555 0.7823 0.8037 0.8084 0.7858 
 
Table 1: Performance of supervised sentiment classification 
 
From Table 1, we can see that impersonal view 
classifier 1f  consistently performs better than 
personal view classifier 2f . Similar to the 
sentence distributions, the difference in the 
classification performances between these two 
views in the DVD domain is the largest (0.6931 
vs. 0.7663). 
Both the combination methods (stacking and 
product rule) significantly outperform the 
baseline in each domain (p-value<0.01) with a 
decent average performance improvement of 
2.61%. Although the performance difference 
between the product rule and stacking is not 
significant, the product rule is obviously a better 
choice as it involves much easier implementation. 
Therefore, in the semi-supervised learning 
process, we only use the product rule to combine 
the individual classifiers. Finally, it shows that 
random generation of two views with the 
combination method of the product rule only 
slightly outperforms the baseline on the average 
(0.7858 vs. 0.7823) but performs much worse 
than our unsupervised mining of personal and 
impersonal views.  
6.3 Experimental Results on 
Semi-supervised Sentiment 
Classification 
We systematically evaluate and compare our 
two-view learning method with various 
semi-supervised ones as follows: 
Self-training, which uses the unlabeled data 
in a bootstrapping way like co-training yet limits 
the number of classifiers and the number of 
views to one. Only the baseline classifier 3f  is 
used to select most confident unlabeled samples 
in each iteration. 
Transductive SVM, which seeks the largest 
separation between labeled and unlabeled data 
through regularization (Joachims, 1999). We 
implement it with the help of the SVM-light tool. 
Co-training with random two-view 
generation (briefly called co-training with 
random views), where two views are generated 
by randomly splitting the whole feature space 
into two parts.  
In semi-supervised sentiment classification, 
the data are randomly partitioned into labeled 
training data, unlabeled data, and testing data 
with the proportion of 10%, 70% and 20% 
respectively. Figure 4 reports the classification 
accuracies in all iterations, where baseline 
indicates the supervised classifier 3f  trained on 
the 10% data; both co-training and single 
classifier and co-training and combined 
classifier refer to co-training using our proposed 
personal and impersonal views. But the former 
merely applies the baseline classifier 3f  trained 
the new labeled data to test on the testing data 
while the latter applies the combined classifier 
1 2 3f f f+ + . In each iteration, two top-confident 
samples in each category are chosen, i.e. 
1 2 3 2n n n= = = . For clarity, results of other 
methods (e.g. self-training, transductive SVM) 
are not shown in Figure 4 but will be reported in 
Figure 5 later.  
Figure 4 shows that co-training and 
combined classifier always outperforms 
co-training and single classifier. This again 
justifies the effectiveness of our two-view 
learning on supervised sentiment classification.
419
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
Domain: Book
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
0.7
Domain: DVD
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.7
0.72
0.74
0.76
0.78
0.8
Domain: Electronic
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
Domain: Kitchen
Iteration Number
Ac
cu
ra
cy
 
 
 
25 50 75 100 125
0.54
0.56
0.58
0.6
0.62
0.64
0.66
Domain: Health
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Domain: Network
Iteration Number
Ac
cu
ra
cy
 
 
Baseline
Co-traning and single classifier
Co-traning and combined classifier
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
Domain: Pet
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
Domain: Software
Iteration Number
Ac
cu
ra
cy
 
 
 
 
Figure 4: Classification performance vs. iteration numbers (using 10% labeled data as training data) 
 
One open question is whether the unlabeled 
data improve the performance. Let us set aside 
the influence of the combination strategy and 
focus on the effectiveness of semi-supervised 
learning by comparing the baseline and 
co-training and single classifier. Figure 4 
shows different results on different domains. 
Semi-supervised learning fails on the DVD 
domain while on the three domains of book, 
electronic, and software, semi-supervised 
learning benefits slightly (p-value>0.05). In 
contrast, semi-supervised learning benefits much 
on the other four domains (health, kitchen, 
network, and pet) from using unlabeled data and 
the performance improvements are statistically 
significant (p-value<0.01). Overall speaking, we 
think that the unlabeled data are very helpful as 
they lead to about 4% accuracy improvement on 
the average except for the DVD domain. Along 
with the supervised combination strategy, our 
approach can significantly improve the 
performance more than 7% on the average 
compared to the baseline. 
Figure 5 shows the classification results of 
different methods with different sizes of the 
labeled data: 5%, 10%, and 15% of all data, 
where the testing data are kept the same (20% of 
all data). Specifically, the results of other 
methods including self-training, transductive 
SVM, and random views are presented when 
10% labeled data are used in training. It shows 
that self-training performs much worse than our 
approach and fails to improve the performance of 
five of the eight domains. Transductive SVM 
performs even worse and can only improve the 
performance of the ?software? domain. Although 
co-training with random views outperforms the 
baseline on four of the eight domains, it performs 
worse than co-training and single classifier. 
This suggests that the impressive improvements 
are mainly due to our unsupervised two-view 
mining rather than the combination strategy.
420
Using 10% labeled data as training data
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Book DVD Electronic Kitchen Health Network Pet Software
Ac
cu
rac
y
Baseline Transductive SVM Self-training
Co-training with random views Co-training and single classifier Co-training and combined classifier
 
Using 5% labeled data as training data
0.69
0.747
0.584
0.525
0.67 0.6530.626
0.55
0.564
0.683
0.495
0.615
0.8675
0.7855
0.7
0.601
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
Using 15% labeled data as training data
0.763
0.6925
0.765
0.5925
0.679
0.564
0.677
0.7375
0.6625
0.735
0.655
0.615
0.8625
0.8325
0.782
0.716
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
 
Figure 5: Performance of semi-supervised sentiment classification when 5%, 10%, and 15% labeled data are used 
 
Figure 5 also shows that our approach is rather 
robust and achieves excellent performances in 
different training data sizes, although our 
approach fails on two domains, i.e. book and 
DVD, when only 5% of the labeled data are used. 
This failure may be due to that some of the 
samples in these two domains are too ambiguous 
and hard to classify. Manual checking shows that 
quite a lot of samples on these two domains are 
even too difficult for professionals to give a 
high-confident label. Another possible reason is 
that there exist too many objective descriptions 
in these two domains, thus introducing too much 
noisy information for semi-supervised learning. 
The effectiveness of different sizes of chosen 
samples in each iteration is also evaluated like 
1 2 3 6n n n= = = and 1 2 33, 6n n n= = = (This 
assignment is considered because the personal 
view classifier performs worse than the other two 
classifiers). Our experimental results are still 
unsuccessful in the DVD domain and do not 
show much difference on other domains. We also 
test the co-training approach without the 
single-view classifier 3f . Experimental results 
show that the inclusion of the single-view 
classifier 3f  slightly helps the co-training 
approach. The detailed discussion of the results 
is omitted due to space limit. 
6.4 Why our approach is effective? 
One main reason for the effectiveness of our 
approach on supervised learning is the way how 
personal and impersonal views are dealt with. As 
personal and impersonal views have different 
ways of expressing opinions, splitting them into 
two separations can filter some classification 
noises. For example, in the sentence of ?I have 
seen amazing dancing, and good dancing. This 
was TERRIBLE dancing!?. The first sentence is 
classified as a personal sentence and the second 
one is an impersonal sentence. Although the 
words ?amazing? and ?good? convey strong 
positive sentiment information, the whole text is 
negative. If we get the bag-of-words from the 
whole text, the classification result will be wrong. 
Rather, splitting the text into two parts based on 
different views allows correct classification as 
the personal view rarely contains impersonal 
words such as ?amazing? and ?good?. The 
classification result will thus be influenced by 
the impersonal view.  
In addition, a document may contain both 
personal and impersonal sentences, and each of 
them, to a certain extent, , provides classification 
evidence. In fact, we randomly select 50 
documents in the domain of kitchen appliances 
and find that 80% of the documents take both 
personal and impersonal sentences in which both 
of them express explicit opinions. That is to say, 
the two views provide different, complementary 
information for classification. This qualifies the 
success requirement of co-training algorithm to 
some extend. This might be the reason for the 
effectiveness of our approach on semi-supervised 
learning. 
421
7 Discussion on Personal/Impersonal vs. 
Subjective/Objective 
As mentioned in Section 1, personal view 
contains X ?s ?subjective? feeling, and 
impersonal view containsY ?s ?objective? (i.e. or 
at least criteria-based) evaluation of the target 
object. However, our technically-defined 
concepts of personal/impersonal are definitely 
different from subjective/objective: Personal 
view can certainly contain many objective 
expressions, e.g. ?I bought this electric kettle? and 
impersonal view can contain many subjective 
expressions, e.g. ?It is disappointing?.  
Our technically-defined personal/impersonal 
views are two different ways to describe 
opinions. Personal sentences are often used to 
express opinions in a direct way and their target 
object should be one of X. Impersonal ones are 
often used to express opinions in an indirect way 
and their target object should be one of Y. The 
ideal definition of personal (or impersonal) view 
given in Section 1 is believed to be a subset of 
our technical definition of personal (or 
impersonal) view. Thus impersonal view may 
contain both Y ?s objective evaluation (more 
likely to be domain independent) and subjective 
Y?s description. 
In addition, simply splitting text into 
subjective/objective views is not particularly 
helpful. Since a piece of objective text provides 
rather limited implicit classification information, 
the classification abilities of the two views are 
very unbalanced. This makes the co-training 
process unfeasible. Therefore, we believe that 
our technically-defined personal/impersonal 
views are more suitable for two-view learning 
compared to subjective/objective views. 
8 Conclusion and Future Work 
In this paper, we propose a robust and effective 
two-view model for sentiment classification 
based on personal/impersonal views. Here, the 
personal view consists of subjective sentences 
whose subject is a person, whereas the 
impersonal view consists of objective sentences 
whose subject is not a person. Such views are 
lexically cued and can be obtained without 
pre-labeled data and thus we explore an 
unsupervised learning approach to mine them.  
Combination methods and a co-training 
algorithm are proposed to deal with supervised 
and semi-supervised sentiment classification 
respectively. Evaluation on product reviews from 
eight domains shows that our approach 
significantly improves the performance across all 
eight domains on supervised sentiment 
classification and greatly outperforms the 
baseline with more than 7% accuracy 
improvement on the average across seven of 
eight domains (except the DVD domain) on 
semi-supervised sentiment classification. 
In the future work, we will integrate the 
subjectivity summarization strategy (Pang and 
Lee, 2004) to help discard noisy objective 
sentences. Moreover, we need to consider the 
cases when both X and Y appear in a sentence. 
For example, the sentence ?I think they're poor? 
should be an impersonal view but wrongly 
classified as a personal one according to our 
technical rules. We believe that these will help 
improve our approach and hopefully are 
applicable to the DVD domain. Another 
interesting and practical idea is to integrate 
active learning (Settles, 2009), another popular 
but principally different kind of semi-supervised 
learning approach, with our two-view learning 
approach to build high-performance systems 
with the least labeled data. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in 
the Hong Kong Polytechnic University and two 
NSFC grants, No. 60873150 and No. 90920004. 
We also thank the three anonymous reviewers 
for their invaluable comments. 
References  
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Blum A. and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In 
Proceedings of COLT-98. 
Crystal D. 2003. The Cambridge Encyclopedia of the 
English Language. Cambridge University Press. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Durant K. and M. Smith. 2007. Predicting the 
Political Sentiment of Web Log Posts using 
422
Supervised Machine Learning Techniques Coupled 
with Feature Selection. In Processing of Advances 
in Web Mining and Web Usage Analysis. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Esuli A. and F. Sebastiani. 2005. Determining the 
Semantic Orientation of Terms through Gloss 
Classification. In Proceedings of CIKM-05. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. 
ICML1999. 
Kennedy A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
McDonald R., K. Hannan, T. Neylon, M. Wells, and J. 
Reynar. 2007. Structured Models for 
Fine-to-coarse Sentiment Analysis. In Proceedings 
of ACL-07. 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06. 
Settles B. 2009. Active Learning Literature Survey. 
Technical Report 1648, Department of Computer 
Sciences, University of Wisconsin at Madison, 
Wisconsin. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu. 1999. A Re-Examination of Text 
Categorization methods. In Proceedings of 
SIGIR-99. 
Zagibalov T. and J. Carroll. 2008. Automatic Seed 
Word Selection for Unsupervised Sentiment 
Classification of Chinese Test. In Proceedings of 
COLING-08.  
Zhu X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report Computer Sciences 1530, 
University of Wisconsin ? Madison. 
 
423
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 45?53,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Text-driven Rule-based System for Emotion Cause Detection 
 
Sophia Yat Mei Lee? Ying Chen?* Chu-Ren Huang?? 
?Department of Chinese and Bilingual Studies  
The Hong Kong Polytechnic University 
*Department of Computer Engineering 
China Agriculture University 
?Institute of Linguistics  
Academia Sinica 
{sophiaym, chenying3176, churenhuang}@gmail.com 
 
 
 
  
Abstract 
Emotion cause detection is a new research area 
in emotion processing even though most theo-
ries of emotion treat recognition of a triggering 
cause event as an integral part of emotion. As a 
first step towards fully automatic inference of 
cause-emotion correlation, we propose a text-
driven, rule-based approach to emotion cause 
detection in this paper. First of all, a Chinese 
emotion cause annotated corpus is constructed 
based on our proposed annotation scheme. By 
analyzing the corpus data, we identify seven 
groups of linguistic cues and generalize two 
sets of linguistic rules for detection of emotion 
causes. With the linguistic rules, we then de-
velop a rule-based system for emotion cause 
detection. In addition, we propose an evaluation 
scheme with two phases for performance as-
sessment. Experiments show that our system 
achieves a promising performance for cause oc-
currence detection as well as cause event detec-
tion. The current study should lay the ground 
for future research on the inferences of implicit 
information and the discovery of new informa-
tion based on cause-event relation. 
1 Introduction 
Text-based emotion processing has attracted plenty 
of attention in NLP. Most research has focused on 
the emotion detection and classification by 
identifying the emotion types, for instances 
happiness and sadness, for a given sentence or 
document (Alm 2005, Mihalcea and Liu 2006, 
Tokuhisa et al 2008). However, on top of this 
surface level information, deeper level information 
regarding emotions, such as the experiencer, cause, 
and result of an emotion, needs to be extracted and 
analyzed for real world applications (Alm 2009). 
In this paper, we aim at mining one of the crucial 
deep level types of information, i.e. emotion cause, 
which provides useful information for applications 
ranging from economic forecasting, public opinion 
mining, to product design. Emotion cause detection 
is a new research area in emotion processing. In 
emotion processing, the cause event and emotion 
correlation is a fertile ground for extraction and 
entailment of new information. As a first step 
towards fully automatic inference of cause-
emotion correlation, we propose a text-driven, 
rule-based approach to emotion cause detection in 
this paper.  
 Most theories of emotion treat recognition of 
a triggering cause event as an integral part of 
emotional experience (Descartes 1649, James 1884, 
Plutchik 1962, Wierzbicka 1999). In this study, 
cause events refer to the explicitly expressed 
arguments or events that evoke the presence of the 
corresponding emotions. They are usually 
expressed by means of propositions, 
nominalizations, and nominals. For example, ?they 
like it? is the cause event of the emotion happiness 
in the sentence ?I was very happy that they like it?. 
Note that we only take into account emotions that 
are explicitly expressed, which are usually 
presented by emotion keywords, e.g. ?This 
surprises me?. Implicit emotions that require 
inference or connotation are not processed in this 
first study. In this study, we first build a Chinese 
emotion cause annotated corpus with five primary 
emotions, i.e. happiness, sadness, anger, fear, and 
surprise. We then examine various linguistic cues 
which help detect emotion cause events: the 
position of cause event and experiencer relative to 
the emotion keyword, causative verbs (e.g. rang4 
?to cause?), action verbs (e.g. xiang3dao4 ?to think 
about?), epistemic markers (e.g. kan4jian4 ?to 
see?), conjunctions (e.g. yin1wei4 ?because?), and 
prepositions (e.g. dui4yu2 ?for?). With the help of 
45
these cues, a list of linguistic rules is generalized. 
Based on the linguistic rules, we develop a rule-
based system for emotion cause detection. 
Experiments show that such a rule-based system 
performs promisingly well. We believe that the 
current study should lay the ground for future 
research on inferences and discovery of new 
information based on cause-event relation, such as 
detection of implicit emotion or cause, as well as 
prediction of public opinion based on cause events, 
etc.  
The paper is organized as follows. Section 2 
discusses the related work on various aspects of 
emotion analysis. Section 3 describes the construc-
tion of the emotion cause corpus. Section 4 
presents our rule-based system for emotion cause 
detection. Section 5 describes its evaluation and 
performance. Section 6 highlights our main contri-
butions. 
2 Previous Work 
We discuss previous studies on emotion analysis in 
this section, and underline fundamental yet unre-
solved issues. We survey the previous attempts on 
textual emotion processing and how the present 
study differs.  
2.1 Emotion Classes 
Various approaches to emotion classification were 
proposed in different fields, such as philosophy 
(Spinoza 1675, James 1884), biology (Darwin 
1859, linguistics (Wierzbicka 1999, K?vecses 
2000), neuropsychology (Plutchik 1962, Turner 
1996), and computer science (Ortony et al 1988, 
Picard 1995), as well as varying from language to 
language. Although there is lack of agreement 
among different theories on emotion classification, 
a small number of primary emotions are commonly 
assumed. Other emotions are secondary emotions 
which are the mixtures of the primary emotions.  
Researchers have attempted to propose the list 
of primary emotions, varying from two to ten basic 
emotions (Ekman 1984, Plutchik 1980, Turner 
2000). Fear and anger appear on every list, whe-
reas happiness and sadness appear on most of the 
lists. These four emotions, i.e. fear, anger, happi-
ness, and sadness, are the most common primary 
emotions. Other less common primary emotions 
are surprise, disgust, shame, distress, guilt, interest, 
pain, and acceptance.  
In this study, we adopt Turner?s emotion clas-
sification (2000), which identifies five primary 
emotions, namely happiness, sadness, fear, anger, 
and surprise. Turner?s list consists of primary emo-
tions agreed upon by most previous work. 
2.2 Emotion Processing in Text 
Textual emotion processing is still in its early stag-
es in NLP. Most of the previous works focus on 
emotion classification given a known emotion con-
text such as a sentence or a document using either 
rule-based (Masum et al 2007, Chaumartin 2007) 
or statistical approaches (Mihalcea and Liu 2005, 
Kozareva et al 2007). However, the performance 
is far from satisfactory. What is more, many basic 
issues remain unresolved, for instances, the rela-
tionships among emotions, emotion type selection, 
etc. Tokuhisa et al (2008) was the first to explore 
both the issues of emotion detection and classifica-
tion. It created a Japanese emotion-provoking 
event corpus for an emotion classification task us-
ing an unsupervised approach. However, only 
49.4% of cases were correctly labeled. Chen et al 
(2009) developed two cognitive-based Chinese 
emotion corpora using a semi-unsupervised ap-
proach, i.e. an emotion-sentence (sentences con-
taining emotions) corpus and a neutral-sentence 
(sentences containing no emotion) corpus. They 
showed that studies based on the emotion-sentence 
corpus (~70%) outperform previous corpora. 
Little research, if not none, has been done to 
examine the interactions between emotions and the 
corresponding cause events, which may make a 
great step towards an effective emotion classifica-
tion model. The lack of research on cause events 
restricted current emotion analysis to simple classi-
ficatory work without exploring the potentials of 
the rich applications of putting emotion ?in con-
text?. In fact, emotions can be invoked by percep-
tions of external events and in turn trigger 
reactions. The ability to detect implicit invoking 
causes as well as predict actual reactions will add 
rich dimensions to emotion analysis and lead to 
further research on event computing.  
3 Emotion Cause Corpus  
This section briefly describes how the emotion 
cause corpus is constructed. We first explain what 
46
an emotion cause is and discuss how emotion 
cause is linguistically expressed in Chinese. We 
then describe the corpus data and the annotation 
scheme. For more detailed discussion on the con-
struction of the emotion cause corpus, please refer 
to Lee et al (2010). 
3.1 Cause Events 
Following Talmy (2000), the cause of an emotion 
should be an event itself. In this work, it is called a 
cause event. By cause event, we do not necessarily 
mean the actual trigger of the emotion or what 
leads to the emotion. Rather, it refers to the imme-
diate cause of the emotion, which can be the actual 
trigger event or the perception of the trigger event. 
Adapting TimeML annotation scheme (Saur? et al 
2004), events refer to situations that happen or oc-
cur. In this study, cause events specifically refer to 
the explicitly expressed arguments or events that 
are highly linked with the presence of the corres-
ponding emotions. In Lee et al?s (2010) corpus, 
cause events are categorized into two types: verbal 
events and nominal events. Verbal events refer to 
events that involve verbs (i.e. propositions and 
nominalizations), whereas nominal events are 
simply nouns (i.e. nominals). Some examples of 
cause event types are given in bold face in (1)-(6). 
 
(1) Zhe4-DET tou2-CL niu2-cattle de-POSS zhu3ren2-owner, 
yan3kan4-see zi4ji3-oneself de-POSS niu2-cattle 
re3chu1-cause huo4-trouble lai2-come le-ASP, 
fei1chang2-very hai4pa4-frighten, jiu4-then ba3-PREP 
zhe4-DET tou2-CL niu2-cattle di1jia4-low price 
mai4chu1-sell.  
 ?The owner was frightened to see that his cattle 
caused troubles, so he sold it at a low price.? 
 
(2) Mei2-not  xiang3dao4-think  ta1-3.SG.F  shuo1-say  de-
POSS  dou1-all shi4-is  zhen1-true  hua4-word,  rang4-
lead  ta1-3.SG.M  zhen4jing1-shocked  bu4yi3-very. 
 ?He was shocked that what she said was the 
truth.? 
 
(3) Ta1-3.SG.M  dui4-for  zhe4-DET  ge4-CL  chong1man3-
full of  nong2hou4-dense  ai4yi4-love  de-DE xiang3fa3-
idea  gao1xing4-happy de-DE  shou3wu3zu2dao3-flourish. 
 ?He was very happy about this loving idea.? 
 
(4) Zhe4-DET ci4-CL yan3chu1-performance de-POSS 
jing1zhi4-exquisite dao4shi4-is ling4-cause wo3-1.SG 
shi2fen1-very jing1ya4-surprise.  
 ?I was very surprised by this exquisite perfor-
mance.?   
 
(5) Ni2ao4-Leo de-POSS hua4-word hen3-very ling4-make 
kai3luo4lin2-Caroline shang1xin1-sad. 
 ?Caroline was very saddened by Leo?s words.? 
 
(6) Dui4yu2-for wei4lai2-future, lao3shi2shuo1-frankly wo3-
1.SG hen3-very hai4pa4-scared.  
 ?Frankly, I am very scared about the future.? 
 
The causes in (1) and (2) are propositional causes, 
which indicate the actual events involved in caus-
ing the emotions. The ones in (3) and (4) are no-
minalized causes, whereas (5) and (6) involve 
nominal causes  
3.2 Corpus Data and Annotation Scheme 
Based on the list of 91 Chinese primary emotion 
keywords identified in Chen et al (2009), we ex-
tract 6,058 instances of sentences by keyword 
matching from the Sinica Corpus 1 , which is a 
tagged balanced corpus of Mandarin Chinese con-
taining a total of ten million words. Each instance 
contains the focus sentence with the emotion key-
word ?<FocusSentence>? plus the sentence before 
?<PrefixSentence>? and after ?<SuffixSentence>? 
it. The extracted instances include all primary emo-
tion keywords occurring in the Sinica Corpus ex-
cept for the emotion class happiness, as the 
keywords of happiness exceptionally outnumber 
other emotion classes. In order to balance the 
number of each emotion class, we set the upper 
limit at about 1,600 instances for each primary 
emotion.  
Note that the presence of emotion keywords 
does not necessarily convey emotional information 
due to different possible reasons such as negative 
polarity and sense ambiguity. Hence, by manual 
inspection, we remove instances that 1) are non-
emotional; 2) contain highly ambiguous emotion 
keywords, such as ru2yi4 ?wish-fulfilled?, hai4xiu1 
?to be shy?, wei2nan2 ?to feel awkward?, from the 
corpus. After the removal, the remaining instances 
in the emotion cause corpus is 5,629. Among the 
remaining instances, we also remove the emotion 
keywords in which the instances do not express 
that particular emotion and yet are emotional. The 
total emotion keywords in the corpus is 5,958. 
For each emotional instance, two annotators 
manually annotate cause events of each keyword. 
Since more than one emotion can be present in an 
                                                           
1
 http://dbo.sinica.edu.tw/SinicaCorpus/ 
47
instance, the emotion keywords are tagged as 
<emotionword id=0>, <emotionword id=1>, and 
so on.  
 
573 Y 0/shang1 xin1/Sadness  
<PrefixSentence> ma1ma ye3 wen4 le ling2 ju1, dan4 shi4 
mei2 you3 ren4 jian4 dao4 xiao3 bai2. </PrefixSentence> 
<FocusSentence>wei4 le [*01n] zhe4 jian4 shi4 [*02n] , wo3 
ceng2 <emotionword id=0>shang1 xin1</emotionword> le 
hou2 jiu3,dan4 ye3 wu2 ji3 yu4 shi4. </FocusSentence> <Suf-
fixSentence>mei3 dang1 zai4 kan4 dao4 bai2 se4 de qi4 gou3, 
bu4 jin4 hui4 xiang3 qi3 xiao3 bai2 </SuffixSentence> 
 
573 Y 0/to be sad/Sadness  
<PrefixSentence> Mom also asked the neighbors, but no one 
ever saw Little White. </PrefixSentence> <FocusSentence> 
Because of [*01n] this [*02n] , I have been feeling very <emo-
tionword id=0> sad </emotionword> for a long time, but this 
did not help.  </FocusSentence> <SuffixSentence> Whenever 
[I] see a white stray dog, [I] cannot help thinking of Little 
White. </SuffixSentence> 
Figure 1: An Example of Cause Event Annotation 
 
Figure 1 shows an example of annotated emotional 
sentences in corpus, presented as pinyin with tones, 
followed by an English translation. For an emotion 
keyword tagged as <id=0>, [*01n] marks the be-
ginning of its cause event while [*02n] marks the 
end. The ?0? shows which index of emotion key-
word it refers to, ?1? marks the beginning of the 
cause event, ?2? marks the end, and ?n? indicates 
that the cause is a nominal event. For an emotion 
keyword tagged as <id=1>, [*11e] marks the be-
ginning of the cause event while [*12e] marks the 
end, in which ?e? refers to a verbal event, i.e. ei-
ther a proposition or a nominalization. An emotion 
keyword can sometimes be associated with more 
than one cause, in which case both causes are 
marked. The emotional sentences containing no 
explicitly expressed cause event remain as they are. 
The actual number of extracted instances of 
each emotion class to be analyzed, the positive 
emotional instances, and the instances with cause 
events are presented in Table 1.  
 
Table 1: Summary of Corpus Data 
Emotions No. of Instances Extracted Emotional With Causes 
Happiness 1,644 1,327 1,132 (85%) 
Sadness 901 616 468 (76%) 
Fear 897 689 567 (82%) 
Anger 1,175 847 629 (74%) 
Surprise 1,341 781 664 (85%) 
Total 5,958 4,260 (72%) 3,460 (81%) 
We can see that 72% of the extracted instances ex-
press emotions, and 81% of the emotional in-
stances have a cause. The corpus contains 
happiness (1,327) instances the most and sadness 
(616) the least. For each emotion type, about 81% 
of the emotional sentences, on average, are consi-
dered as containing a cause event, with surprise 
the highest (85%) and anger the lowest (73%). 
This indicates that an emotion mostly occurs with 
the cause event explicitly expressed in the text, 
which confirms the prominent role of cause events 
in expressing an emotion. 
4 A Rule-based System for Cause Detec-
tion  
4.1 Linguistic Analysis of Emotion Causes 
By analyzing the corpus data, we examine the 
correlations between emotions and cause events in 
terms of various linguistic cues: the position of 
cause event, verbs, epistemic markers, 
conjunctions, and prepositions. We hypothesize 
that these cues will facilitate the detection of 
emotion cause events.  
 First, we calculate the distribution of cause 
event types of each emotion as well as the position 
of cause events relative to emotion keywords and 
experiencers. The total number of emotional 
instances regarding each emotion is given in Table 
2.  
 
Table 2: Cause Event Position of Each Emotion 
Emotions Cause Type (%) Cause Position (%) 
Event Nominal Left Right 
Happiness 76 6 74 29 
Sadness 67 8 80 20 
Fear 68 13 52 48 
Anger 55 18 71 26 
Surprise 73 12 59 41 
 
Table 2 suggests that emotion cause events tend to 
be expressed by verbal events than nominal events 
and that cause events tend to occur at the position 
to the left of the emotion keyword, with fear (52%) 
being no preference. This may be attributed to the 
fact that fear can be triggered by either factive or 
potential causes, which is rare for other primary 
emotions. For fear, factive causes tend to take the 
left position whereas potential causes tend to take 
the right position. 
48
 Second, we identify seven groups of 
linguistic cues that are highly collocated with 
cause events (Lee et al 2010), as shown in Table 3.  
 
Table 3: Seven Groups of Linguistic Cues 
Group Cue Words 
I ?to cause?: rang4, ling4, shi3  
II ?to think about?: e.g. xiang3 dao4, xiang3 qi3, yi1 
xiang3  
?to talk about?: e.g. shuo1dao4, jiang3dao4, tan2dao4  
III ?to say?: e.g. shuo1, dao4 
IV ?to see?: e.g. kan4dao4, kan4jian4, jian4dao4  
?to hear?: e.g. ting1dao4, ting1 shuo1 
?to know?: e.g. zhi1dao4, de2zhi1, fa1xian4 
?to exist?: you3 
V ?for? as in ?I will do this for you?: wei4, wei4le 
?for? as in ?He is too old for the job?: dui4, dui4yu2 
VI ?because?: yin1, yin1wei4, you2yu2 
VII ?is?: deshi4 
?at?: yu2 
?can?: neng2 
 
Group I includes three common causative verbs, 
and Group II a list of verbs of thinking and talking. 
Group III is a list of say verbs. Group IV includes 
four types of epistemic markers which are usually 
verbs marking the cognitive awareness of emotion 
in the complement position (Lee and Huang 2009). 
The epistemic markers include verbs of seeing, 
hearing, knowing, and existing. Group V covers 
some prepositions which all roughly mean ?for?. 
Group VI contains the conjunctions that explicitly 
mark the emotion cause. Group VII includes other 
linguistic cues that do not fall into any of the six 
groups. Each group of linguistic cues serves as an 
indicator marking the cause events in different 
structures of emotional constructions, in which 
Group I specifically marks the end of the cause 
events while the other six groups marks the 
beginning of the cause events. 
4.2 Linguistic Rules for Cause Detection 
We examine 100 emotional sentences of each emo-
tion keyword randomly extracted from the devel-
opment data, and generalize some rules for 
identifying the cause of the corresponding emotion 
verb (Lee 2010). The cause is considered as a 
proposition. It is generally assumed that a proposi-
tion has a verb which optionally takes a noun oc-
curring before it as the subject and a noun after it 
as the object. However, a cause can also be ex-
pressed as a nominal. In other words, both the pre-
dicate and the two arguments are optional provided 
that at least one of them is present.  
We also manually identify the position of the 
experiencer as well as the linguistic cues discussed 
in Section 4.1. All these components may occur in 
the clause containing the emotion verb (focus 
clause), the clause before the focus clause, or the 
clause after the focus clause. The abbreviations 
used in the rules are given as follows:  
 
C = Cause event 
E = Experiencer 
K = Keyword/emotion verb 
B = Clause before the focus clause 
F = Focus clause/the clause containing the emotion verb 
A = Clause after the focus clause 
 
For illustration, an example of the rule description 
is given in Rule 1. 
 
Rule 1: 
i) C(B/F) + I(F) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) before I in F/B  
 
Rule 1 indicates that the experiencer (E) appears to 
be the nearest Na (common noun)/ Nb (proper 
noun)/ Nc (place noun)/ Nh (pronoun) after Group 
I cue words in the focus clause (F), while, at the 
same time, it comes before the keyword (K). Be-
sides, the cause (C) comes before Group I cue 
words. We simplify the proposition as a structure 
of (N)+(V)+(N), which is very likely to contain the 
cause event. Theoretically, in identifying C, we 
should first look for the nearest verb occurring be-
fore Group I cue words in the focus sentence (F) or 
the clause before the focus clause (B), and consider 
this verb as an anchor. From this verb, we search to 
the left for the nearest noun, and consider it as the 
subject; we then search to the right for the nearest 
noun until the presence of the cue words, and con-
sider it as the object. The detected subject, verb, 
and object form the cause event. In most cases, the 
experiencer is covertly expressed. It is, however, 
difficult to detect such causes in practice as causes 
may contain no verbs, and the two arguments are 
optional. Therefore, we take the clause instead of 
the structure of (N)+(V)+(N) as the actual cause. 
Examples are given in (7) and (8). For both sen-
tences, the clause that comes before the cue word 
is taken as the cause event of the emotion in ques-
tion. 
 
49
(7) [C yi1 la1 ke4 xi4 jun1 wu3 qi4 de bao4 guang1], [I 
shi3] [E lian2 he2 guo2 da4 wei2][K zhen4 jing1] . 
?[C The revealing of Iraq?s secret bacteriological 
weapons] [K shocked] [E the United Nations].? 
 
(8) [C heng2 shan1 jin1 tian1 ti2 chu1 ci2 cheng2], [I 
ling4] [E da4 ban3] zhi4 wei2 [K fen4 nu4] ? 
?[C Yokoyama submitted his resignation today], [K 
angered] [E the people of Osaka].? 
 
Table 4 summarizes the generalized rules for de-
tecting the cause events of the five primary emo-
tions in Chinese. We identify two sets of rules: 1) 
the specific rules that apply to all emotional in-
stances (i.e. rules 1-13); 2) the general rules that 
apply to the emotional instances in which causes 
are not found after applying the specific set of 
rules (i.e. rules 14 and 15).  
 
Table 4: Linguistic Rules for Cause Detection 
No. Rules 
1 i) C(B/F) + I(F) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) before I in F/B 
2 i) E(B/F) + II/IV/V/VI(B/F) + C(B/F) + K(F) 
ii) E=the nearest Na/Nb/Nc/Nh before II/IV/V/VI in B/F 
iii) C = the nearest (N)+(V)+(N) before K in F 
3 i) II/IV/V/VI (B) + C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after II/IV/V/VI in B 
4 i) E(B/F) + K(F) + IV/VII(F) + C(F/A) 
ii) E = a: the nearest Na/Nb/Nc/Nh before K in F; b: the 
first Na/Nb/Nc/Nh in B 
iii) C = the nearest (N)+(V)+(N) after IV/VII in F/A 
5 i) E(F)+K(F)+VI(A)+C(A) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after VI in A 
6 i) I(F) + E(F) + K(F) + C(F/A) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) after K in F or A 
7 i) E(B/F) + yue4 C yue4 K ?the more C the more K? (F)  
ii) E = the nearest Na/Nb/Nc/Nh before the first yue4 in 
B/F 
iii) C = the V in between the two yue4?s in F 
8 i) E(F) + K(F) + C(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after K in F 
9 i) E(F) + IV(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before IV in F 
iii) C = IV+(an aspectual marker) in F 
10 i) K(F) + E(F) + de ?possession?(F) + C(F) 
ii) E = the nearest Na/Nb/Nc/Nh after K in F 
iii) C = the nearest (N)+V+(N)+?+N after de in F 
11 i) C(F) + K(F) + E(F) 
ii) E = the nearest Na/Nb/Nc/Nh after K in F 
iii) C = the nearest (N)+(V)+(N) before K in F 
12 i) E(B) + K(B) + III (B) + C(F)  
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after III in F 
13 i) III(B) + C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after III in B 
14 i) C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) before K in B  
15 i) E(B) +C(B) + K(F)  
ii) E = the first Na/Nb/Nc/Nh in B 
iii) C = the nearest (N)+(V)+(N) before K in B 
 
 
Constraints are set to each rule to filter out incor-
rect causes. For instances, in Rule 1, the emotion 
keyword cannot be followed by the words de ?pos-
session?/ deshi4 ?is that?/ shi4 ?is? since it is very 
likely to have the cause event occurring after such 
words; in Rule 2, the cue word in III yuo3 ?to ex-
ist? is excluded as it causes noises; whereas for 
Rule 4, it only applies to instances containing 
keywords of happiness, fear, and surprise. 
5 Experiment  
5.1 Evaluation Metrics 
An evaluation scheme is designed to assess the 
ability to extract the cause of an emotion in context. 
We specifically look into two phases of the per-
formance of such a cause recognition system. 
Phase 1 assesses the detection of an emotion co-
occurrence with a cause; Phrase 2 evaluates the 
recognition of the cause texts for an emotion. 
 
Overall Evaluation:  
The definitions of related metrics are presented in 
Figure 2. For each emotion in a sentence, if neither 
the gold-standard file nor the system file has a 
cause, both precision and recall score 1; otherwise, 
precision and recall are calculated by the scoring 
method ScoreForTwoListOfCauses. As an emotion 
may have more than one cause, ScoreForTwoLis-
tOfCauses calculates the overlap scores between 
two lists of cause texts. Since emotion cause rec-
ognition is rather complicated, two relaxed string 
match scoring methods are selected to compare 
two cause texts, ScoreForTwoStrings: Relaxed 
Match 1 uses the minimal overlap between the 
gold-standard cause and the system cause. The sys-
tem cause is considered as correct provided that 
there is at least one overlapping Chinese character; 
Relaxed Match 2 is more rigid which takes into 
account the overlap text length during scoring. 
 
50
Phase 1: The Detection of Cause Occurrence 
The detection of cause occurrence is considered a 
preliminary task for emotion cause recognition and 
is compounded by the fact that neutral sentences 
are difficult to detect, as observed in Tokuhisa et al 
(2008). For Phase 1, each emotion keyword in a 
sentence has a binary tag: Y (i.e. with a cause) or 
N (without a cause). Similar to other NLP tasks, 
we adopt the common evaluation metrics, i.e. accu-
racy, precision, recall, and F score. 
 
Phase 2: The Detection of Causes 
The evaluation in Phase 2 is limited to the emotion 
keywords with a cause either in the gold-standard 
file or in the system file. The performance is calcu-
lated as in Overall Evaluation scheme. 
 
 
 
5.2 Results and Discussion 
We use 80% sentences as the development data, 
and 20% as the test data. The baseline is designed 
as follows: find a verb to the left of the keyword in 
question, and consider the clause containing the 
verb as a cause.  
Table 5 shows the performances of the overall 
evaluation. We find that the overall performances 
of our system have significantly improved using 
Relaxed Match 1 and Relaxed Match 2 by 19% 
and 19% respectively. Although the overall per-
formance of our system (47.95% F-score for Re-
laxed Match 1 and 41.67% for Relaxed Match 2) is 
not yet very high, it marks a good start for emotion 
 
Overall evaluation formula: 
 Precision (GF, SF) =  
ScoreForTwoListOfCauses ( , ) 
1 
j j
j
j
i i
i i
S GF em S
S SF em S
SCList GCList
?
?
?
?
? ?
? ?
 
 Recall (GF, SF) =  
ScoreForTwoListOfCauses ( , ) 
1 
j j
j
j
i i
i i
S GF em S
S GF em S
SCList GCList
?
?
?
?
? ?
? ?  
Where GF and SF are the gold-standard cause file and system cause file respectively, and both files include 
the same sentences. Si is a sentence, and emj is an emotion keyword in Si. GCListj and SCListj are the lists 
of the gold-standard causes and system causes respectively for the emotion keyword emj.  
 
ScoreForTwoListOfCauses (GCList, SCList):  
 If there is no cause in either GCList or SCList: Precision = 1; Recall = 1 
     Else: 
        Precision =  
( , )
| |
i j
GCi GCListSCj SCList
Max ScoreTwoStrings GC SC
SCList
?
?
?
 
Recall     =  
( , )
| |
i j
SCj SCListGCi GCList
Max ScoreTwoStrings GC SC
GCList
?
?
?
 
 
ScoreForTwoStrings(GC, SC): GC is a gold-standard cause text, and SC is a system cause text. 
Relaxed Match 1:  If overlap existing, both precision and recall are 1; Else, both are 0. 
Relaxed Match 2:    Precision (GC, SC) = ( )
( )
Len overlapText
Len SC
  
Recall (GC, SC)   = ( )
( )
Len overlapText
Len GC
  
Figure 2: The Definitions of Metrics for Cause Detection 
 
51
 Relaxed Match 1 Relaxed Match 2 
 Precision Recall F-score Precision Recall F-score 
Baseline 25.94 31.99  28.65 17.77 29.62  22.21 
Our System 45.06  51.24 47.95 39.89 43.63 41.67 
Table 5: The Overall Performances 
 
 Baseline Rule-based System 
Emotions Precision Recall F-score Precision Recall F-score 
With causes 99.42 79.74 88.50 96.871 80.851 88.139 
Without causes 4.39 66.67 8.23 13.158 52.632 21.053 
Table 7: The Detailed Performances in Phase 1 
 
 Relaxed Match 1 Relaxed Match 2 
 Precision Recall F-score Precision Recall F-score 
Baseline 25.37 39.28 30.83 17.09 36.29  23.24 
Our System 44.64 61.30  51.66 39.18 51.68 44.57 
Table 8: The Detailed Performances in Phase 2 
 
 Baseline Rule-based System 
Accuracy 79.56 79.38 
Table 6: The Overall Accuracy in Phase 1 
 
cause detection and extraction. 
Table 6 and 7 show the performances of the 
baseline and our rule-based system in Phase 1. Ta-
ble 6 shows the overall accuracy, and Table 7 
shows the detailed performances. In Table 6, we 
find that our system and the baseline have similar 
high accuracy scores. Yet Table 7 shows that both 
systems achieve a high performance for emotions 
with a cause, but much worse for emotions without 
a cause. It is important to note that even though the 
naive baseline system has comparably high per-
formance with our rule-based system in judging 
whether there is a cause in context, this result is 
biased by two facts. First, as the corpus contains 
more than 80% of sentences with emotion, a sys-
tem which is biased toward detecting a cause, such 
as the baseline system, naturally performs well. In 
addition, once the actual cause is examined, we can 
see that the baseline actually detects a lot of false 
positives in the sense that the cause it identifies is 
only correct in 4.39%. Our rule-based system 
shows great promise in being able to deal with 
neutral sentences effectively and being able to 
detect the correct cause at least three times more 
often than the baseline.  
Table 8 shows the performances in Phase 2. 
Comparing to the baseline, we find that our rules 
improve the performance of cause recognition us-
ing Relaxed Match 1 and 2 scoring by 21% and 
21% respectively. On the one hand, the 7% gap in 
F-score between Relaxed Match 1 and 2 also indi-
cates that our rules can effectively locate the clause 
of a cause. On the other hand, the rather low per-
formances of the baseline show that most causes 
recognized by the baseline are wrong although the 
baseline effectively detects the cause occurrence, 
as indicated in Table 7. In addition, we check the 
accuracy (precision) and contribution (recall) of 
each rule. In descending order, the top four accu-
rate rules are: Rules 7, 10, 11, and 1; and the top 
four contributive rules are: Rules 2, 15, 14, and 3.  
6 Conclusion  
Emotion processing has been a great challenge in 
NLP. Given the fact that an emotion is often trig-
gered by cause events and that cause events are 
integral parts of emotion, we propose a linguistic-
driven rule-based system for emotion cause detec-
tion, which is proven to be effective. In particular, 
we construct a Chinese emotion cause corpus an-
notated with emotions and the corresponding cause 
events. Since manual detection of cause events is 
labor-intensive and time-consuming, we intend to 
use the emotion cause corpus to produce automatic 
extraction system for emotion cause events with 
machine learning methods. We believe that our 
rule-based system is useful for many real world 
applications. For instance, the information regard-
ing causal relations of emotions is important for 
product design, political evaluation, etc. Such a 
system also shed light on emotion processing as 
the detected emotion cause events can serve as 
clues for the identification of implicit emotions.  
52
References  
Alm, C. O., D. Roth and R. Sproat. 2005. Emotions 
from Text: Machine Learning for Text-based Emo-
tion Prediction. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, Canada, 6-8 October, 
pp. 579-586. 
Alm, C. O. 2009. Affect in Text and Speech. VDM 
Verlag: Saarbr?cken. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. A Cog-
nitive-based Annotation System for Emotion Com-
puting. In Proceedings of the Third Linguistic 
Annotation Workshop (The LAW III), ACL 2009. 
Chaumartin, F.-R. 2007. A Knowledgebased System for 
Headline Sentiment Tagging. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions. 
Darwin, C. 1859. On the Origin of Species by Means of 
Natural Selection. London: John Murray. 
Descartes, R. 1649. The Passions of the Soul. In J. Cot-
tingham et al (Eds), The Philosophical Writings of 
Descartes. Vol. 1, 325-404. 
Ekman, P. 1984. Expression and the Nature of Emotion. 
In Scherer, K. and P. Ekman (Eds.), Approaches to 
Emotion. Hillsdale, N.J.: Lawrence Erlbaum. 319-
343. 
James, W. 1884. What is an Emotion? Mind, 9(34):188?
205. 
Kozareva, Z., B. Navarro, S. Vazquez, and A. Nibtoyo. 
2007. UA-ZBSA: A Headline Emotion Classifica-
tion through Web Information. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions.  
K?vecses, Z. 2000. Metaphor and Emotion: Language, 
Culture and Body in Human Feeling. Cambridge: 
Cambridge University Press. 
Lee, S. Y. M. 2010. A Linguistic Approach towards 
Emotion Detection and Classification. Ph.D. Disser-
tation. Hong Kong. 
Lee, S. Y. M., C. Ying, and C.-R. Huang. 2010. Emo-
tion Cause Events: Corpus Construction and Analy-
sis. In Proceedings of The Seventh International 
Conference on Language Resources and Evaluation 
(LREC 2010). May 19-21. Malta. 
Lee, S. Y. M. and C.-R. Huang. 2009. Explicit Epistem-
ic Markup of Causes in Emotion Constructions. The 
Fifth International Conference on Contemporary 
Chinese Grammar. Hong Kong. November 27 - De-
cember 1. 
Masum, S. M., H. Prendinger, and M. Ishizuka. 2007. 
Emotion Sensitive News Agent: An Approach To-
wards User Centric Emotion Sensing from the News. 
In Proceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based Ap-
proach to Finding Happiness. In Proceedings of the 
AAAI Spring Symposium on Computational Ap-
proaches to Weblogs.  
Ortony A., G. L. Clone, and A. Collins. 1988. The Cog-
nitive Structure of Emotions. New York: Cambridge 
University Press. 
Picard, R.W. 1995. Affective Computing. Cambridge. 
MA: The MIT Press. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Saur?, R., J. Littman, R. Knippen, R. Gaizauskas, A. 
Setzer, and J. Pustejovsky. 2004. TimeML Annota-
tion Guidelines. http://www.timeml.org. 
Spinoza, B. 1985. Ethics. In E. Curley, The Collected 
Works of Spinoza. Princeton, N.J.: Princeton Univer-
sity Press. Vol 1. 
Talmy, L. 2000. Toward a Cognitive Semantics. Vol. 
1and 2. Cambridge: MIT Press. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. Emo-
tion Classification Using Massive Examples Ex-
tracted from the Web. In Proceedings of COLING.   
Turner, J. H. 1996. The Evolution of Emotions in Hu-
mans: A Darwinian-Durkheimian Analysis. Journal 
for the Theory of Social Behaviour, 26:1-34. 
Turner, J. H. 2000. On the Origins of Human Emotions: 
A Sociological Inquiry into the Evolution of Human 
Affect. California: Stanford University Press. 
Wierzbicka, A. 1999. Emotions Across Languages and 
Cultures: Diversity and Universals. Cambridge: 
Cambridge University Press. 
 
53
 Textual Emotion Processing From Event Analysis 
 
 
Chu-Ren Huang?, Ying Chen*?, Sophia Yat Mei Lee?  
?Department of Chinese and Bilingual Studies * Department of Computer Engineering 
The Hong Kong Polytechnic University China Agricultural University 
{churenhuang, chenying3176, sophiaym}@gmail.com 
  
 
 
 
 
 
 
Abstract 
Textual emotion recognition has gained a lot of 
attention recent years; it is however less devel-
oped due to the complexity nature of emotion. In 
this paper, we start with the discussion of a num-
ber of fundamental yet unresolved issues concern-
ing emotion, which includes its definition, 
representation and technology. We then propose 
an alternative solution for emotion recognition 
taking into account of emotion causes. Two pilot 
experiments are done to justify our proposal. The 
first experiment explores the impact of emotion 
recognition. It shows that the context contains rich 
and crucial information that effectively help emo-
tion recognition. The other experiment examines 
emotion cause events in the context. We find that 
most emotions are expressed with the presence of 
causes. The experiments prove that emotion cause 
serves as an important cue for emotion recognition. 
We suggest that the combination of both emotion 
study and event analysis would be a fruitful direc-
tion for deep emotion processing. 
1 Introduction 
The study of emotion attracts increasingly greater 
attention in the field of NLP due to its emerging 
wide applications, such as customer care (Gupta et 
al., 2010), and social information understanding 
(Lisa and Steyvers, 2010). In contrast to sentiment, 
which is the external subjective evaluation, emo-
tion mainly concentrates on the internal mental 
state of human (Ortony et al, 1987). Emotion is 
indeed a highly complicated concept that raises a 
lot of controversies in the theories of emotion re-
garding the fundamental issues such as emotion 
definition, emotion structure and so on. The com-
plexity nature of emotion concept makes auto-
matic emotion processing rather challenging. 
Most emotion studies put great effort on emo-
tion recognition, identifying emotion classes, such 
as happiness, sadness, and fear. On top of this 
surface level information, deeper level informa-
tion regarding emotions such as the experiencer, 
cause, and result of an emotion, needs to be ex-
tracted and analyzed for real world applications. 
In this paper, we discuss these two closely related 
emotion tasks, namely emotion recognition and 
emotion cause detection and how they contribute 
to emotion processing. 
For emotion recognition, we construct an emo-
tion corpus for explicit emotions with an unsuper-
vised method. Explicit emotions are emotions 
represented by emotion keywords such as e.g., 
?shocked? in ?He was shocked after hearing the 
news?.  In the course of emotion recognition, the 
keyword in an explicit emotion expression is de-
leted and only contextual information remains. In 
our pilot experiments, the context-based emotion 
identification works fairly well. This implies that 
plenty of information is provided in the context 
for emotion recognition. Moreover, with an in-
depth analysis of the data, we observe that it is 
often the case that emotions co-occur and interact 
in a sentence. In this paper, we deal with emotion 
recognition from a dependent view so as to cap-
ture complicated emotion expressions.   
Emotion is often invoked by an event, which in 
turn is very likely to elicit an event (Descartes 
1649, James 1884, Plutchik 1980, Wierzbicka 
1999). Despite the fact that most researches rec-
ognize the important role of events in emotion 
theories, little work, if not none, attempts to make 
explicit link between events and emotion. In this 
paper, we examine emotion constructions based 
on contextual information which often contains 
considerable relevant eventive information. In 
particular, the correlations between emotion and 
cause events will be explored based on empirical 
data. Emotion causes refer to explicitly expressed 
propositions that evoke the corresponding emo-
tions.  
To enhance emotion recognition, we examine 
emotion causes occurring in the context of an 
emotion. First, we manually annotate causes for 
emotions in our explicit emotion corpus. Since an 
emotion cause can be a complicated event, we 
model emotion cause detection as a multi-label 
problem to detect a cross-clause emotion cause. 
Furthermore, an in-depth linguistic analysis is 
done to capture the different constructions in ex-
pressing emotion causes.  
The paper is organized as follows. Section 2 
discusses some related work regarding emotion 
recognition and emotion cause detection. In Sec-
tion 3, we present our context-based emotion cor-
pus and provide some data analysis. Section 4 
describes our emotion recognition system, and 
discusses the experiments and results. In Section 5, 
we examine our emotion cause detection system, 
and discuss the performances. Finally, Section 6 
concludes our main findings for emotion process-
ing from the event perspective.   
2 Related Work  
Most current emotion studies focus on the task of 
emotion recognition, especially in affective lexi-
con construction. In comparison with emotion 
recognition, emotion cause detection is a rather 
new research area, which account for emotions 
based on the correlations between emotions and 
cause events. This section discusses the related 
research on emotion recognition and emotion 
cause detection. 
2.1 Emotion Recognition 
Although emotion recognition has been inten-
sively studied, some issues concerning emotion 
remain unresolved, such as emotion definition, 
emotion representation, and emotion classification 
technologies. 
For the emotion definition, emotion has been 
well-known for its abstract and uncertain defini-
tion which hinders emotion processing as a whole. 
Ortony et al, (1987) conducted an empirical study 
for a structure of affective lexicon based on the 
~500 words used in previous emotion studies. 
However, most of the emotion corpora in NLP try 
to avoid the emotion definition problem. Instead, 
they choose to rely on the intuition of annotators 
(Ren?s Blog Emotion Corpus, RBEC, Quan and 
Ren, 2009) or authors (Mishne?s blog emotion 
corpus, Mishne, 2005). Therefore, one of the cru-
cial drawbacks of emotion corpora is the problem 
of poor quality. In this paper, we explore emotion 
annotation from a different perspective. We con-
centrate on explicit emotions, and utilize their 
contextual information for emotion recognition.  
In terms of emotion representation, textual 
emotion corpora are basically annotated using ei-
ther the enumerative representation or the compo-
sitional representation (Chen et al, 2009). The 
enumerative representation assigns an emotion a 
unique label, such as pride and jealousy. The 
compositional representation represents an emo-
tion through a vector with a small set of fixed ba-
sic emotions with associated strength. For instance, 
pride is decomposed into ?happiness + fear? ac-
cording to Turner (2000).  
With regard to emotion recognition technolo-
gies, there are two kinds of classification models. 
One is based on an independent view (Mishne, 
2005; Mihalcea and Liu, 2006; Aman and Szpa-
kowicz, 2007; Tokuhisa et al, 2008; Strapparava 
and Mihalcea, 2008), and the other is a dependent 
view (Abbasi et al 2008; Keshtkar and Inkpen, 
2009). The independent view treats emotions sep-
arately, and often chooses a single-label classifica-
tion approach to identify emotions. In contrast, the 
dependent view takes into account complicated 
emotion expressions, such as emotion interaction 
and emotion co-occurrences, and thus requires 
more complicated models. Abbasi et al (2008) 
adopt an ensemble classifier to detect the co-
occurrences of different emotions; Keshtkar and 
Inkpen (2009) use iteratively single-label classifi-
ers in the top-down order of a given emotion hier-
archy. In this paper, we examine emotion 
recognition as a multi-label problem and investi-
gate several multi-label classification approaches.    
 2.2 Emotion Cause Detection 
Although most emotion theories recognize the 
important role of causes in emotion analysis (Des-
cartes, 1649; James, 1884; Plutchik, 1962; Wierz-
bicka 1996), yet very few studies in NLP explore 
the event composition and causal relation of emo-
tions. As a pilot study, the current study proposes 
an emotion cause detection system.  
Emotion cause detection can be considered as a 
kind of causal relation detection between two 
events. In other words, emotion is envisioned as 
an event type which triggers another event, i.e. 
cause event. We attempt to examine emotion 
cause relations for open domains. However, not 
much work (Marcu and Echihabi, 2002; Girju, 
2003; Chang and Choi, 2006) has been done on 
this kind of general causal relation for open do-
mains. 
Most existing causal relation detection systems 
contain two steps: 1) cause candidate identifica-
tion; 2) causal relation detection. However, Step 1) 
is often oversimplified in real systems. For exam-
ple, the cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with selected conjunction 
words (Marcu and Echihabi, 2002). Moreover, the 
task of Step 2) often is considered as a binary 
classification problem, i.e. ?causal? vs. ?non-
causal?.  
With regard to feature extraction, there are two 
kinds of information extracted to identify the 
causal relation in Step 2). One is constructions 
expressing a cause-effect relation (Chang and 
Choi, 2005; Girju, 2003), and the other is seman-
tic information in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of infor-
mation often interact with each other in a real 
cause detection system. 
3 Emotion Annotated Sinica Corpus 
(EASC) 
EASC is an emotion annotated corpus comprising 
two kinds of sentences: emotional-sentence corpus 
and neutral-sentence corpus. It involves two com-
ponents: one for emotion recognition, which is 
created with an unsupervised method (Chen et al 
2009), and the other is for emotion cause detection, 
which is manually annotated (Chen et al 2010).  
3.1 The Corpus for Emotion Recognition 
With the help of a set of rules and a collection of 
high quality emotion keywords, a pattern-based 
approach is used to extract emotional sentences 
and neutral sentences from the Academia Sinica 
Balanced Corpus of Mandarin Chinese (Sinica 
Corpus). If an emotion keyword occurring in a 
sentence satisfies the given patterns, its corre-
sponding emotion type will be listed for that sen-
tence. As for emotion recognition, each detected 
keyword in a sentence is removed, in other words, 
the sentence provides only the context of that 
emotion. Due to the overwhelming of neutral sen-
tences, EASC only contains partial neutral sen-
tences besides emotional sentences. For 
experiments, 995 sentences are randomly selected 
for human annotation, which serve as the test data. 
The remaining 17,243 sentences are used as the 
training data.  
In addition, in the course of creating the emo-
tion corpus, Chen et al (2009) list the emotion 
labels in a sentence using the enumerative repre-
sentation. Besides, an emotion taxonomy is pro-
vided to re-annotate an emotion with the 
compositional representation. With the taxonomy, 
an emotion is decomposed into a combination of 
primary emotions (i.e. happiness, fear, anger, 
sadness, and surprise). 
From this corpus, we observe that ~54% emo-
tional sentences contain two emotions, yet only 
~2% sentences contain more than two emotions. 
This implies emotion recognition is a typical mul-
ti-label problem. Particularly, more effort should 
be put on the co-occurrences of two emotions. 
3.2 The Corpus for Emotion Cause De-
tection 
Most emotion theories agree that the five primary 
emotions (i.e. happiness, sadness, fear, anger, and 
surprise) are prototypical emotions. Therefore, for 
emotion cause detection, we only deal with the 
emotional sentences containing a keyword repre-
senting one of these primary emotions. Beyond a 
focus sentence, its context (the previous sentence 
and the following sentence) is also extracted, and 
those three sentences constitute an entry. After 
filtering non-emotional and ambiguous sentences, 
5,629 entries remain in the emotion cause corpus.  
Each emotion keyword is annotated with its 
corresponding causes if existing. An emotion 
keyword can sometimes be associated with more 
than one cause, in such a case, both causes are 
marked. Moreover, the cause type is also identi-
fied, which is either a nominal event or a verbal 
event (a verb or a nominalization).  
From the corpus, we notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause, which means that 
causal event is a strong indicator for emotion rec-
ognition.  
Furthermore, since the actual cause can some-
times be so complicated that it involves several 
events, we investigate the span of a cause text as 
follows. For each emotion keyword, an entry is 
segmented into clauses with some punctuations, 
and thus an entry becomes a list of cause candi-
dates. In terms of the cause distribution, we find 
~90% causes occurring between ?left_2? and 
?right_1?. Therefore, our cause search is limited to 
the list of cause candidates which contains five 
text units, i.e. <left_2, left_1, left_0, right_0, 
right_1>. If the clause where emotion keyword 
locates is assumed as a focus clause, ?left_2? and 
?left_1? are the two previous clauses, and ?right_1? 
is the following one. ?left_0? and ?right_0? are the 
partial texts of the focus clause, which locate in 
the left side of and the right side of the emotion 
keyword, respectively. Finally, we find that ~14% 
causes occur cross clauses. 
4 Emotion Processing with multi-label 
models   
4.1 Multi-label Classification for Emo-
tion recognition 
Based on our corpus, two critical issues for emo-
tion recognition need to be dealt with: emotion 
interaction and emotion co-occurrences. Co-
occurrence of multiple emotions in a sentence 
makes emotion recognition a multi-label problem. 
Furthermore, the interaction among different emo-
tions in a sentence requires a multi-label model to 
have a dependent view. In this paper, we explore 
two simple multi-label models for emotion recog-
nition. 
The Binary-based (BB) model: decompose the 
task into multiple independent binary classifiers 
(i.e., ?1? for the presence of one emotion; ?0? for 
the absence of one emotion), where each emotion 
is allocated a classifier. For each test instance, all 
labels (emotions) from the classifiers compose a 
vector. 
The label powset (LP) model: treat each possible 
combination of labels appearing in the training 
data as a unique label, and convert multi-label 
classification to single-label classification.  
Both the BB model and the LP model need a 
multi-class classifier. For our experiment, we 
choose a Max Entropy package, Mallet1. In this 
paper, we use only words in the focus sentence as 
features. 
4.2 Emotion Recognition Experiments 
To demonstrate the impact of our context-based 
emotion corpus to emotion recognition, we com-
pare EASC data to Ren?s Blog Emotion Corpus 
(RBEC). RBEC is a human-annotated emotion 
corpus for both explicit emotions and implicit 
emotions. It adopts the compositional representa-
tion with eight emotion dimensions (anger, anxi-
ety, expect, hate, joy, love, sorrow, and surprise). 
For each dimension, a numerical value ranging in 
{0.0, 0.1, 0.2... 1.0} indicates the intensity of the 
emotion in question. There are totally 35,096 sen-
tences in RBEC. To fairly compare with the 
EASC data, we convert a numerical value to a 
binary value. An emotion exists in a sentence only 
when its corresponding intensity value is greater 
than 0.  
For RBEC data, we use 80% of the corpus as 
the training data, 10% as the development data, 
and 10% as the test data. For EASC, apart from 
the test data, we divide its training data into two 
sets: 90% for our training data, and 10% for our 
development data. For evaluation of a multi-label 
task, three measures are used: accuracy (extract 
match ratio), Micro F1, and Macro F1. Accuracy 
is the extract match ratio of the whole assignments 
in data, and Micro F1 and Macro F1 are the aver- 
 
 
 
                                                           
1
 http://mallet.cs.umass.edu/ 
Table 1: The overall performances for the multi-label models   
 
 
 
 
 
 
 
 
age scores of F scores of all possible values for all 
variables. Micro F1 takes the emotion distribution 
into account, while Macro F1 is just the average 
of all F scores. Note that due to the overwhelming 
percentage of value 0 in the multi-label task, dur-
ing the calculating of Micro F1 and Macro F1, 
most previous multi-label systems take only value 
1 (indicating the existence of the emotion) into 
account. 
In Table 1, we notice that the emotion recogni-
tion system on our context-based corpus achieves 
similar performance as the one on human-
annotated corpus. This implies that there is rich 
contextual information with respect to emotion 
identification. 
5 Emotion Cause Detection 
Most emotion theories agree that there is a strong 
relationship between emotions and events (Des-
cartes 1649, James 1884, Plutchik 1980, Wierz-
bicka 1999). Among the rich information in the 
context of an emotion, cause event is the most 
crucial component of emotion. We therefore at-
tempt to explore emotion causes, and extract 
causes for emotion automatically.  
5.1 Emotion Cause Detection 
Based on the cause distribution analysis in Section 
3.2, in contrast to binary classification used in 
previous work, we formalize emotion cause detec-
tion as a multi-label problem as follows.  
Given an emotion keyword and its context, its 
label is the locations of its causes, such as ?left_1, 
left_0?. Then, we use the LP model to identify the 
cause for each sentence as well as an emotion 
keyword. With regard to emotion cause detection, 
the LP model is more suitable than the BB model 
because the LP model can easily capture the pos-
sible label combinations.  
   In terms of feature extraction, unlike emotion 
recognition, emotion cause detection relies more 
on linguistic constructions, such as ?The BP oil 
spill makes the country angry?, ?I am sad because 
of the oil spill problem? and so on. 
According to our linguistic analysis, we cre-
ate 14 patterns to extraction some common emo-
tion cause expressions. Some patterns are 
designed for general cause detection using linguis-
tic cues such as conjunctions and prepositions. 
Others are designed for some specific emotion 
cause expressions, such as epistemic markers and 
reported verbs. Furthermore, to avoid the low 
coverage problem of the rule-based patterns, we 
create another set of features, which is a group of 
generalized patterns. For details, please refer to 
Chen et al (2010).  
5.2 Experiments 
For EASC, we reserve 80% as the training data, 
10% as the development data, and 10% as the test 
data. For evaluation, we first convert a multi-label 
tag outputted from our system into a binary tag 
(?Y? means the presence of a causal relation; ?N? 
means the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. We then adopt 
three common measures, i.e. precision, recall and 
F-score, to evaluate the result. 
A naive baseline is designed as follows: The 
baseline searches for the cause candidates in the 
order of <left_1, right_0, left_2, left_0, right_1>. 
If the candidate contains a noun or a verb, this 
clause is considered as a cause and the search 
stops. 
Table 2 shows the overall performances of our 
emotion cause detection system. First, our system 
based on a multi-label approach as well as power-
ful linguistic features significantly outperforms 
the na?ve baseline. Moreover, the greatest im-
provement is attributed to the 14 linguistic pat-
terns (LP). This implies the importance of 
linguistic cues for cause detection. Moreover, the 
general patterns (GP) achieve much better per-
 EASC RBEC 
 BB LP BB LP 
Accuracy 21.30 28.07 22.99 28.33 
Micro F1 41.96 46.25 44.77 44.74  
Macro F1 34.78 35.52 36.48 38.88  
formance on the recall and yet slightly hurt on the 
precision. 
The performances (F-scores) for ?Y? and ?N? 
tags separately are shown in Table 3. First, we 
notice that the performances of the ?N? tag are 
much better than the ones of ?Y? tag. Second, it is 
surprising that incorporating the linguistic features 
significantly improves the ?Y? tag only (from 33% 
to 56%), but does not affect ?N? tag. This suggests 
that our linguistic features are effective to detect 
the presence of causal relation, and yet do not hurt 
the detections of ?non_causal? relation. Further-
more, the general feature achieves ~8% improve-
ments for the ?Y? tag. 
 
Table 2: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ GP 73.90 72.70 73.26 
 
Table 3: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ GP 56.84 89.68 
 
6 Discussions 
Many previous works on emotion recognition 
concentrated on emotion keyword detection. 
However, Ortony et al (1987) pointed out the dif-
ficulty of emotion keyword annotation, be it man-
ual or automatic annotation. Emotion keywords 
are rather ambiguous, and also contain other in-
formation besides affective information, such as 
behavior and cognition. Therefore, contextual in-
formation provides important cues for emotion 
recognition. Furthermore, we propose an alterna-
tive way to explore emotion recognition, which is 
based on emotion cause. Through two pilot ex-
periments, we justify the importance of emotion 
contextual information for emotion recognition, 
particularly emotion cause.  
We first examine emotion processing in terms 
of events. Context information is found to be very 
important for emotion recognition. Furthermore, 
most emotions are expressed with the presence of 
causes in context, which implies that emotion 
cause is the crucial information for emotion rec-
ognition. In addition, emotion cause detection also 
explores deep understanding of an emotion. Com-
pared to emotion recognition, emotion cause de-
tection requires more semantic and pragmatic 
information. In this paper, based on the in-depth 
linguistic analysis, we extract different kinds of 
constructs to identify cause events for an emotion.  
To conclude, emotion processing is a compli-
cated problem. In terms of emotion keywords, 
how to understand appropriately to enhance emo-
tion recognition needs more exploration. With 
respect to emotion causes, first, event processing 
itself is a challenging topic, such as event extrac-
tion and co-reference. Second, how to combine 
event and emotion in NLP is still unclear, but it is 
a direction for further emotion studies.  
References  
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. Af-
fect Analysis of Web Forums and Blogs using Cor-
relation Ensembles?. In IEEE Tran. Knowledge and 
Data Engineering, vol. 20(9), pp. 1168-1180. 
Aman, S. and S. Szpakowicz. 2007. Identifying Ex-
pressions of Emotion in Text. In Proceedings of 
10th International Conference on Text, Speech and 
Dialogue, Lecture Notes in Computer Science 4629, 
196--205.   
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for cau-
sality extraction using cue phrase and word pair 
probabilities. Information Processing and Man-
agement. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And Its 
Implications for Emotion Processing. In Proceed-
ings of the 23rd Pacific Asia Conference on Lan-
guage, Information and Computation.  
Chen, Y., Y. M. Lee, S. Li and C.-R. Huang. 2010. 
Emotion Cause Detection with Linguistic Construc-
tions. In Proceedings of the 23rd International 
Conference on Computational Linguistics. 
Descartes, R. 1649. The Passions of the Soul. USA: 
Hackett Publishing Company. 
Ghazi, D., D. Inkpen and S. Szpakowicz. 2010. Hierar-
chical versus Flat Classification of Emotions in Text. 
In Proceedings of NAACL-HLT 2010 Workshop on 
Computational Approaches to Analysis and Genera-
tion of Emotion in Text. Los Angeles, CA: NAACL. 
Girju, R. 2003. Automatic Detection of Causal Rela-
tions for Question Answering. In the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, Workshop on Multilingual Summarization 
and Question Answering - Machine Learning and 
Beyond, Sapporo, Japan. 
Gupta, N., M. Gilbert, and G. D. Fabbrizio. Emotion 
Detection in Email Customer Care. In Proceedings 
of NAACL-HLT 2010 Workshop on Computational 
Approaches to Analysis and Generation of Emotion 
in Text. 
James, W. 1884. What is an Emotion? Mind, 9(34): 
188?205. 
Keshtkar, F. and D. Inkpen. 2009. Using Sentiment 
Orientation Features for Mood Classification in 
Blog Corpus. In Proceedings of IEEE International 
Conference on Natural Language Processing and 
Knowledge Eng. (IEEE NLP-KE 2009), Sep. 24-27. 
Marcu, D. and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R., and H. Liu. 2006. A Corpus-based Ap-
proach to Finding Happiness. In Proceedings of 
AAAI. 
Mishne, G. 2005. Experiments with Mood Classifica-
tion in Blog Posts. In Proceedings of Style2005 ? the 
1st Workshop on Stylistic Analysis of Text for Infor-
mation Access, at SIGIR 2005. 
Ortony, A., G. L. Clore, and M. A. Foss. 1987. The 
Referential Structure of the Affective Lexicon. Cog-
nitive Science, 11: 341-364. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP02, 79-86. 
Pearl, L. and M. Steyvers. 2010. Identifying Emotions, 
Intentions, and Attitudes in Text Using a Game with 
a Purpose. In Proceedings of NAACL-HLT 2010 
Workshop on Computational Approaches to Analy-
sis and Generation of Emotion in Text. Los Angeles, 
CA: NAACL. 
Persing, Isaac and Vincent Ng. 2009. Semi-Supervised 
Cause Identification from Aviation Safety Reports. 
In Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Quan, C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Expression Analysis. 
In Proceedings of EMNLP. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. Emo-
tion Classification Using Massive Examples Ex-
tracted from the Web. In Proceedings of COLING.   
Turner, J. H. 2000. On the Origins of Human Emotions: 
A Sociological Inquiry into the Evolution of Human 
Affect. California: Stanford University Press. 
Wierzbicka, A. 1999. Emotions across Languages and 
Cultures: Diversity and Universals. Cambridge: 
Cambridge University Press. 
  
