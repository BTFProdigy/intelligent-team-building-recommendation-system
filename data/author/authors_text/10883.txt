Automatically Inducing Ontologies from Corpora 
Inderjeet Mani 
 
 
Department of Linguistics 
Georgetown University, ICC 452 
37th and O Sts, NW 
Washington, DC 20057, USA 
im5@georgetown.edu 
Ken Samuel, Kris Concepcion and 
David Vogel 
 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102, USA 
{samuel, kjc9, dvogel}@mitre.org 
 
Abstract 
The emergence of vast quantities of on-line 
information has raised the importance of methods 
for automatic cataloguing of information in a 
variety of domains, including electronic commerce 
and bioinformatics. Ontologies can play a critical 
role in such cataloguing. In this paper, we describe 
a system that automatically induces an ontology 
from any large on-line text collection in a specific 
domain. The ontology that is induced consists of 
domain concepts, related by kind-of and part-of 
links. To achieve domain-independence, we use a 
combination of relatively shallow methods along 
with any available repositories of applicable 
background knowledge. We describe our 
evaluation experiences using these methods, and 
provide examples of induced structures.  
1 Introduction 
The emergence of vast quantities of on-line 
information has raised the importance of methods 
for automatic cataloguing of information in a 
variety of domains, including electronic commerce 
and bioinformatics. Ontologies1 can play a critical 
role in such cataloguing. In bioinformatics, for 
example, there is growing recognition that 
common ontologies, e.g., the Gene Ontology2, are 
critical to interoperation and integration of 
biological data, including both structured data as 
found in protein databases, as well as unstructured 
data, as found in on-line biomedical literature.  
Constructing an ontology is an extremely 
laborious effort. Even with some reuse of ?core? 
knowledge from an Upper Model (Cohen et al 
1999), the task of creating an ontology for a 
particular domain and task has a high cost, 
incurred for each new domain. Tools that could 
automate, or semi-automate, the construction of 
                                                     
1 This research was supported by the National Science 
Foundation (ITR-0205470). 
2 www.geneontology.org 
ontologies for different domains could 
dramatically reduce the knowledge creation cost.  
One approach to developing such tools is to rely 
on information implicit in collections of on-line 
text in a particular domain. If it were possible to 
automatically extract terms and their semantic 
relations from the text corpus, the ontology 
developer could build on that knowledge, revising 
it, as needed, etc. This would be more cost-
effective than having a human develop the 
ontology from scratch.  
Our approach is inspired by research on topic-
focused multi-document summarization of large 
text collections, where there is a need to 
characterize the collection content succinctly in a 
hierarchy of topic terms and their relationships. 
Current approaches to multi-document 
summarization combine linguistic analysis, corpus 
statistics, and the use of background semantic 
knowledge from generic thesauri such as WordNet 
to infer semantic information about a person. In 
extending such approaches to ontology induction, 
the hypothesis is that similar hybrid approaches 
can be used to identify technical terms in a 
domain-specific corpus and infer semantic 
relationships among them.  
In this paper, we describe a system that 
automatically induces an ontology from any large 
on-line text collection in a specific domain, to 
support cataloguing in information access and data 
integration tasks. The induced ontology consists of 
domain concepts related by kind-of and part-of 
links, but does not include more specialized 
relations or axioms. The structure of the ontology 
is a directed acyclic graph (DAG). To achieve 
domain-independence, we use a combination of 
relatively shallow methods along with existing 
repositories of applicable background knowledge. 
These are described in Section 2. In Section 3, we 
also introduce a new metric Relation Precision for 
evaluating induced ontologies in comparison with 
reference ontologies. We have applied our system 
to produce ontologies in numerous domains: 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 47
  
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
IRS 
Publication 17 
285 
 
k1 
0 285 
 
n1 
Reuters 
Corpus 
9 
k2 
19,024 19,043 
n2 
Total 294 19,024 19,328 
 
Table 1: Distribution of ?income tax? in domain and background corpora
(i) newswire from the TREC collection (ii) 
taxation information from the IRS (Publication 17, 
from (IRS 2001)), (iii) epidemiological newsgroup 
messages from the Program for Monitoring 
Emerging Diseases (PROMED) from the 
Federation of American Scientists3, (iv) the text of 
a book by the first author called Automatic 
Summarization, and (v) MEDLINE biomedical 
abstracts retrieved from the National Library of 
Medicine?s PubMed system4. In the latter domain, 
we have begun building a large ontology using the 
ontology induction methods along with post-
editing by domain experts in molecular biology at 
Georgetown University 5 . This ontology, called 
PRONTO, involves hundreds of thousands of 
protein names found in MEDLINE abstracts and in 
UNIPROT, the world?s largest protein database6. It 
is therefore infeasible to construct PRONTO by 
hand from scratch. PRONTO is also much larger 
than other ontologies in the biology area; for 
example, the Gene Ontology is rather high-level, 
and contains (as of March 2004) only about 17,000 
terms. 
                                                     
3 www.fas.org/promed/ 
4www4.ncbi.nlm.nih.gov/PubMed/ 
5 complingone.georgetown.edu/~prot/ 
6pir.georgetown.edu 
2 Approach 
2.1 System Architecture 
An overall architecture for domain-independent 
ontology induction is shown in Figure 1. The 
documents are preprocessed to separate out 
headers. Next, terms are extracted using finite-state 
syntactic parsing and scored to discover domain-
relevant terms. The subsequent processing infers 
semantic relations between pairs of terms using the 
?weak? knowledge sources run in the order 
described below. Evidence from multiple 
knowledge sources is then combined to infer the 
resulting relations. The resulting ontologies are 
written out in a standard XML-based format (e.g., 
XOL, RDF, OWL), for use in various information 
access applications.  
While the ontology induction procedure does not 
involve human labor, except for writing the 
preprocessing and term tokenization program for 
specialized technical domains, the human may edit 
the resulting ontology for use in a given 
application. An ontology editor has been 
developed, discussed briefly in Section 3.1. 
2.2 Term Discovery 
The system takes a collection of documents in a 
subject area, and identifies terms characteristic of 
the domain.  In a given domain such as 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology48
bioinformatics, specialized term tokenization (into 
single- and multi-word terms) is required. The 
protein names can be long, e.g., 
?steroid/thyroid/retinoic nuclear hormone receptor 
homolog nhr-35?, and involve specialized patterns. 
In constructing PRONTO, we have used a protein 
name tagger based on an ensemble of statistical 
classifiers to tag protein names in collections of 
MEDLINE abstracts (Anon 2004). Thus, in such a 
domain, a specialized tagger replaces the 
components in the dotted box in Figure 1. 
In other domains, we adopt a generic term-
discovery approach. Here the text is tagged for 
part-of-speech, and single- and multi-word terms 
consisting of minimal NPs are extracted using 
finite-state parsing with CASS (Abney 1996). All 
punctuation except for hyphens are removed from 
the terms, which are then lower-cased. Each word 
in each term is stemmed, with statistics (see below) 
being gathered for each stemmed term. Multi-word 
terms are clustered so that open, closed and 
hyphenated compounds are treated as equivalent, 
with the most frequent term in the collection being 
used as the cluster representative.  
The terms are scored for domain-relevance based 
on the assumption that if a term occurs 
significantly more in a domain corpus than in a 
more diffuse background corpus, then the term is 
clearly domain relevant.  
As an illustration, in Table 1 we compare the 
number of documents containing the term ?income 
tax? (or ?income taxes?) in a long (2.18 Mb) IRS 
publication, Publication 17, from an IRS web site 
(IRS 2001) compared to a larger (27.63 Mb subset 
of the) Reuters 21578 news corpus7. One would 
expect that ?income tax? is much more a 
characteristic of the IRS publication, and this is 
borne out by the document frequencies in the table. 
We use the log likelihood ratio (LLR) (Dunning 
1993) given by 
-2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2))  
LLR measures the extent to which a 
hypothesized model of the distribution of cell 
counts, Ha, differs from the null hypothesis, Ho  
(namely, that the percentage of documents 
containing this term is the same in both corpora). 
We used a binomial model for Ho and Ha8.   
2.3 Relationship Discovery 
The main innovation in our approach is to fuse 
together information from multiple knowledge 
                                                     
7 In Publication 17, each ?chapter? is a document. 
8From Table 1, p=294/19238=.015, p1=285/285=1.0, 
p2=9/19043=4.72, k1=285, n1=285, k2=9, n2=19043.  
sources as evidence for particular semantic 
relationships between terms. To infer semantic 
relations such as kind-of and part-of, the system 
uses a bottom-up data-driven approach using a 
combination of evidence from shallow methods. 
2.3.1 Subphrase Relations  
These are based on the presence of common 
syntactic heads, and allow us to infer, for example, 
that ?p68 protein? is a kind-of ?protein?. Likewise, 
in the TREC domain, subphrase analysis tells us 
that ?electric car? is a kind of ?car?, and in the IRS 
domain, that ?federal income tax? is a kind of 
?income tax?.  
2.3.2 Existing Ontology Relations 
These are obtained from a thesaurus. For 
example, the Gene Ontology can be used to infer 
that ?ATP-dependent RNA helicase? is a kind of 
?RNA-helicase?. Likewise, in the TREC domain, 
using WordNet tells us that ?tailpipe? is part of 
?automobile?, and in the IRS domain, that ?spouse? 
is a kind of ?person?.  Synonyms are also merged 
together at this stage. 
2.3.3 Contextual Subsumption Relations 
We also infer hierarchical relations between 
terms, by top-down clustering using a context-
based subsumption (CBS) algorithm. The 
algorithm uses a probabilistic measure of set 
covering to find subsumption relations. For each 
term in the corpus, we note the set of contexts in 
which the term appears. Term1 is said to subsume 
term2 when the conditional probability of term1 
appearing in a context given the presence of term2, 
i.e., P(term1|term2), is greater than some threshold.  
CBS is based on the algorithm of (Lawrie et al 
2001), which used a greedy approximation of the 
Domination Set Problem for graphs to discover 
subsumption relations among terms. Unlike their 
work, we did not seek to minimize the set of 
covering terms; therefore, a subsumed term may 
have multiple parents. The conditional probability 
threshold (0.8) we use to determine subsumption is 
much higher than in their approach. We also 
restrict the height of the hierarchies we build to 
three tiers. Tightening these latter two constraints 
appears to notably improve the quality of our 
subsumption relations.  
The largest corpus against which CBS has run is 
the ProMed corpus where, considering each 
paragraph a distinct context, there were 117,690 
contexts in the 11,198 documents. Here is an 
example from ProMed of a transitive relation that 
spans three tiers: ?mosquito? is a hypernym of 
?mosquito pool?, and ?mosquito? is also a 
hypernym of ?standing water?. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 49
2.3.4 Explicit Patterns Relations 
This knowledge source infers specific relations 
between terms based on characteristic cue-phrases 
which relate them. For example, the cue-phrase 
?such as? (Hearst 1992) (Caraballo 1999) suggest a 
kind-of relation, e.g., ?a ligand such as 
triethylphosphine? tells us that ?triethylphosphene? 
is a kind of ?ligand?. Likewise, in the TREC 
domain, ?air toxics such as benzene? can suggest 
that ?benzene? is a kind of ?air toxic?. However, 
since such cue-phrase patterns tend to be sparse in 
occurrence, we do not use them in the evaluations 
described below.  
2.3.5 Domain-Specific Knowledge Sources 
Although our approach is domain-independent, it 
is possible to factor in domain knowledge sources 
for a given domain. For example, in biology, ?ase? 
is usually a suffix indicating an enzyme.  
Postmodifying PPs (found using a CASS grammar) 
can also be useful in some domains, as shown in  
?tax on investment income of child? in Figure 2. 
We have so far, however, not investigated other 
domain-specific knowledge sources. 
 
2.4 Evidence Combination 
The main point about these and other knowledge 
sources is that each may provide only partial 
information. Combining these knowledge sources 
together, we expect, will lead to superior 
performance compared to just any one of them. 
Not only do inferences from different knowledge 
sources support each other, but they are also 
combined to produce new inferences by transitivity 
relations. For example, since phrase analysis tells 
us that ?pyridine metabolism? is a kind-of 
?metabolism?, and Gene Ontology tells us that 
?metabolism? is a kind-of ?biological process?, it 
follows that ?pyridine metabolism? is a kind-of 
?biological process?. The evidence combination, in 
addition to computing transitive closure of these 
relations, also detects inconsistencies, querying the 
user to resolve them when detected. 
3 Evaluation 
3.1 Informal Assessment 
Subphrase Relations is a relatively high-
precision knowledge source compared to the 
others, producing many linked chains. Its 
performance can be improved by flagging and 
excluding proper names and idioms from its input 
(e..g, so that ?palm pilot? doesn?t show up as a 
kind-of ?pilot?). However, a chain of such relations 
can be interrupted by terms that aren?t lexically 
similar, but that are nevertheless in a kind-of 
relation. Some of these gaps are filled by 
transitivity relations involving other knowledge 
sources, especially Existing Ontologies, which is 
especially useful in filling gaps in some of the 
upper levels of the ontology. While Contextual 
Subsumption is good at  discovering associations 
between ?leaves? in the DAG and other concepts, 
the method cannot reliably infer the label of the 
relation. For example, in the IRS domain, we 
obtain ?divorce? as more general than ?decree of 
divorce? and ?separate maintenance?, but we don?t 
know the nature of the relations. Contextual 
Subsumption-inferred links are directed edges with 
label ?unknown?. 
Overall, the ontologies produced are noisy and 
require human correction, and the methods can 
produce many fragments that need to be linked by 
hand. While the system can detect cycles that need 
resolution by the human, these rarely arise
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: An IRS Ontology viewed in the Ontology Editor 
 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology50
 
Term Target  
DF 
Back-
ground 
DF 
LLR IG MI DF TF TF * 
IDF 
electric 80 61 99.9 99.9 81.3 99.9 99.9 27.8 
car 77 56 99.6 99.3 81.5 99.8 99.9 79.4 
battery 54 16 99.0 98.2 86.9 98.7 99.9 94.9 
emission 15 0 96.5 96.8 99.2 79.1 96.6 64.8 
year 58 505 67.9 67.6 25.0 99.2 99.7 65.7 
informal 10 29 66.2 66.3 0.2 48.6 99.7 99.2 
record 8 138 15.2 15.7 4.4 50.2 99.9 99.9 
osha 1 0 0.0 0.0 0.0 0.0 99.9 0.0 
Table 2: Comparing Topic 230 Term Percentile Rankings 
 
For a flavor of the kind of results we get, see 
Figure 2, which displays an ontology induced 
without any human intervention from IRS 
Publication 17. Here the DAG is displayed as a 
tree. The immediate children of  ?person?, a node 
high in the ontology, is shown in the left part of the 
window. Selecting ?child? brings up its kinds as 
well as some other children linked by ?unknown? 
label via Contextual Subsumption, e.g., ?full-time 
student?. A list of orphaned terms that aren?t 
related to any others are shown on the far right. 
The terms with checkboxes are those that occur in 
the corpus; the others are those that are found 
exclusively by Existing Ontology Relations. 
Checking a term allows it to be inspected in its 
occurrence context in the corpus. The editor comes 
with a variety of tools to help integrate ontology 
fragments. 
3.2 Human Evaluation 
3.2.1 Term Scoring  
To evaluate term scoring, we used a corpus of 
news articles about automobiles that consisted of 
85 documents relevant to the TREC Topic 230 
query: ?Is the automobile industry making an 
honest effort to develop and produce an electric-
powered automobile?? In Table 2, we provide 
some examples of how the LLR term scoring 
statistic performed with respect to five others on 
selected unigrams in the Topic 230 domain: term 
frequency, document frequency, term frequency 
times inverse document frequency (TF*IDF), 
pointwise mutual information (MI), and 
information gain (IG). Terms in bold are ones we 
judged important in the Topic 230 domain, the 
others are deemed unimportant. The numbers are 
percentile rankings. LLR and IG do equally well, 
outperforming the others. 
We carried out other comparisons for two other 
domains. In the income-tax domain, a hand-built 
term list from the IRS contained 82 terms which 
occurred in IRS Publication 17, of which the 
system discovered 77 (94% recall). In the ProMed 
domain, a pre-existing hand-built taxonomy 
produced by a bioterrorism analyst had 1048 terms 
which occurred in the ProMed message corpus, of 
which 607 were discovered by the system (58% 
recall). However, the hand-built taxonomy, which 
was built without consulting a corpus, wasn?t a 
full-fledged ontology, for example, there was no 
label for the parent-child relation.  
3.2.2 Term Relationships  
We also carried out an evaluation experiment to 
determine if the relations being discovered by the 
machine were in keeping with human judgments. 
We focused here on an evaluation of pairs of 
knowledge sources. Our experiment examined the 
case where the system discovered a kind-of 
relation. Here each subject was first asked to read 
four newspaper articles from the TREC topic-230 
sub-collection. The articles were then kept 
accessible to the subject in a browser window for 
the subject to consult if needed in answering 
subsequent questions. The subject was asked to 
judge, based on the documents read, whether term 
X was a kind of term Y, term Y was a kind of term 
X, or neither; e.g., ?Is acid a kind of pollutant, or is 
pollutant a kind of acid, or neither??. The subject 
had one of three mutually exclusive choices; the 
first two choices were presented in randomized 
order. 
The subjects were 16 native speakers of English 
unconnected with the project. Each subject was 
given ten questions to answer in each of the 
experiments. For each set of ten questions, five 
were chosen at random from pairs of terms related 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 51
by (immediate) kind-of relations. The remaining 
five questions were chosen at random from pairs of 
terms between which the system found no relation 
whatsoever. 
 
 Human 
System kind-
of(A, B) 
not kind-
of(A,B) 
kind-of(A, B) 56 18 
not kind-
of(A,B) 
6 
 
74 
 
Table 3: Is X a kind-of Y? 
 
We first discuss inter-subject agreement. Three 
subjects given the same relation to judge agreed 
75% of the time, leading to a Kappa score of 0.72, 
indicating a good level of agreement. This means 
that subjects were able to reliably make judgments 
as to whether A is a kind of B in some document. 
The results for the 16 subjects are shown in 
Table 3. When the system is compared to the 
human as ground truth, this gives a Precision of 
.90, a Recall of .75, and an F-measure of .82. This 
performance is also significantly better than 
random assignment: with chi-square=74.29, with p 
< 0.0019. The substantial effect sizes of the chi-
square indicates a very solid result. There were 62 
decisions involving Subphrase Relations (with 44 
True Positives and 18 False Negatives), and 10 
decisions involving WordNet (with 12 True 
Positives). This shows that there is solid agreement 
between the human subjects and the system on the 
kind-of relations.  However, these 154 decisions 
involved only four newspaper articles, so clearly 
more data would be helpful.  
3.3 Automatic Evaluation 
While evaluation by humans is valuable, it is 
expensive to carry out, and this expense must be 
incurred each time one wants to do an evaluation. 
Automatic comparison of a machine-generated 
ontology against reference ontologies constructed 
by humans, e.g., (Zhang et al 1996) (Sekine et al 
1999) (Daude et al 2001), is therefore desirable, 
provided suitable reference ontologies are 
available. In this evaluation, the human-generated 
taxonomy for ProMed described in Section 3.2.1 
was used as a reference ontology, with its 
unlabeled parent-child relation treated as a kind-of 
link. However, the human ?ontology? was created 
without looking at a corpus, and was developed for 
use with a different set of goals in mind. Although 
this involves comparing ?apples? and ?oranges?, a 
comparison is nevertheless illustrative, and can in 
addition be useful when comparing mutiple 
ontologies created under similar conditions. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3:  Automatically Induced Fragment 
from ProMed 
To set aside the problem of differences in 
terminology involved in the comparison, we 
decided to restrict our attention to the set of terms 
TH (of cardinality 3025) in the human ontology 
(H), and have our system induce relations between 
them using the ProMed corpus. Relations were 
induced automatically in the machine ontology (M) 
for just 761 of those terms, yielding a set TH1.  The 
structure of  TH1 is shown in a fragment in Figure 
3.  Here A is a kind-of B if it is printed under B 
without a label; A is a part-of B if it is printed 
under B with a ?p? label. 
We then automatically computed, for each pair 
of terms t1 and t2 in TH1 that were linked distance 1 
apart in M, the distance between those terms in H. 
Likewise, we also computed, for each pair of terms 
t1 and t2 in TH1 distance 1 apart in H, the distance 
between those terms in M. 
The results of this comparison are as follows. 
The number of relations where the two ontologies 
agree exactly is 63 (i.e., the terms are distance 1 
apart in both ontologies). Since, given a set of 
terms, there are many different ways to construct 
an ontology, this is encouraging.   
The number of relations that our system found 
which were ?missed?, i.e., more than distance 1 
away, in H is 1203. Given the previous experiment 
where the human subjects agreed with the system's 
relations, these 1203 relations are likely to contain 
many that the human probably missed. For 
example, the relations in the machine ontology 
between ?eye? and ?farsightedness?, and ?medicine? 
                                                     
9  The chi-square for Subphrase Relations is 61.68, 
and the chi-square for WordNet is 56.73, with p < 0.001 
in all cases. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology52
4 Related Work and ?chiropractic medicine? are missed by H. This 
highlights a problem with human-generated 
ontologies: substantial errors of omission.  The existing approaches to ontology induction include those that start from structured data, 
merging ontologies or database schemas (Doan et 
al. 2002). Other approaches use natural language 
data, sometimes just by analyzing the corpus 
(Sanderson and Croft 1999), (Caraballo 1999) or 
by learning to expand WordNet with clusters of 
terms from a corpus, e.g., (Girju et al 2003). 
Information extraction approaches that infer 
labeled relations either require substantial hand-
created linguistic or domain knowledge, e.g., 
(Craven and Kumlien 1999) (Hull and Gomez 
1993), or require human-annotated training data 
with relation information for each domain (Craven 
et al 1998).  
The number of relations in H that our system 
missed (relations that were more than distance 1 
away in the system ontology), is 3493. However, 
of these 3493 relations, 2955 involved at least 1 
term that was not included in M, leaving 538 
relations that we could calculate the distance for in 
M. These 538 relations in H include relations 
between ?acid indigestion medicine? and ?maalox?, 
and ?alternative medicine? and ?acupuncture? (a 
majority of the misses involved relations between a 
disease and the name of a specific drug for it, 
which aren?t part-of or kind-of relations).  
 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
1 2 3 4 5 6 7 8 9 
D 
Relation-
Prec.(H,M,D)
Relation-
Prec.(M,H,D)
 
Many, though not all, domain-independent 
approaches (Evans et al 1991) (Grefenstette 1997) 
have restricted themselves to discovering term-
associations, rather than labeled relations. A 
notable exception is (Sanderson and Croft 1995), 
which (unlike our approach) assumes the existence 
of a query that was used to originally retrieve the 
documents (so that terms can be extracted from the 
query and then expanded to generate additional 
terms for the ontology). Their approach also is 
restricted to one method to discover relations, 
while we use several.  
Our approach is complementary to approaches 
aimed at automatically enhancing existing 
resources for a particular domain, e.g. (Moldovan 
et al 2000). Finally, the prior methods, while they 
often carry out evaluation, lack standard criteria for 
ontology evaluation. Although ontology evaluation 
remains challenging, we have discussed several 
evaluation methods in this paper. 
Figure 4: Relation Precision 
These observations lead to a metric for 
comparing one ontology with another one serving 
as a reference ontology. Given two ontologies A 
and B, define Relation Precision (A, B, D) as the 
proportion of the distance 1 relations in A that are 
at most a distance D apart in B. This measure can 
be plotted for different values of D. In Figure 4, we 
show the Relation Precision(H, M, D), and 
Relation Precision(M, H, D), for our machine 
ontology M and human ontology H. Both curves 
show Relation Precision(H, M, D) growing faster 
than Relation Precision(M, H, D), with 70% of the 
area being below the former curve and 54% being 
below the latter curve. The graph shows that while 
22% of distance 1 relations in M are at most 3 
apart in H (but keep in mind the errors of omission 
in H), 40% of distance 1 relations in H are at most 
3 apart in M10. 
5 Conclusion 
The evidence combination described above is 
based on transitivity and union. Since the above 
evaluations, we have been experimenting with an 
ad hoc weighted evidence combination scheme, 
based on each knowledge source expressing a 
strength for a posited relation. In future, we will 
also investigate using an initial seed ontology to 
provide a better ?backbone? for induction, and then 
using a spreading activation method to activate 
nodes related by existing knowledge sources to 
seed nodes. Corpus statistics can be used to weight 
the links. For example, based on (Caraballo 1999), 
each parent of a leaf node could be viewed as a 
cluster label for its children, with the weight of a 
parent-child link being determined based on how 
strongly the child is associated with the cluster.  
                                                     
10 The mean distance in H between terms that are 
distance 1 apart in M is 5.17, with a standard deviation 
of 2.12. The mean distance in M between terms which 
are distance 1 apart in H is 3.85, with a standard 
deviation of 1.69. 
The ontology induction methods described here 
can allow for considerable savings in time in 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 53
constructing ontologies. The evaluations we have 
carried out are suggestive, but many issues remain 
open. There are many unanswered questions about 
human-created reference ontologies, including lack 
of inter-annotator agreement studies. Indeed, 
experience shows that without guidelines for 
ontology construction, humans are prone to come 
up with very different ontologies for a domain. 
Comparing a machine-induced ontology against an 
ideal human reference ontology, were one to be 
available, is also fraught with problems. Our 
experience with using an implementation of the 
(Daude et al 2001) constraint relaxation algorithm 
for ontology comparison suggests that much work 
is needed on distance metrics which are not over-
sensitive to small differences in structure.  
Our interest, therefore, is focused more towards 
an extrinsic evaluation. PRONTO, which is due to 
be released in 2004, offers the opportunity to 
measure costs of ontology induction and post-
editing on a large-scale problem of value to the 
biology community. We also plan to measure the 
effectiveness of PRONTO in query expansion for 
information access to MEDLINE and protein 
databases. Finally, we will investigate more 
sophisticated evidence combination methods, and 
compare against other automatic methods for 
ontology induction. 
The ontology induction tools are available for 
free distribution for research purposes. 
References  
Abney, S. 1996. Partial parsing Via Finite-State 
Cascades. Proceedings of the ESSLLI '96 Robust 
Parsing Workshop. 
Caraballo, S. A. 1999. Automatic Construction of a 
hypernym-labeled noun hierarchy from text. In 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'1999), 120-122.  
Cohen, P. R., Chaudhri, V., Pease, A. and Schrag, 
R. 1999. Does Prior Knowledge Facilitate the 
Development of Knowledge-based Systems? The 
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99).  
Craven, M. and Kumlien, J. 1999. Constructing 
biological knowledge bases by extracting 
information from text sources. Proc Int Conf 
Intell Syst Mol Biol., 77-86.  
Craven, M., DiPasquo, D., Freitag, D., McCallum, 
A., Mitchell, T., Nigam, K., and Slattery, S.. 
1998. Learning to Extract Symbolic Knowledge 
from the World Wide Web. Proceedings of 
AAAI-98, 509-516. 
Daude, J., Padro, L. and Rigau, G. 2001 A 
Complete WN1.5 to WN1.6 Mapping. NAACL-
2001 Workshop on WordNet and Other Lexical 
Resources: Applications, Extension, and 
Customization, 83-88.  
Doan, A.,  Madhavan, J. , Domings, P. and Halevy, 
A. 2002. Learning to Map between Ontologies 
on the Semantic Web. WWW?2002. 
Dunning, T. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence,? 
Computational Linguistics, 19(1):61-74, 1993. 
Girju, R.,  Badulescu, A., and Moldovan, D. 2003. 
Learning Semantic Constraints for the Automatic 
Discovery of Part-Whole Relations. Proceedings 
of HLT?2003, Edmonton. 
Grefenstette, G. 1997. Explorations in Automatic 
Thesaurus Discovery.  Kluwer International 
Series in Engineering and Computer Science, 
Vol 278. 
Hearst, M. 1992. Automatic Acquisition of 
Hyponyms from Large Text Corpora. 
Proceedings of the fourteenth International 
Conference on Computational Linguistics, 
Nantes, France, July 1992.  
Hull, R. and Gomez, F. 1993. Inferring Heuristic 
Classification Hierarchies from Natural 
Language Input. Telematics and Informatics, 
9(3/4), pp. 265-281. 
IRS (Internal Revenue Service). 2001. Tax Guide 
2001. Publication 17. http://www.irs.gov/pub/irs-
pdf/p17.pdf 
Lawrie, D., Croft, W. B., and Rosenberg, A. 2001. 
Finding topic words for hierarchical 
summarization. 24th ACM Intl. Conf. on 
Research and Development in Information 
Retrieval, 349-357, 2001. 
Miller, G. (1995). WordNet: A Lexical Database 
for English. Communications Of the Association 
For Computing Machinery (CACM) 38, 39-41.  
Sanderson, M. and Croft, B. 1995. Deriving 
concept hierarchies from text. Proceedings of the 
22nd Annual Internationaql ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, 160-170. 
Sekine, S., Sudo, K. and Ogino, T. 1999. Statistical 
Matching of Two Ontologies. Proceedings of 
ACL SIGLEX99 Workshop: Standardizing 
Lexical Resources. 
Zhang, K., Wang, J. T. L. and Shasha, D.  1996. 
On the Editing Distance between Undirected 
Acyclic Graphs and Related Problems. 
International Journal of Foundations of 
Computer Science 7, 43-58. 
 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology54
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 152?160,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Name Matching Between Chinese and Roman Scripts:                       
Machine Complements Human 
 
Ken Samuel, Alan Rubenstein, Sherri Condon, and Alex Yeh 
The MITRE Corporation; M/S H305; 7515 Colshire Drive; McLean, Virginia 22102-7508 
samuel@mitre.org, rubenstein@mitre.org, scondon@mitre.org, and asy@mitre.org 
 
  
Abstract 
There are generally many ways to translite-
rate a name from one language script into 
another. The resulting ambiguity can make it 
very difficult to ?untransliterate? a name by 
reverse engineering the process. In this paper, 
we present a highly successful cross-script 
name matching system that we developed by 
combining the creativity of human intuition 
with the power of machine learning. Our sys-
tem determines whether a name in Roman 
script and a name in Chinese script match 
each other with an F-score of 96%. In addi-
tion, for name pairs that satisfy a computa-
tional test, the F-score is 98%. 
 
1 Introduction 
There are generally many ways to transliterate a 
person?s name from one language script into 
another. For example, writers have transliterated 
the Arabic name, ??????, into Roman characters 
in at least 13 ways, such as Al Choukri, Ash-
shukri, and al-Schoukri. This ambiguity can 
make it very difficult to ?untransliterate? a name 
by reverse engineering the process. 
We focused on a task that is related to transli-
teration. Cross-script name matching aims to de-
termine whether a given name part in Roman 
script matches a given name part in Chinese 
(Mandarin) script,1 where a name part is a single 
?word? in a person?s name (such as a surname), 
and two names match if one is a transliteration of 
the other.2 Cross-script name matching has many 
                                                 
1 In this paper, we often use the word ?Roman? to refer to 
?Roman script?, and similarly, ?Chinese? usually stands 
for ?Chinese script?. 
2 Sometimes a third script comes between the Roman and 
Chinese versions of the name. For example, a Roman 
name might be transliterated into Arabic, which is then 
transliterated into Chinese, or an Arabic name could be 
transliterated into Roman and Chinese independently. 
applications, such as identity matching, improv-
ing search engines, and aligning parallel corpora. 
We combine a) the creative power of human 
intuition, which can come up with clever ideas 
and b) the computational power of machine 
learning, which can analyze large quantities of 
data. Wan and Verspoor (1998) provided the 
human intuition by designing an algorithm to 
divide names into pieces that are just the right 
size for Roman-Chinese name matching (Section 
2.2.). Armed with Wan and Verspoor?s algo-
rithm, a machine learning approach analyzes 
hundreds of thousands of matched name pairs to 
build a Roman-Chinese name matching system 
(Section 3). 
Our experimental results are in Section 4. The 
system correctly determines whether a Roman 
name and a Chinese name match each other with 
F = 96.5%.3 And F = 97.6% for name pairs that 
satisfy the Perfect Alignment hypothesis condi-
tion, which is defined in Section 2.2. 
 
2 Related Work 
Wan and Verspoor?s (1998) work had a great 
impact on our research, and we explain how we 
use it in Section 2.2. In Section 2.1, we identify 
other related work. 
2.1 Chinese-English Name Matching 
Condon et al (2006) wrote a paper about the 
challenges of matching names across Roman and 
Chinese scripts. In Section 6 of their paper, they 
offered an overview of several papers related to 
Roman-Chinese name matching. (Cohen et al, 
2003; Gao et al, 2004;  Goto et al, 2003; Jung et 
al., 2000; Kang and Choi, 2000; Knight and 
Graehl, 1997; Kondrak, 2000; Kondrak and 
Dorr, 2004; Li et al, 2004; Meng et al, 2001; Oh 
                                                 
3 F stands for F-score, which is a popular evaluation metric. 
(Andrade et al, 2009) 
152
and Choi, 2006; Virga and Khudanpur, 2003; 
Wellner et al, 2005; Winkler, 2002) 
The Levenshtein algorithm is a popular way to 
compute string edit distance. (Levenshtein, 1966) 
It can quantify the similarity between two names. 
However, this algorithm does not work when the 
names are written in different scripts. So Free-
man et al (2006) developed a strategy for Ro-
man-Arabic  string matching that uses equiva-
lence classes of characters to normalize the 
names so that Levenshtein?s method can be ap-
plied.  Later, Mani et al (2006) transformed that 
system from Roman-Arabic to Roman-Chinese 
name matching and extended the Levenshtein 
approach, attaining F = 85.2%. Then when they 
trained a machine learning algorithm on the out-
put, the performance improved to F = 93.1% 
 Mani et al also tried applying a phonological 
alignment system (Kondrak, 2000) to the Ro-
man-Chinese name matching task, and they re-
ported an F-score of 91.2%. However, when they 
trained a machine learning approach on that sys-
tem?s output, the F-score was only 90.6%.  
It is important to recognize that it would be in-
appropriate to present a side-by-side comparison 
between Mani?s work and ours (F = 96.5%), be-
cause there are many differences, such as the 
data that was used for evaluation. 
2.2 Subsyllable Units 
Transliteration is usually based on the way 
names are pronounced.4 However, each character 
in a Roman name generally corresponds to a sin-
gle phoneme, while a Chinese character (CC) 
generally corresponds to a subsyllable unit 
(SSU). A phoneme is the smallest meaningful 
unit of sound, and a subsyllable unit is a se-
quence of one to three phonemes that conform to 
the following three constraints. (Wan and Vers-
poor, 1998) 
                                                 
4  Of course, there are exceptions. For example, when a 
name happens to be a word, sometimes that name is trans-
lated (rather than transliterated) into the other language. 
But our experimental results suggest that the exceptions 
are quite rare. 
(1) There is exactly one vowel phoneme.5 
(2) At most, one consonant phoneme may pre-
cede the vowel phoneme. 
(3) The vowel phoneme may be followed by, at 
most, one nasal phoneme.6 
Consider the example in Table 1. The name 
?Albertson? consists of eight phonemes in three 
syllables.7 The last syllable, SAHN, satisfies the 
definition of SSU, and the other two are split into 
smaller pieces, resulting in a total of five SSUs. 
There are also five CCs in the Chinese version,  
?????. We note that the fourth and sixth rows 
in the table show similarities in their pronuncia-
tions. For example, the first SSU, AE, sounds 
like the first CC, /a/. And, although the sounds 
are not always identical, such as BER and /pei/, 
Wan and Verspoor claimed that these SSU-CC 
correspondences can be generalized in the fol-
lowing way: 
Perfect Alignment (PA) hypothesis 
If a Roman name corresponds to a sequence of n 
SSUs, S1, S2, ..., Sn, and the Chinese form of that 
name is a sequence of n CCs, C1, C2, ..., Cn, then 
Ci matches Si for all 1 ? i ? n. 
In Section 4, we show that the PA hypothesis 
works very well. However, it is not uncommon 
to have more SSUs than CCs in a matching name 
pair, in which case, the PA hypothesis does not 
apply. Often this happens because an SSU is left 
out of the Chinese transliteration, perhaps be-
cause it is a sound that is not common in Chi-
nese. For example, suppose ?Carlberg? (KAA, 
R,L,BER,G) is transliterated as ???? . In 
this example, the SSU, R, does not corres-
pond to any of the CCs. We generalize this 
phenomenon with another hypothesis:  
SSUs Deletion (SSUD) hypothesis 
If a Roman name corresponds to a sequence of 
n+k  SSUs (k>0), S1, S2, ..., Sn+k, and the Chinese 
form of that name is a sequence of n CCs, C1, C2, 
..., Cn, then, for some set of k Si?s, if those SSUs 
are removed from the sequence of SSUs, then the 
PA hypothesis holds. 
And in the case where the number of CCs is 
greater than the number of SSUs, we make the 
                                                 
5 Wan and Verspoor treat the phoneme, /?r/, as in Albertson, 
as a vowel phoneme. 
6 The nasal phonemes are /n/ and /?/, as in ?nothing?. 
7 To represent phonemes, we use two different standards in 
this paper. The symbols between slashes (like /?r/) are in 
the IPA format (International Phonetic Association, 
1999). And the phonemes written in capital letters (like 
ER) are in the ARPABET format (Klatt, 1990). 
Roman Characters: Albertson 
Phonemes: AE,L,B,ER,T,S,AH,N 
Syllables: AEL,BERT,SAHN 
Subsyllable Units: AE,L,BER,T,SAHN 
Chinese: ????? 
Chinese Phonemes: /a/,/?r/,/pei/,/t
h?/,/su?/ 
Table 1. Subsyllable Units 
153
corresponding CCs Deletion (CCD) hypothesis. 
In the next section, we show how we utilize these 
hypotheses. 
 
3 Machine Learning 
We designed a machine learning algorithm to 
establish a mapping between SSUs and CCs. In 
Section 3.1, we show how our system can do 
Roman-Chinese name matching, and then we 
present the training procedure in Section 3.2. 
3.1 Application Mode 
Given a Roman-Chinese name pair, our system 
computes a match score, which is a number be-
tween 0 and 1 that is meant to represent the like-
lihood that two names match.  This is accom-
plished via the process presented in Figure 1. 
Starting in the upper-left node of the diagram 
with a Roman name and a Chinese name, the 
system determines how the Roman name should 
be pronounced by running it through the Festival 
system. (Black et al, 1999) Next, two algorithms 
designed by Wan and Verspoor (1998) join the 
phonemes to form syllables and divide the syl-
lables into SSUs.8 If the number of SSUs is equal 
to the number of characters in the Chinese 
name,9 we apply the PA hypothesis to align each 
SSU with a CC.  
The system computes a match score using a 
data structure called the SSU-CC matrix (subsyl-
lable unit ? Chinese character matrix), which has 
a nonnegative number for each SSU-CC pair, 
and this value should represent the strength of 
the correspondence between the SSU and the 
CC. Table 2 shows an example of an SSU-CC 
matrix. With this matrix, the name pair <Albert, 
????> receives a relatively high match score,  
because the SSUs in Albert are AE, L, BER, and 
T, and the numbers in the SSU-CC matrix for 
<AE,?>, <L,?>, <BER,?> and <T,?> are 2, 2, 
3, and 2, respectively.10 Alternatively, the system 
assigns a very low match score to <Albert,         
????>, because the values of <AE,?>, <L,?>, 
<BER,?>, and <T,?> are all 0. 
3.2 Training Mode 
To generate an SSU-CC matrix, we train our sys-
tem on a corpus of Roman-Chinese name pairs 
                                                 
8  This procedure passes through three separate modules, 
each of which introduces errors, so we would expect the 
system to suffer from compounding errors. However, the 
excellent evaluation results in Section 4 suggest  other-
wise. This may be because the system encounters the 
same kinds of errors during training that it sees in the ap-
plication mode, so perhaps it can learn to compensate for 
them. 
9 Section 3.3 discusses the procedure used when these num-
bers are not equal. 
10 The equation used to derive the match score from these 
values can be found in Section 5. 
 
Figure 2. Training Mode 
 
Figure 1. Application Mode 
 
A 
E 
B 
E 
R 
E 
H G 
K 
A 
A L 
L 
A 
H 
N 
L 
I 
Y 
N 
A 
H R 
S 
A 
H 
N T 
? 0 0 0 0 0 0 1 0 0 0 0 0 
? 0 0 0 0 0 0 0 1 0 0 0 0 
? 0 0 0 0 1 0 0 0 0 0 0 0 
? 0 0 1 0 0 0 0 0 0 0 0 0 
? 0 0 1 0 0 0 0 0 0 0 0 0 
? 0 0 0 0 0 0 0 0 1 0 0 0 
? 0 0 0 0 0 2 0 0 0 1 0 0 
? 0 0 0 0 0 0 0 0 0 0 1 0 
? 0 0 0 0 0 0 0 0 0 0 0 2 
? 0 3 0 0 0 0 0 0 0 0 0 0 
? 0 0 0 0 0 0 1 0 0 0 0 0 
? 0 0 0 1 0 0 0 0 0 0 0 0 
? 2 0 0 0 0 0 0 0 0 0 0 0 
Table 2. SSU-CC Matrix #1 
 
154
that match. Figure 2 shows a diagram of the 
training system. The procedure for transforming 
the Roman name into a sequence of SSUs is 
identical to that presented in Section 3.1. Then, if 
the number of SSUs is the same as the number of 
CCs,9 we apply the PA hypothesis to pair the 
SSUs with the CCs. For example, the third name 
pair in Table 3 has three SSU-CC pairs: <KAA,
?>, <R,?>, and <LIY,?>. So the system mod-
ifies the SSU-CC matrix by adding 1 to each cell 
that corresponds to one of these SSU-CC pairs. 
Training on the five name pairs in Table 3 pro-
duces the SSU-CC matrix in Table 2. 
3.3 Imperfect Alignment 
The system makes two passes through the train-
ing data. In the first pass, whenever the PA hypo-
thesis does not apply to a name pair (because the 
number of SSUs differs from the number of 
CCs), that name pair is skipped.  
Then, in the second pass, the system builds 
another SSU-CC matrix. The procedure for 
processing each name pair that satisfies the PA 
hypothesis?s condition is exactly the same as in 
the first pass (Section 3.2). But the other name 
pairs require the SSUD hypothesis or the CCD 
hypothesis to delete SSUs or CCs. For a given 
Roman-Chinese name pair:  
where D is the set of all deletion sets that make 
the PA hypothesis applicable. Note that the size 
of D grows exponentially as the difference be-
tween the number of SSUs and CCs grows. 
As an example, consider adding the name pair 
<Carlberg, ????> to the data in Table 3. Carl-
berg has five SSUs: KAA,R,L,BER,G, but ???-
? has only four CCs. So the PA hypothesis is not 
applicable, and the system ignores this name pair 
in the first pass. Table 2 shows the values in Ma-
trix #1 when it is completed. 
In the second pass, we must apply the SSUD 
hypothesis to <Carlberg, ????> by deleting 
one of the SSUs. There are five ways to do this, 
as shown in the five rows of Table 4. (For in-
stance, the last row represents the case where G 
is deleted ? the SSU-CC pairs are <KAA,?>, 
<R,?>, <L,?>, <BER,?>, and <G,?>.11) 
Each of the five options are evaluated using 
the values in Matrix #1 (Table 2) to produce the 
scores in the second column of Table 4. Then the 
                                                 
11 The ? represents a deleted SSU. We include a row and 
column named ? in Matrix #2 to record values for the 
cases in which the SSUs and CCs are deleted. 
For every d in D: 
Temporarily make the deletions in d. 
Evaluate the resulting name pair with Matrix #1. 
Scale the evaluation scores of the d?s to sum to 1. 
For every d in D: 
Temporarily make the deletions in d. 
For every SSU-CC pair, ssu-cc, in the result: 
Add d?s scaled score to cell [ssu,cc] in Matrix #2. 
Example # 1 2 3 4 5 
Roman 
Characters 
Albert Albertson Carly Elena Ellenberg 
Subsyllable 
Units 
AE,L,BER,T AE,L,BER,T,SAHN KAA,R,LIY EH,LAHN,NAH EH,LAHN,BER,G 
Chinese 
Characters 
???? ????? ??? ??? ???? 
Table 3. Training Data 
CCs Score Scaled Score 
????? 0.00 0.00 
????? 0.90 0.54 
????? 0.76 0.46 
????? 0.00 0.00 
????? 0.00 0.00 
Table 4. Subsyllable Unit Deletion 
 
 
? 
B 
E 
R G 
K 
A 
A L R ... 
?  0.00 0.00 0.00 0.46 0.54  
? 0.00 0.00 0.00 2.00 0.00 0.00  
? 0.00 0.00 0.00 0.00 2.54 1.46  
? 0.00 4.00 0.00 0.00 0.00 0.00  
? 0.00 0.00 2.00 0.00 0.00 0.00  
...        
Table 5. SSU-CC Matrix #2 
 
155
system scales the scores to sum to 1, as shown in 
the third column, and it uses those values as 
weights to determine how much impact each of 
the five options has on the second matrix. Table 
5 shows part of Matrix #2. 
In application mode, when the system encoun-
ters a name pair that does not satisfy the PA hy-
pothesis?s condition it tries all possible deletion 
sets and selects the one that produces the highest 
match score. 
3.4 Considering Context 
It might be easier to estimate the likelihood that 
an SSU-CC pair is a match by using information 
found in surrounding SSU-CC pairs, such as the 
SSU that follows a given SSU-CC pair. We do 
this by increasing the number of columns in the 
SSU-CC matrix to separate the examples based 
on the surrounding context. 
For example, in Table 2, we cannot determine 
whether LAHN should map to ? or ?. But the 
SSU that follows LAHN clears up the ambiguity, 
because when LAHN immediately precedes 
BER, it maps to  ?, but when it is followed by 
NAH, it corresponds to ?. Table 6 displays a 
portion of the SSU-CC matrix that accounts for 
the contextual information provided by the SSU 
that follows an SSU-CC pair. 
3.5 The Threshold 
Given an SSU-CC name pair, the system produc-
es a number between 0 and 1. But in order to 
evaluate the system in terms of precision, recall, 
and F-score, we need the system to return a yes 
(a match) or no (not a match) response. So we 
use a threshold value to separate those two cases.  
The threshold value can be manually selected 
by a human, but this is often difficult to do effec-
tively. So we developed the following automated 
approach to choose the threshold. After the train-
ing phase finishes developing Matrix #2, the sys-
tem processes the training data12 one more time. 
                                                 
12 We tried selecting the threshold with data that was not 
used in training, and we found no statistically significant 
improvement. 
But this time it runs in application mode (Section 
3.1), computing a match score for each training 
example. Then the system considers all possible 
ways to separate the yes and no responses with a 
threshold, selecting the threshold value that is the 
most effective on the training data. 
Building the SSU-CC matrices does not re-
quire any negative examples (name pairs that do 
not match). However, we do require negative 
examples in order to determine the threshold and 
to evaluate the system. Our technique for gene-
rating negative examples involves randomly 
rearranging the names in the data.13 
 
4 Evaluation of the System 
We ran several experiments to test our system 
under a variety of different conditions. After de-
scribing our data and experimental method, we 
present some of our most interesting experimen-
tal results. 
We used a set of nearly 500,000 Roman-
Chinese person name pairs collected from Xin-
hua News Agency newswire texts. (Huang, 
2005) Table 7 shows the distribution of the data 
based on alignment. Note that the PA hypothesis 
applies to more than 60% of the data. 
We used the popular 10-fold cross validation 
approach 14  to obtain ten different evaluation 
scores. For each experiment we present the aver-
age of these scores. 
Our system?s precision (P), recall (R), and F-
score (F) are: P = 98.19%, R = 94.83%, and F = 
96.48%. These scores are much better than we 
originally expected to see for the challenging 
task of Roman-Chinese name matching.  
Table 8 shows P, R, and F for subsets of the 
test data, organized by the number of SSUs mi-
                                                 
13 Unfortunately, there is no standard way to generate nega-
tive examples. 
14 The data is divided into ten subsets of approximately the 
same size, testing the system on each subset when trained 
on the other nine. 
 
LAHN 
(BER) 
LAHN 
(NAH) 
BER 
(G) 
BER 
(T) 
? 1 0 0 0 
? 0 0 1 2 
? 0 1 0 0 
Table 6. Considering Context 
 
Alignment % of Data 
#SSUs - #CCs ? 3 1.62% 
#SSUs - #CCs = 2 6.66% 
#SSUs - #CCs = 1 20.00% 
#SSUs - #CCs = 0 60.60% 
#SSUs - #CCs = -1 10.48% 
#SSUs - #CCs = -2 0.61% 
#SSUs - #CCs ? -3 0.02% 
Table 7. Statistics of the Data 
156
nus the number of CCs in the name pairs. The 
differences between scores in adjacent rows of 
each column are statistically significant.15  Per-
fectly aligned name pairs proved to be the ea-
siest, with F = 97.55%, but the system was also 
very successful on the examples with the number 
of SSUs and the number of CCs differing by one 
(F = 96.08% and F = 97.37%). These three cases 
account for more than 91% of the positive exam-
ples in our data set. (See Table 7.) 
4.1 Deletion Hypotheses 
We ran tests to determine whether the second 
pass through the training data (in which the 
SSUD and CCD hypotheses are applied) is effec-
tive. Table 9 shows the results on the complete 
set of test data, and all of the differences between 
the scores are statistically significant.  
The first row of Table 9 presents F when the 
system made only one pass through the training 
data. The second row?s experiments utilized the 
CCD hypothesis but ignored examples with more 
SSUs than CCs during training. For the third 
row, we used the SSUD hypothesis, but not the 
CCD hypothesis, and the last row corresponds to 
system runs that used all of the training exam-
ples. From these results, it is clear that both of 
the deletion hypotheses are useful, particularly 
the SSUD hypothesis. 
4.2 Context 
In Section 3.4, we suggested that contextual in-
formation might be useful. So we ran some tests, 
obtaining the results shown in Table 10. For the 
second row, we used no contextual information. 
Row 5 corresponds to the case where we gave 
the system access to the SSU immediately fol-
lowing the SSU-CC pair being analyzed. In row 
                                                 
15 We use the homoscedastic t test (?Student?s t Test?, 2009) 
to decide whether the difference between two results is 
statistically significant. 
6?s experiment, we used the SSU immediately 
preceding the SSU-CC pair under consideration, 
and row 7 corresponds to system runs that ac-
counted for both surrounding SSUs. 
We also tried simplifying the contextual in-
formation to boolean values that specify whether 
an SSU-CC pair is at a boundary of its name or 
not, and rows 1, 3, and 4 of Table 10 show those 
results. ?Left Border? is true if and only if the 
SSU-CC pair is at the beginning of its name, 
?Right Border? is true if and only if the SSU-CC 
pair is at the end of its name, and ?Both Borders? 
is true if and only if the SSU-CC pair is at the 
beginning or end of its name. All differences in 
the table are statistically significant, except for 
those between rows 2, 3, and 4. These results 
suggest that the right border provides no useful 
information, even if the left border is also in-
cluded in the SSU-CC matrix. But when the 
SSU-CC matrix only accounted for the left bor-
der, the F-score was significantly higher than the 
baseline. Providing more specific information in 
the form of SSUs actually made the scores go 
down significantly. 
4.3 Sparse Data 
We were initially surprised to discover that using 
the rich information in the surrounding SSUs 
made the results worse. The explanation for this 
is that adding contextual information increases 
the size of the SSU-CC matrix, and so several of 
the numbers in the matrix become smaller. (For 
example, compare the values in the ?BER? col-
umns in Table 2 and Table 6.) This means that 
the system might have been suffering from a 
sparse data problem, which is a situation where 
there are not enough training examples to distin-
guish correct answers from incorrect answers, 
and so incorrect answers can appear to be correct 
by random chance.  
There are two factors that can contribute to a 
sparse data problem. One is the amount of train-
ing data available ? as the quantity of training 
data increases, the sparse data problem becomes 
less severe. The other factor is the complexity of 
Alignment P R F 
#SSUs - #CCs ? 3 72.38% 94.02% 81.79% 
#SSUs - #CCs = 2 95.26% 92.67% 93.95% 
#SSUs - #CCs = 1 99.07% 93.27% 96.08% 
#SSUs - #CCs = 0 99.87% 95.33% 97.55% 
#SSUs - #CCs = -1 98.33% 96.42% 97.37% 
#SSUs - #CCs = -2 73.80% 94.98% 83.04% 
#SSUs - #CCs ? -3 7.54% 78.04% 13.71% 
Table 8. Varying Alignment of Name Pairs 
# Contextual Information F 
1 Left Border 96.48% 
2 No Context 96.25% 
3 Both Borders 96.24% 
4 Right Border 96.19% 
5 Next SSU 87.53% 
6 Previous SSU 85.89% 
7 Previous SSU and Next SSU 47.89% 
Table 10. Evaluation with Context 
Hypotheses F 
PA 75.25% 
PA & CCD 83.74% 
PA & SSUD 92.86% 
PA & CCD & SSUD 96.48% 
Table 9. Varying the Training Data 
 
157
the learned model ? as the model becomes more 
complex, the sparse data problem worsens. 
Our system?s model is the SSU-CC matrix, 
and a reasonable measure of the its complexity is 
the number of entries in the matrix. The second 
column of Table 11 shows the number of SSU-
CC pairs in training divided by the number of 
cells in the SSU-CC matrix. These ratios are 
quite low, suggesting that there is a sparse data 
problem. Even without using any context, there 
are nearly 8 cells for each SSU-CC pair, on aver-
age.16  
It might be more reasonable to ignore cells 
with extremely low values, since we can assume 
that these values are effectively zero. The third 
column of Table 11 only counts cells that have 
values above 10-7. The numbers in that column 
look better, as the ratio of cells to training pairs 
is better than 1:4 when no context is used. How-
ever, when using the previous SSU, there are still 
more cells than training pairs.  
Another standard way to test for sparse data is 
to compare the system?s results as a function of 
the quantity of training data. As the amount of 
training data increases, we expect the F-score to 
rise, until there is so much training data that the 
F-score is at its optimal value.17 Figure 3 shows 
the results of all of the context experiments that 
we ran, varying the amount of training data. 
(90% of the training data was used to get the F-
scores in Table 10.) The t test tells us that ?No 
Context? is the only curve that does not increase 
significantly on the right end. This suggests that 
all of the other curves might continue increasing 
if we used more training data. So even the ?Both 
SSUs? case could potentially achieve a competi-
tive score, given enough training examples. Also, 
                                                 
16 It is true that a name pair can have multiple SSU-CC 
pairs, but even if the average number of SSU-CC pairs per 
name pair is as high as 8 (and it is not), one training name 
pair per SSU-CC matrix cell is still insufficient. 
17 Note that this value may not be 100%, because there are 
factors that can make perfection difficult to achieve, such 
as errors in the data. 
more training data could produce higher scores 
than 96.48%. 
5 Summary 
We designed a system that achieved an F-score 
of 96.48%, and F = 97.55% on the 60.61% of the 
data that satisfies the PA hypothesis?s condition.  
Due to the paper length restriction, we can on-
ly provide short summaries of the other experi-
ments that that we ran. 
1) We experimentally compared six different 
equations for computing match scores and 
found that the best of them is an arithmetic 
or geometric average of Prob(SSU|CC) and 
Prob(CC|SSU).  
2) We attempted to make use of two simple 
handcrafted rules, but they caused the sys-
tem?s performance to drop significantly. 
3) We compared two approaches for automati-
cally computing the pronunciation of a Ro-
man name and found that using the Festival 
system (Black et al, 1999) alone is just as ef-
fective as using the CMU Pronunciation Dic-
tionary (CMUdict, 1997) supplemented by 
Festival. 
4) We tried computing the threshold value with 
data that was not used in training the system. 
However, this failed to improve the system?s 
performance significantly. 
 
6 Future Work 
There are so many things that we still want to do, 
including: 
1. modifying our system for the task of 
transliteration (Section 6.1),  
2. running fair comparisons between our 
work and related research, 
3. using Levenshtein?s algorithm (Levensh-
tein, 1966) to implement the SSUD and 
Contextual Info. All Cells  Cells > 10-7  
No Context 0.128 4.35 
Right Border 0.071 3.45 
Left Border 0.069 3.45 
Both Borders 0.040 3.13 
Next SSU 0.002 1.12 
Previous SSU 0.001 0.78 
Both SSUs far less far less 
Table 11. Num. SSU-CC Pairs  per Matrix Cell 
 
Figure 3. Testing for Sparse Data 
 
  
40%
50%
60%
70%
80%
90%
100%
10% 20% 30% 40% 50% 60% 70% 80% 90%
F
-S
c
o
re
Training Set Size (% of available data)
Left Border Next SSU
No Context Previous SSU
Right Border Both SSUs
Both Borders
158
CCD hypotheses, instead of exhaustively 
evaluating all possible deletion sets (Sec-
tion 3.3),18  
4. developing a standard methodology for 
creating negative examples,  
5. when using contextual information, split-
ting rows or columns of the SSU-CC 
matrix only when they are ambiguous 
according to a metric such as Informa-
tion Gain (Section 3.4),19 
6. combining our system with other Ro-
man-Chinese name matching systems in 
a voting structure (Van Halteren, Zavrel, 
and Daelemans, 1998), 
7. independently evaluating the modules 
that determine pronunciation, construct 
syllables, and separate subsyllable units 
(Section 3),  
8. converting phonemes into feature vectors 
(Aberdeen, 2006),  
9. modifying our methodology to apply it 
to other similar languages, such as Japa-
nese, Korean, Vietnamese, and Ha-
waiian.  
10. manually creating rules based on infor-
mation in the SSU-CC matrix, and  
11. utilizing graphemic information. 
6.1 Transliteration 
We would like to modify our system to enable 
it to transliterate a given Roman name into Chi-
nese in the following way. First, the system 
computes the SSUs as in Section 3.1. Then it 
produces a match score for every possible se-
quence of CCs that has the same length as the 
sequence of SSUs, returning all of the CC se-
quences with match scores that satisfy a prede-
termined threshold restriction. 
For example, in a preliminary experiment, 
given the Roman name Ellen, the matcher pro-
duced the transliterations below, with the match 
scores in parentheses.20 
 ? ?  (0.32) 
 ? ?  (0.14) 
 ? ?  (0.11)  
 ? ?  (0.05) 
                                                 
18 We thank a reviewer for suggesting this method of im-
proving efficiency. 
19 We thank a reviewer for this clever way to control the 
size of the SSU-CC matrix when context is considered. 
20 A manually-set threshold of 0.05 was used in this experi-
ment. 
Based on our data, the first and fourth results 
are true transliterations of Ellen, and the only 
true transliteration that failed to make the list is 
??. 
 
7 Conclusion 
There was a time when computational linguistics 
research rarely used machine learning. Research-
ers developed programs and then showed how 
they could successfully handle a few examples, 
knowing that their programs were unable to ge-
neralize much further. Then the language com-
munity became aware of the advantages of ma-
chine learning, and statistical systems almost 
completely took over the field. Researchers 
solved all kinds of problems by tapping into the 
computer?s power to process huge corpora of 
data. But eventually, the machine learning sys-
tems reached their limits. 
We believe that, in the future, the most suc-
cessful systems will be those developed by 
people cooperating with machines. Such systems 
can solve problems by combining the computer?s 
ability to process massive quantities of data with 
the human?s ability to intuitively come up with 
new ideas. 
Our system is a success story of human-
computer cooperation. The computer tirelessly 
processes hundreds of thousands of training ex-
amples to generate the SSU-CC matrix. But it 
cannot work at all without the insights of Wan 
and Verspoor. And together, they made a system 
that is successful more than 96% of the time. 
 
References 
Aberdeen, J. (2006) ?geometric-featurechart-jsa-
20060616.xls?. Unpublished. 
Andrade, Miguel. Smith, S. Paul. Cowlisha, Mike F. 
Gantner, Zeno. O?Brien, Philip. Farmbrough, Rich. 
et al ?F1 Score.? (2009) Wikipedia: The Free En-
cyclopedia.  http://en.wikipedia.org/wiki/F-score. 
Black, Alan W. Taylor, Paul. Caley, Richard. (1999) 
The Festival Speech Synthesis System: System Do-
cumentation. Centre for Speech Technology Re-
search (CSTR). The University of Edinburgh. 
http://www.cstr.ed.ac.uk/projects/festival/manual 
CMUdict. (1997) The CMU Pronouncing Dictionary. 
v0.6. The Carnegie Mellon Speech Group. 
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.  
Cohen, W. Ravikumar, P. Fienberg, S. (2003) ?A 
Comparison of String Distance Metrics for Name-
159
Matching Tasks.? Proceedings of the IJCAI-03 
Workshop on Information Integration on the Web. 
Eds. Kambhampati, S. Knoblock, C. 73-78. 
Condon, Sherri. Aberdeen, John. Albin, Matthew. 
Freeman, Andy. Mani, Inderjeet. Rubenstein, Alan. 
Sarver, Keri. Sexton, Mike. Yeh, Alex. (2006) 
?Multilingual Name Matching Mid-Year Status 
Report.? 
Condon, S. Freeman, A. Rubenstein, A. Yeh, A. 
(2006) ?Strategies for Chinese Name Matching.? 
Freeman, A. Condon, S. Ackermann, C. (2006) 
"Cross Linguistic Name Matching in English and 
Arabic: A ?One to Many Mapping? Extension of 
the Levenshtein Edit Distance Algorithm." Pro-
ceedings of NAACL/HLT. 
Gao, W. Wong, K. Lam, W. (2004) ?Phoneme-Based 
Transliteration of Foreign Names for OOV Prob-
lem.? Proceedings of the First International Joint 
Conference on Natural Language Processing. 
Goto, I. Kato, N. Uratani, N. Ehara, T. (2003) ?Trans-
literation Considering Context Information Based 
on the Maximum Entropy Method.? Proceedings 
of MT-Summit IX. 
Huang, Shudong. (2005) ?LDC2005T34: Chinese <-> 
English Named Entity Lists v 1.0.? Linguistics Da-
ta Consortium. Philadelphia, Pennsylvania.  ISBN 
#1-58563-368-2. http://www.ldc.upenn.edu/Cata 
log/CatalogEntry.jsp?catalogId=LDC2005T34. 
International Phonetic Association. (1999) Handbook 
of the International Phonetic Association : A Guide 
to the Use of the International Phonetic Alphabet. 
Cambridge University Press, UK. ISBN 
0521652367. http://www.cambridge.org/uk/cata 
logue/catalogue.asp?isbn=0521652367.  
Jung, S. Hong, S. Paek, E. (2000) ?An English to Ko-
rean Transliteration Model of Extended Markov 
Window.? Proceedings of COLING. 
Kang, B.J. Choi, K.S. (2000) ?Automatic Translitera-
tion and Back-Transliteration by Decision Tree 
Learning.? Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion. 
Klatt, D.H. (1990) ?Review of the ARPA Speech Un-
derstanding Project.? Readings in Speech Recogni-
tion. Morgan Kaufmann Publishers Inc. San Fran-
cisco, CA. ISBN 1-55860-124-4.  554-575. 
Knight, K. Graehl, J. (1997) ?Machine Translitera-
tion.? Proceedings of the Conference of the Asso-
ciation for Computational Linguistics (ACL). 
Kondrak, G. (2000) ?A New Algorithm for the 
Alignment of Phonetic Sequences.? Proceedings of 
the First Meeting of the North American Chapter 
of the Association for Computational Linguistics 
(NAACL). Seattle, Washington. 288-295. 
Kondrak, G. Dorr, B. (2004) ?Identification of Con-
fusable Drug Names: A New Approach and Evalu-
ation Methodology.? Proceedings of the Twentieth 
International Conference on Computational Lin-
guistics (COLING). 952-958. 
 Levenshtein, V.I. (1966) ?Binary Codes Capable of 
Correcting Deletions, Insertions and Reversals.? 
Sov. Phys. Dokl. 6. 707-710. 
Li, H. Zhang, M. Su, J. (2004) ?A Joint Source-
Channel Model for Machine Transliteration.? Pro-
ceedings of ACL 2004. 
Mani, Inderjeet. Yeh, Alexander. Condon, Sherri. 
(2006) "Machine Learning from String Edit Dis-
tance and Phonological Similarity." 
Meng, H. Lo, W. Chen, B. Tang, T. (2001) ?Generat-
ing Phonetic Cognates to Handle Named Entities in 
English-Chinese Cross-Language Spoken Docu-
ment Retrieval.? Proceedings of ASRU. 
Oh, Jong-Hoon. Choi, Key-Sun. (2006) ?An Ensem-
ble of Transliteration Models for Information Re-
trieval.? Information Processing & Management. 
42(4). 980-1002. 
 ?Student?s t Test.? (2009) Wikipedia: The Free En-
cyclopedia. http://en.wikipedia.org/wiki/T_test# 
Equal_sample_sizes.2C_equal_variance. 
Van Halteren, H., Zavrel, J. Daelemans, W. (1998) 
?Improving Data Driven Word-Class Tagging by 
System Combination.? Proceedings of the 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Con-
ference on Computational Linguistics. Montr?al, 
Qu?bec, Canada. 491-497. 
Virga, P. Khudanpur, S. (2003) ?Transliteration of 
Proper Names in Cross-Lingual Information Re-
trieval.? Proceedings of the ACL Workshop on 
Multi-lingual Named Entity Recognition. 
Wan, Stephen. Verspoor, Cornelia Maria. (1998). 
"Automatic English-Chinese Name Transliteration 
for Development of Multilingual Resources." Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics. Montr?al, 
Qu?bec, Canada.                     
Wellner, B. Castano, J. Pustejovsky, J. (2005) ?Adap-
tive String Similarity Metrics for Biomedical Ref-
erence Resolution.? Proceedings of the ACL-ISMB 
Workshop on Linking Biological Literature, Ontol-
ogies, and Databases: Mining Biological Seman-
tics. 9-16. http://www.cs.brandeis.edu/~wellner/ 
pubs/Wellner-StringSim-BioLINK.pdf. 
Winkler, W. ?Methods for Record Linkage and Baye-
sian Networks.? (2002) Proceedings of the Section 
on Survey Research Methods, American Statistical 
Association. http://www.census.gov/srd/www/ 
byyear.html. 
160
