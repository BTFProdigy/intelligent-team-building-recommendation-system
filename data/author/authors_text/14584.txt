Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79?88,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Scaling Textual Inference to the Web
Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195, USA
stef,etzioni,weld@cs.washington.edu
Abstract
Most Web-based Q/A systems work by find-
ing pages that contain an explicit answer to
a question. These systems are helpless if the
answer has to be inferred from multiple sen-
tences, possibly on different pages. To solve
this problem, we introduce the HOLMES sys-
tem, which utilizes textual inference (TI) over
tuples extracted from text.
Whereas previous work on TI (e.g., the lit-
erature on textual entailment) has been ap-
plied to paragraph-sized texts, HOLMES uti-
lizes knowledge-based model construction to
scale TI to a corpus of 117 million Web pages.
Given only a few minutes, HOLMES doubles
recall for example queries in three disparate
domains (geography, business, and nutrition).
Importantly, HOLMES?s runtime is linear in
the size of its input corpus due to a surprising
property of many textual relations in the Web
corpus?they are ?approximately? functional
in a well-defined sense.
1 Introduction and Motivation
Numerous researchers have identified the Web as
a rich source of answers to factual questions, e.g.,
(Kwok et al, 2001; Brill et al, 2002), but often the
desired information is not stated explicitly even in a
textual corpus as massive as the Web. Consider the
question ?What vegetables help prevent osteoporo-
sis?? Since there is likely no sentence on the Web
directly stating ?Kale prevents osteoporosis?, a sys-
tem must infer that kale is an answer by combining
facts from multiple sentences, possibly from differ-
ent pages, which justify that conclusion: i.e., that
kale is a vegetable, kale contains calcium, and cal-
cium helps prevent osteoporosis.
Figure 1: The architecture of HOLMES.
Textual Inference (TI) methods have advanced in
recent years. For example, textual entailment tech-
niques aim to determine whether one textual frag-
ment (the hypothesis) follows from another (the text)
(Dagan et al, 2005). While most TI researchers have
focused on high-quality inferences from a small
source text, we seek to utilize sizable chunks of the
Web corpus as our source text. In order to do this,
we must confront two major challenges. The first is
uncertainty: TI is an imperfect process, particularly
when applied to the Web corpus, hence probabilistic
methods help to assess the confidence in inferences.
The second challenge is scalability: how does infer-
ence time scale given increasingly large corpora as
input?
1.1 HOLMES: A Scalable TI System
This paper describes HOLMES, an implemented sys-
tem, which addresses both challenges by carrying
out scalable, probabilistic inference over ground
assertions extracted from the Web. The input to
HOLMES is a conjunctive query, a set of inference
rules expressed as Horn clauses, and large sets of
ground assertions extracted from theWeb, WordNet,
and other knowledge bases. As shown in Figure 1,
HOLMES chains backward from the query, using the
inference rules to construct a forest of proof trees
from the ground assertions. This forest is converted
79
into a Markov network (a form of Knowledge-
Based Model Construction (KBMC) (Wellman et
al., 1992)) and evaluated using approximate prob-
abilistic inference. HOLMES operates in an anytime
fashion ? if desired it can keep iterating: search-
ing for more proofs, and elaborating the Markov net-
work.
HOLMES makes some important simplifying as-
sumptions. Specifically, we use simple ground
tuples to represent extracted assertions (e.g.,
contains(kale, calcium)). Syntactic prob-
lems (e.g., anaphora, relative clauses) and seman-
tic challenges (e.g., quantification, counterfactuals,
temporal qualification) are delegated to the extrac-
tion system or simply ignored. This paper focuses
on scalability for this subset of the TI task.
1.2 Summary of Experimental Results
We tested HOLMES on 183 million distinct ground
assertions extracted from the Web by the TEX-
TRUNNER system (Banko et al, 2007), coupled
with 159 thousand ground assertions from Word-
Net (Miller et al, 1990), and a compact set of hand-
coded inference rules. Given a total of 55 to 145
seconds, HOLMES was able to produce high-quality
inferences that doubled the number of answers to
example queries in three disparate domains: geog-
raphy, business, and nutrition.
We also evaluated how the speed of HOLMES
scaled with the size of its input corpus. In the
general case, logical inference over a Horn theory
(needed in order to produce the probabilistic net-
work) is polynomial in the number of ground asser-
tions, and hence in the size of the textual corpus.1
Unfortunately, this is prohibitive, since even low-
order polynomial growth is fatal on a 117 million-
page corpus, let alne the full Web.
1.3 Why HOLMES Scales Linearly
Fortunately, the Web?s long tail works in our favor.
The relations we extract from text are approximately
pseudo-functional (APF), as we formalize in Sec-
tion 3, and this property leads to runtime that scales
linearly with the corpus. To see the underlying in-
tuition, consider the APF relation denoted by the
phrase ?is married to;? most of the time it maps a
person?s name to a small number of spousal names
1In fact, it is P-complete ? as hard as any polynomial-time
problem.
so this relation is APF. Section 3 shows why this
APF property ensures linear scaling, and Section 4
demonstrates linear scaling in practice.
2 An Overview of HOLMES
HOLMES is a system designed to answer complex
queries over large, noisy knowledge bases. As a mo-
tivating example, we consider the question ?What
vegetables help prevent osteoporosis?? As of this
writing, Google has no pages explicitly stating ?kale
helps prevent osteoporosis?, making it challenging
to return ?kale? as an answer. However, there are
numerous web pages stating that ?kale is high in cal-
cium? and others declaring that ?calcium helps pre-
vent osteoporosis?. If we could combine those facts
we could easily infer that ?kale? is an answer to the
question ?What vegetables help prevent osteoporo-
sis?? HOLMES was designed to make such infer-
ences while accounting for uncertainty in the pro-
cess.
Given a query, expressed as a conjunctive
Datalog rule, HOLMES generates a probabilistic
model using knowledge-based model construction
(KBMC) (Wellman et al, 1992). Specifically,
HOLMES utilizes fast, logical inference to find the
subset of ground assertions and inference rules that
may influence the answers to the query ? enabling
the construction of a small and focused Markov net-
work. Since this graphical model is much smaller
than one incorporating all ground assertions, prob-
abilistic inference will be much faster than if naive
compilation were used.
Figure 1 summarizes the operation of HOLMES.
As with many theorem provers or KBMC systems,
HOLMES takes three inputs:
1. A set of knowledge bases ? databases of
ground relational assertions, each with an
estimate of its probability, which can be
generated by TextRunner (Banko et al,
2007) or Kylin (Wu and Weld, 2007). In
our example, we would extract the as-
sertions IsHighIn(kale, calcium) and
Prevents(calcium, osteoporosis) from
those sentences.
2. A domain theory ? A set of probabilis-
tic inference rules written as Markov logic
Horn clauses, which can be used to de-
rive new assertions. The weight associ-
ated with each clause specifies its reliability.
80
kaleis high incalcium(TextRunner : 0.39)kaleis high inmagnesium(TextRunner : 0.39) magnesiumhelps preventosteoporosis(TextRunner : 0.39) calciumhelps preventosteoporosis(TextRunner : 0.68) broccoliis high incalcium(TextRunner : 0.39)
kalehelps preventosteoporosis(Inferred : 0.88) broccolihelps preventosteoporosis(Inferred : 0.49)kaleIS-Avegetable(WordNet : 0.9) broccoliIS-Avegetable(WordNet : 0.9)Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in
kale matches the query(Inferred : 0.91) broccoli matches the query(Inferred : 0.58)Query Result Query Result
Figure 2: Partial proof ?tree? (DAG) for the query ?What
vegetables help prevent osteoporosis?? Rectangles de-
pict ground assertions from a knowledge base, rounded
boxes are inferred assertions, and shaded squared repre-
sent the application of inference rules. HOLMES converts
this DAG into a Markov network in order to estimate the
probability of each node.
In Section 2.3 we identify several domain-
independent rules, but a user may (optionally)
specify additional, domain-specific rules if de-
sired. In our example, we assume we are given
the domain-specific rule: Prevents(X,Z) :-
IsHighIn(X,Y) ? Prevents(Y,Z)
3. A conjunctive query is specified as a Datalog
rule. For example, the question ?What vegeta-
bles help prevent osteoporosis?? could be writ-
ten as: query(X) :- IS-A(X,Vegetable)
? Prevents(X,osteoporosis)
and returns a set of answers to the query, each with
an associated probability.
2.1 Basic Operation
To find these answers and their associated proba-
bilities, HOLMES first finds all ground assertions in
the knowledge bases that are potentially relevant to
the query. This is efficiently done using the infer-
ence rules to chain backwards from the query. Note
that the generated candidate answers, themselves,
are less important than the associated proof trees.
Furthermore, since HOLMES uses these ?trees? (ac-
tually, DAGs) to generate a probabilistic graphical
model, HOLMES seeks to find as many proof trees
as possible for each query result ? each may influ-
ence the final belief in that result. Figure 2 shows a
partial proof tree for our example query.
To handle uncertainty, HOLMES now constructs a
ground Markov network from the proof trees and the
Markov-logic-encoded inference rules. Markov net-
works (Pearl, 1988) model the joint distribution of a
set of variables by creating an undirected graph with
one node for each random variable, and represent-
ing dependencies between variables with cliques in
the graph. Each clique has a corresponding poten-
tial function ?k, which returns a non-negative value
based on the state of variables in the clique. The
probability of a state, x, is given by
P (x) =
1
Z
?
?k(x{k})
where the partition functionZ is a normalizing term,
and x{k} denotes the state of all the variables in
clique k.
HOLMES converts the proof trees into a Markov
network in a manner pioneered by the Markov Logic
framework of Richardson and Domingos (2006). A
Boolean variable is created to represent the truth of
each assertion in the proof forest. Next, HOLMES
adds edges to the Markov network to create a clique
corresponding to each application of an inference
rule in the proof forest.
Following the Markov Logic framework, the po-
tential function of a clique has form ?(x) = ew if all
member nodes are true (w denotes the weight of the
inference rule), and ?(x) = 1 otherwise. The proba-
bilities of leaf nodes are derived from the underlying
knowledge base,2 and inferred nodes are biased with
an exponential prior.
Finally, HOLMES computes the approximate
probability of each answer by running a variant
of loopy belief propagation (Pearl, 1988) over the
Markov network. In our experience this method
performs well on networks derived from our Horn
clause proof forest, but one could use Monte Carlo
techniques or even exact methods if desired.
Note that this architecture allows HOLMES to
combine information from multiple web pages to in-
fer assertions not explicitly seen in the textual cor-
pus. Because this inference is done using a Markov
network, it correctly handles uncertain extractions
and probabilistic dependencies. By using KBMC to
create a custom, focused network for each query, the
2In our experiments, ground assertions from WordNet get
a uniformly high probability of correctness (0.9), but those ex-
tracted from the Web are assigned probabilities derived from
redundancy statistics, following the intuition that frequently ex-
tracted facts are more likely to be true (Etzioni et al, 2005).
81
amount of probabilistic inference is reduced to man-
ageable proportions.
2.2 Anytime, Incremental Expansion
Because exact probabilistic inference is #P-
complete, HOLMES uses approximate methods, but
even these techniques have problems if the Markov
network gets too large. As a result, HOLMES creates
the network incrementally. After the first proof trees
are generated, HOLMES creates the model and per-
forms approximate probabilistic inference. If more
time is available then HOLMES searches for addi-
tional proof trees and updates the network (Fig-
ure 1). This incremental process allows HOLMES
to return initial results (with preliminary probability
estimates) as soon as they are discovered.
For efficiency, HOLMES exploits standard Data-
log optimizations (e.g., it only expands proofs of re-
cently added nodes and it uses an approximation to
magic sets (Ullman, 1989), rather than simple back-
wards chaining). For tractability, we also allow the
user to limit the number of transitive inference steps
for any inference rule.
HOLMES also includes a few enhancements for
dealing with information extracted from natural lan-
guage. For example, HOLMES?s inference rules sup-
port substring/regex matching of ground assertions,
to accommodate simple variations in text. HOLMES
also can be restricted to only operate over proper
nouns, which is useful for queries involving named
entities.
2.3 Markov Logic Inference Rules
HOLMES is given the following set of six domain-
independent rules, which are similar to the up-
ward monotone rules introduced by (MacCartney
and Manning, 2007).
1. Observed relations are likely to be true:
R(X,Y) :- ObservedInCorpus(X, R, Y)
2. Synonym substitution preserves meaning:
RTR(X?,Y) :- RTR(X,Y) ? Synonym(X, X?)
3. RTR(X,Y?) :- RTR(X,Y) ? Synonym(Y, Y?)
4. Generalizations preserve meaning:
RTR(X?,Y) :- RTR(X,Y) ? IS-A(X, X?)
5. RTR(X,Y?) :- RTR(X,Y) ? IS-A(Y, Y?)
6. Transitivity of Part Meronyms:
RTR(X,Y?) :- RTR(X,Y) ? Part-Of(Y, Y?)
where RTR matches ?* in? (e.g., ?born in?).
For example, if Q(X):-BornIn(X,?France?),
and we know from WordNet that Paris is in
France, then by inference rule 6, we know that
BornIn(X,?Paris?) will yield valid results for
Q(X). Although all of these rules contain at most
two relations in the body, HOLMES allows an
arbitrary number of relations in the query and rule
bodies. However, we have found that even simple
rules can dramatically improve some queries.
We set the rule weights to capture the intuition
that deeper inferences decrease the likelihood (as
there are more chances to make mistakes), whereas
additional, independent proof trees increase the
likelihood (as there is more supporting evidence).
Specifically, in our experiments we set the prior on
inferred facts to -0.75, the weight on rule 1 to 1.5,
and the weights on all other rules to 0.6.
At present, we define these weights manually, but
we expect to learn the parameter values in the future.
3 Scaling Inference to the Web
If TI is applied to a corpus containing hundreds of
millions or even billions of pages, its run time has to
be at most linear in the size of the corpus. This sec-
tion shows that under some reasonable assumptions
inference does scale linearly.
We start our analysis with two simplifications.
First, we assume that the number of distinct, ground
assertions in the KBs, |A|, grows at most linearly
with the size of the textual corpus. This is cer-
tainly true for assertions extracted by TextRunner
and Kylin, and follows from our exclusion of texts
with complex quantified sentences. Our analysis
now proceeds to consider scaling with respect to |A|
for a fixed query and set of inference rules.
Our second assumption is that the size of every
proof tree is bounded by some constant, m. This
is a strong assumption and one that depends on the
precise set of inference rules and pattern of ground
assertions. However, it holds in our experience, and
if necessary could be enforced by terminating the
search for proof trees at a certain depth, e.g., log(m).
HOLMES?s knowledge-based model construction
has two parts: construction of the proof forest and
conversion of the forest into a Markov network.
Since the Markov network is essentially isomorphic
to the proof forest, the conversion will be O(|A|) if
the forest is linear in size, which is ensured if the
time to construct the proof trees isO(|A|). We show
82
this in the remainder of this section.
Recall that HOLMES requires inference rules to
be function-free Horn clauses. While this limits ex-
pressivity to some degree, it provides a huge speed
benefit ? logical inference over Horn clauses can
be done in polynomial time, whereas general propo-
sitional inference (i.e., from grounded first-order
rules) is NP-complete.
Alas, even low-order polynomial blowup is un-
acceptable when the textual corpus reaches Web
scale; we seek linear growth. Intuitively, there are
two places where polynomial expansion could cause
trouble. First, the number of different types of proofs
(i.e., first order proofs) could grow too quickly, and
secondly, a given type of proof tree might apply
to too many ground assertions (?tuples? in database
lingo). We treat these issues in turn.
Under our assumptions, each proof tree can be
represented as an expression in relational algebra
with at most m equijoins (Ullman, 1989),3 each
stemming from the application of an inference rule.
Since the number of rules is fixed, as is m, there are
a constant number of possible first-order proof trees.
The bigger concern is that any one of these first-
order trees might result in a polynomial number of
ground trees; if so, the size of the ground forest
(and corresponding Markov network) could grow
too quickly. In fact, polynomial growth is a common
phenomena in database query evaluation. Luckily,
most relations in the Web corpus behave more fa-
vorably. We introduce a property of relations that
ensures m-way joins, and therefore all proof trees
up to size m, can be computed in O(|A|) time.
The intuition is that most relations derived from
large corpora have a ?heavy-tailed? distribution,
wherein a few objects appear many times in a rela-
tion, but most appear only once or twice, thus joins
involving rare objects lead to a small number of re-
sults, and so the main limitation on scalability is
common objects. We now prove that if these com-
mon objects account for a small enough fraction of
the relation, then joins will still scale linearly. We
focus on binary relations, but these results can eas-
ily be extended to relations of larger arity.
3Note that an inference rule of the form H(X) :-
R1(X,Y),R2(Y,Z) is equivalent to the algebraic expression
piX(R1 ./ R2). First a join is performed between R1 and R2
testing for equality between values of Y ; then a projection elim-
inates all columns besides X .
Definition 1 A relation, R = {(xi, yi)} ? X ?
Y , is pseudo-functional (PF) in x with degree k, if
?x ? X : |{y|(x, y) ? R}| ? k. When the precise
variable and degree is irrelevant to discussion, we
simply say ?R is PF.?
An m-way equijoin over relations that are PF in
the join variables will have at most km ? |R| results.
Since km is constant for a given join and |R| scales
linearly in the size of the textual corpus, proof tree
construction over PF relations also scales linearly.
However, due to their heavy-tailed distributions,
most relations extracted from theWeb fit the pseudo-
functional definition in most, but not all values of
X . Fortunately, it turns out that in most cases these
?bad? values ofX are rare and hence don?t influence
the join size significantly. We formalize this intu-
ition by defining a class of approximately pseudo-
functional (APF) relations and proving that joining
two APF relations produces at most a linear number
of results.
Definition 2 A relation, R, is approximately
pseudo-functional (APF) in x with degree k, if X
can be partitioned into two sets XG and XB such
that for all x ? XG R is PF with degree k and?
x?XB
|{y|(x, y) ? R}| ? k ? log(|R|)
Theorem 1. If relation R1 is APF in y with de-
gree k1 and R2 is APF in y with degree k2 then
the relation Q = R1 ./ R2 has size at most
O(max(|R1|, |R2|)).
Proof. Since R1 and R2 are APF, we know that
Y can be partitioned into four groups: YBB =
YB1
?
YB2, YBG = YB1
?
YG2, YGB = YG1
?
YB2,
YGG = YG1
?
YG2.4 We can show that each group
leads to at most O(|A|) entries in Q. For y ? YBB
there are at most k1 ? k2 ? log(|R1|) ? log(|R2|) en-
tries in Q. The y ? YGB and y ? YBG lead to at
most k1 ? k2 ? log(|R2|) and k1 ? k2 ? log(|R1|)
entries, respectively. For y ? YGG there are at
most k1 ? k2 ? max(|R1|, |R2|). Summing the re-
sults from the four partitions, we see that |Q| is
O(max(|R1|, |R2|)), thus it is O(|A|).
This theorem and proof can easily be extended to
4YBB are the ?doubly bad? values of y that violate the PF
definition for both relations, YGG are the values that do not vio-
late the PF definition for either relation, and YBG and YGB are
the values that violate it in only R1 or R2, resp.
83
an m-way equijoin, as long as each relation is APF
in all arguments that are being joined.
Theorem 2. IfQ is the relation obtained by an equi-
join over m relationsR1..m, each having size at most
O(|A|), and if all R1..m are APF in all arguments
that they are joined in with degree at most kmax, and
if
?
1?i?m
log(|Ri|) ? |A|, then |Q| is O(|A|).
The inequality in Theorem 2 relates the sizes of
the relations (|R|), the join (m) and the number of
ground assertions (|A|). However, in many cases we
are interested in much smaller values of m than the
inequality enables. We can relax the APF definition
to allow a broader, but still scalable, class ofm-way-
APF relations.
Corollary 3. If Q is the relation obtained by an m-
way join, and if each participating relation is APF
in their joined variables with a bound of ki ? m
?
|Ri|
instead of ki ? log(|Ri|), then the join is O(|A|).
The final step in our scaling argument concerns
probabilistic inference, which is #P-Complete if per-
formed exactly. This is addressed in two ways. First,
HOLMES uses approximate methods, e.g., loopy be-
lief propagation, which avoids the cost of exact in-
ference ? at the cost of reduced precision. Sec-
ondly, at a practical level, HOLMES?s incremental
construction of the graphical model (Figure 1) al-
lows it to bound the size of the network by terminat-
ing the search for additional proofs.
4 Experimental Results
This section reports on measurements that confirm
that linear scaling with |A| occurs in practice, and
that HOLMES?s inference is not only scalable but
also improves precision/recall on sample queries in
a diverse set of domains. After describing the exper-
imental domains and queries, Section 4.2 reports on
the boost to the area under the precision/recall curve
for a set of example queries in three domains: ge-
ography, business, and nutrition. Section 4.3 then
shows that APF relations are very common in the
Web corpus, and finally Section 4.4 demonstrates
empirically that HOLMES?s inference time scales
linearly with the number of pages in the corpus.
4.1 Experimental Setup
HOLMES utilized two knowledge bases in these ex-
periments: TEXTRUNNER and WordNet. TEX-
TRUNNER contains approximately 183 million dis-
tinct ground assertions extracted from over 117 mil-
lion web pages, and WordNet contains 159 thousand
manually created IS-A, Part-Of, and Synonym asser-
tions.
In all queries, HOLMES utilizes the domain-
independent inference rules described in Sec-
tion 2.3. HOLMES additionally makes use of two
domain-specific inference rules in the Nutrition
domain, to demonstrate the benefits of including
domain-specific information. Estimating the preci-
sion and relative recall of HOLMES requires exten-
sive and careful manual tagging of HOLMES output.
To make this feasible, we restricted ourselves to a
set of twenty queries in three domains, but made the
domains diverse to illustrate the broad scope of the
system.
We now describe each domain briefly.
Geography: the query issued is: ?Who was born in
one of the following countries?? More formally,
Q(X) :- BornIn(X,{country}) where {country}
is bound to each of the following nine countries
in turn {France, Germany, China, Thailand, Kenya,
Morocco, Peru, Columbia, Guatemala}, yielding a
total of nine queries.
Because Web text often refers to a person?s
birth city rather than birth country, this query il-
lustrates how combining an ground assertion (e.g.,
BornIn(Alberto Fujimori, Lima)) with back-
ground knowledge (e.g., LocatedIn(Lima, Peru))
enables the system to draw new conclusions (e.g.,
BornIn(Alberto Fujimori, Peru)).
Business: we issued the following two queries.
1) Which companies are acquiring software com-
panies? Formally, Q(X) :- Acquired(X, Y)
? Develops(Y, ?software?) This query tests
HOLMES?s ability to scalably join a large number of
assertions from multiple pages.
2) Which companies are headquartered in the
USA? Q(X) :- HeadquarteredIn(X, ?USA?)
? IS-A(X, ?company?)
Answering this query comprehensively requires
HOLMES to combine a join (over the relations Head-
quarteredIn and IS-A) with transitive inference on
PartOf (e.g., Seattle is PartOf Washington which is
PartOf the USA) and on IS-A (e.g., Microsoft IS-A
software company which IS-A company). The IS-
A assertions came from both TEXTRUNNER (using
patterns from (Hearst, 1992)) and WordNet.
84
0
0.2
0.4
0.6
0.8
1
0 1000 2000 3000 4000 5000Estimated Recall
Precis
ion
BaselineHolmes Increase in AuC
Figure 3: PR Curve for BornIn(X, {country}). Inference
boosts the Area under the PR Curve (AuC) by 102 %.
Domain Increase Total Inference
in AuC Time
Geography +102% 55 s
Business +2,643% 145 s
Nutrition +5,595% 64 s
Table 1: Improvement in the AuC of HOLMES over the
BASELINE and total inference time taken by HOLMES.
Results are summed over all queries in the geography,
business, and nutrition domains. Inference time mea-
sured on unoptimized prototype.
Nutrition: the nine queries issued are instances
of ?What foods prevent disease?? Where a food is
a member of one of the classes: fruit, vegetable, or
grain, and a disease is one of: anemia, scurvy, or
osteoporosis. More formally, Q(X, {disease}) :-
Prevents(X, {disease}) ? IS-A(X, {food})
Our experiments in the nutrition domain utilized
two domain-specific inference rules in addition to
the ones presented in Section 2.3:
Prevents(X,Y):-HighIn(X,Z) ? Prevents(Z,Y)
Prevents(X,Y):-Contains(X,Z) ? Prevents(Z,Y)
4.2 Effect of Inference on Recall
To measure the cost and benefit of HOLMES?s in-
ference we need to define a baseline for compar-
ison. Answering the conjunctive queries in the
business and nutrition domains requires computing
joins, which TEXTRUNNER does not do. Thus, we
defined a baseline system, BASELINE, which has
access to the underlying Knowledge Bases (KBs)
(TEXTRUNNER and WordNet), and the ability to
compute joins using information explicitly stated in
either KB, but does not have the ability to infer new
assertions.
We compared HOLMES with BASELINE in all
three domains. Figure 3 depicts the combined pre-
cision/relative recall curves for the nine Geography
queries. HOLMES yields substantially higher re-
call (the shaded region) at modestly lower preci-
sion, doubling the area under the precision/recall
curve (AuC). The other precision/recall curves also
showed a slight drop in precision for substantial
gains in recall. Table 1 summarizes the results, along
with the total runtime needed for inference. Because
relations in the business domain are much larger
than in the other domains (i.e., 100x ground asser-
tions), inference is slower in this domain.
We note that inference is particularly helpful with
rarely mentioned instances. However, inference can
lead to errors when the proof tree contains joins on
generic terms (e.g., ?company?) or common extrac-
tion errors (e.g., ?LLC? as a company name). This
is a key area for future work.
4.3 Prevalence of APF Relations
To determine the prevalence of APF relations inWeb
text, we examined a sample of 500 binary relations
selected randomly from TEXTRUNNER?s ground as-
sertions. The surface forms of the relations and ar-
guments may misrepresent the true properties of the
underlying concepts, so to better estimate the true
properties we merged synonymous values as given
by Resolver (Yates and Etzioni, 2007) or the most
frequent sense of the word in WordNet. For exam-
ple, we would consider BornIn(baby, hospital)
and BornAt(infant, infirmary) to represent the
same concept, and so would merge them into one
instance of the ?Born In? relation. The largest two re-
lations had over 1.25 million unique instances each,
and 52% of the relations had more than 10,000 in-
stances.
For each relation R, we first found all instances
of R extracted by TEXTRUNNER and merged all
synonymous instances as described above. Then,
for each argument of R we computed the smallest
value, Kmin, such that R is APF with degree Kmin.
Since many interesting assertions can be inferred by
simply joining two relations, we also considered the
special case of 2-way joins using Corollary 3. We
computed the smallest value, K2./, such that the re-
lation is two-way-APF with degree K2./.
Figure 4 shows the fraction of relations with
Kmin andK2./ of at mostK as a function of varying
85
0%
20%
40%
60%
80%
100%
0 1000 2000 3000 4000 5000 6000Degree of Approximate Pseudo-Functionality
APFAPF for two-way join
Figure 4: Prevalence of APF relations in Web text. The
x-axis depicts the degree of pseudo-functionality, e.g.,
Kmin and K2./, (see definition 2); the y-axis lists the
percent of relations that are APF with that degree. Re-
sults are averaged over both arguments.
values of K. The results are averaged over both ar-
guments of each binary relation. For arbitrary joins
in this KB, 80% of the relations are APF with de-
gree less than 496; for 2-way joins (like the ones in
our inference rules and test queries), 80% of the rela-
tions are APF with degree less than 65. These results
indicate that the majority of relations TEXTRUNNER
extracted from text are APF, and so we can expect
HOLMES?s techniques will allow efficient inference
over most relations.
While Theorem 2 guarantees that joins over those
relations will beO(|R|), that notation hides a poten-
tially large constant factor of Kminm. Fortunately
the constant factor is significantly smaller in prac-
tice. To see why, we re-examine the proof: the large
factor comes from assuming that all of R?s first ar-
guments which meet the PF definition are associated
with exactly Kmin distinct second arguments. How-
ever, in our corpus 83% of first arguments are as-
sociated with only one second argument. Clearly,
our worst-case analysis substantially over-estimates
inference time for most queries. Moreover, in ad-
ditional experiments (omitted due to space limita-
tions), measured join sizes grew linearly in the size
of the corpus, but were on average two to three or-
ders of magnitude smaller than the bounds given in
the theory. This observation held across relations
with different sizes and values of Kmin.
While the results in Figure 4 may vary for other
sets of relations, we believe the general trends
hold. This is promising for Question Answering and
Textual Inference systems, since if true it implies
R2 = 0.9881
R2 = 0.9808
R2 = 0.9931
020
4060
80100
120140
160
0% 20% 40% 60% 80% 100%Fraction of Corpus
GeographyBusinessNutrition
Figure 5: The effects of corpus size on total inference
time. We see approximately linear growth in all domains,
and display the best fit lines and coefficient of determina-
tion (R2) of each.
that combining information frommultiple difference
source is feasible, and can allow such systems to in-
fer answers not explicitly seen in any source.
4.4 Scalability of Inference Speed
Since the previous subsection showed that most re-
lations are APF in their arguments, our theory pre-
dicts HOLMES?s inference will scale linearly. We
tested this hypothesis empirically by running infer-
ence over the test queries in our three domains, while
varying the number of pages in the textual corpus.
Figure 5 shows how the inference time HOLMES
used to answer all queries in each domain scales
with KB size. For these queries, and several oth-
ers we tested (not shown here), inference time grows
linearly with the size of the KB. Based on these re-
sults we believe that HOLMES can provide scalable
inference over a wide variety of domains.
5 Related Work
Textual Entailment systems are given two textual
fragments, text T and hypothesis H , and attempt to
decide if the meaning of H can be inferred from
the meaning of T (Dagan et al, 2005). While
many approaches have addressed this problem, our
work is most closely related to that of (Raina et al,
2005; MacCartney and Manning, 2007; Tatu and
Moldovan, 2006; Braz et al, 2005), which convert
the inputs into logical forms and then attempt to
?prove? H from T plus a set of axioms. For in-
stance, (Braz et al, 2005) represents T , H , and a
set of rewrite rules in a description logic framework,
and determines entailment by solving an integer lin-
86
ear program derived from that representation.
These approaches and related ones (e.g.,
(Van Durme and Schubert, 2008)) use highly
expressive representations, enabling them to ex-
press negation, temporal information, and more.
HOLMES?s representation is much simpler?
Markov Logic Horn Clauses for inference rules
coupled with a massive database of ground asser-
tions. However, this simplification allows HOLMES
to tackle a ?text? of enormously larger size: 117
million Web pages versus a single paragraph. A sec-
ond, if smaller, difference stems from the fact that
instead of determining whether a single hypothesis
sentence, H , follows from the text, HOLMES tries to
find all consequents that match a conjunctive query.
HOLMES is also related to open-domain question-
answering systems such as Mulder (Kwok et al,
2001), AskMSR (Brill et al, 2002), and others
(Harabagiu et al, 2000; Brill et al, 2001). How-
ever, these Q/A systems attempt to find individual
documents or sentences containing the answer. They
often perform deep analysis on promising texts, and
back off to shallower, less reliable methods if those
fail. In contrast, HOLMES utilizes TI and attempts
to combine information from multiple different sen-
tences in a scalable way.
While its ability to combine information from
multiple sources is promising, HOLMES has several
limitations these Q/A systems do not have. Since
HOLMES relies on an information extraction sys-
tem to convert sentences into ground predicates,
any limitations of the IE system will be propagated
to HOLMES. Additionally, the logical representa-
tion HOLMES uses limits the reasoning and types
of questions it can answer. HOLMES is geared to-
wards answering questions which are naturally ex-
pressed as properties and relations of entities, and is
not well suited to answering more abstract or open
ended questions. Although we have demonstrated
that HOLMES is scalable, further work is needed to
make it to run at interactive speeds.
Finally, research in statistical relational learning
such as MLNs (Richardson and Domingos, 2006),
RMNs (Taskar et al, 2002), and others (Getoor
and Taskar, 2007) have studied techniques for com-
bining logical and probabilistic inference. Our in-
ference rules are more restrictive than those al-
lowed in MLNs, but this trade-off allows us to ef-
ficiently scale inference to large, open domain cor-
pora. By constructing only cliques for satisfied in-
ference rules, HOLMES explicitly models the intu-
ition behind LazySAT inference (Singla and Domin-
gos, 2006) as used in MLNs. I.e., most Horn clause
inference rules will be trivially satisfied since their
antecedents will be false, so we only need to worry
about ones where the antecedent is true.
6 Conclusions
This paper makes three main contributions:
1. We introduce and evaluate the HOLMES sys-
tem, which leverages KBMC methods in order
to scale a class of TI methods to the Web.
2. We define the notion of Approximately Pseudo-
Functional (APF) relations and prove that, for
a APF relations, HOLMES?s inference time in-
creases linearly with the size of the input cor-
pus. We show empirically that APF relations
appear to be prevalent in our Web corpus (Fig-
ure 4), and that HOLMES?s runtime does scale
linearly with the size of its input (Figure 5), tak-
ing only a few CPU minutes when run over 183
million distinct ground assertions.
3. We present experiments demonstrating that, for
a set of queries in the domains of geography,
business, and nutrition, HOLMES substantially
improves the quality of answers (measured by
AuC) relative to a ?no inference? baseline.
In the future, we plan more extensive tests to char-
acterize when HOLMES?s inference is helpful. We
also hope to examine in what cases jointly perform-
ing extraction and inference (as opposed to perform-
ing them separately) is feasible at scale. Finally, we
plan to examine methods for HOLMES to learn both
rule weights and new inference rules.
Acknowledgements
We thank the following for helpful comments on
previous drafts: Fei Wu, Michele Banko, Mausam,
Doug Downey, and Alan Ritter. This research was
supported in part by NSF grants IIS-0535284, IIS-
0312988, and IIS-0307906, ONR grants N00014-
08-1-0431 and N00014-06-1-0147, CALO grant 03-
000225, the WRF / TJ Cable Professorship as well
as gifts from Google. The work was performed at
the University of Washington?s Turing Center.
87
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. An inference model for semantic en-
tailment in natural language. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 1678?1679.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.
2001. Data-intensive question answering. In Procs.
of Text REtrieval Conference (TREC-10), pages 393?
400.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the AskMSR question-answering system.
In EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 257?264, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
L. Getoor and B. Taskar. 2007. Introduction to Statistical
Relational Learning. MIT Press.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Exper-
iments with open-domain textual question answering.
In Procs. of the COLING-2000.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scal-
ing question answering to the Web. Proceedings of
the 10th international conference on World Wide Web,
pages 150?161.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to wordnet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann Publishers Inc. San Francisco, CA, USA.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
Parag Singla and Pedro Domingos. 2006. Memory-
efficient inference in relational domains. In AAAI.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discrimi-
native probabilistic models for relational data. Eigh-
teenth Conference on Uncertainty in Artificial Intelli-
gence (UAI02).
Marta Tatu and Dan Moldovan. 2006. A logic-based
semantic approach to recognizing textual entailment.
In Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 819?826, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Ullman. 1989. Database and knowledge-base systems.
Computer Science Press.
B. Van Durme and L.K. Schubert. 2008. Open knowl-
edge extraction through compositional language pro-
cessing. In Symposium on Semantics in Systems for
Text Processing.
M. Wellman, J. Breese, and R. Goldman. 1992. From
knowledge bases to decision models. The Knowledge
Engineering Review, 7(1):35?53.
F. Wu and D. Weld. 2007. Autonomously semantifying
Wikipedia. In Proceedings of the ACM Sixteenth Con-
ference on Information and Knowledge Management
(CIKM-07), Lisbon, Porgugal.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
88
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Compiling a Massive, Multilingual Dictionary via Probabilistic Inference
Mausam Stephen Soderland Oren Etzioni
Daniel S. Weld Michael Skinner* Jeff Bilmes
University of Washington, Seattle *Google, Seattle
{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.com
Abstract
Can we automatically compose a large set
of Wiktionaries and translation dictionar-
ies to yield a massive, multilingual dic-
tionary whose coverage is substantially
greater than that of any of its constituent
dictionaries?
The composition of multiple translation
dictionaries leads to a transitive inference
problem: if word A translates to word
B which in turn translates to word C,
what is the probability that C is a trans-
lation of A? The paper introduces a
novel algorithm that solves this problem
for 10,000,000 words in more than 1,000
languages. The algorithm yields PANDIC-
TIONARY, a novel multilingual dictionary.
PANDICTIONARY contains more than four
times as many translations than in the
largest Wiktionary at precision 0.90 and
over 200,000,000 pairwise translations in
over 200,000 language pairs at precision
0.8.
1 Introduction and Motivation
In the era of globalization, inter-lingual com-
munication is becoming increasingly important.
Although nearly 7,000 languages are in use to-
day (Gordon, 2005), most language resources are
mono-lingual, or bi-lingual.1 This paper investi-
gates whether Wiktionaries and other translation
dictionaries available over the Web can be auto-
matically composed to yield a massive, multilin-
gual dictionary with superior coverage at compa-
rable precision.
We describe the automatic construction of a
massive multilingual translation dictionary, called
1The English Wiktionary, a lexical resource developed by
volunteers over the Internet is one notable exception that con-
tains translations of English words in about 500 languages.
Figure 1: A fragment of the translation graph for two senses
of the English word ?spring?. Edges labeled ?1? and ?3? are
for spring in the sense of a season, and ?2? and ?4? are for
the flexible coil sense. The graph shows translation entries
from an English dictionary merged with ones from a French
dictionary.
PANDICTIONARY, that could serve as a resource
for translation systems operating over a very
broad set of language pairs. The most immedi-
ate application of PANDICTIONARY is to lexical
translation?the translation of individual words or
simple phrases (e.g., ?sweet potato?). Because
lexical translation does not require aligned cor-
pora as input, it is feasible for a much broader
set of languages than statistical Machine Transla-
tion (SMT). Of course, lexical translation cannot
replace SMT, but it is useful for several applica-
tions including translating search-engine queries,
library classifications, meta-data tags,2 and recent
applications like cross-lingual image search (Et-
zioni et al, 2007), and enhancing multi-lingual
Wikipedias (Adar et al, 2009). Furthermore,
lexical translation is a valuable component in
knowledge-based Machine Translation systems,
e.g., (Bond et al, 2005; Carbonell et al, 2006).
PANDICTIONARY currently contains over 200
million pairwise translations in over 200,000 lan-
guage pairs at precision 0.8. It is constructed from
information harvested from 631 online dictionar-
ies and Wiktionaries. This necessitates match-
2Meta-data tags appear in community Web sites such as
flickr.com and del.icio.us.
262
ing word senses across multiple, independently-
authored dictionaries. Because of the millions of
translations in the dictionaries, a feasible solution
to this sense matching problem has to be scalable;
because sense matches are imperfect and uncer-
tain, the solution has to be probabilistic.
The core contribution of this paper is a princi-
pled method for probabilistic sense matching to in-
fer lexical translations between two languages that
do not share a translation dictionary. For exam-
ple, our algorithm can conclude that Basque word
?udaherri? is a translation of Maori word ?koanga?
in Figure 1. Our contributions are as follows:
1. We describe the design and construction of
PANDICTIONARY?a novel lexical resource
that spans over 200 million pairwise transla-
tions in over 200,000 language pairs at 0.8
precision, a four-fold increase when com-
pared to the union of its input translation dic-
tionaries.
2. We introduce SenseUniformPaths, a scal-
able probabilistic method, based on graph
sampling, for inferring lexical translations,
which finds 3.5 times more inferred transla-
tions at precison 0.9 than the previous best
method.
3. We experimentally contrast PANDIC-
TIONARY with the English Wiktionary and
show that PANDICTIONARY is from 4.5 to
24 times larger depending on the desired
precision.
The remainder of this paper is organized as fol-
lows. Section 2 describes our earlier work on
sense matching (Etzioni et al, 2007). Section 3
describes how the PANDICTIONARY builds on and
improves on their approach. Section 4 reports on
our experimental results. Section 5 considers re-
lated work on lexical translation. The paper con-
cludes in Section 6 with directions for future work.
2 Building a Translation Graph
In previous work (Etzioni et al, 2007) we intro-
duced an approach to sense matching that is based
on translation graphs (see Figure 1 for an exam-
ple). Each vertex v ? V in the graph is an or-
dered pair (w, l) where w is a word in a language
l. Undirected edges in the graph denote transla-
tions between words: an edge e ? E between (w1,
l1) and (w2, l2) represents the belief that w1 and
w2 share at least one word sense.
Construction: The Web hosts a large num-
ber of bilingual dictionaries in different languages
and several Wiktionaries. Bilingual dictionaries
translate words from one language to another, of-
ten without distinguishing the intended sense. For
example, an Indonesian-English dictionary gives
?light? as a translation of the Indonesian word ?en-
teng?, but does not indicate whether this means il-
lumination, light weight, light color, or the action
of lighting fire.
The Wiktionaries (wiktionary.org) are sense-
distinguished, multilingual dictionaries created by
volunteers collaborating over the Web. A transla-
tion graph is constructed by locating these dictio-
naries, parsing them into a common XML format,
and adding the nodes and edges to the graph.
Figure 1 shows a fragment of a translation
graph, which was constructed from two sets of
translations for the word ?spring? from an English
Wiktionary, and two corresponding entries from
a French Wiktionary for ?printemps? (spring sea-
son) and ?ressort? (flexible spring). Translations of
the season ?spring? have edges labeled with sense
ID=1, the flexible coil sense has ID=2, translations
of ?printemps? have ID=3, and so forth.3
For clarity, we show only a few of the actual
vertices and edges; e.g., the figure doesn?t show
the edge (ID=1) between ?udaherri? and ?primav-
era?.
Inference: In our previous system we had
a simple inference procedure over translation
graphs, called TRANSGRAPH, to find translations
beyond those provided by any source dictionary.
TRANSGRAPH searched for paths in the graph be-
tween two vertices and estimated the probability
that the path maintains the same word sense along
all edges in the path, even when the edges come
from different dictionaries. For example, there are
several paths between ?udaherri? and ?koanga? in
Figure 1, but all shift from sense ID 1 to 3. The
probability that the two words are translations is
equivalent to the probability that IDs 1 and 3 rep-
resent the same sense.
TRANSGRAPH used two formulae to estimate
these probabilities. One formula estimates the
probability that two multi-lingual dictionary en-
tries represent the same word sense, based on the
proportion of overlapping translations for the two
entries. For example, most of the translations of
3Sense-distinguished multi-lingual entries give rise to
cliques all of which share a common sense ID.
263
French ?printemps? are also translations of the sea-
son sense of ?spring?. A second formula is based
on triangles in the graph (useful for bilingual dic-
tionaries): a clique of 3 nodes with an edge be-
tween each pair of nodes. In such cases, there is
a high probability that all 3 nodes share a word
sense.
Critique: While TRANSGRAPH was the first
to present a scalable inference method for lexical
translation, it suffers from several drawbacks. Its
formulae operate only on local information: pairs
of senses that are adjacent in the graph or triangles.
It does not incorporate evidence from longer paths
when an explicit triangle is not present. Moreover,
the probabilities from different paths are com-
bined conservatively (either taking the max over
all paths, or using ?noisy or? on paths that are
completely disjoint, except end points), thus lead-
ing to suboptimal precision/recall.
In response to this critique, the next section
presents an inference algorithm, called SenseUni-
formPaths (SP), with substantially improved recall
at equivalent precision.
3 Translation Inference Algorithms
In essence, inference over a translation graph
amounts to transitive sense matching: if word A
translates to word B, which translates in turn to
word C, what is the probability that C is a trans-
lation of A? If B is polysemous then C may not
share a sense with A. For example, in Figure 2(a)
if A is the French word ?ressort? (the flexible-
coil sense of spring) and B is the English word
?spring?, then Slovenian word ?vzmet? may or may
not be a correct translation of ?ressort? depending
on whether the edge (B,C) denotes the flexible-
coil sense of spring, the season sense, or another
sense. Indeed, given only the knowledge of the
path A ? B ? C we cannot claim anything with
certainty regarding A to C.
However, if A, B, and C are on a circuit that
starts at A, passes through B and C and re-
turns to A, there is a high probability that all
nodes on that circuit share a common word sense,
given certain restrictions that we enumerate later.
Where TRANSGRAPH used evidence from circuits
of length 3, we extend this to paths of arbitrary
lengths.
To see how this works, let us begin with the sim-
plest circuit, a triangle of three nodes as shown in
Figure 2(b). We can be quite certain that ?vzmet?
shares the sense of coil with both ?spring? and
?ressort?. Our reasoning is as follows: even
though both ?ressort? and ?spring? are polysemous
they share only one sense. For a triangle to form
we have two choices ? (1) either ?vzmet? means
spring coil, or (2) ?vzmet? means both the spring
season and jurisdiction, but not spring coil. The
latter is possible but such a coincidence is very un-
likely, which is why a triangle is strong evidence
for the three words to share a sense.
As an example of longer paths, our inference
algorithms can conclude that in Figure 2(c), both
?molla? and ?vzmet? have the sense coil, even
though no explicit triangle is present. To show
this, let us define a translation circuit as follows:
Definition 1 A translation circuit from v?1 with
sense s? is a cycle that starts and ends at v?1 with
no repeated vertices (other than v?1 at end points).
Moreover, the path includes an edge between v?1
and another vertex v?2 that also has sense s
?.
All vertices on a translation circuit are mutual
translations with high probability, as in Figure
2(c). The edge from ?spring? indicates that ?vzmet?
means either coil or season, while the edge from
?ressort? indicates that ?molla? means either coil
or jurisdiction. The edge from ?vzmet? to ?molla?
indicates that they share a sense, which will hap-
pen if all nodes share the sense season or if either
?vzmet? has the unlikely combination of coil and
jurisdiction (or ?molla? has coil and season).
We also develop a mathematical model of
sense-assignment to words that lets us formally
prove these insights. For more details on the the-
ory please refer to our extended version. This pa-
per reports on our novel algorithm and experimen-
tal results.
These insights suggest a basic version of our al-
gorithm: ?given two vertices, v?1 and v
?
2 , that share
a sense (say s?) compute all translation circuits
from v?1 in the sense s
?; mark all vertices in the
circuits as translations of the sense s??.
To implement this algorithm we need to decide
whether a vertex lies on a translation circuit, which
is trickier than it seems. Notice that knowing
that v is connected independently to v?1 and v
?
2
doesn?t imply that there exists a translation circuit
through v, because both paths may go through a
common node, thus violating of the definition of
translation circuit. For example, in Figure 2(d) the
Catalan word ?ploma? has paths to both spring and
ressort, but there is no translation circuit through
264
spring
English
ressort
French
vzmet
Slovenian
spring
English
ressort
French
vzmet
Slovenian
spring
English
vzmet
Slovenian
ressort
French
molla
Italian
spring
English
ressort
French
ploma
Catalan
Feder
German
???? 
Russian
spring
English
ressort
French
fj?der
Swedish
penna
Italian
Feder
German
(a)                         (b)                                   (c)                                (d)                     (e)
season
coil
jurisdiction
coil
s* s*
s* s*
s*
? ? ?
? ?
feather
coil
?
?
Figure 2: Snippets of translation graphs illustrating various inference scenarios. The nodes in question mark represent the
nodes in focus for each illustration. For all cases we are trying to infer translations of the flexible coil sense of spring.
it. Hence, it will not be considered a transla-
tion. This example also illustrates potential errors
avoided by our algorithm ? here, German word
?Feder? mean feather and spring coil, but ?ploma?
means feather and not the coil.
An exhaustive search to find translation circuits
would be too slow, so we approximate the solution
by a random walk scheme. We start the random
walk from v?1 (or v
?
2) and choose random edges
without repeating any vertices in the current path.
At each step we check if the current node has an
edge to v?2 (or v
?
1). If it does, then all the ver-
tices in the current path form a translation circuit
and, thus, are valid translations. We repeat this
random walk many times and keep marking the
nodes. In our experiments for each inference task
we performed a total of 2,000 random walks (NR
in pseudo-code) of max circuit length 7. We chose
these parameters based on a development set of 50
inference tasks.
Our first experiments with this basic algorithm
resulted in a much higher recall than TRANS-
GRAPH, albeit, at a significantly lower precision.
A closer examination of the results revealed two
sources of error ? (1) errors in source dictionary
data, and (2) correlated sense shifts in translation
circuits. Below we add two new features to our
algorithm to deal with each of these error sources,
respectively.
3.1 Errors in Source Dictionaries
In practice, source dictionaries contain mistakes
and errors occur in processing the dictionaries to
create the translation graph. Thus, existence of a
single translation circuit is only limited evidence
for a vertex as a translation. We wish to exploit
the insight that more translation circuits constitute
stronger evidence. However, the different circuits
may share some edges, and thus the evidence can-
not be simply the number of translation circuits.
We model the errors in dictionaries by assigning
a probability less than 1.0 to each edge4 (pe in the
4In our experiments we used a flat value of 0.6, chosen by
pseudo-code). We assume that the probability of
an edge being erroneous is independent of the rest
of the graph. Thus, a translation graph with pos-
sible data errors converts into a distribution over
accurate translation graphs.
Under this distribution, we can use the proba-
bility of existence of a translation circuit through a
vertex as the probability that the vertex is a trans-
lation. This value captures our insights, since a
larger number of translation circuits gives a higher
probability value.
We sample different graph topologies from our
given distribution. Some translation circuits will
exist in some of the sampled graphs, but not in
others. This, in turn, means that a given vertex v
will only be on a circuit for a fraction of the sam-
pled graphs. We take the proportion of samples in
which v is on a circuit to be the probability that v
is in the translation set. We refer to this algorithm
as Unpruned SenseUniformPaths (uSP).
3.2 Avoiding Correlated Sense-shifts
The second source of errors are circuits that in-
clude a pair of nodes sharing the same polysemy,
i.e., having the same pair of senses. A circuit
might maintain sense s? until it reaches a node that
has both s? and a distinct si. The next edge may
lead to a node with si, but not s?, causing an ex-
traction error. The path later shifts back to sense
s? at a second node that also has s? and si. An ex-
ample for this is illustrated in Figure 2(e), where
both the German and Swedish words mean feather
and spring coil. Here, Italian ?penna? means only
the feather and not the coil.
Two nodes that share the same two senses oc-
cur frequently in practice. For example, many
languages use the same word for ?heart? (the or-
gan) and center; similarly, it is common for lan-
guages to use the same word for ?silver?, the metal
and the color. These correlations stem from com-
parameter tuning on a development set of 50 inference tasks.
In future we can use different values for different dictionaries
based on our confidence in their accuracy.
265
Figure 3: The set {B, C} has a shared ambiguity - each
node has both sense 1 (from the lower clique) and sense 2
(from the upper clique). A circuit that contains two nodes
from the same ambiguity set with an intervening node not in
that set is likely to create translation errors.
mon metaphor and the shared evolutionary roots
of some languages.
We are able to avoid circuits with this type of
correlated sense-shift by automatically identifying
ambiguity sets, sets of nodes known to share mul-
tiple senses. For instance, in Figure 2(e) ?Feder?
and ?fj?der? form an ambiguity set (shown within
dashed lines), as they both mean feather and coil.
Definition 2 An ambiguity set A is a set of ver-
tices that all share the same two senses. I.e.,
?s1, s2, with s1 6= s2 s.t. ?v ? A, sense(v, s1)?
sense(v, s2), where sense(v, s) denotes that v has
sense s.
To increase the precision of our algorithm we
prune the circuits that contain two nodes in the
same ambiguity set and also have one or more in-
tervening nodes that are not in the ambiguity set.
There is a strong likelihood that the intervening
nodes will represent a translation error.
Ambiguity sets can be detected from the graph
topology as follows. Each clique in the graph rep-
resents a set of vertices that share a common word
sense. When two cliques intersect in two or more
vertices, the intersecting vertices share the word
sense of both cliques. This may either mean that
both cliques represent the same word sense, or that
the intersecting vertices form an ambiguity set. A
large overlap between two cliques makes the for-
mer case more likely; a small overlap makes it
more likely that we have found an ambiguity set.
Figure 3 illustrates one such computation.
All nodes of the clique V1, V2, A,B,C,D share
a word sense, and all nodes of the clique
B,C,E, F,G,H also share a word sense. The set
{B,C} has nodes that have both senses, forming
an ambiguity set. We denote the set of ambiguity
sets by A in the pseudo-code.
Having identified these ambiguity sets, we mod-
ify our random walk scheme by keeping track of
whether we are entering or leaving an ambiguity
set. We prune away all paths that enter the same
ambiguity set twice. We name the resulting algo-
rithm SenseUniformPaths (SP), summarized at a
high level in Algorithm 1.
Comparing Inference Algorithms Our evalua-
tion demonstrated that SP outperforms uSP. Both
these algorithms have significantly higher recall
than TRANSGRAPH algorithm. The detailed re-
sults are presented in Section 4.2. We choose SP
as our inference algorithm for all further research,
in particular to create PANDICTIONARY.
3.3 Compiling PanDictionary
Our goal is to automatically compile PANDIC-
TIONARY, a sense-distinguished lexical transla-
tion resource, where each entry is a distinct word
sense. Associated with each word sense is a list of
translations in multiple languages.
We use Wiktionary senses as the base senses
for PANDICTIONARY. Recall that SP requires two
nodes (v?1 and v
?
2) for inference. We use the Wik-
tionary source word as v?1 and automatically pick
the second word from the set of Wiktionary trans-
lations of that sense by choosing a word that is
well connected, and, which does not appear in
other senses of v?1 (i.e., is expected to share only
one sense with v?1).
We first run SenseUniformPaths to expand the
approximately 50,000 senses in the English Wik-
tionary. We further expand any senses from the
other Wiktionaries that are not yet covered by
PANDICTIONARY, and add these to PANDIC-
TIONARY. This results in the creation of the
world?s largest multilingual, sense-distinguished
translation resource, PANDICTIONARY. It con-
tains a little over 80,000 senses. Its construction
takes about three weeks on a 3.4 GHz processor
with a 2 GB memory.
Algorithm 1 S.P.(G, v?1, v
?
2,A)
1: parameters NG: no. of graph samples, NR: no. of ran-
dom walks, pe: prob. of sampling an edge
2: createNG versions ofG by sampling each edge indepen-
dently with probability pe
3: for all i = 1..NG do
4: for all vertices v : rp[v][i] = 0
5: perform NR random walks starting at v?1 (or v
?
2 ) and
pruning any walk that enters (or exits) an ambiguity
set in A twice. All walks that connect to v?2 (or v
?
1 )
form a translation circuit.
6: for all vertices v do
7: if(v is on a translation circuit) rp[v][i] = 1
8: return
?
i
rp[v][i]
NG
as the prob. that v is a translation
266
4 Empirical Evaluation
In our experiments we investigate three key ques-
tions: (1) which of the three algorithms (TG, uSP
and SP) is superior for translation inference (Sec-
tion 4.2)? (2) how does the coverage of PANDIC-
TIONARY compare with the largest existing mul-
tilingual dictionary, the English Wiktionary (Sec-
tion 4.3)? (3) what is the benefit of inference over
the mere aggregation of 631 dictionaries (Section
4.4)? Additionally, we evaluate the inference algo-
rithm on two other dimensions ? variation with the
degree of polysemy of source word, and variation
with original size of the seed translation set.
4.1 Experimental Methodology
Ideally, we would like to evaluate a random sam-
ple of the more than 1,000 languages represented
in PANDICTIONARY.5 However, a high-quality
evaluation of translation between two languages
requires a person who is fluent in both languages.
Such people are hard to find and may not even
exist for many language pairs (e.g., Basque and
Maori). Thus, our evaluation was guided by our
ability to recruit volunteer evaluators. Since we
are based in an English speaking country we were
able to recruit local volunteers who are fluent in
a range of languages and language families, and
who are also bilingual in English.6
The experiments in Sections 4.2 and 4.3 test
whether translations in a PANDICTIONARY have
accurate word senses. We provided our evalua-
tors with a random sample of translations into their
native language. For each translation we showed
the English source word and gloss of the intended
sense. For example, a Dutch evaluator was shown
the sense ?free (not imprisoned)? together with the
Dutch word ?loslopende?. The instructions were
to mark a word as correct if it could be used to ex-
press the intended sense in a sentence in their na-
tive language. For experiments in Section 4.4 we
tested precision of pairwise translations, by having
informants in several pairs of languages discuss
whether the words in their respective languages
can be used for the same sense.
We use the tags of correct or incorrect to com-
pute the precision: the percentage of correct trans-
5The distribution of words in PANDICTIONARY is highly
non-uniform ranging from 182,988 words in English to 6,154
words in Luxembourgish and 189 words in Tuvalu.
6The languages used was based on the availability of na-
tive speakers. This varied between the different experiments,
which were conducted at different times.
Figure 4: The SenseUniformPaths algorithm (SP) more
than doubles the number of correct translations at precision
0.95, compared to a baseline of translations that can be found
without inference.
lations divided by correct plus incorrect transla-
tions. We then order the translations by probabil-
ity and compute the precision at various probabil-
ity thresholds.
4.2 Comparing Inference Algorithms
Our first evaluation compares our SenseUniform-
Paths (SP) algorithm (before and after pruning)
with TRANSGRAPH on both precision and num-
ber of translations.
To carry out this comparison, we randomly sam-
pled 1,000 senses from English Wiktionary and
ran the three algorithms over them. We evalu-
ated the results on 7 languages ? Chinese, Danish,
German, Hindi, Japanese, Russian, and Turkish.
Each informant tagged 60 random translations in-
ferred by each algorithm, which resulted in 360-
400 tags per algorithm7. The precision over these
was taken as a surrogate for the precision across
all the senses.
We compare the number of translations for each
algorithm at comparable precisions. The baseline
is the set of translations (for these 1000 senses)
found in the source dictionaries without inference,
which has a precision 0.95 (as evaluated by our
informants).8
Our results are shown in Figure 4. At this high
precision, SP more than doubles the number of
baseline translations, finding 5 times as many in-
ferred translations (in black) as TG.
Indeed, both uSP and SP massively outperform
TG. SP is consistently better than uSP, since it
performs better for polysemous words, due to its
pruning based on ambiguity sets. We conclude
7Some translations were marked as ?Don?t know?.
8Our informants tended to underestimate precision, often
marking correct translations in minor senses of a word as in-
correct.
267
0.5
0.6
0.7
0.8
0.9
1
0.0 4.0 8.0 12.0 16.0
Pr
ec
is
io
n
Translations in Millions
PanDictionary
English Wiktionary
Figure 5: Precision vs. coverage curve for PANDIC-
TIONARY. It quadruples the size of the English Wiktionary at
precision 0.90, is more than 8 times larger at precision 0.85
and is almost 24 times the size at precision 0.7.
that SP is the best inference algorithm and employ
it for PANDICTIONARY construction.
4.3 Comparison with English Wiktionary
We now compare the coverage of PANDIC-
TIONARY with the English Wiktionary at varying
levels of precision. The English Wiktionary is the
largest Wiktionary with a total of 403,413 transla-
tions. It is also more reliable than some other Wik-
tionaries in making word sense distinctions. In this
study we use only the subset of PANDICTIONARY
that was computed starting from the English Wik-
tionary senses. Thus, this subsection under-reports
PANDICTIONARY?s coverage.
To evaluate a huge resource such as PANDIC-
TIONARY we recruited native speakers of 14 lan-
guages ? Arabic, Bulgarian, Danish, Dutch, Ger-
man, Hebrew, Hindi, Indonesian, Japanese, Ko-
rean, Spanish, Turkish, Urdu, and Vietnamese. We
randomly sampled 200 translations per language,
which resulted in about 2,500 tags. Figure 5
shows the total number of translations in PANDIC-
TIONARY in senses from the English Wiktionary.
At precision 0.90, PANDICTIONARY has 1.8 mil-
lion translations, 4.5 times as many as the English
Wiktionary.
We also compare the coverage of PANDIC-
TIONARY with that of the English Wiktionary in
terms of languages covered. Table 1 reports, for
each resource, the number of languages that have
a minimum number of distinct words in the re-
source. PANDICTIONARY has 1.4 times as many
languages with at least 1,000 translations at pre-
cision 0.90 and more than twice at precision 0.7.
These observations reaffirm our faith in the pan-
lingual nature of the resource.
PANDICTIONARY?s ability to expand the lists
of translations provided by the EnglishWiktionary
is most pronounced for senses with a small num-
0.75
0.8
0.85
0.9
0.95
1 2 3,4 >4
Pre
cis
ion
Avg precision 0.90
Avg precision 0.85
Polysemy of the English source word
3-4
Figure 6: Variation of precision with the degree of poly-
semy of the source English word. The precision decreases as
polysemy increases, still maintaining reasonably high values.
ber of translations. For example, at precision 0.90,
senses that originally had 3 to 6 translations are in-
creased 5.3 times in size. The increase is 2.2 times
when the original sense size is greater than 20.
For closer analysis we divided the English
source words (v?1) into different bins based on the
number of senses that English Wiktionary lists for
them. Figure 6 plots the variation of precision with
this degree of polysemy. We find that translation
quality decreases as degree of polysemy increases,
but this decline is gradual, which suggests that SP
algorithm is able to hold its ground well in difficult
inference tasks.
4.4 Comparison with All Source Dictionaries
We have shown that PANDICTIONARY has much
broader coverage than the English Wiktionary, but
how much of this increase is due to the inference
algorithm versus the mere aggregation of hundreds
of translation dictionaries in PANDICTIONARY?
Since most bilingual dictionaries are not sense-
distinguished, we ignore the word senses and
count the number of distinct (word1, word2) trans-
lation pairs.
We evaluated the precision of word-word trans-
lations by a collaborative tagging scheme, with
two native speakers of different languages, who
are both bi-lingual in English. For each sug-
gested translation they discussed the various
senses of words in their respective languages
and tag a translation correct if they found some
sense that is shared by both words. For this
study we tagged 7 language pairs: Hindi-Hebrew,
# languages with distinct words
? 1000 ? 100 ? 1
English Wiktionary 49 107 505
PanDictionary (0.90) 67 146 608
PanDictionary (0.85) 75 175 794
PanDictionary (0.70) 107 607 1066
Table 1: PANDICTIONARY covers substantially more lan-
guages than the English Wiktionary.
268
050
100
150
200
250
EW 631D PD(0.9) PD(0.85) PD(0.8)
Inferred transl. Direct transl.
Tra
nsl
ati
on
s(i
n m
illio
ns)
Figure 7: The number of distinct word-word translation
pairs from PANDICTIONARY is several times higher than the
number of translation pairs in the English Wiktionary (EW)
or in all 631 source dictionaries combined (631 D). A major-
ity of PANDICTIONARY translations are inferred by combin-
ing entries from multiple dictionaries.
Japanese-Russian, Chinese-Turkish, Japanese-
German, Chinese-Russian, Bengali-German, and
Hindi-Turkish.
Figure 7 compares the number of word-word
translation pairs in the English Wiktionary (EW),
in all 631 source dictionaries (631 D), and in PAN-
DICTIONARY at precisions 0.90, 0.85, and 0.80.
PANDICTIONARY increases the number of word-
word translations by 73% over the source dictio-
nary translations at precision 0.90 and increases it
by 2.7 times at precision 0.85. PANDICTIONARY
also adds value by identifying the word sense of
the translation, which is not given in most of the
source dictionaries.
5 Related Work
Because we are considering a relatively new prob-
lem (automatically building a panlingual transla-
tion resource) there is little work that is directly re-
lated to our own. The closest research is our previ-
ous work on TRANSGRAPH algorithm (Etzioni et
al., 2007). Our current algorithm outperforms the
previous state of the art by 3.5 times at precision
0.9 (see Figure 4). Moreover, we compile this in a
dictionary format, thus considerably reducing the
response time compared to TRANSGRAPH, which
performed inference at query time.
There has been considerable research on meth-
ods to acquire translation lexicons from either
MRDs (Neff and McCord, 1990; Helmreich et
al., 1993; Copestake et al, 1994) or from par-
allel text (Gale and Church, 1991; Fung, 1995;
Melamed, 1997; Franz et al, 2001), but this has
generally been limited to a small number of lan-
guages. Manually engineered dictionaries such as
EuroWordNet (Vossen, 1998) are also limited to
a relatively small set of languages. There is some
recent work on compiling dictionaries from mono-
lingual corpora, which may scale to several lan-
guage pairs in future (Haghighi et al, 2008).
Little work has been done in combining mul-
tiple dictionaries in a way that maintains word
senses across dictionaries. Gollins and Sanderson
(2001) explored using triangulation between alter-
nate pivot languages in cross-lingual information
retrieval. Their triangulation essentially mixes
together circuits for all word senses, hence, is un-
able to achieve high precision.
Dyvik?s ?semantic mirrors? uses translation
paths to tease apart distinct word senses from
inputs that are not sense-distinguished (Dyvik,
2004). However, its expensive processing and
reliance on parallel corpora would not scale to
large numbers of languages. Earlier (Knight and
Luk, 1994) discovered senses of Spanish words by
matching several English translations to a Word-
Net synset. This approach applies only to specific
kinds of bilingual dictionaries, and also requires a
taxonomy of synsets in the target language.
Random walks, graph sampling and Monte
Carlo simulations are popular in literature, though,
to our knowledge, none have applied these to our
specific problems (Henzinger et al, 1999; Andrieu
et al, 2003; Karger, 1999).
6 Conclusions
We have described the automatic construction of
a unique multilingual translation resource, called
PANDICTIONARY, by performing probabilistic in-
ference over the translation graph. Overall, the
construction process consists of large scale in-
formation extraction over the Web (parsing dic-
tionaries), combining it into a single resource (a
translation graph), and then performing automated
reasoning over the graph (SenseUniformPaths) to
yield a much more extensive and useful knowl-
edge base.
We have shown that PANDICTIONARY has
more coverage than any other existing bilingual
or multilingual dictionary. Even at the high preci-
sion of 0.90, PANDICTIONARY more than quadru-
ples the size of the English Wiktionary, the largest
available multilingual resource today.
We plan to make PANDICTIONARY available
to the research community, and also to the Wik-
tionary community in an effort to bolster their ef-
forts. PANDICTIONARY entries can suggest new
translations for volunteers to add to Wiktionary
entries, particularly if combined with an intelli-
gent editing tool (e.g., (Hoffmann et al, 2009)).
269
Acknowledgments
This research was supported by a gift from the
Utilika Foundation to the Turing Center at Uni-
versity of Washington. We acknowledge Paul
Beame, Nilesh Dalvi, Pedro Domingos, Rohit
Khandekar, Daniel Lowd, Parag, Jonathan Pool,
Hoifung Poon, Vibhor Rastogi, Gyanit Singh for
fruitful discussions and insightful comments on
the research. We thank the language experts who
donated their time and language expertise to eval-
uate our systems. We also thank the anynomous
reviewers of the previous drafts of this paper for
their valuable suggestions in improving the evalu-
ation and presentation.
References
E. Adar, M. Skinner, and D. Weld. 2009. Information
arbitrage in multi-lingual Wikipedia. In Procs. of
Web Search and Data Mining(WSDM 2009).
C. Andrieu, N. De Freitas, A. Doucet, and M. Jor-
dan. 2003. An Introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
F. Bond, S. Oepen, M. Siegel, A. Copestake, and
D D. Flickinger. 2005. Open source machine trans-
lation with DELPH-IN. In Open-Source Machine
Translation Workshop at MT Summit X.
J. Carbonell, S. Klein, D. Miller, M. Steinbaum,
T. Grassiany, and J. Frey. 2006. Context-based ma-
chine translation. In AMTA.
A. Copestake, T. Briscoe, P. Vossen, A. Ageno,
I. Castellon, F. Ribas, G. Rigau, H. Rodriquez, and
A. Samiotou. 1994. Acquisition of lexical trans-
lation relations from MRDs. Machine Translation,
3(3?4):183?219.
H. Dyvik. 2004. Translation as semantic mirrors: from
parallel corpus to WordNet. Language and Comput-
ers, 49(1):311?326.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007. Lexical translation with application to image
search on the Web. In Machine Translation Summit
XI.
M. Franz, S. McCarly, and W. Zhu. 2001. English-
Chinese information retrieval at IBM. In Proceed-
ings of TREC 2001.
P. Fung. 1995. A pattern matching method for finding
noun and proper noun translations from noisy paral-
lel corpora. In Proceedings of ACL-1995.
W. Gale and K.W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-1991.
T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
SIGIR.
Raymond G. Gordon, Jr., editor. 2005. Ethnologue:
Languages of the World (Fifteenth Edition). SIL In-
ternational.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL.
S. Helmreich, L. Guthrie, and Y. Wilks. 1993. The
use of machine readable dictionaries in the Pangloss
project. In AAAI Spring Symposium on Building
Lexicons for Machine Translation.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 1999. Measuring index
quality using random walks on the web. In WWW.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-
rty, and D. S. Weld. 2009. Amplifying commu-
nity content creation with mixed-initiative informa-
tion extraction. In ACM SIGCHI (CHI2009).
D. R. Karger. 1999. A randomized fully polynomial
approximation scheme for the all-terminal network
reliability problem. SIAM Journal of Computation,
29(2):492?514.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In AAAI.
I.D. Melamed. 1997. A Word-to-Word Model of
Translational Equivalence. In Proceedings of ACL-
1997 and EACL-1997, pages 490?497.
M. Neff and M. McCord. 1990. Acquiring lexical data
from machine-readable dictionary resources for ma-
chine translation. In 3rd Intl Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion of Natural Language.
P. Vossen, editor. 1998. EuroWordNet: A multilingual
database with lexical semantic networds. Kluwer
Academic Publishers.
270
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088?1098,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning First-Order Horn Clauses from Web Text
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98125, USA
stef,etzioni,weld@cs.washington.edu
Jesse Davis
Katholieke Universiteit Leuven
Department of Computer Science
POBox 02402 Celestijnenlaan 200a
B-3001 Heverlee, Belgium
jesse.davis@cs.kuleuven.be
Abstract
Even the entire Web corpus does not explic-
itly answer all questions, yet inference can un-
cover many implicit answers. But where do
inference rules come from?
This paper investigates the problem of learn-
ing inference rules from Web text in an un-
supervised, domain-independent manner. The
SHERLOCK system, described herein, is a
first-order learner that acquires over 30,000
Horn clauses from Web text. SHERLOCK em-
bodies several innovations, including a novel
rule scoring function based on Statistical Rel-
evance (Salmon et al, 1971) which is effec-
tive on ambiguous, noisy and incomplete Web
extractions. Our experiments show that in-
ference over the learned rules discovers three
times as many facts (at precision 0.8) as the
TEXTRUNNER system which merely extracts
facts explicitly stated in Web text.
1 Introduction
Today?s Web search engines locate pages that match
keyword queries. Even sophisticated Web-based
Q/A systems merely locate pages that contain an ex-
plicit answer to a question. These systems are help-
less if the answer has to be inferred from multiple
sentences, possibly on different pages. To solve this
problem, Schoenmackers et al(2008) introduced the
HOLMES system, which infers answers from tuples
extracted from text.
HOLMES?s distinction is that it is domain inde-
pendent and that its inference time is linear in the
size of its input corpus, which enables it to scale to
the Web. However, HOLMES?s Achilles heel is that
it requires hand-coded, first-order, Horn clauses as
input. Thus, while HOLMES?s inference run time
is highly scalable, it requires substantial labor and
expertise to hand-craft the appropriate set of Horn
clauses for each new domain.
Is it possible to learn effective first-order Horn
clauses automatically from Web text in a domain-
independent and scalable manner? We refer to the
set of ground facts derived from Web text as open-
domain theories. Learning Horn clauses has been
studied extensively in the Inductive Logic Program-
ming (ILP) literature (Quinlan, 1990; Muggleton,
1995). However, learning Horn clauses from open-
domain theories is particularly challenging for sev-
eral reasons. First, the theories denote instances of
an unbounded and unknown set of relations. Sec-
ond, the ground facts in the theories are noisy, and
incomplete. Negative examples are mostly absent,
and certainly we cannot make the closed-world as-
sumption typically made by ILP systems. Finally,
the names used to denote both entities and relations
are rife with both synonyms and polysymes making
their referents ambiguous and resulting in a particu-
larly noisy and ambiguous set of ground facts.
This paper presents a new ILP method, which is
optimized to operate on open-domain theories de-
rived from massive and diverse corpora such as the
Web, and experimentally confirms both its effective-
ness and superiority over traditional ILP algorithms
in this context. Table 1 shows some example rules
that were learned by SHERLOCK.
This work makes the following contributions:
1. We describe the design and implementation of
the SHERLOCK system, which utilizes a novel,
unsupervised ILP method to learn first-order
Horn clauses from open-domain Web text.
1088
IsHeadquarteredIn(Company, State) :-
IsBasedIn(Company, City) ? IsLocatedIn(City, State);
Contains(Food, Chemical) :-
IsMadeFrom(Food, Ingredient) ? Contains(Ingredient, Chemical);
Reduce(Medication, Factor) :-
KnownGenericallyAs(Medication, Drug) ? Reduce(Drug, Factor);
ReturnTo(Writer, Place) :- BornIn(Writer, City) ? CapitalOf(City, Place);
Make(Company1, Device) :- Buy(Company1, Company2) ? Make(Company2, Device);
Table 1: Example rules learned by SHERLOCK from Web extractions. Note that the italicized rules are unsound.
2. We derive an innovative scoring function that is
particularly well-suited to unsupervised learn-
ing from noisy text. For Web text, the scoring
function yields more accurate rules than several
functions from the ILP literature.
3. We demonstrate the utility of SHERLOCK?s
automatically learned inference rules. Infer-
ence using SHERLOCK?s learned rules identi-
fies three times as many high quality facts (e.g.,
precision ? 0.8) as were originally extracted
from the Web text corpus.
The remainder of this paper is organized as fol-
lows. We start by describing previous work. Sec-
tion 3 introduces the SHERLOCK rule learning sys-
tem, with Section 3.4 describing how it estimates
rule quality. We empirically evaluate SHERLOCK in
Section 4, and conclude.
2 Previous Work
SHERLOCK is one of the first systems to learn first-
order Horn clauses from open-domain Web extrac-
tions. The learning method in SHERLOCK belongs
to the Inductive logic programming (ILP) subfield
of machine learning (Lavrac and Dzeroski, 2001).
However, classical ILP systems (e.g., FOIL (Quin-
lan, 1990) and Progol (Muggleton, 1995)) make
strong assumptions that are inappropriate for open
domains. First, ILP systems assume high-quality,
hand-labeled training examples for each relation of
interest. Second, ILP systems assume that constants
uniquely denote individuals; however, in Web text
strings such as ?dad? or ?John Smith? are highly
ambiguous. Third, ILP system typically assume
complete, largely noise-free data whereas tuples ex-
tracted from Web text are both noisy and radically
incomplete. Finally, ILP systems typically utilize
negative examples, which are not available when
learning from open-domain facts. One system that
does not require negative examples is LIME (Mc-
Creath and Sharma, 1997); We compare SHERLOCK
with LIME?s methods in Section 4.3. Most prior ILP
and Markov logic structure learning systems (e.g.,
(Kok and Domingos, 2005)) are not designed to han-
dle the noise and incompleteness of open-domain,
extracted facts.
NELL (Carlson et al, 2010) performs coupled
semi-supervised learning to extract a large knowl-
edge base of instances, relations, and inference
rules, bootstrapping from a few seed examples of
each class and relation of interest and a few con-
straints among them. In contrast, SHERLOCK fo-
cuses mainly on learning inference rules, but does so
without any manually specified seeds or constraints.
Craven et al(1998) also used ILP to help infor-
mation extraction on the Web, but required training
examples and focused on a single domain.
Two other notable systems that learn inference
rules from text are DIRT (Lin and Pantel, 2001)
and RESOLVER (Yates and Etzioni, 2007). How-
ever, both DIRT and RESOLVER learn only a lim-
ited set of rules capturing synonyms, paraphrases,
and simple entailments, not more expressive multi-
part Horn clauses. For example, these systems may
learn the rule X acquired Y =? X bought Y ,
which captures different ways of describing a pur-
chase. Applications of these rules often depend on
context (e.g., if a person acquires a skill, that does
not mean they bought the skill). To add the neces-
sary context, ISP (Pantel et al, 2007) learned selec-
tional preferences (Resnik, 1997) for DIRT?s rules.
The selectional preferences act as type restrictions
1089
Figure 1: Architecture of SHERLOCK. SHERLOCK learns
inference rules offline and provides them to the HOLMES
inference engine, which uses the rules to answer queries.
on the arguments, and attempt to filter out incorrect
inferences. While these approaches are useful, they
are strictly more limited than the rules learned by
SHERLOCK.
The Recognizing Textual Entailment (RTE)
task (Dagan et al, 2005) is to determine whether
one sentence entails another. Approaches to RTE
include those of Tatu and Moldovan (2007), which
generates inference rules from WordNet lexical
chains and a set of axiom templates, and Pennac-
chiotti and Zanzotto (2007), which learns inference
rules based on similarity across entailment pairs. In
contrast with this work, RTE systems reason over
full sentences, but benefit by being given the sen-
tences and training data. SHERLOCK operates over
simpler Web extractions, but is not given guidance
about which facts may interact.
3 System Description
SHERLOCK takes as input a large set of open domain
facts, and returns a set of weighted Horn-clause in-
ference rules. Other systems (e.g., HOLMES) use the
rules to answer questions, infer additional facts, etc.
SHERLOCK?s basic architecture is depicted in
Figure 1. To learn inference rules, SHERLOCK per-
forms the following steps:
1. Identify a ?productive? set of classes and in-
stances of those classes
2. Discover relations between classes
3. Learn inference rules using the discovered rela-
tions and determine the confidence in each rule
The first two steps help deal with the synonyms,
homonyms, and noise present in open-domain the-
ories by identifying a smaller, cleaner, and more co-
hesive set of facts to learn rules over.
SHERLOCK learns inference rules from a collec-
tion of open-domain extractions produced by TEX-
TRUNNER (Banko et al, 2007). The rules learned
by SHERLOCK are input to an inference engine and
used to find answers to a user?s query. In this paper,
SHERLOCK utilizes HOLMES as its inference engine
when answering queries, and uses extracted facts
of the form R(arg1, arg2) provided by the authors
of TEXTRUNNER, but the techniques presented are
more broadly applicable.
3.1 Finding Classes and Instances
SHERLOCK first searches for a set of well-defined
classes and class instances. Instances of the same
class tend to behave similarly, so identifying a good
set of instances will make it easier to discover the
general properties of the entire class.
Options for identifying interesting classes include
manually created methods (WordNet (Miller et al,
1990)), textual patterns (Hearst, 1992), automated
clustering (Lin and Pantel, 2002), and combina-
tions (Snow et al, 2006). We use Hearst patterns
because they are simple, capture how classes and in-
stances are mentioned in Web text, and yield intu-
itive, explainable groups.
Hearst (1992) identified a set of textual patterns
which indicate hyponymy (e.g., ?Class such as In-
stance?). Using these patterns, we extracted 29 mil-
lion (instance, class) pairs from a large Web crawl.
We then cleaned them using word stemming, nor-
malization, and by dropping modifiers.
Unfortunately, the patterns make systematic er-
rors (e.g., extracting Canada as the name of a city
from the phrase ?Toronto, Canada and other cities.?)
To address this issue, we discard the low frequency
classes of each instance. This heuristic reduces the
noise due to systematic error while still capturing the
important senses of each word. Additionally, we use
the extraction frequency to estimate the probability
that a particular mention of an instance refers to each
of its potential classes (e.g., New York appears as a
city 40% of the time, a state 35% of the time, and a
place, area, or center the rest of the time).
1090
Ambiguity presents a significant obstacle when
learning inference rules. For example, the corpus
contains the sentences ?broccoli contains this vita-
min? and ?this vitamin prevents scurvy,? but it is un-
clear if the sentences refer to the same vitamin. The
two main sources of ambiguity we observed are ref-
erences to a more general class instead of a specific
instance (e.g., ?vitamin?), and references to a person
by only their first or last name. We eliminate the
first by removing terms that frequently appear as the
class name with other instances, and the second by
removing common first and last names.
The 250 most frequently mentioned class names
include a large number of interesting classes (e.g.,
companies, cities, foods, nutrients, locations) as
well as ambiguous concepts (e.g., ideas, things). We
focus on the less ambiguous classes by eliminating
any class not appearing as a descendant of physical
entity, social group, physical condition, or event in
WordNet. Beyond this filtering we make no use of a
type hierarchy and treat classes independently.
In our corpus, we identify 1.1 million distinct,
cleaned (instance, class) pairs for 156 classes.
3.2 Discovering Relations between Classes
Next, SHERLOCK discovers how classes relate to
and interact with each other. Prior work in relation
discovery (Shinyama and Sekine, 2006) has investi-
gated the problem of finding relationships between
different classes. However, the goal of this work is
to learn rules on top of the discovered typed rela-
tions. We use a few simple heuristics to automati-
cally identify interesting relations.
For every pair of classes (C1, C2), we find a set
of typed, candidate relations from the 100 most fre-
quent relations in the corpus where the first argu-
ment is an instance of C1 and the second argument
is an instance of C2. For extraction terms with mul-
tiple senses (e.g., New York), we split their weight
based on how frequently they appear with each class
in the Hearst patterns.
However, many discovered relations are rare and
meaningless, arising from either an extraction error
or word-sense ambiguity. For example, the extrac-
tion ?Apple is based in Cupertino? gives some evi-
dence that a fruit may possibly be based in a city.
We attempt to filter out incorrectly-typed relations
using two heuristics. We first discard any relation
whose weighted frequency falls below a threshold,
since rare relations are more likely to arise due to
extraction errors or word-sense ambiguity. We also
remove relations whose pointwise mutual informa-
tion (PMI) is below a threshold T=exp(2) ? 7.4:
PMI(R(C1, C2)) =
p(R,C1, C2)
p(R, ?, ?) ? p(?, C1, ?) ? p(?, ?, C2)
where p(R, ?, ?) is the probability a random extrac-
tion has relation R, p(?, C1, ?) is the probability a
random extraction has an instance of C1 as its first
argument, p(?, ?, C2) is similar for the second argu-
ment, and p(R,C1, C2) is the probability that a ran-
dom extraction has relation R and instances of C1
and C2 as its first and second arguments, respec-
tively. A low PMI indicates the relation occurred by
random chance, which is typically due to ambiguous
terms or extraction errors.
Finally, we use two TEXTRUNNER specific clean-
ing heuristics: we ignore a small set of stop-relations
(?be?, ?have?, and ?be preposition?) and extractions
whose arguments are more than four tokens apart.
This process identifies 10,000 typed relations.
3.3 Learning Inference Rules
SHERLOCK attempts to learn inference rules for
each typed relation in turn. SHERLOCK receives a
target relation, R, a set of observed examples of the
relation, E+, a maximum clause length k, a mini-
mum support, s, and an acceptance threshold, t, as
input. SHERLOCK generates all first-order, definite
clauses up to length k, where R appears as the head
of the clause. It retains each clause that:
1. Contains no unbound variables
2. Infers at least s examples from E+
3. Scores at least t according to the score function
We now propose a novel score function, and empir-
ically validate our choice in Sections 4.3 and 4.4.
3.4 Evaluating Rules by Statistical Relevance
The problem of evaluating candidate rules has been
studied by many researchers, but typically in either a
supervised or propositional context whereas we are
learning first-order Horn-clauses from a noisy set of
positive examples. Moreover, due to the incomplete
nature of the input corpus and the imperfect yield of
1091
extraction?many true facts are not stated explicitly
in the set of ground assertions used by the learner to
evaluate rules.
The absence of negative examples, coupled with
noise, means that standard ILP evaluation functions
(e.g., (Quinlan, 1990) and (Dzeroski and Bratko,
1992)) are not appropriate. Furthermore, when eval-
uating a particular rule with consequent C and an-
tecedent A, it is natural to consider p(C|A) but, due
to missing data, this absolute probability estimate is
often misleading: in many cases C will hold given
A but the fact C is not mentioned in the corpus.
Thus to evaluate rules over extractions, we need
to consider relative probability estimates. I.e., is
p(C|A)  p(C)? If so, then A is said to be sta-
tistically relevant to C (Salmon et al, 1971).
Statistical relevance tries to infer the simplest set
of factors which explain an observation. It can be
viewed as searching for the simplest propositional
Horn-clause which increases the likelihood of a goal
proposition g. The two key ideas in determining sta-
tistical relevance are discovering factors which sub-
stantially increase the likelihood of g (even if the
probabilities are small in an absolute sense), and dis-
missing irrelevant factors.
To illustrate these concepts, consider the follow-
ing example. Suppose our goal is to predict if New
York City will have a storm (S). On an arbitrary
day, the probability of having a storm is fairly low
(p(S)  1). However, if we know that the atmo-
spheric pressure on that day is low, this substantially
increases the probability of having a storm (although
that absolute probability may still be small). Ac-
cording to the principle of statistical relevance, low
atmospheric pressure (LP ) is a factor which predicts
storms (S :- LP ), since p(S|LP ) p(S) .
The principle of statistical relevance also identi-
fies and removes irrelevant factors. For example, let
M denote the gender of New York?s mayor. Since
p(S|LP,M) p(S), it na??vely appears that storms
in New York depend on the gender of the mayor in
addition to the air pressure. The statistical relevance
principle sidesteps this trap by removing any fac-
tors which are conditionally independent of the goal,
given the remaining factors. For example, we ob-
serve p(S|LP )=p(S|LP,M), and so we say that M
is not statistically relevant to S. This test applies Oc-
cam?s razor by searching for the simplest rule which
explains the goal.
Statistical relevance appears useful in the open-
domain context, since all the necessary probabilities
can be estimated from only positive examples. Fur-
thermore, approximating relative probabilities in the
presence of missing data is much more reliable than
determining absolute probabilities.
Unfortunately, Salmon defined statistical rele-
vance in a propositional context. One technical
contribution of our work is to lift statistical rele-
vance to first order Horn-clauses as follows. For
the Horn-clause Head(v1, ..., vn):-Body(v1, ..., vm)
(where Body(v1, ..., vm) is a conjunction of function-
free, non-negated, first-order relations, and vi ? V
is the set of typed variables used in the rule), we say
the body helps explain the head if:
1. Observing an instance of the body substantially
increases the probability of observing the head.
2. The body contains no irrelevant (conditionally
independent) terms.
We evaluate conditional independence of terms
using ILP?s technique of ?-subsumption, ensuring
there is no more general clause that is similarly
predictive of the head. Formally, clause C1 ?-
subsumes clause C2 if and only if there exists a sub-
stitution ? such thatC1? ? C2 where each clause is
treated as the set of its literals. For example, R(x, y)
?-subsumes R(x, x), since {R(x, y)}? ? {R(x, x)}
when ?={y/x}. Intuitively, if C1 ?-subsumes C2,
it means that C1 is more general than C2.
Definition 1 A first-order Horn-clause
Head(...):-Body(...) is statistically relevant if
p(Head(...)|Body(...))  p(Head(...)) and if there
is no clause body B?(...)? ? Body(...) such that
p(Head(...)|Body(...)) ? p(Head(...)|B?(...))
In practice it is difficult to determine the proba-
bilities exactly, so when checking for statistical rele-
vance we ensure that the probability of the rule is at
least a factor t greater than the probability of any
subsuming rule, that is, p(Head(...)|Body(...)) ?
t ? p(Head(...)|B?(...))
We estimate p(Head(...)|B(...)) from the observed
facts by assuming values of Head(...) are generated
by sampling values of B(...) as follows: for variables
vs shared between Head(...) and B(...), we sample
1092
values of vs uniformly from all observed ground-
ings of B(...). For variables vi, if any, that appear
in Head(...) but not in B(...), we sample their values
according to a distribution p(vi|classi). We estimate
p(vi|classi) based on the relative frequency that vi
was extracted using a Hearst pattern with classi.
Finally, we ensure the differences are statistically
significant using the likelihood ratio statistic:
2Nr
X
H(...)?
{Head(...),?Head(...)}
p(H(...)|Body(...)) ? log
p(H(...)|Body(...))
p(H(...)|B?(...))
where p(?Head(...)|B(...)) = 1?p(Head(...)|B(...))
and Nr is the number of results inferred by the
rule Head(...):-Body(...). This test is distributed ap-
proximately as ?2 with one degree of freedom. It
is similar to the statistical significance test used in
mFOIL (Dzeroski and Bratko, 1992), but has two
modifications since SHERLOCK doesn?t have train-
ing data. In lieu of positive and negative examples,
we use whether or not the inferred head value was
observed, and compare against the distribution of a
subsuming clause B?(...) rather than a known prior.
This method of evaluating rules has two impor-
tant differences from ILP under a closed world as-
sumption. First, our probability estimates consider
the fact that examples provide varying amounts of
information. Second, statistical relevance finds rules
with large increases in relative probability, not nec-
essarily a large absolute probability. This is crucial
in an open domain setting where most facts are false,
which means the trivial rule that everything is false
will have high accuracy. Even for true rules, the ob-
served estimates p(Head(...)|Body(...))  1 due to
missing data and noise.
3.5 Making Inferences
In order to benefit from learned rules, we need
an inference engine; with its linear-time scalabil-
ity, HOLMES is a natural choice (Schoenmackers
et al, 2008). As input HOLMES requires a target
atom H(...), an evidence set E and weighted rule
set R as input. It performs a form of knowledge
based model construction (Wellman et al, 1992),
first finding facts using logical inference, then esti-
mating the confidence of each using a Markov Logic
Network (Richardson and Domingos, 2006).
Prior to running inference, it is necessary to assign
a weight to each rule learned by SHERLOCK. Since
the rules and inferences are based on a set of noisy
and incomplete extractions, the algorithms used for
both weight learning and inference should capture
the following characteristics of our problem:
C1. Any arbitrary unknown fact is highly unlikely
to be true.
C2. The more frequently a fact is extracted from the
Web, the more likely it is to be true. However,
facts in E should have a confidence bounded
by a threshold pmax < 1. E contains system-
atic extraction errors, so we want uncertainty in
even the most frequently extracted facts.
C3. An inference that combines uncertain facts
should be less likely than each fact it uses.
Next, we describe the needed modifications to the
weight learning and inference algorithm to achieve
the desired behavior.
3.5.1 Weight Learning
We use the discriminative weight learning proce-
dure described by Huynh and Mooney (2008). Set-
ting the weights involves counting the number of
true groundings for each rule in the data (Richard-
son and Domingos, 2006). However, the noisy na-
ture of Web extractions will make this an overesti-
mate. Consequently, we compute ni(E), the number
of true groundings of rule i, as follows:
ni(E) =
?
j
max
k
?
B(...)?Bodyijk
p(B(...)) (1)
where E is the evidence, j ranges over heads of the
rule, Bodyijk is the body of the kth grounding for
jth head of rule i, and p(B(...)) is approximated us-
ing a logistic function of the number of times B(...)
was extracted,1 scaled to be in the range [0,0.75].
This models C2 by giving increasing but bounded
confidence for more frequently extracted facts. In
practice, this also helps address C3 by giving longer
rules smaller values of ni, which reflects that infer-
ences arrived at through a combination of multiple,
noisy facts should have lower confidence. Longer
rules are also more likely to have multiple ground-
ings that infer a particular head, so keeping only the
most likely grounding prevents a head from receiv-
ing undue weight from any single rule.
1We note that this approximation is equivalent to an MLN
which uses only the two rules defined in 3.5.2
1093
Finally, we place a very strong Gaussian prior
(i.e., L2 penalty) on the weights. Longer rules have a
higher prior to capture the notion that they are more
likely to make incorrect inferences. Without a high
prior, each rule would receive an unduly high weight
as we have no negative examples.
3.5.2 Probabilistic Inference
After learning the weights, we add the following
two rules to our rule set:
1. H(...) with negative weight wprior
2. H(...):-ExtractedFrom(H(...),sentencei)
with weight 1
The first rule models C1, by saying that most facts
are false. The second rule models C2, by stating the
probability of fact depends on the number of times it
was extracted. The weights of these rules are fixed.
We do not include these rules during weight learning
as doing so swamps the effects of the other inference
rules (i.e., forces them to zero).
HOLMES attempts to infer the truth value of each
ground atom H(...) in turn by treating all other ex-
tractionsE in our corpus as evidence. Inference also
requires computing ni(E) which we do according to
Equation 1 as in weight learning.
4 Experiments
One can attempt to evaluate a rule learner by esti-
mating the quality of learned rules, or by measuring
their impact on a system that uses the learned rules.
Since the notion of ?rule quality? is vague except
in the context of an application, we evaluate SHER-
LOCK in the context of the HOLMES inference-based
question answering system.
Our evaluation focuses on three main questions:
1. Does inference utilizing learned Horn rules im-
prove the precision/recall of question answer-
ing and by how much?
2. How do different rule-scoring functions affect
the performance of learning?
3. What role does each of SHERLOCK?s compo-
nents have in the resulting performance?
4.1 Methodology
Our objective with rule learning was to improve the
system?s ability to answer questions such as ?What
foods prevent disease?? So we focus our evaluation
on the task of computing as many instances as pos-
sible of an atomic pattern Rel(x, y). In this exam-
ple, Rel would be bound to ?Prevents?, xwould have
type ?Food? and y would have type ?Disease.?
But which relations should be used in the test?
There is a large variance in behavior across relations,
so examining any particular relation may give mis-
leading results. Instead, we examine the global per-
formance of the system by querying HOLMES for
all open-domain relations identified in Section 3.2
as follows:
1. Score all candidate rules according to the rule
scoring metric M , accept all rules with a score
at least tM (tuned on a small development set of
rules), and learn weights for all accepted rules.
2. Find all facts inferred by the rules and use the
rule weights to estimate the fact probabilities.
3. Reduce type information. For each fact, (e.g.,
BasedIn(Diebold, Ohio)) which has been de-
duced with multiple type signatures (e.g., Ohio
is both a state and a geographic location), keep
only the one with maximum probability (i.e.,
conservatively assuming dependence).
4. Place all results into bins based on their proba-
bilities, and estimate the precision and the num-
ber of correct facts in the bin using a random
sample.
In these experiments we consider rules with up to
k = 2 relations in the body. We use a corpus of
1 million raw extractions, corresponding to 250,000
distinct facts. SHERLOCK found 5 million candidate
rules that infer at least two of the observed facts. Un-
less otherwise noted, we use SHERLOCK?s rule scor-
ing function to evaluate the rules (Section 3.4).
The results represent a wide variety of domains,
covering a total of 10,672 typed relations. We ob-
serve between a dozen and 2,375 distinct, ground
facts for each relation. SHERLOCK learned a total
of 31,000 inference rules.2 Learning all rules, rule
2The learned rules are available at:
http://www.cs.washington.edu/research/sherlock-hornclauses/
1094
 0
 0.2
 0.4
 0.6
 0.8
 1
0 350000 700000 1050000 1400000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Benefits of Inference using Learned Rules
Sherlock With Complex Rules
Sherlock With Only Simple Entailments
No Inference
Inferred by Simple
Entailment Rules
Inferred by 
Multi-Part
Horn Rules
Extracted
Facts
Figure 2: Inference discovers many facts which are not
explicitly extracted, identifying 3x as many high quality
facts (precision 0.8) and more than 5x as many facts over-
all. Horn-clauses with multiple relations in the body in-
fer 30% more correct facts than are identified by simpler
entailment rules, inferring many facts not present in the
corpus in any form.
weights, and performing the inference took 50 min-
utes on a 72 core cluster. However, we note that for
half of the relations SHERLOCK accepts no inference
rules, and remind the reader that the performance on
any particular relation may be substantially differ-
ent, and depends on the facts observed in the corpus
and on the rules learned.
4.2 Benefits of Inference
We first evaluate the utility of the learned Horn rules
by contrasting the precision and number of correct
and incorrect facts identified with and without infer-
ence over learned rules. We compare against two
simpler variants of SHERLOCK. The first is a no-
inference baseline that uses no rules, returning only
facts that are explicitly extracted. The second base-
line only accepts rules of length k = 1, allowing it to
make simple entailments but not more complicated
inferences using multiple facts.
Figure 2 compares the precision and estimated
number of correct facts with and without inference.
As is apparent, the learned inference rules substan-
tially increase the number of known facts, quadru-
pling the number of correct facts beyond what are
explicitly extracted.
The Horn rules having a body-length of two iden-
tify 30% more facts than the simpler length-one
rules. Furthermore, we find the Horn rules yield
slightly increased precision at comparable levels of
recall, although the increase is not statistically sig-
nificant. This behavior can be attributed to learn-
ing smaller weights for the length-two rules than
the length-one rules, allowing the length-two rules
provide a small amount of additional evidence as
to which facts are true, but typically not enough to
overcome the confidence of a more reliable length-
one rule.
Analyzing the errors, we found that about
one third of SHERLOCK?s mistakes are due
to metonymy and word sense ambiguity (e.g.,
confusing Vancouver, British Columbia with
Vancouver, Washington), one third are due to
inferences based on incorrectly-extracted facts
(e.g., inferences based on the incorrect fact
IsLocatedIn(New York, Suffolk County),
which was extracted from sentences like ?Deer
Park, New York is located in Suffolk County?),
and the rest are due to unsound or incorrect
inference rules (e.g., BasedIn(Company, City):-
BasedIn(Company, Country)? CapitalOf(City,
Country)). Without negative examples it is difficult
to distinguish correct rules from these unsound
rules, since the unsound rules are correct more often
than expected by chance.
Finally, we note that although simple, length-one
rules capture many of the results, in some respects
they are just rephrasing facts that are extracted in
another form. However, the more complex, length-
two rules synthesize facts extracted from multiple
pages, and infer results that are not stated anywhere
in the corpus.
4.3 Effect of Scoring Function
We now examine how SHERLOCK?s rule scoring
function affects its results, by comparing it with
three rule scoring functions used in prior work:
LIME. The LIME ILP system (McCreath and
Sharma, 1997) proposed a metric that generalized
Muggleton?s (1997) positive-only score function
by modeling noise and limited sample sizes.
M-Estimate of rule precision. This is a common
approach for handling noise in ILP (Dzeroski and
Bratko, 1992). It requires negative examples,
which we generated by randomly swapping argu-
ments between positive examples.
1095
 0
 0.2
 0.4
 0.6
 0.8
 1
0 500000 1000000 1500000 2000000 2500000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Comparison of Rule Scoring Functions
Sherlock
LIME
M-Estimate
L1 Reg.
Figure 3: SHERLOCK identifies rules that lead to more
accurate inferences over a large set of open-domain ex-
tracted facts, deducing 2x as many facts at precision 0.8.
L1 Regularization. As proposed in (Huynh and
Mooney, 2008), this learns weights for all can-
didate rules using L1-regularization (encouraging
sparsity) instead of L2-regularization, and retains
only those with non-zero weight.
Figure 3 compares the precision and estimated
number of correct facts inferred by the rules of
each scoring function. SHERLOCK has consistently
higher precision, and finds twice as many correct
facts at precision 0.8.
M-Estimate accepted eight times as many rules as
SHERLOCK, increasing the number of inferred facts
at the cost of precision and longer inference times.
Most of the errors in M-Estimate and L1 Regulariza-
tion come from incorrect or unsound rules, whereas
most of the errors for LIME stem from systematic
extraction errors.
4.4 Scoring Function Design Decisions
SHERLOCK requires a rule to have statistical rele-
vance and statistical significance. We perform an
ablation study to understand how each of these con-
tribute to SHERLOCK?s results.
Figure 4 compares the precision and estimated
number of correct facts obtained when requiring
rules to be only statistically relevant, only statisti-
cally significant, or both. As is expected, there is
a precision/recall tradeoff. SHERLOCK has higher
precision, finding more than twice as many results at
precision 0.8 and reducing the error by 39% at a re-
call of 1 million correct facts. Statistical significance
finds twice as many correct facts as SHERLOCK, but
the extra facts it discovers have precision < 0.4.
 0
 0.2
 0.4
 0.6
 0.8
 1
0 500000 1000000 1500000 2000000 2500000 3000000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Design Decisions of Sherlock?s Scoring Function
Sherlock
Statistical Relevance
Statistical Significance
Figure 4: By requiring rules to have both statistical rel-
evance and statistical significance, SHERLOCK rejects
many error-prone rules that are accepted by the metrics
individually. The better rule set yields more accurate in-
ferences, but identifies fewer correct facts.
Comparing the rules accepted in each case, we
found that statistical relevance and statistical signifi-
cance each accepted about 180,000 rules, compared
to about 31,000 for SHERLOCK. The smaller set
of rules accepted by SHERLOCK not only leads to
higher precision inferences, but also speeds up in-
ference time by a factor of seven.
In a qualitative analysis, we found the statisti-
cal relevance metric overestimates probabilities for
sparse rules, which leads to a number of very high
scoring but meaningless rules. The statistical signif-
icance metric handles sparse rules better, but is still
overconfident in the case of many unsound rules.
4.5 Analysis of Weight Learning
Finally, we empirically validate the modifications of
the weight learning algorithm from Section 3.5.1.
The learned-rule weights only affect the probabil-
ities of the inferred facts, not the inferred facts them-
selves, so to measure the influence of the weight
learning algorithm we examine the recall at preci-
sion 0.8 and the area under the precision-recall curve
(AuC). We build a test set by holding SHERLOCK?s
inference rules constant and randomly sampling 700
inferred facts. We test the effects of:
? Fixed vs. Variable Penalty - Do we use the
same L2 penalty on the weights for all rules or
a stronger L2 penalty for longer rules?
? Full vs. Weighted Grounding Counts - Do we
count all unweighted rule groundings (as in
(Huynh and Mooney, 2008)), or only the best
weighted one (as in Equation 1)?
1096
Recall
(p=0.8) AuC
Variable Penalty, Weighted 0.35 0.735
Counts (used by SHERLOCK)
Variable Penalty, Full Counts 0.28 0.726
Fixed Penalty, Weighted Counts 0.27 0.675
Fixed Penalty, Full Counts 0.17 0.488
Table 2: SHERLOCK?s modified weight learning algo-
rithm gives better probability estimates over noisy and in-
complete Web extractions. Most of the gains come from
penalizing longer rules more, but using weighted ground-
ing counts further improves recall by 0.07, which corre-
sponds to almost 100,000 additional facts at precision 0.8.
We vary each of these independently, and give the
performance of all 4 combinations in Table 2.
The modifications from Section 3.5.1 improve
both the AuC and the recall at precision 0.8. Most
of the improvement is due to using stronger penal-
ties on longer rules, but using the weighted counts
in Equation 1 improves recall by a factor of 1.25 at
precision 0.8. While this may not seem like much,
the scale is such that it leads to almost 100,000 ad-
ditional correct facts at precision 0.8.
5 Conclusion
This paper addressed the problem of learning first-
order Horn clauses from the noisy and heteroge-
neous corpus of open-domain facts extracted from
Web text. We showed that SHERLOCK is able
to learn Horn clauses in a large-scale, domain-
independent manner. Furthermore, the learned rules
are valuable, because they infer a substantial number
of facts which were not extracted from the corpus.
While SHERLOCK belongs to the broad category
of ILP learners, it has a number of novel features that
enable it to succeed in the challenging, open-domain
context. First, SHERLOCK automatically identifies
a set of high-quality extracted facts, using several
simple but effective heuristics to defeat noise and
ambiguity. Second, SHERLOCK is unsupervised and
does not require negative examples; this enables it to
scale to an unbounded number of relations. Third, it
utilizes a novel rule-scoring function, which is toler-
ant of the noise, ambiguity, and missing data issues
prevalent in facts extracted from Web text. The ex-
periments in Figure 3 show that, for open-domain
facts, SHERLOCK?s method represents a substantial
improvement over traditional ILP scoring functions.
Directions for future work include inducing
longer inference rules, investigating better methods
for combining the rules, allowing deeper inferences
across multiple rules, evaluating our system on other
corpora and devising better techniques for handling
word sense ambiguity.
Acknowledgements
We thank Sonal Gupta and the anonymous review-
ers for their helpful comments. This research was
supported in part by NSF grant IIS-0803481, ONR
grant N00014-08-1-0431, the WRF / TJ Cable Pro-
fessorship and carried out at the University of Wash-
ington?s Turing Center. The University of Washing-
ton gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract nos. FA8750-
09-C-0179 and FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
M. Craven, D. DiPasquo, D. Freitag, A.K. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to Extract Symbolic Knowledge from the World
Wide Web. In Procs. of the 15th Conference of the
American Association for Artificial Intelligence, pages
509?516, Madison, US. AAAI Press, Menlo Park, US.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
S. Dzeroski and I. Bratko. 1992. Handling noise in in-
ductive logic programming. In Proceedings of the 2nd
1097
International Workshop on Inductive Logic Program-
ming.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
structure and parameter learning for Markov logic net-
works. In Proceedings of the 25th international con-
ference on Machine learning, pages 416?423. ACM.
Stanley Kok and Pedro Domingos. 2005. Learning the
structure of markov logic networks. In ICML ?05:
Proceedings of the 22nd international conference on
Machine learning, pages 441?448, New York, NY,
USA. ACM.
N. Lavrac and S. Dzeroski, editors. 2001. Relational
Data Mining. Springer-Verlag, Berlin, September.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Infer-
ence Rules from Text. In KDD.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
E. McCreath and A. Sharma. 1997. ILP with noise
and fixed example size: a Bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on Artifical intelligence-Volume 2, pages 1310?1315.
Morgan Kaufmann Publishers Inc.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
S. Muggleton. 1995. Inverse entailment and Progol.
New Generation Computing, 13:245?286.
S. Muggleton. 1997. Learning from positive data. Lec-
ture Notes in Computer Science, 1314:358?376.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and
E. Hovy. 2007. ISP: Learning inferential selectional
preferences. In Proceedings of NAACL HLT, vol-
ume 7, pages 564?571.
M. Pennacchiotti and F.M. Zanzotto. 2007. Learning
Shallow Semantic Rules for Textual Entailment. Pro-
ceedings of RANLP 2007.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5:239?2666.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
W.C. Salmon, R.C. Jeffrey, and J.G. Greeno. 1971. Sta-
tistical explanation & statistical relevance. Univ of
Pittsburgh Pr.
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing Textual Inference to the Web. In Procs. of EMNLP.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL 2006.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing, pages 22?27.
M.P. Wellman, J.S. Breese, and R.P. Goldman. 1992.
From knowledge bases to decision models. The
Knowledge Engineering Review, 7(1):35?53.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
1098
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289?299,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Coreference Resolution and Named-Entity Linking
with Multi-pass Sieves
Hannaneh Hajishirzi Leila Zilles Daniel S. Weld Luke Zettlemoyer
Department of Computer Science and Electrical Engineering
University of Washington
{hannaneh,lzilles,lsz,weld}@cs.washington.edu
Abstract
Many errors in coreference resolution come
from semantic mismatches due to inadequate
world knowledge. Errors in named-entity
linking (NEL), on the other hand, are of-
ten caused by superficial modeling of entity
context. This paper demonstrates that these
two tasks are complementary. We introduce
NECO, a new model for named entity linking
and coreference resolution, which solves both
problems jointly, reducing the errors made on
each. NECO extends the Stanford determinis-
tic coreference system by automatically link-
ing mentions to Wikipedia and introducing
new NEL-informed mention-merging sieves.
Linking improves mention-detection and en-
ables new semantic attributes to be incorpo-
rated from Freebase, while coreference pro-
vides better context modeling by propagat-
ing named-entity links within mention clus-
ters. Experiments show consistent improve-
ments across a number of datasets and ex-
perimental conditions, including over 11% re-
duction in MUC coreference error and nearly
21% reduction in F1 NEL error on ACE 2004
newswire data.
1 Introduction
Coreference resolution and named-entity linking are
closely related problems, but have been largely stud-
ied in isolation. This paper demonstrates that they
are complementary by introducing a simple joint
model that improves performance on both tasks.
Coreference resolution is the task of determining
when two textual mentions name the same individ-
[Michael Eisner]1 and [Donald Tsang]2 announced the
grand opening of [[Hong Kong]3 Disneyland]4 yester-
day. [Eisner]1 thanked [the President]2 and welcomed
[fans]5 to [the park]4.
Figure 1: A text passage illustrating interactions between
coreference resolution and NEL.
ual. The biggest challenge in coreference resolu-
tion ? accounting for 42% of errors in the state-
of-the-art Stanford system ? is the inability to rea-
son effectively about background semantic knowl-
edge (Lee et al, 2013). For example, consider the
sentence in Figure 1. ?President? refers to ?Donald
Tsang? and ?the park? refers to ?Hong Kong Dis-
neyland,? but automated algorithms typically lack
the background knowledge to draw such inferences.
Incorporating knowledge is challenging, and many
efforts to do so have actually hurt performance,
e.g. (Lee et al, 2011; Durrett and Klein, 2013).
Named-entity linking (NEL) is the task of match-
ing textual mentions to corresponding entities in a
knowledge base, such as Wikipedia or Freebase.
Such links provide rich sources of semantic knowl-
edge about entity attributes ? Freebase includes
president as Tsang?s title and Disneyland as hav-
ing the attribute park. But NEL is itself a chal-
lenging problem, and finding the correct link re-
quires disambiguating based on the mention string
and often non-local contextual features. For exam-
ple, ?Michael Eisner? is relatively unambiguous but
the isolated mention ?Eisner? is more challenging.
However, these mentions could be clustered with
a coreference model, allowing for improved NEL
through link propagation from the easier mentions.
289
We present NECO, a new algorithm for jointly solv-
ing named entity linking and coreference resolu-
tion. Our work is related to that of Ratinov and
Roth (2012), which also uses knowledge derived
from an NEL system to improve coreference. How-
ever, NECO is the first joint model we know of, is
purely deterministic with no learning phase, does
automatic mention detection, and improves perfor-
mance on both tasks.
NECO extends the Stanford?s sieve-based model,
in which a high recall mention detection phase is
followed by a sequence of cluster merging opera-
tions ordered by decreasing precision (Raghunathan
et al, 2010; Lee et al, 2013). At each step, it
merges two clusters only if all available information
about their respective entities is consistent. We use
NEL to increase recall during the mention detection
phase and introduce two new cluster-merging sieves,
which compare the Freebase attributes of entities.
NECO also improves NEL by initially favoring high
precision linking results and then propagating links
and attributes as clusters are formed.
In summary we make the following contributions:
? We introduce NECO, a novel, joint approach
to solving coreference and NEL, demonstrating
that these tasks are complementary by achiev-
ing joint error reduction.
? We present experiments showing improved per-
formance at coreference resolution, given both
gold and automatic mention detection: e.g.,
6.2 point improvement in MUC recall on ACE
2004 newswire text and 3.1 point improvement
in MUC precision the CoNLL 2011 test set.
? NECO also leads to better performance at
named-entity linking, given both gold and au-
tomatic linking, improving F1 from 61.7% to
69.2% on a newly labeled test set.1
2 Background
We make use of existing models for coreference res-
olution and named entity linking.
1Our corpus and the source code for NECO can be down-
loaded from https://www.cs.washington.edu/
research-projects/nlp/neco.
2.1 Coreference Resolution
Coreference resolution is the the task of identifying
all text spans (called mentions) that refer to the same
entity, forming mention clusters.
Stanford?s SieveModel is a state-of-the-art coref-
erence resolver comprising a pipeline of ?sieves?
that merge coreferent mentions according to deter-
ministic rules. Mentions are automatically predicted
by selecting all noun phrases (NP), pronouns, and
named entities. Each sieve either merges a cluster
to its single best antecedent from a list of previous
clusters, or declines to merge.
Higher precision sieves are applied earlier in the
pipeline according to the following order, looking at
different aspects of the text, including: (1) speaker
identification, (2-3) exact and relaxed string matches
between mentions, (4) precise constructs, including
appositives, acronyms and demonyms, (5-9) differ-
ent notions of strict and relaxed head matches be-
tween mentions, and finally (10) a number of syn-
tactic and distance cues for pronoun resolution.
2.2 Named Entity Linking
Named-entity linking (NEL) is the task of identi-
fying mentions in a text and linking them to the
entity they name in a knowledge base, usually
Wikipedia. NECO uses two existing NEL sys-
tems: GLOW (Ratinov et al, 2011) and Wikipedi-
aMiner (Milne and Witten, 2008).
WikipediaMiner links mentions based on a notion
of semantic similarity to Wikipedia pages, consider-
ing all substrings up to a fixed length. Since there
are often many possible links, it disambiguates by
choosing the entity whose Wikipedia page is most
semantically related to the nearby context of the
mention. The semantic scoring function includes n-
gram statistics and also counts shared links to other
unambiguous mentions in the text.
GLOW finds mentions by selecting all the NPs
and named entities in the text. Linking is framed
as an integer linear programming optimization prob-
lem that takes into account using similar local con-
straints but also includes global constraints such as
entity link co-occurrence.
Both systems return confidence values. To main-
tain high precision, NECO uses an ensemble of
290
? Let Exemplar(c) be a representative mention of the cluster c, computed as defined below
? Let cj be an antecedent cluster of ci if cj has a mention which is before the first mention of ci
? Let l(m) be a Wikipedia page linked to mention m or ? if there is no link
? Let l(c) be a Wikipedia page linked to mention Exemplar(c) or ? if there is no link
1. Initialize Linked Mentions:
(a) Let MNEL = {mi | i = 1 . . . p} be the NEL output mentions, mi, each with a link l(mi)
(b) Let MCR = {mi | i = 1 . . . q} be the mentions mi from coreference mention detection
(c) Let M ?MCR ?MNEL (Sec. 3.1)
(d) Update entity links for all m ?M and prune M (Sec. 3.2)
(e) Extract attributes from Wikipedia and Freebase for all m ?M (Sec. 3.3)
(f) Let C ?M be singleton mention clusters where Exemplar(ci) = mi, l(ci) = l(mi)
2. Merge Clusters: For every sieve S (including NEL sieves, Sec. 3.6) and cluster ci ? C
(a) For every cluster cj , j = [i? 1 . . . 1] (traverse the preceding clusters in reverse order)
i. NEL constraints: Prevent merge if l(ci) 6= l(cj) (Sec. 3.4)
ii. If all rules of sieve S are satisfied for clusters ci and cj
A. ck ? Merge(ci, cj), including entity link and attribute updates (Sec. 3.5)
B. C ? C ? {ck} \ {ci, cj}
3. Output: Coreference clusters C and linked Wikipedia pages l(ci)?ci ? C
Figure 2: NECO: A joint algorithm for named-entity linking and coreference resolution.
GLOW and WikipediaMiner, selecting only high
confidence links.
3 Joint Coreference and Linking
We introduce a joint model for coreference resolu-
tion and NEL. Building on the Stanford sieve ar-
chitecture, our algorithm incrementally constructs
clusters of mentions using deterministic coreference
rules under NEL constraints.
Figure 2 presents the complete algorithm. The in-
put to NECO is a document and the output is a set C
of coreference clusters, with links l(c) to Wikipedia
pages for a subset of the clusters c ? C. Step 1
detects mentions, merging the outputs of the base
systems (Sec. 3.1). Step 2 repeatedly merges coref-
erence clusters, while ensuring that NEL constraints
(Sec. 3.4) are satisfied. It uses the original Stan-
ford sieves and also two new NEL-informed sieves
(Sec. 3.6). NEL links are propagated to new clusters
as they are formed (Sec. 3.5).
3.1 Mention Detection
In Steps 1(a-c) in Fig. 2, NECO combines mentions
from the base coreference and NEL systems.
Let MCR be the set of mentions returned by us-
ing Stanford?s rule-based mention detection algo-
rithm (Lee et al, 2013). Let MNEL be the set of
mentions output by the two NEL systems. NECO
creates an initial set of mentions, M , by taking the
union of all the mentions in MNEL and MCR. In
practice, taking the union increases diversity in the
mention pool. For example, it is often the case that
MNEL will include sub-phrases such as ?Suharto?
when they are part of a larger mention ?ex-dictator
Suharto? that is detected in MCR.
3.2 Mention Entity Links and Pruning
Step 1(d) in Fig. 2 assigns Wikipedia links to a sub-
set of the detected mentions.
For mentions m output by the base NEL sys-
tems, we assign an exact link l(m) if the entire
mention span is linked. Mentions m? that differ
from an exact linked mention m by only a pre- or
post-fix stop word are similarly assigned exact links
l(m?) = l(m). For example, the mention ?the pres-
ident? will be assigned the same link as ?president?
but ?The governor of Alaska Sarah Palin? would not
be assigned an exact link to Sarah Palin.
For mentions m? that do not receive an exact link,
we assign a head link h(m?) if the head word2 m has
been linked, by setting h(m?) = l(m). For instance,
the head link for the mention ?President Clinton?
(with ?Clinton? as head word) will be the Wikipedia
title of Bill Clinton. We use head links for the
Relaxed NEL sieve (Sec. 3.6).
Next, we define L(m) to be the set con-
2A head word is assigned to every mention with the Stanford
parser head finding rules (Klein and Manning, 2003).
291
country president city area
company state region location
place agency power unit
body market park province
manager organization owner trial
site prosecutor attorney county
senator stadium network building
attraction government department person
origin plant airport kingdom
capital operation author period
nominee candidate film venue
Figure 3: The most commonly used fine-grained at-
tributes from Freebase and Wikipedia (out of over 500
total attributes).
taining l(m) and l(m?) for all sub-phrases m?
of m. We add the sub-phrase links only
if their confidence is higher than the confi-
dence for l(m). For instance, assuming ap-
propriate confidence values, L(m) would in-
clude the pages for {List of governors of
Alaska, Alaska, Sarah Palin} given the
mention ?The governor of Alaska Sarah Palin.? We
will use L(m) for NEL constraints and filtering
(Sec. 3.4).
After updating the entity links for all mentions,
NECO prunes spurious mentions that begin or
end with a stop word where the remaining sub-
expression of the mention exists in M . It also re-
moves time expressions and numbers from M if they
are not included in MNEL.
3.3 Mention Attributes
Step 1(e) in Fig. 2 also assigns attributes for a
mention m linked to Wikipedia page l(m), at both
coarse and fine-grained levels, based on information
from the Freebase entry corresponding to exact link
l(m) or head link h(m).
The coarse attributes include gender, type, and
NER classes such as PERSON, LOCATION, and OR-
GANIZATION. These attributes are part of the orig-
inal Stanford coreference system and are used to
avoid merging conflicting clusters. We use the Free-
base values for these attributes when available. For
instance, if the linked entity contains the Freebase
type location or organization, we include the coarse
type to LOCATION or ORGANIZATION respectively.
In order to account for both links to specific peo-
ple (Barack Obama) and generic links to positions
held by people (President), we include the type PER-
SON if the linked entity has any of the Freebase types
person, job title, or government office or title. If no
coarse Freebase types are available for an attribute,
we default to predicted NER classes.
We add fine-grained attributes from Freebase and
Wikipedia by importing additional type information.
We use all of the Freebase notable types, a set of
hundreds of commonly used Freebase types, rang-
ing from us president to tropical cyclone and syn-
thpop album. We also include all of the Wikipedia
categories, on average six per entity. For example,
the mention ?Indonesia? is assigned fine-grained at-
tributes such as book subject, military power, and
olympic participating country. Since many of these
fine-grained attributes are extremely specific, we use
the last word of each attribute to define an addi-
tional fine-grained attribute (see Fig. 3). These fine-
grained attributes are used in the Relaxed NEL sieve
(Sec. 3.6).
3.4 NEL Constraints
While applying sieves to merge clusters in Figure 2
Step 2(a), NECO uses NEL constraints to eliminate
some otherwise acceptable merges.
We avoid merging inconsistent clusters that link
to different entities. Clusters ci and cj are incon-
sistent if both are linked (i.e., both clusters have
non-null entity assignments) and l(ci) 6= l(cj) or
h(ci) 6= h(cj). Also, in order to consider an an-
tecedent cluster c as a merge candidate, we require a
pair of entities in the set of linked entities L(c) to be
related to one another in Freebase. Two entities are
related in Freebase if they both appear in a relation;
for example, Bill Clinton and Arkansas are
related because Bill Clinton has a ?governor-of? re-
lation with Arkansas.
3.5 Merging Clusters and Update Entity Links
When two clusters ci and cj are merged to form a
new cluster ck, the entity link information L(ck),
l(ck), and h(ck) must be updated (Step 2 of Fig. 2).
We set L(ck) to the union of the linked entities found
in l(ci) and l(cj) and merge coarse attributes at this
point.
In order to set the exact and head entity links
l(ck) and h(ck), we use the exemplar mention
292
Exemplar(ck) that denotes the most representative
mention of the cluster. Exemplar(c) is selected
according to a set of rules in the Stanford system,
based on textual position and mention type (proper
noun vs. common). We augment this function by
considering information from exact and head en-
tity links as well. Mentions appearing earlier in
text, proper mentions, and mentions that have ex-
act or head named-entity links are preferred to those
which do not. Given exemplars, we set l(ck) =
l(Exemplar(ck)) and h(ck) = h(Exemplar(ck)).
3.6 NEL Sieves
Finally, we introduce two new sieves that use NEL
information at the beginning and end of the Stan-
ford sieves pipeline in the merging stage (Step 2 of
Fig. 2).
Exact NEL sieve The Exact NEL sieve merges
two clusters ci and cj if both are linked and their
links match, l(ci) = l(cj). For example, all men-
tions that have been linked to Barack Obama will
become members of the same coreference cluster.
Because the Exact NEL sieve has high precision, we
place it at the very beginning of the pipeline.
Relaxed NEL sieve The Relaxed NEL sieve uses
fine-grained attributes of the linked mentions to
merge proper nouns with common nouns when they
share attributes. For example, this sieve is able to
merge the proper mention ?Disneyland? with the
?the mysterious park?, because park is one of the
fine-grained attributes assigned to Disneyland.
More formally, let mi = Exemplar(ci) and
mj = Exemplar(cj). For every common noun
mention mi, we merge ci with an antecedent clus-
ter cj if (1) mj is a linked proper noun, (2) if mi or
the title of its linked Wikipedia page is in the list of
fine-grained attributes of mj , or (3) if h(mj) is re-
lated to the head link h(mi) according to Freebase
as defined above.
Because this sieve has low precision, we only
allow merges between mentions that have a maxi-
mum distance of three sentences between one an-
other. We add the Relaxed NEL sieve near the end
of the pipeline, just before pronoun resolution.
4 Experimental Setup
Core Components and Baselines The Stanford
sieve-based coreference system (Lee et al, 2013),
the GLOW NEL system (Ratinov et al, 2011), and
WikipediaMiner (Milne and Witten, 2008) provide
core functionality for our joint model, and are also
the state-of-the-art baselines against which we mea-
sure performance.
Parameter Settings Based on performance on the
development set, we set the GLOW?s confidence pa-
rameter to 1.0 and WikipediaMiner?s to 0.4 to assure
high-precision NEL. We also optimized for the set of
fine-grained attributes to import from Wikipedia and
Freebase, and the best way to incorporate the NEL
constraints into the sieve architecture.
Datasets We report results on the following
three datasets: ACE????-NWIRE, CONLL????,
and ACE????-NWIRE-NEL. ACE????-NWIRE, the
newswire subset of the ACE 2004 corpus (NIST,
2004), includes 128 documents. The CONLL????
coreference dataset includes text from five different
domains: broadcast conversation (BC), broadcast
news (BN), magazine (MZ), newswire (NW), and
web data (WB) (Pradhan et al, 2011). The broadcast
conversation and broadcast news domains consist of
transcripts, whereas magazine and newswire contain
more standard written text. The development data
includes 303 documents and the test data includes
322 documents.
We created ACE????-NWIRE-NEL by taking a
subset of ACE????-NWIRE and annotating with
gold-standard entity links. We segment and link all
the expressions in text that refer to Wikipedia pages,
allowing for nested linking. For instance, both the
phrase ?Hong Kong Disneyland,? and the sub-phrase
?Hong Kong? are linked. This dataset includes 12
documents and 350 linked entities.
Metrics We evaluate our system using MUC (Vi-
lain et al, 1995), B3 (Bagga and Baldwin, 1998),
and pairwise scores. MUC is a link-based met-
ric which measures how many clusters need to be
merged to cover the gold clusters and favors larger
clusters; B3 computes the proportion of intersec-
tion between predicted and gold clusters for every
mention and favors singletons (Recasens and Hovy,
2010). We computed the scores using the Stanford
293
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6
NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4
No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1
No Mention Pruning 43.6 45.6 44.6 70.5 69.9 70.2 46.2 29.4 35.9
No Attributes 45.9 47.4 46.6 71.8 69.7 70.7 48.6 27.0 34.7
No Constraints 42.3 49.3 45.5 68.3 72.3 70.2 44.2 28.6 34.7
Table 1: Coreference results on ACE????-NWIRE with predicted mentions and automatic linking.
coreference software for ACE2004 and using the
CoNLL scorer for the CoNLL 2011 dataset.
5 Experimental Results
We first look at NECO?s performance at coreference
resolution and then evaluate its ability at NEL.
5.1 Coref. Results with Predicted Mentions
Overall System Performance on ACE Data Ta-
ble 1 shows NECO?s performance at coreference
resolution on ACE-???? compared to the Stanford
sieve implementation (Lee et al, 2013). The table
shows that NECO has both significantly improved
precision and recall compared to the Stanford base-
line, across all metrics. We generally observe larger
gains in MUC due to better mention detection and
the Relaxed NEL Sieve.
Contribution of System Components Table 1
also details the performance of four variants of our
system that ablate various components and features.
Specifically, we consider the following cases:
? No NEL Mentions: We discard additional
mentions, MNEL, provided by NEL (Sec. 3.1).
This increases B3 precision at the expense of
recall. Inspection shows that some of the errors
introduced by MNEL are actually due to cor-
rectly linked entities that were not annotated as
mentions in the dataset, but also some improp-
erly linked mentions.
? No Mention Pruning: We disable the initial
step of updating mention boundaries and re-
moving spurious mentions (Sec. 3.2). As ex-
pected, removing this step drops precision and
recall significantly, even compared to the No
NEL Mentions variant.
? No Attributes: Ablating coarse and fine-
grained attributes (Sec. 3.3) drops F1 and re-
call measures across all metrics. To under-
stand this effect, note that NECO uses at-
tributes in two different settings. Updating
coarse attributes tends to increase precision be-
cause it prevents dangerous merges, such as
merging ?Staples? with the mention ?it? in
a situation when ?Staples? refers to the per-
son entity Todd Staples. Fine-grained at-
tributes also help with recall, when merging
a specific name of an entity with a mention
that uses a more general term; for instance,
?Hong Kong Disneyland? can be merged with
?the mysterious park? because ?park? is a fine-
grained attribute for Disneyland. However,
when fine-grained attributes are used, precision
sometimes drops (e.g., when ?president? might
merge with ?Bush? when it should really merge
with ?Clinton?).
? No NEL Constraints: Removing these con-
straints (Sec. 3.4) drops precision dramatically
leading to drop in F1. In the case of incor-
rect linking, however, NEL constraints can af-
fect recall. For instance, NEL constraints might
prevent merging ?Staples? with ?Todd Staples?
if the former were linked to the company and
the latter to the politician.
Overall System Performance on CoNLL Data
We also compare our full system (with added NEL
sieves, constraints, and mention pruning3) with the
Stanford sieve coreference system on CoNLL data
3Due to CoNLL annotation guidelines, a named entity is
added to the mention list if it is not inside a larger mention with
an exact named entity link.
294
MUC B3
Category: Method P R F1 P R F1
BC: NECO 62.1 64.7 63.4 69.8 57.8 63.2
BC: Stanford Sieves 60.9 65.0 62.9 69.2 58.0 63.1
BN: NECO 69.3 59.4 64.0 78.8 60.8 68.6
BN: Stanford Sieves 68.0 58.9 63.1 79.0 60.2 68.3
MZ: NECO 67.6 62.9 65.2 78.4 61.1 68.7
MZ: Stanford Sieves 66.0 63.4 64.9 77.9 61.5 68.7
NW: NECO 62.0 54.5 58.0 74.9 57.4 65.0
NW: Stanford Sieves 60.0 54.2 56.9 75.3 57.0 64.9
Table 3: Coreference results on the individual categories of CoNLL 2011 development data. (BC=broadcast conver-
sation, BN=broadcast news, MZ=magazine, NW=newswire)
MUC B3
Method P R F1 P R F1
Development Data
NECO 64.1+ 59.4 61.7+ 74.7 58.7 65.7
Stanford 62.7 59.0 60.8 74.8 58.3 65.6
NECO* 56.4+ 50.0 53.0+ 72.6 51.6 60.3
Stanford* 53.5 50.0 51.6 71.8 51.3 59.9
Test Data
NECO 61.2+ 58.4 59.8+ 72.2 56.4 63.3
Stanford 59.2 58.8 59.0 71.3 56.1 62.8
NECO* 55.1+ 51.7 53.3+ 70.0 50.8 58.8
Stanford* 52.0 52.3+ 52.1 68.9 50.8 58.5
Table 2: Coreference results on CoNLL 2011 develop-
ment and test data, using predicted mentions. Rows de-
noted with * indicate runs using the fully automated Stan-
ford CoreNLP pipeline rather than the predicted annota-
tions provided with the CoNLL data. Given the relatively
close results, we ran the Mann-Whitney U test for this
table; values with the + superscript are significant with
p < 0.05.
(Table 2). We ran NECO and the baseline in two set-
tings: in the first, we use the standard predicted an-
notations (for POS, parses, NER, and speaker tags)
provided with the CoNLL data, and in the second,
we use the automated Stanford CoreNLP pipeline
to predict this information. On both the develop-
ment and test sets, we gain about 1 point in MUC
F1 as well as a smaller improvement in B3. Closer
inspection indicates that our system increases pre-
cision primarily due to mention pruning and NEL
constraints. Due to the differences in mention anno-
tation guidelines between ACE and CoNLL, perfor-
mance on ACE benefits more from improved men-
tion detection from NEL. Moreover, the ACE cor-
pus is all newswire text, which contains more enti-
ties that can benefit from linking. CoNLL, on the
other hand, contains a wider variety of texts, some
of which do not mention many named entities in
Wikipedia.
To examine the performance of our system on the
different domains covered by the CoNLL data, we
also test our system on each domain separately (Ta-
ble 3). We found NEL provided the biggest im-
provement for the news domains, broadcast news
(BN) and newswire (NW). These domains espe-
cially benefit from the improved mention detection
and pruning provided by NEL, and strong linking
benefitted both precision and recall in these do-
mains. We found that the magazine (MZ) section
of the corpus benefited the least from NEL, as there
were relatively few entities that our NEL systems
were able to connect to Wikipedia.
5.2 Coreference Results with Gold Linking
Some of the errors introduced in our system are due
to incorrect or incomplete links discovered by the
automatic linking system. To assess the effect of
NEL performance on NECO, we tested on a por-
tion of ACE????-NWIRE dataset for which we hand-
labeled correct links for the gold and predicted men-
tions. ?NECO + Gold NEL? denotes a version of our
system which uses gold links instead of those pre-
dicted by NEL. As shown in Table 4, gold linking
significantly improves the performance of our sys-
tem across all measures. This suggests that further
work to improve automatic NEL may have substan-
tial reward.
Gold linking improves precision for two main rea-
295
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Gold Mentions
NECO + Gold NEL 85.8 75.5 80.3 91.4 81.2 86.0 89.1 68.0 77.1
NECO 84.6 74.0 78.9 90.5 80.4 85.2 83.9 66.0 73.9
Stanford Sieves 84.5 72.2 77.8 89.9 77.7 83.4 89.9 57.3 68.1
Predicted Mentions
NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4
NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2
Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4
Table 4: Coreference results on ACE????-NWIRE-NEL with gold and predicted mentions and gold or automatic linking.
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8
Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4
Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7
Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0
Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9
Table 5: Coreference results on ACE????-NWIRE with gold mentions and automatic linking.
sons. First, it reduces the coreference errors caused
by incorrect NEL links. For instance, gold link-
ing replaces the erroneous link generated by our
NEL systems for ?Nasser al-Kidwa? to the correct
Wikipedia entity. As another example, two men-
tions of ?Rutgers? will not be merged if one links
to the university and the other links to their football
team. Second, gold linking leads to better mention
detection and better linked mentions. For instance,
under gold linking, the whole mention, ?The gover-
nor of Alaska, Sarah Palin,? is linked to the politi-
cian, while automatic linking systems only link the
substring containing her name, ?Sarah Palin.? Still,
gold NEL cannot compensate for all coreference er-
rors in cases of generic or unlinked entities.
5.3 Coreference Results with Gold Mentions
Many of the previous papers evaluate coreference
resolution assuming gold mentions so we also run
under that condition (Table 5) using ACE????-
NWIRE data. As the table shows, with gold mentions
our system outperforms Haghighi and Klein (2009),
Poon and Domingos (2008), Finkel and Man-
ning (2008) and the Stanford sieve algorithm across
all metrics. Our method shows a relatively smaller
gain in precision, because this condition adds no
benefit to our technique of using NEL information
for pruning mentions.
5.4 Improving Named Entity Linking
While our previous experiments show that named-
entity linking can improve coreference resolution,
we now address the question of whether coreference
techniques can help NEL. We compare NECO with
a baseline ensemble4 composed of GLOW (Ratinov
et al, 2011) and WikipediaMiner (Milne and Witten,
2008) on our ACE????-NWIRE-NEL dataset (Table
6). Our system gains about 8% in absolute recall
and 5% in absolute precision. For instance, our sys-
tem correctly adds links from ?Bullock? to the en-
tity Sandra Bullock because coreference reso-
lution merges two mentions. In another example, it
correctly links ?company? to Nokia. Overall, there
is a 21% relative reduction in F1 error.
4We take the union of all the links returned by GLOW and
WikipediaMiner, but if they link a mention to two different en-
tities, we use only the output of WikipediaMiner.
296
Method F1 Precision Recall
NECO 70.6 72.0 69.2
Baseline NEL 64.4 67.4 61.7
Table 6: NEL performance of our system and the ensem-
ble baseline linker on ACE????-NWIRE-NEL.
5.5 Error Analysis
We analyzed 90 precision and recall errors and
present our findings in Table 7. Spurious mentions
accounted for the majority of non-semantic errors.
Despite the improvements that come from NEL, a
large portion of coreference errors can still be at-
tributed to incomplete semantic information, includ-
ing precision errors caused by incorrect linking. For
instance, the mention ?Disney? sometimes refers to
the company, and other times refers to the amuse-
ment park; however, the NEL systems we used had
difficulty disambiguating these cases, and NECO of-
ten incorrectly merges such mentions. Overly gen-
eral fine-grained attributes caused precision errors in
cases where many proper noun mentions were po-
tential antecedents for a common noun. Although
attributes such as country are useful for resolving a
generic ?country? mention, this information is insuf-
ficient when two distinct mentions such as ?China?
and ?Russia? both have the country attribute.
However, many recall errors are also caused by
the lack of fine-grained attributes. Finding the ideal
set of fine-grained attributes remains an open prob-
lem.
6 Related Work
Coreference resolution has a fifty year history which
defies brief summarization; see Ng (2010) for a
recent survey. Section 2.1 described the Stanford
multi-pass sieve algorithm, which is the foundation
for NECO.
Earlier coreference resolution systems used shal-
low semantics and pioneered knowledge extraction
from online encyclopedias (Ponzetto and Strube,
2006; Daume? III and Marcu, 2005; Ng, 2007). Some
recent work shows improvement in coreference res-
olution by incorporating semantic information from
Web-scale structured knowledge bases. Haghighi
and Klein (2009) use a rule-based system to extract
fine-grained attributes for mentions by analyzing
precise constructs (e.g., appositives) in Wikipedia
articles. Subsequently, Haghighi and Klein (2010)
used a generative approach to learn entity types from
an initial list of unambiguous mention types. Bansal
and Klein (2012) use statistical analysis of Web n-
gram features including lexical relations.
Rahman and Ng (2011) use YAGO to extract type
relations for all mentions. These methods incor-
porate knowledge about all possible meanings of a
mention. If a mention has multiple meanings, ex-
traneous information might be associated with it.
Zheng et al (2013) use a ranked list of candidate en-
tities for each mention and maintain the ranked list
when mentions are merged. Unlike previous work,
our method relies on NEL systems to disambiguate
possible meanings of a mention and capture high-
precision semantic knowledge from Wikipedia cate-
gories and Freebase notable types.
Ratinov and Roth (2012) investigated using NEL
to improve coreference resolution, but did not con-
sider a joint approach. They extracted attributes
from Wikipedia categories and used them as fea-
tures in a learned mention-pair model, but did not
do mention detection. Unfortunately, it is difficult
to compare directly to the results of both systems,
since they reported results on portions of ACE and
CoNLL datasets using gold mentions. However,
our approach provides independent evidence for the
benefit of NEL, and joint modeling in particular,
since it outperforms the state-of-the-art Stanford
sieve system (winner of the CoNLL 2011 shared
task (Pradhan et al, 2011)) and other recent com-
parable approaches on benchmark datasets.
Our work also builds on a long trajectory of
work in named entity resolution stemming from
SemTag (Dill et al, 2003). Section 2.2 discussed
GLOW and WikipediaMiner (Ratinov et al, 2011;
Milne and Witten, 2008). Kulkarni et al (2009)
present an elegant collective disambiguation model,
but do not exploit the syntactic nuances gleaned by
within-document coreference resolution. Hachey et
al. (2013) provide an insightful summary and evalu-
ation of different approaches to NEL.
7 Conclusions
Observing that existing coreference resolution and
named-entity linking have complementary strengths
297
Error Type Percentage Example
Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the
fact ...
Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals for
the first time], performed well , because they had strong events and their
movements had difficulty .
Contextual
semantic
16.6 [The Chinese side] hopes that each party concerned continues to make
constructive efforts to ...Considering the requirements of the Korean side
, ... the Chinese government decided to ...
NEL semantic 13.3 The most important thing about Disney is that it is a global brand. ... The
subway to Disney has already been constructed.
Attributes 11.1 The Hong Kong government turned over to Disney Corporation [200
hectares of land ...]. ... this area has become a prohibited zone in Hong
Kong.
Table 7: Examples of different error categories and the relative frequency of each. For every example, the mention to
be resolved is underlined, and the correct antecedent is italicized. For precision errors, the wrongly merged mention
is bolded. For recall errors, the missed mention is surrounded by [brackets].
and weaknesses, we present a joint approach. We
introduce NECO, a novel algorithm which solves
the problems jointly, demonstrating improved per-
formance on both tasks.
We envision several ways to improve the joint
model. While the current implementation of NECO
only introduces NEL once, we could also integrate
predictions with different levels of confidence into
different sieves. It would be interesting to more
tightly integrate the NEL system so it operates on
clusters rather than individual mentions ? after
each sieve merges an unlinked cluster, the algorithm
would retry NEL with the new context information.
NECO uses a relatively modest number of Freebase
attributes. While using more semantic knowledge
holds the promise of increased recall, the challenge
is maintaining precision. Finally, we would also like
to explore the extent to which a joint probabilistic
model (e.g., (Durrett and Klein, 2013)) might be
used to learn how to best make this tradeoff.
8 Acknowledgements
The research was supported in part by grants
from DARPA under the DEFT program through
the AFRL (FA8750-13-2-0019) and the CSSG
(N11AP20020), the ONR (N00014-12-1-0211), and
the NSF (IIS-1115966). Support was also provided
by a gift from Google, an NSF Graduate Research
Fellowship, and the WRF / TJ Cable Professor-
ship. The authors thank Greg Durrett, Heeyoung
Lee, Mitchell Koch, Xiao Ling, Mark Yatskar, Ken-
ton Lee, Eunsol Choi, Gabriel Schubiner, Nicholas
FitzGerald, Tom Kwiatkowski, and the anonymous
reviewers for helpful comments and feedback on the
work.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In International Confer-
ence on Language Resources and Evaluation Work-
shop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl,
R. Guha, Anant Jhingran, Tapas Kanungo, Sridhar Ra-
jagopalan, Andrew Tomkins, John A. Tomlin, and Ja-
son Y. Zien. 2003. SemTag and Seeker: bootstrapping
the semantic web via automated semantic annotation.
In Proceedings of the 12th International Conference
on World Wide Web.
298
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating entity
linking with Wikipedia. Artificial Intelligence Jour-
nal, 194.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In Proceedings of the
2009 Conference on Knowledge Discovery and Data
Mining.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings of
the Conference on Computational Natural Language
Learning.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Dan Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of the ACM Confer-
ence on Information and Knowledge Management.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
NIST. 2004. The ACE 2004 evaluation planXPToolkit
architecture.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, Wordnet and
Wikipedia for coreference resolution. In Proceedings
of the North American Association for Natural Lan-
guage Processing on Human Language Technologies.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
Lev Ratinov and Dan Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics.
Marta Recasens and Eduard Hovy. 2010. Coreference
resolution across corpora: languages, coding schemes,
and preprocessing information. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of the 6th conference on Message Understanding.
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolution.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning.
299
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1776?1786,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Harvesting Parallel News Streams to Generate Paraphrases of Event
Relations
Congle Zhang, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{clzhang,weld}@cs.washington.edu
Abstract
The distributional hypothesis, which states
that words that occur in similar contexts tend
to have similar meanings, has inspired sev-
eral Web mining algorithms for paraphras-
ing semantically equivalent phrases. Unfortu-
nately, these methods have several drawbacks,
such as confusing synonyms with antonyms
and causes with effects. This paper intro-
duces three Temporal Correspondence Heuris-
tics, that characterize regularities in parallel
news streams, and shows how they may be
used to generate high precision paraphrases
for event relations. We encode the heuristics
in a probabilistic graphical model to create
the NEWSSPIKE algorithm for mining news
streams. We present experiments demon-
strating that NEWSSPIKE significantly outper-
forms several competitive baselines. In order
to spur further research, we provide a large
annotated corpus of timestamped news arti-
cles as well as the paraphrases produced by
NEWSSPIKE.
1 Introduction
Paraphrasing, the task of finding sets of semantically
equivalent surface forms, is crucial to many natu-
ral language processing applications, including re-
lation extraction (Bhagat and Ravichandran, 2008),
question answering (Fader et al, 2013), summa-
rization (Barzilay et al, 1999) and machine transla-
tion (Callison-Burch et al, 2006). While the benefits
of paraphrasing have been demonstrated, creating a
large-scale corpus of high precision paraphrases re-
mains a challenge ? especially for event relations.
Many researchers have considered generating
paraphrases by mining the Web guided by the dis-
tributional hypothesis, which states that words oc-
curring in similar contexts tend to have similar
meanings (Harris, 1954). For example, DIRT (Lin
and Pantel, 2001) and Resolver (Yates and Etzioni,
2009) identify synonymous relation phrases by the
distributions of their arguments. However, the dis-
tributional hypothesis has several drawbacks. First,
it can confuse antonyms with synonyms because
antonymous phrases appear in similar contexts as of-
ten as synonymous phrases. For the same reasons, it
also often confuses causes with effects. For exam-
ple, DIRT reports that the closest phrase to fall is
rise, and the closest phrase to shoot is kill.1 Sec-
ond, the distributional hypothesis relies on statis-
tics over large corpora to produce accurate similarity
statistics. It remains unclear how to accurately para-
phrase less frequent relations with the distributional
hypothesis.
Another common approach employs the use of
parallel corpora. News articles are an interesting
target, because there often exist articles from dif-
ferent sources describing the same daily events.
This peculiar property allows the use of the tem-
poral assumption, which assumes that phrases in
articles published at the same time tend to have
similar meanings. For example, the approaches by
Dolan et al (2004) and Barzilay et al (2003) iden-
tify pairs of sentential paraphrases in similar arti-
cles that have appeared in the same period of time.
While these approaches use temporal information
as a coarse filter in the data generation stage, they
still largely rely on text metrics in the prediction
stage. This not only reduces precision, but also lim-
its the discovery of paraphrases with dissimilar sur-
1http://demo.patrickpantel.com/demos/
lexsem/paraphrase.htm
1776
face strings.
The goal of our research is to develop a technique
to generate paraphrases for large numbers of event
relation with high precision, using only minimal hu-
man effort. The key to our approach is a joint cluster
model using the temporal attributes of news streams,
which allows us to identify semantic equivalence
of event relation phrases with greater precision. In
summary, this paper makes the following contribu-
tions:
? We formulate a set of three temporal corre-
spondence heuristics that characterize regulari-
ties over parallel news streams.
? We develop a novel program, NEWSSPIKE,
based on a probabilistic graphical model that
jointly encodes these heuristics. We present in-
ference and learning algorithms for our model.
? We present a series of detailed experiments
demonstrating that NEWSSPIKE outperforms
several competitive baselines, and show through
ablation tests how each of the temporal heuris-
tics affects performance.
? To spur further research on this topic, we pro-
vide both our generated paraphrase clusters and
a corpus of 0.5M time-stamped news articles2,
collected over a period of about 50 days from
hundreds of news sources.
2 System Overview
The main goal of this work is to generate high preci-
sion paraphrases for relation phrases. News streams
are a promising resource, since articles from dif-
ferent sources tend to use semantically equivalent
phrases to describe the same daily events. For ex-
ample, when a recent scandal hit, headlines read:
?Armstrong steps down from Livestrong?; ?Arm-
strong resigns from Livestrong? and ?Armstrong
cuts ties with Livestrong?. From these we can con-
clude that the following relation phrases are seman-
tically similar: {step down from, resign from, cut ties
with}.
To realize this intuition, our first challenge is
to represent an event. In practice, a question like
?What happened to Armstrong and Livestrong on
Oct 17?? could often lead to a unique answer. It im-
2https://www.cs.washington.edu/node/
9473/
Given news streams 
OpenIE 
Joint inference 
model 
(a1,r,a2,t) 
Temporal Heuristics 
Temporal features 
& constraints 
Extracted Event 
candidates (EEC) & 
relation phrases 
r1 r2 r3   
(a1,a2,t) 
r1 r2 r3   r4   r5 r1 r2 r3   
(a1,a2,t) 
r1 r  r    r4   r  
Shallow timestamped 
extractions 
Group 
Relation phrases 
Describing the EEC 
r1 r2 r3   
(a1,a2,t) 
r1 r2 r3   r4   r5 r1 r2 r3   
(a1,a2,t) 
{r1, r3, r4} 
Paraphrase 
clusters 
Create  
clusters 
r1 r3 r4   
Figure 1: NEWSSPIKE first applies open informa-
tion extraction to articles in the news streams, obtain-
ing shallow extractions with time-stamps. Next, an
extracted event candidate (EEC) is obtained after group-
ing daily extractions by argument pairs. Temporal fea-
tures and constraints are developed based on our tempo-
ral correspondence heuristics and encoded into a joint in-
ference model. The model finally creates the paraphrase
clusters by predicting the relation phrases that describe
the EEC.
plies that using an argument pair and a time-stamp
could be an effective way to identify an event (e.g.
(Armstrong, Livestrong, Oct 17) for the previous
question). Based on this observation, this paper in-
troduces a novel mechanism to paraphrase relations
as summarized in Figure 1.
NEWSSPIKE first applies the ReVerb open infor-
mation extraction (IE) system (Fader et al, 2011)
on the news streams to obtain a set of (a1, r, a2, t)
tuples, where the ai are the arguments, r is a re-
lation phrase, and t is the time-stamp of the cor-
responding news article. When (a1, a2, t) suggests
a real word event, the relation r of (a1, r, a2, t) is
likely to describe that event (e.g. (Armstrong, resign
from, Livestrong, Oct 17). We call every (a1, a2, t)
an extracted event candidate (EEC), and every rela-
tion describing the event an event-mention.
For each EEC (a1, a2, t), suppose there are m ex-
traction tuples (a1, r1, a2, t) . . . (a1, rm, a2, t) shar-
ing the values of a1, a2, and t. We refer to this
set of extraction tuples as the EEC-set, and denote
it (a1, a2, t, {r1 . . . rm}). All the event-mentions in
the EEC-set may be semantically equivalent and are
hence candidates for a good paraphrase cluster.
Thus, the paraphrasing problem becomes a pre-
diction problem: for each relation ri in the EEC-set,
does it or does it not describe the hypothesized
event? We solve this problem in two steps. The
1777
next section proposes a set of temporal correspon-
dence heuristics that partially characterize semanti-
cally equivalent EEC-sets. Then, in Section 4, we
present a joint inference model designed to use these
heuristics to solve the prediction problem and to
generate paraphrase clusters.
3 Temporal Correspondence Heuristics
In this section, we propose a set of temporal heuris-
tics that are useful to generate paraphrases at high
precision. Our heuristics start from the basic obser-
vation mentioned previously ? events can often be
uniquely determined by their arguments and time.
Additionally, we find that it is not just the publica-
tion time of the news story that matters, the verb
tenses of the sentences are also important. For ex-
ample, the two sentences ?Armstrong was the chair-
man of Livestrong? and ?Armstrong steps down
from Livestrong? have past and present tense re-
spectively, which suggests that the relation phrases
are less likely to describe the same event and are
thus not semantically equivalent. To capture these
intuitions, we propose the Temporal Functionality
Heuristic:
Temporal Functionality Heuristic. News articles
published at the same time that mention the same
entities and use the same tense tend to describe the
same events.
Unfortunately, we find that not all the event can-
didates, (a1, a2, t), are equally good for paraphras-
ing. For example, today?s news might include
both ?Barack Obama heads to the White House?
and ?Barack Obama greets reporters at the White
House?. Although the two sentences are highly
similar, sharing a1 = ?Barack Obama? and a2 =
?White House,? and were published at the same
time, they describe different events.
From a probabilistic point of view, we can treat
each sentence as being generated by a particular hid-
den event which involves several actors. Clearly,
some of these actors, like Obama, participate in
many more events than others, and in such cases
we observe sentences generated from a mixture of
events. Since two event mentions from such a mix-
ture are much less likely to denote the same event
or relation, we wish to distinguish them from the
better (semantically homogeneous) EECs like the
(Armstrong, Livestrong) example. The question be-
comes ?How one can distinguish good entity pairs
from bad??
Our method rests on the simple observation that
an entity which participates in many different events
on one day is likely to have participated in events
in recent days. Therefore we can judge whether an
entity pair is good for paraphrasing by looking at
the history of the frequencies that the entity pair is
mentioned in the news streams, which is the time
series of that entity pair. The time series of the entity
pair (Barack Obama, the White House) tends to be
high over time, while the time series of the entity
pair (Armstrong, Livestrong) is flat for a long time
and suddenly spikes upwards on a single day. This
observation leads to:
Temporal Burstiness Heuristic. If an entity or an
entity pair appears significantly more frequently in
one day?s news than in recent history, the corre-
sponding event candidates are likely to be good to
generate paraphrase.
The temporal burstiness heuristic implies that a
good EEC (a1, a2, t) tends to have a spike in the time
series of its entities ai, or argument pair (a1, a2), on
day t.
However, even if we have selected a good EEC
for paraphrasing, it is likely that it contains a few
relation phrases that are related to (but not synony-
mous with) the other relations included in the EEC.
For example, it?s likely that the news story report-
ing ?Armstrong steps down from Livestrong.? might
also mention ?Armstrong is the founder of Live-
strong.? and so both ?steps down from? and ?is the
founder of? relation phrases would be part of the
same EEC-set. Inspired by the idea of one sense per
discourse from (Gale et al, 1992), we propose:
One Event-Mention Per Discourse Heuristic. A
news article tends not to state the same fact more
than once.
The one event-mention per discourse heuristic is
proposed in order to gain precision at the expense
of recall ? the heuristic directs an algorithm to
choose, from a news story, the single ?best? relation
phrase connecting a pair of two entities. Of course,
this doesn?t answer the question of deciding which
phrase is ?best.? In Section 4.3, we describe how
to learn a probabilistic graphical model which does
exactly this.
1778
4 Exploiting the Temporal Heuristics
In this section we propose several models to capture
the temporal correspondence heuristics, and discuss
their pros and cons.
4.1 Baseline Model
An easy way to use an EEC-set is to simply predict
that all ri in the EEC-set are event-mentions, and
hence are semantically equivalent. That is, given
EEC-set (a1, a2, t, {r1 . . . rm}), the output cluster is
{r1 . . . rm}.
This baseline model captures the most of the tem-
poral functionality heuristic, except for the tense re-
quirement. Our empirical study shows that it per-
forms surprisingly well. This demonstrates that the
quality of our input for the learning model is good:
the EEC-sets are promising resources for paraphras-
ing.
Unfortunately, the baseline model cannot deal
with the other heuristics, a problem we will remedy
in the following sections.
4.2 Pairwise Model
The temporal functionality heuristic suggests we ex-
ploit the tenses of the relations in an EEC-set; while
the temporal burstiness heuristic suggests we ex-
ploit the time series of its arguments. A pairwise
model can be designed to capture them: we compare
pairs of relations in the EEC-set, and predict whether
each pair is synonymous or non-synonymous. Para-
phrase clusters are then generated according to some
heuristic rules (e.g. assuming transitivity among
synonyms). The tenses of the relations and time se-
ries of the arguments are encoded as features, which
we call tense features and spike features respec-
tively. An example tense feature is whether one re-
lation is past tense while the other relation is present
tense; an example spike feature is the covariance of
the time series.
The pairwise model can be considered similar to
paraphrasing techniques which examine two sen-
tences and determine whether they are semantically
equivalent (Dolan and Brockett, 2005; Socher et al,
2011). Unfortunately, these techniques often based
purely on text metrics and does not consider any
temporal attributes. In section 5, we evaluate the
effect of applying these techniques.
1 
? (Armstrong,Livestrong,Oct.17) 
0 
1 
0 
? be founder of 
? step down 
? give speech at 
0 
1 
? be chairman of 
Article2 Article1 
? resign from 
?joint 
?Z 
?2Y 
?1Y 
Figure 2: an example model for EEC (Armstrong, Live-
strong, Oct 17). Y and Z are binary random variables.
?Y , ?Z and ?joint are factors. be founder of and step
down come from article 1 while give speech at, be chair-
man of and resign from come from article 2.
4.3 Joint Cluster Model
The pairwise model has several drawbacks: 1) it
lacks the ability to handle constraints, such as the
mutual exclusion constraint implied by the one-
mention per discourse heuristic; 2) ad-hoc rules,
rather than formal optimizations, are required to
generate clusters containing more than two relations.
A common approach to overcome the drawbacks
of the pairwise model and to combine heuristics to-
gether is to introduce a joint cluster model, in which
heuristics are encoded as features and constraints.
Data, instead of ad-hoc rules, determines the rel-
evance of different insights, which can be learned
as parameters. The advantage of the joint model
is analogous to that of cluster-based approaches for
coreference resolution (CR). In particular, a joint
model can better capture constraints on multiple
variables and can yield higher quality results than
pairwise CR models (Rahman and Ng, 2009).
We propose an undirected graphical model,
NEWSSPIKE, which jointly clusters relations. Con-
straints are captured by factors connecting multiple
random variables. We introduce random variables,
the factors, the objective function, the inference al-
gorithm, and the learning algorithm in the following
sections. Figure 2 shows an example model for EEC
(Armstrong, Livestrong, Oct 17).
4.3.1 Random Variables
For the EEC-set (a1, a2, t, {r1, . . . rm}), we intro-
duce one event variable and m relation variables, all
boolean valued. The event variable Z(a1,a2,t) indi-
1779
cates whether (a1, a2, t) is a good event for para-
phrasing. It is designed in accordance with the
temporal burstiness heuristic: for the EEC (Barack
Obama, the White House, Oct 17), Z should be as-
signed the value 0.
The relation variable Y r indicates whether rela-
tion r describes the EEC (a1, a2, t) or not (i.e. r is an
event-mention or not). The set of all event-mentions
with Y r = 1 define a paraphrase cluster, contain-
ing relation phrases. For example, the assignments
Y step down = Y resign from = 1 produce a paraphrase
cluster {step down, resign from}.
4.3.2 Factors and the Joint Distribution
In this section, we introduce a conditional proba-
bility model defining a joint distribution over all of
the event and relation variables. The joint distribu-
tion is a function over factors. Our model contains
event factors, relation factors and joint factors.
The event factor ?Z is a log-linear function with
spike features, used to distinguish good events. A re-
lation factor ?Y is also a log-linear function. It can
be defined for individual relation variables (e.g. ?Y1
in Figure 2) with features such as whether a relation
phrase comes from a clausal complement3. A rela-
tion factor can also be defined for a pair of relation
variables (e.g. ?Y2 in Figure 2) with features captur-
ing the pairwise evidence for paraphrasing, such as
if two relation phrases have the same tense.
The joint factors ?joint are defined to apply con-
straints implied by the temporal heuristics. They
play two roles in our model: 1) to satisfy the tempo-
ral burstiness heuristic, when the value of the event
variable is false, the EEC is not appropriate for para-
phrasing, and so all relation variables should also be
false; and 2) to satisfy the one-mention per discourse
heuristic, at most one relation variable from a single
article could be true.
We define the joint distribution over these vari-
ables and factors as follows. Let Y = (Y r1 . . . Y rm)
be the vector of relation variables; let x be the fea-
tures. The joint distribution is:
3Relation phrases in clausal complement are less useful for
paraphrasing because they often do not describe a fact. For ex-
ample, in the sentence He heard Romney had won the election,
the extraction (Romney, had won, the election) is not a fact at
all.
p(Z = z,Y = y|x; ?)
def
=
1
Zx
?Z(z,x)
?
?
d
?joint(z,yd,x)
?
i,j
?Y (yi, yj ,x)
where yd indicates the subset of relation variables
from a particular article d, and the parameter vector
? is the weight vector of the features in ?Z and ?Y ,
which are log-linear functions; i.e.,
?Y (yi, yj ,x)
def
= exp
?
?
?
j
?j?j(yi, yj ,x)
?
?
where ?j is the jth feature function.
The joint factors ?joint are used to apply the tem-
poral burstiness heuristic and the one event-mention
per discourse heuristic. ?joint is zero when the EEC
is not good for paraphrasing, but some yr = 1; or
when there is more than one r in a single article such
that yr = 1. Formally, it is calculated as:
?joint(z,yd,x)
def
=
?
??
??
0 if z = 0 ? ?yr = 1
0 if
?
yr?yd
yr > 1
1 otherwise
4.3.3 Maximum a Posteriori Inference
The goal of inference is to find the predictions z,y
which yield the greatest probability, i.e.,
z?,y? = arg max
z,y
p(Z = z,Y = y|x; ?)
This can be viewed as a MAP inference problem.
In general, inference in a graphical model is chal-
lenging. Fortunately, the joint factors in our model
are linear, and the event and relation factors are log-
linear; we can cast MAP inference as an integer lin-
ear programming (ILP) problem, and then compute
an approximation in polynomial time by means of
linear programming using randomized rounding, as
proposed in (Yannakakis, 1992).
We build one ILP problem for every EEC. The
variables of the ILP are Z and Y, which only take
values of 0 or 1. The objective function is the sum
of logs of the event and relation factors ?Z and
?Y . The temporal burstiness heuristic of ?joint is
encoded as a linear inequality constraint z ? yi; the
one-mention per discourse heuristic of ?joint is en-
coded as the constraint
?
yi?yd
yi ? 1.
1780
4.3.4 Learning
Our training data consists a set of N = 500 la-
beled EEC-sets each in the form of {(Ri, R
gold
i ) |
N
i=1
}. Each R is the set of all relations in the EEC-set
while Rgold is a manually selected subset of R con-
taining relations describing the EEC. Rgold could be
empty if the EEC was deemed poor for paraphras-
ing. For our model, the gold assignment yrgold = 1
if r ? Rgold; the gold assignment zgold = 1 if Rgold
is not empty.
Given {(Ri, R
gold
i ) |
N
i=1}, learning over similar
models is commonly done via maximum likelihood
estimation as follows:
L(?) = log
?
i
p(Zi = z
gold
i ,Yi = y
gold
i | xi,?)
For features in relation factors, the partial deriva-
tive for the ith model is:
?j(y
gold
i ,xi)? Ep(zi,yi|,xi,?)?j(yi,xi)
where ?j(yi,xi) =
?
?j(X,Y,x), the sum of val-
ues for the jth feature in the ith model; and values
of X,Y come from the assignment yi. For features
in event factors, the partial derivative is derived sim-
ilarly as
?j(z
gold
i ,xi)? Ep(zi,yi|,xi,?)?j(zi,xi)
It is unclear how to efficiently compute the expec-
tations in the above formula, a brute force approach
requires enumerating all assignments of yi, which
is exponentially large with the number of relations.
Instead, we opt to use a more tractable perceptron
learning approach (Collins, 2002; Hoffmann et al,
2011). Instead of computing the expectations, we
simply compute ?j(z?i ,xi) and ?j(y
?
i ,xi), where
z?i ,y
?
i is the assignment with the highest probabil-
ity, generated by the MAP inference algorithm us-
ing the current weight vector. The weight updates
are the following:
?j(y
gold
i ,xi)? ?j(y
?
i ,xi) (1)
?j(z
gold
i ,xi)? ?j(z
?
i ,xi) (2)
The updates can be intuitively explained as penal-
ties on errors. In sum, our learning algorithm con-
sists of iterating the following two steps: (1) in-
fer the most probable assignment given the current
weights; (2) update the weights by comparing in-
ferred assignments and the truth assignment.
5 Empirical Study
We first introduce the experimental setup for our em-
pirical study, and then we attempt to answer two
questions in sections 5.2 and 5.3 respectively: First,
does the NEWSSPIKE algorithm effectively exploit
the proposed heuristics and outperform other ap-
proaches which also use news streams? Secondly,
do the proposed temporal heuristics paraphrase re-
lations with greater precision than the distributional
hypothesis?
5.1 Experimental Setup
Since we were unable to find any elaborate time-
stamped, parallel, news corpus, we collected data
using the following procedure:
? Collect RSS news seeds, which contain the title,
time-stamp, and abstract of the news items.
? Use these titles to query the Bing news search
engine API and collect additional time-stamped
news articles.
? Strip HTML tags from the news articles using
Boilerpipe (Kohlschu?tter et al, 2010); keep only
the title and first paragraph of each article.
? Extract shallow relation tuples using the OpenIE
system (Fader et al, 2011).
We performed these steps every day from Jan-
uary 1 to February 22, 2013. In total, we collected
546,713 news articles, for which 2.6 million extrac-
tions had 529 thousand unique relations.
We used several types of features for paraphras-
ing: 1) spike features obtained from time series; 2)
tense features, such as whether two relation phrases
are both in the present tense; 3) cause-effect fea-
tures, such as whether two relation phrases often ap-
pear successively in the news articles; 4) text fea-
tures, such as whether sentences are similar; 5) syn-
tactic features, such as whether a relation phrase
appears in a clausal complement; and 6) semantic
features, such as whether a relation phrase contains
negative words.
Text and semantic features are encoded using the
relation factors of section 4.3.2. For example, in Fig-
ure 2, the factor ?Y2 includes the textual similarity
between the sentences containing the phrases ?step
down? and ?be chairman of? respectively; it also
includes the feature that the tense of ?step down?
(present) is different from the tense of ?be chairman
1781
output
{go into, go to, speak, return,
head to}
gold {go into, go to, approach, head to}
golddiv {go ?, approach, head to}
P/R precision = 3/5 recall = 3/4
P/Rdiv precisiondiv = 2/4 recalldiv = 2/3
Figure 3: an example pair of the output cluster and the
gold cluster, and the corresponding precision recall num-
bers.
of? (past).
5.2 Comparison with Methods using Parallel
News Corpora
We evaluated NEWSSPIKE against other methods
that also use time-stamped news. These include the
models mentioned in section 3 and state-of-the-art
paraphrasing techniques.
Human annotators created gold paraphrase clus-
ters for 500 EEC-sets; note that some EEC-sets
yield no gold cluster, since at least two synonymous
phrases. Two annotators were shown a set of candi-
date relation phrases in context and asked to select a
subset of these that described a shared event (if one
existed). There was 98% phrase-level agreement.
Precision and recall were computed by comparing
an algorithm?s output clusters to the gold cluster of
each EEC. We consider paraphrases with minor lex-
ical diversity, e.g. (go to, go into), to be of lesser in-
terest. Since counting these trivial paraphrases tends
to exaggerate the performance of a system, we also
report precision and recall on diverse clusters i.e.,
those whose relation phrases all have different head
verbs. Figure 3 illustrates these metrics with an ex-
ample; note under our diverse metrics, all phrases
matching go * count as one when computing both
precision and recall. We conduct 5-fold cross val-
idation on our labeled dataset to get precision and
recall numbers when the system requires training.
We compare NEWSSPIKE with the models in Sec-
tion 4, and also with the state-of-the-art paraphrase
extraction method:
Baseline: the model discussed in Section 4.1.
This system does not need any training, and gener-
ates outputs with perfect recall.
Pairwise: the pairwise model discussed in Sec-
tion 4.2 and using the same set of features as used
System
P/R P/R diverse
prec rec prec rec
Baseline 0.67 1.00 0.53 1.00
Pairwise 0.90 0.60 0.81 0.37
Socher 0.81 0.35 0.68 0.29
NEWSSPIKE 0.92 0.55 0.87 0.31
Table 1: Comparison with methods using parallel news
corpora
by NEWSSPIKE. To generate output clusters, transi-
tivity is assumed inside the EEC-set. For example,
when the pairwise model predicts that (r1, r2) and
(r1, r3) are both paraphrases, the resulting cluster is
{r1, r2, r3}.
Socher: Socher et al (2011) achieved the best re-
sults on the Dolan et al (2004) dataset, and released
their code and models. We used their off-the-shelf
predictor to replace the classifier in our Pairwise
model. Given sentential paraphrases, aligning rela-
tion phrases is natural, because OpenIE has already
identified the relation phrases.
Table 1 shows precision and recall numbers. It
is interesting that the basic model already obtains
0.67 precision overall and 0.53 in the diverse con-
dition. This demonstrates that the EEC-sets gen-
erated from the news streams are a promising re-
source for paraphrasing. Socher?s method performs
better, but not as well as Pairwise or NEWSSPIKE,
especially in the diverse cases. This is probably
due to the fact that Socher?s method is based purely
on text metrics and does not consider any tempo-
ral attributes. Taking into account the features used
by NEWSSPIKE, Pairwise significantly improves the
precision, which demonstrates the power of our tem-
poral correspondence heuristics. Our joint cluster
model, NEWSSPIKE, which considers both temporal
features and constraints, gets the best performance
in both conditions.
We conducted ablation testing to evaluate how
spike features and tense features, which are par-
ticularly relevant to the temporal aspects of news
streams, can improve performance. Figure 4 com-
pares the precision/recall curves for three systems
in the diverse condition: (1) NEWSSPIKE; (2)
w/oSpike: turning off all spike features; and (3)
w/oTense: turning off all features about tense.
(4) w/oDiscourse: turning off one event-mention
per discourse heuristic. There are some dips in
1782
0.1 0.2 0.3 0.4
0.6
0.7
0.8
0.9
1.0
w/oSpike
w/oTense
NewsSpike
Recall
Precision
w/oDiscourse
Figure 4: Precision recall curves on hard, diverse cases
for NewsSpike, w/oSpike, w/oTense and w/oDiscourse.
the curves because they are drawn after sorting
the predictions by the value of the corresponding
ILP objective functions, which do not perfectly re-
flect prediction accuracy. However, it is clear that
NEWSSPIKE produces greater precision over all
ranges of recall.
5.3 Comparison with Methods using the
Distributional Hypothesis
We evaluated our model against methods based on
the distributional hypothesis. We ran NEWSSPIKE
over all EEC-sets except for the development set and
compared to the following systems:
Resolver: Resolver (Yates and Etzioni, 2009)
uses a set of extraction tuples in the form of
(a1, r, a2) as the input and creates a set of relation
clusters as the output paraphrases. Resolver also
produces argument clusters, but this paper only eval-
uates relation clustering. We evaluated Resolver?s
performance with an input of the 2.6 million extrac-
tions described in section 5.1, using Resolver?s de-
fault parameters.
ResolverNYT: Since Resolver is supposed to
perform better when given more accurate statis-
tics from a larger corpus, we tried giving it more
data. Specifically, we ran ReVerb on 1.8 million NY
Times articles published between 1987 and 2007 ob-
tain 60 million extractions (Sandhaus, 2008). We ran
Resolver on the union of this and our standard test
set, but report performance only on clusters whose
relations were seen in our news stream.
System
all diverse
prec #rels prec #rels
Resolver 0.78 129 0.65 57
ResolverNyt 0.64 1461 0.52 841
ResolverNytTop 0.83 207 0.72 79
Cosine 0.65 17 0.33 9
CosineNyt 0.56 73 0.46 59
NEWSSPIKE 0.93 24843 0.87 5574
Table 2: Comparison with methods using the distribu-
tional hypothesis
ResolverNytTop: Resolver is designed to
achieve good performance on its top results. We thus
ranked the ResolverNYT outputs by their scores and
report the precision of the top 100 clusters.
Cosine: Cosine similarity is a basic metric for
the distributional hypothesis. This system employs
the same setup as Resolver in order to generate
paraphrase clusters, except that Resolver?s similar-
ity metric is replaced with the cosine. Each relation
is represented by a vector of argument pairs. The
similarity threshold to merge two clusters was 0.5.
CosineNYT: As for ResolverNYT, we ran Cosi-
neNYT with an extra 60 million extractions and re-
ported the performance on relations seen in our news
stream.
We measured the precision of each system by
manually labeling all output if 100 or fewer clus-
ters were generated (e.g. ResolverNytTop), other-
wise 100 randomly chosen clusters were sampled.
Annotators first determined the meaning of every
output cluster and then created a gold cluster by
choosing the correct relations. The gold cluster
could be empty if the output cluster was nonsensi-
cal. Unlike many papers that simply report recall on
the most frequent relations, we evaluated the total
number of returned relations in the output clusters.
As in Section 5.2, we also report numbers for the
case of lexically diverse relation phrases.
As can be seen in Table 2, NEWSSPIKE outper-
formed methods based on the distributional hypoth-
esis. The performance of the Cosine and Cosi-
neNyt was very low, suggesting that simple simi-
larity metrics are insufficient for handling the para-
phrasing problem, even when large-scale input is in-
volved. Resolver and ResolverNyt employ an ad-
vanced similarity measurement and achieve better
results. However, it is surprising that Resolver re-
sults in a greater precision than ResolverNyt. It
1783
is possible that argument pairs from news streams
spanning 20 years sometimes provide incorrect ev-
idence for paraphrasing. For example, there were
extractions like (the Rangers, be third in, the NHL)
and (the Rangers, be fourth in, the NHL) from news
in 2007 and 2003 respectively. Using these phrases,
ResolverNyt produced the incorrect cluster {be third
in, be fourth in}. NEWSSPIKE achieves greater pre-
cision than even the best results from ResolverNyt-
Top, because NEWSSPIKE successfully captures the
temporal heuristics, and does not confuse synonyms
with antonyms, or causes with effects. NEWSSPIKE
also returned on order of magnitude more relations
than other methods.
5.4 Discussion
Unlike some domain-specific clustering methods,
we tested on all relation phrases extracted by Ope-
nIE on the collected news streams. There are no
restrictions on the types of relations. Output para-
phrases cover a broad range, including politics,
sports, entertainment, health, science, etc. There
are 10 thousand nonempty clusters over 17 thousand
distinct phrases with average size 2.4. Unlike meth-
ods based on distributional similarity, NewsSpike
correctly clusters infrequently appearing phrases.
Since we focus on high precision, it is not sur-
prising that most clusters are of size 2 and 3. These
high precision clusters can contribute a lot to gen-
erate larger paraphrase clusters. For example, one
can invent the technique to merge smaller clusters
together. The work presented here provides a foun-
dation for future work to more closely examine these
challenges.
While this paper gives promising results, there
are still behaviors found in news streams that prove
challenging. Many errors are due to the discourse
context: the two sentences are synonymous in the
given EEC-set, but the relation phrases are not
paraphrases in general. For example, consider the
following two sentences: ?DA14 narrowly misses
Earth? and ?DA14 flies so close to Earth?. Statis-
tics information from large corpus would be helpful
to handle such challenges. Note in this paper, in or-
der to fairly compare with the distributional hypoth-
esis, we purposely forced NEWSSPIKE not to rely
on any distributional similarity. But NEWSSPIKE?s
graphical model has the flexibility to incorporate any
similarity metrics as features. Such a hybrid model
has great potential to increase both precision and re-
call, which is one goal for future work.
6 Related Work
The vast majority of paraphrasing work falls into
two categories: approaches based on the distribu-
tional hypothesis or those exploiting on correspon-
dences between parallel corpora (Androutsopoulos
and Malakasiotis, 2010; Madnani and Dorr, 2010).
Using Distribution Similarity: Lin and Pan-
tel?s (2001) DIRT employ mutual information statis-
tics to compute the similarity between relations rep-
resented in dependency paths. Resolver (Yates and
Etzioni, 2009) introduces a new similarity metric
called the Extracted Shared Property (ESP) and uses
a probabilistic model to merge ESP with surface
string similarity.
Identifying the semantic equivalence of relation
phrases is also called relation discovery or unsu-
pervised semantic parsing. Often techniques don?t
compute the similarity explicitly but rely implic-
itly on the distributional hypothesis. Poon and
Domingos? (2009) USP clusters relations repre-
sented with fragments of dependency trees by re-
peatedly merging relations having similar context.
Yao et al (2011; 2012) introduces generative mod-
els for relation discovery using LDA-style algorithm
over a relation-feature matrix. Chen et al (2011) fo-
cuses on domain-dependent relation discovery, ex-
tending a generative model with meta-constraints
from lexical, syntactic and discourse regularities.
Our work solves a major problem with these ap-
proaches, avoiding errors such as confusing syn-
onyms with antonyms and causes with effects. Fur-
thermore, NEWSSPIKE doesn?t require massive sta-
tistical evidence as do most approaches based on the
distributional hypothesis.
Using Parallel Corpora: Comparable and par-
allel corpora, including news streams and multiple
translations of the same story, have been used to
generate paraphrases, both sentential (Barzilay and
Lee, 2003; Dolan et al, 2004; Shinyama and Sekine,
2003) and phrasal (Barzilay and McKeown, 2001;
Shen et al, 2006; Pang et al, 2003). Typical meth-
ods first gather relevant articles and then pair sen-
tences that are potential paraphrases. Given a train-
ing set of paraphrases, models are learned and ap-
plied to unlabeled pairs (Dolan and Brockett, 2005;
1784
Socher et al, 2011). Phrasal paraphrases are often
obtained by running an alignment algorithm over the
paraphrased sentence pairs.
While prior work uses the temporal aspects of
news streams as a coarse filter, it largely relies on
text metrics, such as context similarity and edit dis-
tance, to make predictions and alignments. These
metrics are usually insufficient to produce high pre-
cision results; moreover they tend to produce para-
phrases that are simple lexical variants (e.g. {go to,
go into}.). In contrast, NEWSSPIKE generates para-
phrase clusters with both high precision and high di-
versity.
Others: Textual entailment (Dagan et al, 2009),
which finds a phrase implying another phrase,
is closely related to the paraphrasing task. Be-
rant et al (2011) notes the flaws in distributional
similarity and proposes local entailment classi-
fiers, which are able to combine many features.
Lin et al (2012) also uses temporal information to
detect the semantics of entities. In a manner similar
to our approach, Recasens et al (2013) mines paral-
lel news stories to find opaque coreferent mentions.
7 Conclusion
Paraphrasing event relations is crucial to many natu-
ral language processing applications, including re-
lation extraction, question answering, summariza-
tion, and machine translation. Unfortunately, previ-
ous approaches based on distribution similarity and
parallel corpora, often produce low precision clus-
ters. This paper introduces three Temporal Corre-
spondence Heuristics that characterize semantically
equivalent phrases in news streams. We present a
novel algorithm, NEWSSPIKE, based on a proba-
bilistic graphical model encoding these heuristics,
which harvests high-quality paraphrases of event re-
lations.
Experiments show NEWSSPIKE?s improvement
relative to several other methods, especially at pro-
ducing lexically diverse clusters. Ablation tests
confirm that our temporal features are crucial to
NEWSSPIKE?s precision. In order to spur future
research, we are releasing an annotated corpus of
time-stamped news articles and our harvested rela-
tion clusters.
Acknowledgments
We thank Oren Etzioni, Anthony Fader, Raphael
Hoffmann, Ben Taskar, Luke Zettlemoyer, and the
anonymous reviewers for providing valuable ad-
vice. We also thank Shengliang Xu for annotat-
ing the datasets. We gratefully acknowledge the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181, ONR grant N00014-
12-1-0211, a gift from Google, and the WRF / TJ
Cable Professorship. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not neces-
sarily reflect the view of DARPA, AFRL, or the US
government.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. In Journal of Artificial Intelligence Re-
search, pages 135?187.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In HLT-NAACL, pages 16?23.
Association for Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL,
pages 50?57. Association for Computational Linguis-
tics.
Regina Barzilay, Kathleen R McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In ACL, pages 550?
557. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL-HLT, pages 610?619. Association for Computa-
tional Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In ACL, volume 8, pages 674?682. Associa-
tion for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In NAACL, pages 17?24. Associa-
tion for Computational Linguistics.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In ACL-
HLT, pages 530?540. Association for Computational
Linguistics.
1785
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In ACL, pages 1?8. Asso-
ciation for Computational Linguistics.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(04):i?xvii.
William B Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Computational Linguistics, page 350. Association for
Computational Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP. Association for Computational
Linguistics, July 27-31.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL. Association for Computational
Linguistics.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233?237. Association for Computational
Linguistics.
Zellig S Harris. 1954. Distributional structure. Word.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL-HLT, pages 541?550.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In WSDM, pages 441?450. ACM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(4):343?360.
Thomas Lin, Oren Etzioni, et al 2012. No noun phrase
left behind: detecting and typing unlinkable entities.
In EMNLP, pages 893?903. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie J Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
NAACL, pages 102?109. Association for Computa-
tional Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10. As-
sociation for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In EMNLP, pages 968?
977. Association for Computational Linguistics.
Marta Recasens, Matthew Can, and Dan Jurafsky. 2013.
Same referent, different words: Unsupervised min-
ing of opaque coreferent mentions. In Proceedings of
NAACL-HLT, pages 897?906.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Siwei Shen, Dragomir R Radev, Agam Patel, and Gu?nes?
Erkan. 2006. Adding syntax to dynamic program-
ming for aligning comparable texts for the generation
of paraphrases. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 747?754.
Association for Computational Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In Pro-
ceedings of the second international workshop on
Paraphrasing-Volume 16, pages 65?71. Association
for Computational Linguistics.
Richard Socher, Eric H Huang, Jeffrey Pennington, An-
drew Y Ng, and Christopher D Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. NIPS, 24:801?809.
Mihalis Yannakakis. 1992. On the approximation of
maximum satisfiability. In Proceedings of the third an-
nual ACM-SIAM symposium on Discrete algorithms,
SODA ?92, pages 1?9.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In EMNLP, pages 1456?1466. As-
sociation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense dis-
ambiguation. In ACL, pages 712?720. Association for
Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34(1):255.
1786
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1891?1901,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Type-Aware Distantly Supervised Relation Extraction
with Linked Arguments
Mitchell Koch John Gilmer Stephen Soderland Daniel S. Weld
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{mkoch,jgilme1,soderlan,weld}@cs.washington.edu
Abstract
Distant supervision has become the lead-
ing method for training large-scale rela-
tion extractors, with nearly universal adop-
tion in recent TAC knowledge-base pop-
ulation competitions. However, there are
still many questions about the best way
to learn such extractors. In this paper we
investigate four orthogonal improvements:
integrating named entity linking (NEL)
and coreference resolution into argument
identification for training and extraction,
enforcing type constraints of linked argu-
ments, and partitioning the model by rela-
tion type signature.
We evaluate sentential extraction perfor-
mance on two datasets: the popular set of
NY Times articles partially annotated by
Hoffmann et al. (2011) and a new dataset,
called GORECO, that is comprehensively
annotated for 48 common relations. We
find that using NEL for argument identi-
fication boosts performance over the tra-
ditional approach (named entity recogni-
tion with string match), and there is further
improvement from using argument types.
Our best system boosts precision by 44%
and recall by 70%.
1 Introduction
Relation extractors are commonly trained by dis-
tant supervision (also known as knowledge-based
weak supervision (Hoffmann et al., 2011)), an au-
tonomous technique that creates a labeled train-
ing set by heuristically matching the contents of a
knowledge base (KB) to mentions (substrings) in
a textual corpus. For example, if a KB contained
the ground tuple BornIn(Albert Einstein, Ulm) then
Training
Extraction
KB
Argument 
Identification
Matching
Train Extractor Extractor
Argument 
Identification
Figure 1: Distantly supervised extraction pipeline.
a distant supervision system might label the sen-
tence ?While [Einstein]
1
was born in [Ulm]
2
, he
moved to Munich at an early age.? as a positive
training instance of the BornIn relation. Although
distant supervision is a simple idea and often cre-
ates data with false positives, it has become ubiq-
uitous; for example, all top-performing systems in
recent TAC-KBP slot filling competitions used the
method.
Surprisingly, however, many aspects of distant
supervision are poorly studied. In response we
perform an extensive search of ways to improve
distant supervision and the extraction process, in-
cluding using named entity linking (NEL) and
coreference to identify arguments for distant su-
pervision and extraction, as well as using type con-
straints and partitioning the trained model by rela-
tion type signatures.
The first step in the distant supervision process
is argument identification (Figure 1) ? finding
textual mentions referring to entities that might be
in some relation. Next comes matching, where KB
facts, e.g. tuples such as R(e
1
, e
2
), are associated
with sentences mentioning entities e
1
and e
2
in
the assumption that many of these sentences de-
scribe the relation R. Most previous systems per-
form these steps by first using named entity recog-
nition (NER) to identify possible arguments and
then using a simple string match, but this crude
1891
approach misses many possible instances. Since
the separately-studied task of named entity linking
(NEL) is precisely what is needed to perform dis-
tant supervision, it is interesting to see if today?s
optimized linkers lead to improved performance
when used to train extractors.
Coreference, the task of clustering mentions
that describe the same entity, may also be use-
ful for increasing the number of candidate argu-
ments. Consider the following variant of our pre-
vious example: ?While [he]
1
was born in [Ulm]
2
,
[Einstein]
3
moved to Munich at an early age.?
Since mentions 1 and 3 corefer, one could con-
sider using either the pair ?1, 2? or ?3, 2? (or both)
for training. Intuitively, it seems that ?1, 2? is more
representative of BornIn and might generalize bet-
ter, so we consider the use of coreference at both
training and extraction time.
Semantic relations often have selectional prefer-
ences (also known as type signatures); for exam-
ple, BornIn holds between people and locations.
Therefore, it seems promising to include entity
types, whether coarse or fine grained in the dis-
tantly supervised relation extraction process. We
consider two ways of adding this information. By
using NEL to get linked entities, we can impose
type constraints on the relation extraction system
to only allow relations over appropriately typed
mentions. We also investigate using coarse types
from NER to learn separate models for different
relation type signatures in order to make the mod-
els more effective.
In summary, this paper represents the following
contributions:
? We explore several dimensions for improv-
ing distantly supervised relation extraction,
including better argument identification dur-
ing training and extraction using both NEL
and coreference, partitioning the model by
relation type signatures, and enforcing type
constraints of linked arguments as a post-
processing step. While some of these ideas
may seem straightforward, to our knowledge
they have not been systematically studied.
And, as we show, they lead to dramatic im-
provements.
? Since previous datasets are incapable of mea-
suring an extractor?s true recall, we intro-
duce GORECO, a new exhaustively-labeled
dataset with gold annotations for sentential
instances of 48 relations across 128 newswire
documents from the ACE 2004 corpus (Dod-
dington et al., 2004).
? We demonstrate that NEL argument identifi-
cation boosts both precision and recall, and
using type constraints with linked arguments
further boosts precision, yielding a 43% in-
crease in precision and 27% boost to re-
call. Using coreference during training ar-
gument identification gives an additional 7%
improvement to precision and further boosts
recall by 9%. Partitioning the model by rela-
tion type signature offers further benefits, so
our best system yields a total boost of 44% to
precision and 70% to recall.
2 Distantly Supervised Extraction
At a sentence-level, the goal for relation extrac-
tion is to determine for each sentence, what facts
are expressed. We describe these as relation an-
notations of the form s?R(m
1
,m
2
), where s is
a sentence, R ? R is a relation name, R is our
finite set of target relations, and m
1
and m
2
are
grounded entity mentions of the form (s, t
1
, t
2
, e),
where t
1
and t
2
delimit a text span in the sentence,
and e is a grounded entity.
2.1 Training
During training, the contents of the KB are heuris-
tically matched to the training corpus according
to the distant supervision hypothesis: if a relation
holds between two entities, any sentence contain-
ing those two entities is likely to express that rela-
tion.
The training KB ? contains fact tuples of form
R(e
1
, e
2
), where R ? R is a relation name, R is
our finite set of target relations, and e
1
and e
2
are
ground entities. The training text corpus ? con-
tains documents, which contain sentences. Argu-
ment identification is performed over the text cor-
pus to get grounded mentionsm. Then during sen-
tential instance generation, sentential instances of
the form (s,m
1
,m
2
) are generated representing
a sentence with two grounded mentions. At this
point, these sentential instances can be matched
to the seed KB, yielding candidate relation anno-
tations of the form s?R(m
1
,m
2
) by finding all
relations that hold over the entities in a sentential
instance. These candidate relation annotations are
all positive instances to use for training. Negative
instance generation is also performed, generating
1892
negative examples of the form s?NA(m
1
,m
2
) in-
dicating that no relation holds between m
1
and
m
2
. There are several heuristics for generating
negative instances, and the number of negative ex-
amples and how they are treated can greatly affect
performance (Min et al., 2013).
Because the distant supervision hypothesis of-
ten does not hold, this training data is noisy. That
a fact is in the KB does not imply that the sen-
tence in question is expressing the relation. There
has been much work in combating noise in dis-
tant supervision training data, but one of the most
successful ideas is to train a multi-instance classi-
fier which assumes at-least-one relation holds for
positive bags. We use Hoffmann et al. (2011)?s
MULTIR system, which uses a probabilistic graph-
ical model to jointly reason at the corpus-level
and sentence-level, handles overlapping relations
in the KB so that multiple relations can hold over
an entity pair, and scales to large datasets.
2.2 Extraction
The trained relation extractor can assign a most
likely relation and a confidence score to a senten-
tial instance (s,m
1
,m
2
). To get these sentential
instances, argument identification and sentential
instance generation are applied to new documents.
Then the relation extractor potentially yields a re-
lation annotation of the form s?R(m
1
,m
2
), or
potentially no relation. At extraction time a men-
tion m might have a NIL link if a correspond-
ing ground entity was not found during argument
identification (meaning the entity is not in the KB).
The relation annotations have associated confi-
dence scores, so a threshold can be chosen to only
use high-confidence relation annotations.
3 Argument Identification
An important piece of relation extraction is deter-
mining what can be an argument, and how to form
a semantic representation of it. We define an argu-
ment identification function ArgIdent
?
(D), which
takes a document D, finds potential arguments,
and links them to entities in ? if possible, yield-
ing m, a set of grounded mentions in D. Pre-
vious relation extraction systems have based this
on NER. We evaluate NER-based argument iden-
tification against argument identification based on
NEL, as well as NEL with coreference.
3.1 Named Entity Recognition
Named entity recognition (NER) tags spans of to-
kens with basic types such as PERSON, ORGANI-
ZATION, LOCATION, and MISC. This is a high
accuracy tagging task often performed using a
sequence classifier (Finkel et al., 2005). Rela-
tion extraction systems can base their argument
identification on NER, by using NER to identify
text spans indicating entities and then find corre-
sponding entities in the KB through exact string
match (Riedel et al., 2010). Some downsides of
using NER with exact string match for relation ex-
traction is that it does not allow for overlapping
mentions, it can only capture arguments with full
names, and it can only capture arguments with
types of the NER system, e.g., ?politician? might
not be captured.
3.2 Named Entity Linking
Named entity linking (NEL) is the task of ground-
ing textual mentions to entities in a KB, such as
Wikipedia. Thus ?named entity? here, has a some-
what broader definition than in NER ? these are
any entities in the KB, not just those expressed
with proper names. Hachey et al. (2013) define
three stages that NEL systems take to perform
this task: extraction (mention detection), search
(generating candidate KB entities for a mention),
and disambiguation (selecting the best entity for a
mention). There has been much work on the task
of NEL in recent years (Milne and Witten, 2008;
Kulkarni et al., 2009; Ratinov et al., 2011; Cheng
and Roth, 2013).
Our definition of a function ArgIdent(D) is
completely served by an NEL system. It can
find any entity in the KB, and those entities are
grounded. Additionally, NEL can have overlap-
ping mentions as well as support for abbreviated
mentions like ?Obama?, or acronyms like ?US?.
NEL does not seek to capture anaphoric mentions,
however.
3.3 Coreference Resolution
Coreference resolution is the task of clustering
mentions of entities together, typically within a
single document. Using coreference, we can find
even more mentions than NEL, since it can find
pronouns and anaphoric mentions. We seek to use
coreference to add additional arguments to those
found by NEL, and we refer to this combined ar-
gument identification method as NEL+Coref. Tak-
1893
ing in arguments from NEL argument identifica-
tion and coreference clusters, we ground the clus-
ters by picking the most common grounded entity
from NEL mentions that occur in a coreference
cluster. A difficulty is that mentions from NEL
and coreference can have small differences in text
spans, such as whether determiners are included.
We try to assign each NEL argument to a corefer-
ence cluster, first looking for an exact span match,
then by looking for the shortest coreference men-
tion that contains it. If the coreference cluster al-
ready has matched an NEL argument through ex-
act span match that is different from the one found
by looking for the shortest containing coreference
mention, the new NEL argument is not added.
This gives for each coreference cluster a possible
grounding to an entity in the KB. What is provided
as final arguments for NEL+Coref argument iden-
tification are, in order, grounded NEL arguments,
grounded coreference arguments that do not over-
lap with previous arguments, NIL arguments from
NEL that do not overlap with previous arguments,
and NIL arguments from coreference that do not
overlap with previous arguments.
4 Type-Awareness
Relations have expected types for each argument.
Entity types, whether coarse-grained, such as from
NER, or fine-grained, such as from Freebase enti-
ties, are an important source of information that
can be useful for making decisions in relation ex-
traction. We bring type-awareness into the system
through partitioning the model, as well as by en-
forcing type constraints on output relation annota-
tions.
Model Partitioning Instead of building a single
relation extractor that can generate sentential in-
stances and then relation annotations with argu-
ments of any type, we can instead build separate
relation extractors for each possible coarse type
signature, e.g., (PERSON, PERSON), (PERSON, LO-
CATION), etc., and combine the extractions from
the extractor for each type signature. This modi-
fication allows each trained model to only handle
instances of specific types, and thus relations of
that type signature, allowing each to do a better job
of choosing relations within the type signature.
Type Constraints We can additionally reject re-
lation annotations where the types of the argu-
ments do not agree with the expected types of the
relation. That is, we only accept a relation annota-
tion s?R(m
1
,m
2
) when EntityTypes(e
1
) ? ?
1
6=
? and EntityTypes(e
2
)??
2
6= ?, wherem
1
is linked
to e
1
, m
2
is linked to e
2
, EntityTypes provides the
set of valid types for an entity, ?
1
is the set of al-
lowed types for the first argument of target relation
r, and ?
2
for the second argument.
5 Evaluation Setups
Relation extraction is often evaluated from a
macro-reading perspective (Mitchell et al., 2009),
in which the extracted facts, R(e
1
, e
2
), are judged
true or false independent of any supporting sen-
tence. For these experiments, however, we take a
micro-reading approach in order to strictly eval-
uate whether a relation extractor is able to extract
every fact expressed by a sentence s?R(m
1
,m
2
).
Micro-reading is more difficult, but it provides
fully semantic information at the sentence and
document level allowing detailed justifications,
and, for our purposes, allows us to better under-
stand the effects of our modifications. In order
to fairly evaluate different systems, even those us-
ing different methods of argument identification,
we want to use gold evaluation data allowing for
varying mention types. We additionally use Hoff-
mann et al. (2011)?s sentential evaluation as-is in
order to better compare with prior work. For our
training corpus, we use the TAC-KBP 2009 (Mc-
Namee and Dang, 2009) English newswire corpus
containing one million documents with 27 million
sentences.
5.1 Hoffmann et al. Sentential Evaluation
Hoffmann et al. (2011) generated their gold data
by taking the union of sentential instances where
some system being evaluated extracted a relation
as well as the sentential instances matching ar-
guments in the KB. They took a random sample
of these sentential instances and manually labeled
them with either a single relation or NA. Although
this process provides good coverage, since is is
sampled from extractions over a large corpus, it
does not allow one to measure true recall. Indeed,
Hoffmann?s method significantly overestimates re-
call, since the random sample is only over senten-
tial instances where a program detected an extrac-
tion or a KB match was found. Furthermore, this
test set only contains sentential instances in which
arguments are marked using NER, which makes
it impossible to determine if the use of NEL or
1894
coreference confers any benefit.
Finally, it does not allow for the possibility that
there may be multiple relations that should be ex-
tracted for a pair of arguments. For example, a
CeoOf relation, and an EmployedBy relation might
both be present for (Larry Page, Google). To ad-
dress these issues, we manually annotate a full set
of documents with relation annotations. Because
we are evaluating changing various aspects of the
distant supervision process, we cannot use Riedel
et al. (2010)?s distant supervision data as-is as oth-
ers did on the Hoffmann et al. (2011) sentential
evaluation. Instead, we use the TAC-KBP data de-
scribed above.
5.2 GoReCo Evaluation
In order to allow for variations on mentions (NER,
NEL, and coreference each has its own definition
of what a mention boundary should be), we want
gold relation annotations over coreference clus-
ters broadly defined to allow mentions obtained
from NER and NEL, as well as gold coreference
mentions. So as long as a relation extraction sys-
tem extracts a relation annotation s?R(m
1
,m
2
)
where m
1
and m
2
are allowed options (based on
text spans), it will get credit for extracting the
relation annotation. We introduce the GORECO
(gold relations and coreference) evaluation to sat-
isfy these constraints.
We start with an existing gold coreference
dataset, ACE 2004 (Doddington et al., 2004)
newswire, consisting of 128 documents. To get
relation annotations over coreference clusters, we
define two human annotation tasks and use the
BRAT (Stenetorp et al., 2012) tool for visualization
and relation and coreference annotations.
Relation Annotation The annotator is pre-
sented with a document with gold mentions indi-
cated and asked to determine for each sentence,
what facts involving target relations are expressed
by the sentence. They draw an arrow for each fact
and label it with the relation. They also have the
ability to add mentions not present (ReAnn men-
tions).
Supplemental Coreference Mentions from
NER and NEL are displayed along with ACE and
ReAnn mentions from the previous task. The
annotator draws coreference links from NER or
NEL mentions to an ACE or ReAnn mention if
they are coreferent.
We randomly shuffle the 128 ACE 2004
newswire documents and use 64 as a development
set and 64 as a test set. To complete annotations
of these documents, we only used one original hu-
man annotator (hired using the oDesk crowdsourc-
ing platform) and found mistakes by having others
check the work, as well as checking false positives
of relation extractors on the development set to
find patterns of annotation mistakes. On average,
there are 7 relation annotations per document.
For the GORECO evaluation, we define our
train/test split (with the separate TAC-KBP corpus
used for training) such that each has a different set
of documents and entities, in order to evaluate how
well the system performs on unseen entities. To do
this, we remove entities found in the gold evalua-
tion set from the training KB. (We do not remove
entities for the Hoffmann et al. (2011) evaluation,
since they do not.) We choose the threshold con-
fidence score for each system using the develop-
ment set to optimize for F1 and report results on
the test set.
5.2.1 Target Relations
Since we use a different evaluation, we also seek to
choose a more comprehensive and interesting set
of relations than prior work. Riedel et al. (2010),
whose train and test data is also used by Hoff-
mann et al. (2011) and Surdeanu et al. (2012), use
Freebase properties under domains /people, /busi-
ness, and /location. Since /location relations such
as /location/location/contains dominate the results
(and are relatively uninteresting in that they rarely
change), we do not use any /location relations, and
instead use the domains /people, /business, and
/organization (Google, 2012).
Since many Freebase properties are between
an entity and a table instead of another
entity, we also use joined relations, such
as /people/person/employment_history ./ /busi-
ness/employment_tenure/company , in this case
representing employment. We bring in an addi-
tional 20 relations of this form, also under /person,
/business, and /organization. Additionally we use
NELL (Carlson et al., 2010a) relations mapped to
Freebase by Zhang et al. (2012).
We only include a relation in our set of target
relations if both of its entity arguments are con-
tained in the set of entities found via NER with
exact string match or NEL over the training cor-
pus. We also remove inverse relations, since they
represent needless duplication. This gives us a set
1895
R of 105 target relations based on joins and unions
of Freebase properties. Of the 105 target relations,
48 were used at least once in the GORECO data.
6 Experiments and Results
We conduct experiments to determine how chang-
ing distantly supervised relation extraction along
various dimensions affects performance. We ex-
amine the choice of argument identification dur-
ing training and extraction, as well as the effects
of model type partitioning, and type constraints.
We consider the space of all combinations of these
dimensions, but focus on specific combinations
where we find improvements.
6.1 Relation Extraction Setup
We use and modify Hoffmann et al. (2011)?s sys-
tem MULTIR to control our experiments and as
a baseline. For NER argument identification as
well as for the use of NER in the features, we use
use Stanford NER (Finkel et al., 2005). For NEL
argument identification we use Wikipedia Miner
with the default threshold 0.5, and allowing re-
peated mentions (Milne and Witten, 2008). Since
Wikipedia Miner does not support NIL links, we
use non-overlapping NER mentions as NIL links.
For coreference, we use Stanford?s sieve-based de-
terministic coreference system (Lee et al., 2013).
For sentential instance generation, we take all
pairs of non-overlapping arguments in a sentence
(in either order). If the arguments have KB links,
we do not allow sentential instances where both
arguments represent the same entity. We use the
same lexical and syntactic features as MULTIR,
based on the features of Mintz et al. (2009). As
required for features, we use Stanford CoreNLP?s
tokenizer, part of speech tagger (Toutanova et al.,
2003), and dependency parser (de Marneffe and
Manning, 2008), and use the Charniak Johnson
constiuent parser (Charniak and Johnson, 2005).
For negative training generation, we take a simi-
lar approach to Riedel et al. (2010) and define a
percentage parameter n of the number of nega-
tive instances divided by the number of total in-
stances. Experimenting with n ? {0, 20%, 80%},
we find that n = 20% works best for our evalua-
tions, optimizing for F1, although using 80% neg-
ative training gives high precision at lower recall.
We use frequency-based feature selection to elimi-
nate features that appear less than 10 times, which
is helpful both for reducing overfitting as well as
0.60 0.1 0.2 0.3 0.4 0.5
1
0.7
0.8
0.9
Relative Recall
Prec
ision
Hoffmann et al. (2011)
NER+LTNER
NER+LT+CT
Figure 2: Methods evaluated in the context of
Hoffmann et al. (2011)?s sentential extraction
evaluation. NER: our NER baseline used for
training and extraction, LT: use NEL for train-
ing only, CT: use coreference for training only.
(NER+LT+CT means we use NER for extraction,
and NEL+Coref for training.)
constraining memory usage. Since the perceptron
learning of MULTIR is sensitive to instance order-
ing, we perform 10 random shuffles and average
the models.
For model type partitioning, when training with
NER, we ensure that the NER types match the
coarse relation type signatures. For NEL, we at-
tempt to use NER for coarse types of arguments,
but if an NER type is not present, we map the Free-
base type to its FIGER type (Ling and Weld, 2012)
to its coarse type. For type constraints, we use
Freebase?s expected input and output types for re-
lations. For NIL links, we use the NER type of
PERSON, ORGANIZATION, or LOCATION, if avail-
able, mapping it to appropriate Freebase types.
6.2 NER Baseline
As a result of a larger training set, as well as model
averaging, our baseline, which is otherwise equiv-
alent to the methods of Hoffmann et al. (2011)
and uses their MULTIR system, has slightly higher
precision as shown in Figure 2, curve NER. It is
also higher than that of Xu et al. (2013), who
achieved higher performance than Hoffmann et
al. (2011); our baseline gets 89.9% precision and
59.6% relative recall, while Xu et al. (2013)?s sys-
tem gets 84.6% precision and 56.1% relative re-
call. See Figure 3 and Table 1 for results on
GORECO.
6.3 NEL and Type Constraints
On GORECO, using NEL argument identification
increases recall and gives higher precision over the
entire curve. We further find that filtering results
using type constraints gives a large boost in pre-
1896
cision at a small cost to recall. Note the increase
in performance from NER to NEL to NEL+TC in
Figure 3a, as well as in Table 1. Using NEL gives
more recall, since it is able to capture arguments
that NER cannot, such as professions like ?pa-
leontologist?. The decrease in recall from type
constraints comes from false positives in the type
constraints process including from non-ideal links,
e.g., ?paleontologist? might get linked to the entity
Paleontology, so will not have the type required for
the Profession relation.
On the Hoffmann et al. (2011) sentential evalu-
ation, we were not able to use NEL argument iden-
tification at extraction time, because the instances
in the test set are from NER argument identifica-
tion. We tried using NEL only at training time
and found that it got similar performance to using
NER (Figure 2, curve NER+LT). Doing the same
on GORECO yielded slightly lower recall, because
of the mismatch of features learned from NEL ar-
guments (Figure 3b, curve NER+LT).
6.4 NEL+Coref Argument Identification
Using NEL+Coref for both training and extrac-
tion (see Table 1) introduces noise from arguments
not encountered during training time, but using
NEL+Coref just for training results in a decrease
in recall but similar precision (Figures 2 and 3b).
We found using NEL+Coref at test time unhelp-
ful for this dataset, because there were no exam-
ples we could find where coreference could re-
cover arguments that NEL could not. There were
three true positives from NEL+Coref involving
pronouns in the GORECO development set, but
there were also proper name versions of the ar-
guments nearby in the same sentences, making
coreference unnecessary. Additionally, corefer-
ence brings in many mentions such as times like
?Friday? or ?1954? that do not have corresponding
KB matches during training time. These sentential
instances have similar features to others involv-
ing coreference mentions, and there are not neg-
ative instances to weigh against these, since these
types do not appear in the training data. Better fea-
tures more suited to coreference mentions could be
helpful here.
At both training and extraction time, corefer-
ence can cluster together mentions that can be con-
sidered to be separate, such as in ?Brian Kain, a
33-year-old accountant?, ?Brian Kain? and ?ac-
countant? are coreferent in the gold ACE 2004
770 10 20 30 40 50 60 70
1
0
0.2
0.4
0.6
0.8
True Positives Count
Prec
ision
NER
NEL+TC
NELNER+TP
NEL+TC+TP
(a)
770 10 20 30 40 50 60 70
1
0
0.2
0.4
0.6
0.8
True Positives Count
Prec
ision
NER
NER+LT
NER+LT+CT
NEL+TC+CT+TP
NEL+TP
NEL+TC
NEL+TC+TP
NEL+CT
NEL+TC+CT
(b)
Figure 3: Precision versus true positives count
curves for different versions of the system evalu-
ated on the GORECO test set, containing 470 gold
instances. NER/NEL: argument identification used
in training and extraction, LT: use NEL for train-
ing only, CT: use coreference for training only, TC:
type constraints, TP: model type partitioning.
dataset. This means that type constraints will
disregard a Profession annotation between these
when it should not, because ?Brian Kain? (which
would have been a NIL link) gets the link of ?ac-
countant?. This effect contributes to the decrease
in recall.
6.5 Model Type Partitioning
Using type partitioning helps both NER and NEL
based models as shown with the +TP curves in
Figure 3). Partitioning by type signature results in
each model being able to better choose relations
for sentential instances of that type signature. In
the Partitioned columns of Table 1, removing type
partitioning from the best system (NEL training
1897
Single Partitioned
R P F1 R P F1
NER training
NER extraction 7.9 21.8 11.6 11.3 21.0 14.7
NEL extraction 8.5 21.4 12.2 9.8 19.7 13.1
NEL training
NER extraction 9.6 21.1 13.2 8.9 25.1 13.2
NEL extraction 10.0 30.5 15.1 15.3 16.7 16.0
NEL w/TC extraction 11.7 31.1 17.0 13.4 31.3 18.8
NEL+Coref training
NER extraction 9.4 19.2 12.6 6.8 28.3 11.0
NEL extraction 12.1 27.5 16.8 11.1 21.6 14.6
NEL w/TC extraction 12.8 33.3 18.5 12.1 34.1 17.9
NEL+Coref extraction 10.6 20.4 14.0 10.0 12.9 11.3
NEL+Coref w/TC extraction 9.4 22.7 13.3 7.9 19.1 11.1
Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For
nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either
training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1
except with coreference at extraction time and when combined with type partitioning.
and extraction, with type constraints, Partitioned)
results in a decrease in F1 from 18.8% to 17.0%.
Table 2 shows by-relation performance results for
the best system (curve NEL+TC+TP in Figure 3a).
6.6 Other Dimensions Explored
We also experimented with adding generalized
features that replaced lexemes with WordNet
classes (Fellbaum, 1998), which had uneven re-
sults. We observed a small but consistent improve-
ment on the NER baseline (11.6% F1 to 12.7%
F1 on GORECO), but after introducing NEL argu-
ment identification and partitioning, we no longer
observed the improvement. For some relations,
there was a small gain in recall that was offset by
a loss in precision, but for others, the gain in recall
outweighed the loss of precision.
We experimented with a negative instance feed-
back loop that ran a trained extractor over the
training corpus and tested whether each extrac-
tion made was in fact a negative example. Even
though the training corpus contains one million
documents, this method only yielded a few thou-
sand new negative instances due to the difficulty
of being certain an extraction should be negative.
A na?ve approach would simply ensure that both
entities participate in a relation in the KB; this is
troublesome, because of KB incompleteness and
because of type errors. For example Freebase con-
tains BornIn(Barack Obama, Honolulu), but our ex-
tractor extracted BornIn(Barack Obama, Hawaii).
To avoid labeling this true extraction as a nega-
tive instance we have to be robust about location
semantics. We selected new negative instances
NA(e
1
, e
2
) from our initial extractor that had e
1
in the knowledge base, with e
1
participating as the
first argument in the extracted relation but with-
out e
2
as the second argument. The results were
promising for some relations but overall inconclu-
sive as identifying true negatives is quite difficult.
Relation #Extractions #TP #FP
Nationality 50 11 38
Profession 43 23 20
EmployedBy 27 17 10
Spouse 22 2 20
LivedIn 6 4 2
OrgInCitytown 4 3 1
AthletePlaysForTeam 2 2 0
OrgType 1 1 0
Table 2: By-relation evaluation of the best system
(NEL with type constraints and type partitioning)
on the GORECO test set. The true positives (TP)
are the number of gold relations over coreference
clusters that matched, so multiple extractions can
match a single true positive.
7 Related Work
There has been much recent work on distantly su-
pervised relation extraction. Mintz et al. (2009)
use Freebase to train relation extractors over
Wikipedia without labeled data using multi-class
logistic regression and lexical and syntactic fea-
tures. Hoffmann et al. (2011) use a probabilis-
tic graphical model for multi-instance, multi-label
1898
learning and extract over newswire text using
Freebase relations. Surdeanu et al. (2012) take a
similar approach and use soft constraints and lo-
gistic regression. Riedel et al. (2013) integrate
open information extraction with schema-based,
proposing a universal schema approach, including
using features based on latent types. There has
also been recent work on reducing noise in dis-
tantly supervised relation extraction (Nguyen and
Moschitti, 2011; Takamatsu et al., 2012; Roth et
al., 2013; Ritter et al., 2013). Xu et al. (2013) and
Min et al. (2013) improve the quality of distant su-
pervision training data by reducing false negative
examples.
Distant supervision is related to semi-
supervised bootstrap learning work such as
Carlson et al. (2010b) and many others. Note that
distant supervision can be viewed as a subroutine
of bootstrap learning; bootstrap learning can
continue the process of distant supervision by
taking the new tuples found and then training on
those again, and repeating the process.
There has also been work on performing NEL
and coreference jointly (Cucerzan, 2007; Ha-
jishirzi et al., 2013), however these systems do not
perform relation extraction. Singh et al. (2013)
performs joint relation extraction, NER, and coref-
erence in a fully-supervised manner. They get
slight improvement by adding coreference, but do
not use NEL. Ling and Weld (2013) extend MUL-
TIR to find meronym relations in a biology text-
book. They get slight improvement over NER by
using coreference to pick the best mention of an
entity in the sentence for the meronym relation at
training and extraction time.
8 Conclusions and Future Work
Given the growing importance of distant supervi-
sion, a comprehensive understanding of its vari-
ants is crucial. While some of the optimizations
we propose may seem intuitive, they have not pre-
viously been systematically explored. Our experi-
ments show that NEL, type constraints, and type
partitioning are extremely important in order to
best take advantage of the seed KB during training
as well as known information at extraction time.
Our best system results in a 44% increase in pre-
cision, and a 70% increase in recall over our NER
baseline using GORECO. While we were not able
to evaluate all our methods on Hoffmann et al.
(2011)?s sentential evaluation, our baseline per-
forms significantly better than previous methods,
especially in precision, and training-only modifi-
cations perform similarly in both evaluations.
Future work will explore the use of NEL in dis-
tantly supervised relation extraction further, tun-
ing a confidence parameter for the NEL system,
and determining whether different confidence pa-
rameters should be used for training and extrac-
tion. Another possible direction is interleaving
NEL with relation extraction by using newly ex-
tracted facts to try to improve NEL performance.
We freely distribute GORECO a new gold stan-
dard evaluation for relation extraction consisting
of exhaustive annotations of the 128 documents
from ACE 2004 newswire for 48 relations. The
source code of our system, its output, as well as
our gold data are available at
http://cs.uw.edu/homes/mkoch/re.
Acknowledgements
We thank Raphael Hoffmann, Luke Zettlemoyer,
Mausam, Xiao Ling, Congle Zhang, Hannaneh
Hajishirzi, Leila Zilles, and the anonymous re-
viewers for helpful feedback. Additionally, we
thank Anand Mohan and Graeme Britz for annota-
tions and revisions of the GORECO dataset. This
work was supported by Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-
0181, ONR grant N00014-12-1-0211, a gift from
Google, a grant from Vulcan, and the WRF / TJ
Cable Professorship. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship under Grant
No. DGE-1256082.
References
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010a. Toward an architecture for never-
ending language learning. In Proceedings of the
AAAI Conference on Artificial Intelligence (AAAI-
10).
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka, Jr., and Tom M. Mitchell.
2010b. Coupled semi-supervised learning for infor-
mation extraction. In Proceedings of the Third ACM
International Conference on Web Search and Data
Mining, WSDM ?10, pages 101?110, New York,
NY, USA. ACM.
1899
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Xiao Cheng and Dan Roth. 2013. Relational inference
for wikification. In EMNLP.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708?716.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 1?8,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 363?370, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Google. 2012. Freebase data dumps.
https://developers.google.com/
freebase/data.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evalu-
ating entity linking with wikipedia. Artif. Intell.,
194:130?150, January.
Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and
Luke S. Zettlemoyer. 2013. Joint coreference res-
olution and named-entity linking with multi-pass
sieves. In EMNLP, pages 289?299. ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In ACL-HLT,
pages 541?550.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
KDD ?09, pages 457?466, New York, NY, USA.
ACM.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Comput. Linguist., 39(4):885?916, December.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In Proceedings of the 26th Con-
ference on Artificial Intelligence (AAAI).
Xiao Ling and Daniel S. Weld. 2013. Extracting
meronyms for a biology knowledge base using dis-
tant supervision. In Automated Knowledge Base
Construction (AKBC) 2013: The 3rd Workshop on
Knowledge Extraction at CIKM.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the tac 2009 knowledge base population track. In
Text Analysis Conference (TAC), volume 17, pages
111?113.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th ACM
Conference on Information and Knowledge Man-
agement, CIKM ?08, pages 509?518, New York,
NY, USA. ACM.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of NAACL-HLT, pages 777?
782.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2009), pages 1003?1011.
Tom M. Mitchell, Justin Betteridge, Andrew Carlson,
Estevam Hruschka, and Richard Wang. 2009. Pop-
ulating the semantic web by macro-reading internet
text. In The Semantic Web-ISWC 2009, pages 998?
1002. Springer.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, HLT
?11, pages 277?282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1375?1384,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In ECML/PKDD (3), pages 148?
163.
1900
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant super-
vision for information extraction. TACL, 1:367?378.
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow. 2013. A survey of noise reduction
methods for distant supervision. In Proceedings of
the 2013 Workshop on Automated Knowledge Base
Construction, AKBC ?13, pages 73?78, New York,
NY, USA. ACM.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In CIKM
Workshop on Automated Knowledge Base Construc-
tion (AKBC).
Pontus Stenetorp, Sampo Pyysalo, Goran Topi
?
c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Stroudsburg, PA, USA, April. Association for Com-
putational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wei Xu, Zhao Le, Raphael Hoffmann, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 2013 Conference of the Association for
Computational Linguistics (ACL 2013), Sofia, Bul-
garia, July. Association for Computational Linguis-
tics.
Congle Zhang, Raphael Hoffmann, and Daniel S.
Weld. 2012. Ontological smoothing for relation ex-
traction with minimal supervision. In AAAI.
1901
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118?127,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Open Information Extraction using Wikipedia
Fei Wu
University of Washington
Seattle, WA, USA
wufei@cs.washington.edu
Daniel S. Weld
University of Washington
Seattle, WA, USA
weld@cs.washington.edu
Abstract
Information-extraction (IE) systems seek
to distill semantic relations from natural-
language text, but most systems use super-
vised learning of relation-specific examples
and are thus limited by the availability of
training data. Open IE systems such as
TextRunner, on the other hand, aim to handle
the unbounded number of relations found
on the Web. But how well can these open
systems perform?
This paper presents WOE, an open IE system
which improves dramatically on TextRunner?s
precision and recall. The key to WOE?s per-
formance is a novel form of self-supervised
learning for open extractors ? using heuris-
tic matches between Wikipedia infobox at-
tribute values and corresponding sentences to
construct training data. Like TextRunner,
WOE?s extractor eschews lexicalized features
and handles an unbounded set of semantic
relations. WOE can operate in two modes:
when restricted to POS tag features, it runs
as quickly as TextRunner, but when set to use
dependency-parse features its precision and
recall rise even higher.
1 Introduction
The problem of information-extraction (IE), gen-
erating relational data from natural-language text,
has received increasing attention in recent years.
A large, high-quality repository of extracted tu-
ples can potentially benefit a wide range of NLP
tasks such as question answering, ontology learn-
ing, and summarization. The vast majority of
IE work uses supervised learning of relation-
specific examples. For example, the WebKB
project (Craven et al, 1998) used labeled exam-
ples of the courses-taught-by relation to in-
duce rules for identifying additional instances of
the relation. While these methods can achieve
high precision and recall, they are limited by the
availability of training data and are unlikely to
scale to the thousands of relations found in text
on the Web.
An alternative paradigm, Open IE, pioneered
by the TextRunner system (Banko et al, 2007)
and the ?preemptive IE? in (Shinyama and Sekine,
2006), aims to handle an unbounded number of
relations and run quickly enough to process Web-
scale corpora. Domain independence is achieved
by extracting the relation name as well as its
two arguments. Most open IE systems use self-
supervised learning, in which automatic heuristics
generate labeled data for training the extractor. For
example, TextRunner uses a small set of hand-
written rules to heuristically label training exam-
ples from sentences in the Penn Treebank.
This paper presents WOE (Wikipedia-based
Open Extractor), the first system that au-
tonomously transfers knowledge from random ed-
itors? effort of collaboratively editing Wikipedia to
train an open information extractor. Specifically,
WOE generates relation-specific training examples
by matching Infobox1 attribute values to corre-
sponding sentences (as done in Kylin (Wu and
Weld, 2007) and Luchs (Hoffmann et al, 2010)),
but WOE abstracts these examples to relation-
independent training data to learn an unlexical-
ized extractor, akin to that of TextRunner. WOE
can operate in two modes: when restricted to
shallow features like part-of-speech (POS) tags, it
runs as quickly as Textrunner, but when set to use
dependency-parse features its precision and recall
rise even higher. We present a thorough experi-
mental evaluation, making the following contribu-
tions:
? We present WOE, a new approach to open IE
that uses Wikipedia for self-supervised learn-
1An infobox is a set of tuples summarizing the key at-
tributes of the subject in a Wikipedia article. For example,
the infobox in the article on ?Sweden? contains attributes like
Capital, Population and GDP.
118
ing of unlexicalized extractors. Compared
with TextRunner (the state of the art) on three
corpora, WOE yields between 72% and 91%
improved F-measure ? generalizing well be-
yond Wikipedia.
? Using the same learning algorithm and fea-
tures as TextRunner, we compare four dif-
ferent ways to generate positive and negative
training data with TextRunner?s method, con-
cluding that our Wikipedia heuristic is respon-
sible for the bulk of WOE?s improved accuracy.
? The biggest win arises from using parser fea-
tures. Previous work (Jiang and Zhai, 2007)
concluded that parser-based features are un-
necessary for information extraction, but that
work assumed the presence of lexical features.
We show that abstract dependency paths are
a highly informative feature when performing
unlexicalized extraction.
2 Problem Definition
An open information extractor is a function
from a document, d, to a set of triples,
{?arg1,rel,arg2?}, where the args are noun
phrases and rel is a textual fragment indicat-
ing an implicit, semantic relation between the two
noun phrases. The extractor should produce one
triple for every relation stated explicitly in the text,
but is not required to infer implicit facts. In this
paper, we assume that all relational instances are
stated within a single sentence. Note the dif-
ference between open IE and the traditional ap-
proaches (e.g., as in WebKB), where the task is
to decide whether some pre-defined relation holds
between (two) arguments in the sentence.
We wish to learn an open extractor without di-
rect supervision, i.e. without annotated training
examples or hand-crafted patterns. Our input is
Wikipedia, a collaboratively-constructed encyclo-
pedia2. As output, WOE produces an unlexicalized
and relation-independent open extractor. Our ob-
jective is an extractor which generalizes beyond
Wikipedia, handling other corpora such as the gen-
eral Web.
3 Wikipedia-based Open IE
The key idea underlying WOE is the automatic
construction of training examples by heuristically
matching Wikipedia infobox values and corre-
sponding text; these examples are used to generate
2We also use DBpedia (Auer and Lehmann, 2007) as a
collection of conveniently parsed Wikipedia infoboxes
		
	
		
 
			
	
 	

 		Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 286?295,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning 5000 Relational Extractors
Raphael Hoffmann, Congle Zhang, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA-98195, USA
{raphaelh,clzhang,weld}@cs.washington.edu
Abstract
Many researchers are trying to use information
extraction (IE) to create large-scale knowl-
edge bases from natural language text on the
Web. However, the primary approach (su-
pervised learning of relation-specific extrac-
tors) requires manually-labeled training data
for each relation and doesn?t scale to the thou-
sands of relations encoded in Web text.
This paper presents LUCHS, a self-supervised,
relation-specific IE system which learns 5025
relations ? more than an order of magnitude
greater than any previous approach ? with an
average F1 score of 61%. Crucial to LUCHS?s
performance is an automated system for dy-
namic lexicon learning, which allows it to
learn accurately from heuristically-generated
training data, which is often noisy and sparse.
1 Introduction
Information extraction (IE), the process of gen-
erating relational data from natural-language text,
has gained popularity for its potential applications
in Web search, question answering and other tasks.
Two main approaches have been attempted:
? Supervised learning of relation-specific ex-
tractors (e.g., (Freitag, 1998)), and
? ?Open? IE ? self-supervised learning of
unlexicalized, relation-independent extractors
(e.g., Textrunner (Banko et al, 2007)).
Unfortunately, both methods have problems.
Supervised approaches require manually-labeled
training data for each relation and hence can?t
scale to handle the thousands of relations encoded
in Web text. Open extraction is more scalable,
but has lower precision and recall. Furthermore,
open extraction doesn?t canonicalize relations, so
any application using the output must deal with
homonymy and synonymy.
A third approach, sometimes refered to as weak
supervision, is to heuristically match values from
a database to text, thus generating a set of train-
ing data for self-supervised learning of relation-
specific extractors (Craven and Kumlien, 1999).
With the Kylin system (Wu and Weld, 2007) ap-
plied this idea to Wikipedia by matching values
of an article?s infobox1 attributes to corresponding
sentences in the article, and suggested that their
approach could extract thousands of relations (Wu
et al, 2008). Unfortunately, however, they never
tested the idea on more than a dozen relations. In-
deed, no one has demonstrated a practical way to
extract more than about one hundred relations.
We note that Wikipedia?s infobox ?ontology? is
a particularly interesting target for extraction. As a
by-product of thousands of contributors, it is broad
in coverage and growing quickly. Unfortunately,
the schemata are surprisingly noisy and most are
sparsely populated; challenging conditions for ex-
traction.
This paper presents LUCHS, an autonomous,
self-supervised system, which learns 5025 rela-
tional extractors ? an order of magnitude greater
than any previous effort. Like Kylin, LUCHS cre-
ates training data by matching Wikipedia attribute
values with corresponding sentences, but by itself,
this method was insufficient for accurate extrac-
tion of most relations. Thus, LUCHS introduces
a new technique, dynamic lexicon features, which
dramatically improves performance when learning
from sparse data and that way enables scalability.
1.1 Dynamic Lexicon Features
Figure 1 summarizes the architecture of LUCHS.
At the highest level, LUCHS?s offline training pro-
cess resembles that of Kylin. Wikipedia pages
1A sizable fraction of Wikipedia articles have associated
infoboxes ? relational summaries of the key aspects of the
subject of the article. For example, the infobox for Alan Tur-
ing?s Wikipedia page lists the values of 10 attributes, includ-
ing his birthdate, nationality and doctoral advisor.
286
Matcher Harvester
CRF 
Learner
Filtered Lists
WWW
Lexicon 
Learner
Classifier
Learner
Training Data
Extractor
Training Data
Lexicons
TuplesPages
Article
Classifier ExtractorExtractorClassified Pages
Extraction
Learning
Figure 1: Architecture of LUCHS. In order to
handle sparsity in its heuristically-generated train-
ing data, LUCHS generates custom lexicon features
when learning each relational extractor.
containing infoboxes are used to train a classi-
fier that can predict the appropriate schema for
pages missing infoboxes. Additionally, the val-
ues of infobox attributes are compared with article
sentences to heuristically generate training data.
LUCHS?s major innovation is a feature-generation
process, which starts by harvesting HTML lists
from a 5B document Web crawl, discarding 98%
to create a set of 49M semantically-relevant lists.
When learning an extractor for relation R, LUCHS
extracts seed phrases from R?s training data and
uses a semi-supervised learning algorithm to cre-
ate several relation-specific lexicons at different
points on a precision-recall spectrum. These lex-
icons form Boolean features which, along with
lexical and dependency parser-based features, are
used to produce a CRF extractor for each relation
? one which performs much better than lexicon-
free extraction on sparse training data.
At runtime, LUCHS feeds pages to the article
classfier, which predicts which infobox schema
is most appropriate for extraction. Then a small
set of relation-specific extractors are applied to
each sentence, outputting tuples. Our experiments
demonstrate a high F1 score, 61%, across the 5025
relational extractors learned.
1.2 Summary
This paper makes several contributions:
? We present LUCHS, a self-supervised IE sys-
tem capable of learning more than an order
of magnitude more relation-specific extractors
than previous systems.
? We describe the construction and use of dy-
namic lexicon features, a novel technique, that
enables hyper-lexicalized extractors which
cope effectively with sparse training data.
? We evaluate the overall end-to-end perfor-
mance of LUCHS, showing an F1 score of 61%
when extracting relations from randomly se-
lected Wikipedia pages.
? We present a comprehensive set of additional
experiments, evaluating LUCHS?s individual
components, measuring the effect of dynamic
lexicon features, testing sensitivity to varying
amounts of training data, and categorizing the
types of relations LUCHS can extract.
2 Heuristic Generation of Training Data
Wikipedia is an ideal starting point for our long-
term goal of creating a massive knowledge base of
extracted facts for two reasons. First, it is com-
prehensive, containing a diverse body of content
with significant depth. Perhaps more importantly,
Wikipedia?s structure facilitates self-supervised
extraction. Infoboxes are short, manually-created
tabular summaries of many articles? key facts ?
effectively defining a relational schema for that
class of entity. Since the same facts are often ex-
pressed in both article and ontology, matching val-
ues of the ontology to the article can deliver valu-
able, though noisy, training data.
For example, the Wikipedia article on ?Jerry Se-
infeld? contains the sentence ?Seinfeld was born
in Brooklyn, New York.? and the article?s infobox
contains the attribute ?birth place = Brooklyn?.
By matching the attribute?s value ?Brooklyn? to
the sentence, we can heuristically generate train-
ing data for a birth place extractor. This data is
noisy; some attributes will not find matches, while
others will find many co-incidental matches.
3 Learning Extractors
We first assume that each Wikipedia infobox at-
tribute corresponds to a unique relation (but see
Section 5.6) for which we would like to learn a
specific extractor. A major challenge with such
an approach is scalability. Running a relation-
specific extractor for each of Wikipedia?s 34,000
unique infobox attributes on each of Wikipedia?s
50 million sentences would require 1.7 trillion ex-
tractor executions.
We therefore choose a hierarchical approach
that combines both article classifiers and rela-
tion extractors. For each infobox schema, LUCHS
trains a classifier that predicts if an article is likely
to contain that schema. Only when an article
287
is likely to contain a schema, does LUCHS run
that schema?s relation extractors. To extract in-
fobox attributes from all of Wikipedia, LUCHS
now needs orders of magnitude fewer executions.
While this approach does not propagate infor-
mation from extractors back to article classifiers,
experiments confirm that our article classifiers
nonetheless deliver accurate results (Section 5.2),
reducing the potential benefit of joint inference. In
addition, our approach reduces the need for extrac-
tors to keep track of the larger context, thus sim-
plifying the extraction problem.
We briefly summarize article classification: We
use a linear, multi-class classifier with six kinds of
features: words in the article title, words in the
first sentence, words in the first sentence which
are direct objects to the verb ?to be?, article sec-
tion headers, Wikipedia categories, and their an-
cestor categories. We use the voted perceptron al-
gorithm (Freund and Schapire, 1999) for training.
More challenging are the attribute extractors,
which we wish to be simple, fast, and able to well
capture local dependencies. We use a linear-chain
conditional random field (CRF) ? an undirected
graphical model connecting a sequence of input
and output random variables, x = (x0, . . . , xT )
and y = (y0, . . . , yT ) (Lafferty et al, 2001). In-
put variables are assigned words w. The states
of output variables represent discrete labels l, e.g.
Argi-of-Relj and Other. In our case, variables
are connected in a chain, following the first-order
Markov assumption. We train to maximize condi-
tional likelihood of output variables given an input
probability distribution. The CRF models p(y|x)
are represented with a log-linear distribution
p(y|x) =
1
Z(x)
exp
T?
t=1
K?
k=1
?kfk(yt?1, yt, x, t)
where feature functions, f , encode sufficient
statistics of (x, y), T is the length of the sequence,
K is the number of feature functions, and ?k are
parameters representing feature weights, which
we learn during training. Z(x) is a partition func-
tion used to normalize the probabilities to 1. Fea-
ture functions allow complex, overlapping global
features with lookahead.
Common techniques for learning the weights ?k
include numeric optimization algorithms such as
stochastic gradient descent or L-BFGS. In our ex-
periments, we again use the simpler and more effi-
cient voted-perceptron algorithm (Collins, 2002).
The linear-chain layout enables efficient interence
using the dynamic programming-based Viterbi al-
gorithm (Lafferty et al, 2001).
We evaluate nine kinds of Boolean features:
Words For each input word w we introduce fea-
ture fww (yt?1, yt, x, t) := 1[xt=w].
State Transitions For each transition be-
tween output labels li, lj we add feature
f tranli,lj (yt?1, yt, x, t) := 1[yt?1=li?yt=lj ].
Word Contextualization For parameters p and
s we add features fprevw (yt?1, yt, x, t) :=
1[w?{xt?p,...,xt?1}] and f
sub
w (yt?1, yt, x, t) :=
1[w?{xt+1,...,xt+s}] which capture a window of
words appearing before and after each position t.
Capitalization We add feature
fcap(yt?1, yt, x, t) := 1[xtis capitalized].
Digits We add feature fdig(yt?1, yt, x, t) :=
1[xtis digits].
Dependencies We set fdep(yt?1, yt, x, t) to the
lemmatized sequence of words from xt to the root
of the dependency tree, computed using the Stan-
ford parser (Marneffe et al, 2006).
First Sentence We set f fs(yt?1, yt, x, t) :=
1[xtin first sentence of article].
Gaussians For numeric attributes, we fit a Gaus-
sian (?, ?) and add feature fgaui (yt?1, yt, x, t) :=
1[|xt??|<i?] for parameters i.
Lexicons For non-numeric attributes, and for a
lexicon l, i.e. a set of related words, we add fea-
ture f lexl (yt?1, yt, x, t) := 1[xt?l]. Lexicons are
explained in the following section.
4 Extraction with Lexicons
It is often possible to group words that are likely
to be assigned similar labels, even if many of these
words do not appear in our training set. The ob-
tained lexicons then provide an elegant way to im-
prove the generalization ability of an extractor, es-
pecially when only little training data is available.
However, there is a danger of overfitting, which
we discuss in Section 4.2.4.
The next section explains how we mine the Web
to obtain a large corpus of quality lists. Then Sec-
tion 4.2 presents our semi-supervised algorithm
for learning semantic lexicons from these lists.
288
4.1 Harvesting Lists from the Web
Domain-independence requires access to an ex-
tremely large number of lists, but our tight in-
tegration of lexicon acquisition and CRF learn-
ing requires that relevant lists be accessed instan-
taneously. Approaches using search engines or
wrappers at query time (Etzioni et al, 2004; Wang
and Cohen, 2008) are too slow; we must extract
and index lists prior to learning.
We begin with a 5 billion page Web crawl.
LUCHS can be combined with any list harvesting
technique, but we choose a simple approach, ex-
tracting lists defined by HTML <ul> or <ol>
tags. The set of lists obtained in this way is ex-
tremely noisy ? many lists comprise navigation
bars, tag sets, spam links, or a series of long text
paragraphs. This is consistent with the observation
that less than 2% of Web tables are relational (Ca-
farella et al, 2008).
We therefore apply a series of filtering steps.
We remove lists of only one or two items, lists
containing long phrases, and duplicate lists from
the same host. After filtering we obtain 49 million
lists, containing 56 million unique phrases.
4.2 Semi-Supervised Learning of Lexicons
While training a CRF extractor for a given rela-
tion, LUCHS uses its corpus of lists to automati-
cally generate a set of semantic lexicons ? spe-
cific to that relation. The technique proceeds in
three steps, which have been engineered to run ex-
tremely quickly:
1. Seed phrases are extracted from the labeled
training set.
2. A learning algorithm expands the seed
phrases into a set of lexicons.
3. The semantic lexicons are added as features
to the CRF learning algorithm.
4.2.1 Extracting Seed Phrases
For each training sentence LUCHS first identifies
subsequences of labeled words, and for each such
labeled subsequence, LUCHS creates one or more
seed phrases p. Typically, a set of seeds con-
sists precisely of the labeled subsequences. How-
ever, if the labeled subsequences are long and have
substructure, e.g., ?San Remo, Italy?, our system
splits at the separator token, and creates additional
seed sets from prefixes and postfixes.
4.2.2 From Seeds to Lexicons
To expand a set of seeds into a lexicon, LUCHS
must identify relevant lists in the corpus. Rele-
vancy can be computed by defining a similarity be-
tween lists using the vector-space model. Specifi-
cally, let L denote the corpus of lists, and P be the
set of unique phrases from L. Each list l0 ? L can
be represented as a vector of weighted phrases p ?
P appearing on the list, l0 = (l0p1 l
0
p2 . . . l
0
p|P|). Fol-
lowing the notion of inverse document frequency,
a phrase?s weight is inversely proportional to the
number of lists containing the phrase. Popular
phrases which appear on many lists thus receive
a small weight, whereas rare phrases are weighted
higher:
l0pi =
1
|{l ? L|p ? l}|
Unlike the vector space model for documents, we
ignore term frequency, since the vast majority of
lists in our corpus don?t contain duplicates. This
vector representation supports the simple cosine
definition of list similarity, which for lists l0, l1 ?
L is defined as
simcos :=
l0 ? l1
?l0??l1?
.
Intuitively, two lists are similar if they have many
overlapping phrases, the phrases are not too com-
mon, and the lists don?t contain many other
phrases. By representing the seed set as another
vector, we can find similar lists, hopefully contain-
ing related phrases. We then create a semantic lex-
icon by collecting phrases from a range of related
lists.
For example, one lexicon may be created as the
union of all phrases on lists that have non-zero
similarity to the seed list. Unfortunately, due to
the noisy nature of the Web lists such a lexicon
may be very large and may contain many irrele-
vant phrases. We expect that lists with higher sim-
ilarity are more likely to contain phrases which are
related to our seeds; hence, by varying the sim-
ilarity threshold one may produce lexicons rep-
resenting different compromises between lexicon
precision and recall. Not knowing which lexicon
will be most useful to the extractors, LUCHS gen-
erates several and lets the extractors learn appro-
priate weights.
However, since list similarities vary depending
on the seeds, fixed thresholds are not an option. If
#similarlists denotes the number of lists that have
non-zero similarity to the seed list and #lexicons
289
the total number of lexicons we want to generate,
LUCHS sets lexicon i ? {0, . . . ,#lexicons ? 1}
to be the union of prases on the
#similarlistsi/#lexicons
most similar lists.2
4.2.3 Efficiently Creating Lexicons
We create lexicons from lists that are similar to
our seed vector, so we only consider lists that have
at least one phrase in common. Importantly, our
index structures allow LUCHS to select the rele-
vant lists efficiently. For each seed, LUCHS re-
trieves the set of containing lists as a sorted se-
quence of list identifiers. These sequences are
then merged yielding a sequence of list identifiers
with associated seed-hit counts. Precomputed list
lengths and inverse document frequencies are also
retrieved from indices, allowing efficient compu-
tation of similarity. The worst case complexity is
O(log(S)SK) where S is the number of seeds and
K the maximum number of lists to consider per
seed.
4.2.4 Preventing Lexicon Overfitting
Finally, we integrate the acquired semantic lexi-
cons as features into the CRF. Although Section 3
discussed how to use lexicons as CRF features,
there are some subtleties. Recall that the lexi-
cons were created from seeds extracted from the
training set. If we now train the CRF on the same
examples that generated the lexicon features, then
the CRF will likely overfit, and weight the lexicon
features too highly!
Before training, we therefore split the training
set into k partitions. For each example in a par-
tition we assign features based on lexicons gener-
ated from only the k?1 remaining partitions. This
avoids overfitting and ensures that we will not per-
form much worse than without lexicon features.
When we apply the CRF to our test set, we use the
lexicons based on all k partitions. We refer to this
technique as cross-training.
5 Experiments
We start by evaluating end-to-end performance of
LUCHS when applied to Wikipedia text, then an-
alyze the characteristics of its components. Our
experiments use the 10/2008 English Wikipedia
dump.
2For practical reasons, we exclude the case i = #lexicons
in our experiments.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
recall
precision
Figure 2: Precision / recall curve for end-to-end
system performance on 100 random articles.
5.1 Overall Extraction Performance
To evaluate the end-to-end performance of
LUCHS, we test the pipeline which first classifies
incoming pages, activating a small set of extrac-
tors on the text. To ensure adequate training and
test data, we limit ourselves to infobox classes
with at least ten instances; there exist 1,583 such
classes, together comprising 981,387 articles. We
only consider the first ten sentences for each ar-
ticle, and we only consider 5025 attributes.3 We
create a test set by sampling 100 articles ran-
domly; these articles are not used to train article
classifiers or extractors. Each test article is then
automatically classified, and a random attribute
of the predicted schema is selected for extraction.
Gold labels for the selected attribute and article are
created manually by a human judge and compared
to the token-level predictions from the extractors
which are trainined on the remaining articles with
heuristic matches.
Overall, LUCHS reaches a precision of .55 at a
recall of .68, giving an F1-score of .61 (Figure 2).
Analyzing the errors in more detail, we find that in
11 of 100 cases an article was incorrectly classi-
fied. We note that in at least two of these cases the
predicted class could also be considered correct.
For example, instead of Infobox Minor Planet the
extractor predicted Infobox Planet.
On five of the selected attributes the extrac-
tor failed because the attributes could be consid-
ered unlearnable: The flexibility of Wikipedia?s
infobox system allows contributors to introduce
attributes for formatting, for example defining el-
3Attributes were selected to have at least 10 heuristic
matches, to have 10% of values covered by matches, and 10%
of articles with attribute in infobox covered by matches.
290
ement order. In the future we wish to train LUCHS
to ignore this type of attribute.
We also compared the heuristic matches con-
tained in the selected 100 articles to the gold stan-
dard: The matches reach a precision of .90 at a
recall of .33, giving an F1-score of .48. So while
most heuristic matches hit mentions of attribute
values, many other mentions go unmatched. Man-
ual analysis shows that these values are often miss-
ing from an infobox, are formatted differently, or
are inconsistent to what is stated in the article.
So why did the low recall of the heuristic
matches not adversely affect recall of our extrac-
tors? For most articles, an attribute can be as-
signed a single unique value. When training an
attribute extractor, only articles that contained a
heuristic match for that attribute were considered,
thus avoiding many cases of unmatched mentions.
Subsequent experiments evaluate the perfor-
mance of LUCHS components in more detail.
5.2 Article Classification
The first step in LUCHS?s run-time pipeline is de-
termining which infobox schemata are most likely
to be found in a given article. To test this, we ran-
domly split our 981,387 articles into 4/5 for train-
ing and 1/5 for testing, and train a single multi-
class classifier. For this experiment, we use the
original infobox class of an article as its gold la-
bel. We compute the accuracy of the prediction at
.92. Since some classes can be considered inter-
changeable, this number represents a lower bound
on performance.
5.3 Factors Affecting Extraction Accuracy
We now evaluate attribute extraction assuming
perfect article classification. To keep training time
manageable, we sample 100 articles for training
and 100 articles for testing4 for each of 100 ran-
dom attributes. We again only consider the first
ten sentences of each article, and we only con-
sider articles that have heuristic matches with the
attribute. We measure F1-score at a token-level,
taking the heuristic matches as ground-truth.
We first test the performance of extractors
trained using our basic features (Section 3)5, not
including lexicons and Gaussians. We begin us-
ing word features and obtain a token-level F1-
score of .311 for text and .311 for numeric at-
tributes. Adding any of our additional features
4These numbers are smaller for attributes with less train-
ing data available, but the same split is maintained.
5For contextualization features we choose p, s = 5.
Features F1-Score
Text attributes
Baseline .491
Baseline + Lexicons w/o CT .367
Baseline + Lexicons .545
Numeric attributes
Baseline .586
Baseline + Gaussians w/o CT .623
Baseline + Gaussians .627
Table 1: Impact of Lexicon and Gaussian features.
Cross-Training (CT) is essential to improve per-
formance.
improves these scores, but the relative improve-
ments vary: For both text and numeric attributes,
contextualization and dependency features deliver
the largest improvement. We then iteratively add
the feature with largest improvement until no fur-
ther improvement is observed. We finally obtain
an F1-score of .491 for text and .586 for numeric
attributes. For text attributes the extractor uses
word, contextualization, first sentence, capitaliza-
tion, and digit features; for numeric attributes the
extractor uses word, contextualization, digit, first
sentence, and dependency features. We use these
extractors as a baseline to evaluate our lexicon and
Gaussian features.
Varying the size of the training sets affects re-
sults: Taking more articles raises the F1-score, but
taking more sentences per article reduces it. This
is because Wikipedia articles often summarize a
topic in the first few paragraphs and later discuss
related topics, necessitating reference resolution
which we plan to add in future work.
5.4 Lexicon and Gaussian Features
We next study how our distribution features6 im-
pact the quality of the baseline extractors (Table
1). Without cross-training we observe a reduction
in performance, due to overfitting. Cross-training
avoids this, and substantially improves results over
the baseline. While cross-training is particularly
critical for lexicon features, it is less needed for
Gaussians where only two parameters, mean and
deviation, are fitted to the training set.
The relative improvements depend on the num-
ber of available training examples (Table 2). Lex-
icon and Gaussian features especially benefit ex-
tractors for sparse attributes. Here we can also see
that the improvements are mainly due to increases
in recall.
6We set the number of lexicon and Gaussian features to 4.
291
# Train F1-B F1-LUCHS ?F1 ?Pr ?Re
Text attributes
10 .379 .439 +16% +10% +20%
25 .447 .504 +13% +7% +20%
100 .491 .545 +11% +5% +17%
Numeric attributes
10 .484 .531 +10% +4% +13%
25 .552 .596 +8% +4% +10%
100 .586 .627 +7% +5% +8%
Table 2: Lexicon and Gaussian features greatly ex-
pand F1 score (F1-LUCHS) over the baseline (F1-
B), in particular for attributes with few training ex-
amples. Gains are mainly due to increased recall.
5.5 Scaling to All of Wikipedia
Finally, we take our best extractors and run them
on all 5025 attributes, again assuming perfect ar-
ticle classification and using heuristic matches as
gold-standard. Figure 3 shows the distribution of
obtained F1 scores. 810 text attributes and 328 nu-
meric attributes reach a score of 0.80 or higher.
The performance depends on the number of
available training examples, and that number is
governed by a long-tailed distribution. For ex-
ample, 61% of the attributes in our set have 50
or fewer examples, 36% have 20 or fewer. Inter-
estingly, the number of training examples had a
smaller effect on performance than expected. Fig-
ure 4 shows the correlation between these vari-
ables. Lexicon and Gaussian features enables ac-
ceptable performance even for sparse attributes.
Averaging across all attributes we obtain F1
scores of 0.56 and 0.60 for textual and numeric
values respectively. We note that these scores
assume that all attributes are equally important,
weighting rare attributes just like common ones.
If we weight scores by the number of attribute in-
stances, we obtain F1 scores of 0.64 (textual) and
0.78 (numeric). In each case, precision is slightly
higher than recall.
5.6 Towards an Attribute Ontology
The true promise of relation-specific extractors
comes when an ontology ties the system together.
By learning a probabilistic model of selectional
preferences, one can use joint inference to improve
extraction accuracy. One can also answer scien-
tific questions, such as ?How many of the learned
Wikipedia attributes are distinct?? It is clear that
many duplicates exist due to collaborative sloppi-
ness, but semantic similarity is a matter of opinion
and an exact answer is impossible.
0% 20% 40% 60% 80% 100%
0.0
0.2
0.4
0.6
0.8
1.0
Text attr. (3962)
Numeric attr. (1063)
# Attributes
F1 Score
Figure 3: F1 scores among attributes, ranked by
score. 810 text attributes (20%) and 328 numeric
attributes (31%) had an F1-score of .80 or higher.
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
Text attr.
Numeric attr.
# Training Examples
Average F1 Sco
re
Figure 4: Average F1 score by number of training
examples. While more training data helps, even
sparse attributes reach acceptable performance.
Nevertheless, we clustered the textual attributes
in several ways. First, we cleaned the attribute
names heuristically and performed spell check.
The ?distance? between two attributes was calcu-
lated with a combination of edit distance and IR
metrics with Wordnet synonyms; then hierarchical
agglomerative clustering was performed. We man-
ually assigned names to the clusters and cleaned
them, splitting and joining as needed. The result is
too crude to be called an ontology, but we continue
its elaboration. There are a total of 3962 attributes
grouped in about 1282 clusters (not yet counting
attributes with numerical values); the largest clus-
ter, location, has 115 similar attributes. Figure 5
shows the confusion matrix between attributes in
the biggest clusters; the shade of the i, jth pixel
indicates the F1 score achieved by training on in-
stances of attribute i and testing on attribute j.
292
loca
tion
birt
hpla
cep
title cou
ntry
full
 nam
e
city nat
iona
lity
nati
ona
lity
birt
h na
me
date
 of 
birt
h
date
 of 
dea
th
date stat
es
Figure 5: Confusion matrix for extractor accuracy
training on one attribute then testing on another.
Note the extraction similarity between title and
full-name, as well as between dates of birth and
death. Space constraints allow us to show only
1000 of LUCHS?s 5025 extracted attributes, those
in the largest clusters.
6 Related Work
Large-scale extraction A popular approach to IE
is supervised learning of relation-specific extrac-
tors (Freitag, 1998). Open IE, self-supervised
learning of unlexicalized, relation-independent ex-
tractors (Banko et al, 2007), is a more scalable
approach, but suffers from lower precision and
recall, and doesn?t canonicalize the relations. A
third approach, weak supervision, performs self-
supervised learning of relation-specific extractors
from noisy training data, heuristically generated
by matching database values to text. (Craven and
Kumlien, 1999; Hirschman et al, 2002) apply this
technique to the biological domain, and (Mintz
et al, 2009) apply it to 102 relations from Free-
base. LUCHS differs from these approaches in that
its ?database? ? the set of infobox values ? itself
is noisy, contains many more relations, and has
few instances per relation. Whereas the existing
approaches focus on syntactic extraction patterns,
LUCHS focuses on lexical information enhanced
by dynamic lexicon learning.
Extraction from Wikipedia Wikipedia has
become an interesting target for extraction.
(Suchanek et al, 2008) build a knowledgebase
from Wikipedia?s semi-structured data. (Wang et
al., 2007) propose a semisupervised positive-only
learning technique. Although that extracts from
text, its reliance on hyperlinks and other semi-
structured data limits extraction. (Wu and Weld,
2007; Wu et al, 2008)?s systems generate train-
ing data similar to LUCHS, but were only on a few
infobox classes. In contrast, LUCHS shows that
the idea scales to more than 5000 relations, but
that additional techniques, such as dynamic lexi-
con learning, are necessary to deal with sparsity.
Extraction with lexicons While lexicons have
been commonly used for IE (Cohen and Sarawagi,
2004; Agichtein and Ganti, 2004; Bellare and Mc-
Callum, 2007), many approaches assume that lex-
icons are clean and are supplied by a user before
training. Other approaches (Talukdar et al, 2006;
Miller et al, 2004; Riloff, 1993) learn lexicons
automatically from distributional patterns in text.
(Wang et al, 2009) learns lexicons from Web lists
for query tagging. LUCHS differs from these ap-
proaches in that it is not limited to a small set of
well-defined relations. Rather than creating large
lexicons of common entities, LUCHS attempts to
efficiently instantiate a series of lexicons from a
small set of seeds to bias extractors of sparse at-
tributes. Crucual to LUCHS?s different setting is
also the need to avoid overfitting.
Set expansion A large amount of work has
looked at automatically generating sets of related
items. Starting with a set of seed terms, (Etzioni
et al, 2004) extract lists by learning wrappers for
Web pages containing those terms. (Wang and Co-
hen, 2007; Wang and Cohen, 2008) extend the
idea, computing term relatedness through a ran-
dom walk algorithm that takes into account seeds,
documents, wrappers and mentions. Other ap-
proaches include Bayesian methods (Ghahramani
and Heller, 2005) and graph label propagation al-
gorithms (Talukdar et al, 2008; Bengio et al,
2006). The goal of set expansion techniques is
to generate high precision sets of related items;
hence, these techniques are evaluated based on
lexicon precision and recall. For LUCHS, which is
evaluated based on the quality of an extractor us-
ing the lexicons, lexicon precision is not important
? as long as it does not confuse the extractor.
7 Future Work
We envision a Web-scale machine reading system
which simultaneously learns ontologies and ex-
tractors, and we believe that LUCHS?s approach
of leveraging noisy semi-structured information
(such as lists or formatting templates) is a key to-
wards this goal. For future work, we plan to en-
hance LUCHS in two major ways.
First, we note that a big weakness is that the
system currently only works for Wikipedia pages.
293
For example, LUCHS assumes that each page cor-
responds to exactly one schema and that the sub-
ject of relations on a page are the same. Also,
LUCHS makes predictions on a token basis, thus
sometimes failing to recognize larger segments.
To remove these limitations we plan to add a
deeper linguistic analysis, making better use of
parse and dependency information and including
coreference resolution. We also plan to employ
relation-independent Open extraction techniques,
e.g. as suggested in (Wu and Weld, 2008) (retrain-
ing).
Second, we note that LUCHS?s performance
may benefit substantially from an attribute ontol-
ogy. As we showed in Section 5.6, LUCHS?s cur-
rent extractors can also greatly facilitate learning
a full attribute ontology. We therefore plan to in-
terleave extractor learning and ontology inference,
hence jointly learning ontology and extractors.
8 Conclusion
Many researchers are trying to use IE to cre-
ate large-scale knowledge bases from natural lan-
guage text on the Web, but existing relation-
specific techniques do not scale to the thousands
of relations encoded in Web text ? while relation-
independent techniques suffer from lower preci-
sion and recall, and do not canonicalize the rela-
tions. This paper shows that ? with new techniques
? self-supervised learning of relation-specific ex-
tractors from Wikipedia infoboxes does scale.
In particular, we present LUCHS, a self-
supervised IE system capable of learning more
than an order of magnitude more relation-specific
extractors than previous systems. LUCHS uses
dynamic lexicon features that enable hyper-
lexicalized extractors which cope effectively with
sparse training data. We show an overall perfor-
mance of 61% F1 score, and present experiments
evaluating LUCHS?s individual components.
Datasets generated in this work are available to
the community7.
Acknowledgments
We thank Jesse Davis, Oren Etzioni, Andrey Kolobov,
Mausam, Fei Wu, and the anonymous reviewers for helpful
comments and suggestions.
This material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by the Air
Force Research Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of
7http://www.cs.washington.edu/ai/iwp
the author(s) and do not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Eugene Agichtein and Venkatesh Ganti. 2004. Mining refer-
ence tables for automatic text segmentation. In Proceed-
ings of the Tenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-2004),
pages 20?29.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007.
Dbpedia: A nucleus for a web of open data. In Proceed-
ings of the 6th International Semantic Web Conference
and 2nd Asian Semantic Web Conference (ISWC/ASWC-
2007), pages 722?735.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of the
20th International Joint Conference on Artificial Intelli-
gence (IJCAI-2007), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learning ex-
tractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration
on the Web.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.
2006. Label propagation and quadratic criterion. In
Olivier Chapelle, Bernhard Scho?lkopf, and Alexander
Zien, editors, Semi-Supervised Learning, pages 193?216.
MIT Press.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eu-
gene Wu, and Yang Zhang. 2008. Webtables: exploring
the power of tables on the web. Proceedings of the In-
ternational Conference on Very Large Databases (VLDB-
2008), 1(1):538?549.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. 2009a. Coupling semi-supervised
learning of categories and relations. In NAACL HLT 2009
Workskop on Semi-supervised Learning for Natural Lan-
guage Processing.
Andrew Carlson, Scott Gaffney, and Flavian Vasile. 2009b.
Learning a named entity tagger from gazetteers with the
partial perceptron. In AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
William W. Cohen and Sunita Sarawagi. 2004. Exploiting
dictionaries in named entity extraction: combining semi-
markov extraction processes and data integration methods.
In Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining
(KDD-2004), pages 89?98.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information from
text sources. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology
(ISMB-1999), pages 77?86.
294
Benjamin Van Durme and Marius Pasca. 2008. Finding cars,
goddesses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extraction.
In Proceedings of the Twenty-Third AAAI Conference on
Artificial Intelligence (AAAI-2008), pages 1243?1248.
Oren Etzioni, Michael J. Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S.
Weld, and Alexander Yates. 2004. Methods for domain-
independent information extraction from the web: An ex-
perimental comparison. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
2004), pages 391?398.
Dayne Freitag. 1998. Toward general-purpose learning for
information extraction. In Proceedings of the 17th inter-
national conference on Computational linguistics, pages
404?408. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Neural Information Processing Systems
(NIPS-2005).
Lynette Hirschman, Alexander A. Morgan, and Alexander S.
Yeh. 2002. Rutabaga by any other name: extracting
biological names. Journal of Biomedical Informatics,
35(4):247?259.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning (ICML-2001), pages 282?289.
Marie-Catherine De Marneffe, Bill Maccartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
fifth international conference on Language Resources and
Evaluation (LREC-2006).
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative train-
ing. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL-2004).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky.
2009. Distant supervision for relation extraction without
labeled data. In The Annual Meeting of the Association
for Computational Linguistics (ACL-2009).
Marius Pasca. 2009. Outclassing wikipedia in open-domain
information extraction: Weakly-supervised acquisition of
attributes over conceptual hierarchies. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-2009),
pages 639?647.
Ellen Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In Proceedings of the
11th National Conference on Artificial Intelligence (AAAI-
1993), pages 811?816.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008. Yago: A large ontology from wikipedia and word-
net. Elsevier Journal of Web Semantics, 6(3):203?217.
Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum.
2009. Sofie: A self-organizing framework for informa-
tion extraction. In Proceedings of the 18th International
Conference on World Wide Web (WWW-2009).
Partha Pratim Talukdar, Thorsten Brants, Mark Liberman,
and Fernando Pereira. 2006. A context pattern induction
method for named entity extraction. In The Tenth Confer-
ence on Natural Language Learning (CoNLL-X-2006).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of labeled
class instances using graph random walks. In EMNLP,
pages 582?590.
Richard C. Wang and William W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. In Proceedings of the 7th IEEE International Con-
ference on Data Mining (ICDM-2007), pages 342?350.
Richard C. Wang and William W. Cohen. 2008. Iterative set
expansion of named entities using the web. In Proceed-
ings of the 8th IEEE International Conference on Data
Mining (ICDM-2008).
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of the 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC-2007), pages 580?594.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Alex Acero.
2009. Semi-supervised acquisition of semantic classes ?
from the web and for the web. In International Confer-
ence on Information and Knowledge Management (CIKM-
2009), pages 37?46.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Management
(CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using wikipedia. In The Annual Meeting of the
Association for Computational Linguistics (ACL-2010).
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008. In-
formation extraction from wikipedia: moving down the
long tail. In Proceedings of the 14th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2008), pages 731?739.
295
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541?550,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Knowledge-Based Weak Supervision for Information Extraction
of Overlapping Relations
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{raphaelh,clzhang,xiaoling,lsz,weld}@cs.washington.edu
Abstract
Information extraction (IE) holds the promise
of generating a large-scale knowledge
base from the Web?s natural language text.
Knowledge-based weak supervision, using
structured data to heuristically label a training
corpus, works towards this goal by enabling
the automated learning of a potentially
unbounded number of relation extractors.
Recently, researchers have developed multi-
instance learning algorithms to combat the
noisy training data that can come from
heuristic labeling, but their models assume
relations are disjoint ? for example they
cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple).
This paper presents a novel approach for
multi-instance learning with overlapping re-
lations that combines a sentence-level extrac-
tion model with a simple, corpus-level compo-
nent for aggregating the individual facts. We
apply our model to learn extractors for NY
Times text using weak supervision from Free-
base. Experiments show that the approach
runs quickly and yields surprising gains in
accuracy, at both the aggregate and sentence
level.
1 Introduction
Information-extraction (IE), the process of generat-
ing relational data from natural-language text, con-
tinues to gain attention. Many researchers dream of
creating a large repository of high-quality extracted
tuples, arguing that such a knowledge base could
benefit many important tasks such as question an-
swering and summarization. Most approaches to IE
use supervised learning of relation-specific exam-
ples, which can achieve high precision and recall.
Unfortunately, however, fully supervised methods
are limited by the availability of training data and are
unlikely to scale to the thousands of relations found
on the Web.
A more promising approach, often called ?weak?
or ?distant? supervision, creates its own training
data by heuristically matching the contents of a
database to corresponding text (Craven and Kum-
lien, 1999). For example, suppose that r(e1, e2) =
Founded(Jobs,Apple) is a ground tuple in the
database and s =?Steve Jobs founded Apple, Inc.?
is a sentence containing synonyms for both e1 =
Jobs and e2 = Apple, then s may be a natural
language expression of the fact that r(e1, e2) holds
and could be a useful training example.
While weak supervision works well when the tex-
tual corpus is tightly aligned to the database con-
tents (e.g., matching Wikipedia infoboxes to as-
sociated articles (Hoffmann et al, 2010)), Riedel
et al (2010) observe that the heuristic leads to
noisy data and poor extraction performance when
the method is applied more broadly (e.g., matching
Freebase records to NY Times articles). To fix
this problem they cast weak supervision as a form of
multi-instance learning, assuming only that at least
one of the sentences containing e1 and e2 are ex-
pressing r(e1, e2), and their method yields a sub-
stantial improvement in extraction performance.
However, Riedel et al?s model (like that of
previous systems (Mintz et al, 2009)) assumes
that relations do not overlap ? there cannot
exist two facts r(e1, e2) and q(e1, e2) that are
both true for any pair of entities, e1 and e2.
Unfortunately, this assumption is often violated;
541
for example both Founded(Jobs, Apple) and
CEO-of(Jobs, Apple) are clearly true. In-
deed, 18.3% of the weak supervision facts in Free-
base that match sentences in the NY Times 2007 cor-
pus have overlapping relations.
This paper presents MULTIR, a novel model of
weak supervision that makes the following contri-
butions:
? MULTIR introduces a probabilistic, graphical
model of multi-instance learning which handles
overlapping relations.
? MULTIR also produces accurate sentence-level
predictions, decoding individual sentences as
well as making corpus-level extractions.
? MULTIR is computationally tractable. Inference
reduces to weighted set cover, for which it uses
a greedy approximation with worst case running
time O(|R| ? |S|) where R is the set of possi-
ble relations and S is largest set of sentences for
any entity pair. In practice, MULTIR runs very
quickly.
? We present experiments showing that MULTIR
outperforms a reimplementation of Riedel
et al (2010)?s approach on both aggregate (cor-
pus as a whole) and sentential extractions.
Additional experiments characterize aspects of
MULTIR?s performance.
2 Weak Supervision from a Database
Given a corpus of text, we seek to extract facts about
entities, such as the company Apple or the city
Boston. A ground fact (or relation instance), is
an expression r(e) where r is a relation name, for
example Founded or CEO-of, and e = e1, . . . , en
is a list of entities.
An entity mention is a contiguous sequence of tex-
tual tokens denoting an entity. In this paper we as-
sume that there is an oracle which can identify all
entity mentions in a corpus, but the oracle doesn?t
normalize or disambiguate these mentions. We use
ei ? E to denote both an entity and its name (i.e.,
the tokens in its mention).
A relation mention is a sequence of text (in-
cluding one or more entity mentions) which states
that some ground fact r(e) is true. For example,
?Steve Ballmer, CEO of Microsoft, spoke recently
at CES.? contains three entity mentions as well as a
relation mention for CEO-of(Steve Ballmer,
Microsoft). In this paper we restrict our atten-
tion to binary relations. Furthermore, we assume
that both entity mentions appear as noun phrases in
a single sentence.
The task of aggregate extraction takes two inputs,
?, a set of sentences comprising the corpus, and an
extraction model; as output it should produce a set
of ground facts, I , such that each fact r(e) ? I is
expressed somewhere in the corpus.
Sentential extraction takes the same input and
likewise produces I , but in addition it also produces
a function, ? : I ? P(?), which identifies, for
each r(e) ? I , the set of sentences in ? that contain
a mention describing r(e). In general, the corpus-
level extraction problem is easier, since it need only
make aggregate predictions, perhaps using corpus-
wide statistics. In contrast, sentence-level extrac-
tion must justify each extraction with every sentence
which expresses the fact.
The knowledge-based weakly supervised learning
problem takes as input (1) ?, a training corpus, (2)
E, a set of entities mentioned in that corpus, (3) R,
a set of relation names, and (4), ?, a set of ground
facts of relations in R. As output the learner pro-
duces an extraction model.
3 Modeling Overlapping Relations
We define an undirected graphical model that al-
lows joint reasoning about aggregate (corpus-level)
and sentence-level extraction decisions. Figure 1(a)
shows the model in plate form.
3.1 Random Variables
There exists a connected component for each pair of
entities e = (e1, e2) ? E ? E that models all of
the extraction decisions for this pair. There is one
Boolean output variable Y r for each relation name
r ? R, which represents whether the ground fact
r(e) is true. Including this set of binary random
variables enables our model to extract overlapping
relations.
Let S(e1,e2) ? ? be the set of sentences which
contain mentions of both of the entities. For each
sentence xi ? S(e1,e2) there exists a latent variable
Zi which ranges over the relation names r ? R and,
542
E ? E 
? 
R 
S 
?? 
(a)
Steve Jobs was founder  
of Apple. 
Steve Jobs, Steve Wozniak and 
Ronald Wayne founded Apple. 
Steve Jobs is CEO of  
Apple. 
founder ?? founder none 
0 
?????? 
1 0 0 
?? ?? 
... 
... 
... 
????????? ????????? ????????? 
(b)
Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entities
Steve Jobs, Apple.
importantly, also the distinct value none. Zi should
be assigned a value r ? R only when xi expresses
the ground fact r(e), thereby modeling sentence-
level extraction.
Figure 1(b) shows an example instantiation of the
model with four relation names and three sentences.
3.2 A Joint, Conditional Extraction Model
We use a conditional probability model that defines
a joint distribution over all of the extraction random
variables defined above. The model is undirected
and includes repeated factors for making sentence
level predictions as well as globals factors for ag-
gregating these choices.
For each entity pair e = (e1, e2), define x to
be a vector concatenating the individual sentences
xi ? S(e1,e2), Y to be vector of binary Y
r random
variables, one for each r ? R, and Z to be the vec-
tor of Zi variables, one for each sentence xi. Our
conditional extraction model is defined as follows:
p(Y = y,Z = z|x; ?)
def
=
1
Zx
?
r
?join(yr, z)
?
i
?extract(zi, xi)
where the parameter vector ? is used, below, to de-
fine the factor ?extract.
The factors ?join are deterministic OR operators
?join(yr, z)
def
=
{
1 if yr = true ? ?i : zi = r
0 otherwise
which are included to ensure that the ground fact
r(e) is predicted at the aggregate level for the as-
signment Y r = yr only if at least one of the sen-
tence level assignments Zi = zi signals a mention
of r(e).
The extraction factors ?extract are given by
?extract(zi, xi)
def
= exp
?
?
?
j
?j?j(zi, xi)
?
?
where the features ?j are sensitive to the relation
name assigned to extraction variable zi, if any, and
cues from the sentence xi. We will make use of the
Mintz et al (2009) sentence-level features in the ex-
peiments, as described in Section 7.
3.3 Discussion
This model was designed to provide a joint approach
where extraction decisions are almost entirely driven
by sentence-level reasoning. However, defining the
Y r random variables and tying them to the sentence-
level variables, Zi, provides a direct method for
modeling weak supervision. We can simply train the
model so that the Y variables match the facts in the
database, treating the Zi as hidden variables that can
take any value, as long as they produce the correct
aggregate predictions.
This approach is related to the multi-instance
learning approach of Riedel et al (2010), in that
both models include sentence-level and aggregate
random variables. However, their sentence level
variables are binary and they only have a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. Addition-
ally, their aggregate decisions make use of Mintz-
style aggregate features (Mintz et al, 2009), that col-
lect evidence from multiple sentences, while we use
543
Inputs:
(1) ?, a set of sentences,
(2)E, a set of entities mentioned in the sentences,
(3) R, a set of relation names, and
(4) ?, a database of atomic facts of the form
r(e1, e2) for r ? R and ei ? E.
Definitions:
We define the training set {(xi,yi)|i = 1 . . . n},
where i is an index corresponding to a particu-
lar entity pair (ej , ek) in ?, xi contains all of
the sentences in ? with mentions of this pair, and
yi = relVector(ej , ek).
Computation:
initialize parameter vector ?? 0
for t = 1...T do
for i = 1...n do
(y?, z?)? arg maxy,z p(y, z|xi; ?)
if y? 6= yi then
z? ? arg maxz p(z|xi,yi; ?)
?? ? + ?(xi, z?)? ?(xi, z?)
end if
end for
end for
Return ?
Figure 2: The MULTIR Learning Algorithm
only the deterministic OR nodes. Perhaps surpris-
ing, we are still able to improve performance at both
the sentential and aggregate extraction tasks.
4 Learning
We now present a multi-instance learning algo-
rithm for our weak-supervision model that treats the
sentence-level extraction random variables Zi as la-
tent, and uses facts from a database (e.g., Freebase)
as supervision for the aggregate-level variables Y r.
As input we have (1) ?, a set of sentences, (2)
E, a set of entities mentioned in the sentences, (3)
R, a set of relation names, and (4) ?, a database
of atomic facts of the form r(e1, e2) for r ? R and
ei ? E. Since we are using weak learning, the Y r
variables in Y are not directly observed, but can be
approximated from the database ?. We use a proce-
dure, relVector(e1, e2) to return a bit vector whose
jth bit is one if rj(e1, e2) ? ?. The vector does not
have a bit for the special none relation; if there is no
relation between the two entities, all bits are zero.
Finally, we can now define the training set to be
pairs {(xi,yi)|i = 1 . . . n}, where i is an index
corresponding to a particular entity pair (ej , ek), xi
contains all of the sentences with mentions of this
pair, and yi = relVector(ej , ek).
Given this form of supervision, we would like to
find the setting for ? with the highest likelihood:
O(?) =
?
i
p(yi|xi; ?) =
?
i
?
z
p(yi, z|xi; ?)
However, this objective would be difficult to op-
timize exactly, and algorithms for doing so would
be unlikely to scale to data sets of the size we con-
sider. Instead, we make two approximations, de-
scribed below, leading to a Perceptron-style addi-
tive (Collins, 2002) parameter update scheme which
has been modified to reason about hidden variables,
similar in style to the approaches of (Liang et al,
2006; Zettlemoyer and Collins, 2007), but adapted
for our specific model. This approximate algorithm
is computationally efficient and, as we will see,
works well in practice.
Our first modification is to do online learning
instead of optimizing the full objective. Define the
feature sums ?(x, z) =
?
j ?(xj , zj) which range
over the sentences, as indexed by j. Now, we can
define an update based on the gradient of the local
log likelihood for example i:
? logOi(?)
??j
= Ep(z|xi,yi;?)[?j(xi, z)]
?Ep(y,z|xi;?)[?j(xi, z)]
where the deterministic OR ?join factors ensure that
the first expectation assigns positive probability only
to assignments that produce the labeled facts yi but
that the second considers all valid sets of extractions.
Of course, these expectations themselves, espe-
cially the second one, would be difficult to com-
pute exactly. Our second modification is to do
a Viterbi approximation, by replacing the expecta-
tions with maximizations. Specifically, we compute
the most likely sentence extractions for the label
facts arg maxz p(z|xi,yi; ?) and the most likely ex-
traction for the input, without regard to the labels,
arg maxy,z p(y, z|xi; ?). We then compute the fea-
tures for these assignments and do a simple additive
update. The final algorithm is detailed in Figure 2.
544
5 Inference
To support learning, as described above, we need
to compute assignments arg maxz p(z|x,y; ?) and
arg maxy,z p(y, z|x; ?). In this section, we describe
algorithms for both cases that use the deterministic
OR nodes to simplify the required computations.
Predicting the most likely joint extraction
arg maxy,z p(y, z|x; ?) can be done efficiently
given the structure of our model. In particular, we
note that the factors ?join represent deterministic de-
pendencies between Z and Y, which when satisfied
do not affect the probability of the solution. It is thus
sufficient to independently compute an assignment
for each sentence-level extraction variable Zi, ignor-
ing the deterministic dependencies. The optimal set-
ting for the aggregate variables Y is then simply the
assignment that is consistent with these extractions.
The time complexity is O(|R| ? |S|).
Predicting sentence level extractions given weak
supervision facts, arg maxz p(z|x,y; ?), is more
challenging. We start by computing extraction
scores ?extract(xi, zi) for each possible extraction as-
signment Zi = zi at each sentence xi ? S, and
storing the values in a dynamic programming table.
Next, we must find the most likely assignment z that
respects our output variables y. It turns out that
this problem is a variant of the weighted, edge-cover
problem, for which there exist polynomial time op-
timal solutions.
Let G = (E ,V = VS ? Vy) be a complete
weighted bipartite graph with one node vSi ? V
S for
each sentence xi ? S and one node v
y
r ? Vy for each
relation r ? R where yr = 1. The edge weights are
given by c((vSi , v
y
r ))
def
= ?extract(xi, zi). Our goal is
to select a subset of the edges which maximizes the
sum of their weights, subject to each node vSi ? V
S
being incident to exactly one edge, and each node
vyr ? Vy being incident to at least one edge.
Exact Solution An exact solution can be obtained
by first computing the maximum weighted bipartite
matching, and adding edges to nodes which are not
incident to an edge. This can be computed in time
O(|V|(|E| + |V| log |V|)), which we can rewrite as
O((|R|+ |S|)(|R||S|+ (|R|+ |S|) log(|R|+ |S|))).
Approximate Solution An approximate solution
can be obtained by iterating over the nodes in Vy,
????????  ???????????  
?? ?? ?? 
????????????? ???????????????? 
????? 
Figure 3: Inference of arg maxz p(Z = z|x,y) requires
solving a weighted, edge-cover problem.
and each time adding the highest weight incident
edge whose addition doesn?t violate a constraint.
The running time is O(|R||S|). This greedy search
guarantees each fact is extracted at least once and
allows any additional extractions that increase the
overall probability of the assignment. Given the
computational advantage, we use it in all of the ex-
perimental evaluations.
6 Experimental Setup
We follow the approach of Riedel et al (2010) for
generating weak supervision data, computing fea-
tures, and evaluating aggregate extraction. We also
introduce new metrics for measuring sentential ex-
traction performance, both relation-independent and
relation-specific.
6.1 Data Generation
We used the same data sets as Riedel et al (2010)
for weak supervision. The data was first tagged with
the Stanford NER system (Finkel et al, 2005) and
then entity mentions were found by collecting each
continuous phrase where words were tagged iden-
tically (i.e., as a person, location, or organization).
Finally, these phrases were matched to the names of
Freebase entities.
Given the set of matches, define ? to be set of NY
Times sentences with two matched phrases, E to be
the set of Freebase entities which were mentioned in
one or more sentences, ? to be the set of Freebase
facts whose arguments, e1 and e2 were mentioned in
a sentence in ?, and R to be set of relations names
used in the facts of ?. These sets define the weak
supervision data.
6.2 Features and Initialization
We use the set of sentence-level features described
by Riedel et al (2010), which were originally de-
545
veloped by Mintz et al (2009). These include in-
dicators for various lexical, part of speech, named
entity, and dependency tree path properties of entity
mentions in specific sentences, as computed with the
Malt dependency parser (Nivre and Nilsson, 2004)
and OpenNLP POS tagger1. However, unlike the
previous work, we did not make use of any features
that explicitly aggregate these properties across mul-
tiple mention instances.
The MULTIR algorithm has a single parameter T ,
the number of training iterations, that must be spec-
ified manually. We used T = 50 iterations, which
performed best in development experiments.
6.3 Evaluation Metrics
Evaluation is challenging, since only a small per-
centage (approximately 3%) of sentences match
facts in Freebase, and the number of matches is
highly unbalanced across relations, as we will see
in more detail later. We use the following metrics.
Aggregate Extraction Let ?e be the set of ex-
tracted relations for any of the systems; we com-
pute aggregate precision and recall by comparing
?e with ?. This metric is easily computed but un-
derestimates extraction accuracy because Freebase
is incomplete and some true relations in ?e will be
marked wrong.
Sentential Extraction Let Se be the sentences
where some system extracted a relation and SF be
the sentences that match the arguments of a fact in
?. We manually compute sentential extraction ac-
curacy by sampling a set of 1000 sentences from
Se ? SF and manually labeling the correct extrac-
tion decision, either a relation r ? R or none. We
then report precision and recall for each system on
this set of sampled sentences. These results provide
a good approximation to the true precision but can
overestimate the actual recall, since we did not man-
ually check the much larger set of sentences where
no approach predicted extractions.
6.4 Precision / Recall Curves
To compute precision / recall curves for the tasks,
we ranked the MULTIR extractions as follows. For
sentence-level evaluations, we ordered according to
1http://opennlp.sourceforge.net/
Recall
Precision
0.00 0.05 0.10 0.15 0.20 0.25 0.30
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
Riedel et al, 2010
MULTIR
Figure 4: Aggregate extraction precision / recall curves
for Riedel et al (2010), a reimplementation of that ap-
proach (SOLOR), and our algorithm (MULTIR).
the extraction factor score ?extract(zi, xi). For aggre-
gate comparisons, we set the score for an extraction
Y r = true to be the max of the extraction factor
scores for the sentences where r was extracted.
7 Experiments
To evaluate our algorithm, we first compare it to an
existing approach for using multi-instance learning
with weak supervision (Riedel et al, 2010), using
the same data and features. We report both aggregate
extraction and sentential extraction results. We then
investigate relation-specific performance of our sys-
tem. Finally, we report running time comparisons.
7.1 Aggregate Extraction
Figure 4 shows approximate precision / recall curves
for three systems computed with aggregate metrics
(Section 6.3) that test how closely the extractions
match the facts in Freebase. The systems include the
original results reported by Riedel et al (2010) as
well as our new model (MULTIR). We also compare
with SOLOR, a reimplementation of their algorithm,
which we built in Factorie (McCallum et al, 2009),
and will use later to evaluate sentential extraction.
MULTIR achieves competitive or higher preci-
sion over all ranges of recall, with the exception
of the very low recall range of approximately 0-
1%. It also significantly extends the highest recall
achieved, from 20% to 25%, with little loss in preci-
sion. To investigate the low precision in the 0-1% re-
call range, we manually checked the ten highest con-
546
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
MULTIR
Figure 5: Sentential extraction precision / recall curves
for MULTIR and SOLOR.
fidence extractions produced by MULTIR that were
marked wrong. We found that all ten were true facts
that were simply missing from Freebase. A manual
evaluation, as we perform next for sentential extrac-
tion, would remove this dip.
7.2 Sentential Extraction
Although their model includes variables to model
sentential extraction, Riedel et al (2010) did not re-
port sentence level performance. To generate the
precision / recall curve we used the joint model as-
signment score for each of the sentences that con-
tributed to the aggregate extraction decision.
Figure 4 shows approximate precision / recall
curves for MULTIR and SOLOR computed against
manually generated sentence labels, as defined in
Section 6.3. MULTIR achieves significantly higher
recall with a consistently high level of precision. At
the highest recall point, MULTIR reaches 72.4% pre-
cision and 51.9% recall, for an F1 score of 60.5%.
7.3 Relation-Specific Performance
Since the data contains an unbalanced number of in-
stances of each relation, we also report precision and
recall for each of the ten most frequent relations. Let
SMr be the sentences where MULTIR extracted an
instance of relation r ? R, and let SFr be the sen-
tences that match the arguments of a fact about re-
lation r in ?. For each r, we sample 100 sentences
from both SMr and S
F
r and manually check accu-
racy. To estimate precision P?r we compute the ratio
of true relation mentions in SMr , and to estimate re-
call R?r we take the ratio of true relation mentions in
SFr which are returned by our system.
Table 1 presents this approximate precision and
recall for MULTIR on each of the relations, along
with statistics we computed to measure the qual-
ity of the weak supervision. Precision is high for
the majority of relations but recall is consistently
lower. We also see that the Freebase matches are
highly skewed in quantity and can be low quality for
some relations, with very few of them actually cor-
responding to true extractions. The approach gener-
ally performs best on the relations with a sufficiently
large number of true matches, in many cases even
achieving precision that outperforms the accuracy of
the heuristic matches, at reasonable recall levels.
7.4 Overlapping Relations
Table 1 also highlights some of the effects of learn-
ing with overlapping relations. For example, in the
data, almost all of the matches for the administra-
tive divisions relation overlap with the contains re-
lation, because they both model relationships for a
pair of locations. Since, in general, sentences are
much more likely to describe a contains relation, this
overlap leads to a situation were almost none of the
administrate division matches are true ones, and we
cannot accurately learn an extractor. However, we
can still learn to accurately extract the contains rela-
tion, despite the distracting matches. Similarly, the
place of birth and place of death relations tend to
overlap, since it is often the case that people are born
and die in the same city. In both cases, the precision
outperforms the labeling accuracy and the recall is
relatively high.
To measure the impact of modeling overlapping
relations, we also evaluated a simple, restricted
baseline. Instead of labeling each entity pair with
the set of all true Freebase facts, we created a dataset
where each true relation was used to create a dif-
ferent training example. Training MULTIR on this
data simulates effects of conflicting supervision that
can come from not modeling overlaps. On average
across relations, precision increases 12 points but re-
call drops 26 points, for an overall reduction in F1
score from 60.5% to 40.3%.
7.5 Running Time
One final advantage of our model is the mod-
est running time. Our implementation of the
547
Relation
Freebase Matches MULTIR
#sents % true P? R?
/business/person/company 302 89.0 100.0 25.8
/people/person/place lived 450 60.0 80.0 6.7
/location/location/contains 2793 51.0 100.0 56.0
/business/company/founders 95 48.4 71.4 10.9
/people/person/nationality 723 41.0 85.7 15.0
/location/neighborhood/neighborhood of 68 39.7 100.0 11.1
/people/person/children 30 80.0 100.0 8.3
/people/deceased person/place of death 68 22.1 100.0 20.0
/people/person/place of birth 162 12.0 100.0 33.0
/location/country/administrative divisions 424 0.2 N/A 0.0
Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy
(% true) of matches between sentences and facts in Freebase.
Riedel et al (2010) approach required approxi-
mately 6 hours to train on NY Times 05-06 and 4
hours to test on the NY Times 07, each without pre-
processing. Although they do sampling for infer-
ence, the global aggregation variables require rea-
soning about an exponentially large (in the number
of sentences) sample space.
In contrast, our approach required approximately
one minute to train and less than one second to test,
on the same data. This advantage comes from the
decomposition that is possible with the determinis-
tic OR aggregation variables. For test, we simply
consider each sentence in isolation and during train-
ing our approximation to the weighted assignment
problem is linear in the number of sentences.
7.6 Discussion
The sentential extraction results demonstrates the
advantages of learning a model that is primarily
driven by sentence-level features. Although previ-
ous approaches have used more sophisticated fea-
tures for aggregating the evidence from individual
sentences, we demonstrate that aggregating strong
sentence-level evidence with a simple deterministic
OR that models overlapping relations is more effec-
tive, and also enables training of a sentence extractor
that runs with no aggregate information.
While the Riedel et al approach does include a
model of which sentences express relations, it makes
significant use of aggregate features that are primar-
ily designed to do entity-level relation predictions
and has a less detailed model of extractions at the
individual sentence level. Perhaps surprisingly, our
model is able to do better at both the sentential and
aggregate levels.
8 Related Work
Supervised-learning approaches to IE were intro-
duced in (Soderland et al, 1995) and are too nu-
merous to summarize here. While they offer high
precision and recall, these methods are unlikely to
scale to the thousands of relations found in text on
the Web. Open IE systems, which perform self-
supervised learning of relation-independent extrac-
tors (e.g., Preemptive IE (Shinyama and Sekine,
2006), TEXTRUNNER (Banko et al, 2007; Banko
and Etzioni, 2008) and WOE (Wu and Weld, 2010))
can scale to millions of documents, but don?t output
canonicalized relations.
8.1 Weak Supervision
Weak supervision (also known as distant- or self su-
pervision) refers to a broad class of methods, but
we focus on the increasingly-popular idea of using
a store of structured data to heuristicaly label a tex-
tual corpus. Craven and Kumlien (1999) introduced
the idea by matching the Yeast Protein Database
(YPD) to the abstracts of papers in PubMed and
training a naive-Bayes extractor. Bellare and Mc-
Callum (2007) used a database of BibTex records
to train a CRF extractor on 12 bibliographic rela-
tions. The KYLIN system aplied weak supervision
to learn relations from Wikipedia, treating infoboxes
as the associated database (Wu and Weld, 2007);
Wu et al (2008) extended the system to use smooth-
ing over an automatically generated infobox taxon-
548
omy. Mintz et al (2009) used Freebase facts to train
100 relational extractors on Wikipedia. Hoffmann
et al (2010) describe a system similar to KYLIN,
but which dynamically generates lexicons in order
to handle sparse data, learning over 5000 Infobox
relations with an average F1 score of 61%. Yao
et al (2010) perform weak supervision, while using
selectional preference constraints to a jointly reason
about entity types.
The NELL system (Carlson et al, 2010) can also
be viewed as performing weak supervision. Its ini-
tial knowledge consists of a selectional preference
constraint and 20 ground fact seeds. NELL then
matches entity pairs from the seeds to a Web cor-
pus, but instead of learning a probabilistic model,
it bootstraps a set of extraction patterns using semi-
supervised methods for multitask learning.
8.2 Multi-Instance Learning
Multi-instance learning was introduced in order to
combat the problem of ambiguously-labeled train-
ing data when predicting the activity of differ-
ent drugs (Dietterich et al, 1997). Bunescu and
Mooney (2007) connect weak supervision with
multi-instance learning and extend their relational
extraction kernel to this context.
Riedel et al (2010), combine weak supervision
and multi-instance learning in a more sophisticated
manner, training a graphical model, which assumes
only that at least one of the matches between the
arguments of a Freebase fact and sentences in the
corpus is a true relational mention. Our model may
be seen as an extension of theirs, since both models
include sentence-level and aggregate random vari-
ables. However, Riedel et al have only a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. We have
discussed the comparison in more detail throughout
the paper, including in the model formulation sec-
tion and experiments.
9 Conclusion
We argue that weak supervision is promising method
for scaling information extraction to the level where
it can handle the myriad, different relations on the
Web. By using the contents of a database to heuris-
tically label a training corpus, we may be able to
automatically learn a nearly unbounded number of
relational extractors. Since the processs of match-
ing database tuples to sentences is inherently heuris-
tic, researchers have proposed multi-instance learn-
ing algorithms as a means for coping with the result-
ing noisy data. Unfortunately, previous approaches
assume that all relations are disjoint ? for exam-
ple they cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple), because
two relations are not allowed to have the same argu-
ments.
This paper presents a novel approach for multi-
instance learning with overlapping relations that
combines a sentence-level extraction model with a
simple, corpus-level component for aggregating the
individual facts. We apply our model to learn extrac-
tors for NY Times text using weak supervision from
Freebase. Experiments show improvements for both
sentential and aggregate (corpus level) extraction,
and demonstrate that the approach is computation-
ally efficient.
Our early progress suggests many interesting di-
rections. By joining two or more Freebase tables,
we can generate many more matches and learn more
relations. We also wish to refine our model in order
to improve precision. For example, we would like
to add type reasoning about entities and selectional
preference constraints for relations. Finally, we are
also interested in applying the overall learning ap-
proaches to other tasks that could be modeled with
weak supervision, such as coreference and named
entity classification.
The source code of our system, its out-
put, and all data annotations are available at
http://cs.uw.edu/homes/raphaelh/mr.
Acknowledgments
We thank Sebastian Riedel and Limin Yao for shar-
ing their data and providing valuable advice. This
material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
549
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-08), pages
28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth International Workshop on Infor-
mation Integration on the Web.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI-10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology, pages 77?86.
Thomas G. Dietterich, Richard H. Lathrop, and Toma?s
Lozano-Pe?rez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial Intel-
ligence, 89:31?71, January.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
05), pages 363?370.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
286?295.
Percy Liang, A. Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In International Conference on
Computational Linguistics and Association for Com-
putational Linguistics (COLING/ACL).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Neural Information
Processing Systems Conference (NIPS).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003?1011.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148?163.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computation Linguistics (HLT-
NAACL-06).
Stephen Soderland, David Fisher, Jonathan Aseltine, and
Wendy G. Lehnert. 1995. Crystal: Inducing a concep-
tual dictionary. In Proceedings of the Fourteenth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-1995), pages 1314?1321.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In The Annual Meeting of
the Association for Computational Linguistics (ACL-
2010), pages 118?127.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013?1023.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-2007).
550
