Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886?897,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
What Can We Get From 1000 Tokens?
A Case Study of Multilingual POS Tagging For Resource-Poor Languages
Long Duong,
12
Trevor Cohn,
1
Karin Verspoor,
1
Steven Bird,
1
and Paul Cook
1
1
Department of Computing and Information Systems,
The University of Melbourne
2
National ICT Australia, Victoria Research Laboratory
lduong@student.unimelb.edu.au
{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.au
Abstract
In this paper we address the problem
of multilingual part-of-speech tagging for
resource-poor languages. We use par-
allel data to transfer part-of-speech in-
formation from resource-rich to resource-
poor languages. Additionally, we use a
small amount of annotated data to learn to
?correct? errors from projected approach
such as tagset mismatch between lan-
guages, achieving state-of-the-art perfor-
mance (91.3%) across 8 languages. Our
approach is based on modest data require-
ments, and uses minimum divergence clas-
sification. For situations where no uni-
versal tagset mapping is available, we
propose an alternate method, resulting
in state-of-the-art 85.6% accuracy on the
resource-poor language Malagasy.
1 Introduction
Part-of-speech (POS) tagging is a crucial task for
natural language processing (NLP) tasks, provid-
ing basic information about syntax. Supervised
POS tagging has achieved great success, reach-
ing as high as 95% accuracy for many languages
(Petrov et al., 2012). However, supervised tech-
niques need manually annotated data, and this
is either lacking or limited in most resource-
poor languages. Fully unsupervised POS tagging
is not yet useful in practice due to low accu-
racy (Christodoulopoulos et al., 2010). In this pa-
per, we propose a semi-supervised method to nar-
row the gap between supervised and unsupervised
approaches. We demonstrate that even a small
amount of supervised data leads to substantial im-
provement.
Our method is motivated by the availability of
parallel data. Thanks to the development of mul-
tilingual documents from government projects,
book translations, multilingual websites, and so
forth, parallel data between resource-rich and
resource-poor languages is relatively easy to ac-
quire. This parallel data provides the bridge that
permits us to transfer POS information from a
resource-rich to a resource-poor language.
Systems that make use of cross-lingual tag
projection typically face several issues, includ-
ing mismatches between the tagsets used for the
languages, artifacts from noisy alignments and
cross-lingual syntactic divergence. Our approach
compensates for these issues by training on a
small amount of annotated data on the target side,
demonstrating that only 1k tokens of annotated
data is sufficient to improve performance.
We first tag the resource-rich language using a
supervised POS tagger. We then project POS tags
from the resource-rich language to the resource-
poor language using parallel word alignments.
The projected labels are noisy, and so we use
various heuristics to select only ?good? training
examples. We train the model in two stages.
First, we build a maximum entropy classifier T
on the (noisy) projected data. Next, we train
a supervised classifier P on a small amount of
annotated data (1,000 tokens) in the target lan-
guage, using a minimum divergence technique
to incorporate the first model, T . Compared
with the state of the art (T?ackstr?om et al., 2013),
we make more-realistic assumptions (e.g. relying
on a tiny amount of annotated data rather than
a huge crowd-sourced dictionary) and use less
parallel data, yet achieve a better overall result.
We achieved 91.3% average accuracy over 8 lan-
guages, exceeding T?ackstr?om et al. (2013)?s result
of 88.8%.
The test data we employ makes use of map-
pings from language-specific POS tag inventories
to a universal tagset (Petrov et al., 2012). How-
ever, such a mapping might not be available for
resource-poor languages. Therefore, we also pro-
886
pose a variant of our method which removes the
need for identical tagsets between the projection
model T and the correction model P , based on
a two-output maximum entropy model over tag
pairs. Evaluating on the resource-poor language
Malagasy, we achieved 85.6% accuracy, exceed-
ing the state-of-the-art of 81.2% (Garrette et al.,
2013).
2 Background and Related Work
There is a wealth of prior work on multilingual
POS tagging. The simplest approach takes advan-
tage of the typological similarities that exist be-
tween languages pairs such as Czech and Russian,
or Serbian and Croatian. They build the tagger
? or estimate part of the tagger ? on one lan-
guage and apply it to the other language (Reddy
and Sharoff, 2011, Hana et al., 2004).
Yarowsky and Ngai (2001) pioneered the use of
parallel data for projecting tag information from
a resource-rich language to a resource-poor lan-
guage. Duong et al. (2013b) used a similar method
on using sentence alignment scores to rank the
goodness of sentences. They trained a seed model
from a small part of the data, then applied this
model to the rest of the data using self-training
with revision.
Das and Petrov (2011) also used parallel data
but additionally exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
Each node in the graph represents a trigram in the
target language. Each edge connects two nodes
which have similar context. Originally, only some
nodes received a label from direct label projection,
and then labels were propagated to the rest of the
graph. They only extracted the dictionary from
the graph because the labels of nodes are noisy.
They used the dictionary as the constraints for a
feature-based HMM tagger (Berg-Kirkpatrick et
al., 2010). Both Duong et al. (2013b) and Das and
Petrov (2011) achieved 83.4% accuracy on the test
set of 8 European languages.
Goldberg et al. (2008) pointed out that, with the
presence of a dictionary, even an incomplete one,
a modest POS tagger can be built using simple
methods such as expectation maximization. This
is because most of the time, words have a very
limited number of possible tags, thus a dictionary
that specifies the allowable tags for a word helps
to restrict the search space. With a gold-standard
dictionary, Das and Petrov (2011) achieved an ac-
curacy of approximately 94% on the same 8 lan-
guages. The effectiveness of a gold-standard dic-
tionary is undeniable, however it is costly to build
one, especially for resource-poor languages. Li et
al. (2012) used the dictionary from Wiktionary,
1
a
crowd-sourced dictionary. They scored 84.8% ac-
curacy on the same 8 languages. Currently, Wik-
tionary covers over 170 languages, but the cov-
erage varies substantially between languages and,
unsurprisingly, it is poor for resource-poor lan-
guages. Therefore, relying on Wiktionary is not
effective for building POS taggers for resource-
poor languages.
T?ackstr?om et al. (2013) combined both token
information (from direct projected data) and type
constraints (from Wiktionary?s dictionary) to form
the state-of-the-art multilingual tagger. They built
a tag lattice and used these token and type con-
straints to prune it. The remaining paths are the
training data for a CRF tagger. They achieved
88.8% accuracy on the same 8 languages.
Table 1 summarises the performance of the
above models across all 8 languages. Note that
these methods vary in their reliance on external
resources. Duong et al. (2013b) use the least, i.e.
only the Europarl Corpus (Koehn, 2005). Das and
Petrov (2011) additionally use the United Nation
Parallel Corpus. Li et al. (2012) didn?t use any par-
allel text but used Wiktionary instead. T?ackstr?om
et al. (2013) exploited more parallel data than Das
and Petrov (2011) and also used a dictionary
from Li et al. (2012).
Another approach for resource-poor languages
is based on the availability of a small amount
of annotated data. Garrette et al. (2013) built a
POS tagger for Kinyarwanda and Malagasy. They
didn?t use parallel data but instead exploited four
hours of manual annotation to build?4,000 tokens
or ?3,000 word-types of annotated data. These
tokens or word-types were used to build a tag dic-
tionary. They employed label propagation for ex-
panding the coverage of this dictionary in a sim-
ilar vein to Das and Petrov (2011), but they also
used an external dictionary. They built training
examples using the combined dictionary and then
trained the tagger on this data. They achieved
81.9% and 81.2% accuracy for Kinyarwanda and
Malagasy respectively. Note that their usage of an
external dictionary compromises their claim of us-
ing only 4 hours of annotation.
1
http://www.wiktionary.org/
887
da nl de el it pt es sv Average
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages
? Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish
(sv) ? evaluated on CoNLL data (Buchholz and Marsi, 2006).
The method we propose in this paper is similar
in only using a small amount of annotation. How-
ever, we directly use the annotated data to train
the model rather than using a dictionary. We argue
that with a proper ?guide?, we can take advantage
of very limited annotated data.
2.1 Annotated data
Our annotated data mainly comes from CoNLL
shared tasks on dependency parsing (Buchholz
and Marsi, 2006). The language specific tagsets
are mapped into the universal tagset. We will
use this annotated data mainly for evaluation. Ta-
ble 2 shows the size of annotated data for each
language. The 8 languages we are considering
in this experiment are not actually resource-poor
languages. However, running on these 8 lan-
guages makes our system comparable with pre-
viously proposed methods. Nevertheless, we try
to use as few resources as possible, in order to
simulate the situation for resource-poor languages.
Later in Section 6 we adapt the approach for Mala-
gasy, a truly resource-poor language.
2.2 Universal tagset
We employ the universal tagset from (Petrov et
al., 2012) for our experiment. It consists of 12
common tags: NOUN, VERB, ADJ (adjective),
ADV (adverb), PRON (pronoun), DET (deter-
miner and article), ADP (preposition and post-
position), CONJ (conjunctions), NUM (numeri-
cal), PRT (particle), PUNC (punctuation) and X
(all other categories including foreign words and
abbreviations). Petrov et al. (2012) provide the
mapping from each language-specific tagset to the
universal tagset.
The idea of using the universal tagset is of great
use in multilingual applications, enabling compar-
ison across languages. However, the mapping is
not always straightforward. Table 2 shows the size
of the annotated data for each language, the num-
ber of tags presented in the data, and the list of
tags that are not matched. We can see that only 8
tags are presented in the annotated data for Dan-
ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are
missing.
2
Thus, a classifier using all 12 tags will
be heavily penalized in the evaluation.
Li et al. (2012) considered this problem and
tried to manually modify the Danish mappings.
Moreover, PRT is not really a universal tag since
it only appears in 3 out of the 8 languages. Plank
et al. (2014) pointed out that PRT often gets con-
fused with ADP even in English. We will later
show that the mapping problem causes substantial
degradation in the performance of a POS tagger
exploiting parallel data. The method we present
here is more target-language oriented: our model
is trained on the target language, in this way, only
relevant information from the source language is
retained. Thus, we automatically correct the map-
ping, and other incompatibilities arising from in-
correct alignments and syntactic divergence be-
tween the source and target languages.
Lang Size(k) # Tags Not Matched
da 94 8 DET, PRT, PUNC, NUM
nl 203 11 PRT
de 712 12
el 70 12
it 76 11 PRT
pt 207 11 PRT
es 89 11 PRT
sv 191 11 DET
AVG 205
Table 2: The size of annotated data from
CoNLL (Buchholz and Marsi, 2006), and the
number of tags included and missing for 8 lan-
guages.
2
Many of these are mistakes in the mapping, however,
they are indicative of the kinds of issues expected in low-
resource languages.
888
3 Directly Projected Model (DPM)
In this section we describe a maximum entropy
tagger that only uses information from directly
projected data.
3.1 Parallel data
We first collect Europarl data having English as
the source language, an average of 1.85 million
parallel sentences for each of the 8 language pairs.
In terms of parallel data, we use far less data com-
pared with other recent work. Das and Petrov
(2011) used Europarl and the ODS United Na-
tion dataset, while T?ackstr?om et al. (2013) addi-
tionally used parallel data crawled from the web.
The amount of parallel data is crucial for align-
ment quality. Since DPM uses alignments to trans-
fer tags from source to target language, the per-
formance of DPM (and other models that exploit
projection) largely depends on the quantity of par-
allel data. The ?No LP? model of Das and Petrov
(2011), which only uses directly projected labels
(without label propagation), scored 81.3% for 8
languages. However, using the same model but
with more parallel data, T?ackstr?om et al. (2013)
scored 84.9% on the same test set.
3.2 Label projection
We use the standard alignment tool Giza++ (Och
and Ney, 2003) to word align the parallel data. We
employ the Stanford POS tagger (Toutanova et al.,
2003) to tag the English side of the parallel data
and then project the label to the target side. It has
been confirmed in many studies (T?ackstr?om et al.,
2013, Das and Petrov, 2011, Toutanova and John-
son, 2008) that directly projected labels are noisy.
Thus we need a method to reduce the noise. We
employ the strategy of Yarowsky and Ngai (2001)
of ranking sentences using a their alignment scores
from IBM model 3.
Firstly, we want to know how noisy the pro-
jected data is. Thus, we use the test data to build
a simple supervised POS tagger using the TnT
tagger (Brants, 2000) which employs a second-
order Hidden Markov Model (HMM). We tag the
projected data and compare the label from direct
projection and from the TnT tagger. The labels
from the TnT Tagger are considered as pseudo-
gold labels. Column ?Without Mapping? from Ta-
ble 3 shows the average accuracy for the first n-
sentences (n = 60k, 100k, 200k, 500k) for 8 lan-
guages according to the ranking. Column ?Cov-
erage? shows the percentages of projected label
(the other tokens are Null aligned). We can see
that when we select more data, both coverage and
accuracy fall. In other words, using the sentence
alignment score, we can rank sentences with high
coverage and accuracy first. However, even after
ranking, the accuracy of projected labels is less
than 80% demonstrating how noisy the projected
labels are.
Table 3 (column ?With Mapping?) additionally
shows the accuracy using simple tagset mapping,
i.e. mapping each tag to the tag it is assigned most
frequently in the test data. For example DET, PRT,
PUNC, NUM, missing from Danish gold data, will
be matched to PRON, X, X, ADJ respectively. This
simple matching yields a ? 4% (absolute) im-
provement in average accuracy. This illustrates the
importance of handling tagset mapping carefully.
3.3 The model
In this section, we introduce a maximum entropy
tagger exploiting the projected data. We select the
first 200k sentences from Table 3 for this experi-
ment. This number represents a trade-off between
size and accuracy. More sentences provide more
information but at the cost of noisier data. Duong
et al. (2013b) also used sentence alignment scores
to rank sentences. Their model stabilizes after us-
ing 200k sentences. We conclude that 200k sen-
tences is enough and capture most information
from the parallel data.
Features Descriptions
W@-1 Previous word
W@+1 Next word
W@0 Current word
CAP First character is capitalized
NUMBER Is number
PUNCT Is punctuation
SUFFIX@k Suffix up to length 3 (k <= 3)
WC Word class
Table 4: Feature template for a maximum entropy
tagger
We ignore tokens that don?t have labels, which
arise from null alignments and constitute approxi-
mately 14% of the data. The remaining data (?1.4
million tokens) are used to train a maximum en-
tropy (MaxEnt) model. MaxEnt is one of the
simplest forms of probabilistic classifier, and is
appropriate in this setting due to the incomplete
889
Data Size (k) Coverage (%) Without Mapping With Mapping
60 91.5 79.9 84.2
100 89.1 79.4 83.6
200 86.1 79.1 82.9
500 82.4 78.0 81.5
Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projected
labels, averaged over 8 languages for different data sizes
Model da nl de el it pt es sv Avg
All features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9
- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6
- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6
- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8
Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature
set at a time
sequence data. While sequence models such as
HMMs or CRFs can provide more accurate mod-
els of label sequences, they impose a more strin-
gent training requirement.
3
We also experimented
with a first-order linear chain CRF trained on con-
tiguous sub-sequences but observed ? 4% (abso-
lute) drop in performance.
The maximum entropy classifier estimates the
probability of tag t given a word w as
P (t|w) =
1
Z(w)
exp
D
?
j=1
?
j
f
j
(w, t) ,
where Z(w) =
?
t
exp
?
D
j=1
?
j
f
j
(w, t) is the
normalization factor to ensure the probabilities
P (t|w) sum to one. Here f
j
is a feature function
and ?
j
is the weight for this feature, learned as
part of training. We use Maximum A Posteriori
(MAP) estimation to maximize the log likelihood
of the training data, D = {w
i
, t
i
}
N
i=1
, subject to a
zero-mean Gaussian regularisation term,
L = logP (?)
N
?
i=1
P (t
(i)
|w
(i)
)
= ?
D
?
j=1
?
2
j
2?
2
+
N
?
i=1
D
?
j=1
?
j
f
j
(w
i
, t
i
)? logZ(w
i
)
where the regularisation term limits over-fitting,
an important concern when using large feature
3
T?ackstr?om et al. (2013) train a CRF on incomplete data,
using a tag dictionary heuristic to define a ?gold standard?
lattice over label sequences.
sets. For our experiments we set ?
2
= 1. We use
L-BFGS which performs gradient ascent to maxi-
mize L. Table 4 shows the features we considered
for building the DPM. We use mkcls, an unsu-
pervised method for word class induction which is
widely used in machine translation (Och, 1999).
We run mkcls to obtain 100 word classes, using
only the target language side of the parallel data.
Table 5 shows the accuracy of the DPM evalu-
ated on 8 languages (?All features model?). DPM
performs poorly on Danish, probably because of
the tagset mapping issue discussed above. The
DPM result of 80.2% accuracy is encouraging,
particularly because the model had no explicit su-
pervision.
To see what features are meaningful for our
model, we remove features in turn and report
the result. The result in Table 5 disagrees with
T?ackstr?om et al. (2013) on the word class features.
They reported a gain of approximately 3% (ab-
solute) using the word class. However, it seems
to us that these features are not especially mean-
ingful (at least in the present setting). Possible
reasons for the discrepancy are that they train the
word class model on a massive quantity of exter-
nal monolingual data, or their algorithms for word
clustering are better (Uszkoreit and Brants, 2008).
We can see that the most informative features are
Capitalization, Number and Punctuation. This
makes sense because in languages such as Ger-
man, capitalization is a strong indicator of NOUN.
Number and punctuation features ensure that we
classify NUM and PUNCT tags correctly.
890
4 Correction Model
In this section we incorporate the directly pro-
jected model into a second correction model
trained on a small supervised sample of 1,000 an-
notated tokens. Our DPM model is not very accu-
rate; as we have discussed it makes many errors,
due to invalid or inconsistent tag mappings, noisy
alignments, and cross-linguistic syntactic diver-
gence. However, our aim is to see how effectively
we can exploit the strengths of the DPM model
while correcting for its inadequacies using direct
supervision. We select only 1,000 annotated to-
kens to reflect a low resource scenario. A small
supervised training sample is a more realistic form
of supervision than a tag dictionary (noisy or oth-
erwise). Although used in most prior work, a tag
dictionary for a new language requires significant
manual effort to construct. Garrette and Baldridge
(2013) showed that a 1,000 token dataset could be
collected very cheaply, requiring less than 2 hours
of non-expert time.
Our correction model makes use of a mini-
mum divergence (MD) model (Berger et al., 1996),
a variant of the maximum entropy model which
biases the target distribution to be similar to a
static reference distribution. The method has been
used in several language applications including
machine translation (Foster, 2000) and parsing
(Plank and van Noord, 2008, Johnson and Riezler,
2000). These previous approaches have used var-
ious sources of reference distribution, e.g., incor-
porating information from a simpler model (John-
son and Riezler, 2000) or combining in- and out-
of-domain models (Plank and van Noord, 2008).
Plank and van Noord (2008) concluded that this
method for adding prior knowledge only works
with high quality reference distributions, other-
wise performance suffers.
In contrast to these previous approaches, we
consider the specific setting where both the
learned model and the reference model s
o
=
P (t|w) are both maximum entropy models. In this
case we show that the MD setup can be simplified
to a regularization term, namely a Gaussian prior
with a non-zero mean. We model the classification
probability, P
?
(t|w) as the product between a base
model and a maximum entropy classifier,
P
?
(t|w) ? P (t|w) exp
D
?
j=1
?
j
f
j
(w, t)
where here we use the DPM model as base model
P (t|w). Under this setup, where P
?
uses the same
features as P , and both are log-linear models, this
simplifies to
P
?
(t|w) ? exp
?
?
D
?
j=1
?
j
f
j
(w, t) +
D
?
j=1
?
j
f
j
(w, t)
?
?
? exp
D
?
j=1
(?
j
+ ?
j
) f
j
(w, t) (1)
where the constant of proportionality is Z
?
(w) =
?
t
exp
?
D
j=1
(?
j
+ ?
j
) f
j
(w, t). It is clear that
Equation (1) also defines a maximum entropy clas-
sifier, with parameters ?
j
= ?
j
+ ?
j
, and conse-
quently this might seem to be a pointless exercise.
The utility of this approach arises from the prior:
MAP training with a zero mean Gaussian prior
over ? is equivalent to a Gaussian prior over the
aggregate weights, ?
j
? N (?
j
, ?
2
). This prior
enforces parameter sharing between the two mod-
els by penalising parameter divergence from the
underlying DPM model ?. The resulting training
objective is
L
corr
= logP (t|w, ?)?
1
2?
2
D
?
j=1
(?
j
? ?
j
)
2
which can be easily optimised using standard
gradient-based methods, e.g., L-BFGS. The con-
tribution of the regulariser is scaled by the constant
1
2?
2
.
4.1 Regulariser sensitivity
Careful tuning of the regularisation term ?
2
is crit-
ical for the correction model, both to limit over-
fitting on the very small training sample of 1,000
tokens, and to control the extent of the influence
of the DPM model over the correction model.
A larger value of ?
2
lessens the reliance on the
DPM and allows for more flexible modelling of
the training set, while a small value of ?
2
forces
the parameters to be close to the DPM estimates at
the expense of data fit. We expect the best value
to be somewhere between these extremes, and use
line-search to find the optimal value for ?
2
. For
this purpose, we hold out 100 tokens from the
1,000 instance training set, for use as our devel-
opment set for hyper-parameter selection.
From Figure 1, we can see that the model per-
forms poorly on small values of ?
2
. This is under-
standable because the small ?
2
makes the model
891
ll
l
l
l l l l l
l l
0.0
1 0.1 1 10 70 100 100
0
100
00
1e+
05
1e+
06
1e+
07
Variance
80
84
88
Acc
ura
cy (%
) 
l Average Acc
Figure 1: Sensitivity of regularisation parameter
?
2
against the average accuracy measured on 8
languages on the development set
too similar to DPM, which is not very accurate
(80.2%). At the other extreme, if ?
2
is large, the
DPM model is ignored, and the correction model
is equivalent with the supervised model (? 88%
accuracy). We select the value of ?
2
= 70, which
maximizes the accuracy on the development set.
4.2 The model
Using the value of ?
2
= 70, we retrain the model
on the whole 1,000-token training set and evalu-
ate the model on the rest of the annotated data.
Table 6 shows the performance of DPM, Super-
vised model, Correction model and the state-of-
the-art model (T?ackstr?om et al., 2013). The super-
vised model trains a maximum entropy tagger us-
ing the same features as in Table 4 on this 1000 to-
kens. The only difference between the supervised
model and the correction model is that in the cor-
rection model we additionally incorporate DPM as
the prior.
The supervised model performs surprisingly
well confirming that our features are meaning-
ful in distinguishing between tags. This model
achieves high accuracy on Danish compared with
other languages probably because Danish is eas-
ier to learn since it contains only 8 tags. Despite
the fact that the DPM is not very accurate, the cor-
rection model consistently outperforms the super-
vised model on all considered languages, approx-
imately 4.3% (absolute) better on average. This
shows that our method of incorporating DPM to
the model is efficient and robust.
The correction model performs much bet-
ter than the state-of-the-art for 7 languages but
l
l l
l l l
l l
l l l
100 300 500 700 100
0
150
0
200
0
500
0
100
00
150
00
500
00
Data Size
65
75
85
95
Acc
urac
y (%
) 
l Correction ModelSupervised Model
Figure 2: Learning curve for correction model and
supervised model: the x-axis is the size of data
(number of tokens); the y-axis is the average ac-
curacy measured on 8 languages; the dashed line
shows the data condition reported in Table 6
slightly worse for 1 language. On average we
achieve 91.3% accuracy compared with 88.8%
for the state-of-the-art, an error rate reduction of
22.3%. This is despite using fewer resources and
only modest supervision.
5 Analysis
Tagset mismatch In the correction model, we
implicitly resolve the mismatched tagset issue.
DPM might contain tags that don?t appear in the
target language or generally are errors in the map-
ping. However, when incorporating DPM into the
correction model, only the feature weight of tags
that appear in the target language are retained. In
general, because we don?t explicitly do any map-
ping between languages, we might have trouble if
the tagset size of the target language is bigger than
the source language tagset. However, this is not
the case for our experiment because we choose En-
glish as the source-side and English has the full 12
tags.
Learning curve We investigate the impact of
the number of available annotated tokens on the
correction model. Figure 2 shows the learning
curve of the correction model and the supervised
model. We can clearly see the differences be-
tween 2 models when the size of training data is
small. For example, at 100 tokens, the difference
is very large, approximately 18% (absolute), it is
also 6% (absolute) better than DPM. This differ-
ence diminishes as we add more data. This make
sense because when we add more data, the super-
vised model become stronger, while the effective-
892
Model da nl de el it pt es sv Avg
DPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0
Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3
DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8
Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6
Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the
state-of-the-art system (T?ackstr?om et al., 2013). The best performance for each language is shown in
bold. The models that are built with a dictionary are provided for reference.
ness of the DPM prior on the correction model is
wearing off. An interesting observation is that the
correction model is always better, even when we
add massive amounts of annotated data. At 50,000
tokens, when the supervised model reaches 96%
accuracy, the correction model is still 0.3% (abso-
lute) better, reaching 96.3%. It means that even
at that high level of confidence, some informa-
tion can still be added from DPM to the correc-
tion model. This improvement probably comes
from the observation that the ambiguity in one
language is explained through the alignment. It
also suggests that this method could improve the
performance of a supervised POS tagger even for
resource-rich languages.
Our methods are also relevant for annotation
projects for resource-poor languages. Assuming
that it is very costly to annotate even 100 tokens,
applying our methods can save annotation effort
but maintain high performance. For example, we
just need 100 tokens to match the accuracy of a su-
pervised method trained on 700 tokens, or we just
need 500 tokens to match the performance with
nearly 2,000 tokens of supervised learning.
Our method is simple, but particularly suitable
for resource-poor languages. We need a small
amount of annotated data for a high performance
POS tagger. For example, we need only around
300 annotated tokens to reach the same accuracy
as the state-of-the-art unsupervised POS tagger
(88.8%).
Tag dictionary Although, it is not our objec-
tive to rely on the dictionary, we are interested
in whether the gains from the correction model
still persist when the DPM performance is im-
proved. We attempt to improve DPM, following
the method of Li et al. (2012) by building a tag dic-
tionary using Wiktionary. This dictionary is then
used as a feature which fires for word-tag pairings
present in the dictionary. We expect that when we
add this additional supervision, the DPM model
should perform better. Table 6 shows the perfor-
mance of DPM and the correction model when in-
corporating the dictionary. The DPM model only
increases 0.6% absolute but the correction model
increases 1.3%. Additionally, it shows that our
model can improve further by incorporating exter-
nal information where available.
CRF Our approach of using simple classifiers
begs the question of whether better results could
be obtained using sequence models, such as con-
ditional random fields (CRFs). As mentioned pre-
viously, a CRF is not well suited for incomplete
data. However, as our second ?correction? model
is trained on complete sequences, we now con-
sider using a CRF in this stage. The training al-
gorithm is as follows: first we estimate the DPM
feature weights on the incomplete data as before,
and next we incorporate the feature weights into a
CRF trained on the 1,000 annotated tokens. This is
complicated by the different feature sets between
the MaxEnt classifier and the CRF, however the
classifier uses a strict subset of the CRF features.
Thus, we use the minimum divergence prior for
the token level features, and a standard zero-mean
prior for the sequence features. That is, the ob-
jective function of the CRF correction model be-
comes:
L
corr
crf
= logP (t|w, ?)
?
1
2?
2
1
?
j?F
1
(?
j
? ?
j
)
2
?
1
2?
2
2
?
j?F
2
?
2
j
(2)
where F
1
is the set of features referring to only
one label as in the DPM maxent model and F
2
is the set of features over label pairs. The union
of F = F
1
? F
2
is the set of all features for
the CRF. We perform grid search using held out
893
data as before for ?
2
1
and ?
2
2
. The CRF correc-
tion model scores 88.1% compared with 86.5% of
the supervised CRF model trained on the 1,000
tokens. Clearly, this is beneficial, however, the
CRF correction model still performs worse than
the MaxEnt correction model (91.3%). We are not
sure why but one reason might be overfitting of
the CRF, due to its large feature set and tiny train-
ing sample. Moreover, this CRF approach is or-
thogonal to T?ackstr?om et al. (2013): we could use
their CRF model as the DPM model and train the
CRF correction model using the same minimum
divergence method, presumably resulting in even
higher performance.
6 Two-output model
Garrette and Baldridge (2013) also use only a
small amount of annotated data, evaluating on
two resource-poor languages Kinyarwanda (KIN)
and Malagasy (MLG). As a simple baseline, we
trained a maxent supervised classifier on this data,
achieving competitive results of 76.4% and 80.0%
accuracy compared with their published results
of 81.9% and 81.2% for KIN and MLG, respec-
tively. Note that the Garrette and Baldridge (2013)
method is much more complicated than this base-
line, and additionally uses an external dictionary.
We want to further improve the accuracy of
MLG using parallel data. Applying the technique
from Section 4 will not work directly, due to the
tagset mismatch (the Malagasy tagset contains 24
tags) which results in highly different feature sets.
Moreover, we don?t have the language expertise
to manually map the tagset. Thus, in this section,
we propose a method capable of handling tagset
mismatch. For data, we use a parallel English-
Malagasy corpus of ?100k sentences,
4
and the
POS annotated dataset developed by Garrette and
Baldridge (2013), which comprises 4230 tokens
for training and 5300 tokens for testing.
6.1 The model
Traditionally, MaxEnt classifiers are trained us-
ing a single label.
5
The method we propose is
trained with pairs of output labels: one for the
4
http://www.ark.cs.cmu.edu/global-voices/
5
Or else a sequence of labels, in the case of a conditional
random field (Lafferty et al., 2001). However, even in this
case, each token is usually assigned a single label. An excep-
tion is the factorial CRF (Sutton et al., 2007), which models
several co-dependent sequences. Our approach is equivalent
to a factorial CRF without edges between tags for adjacent
tokens in the input.
Malagasy tag (t
M
) and one for the universal tag
(t
U
), which are both predicted conditioned on a
Malagasy word (w
M
) in context. Our two-output
model is defined as
P (t
M
, t
U
|w
M
) =
1
Z(w
M
)
exp
(
D
?
j=1
?
j
f
M
j
(w, t
M
)
+
E
?
j=1
?
j
f
U
j
(w, t
U
) +
F
?
j=1
?
j
f
B
j
(w, t
M
, t
U
)
)
(3)
where f
M
, f
U
, f
B
are the feature functions con-
sidering t
M
only, t
U
only, and over both outputs
t
M
and t
U
respectively, and Z(w
M
) is the parti-
tion function. We can think of Eq. (3) as the com-
bination of 3 models: the Malagasy maxent super-
vised model, the DPM model, and the tagset map-
ping model. The central idea behind this model is
to learn to predict not just the MLG tags, as in a
standard supervised model, but also to learn the
mapping between MLG and the noisy projected
universal tags. Framing this as a two output model
allows for information to flow both ways, such that
confident taggings in either space can inform the
other, and accordingly the mapping weights ? are
optimised to maximally exploit this effect.
One important question is how to obtain la-
belled data for training the two-output model, as
our small supervised sample of MLG text is only
annotated for MLG labels t
M
. We resolve this
by first learning the DPM model on the projected
labels, after which we automatically label our
correction training set with predicted tags from
the DPM model. That is, we augment the an-
notated training data from (t
M
, w
M
) to become
(t
M
, t
U
, w
M
). This is then used to train the two-
output maxent classifier, optimising a MAP ob-
jective using standard gradient descent. Note that
it would be possible to apply the same minimum
divergence technique for the two-output maxent
model. In this case the correction model would
include a regularization term over the ? to bias to-
wards the DPM parameters, while ? and ? would
use a zero-mean regularizer. However, we leave
this for future work.
Table 7 summarises the performance of the
state-of-the-art (Garrette et al., 2013), the super-
vised model and the two-output maxent model
evaluated on the Malagasy test set. The two-output
maxent model performs much better than the su-
pervised model, achieving ?5.3% (absolute) im-
894
Model Accuracy (%)
Garrette et al. (2013) 81.2
MaxEnt Supervised 80.0
2-output MaxEnt (Universal tagset) 85.3
2-output MaxEnt (Penn tagset) 85.6
Table 7: The performance of different models for
Malagasy.
provement. An interesting property of this ap-
proach is that we can use different tagsets for the
DPM. We also tried the original Penn treebank
tagset which is much larger than the universal
tagset (48 vs. 12 tags). We observed a small im-
provement reaching 85.6%, suggesting that some
pertinent information is lost in the universal tagset.
All in all, this is a substantial improvement over
the state-of-the-art result of 81.2% (Garrette et al.,
2013) and an error reduction of 23.4%.
7 Conclusion
In this paper, we thoroughly review the work on
multilingual POS tagging of the past decade. We
propose a simple method for building a POS tag-
ger for resource-poor languages by taking advan-
tage of parallel data and a small amount of anno-
tated data. Our method also efficiently resolves
the tagset mismatch issue identified for some lan-
guage pairs. We carefully choose and tune the
model. Comparing with the state-of-the-art, we
are using the more realistic assumption that a
small amount of labelled data can be made avail-
able rather than requiring a crowd-sourced dic-
tionary. We use less parallel data which as we
pointed out in section 3.1, could have been a huge
disadvantage for us. Moreover, we did not exploit
any external monolingual data. Importantly, our
method is simpler but performs better than previ-
ously proposed methods. With only 1,000 anno-
tated tokens, less than 1% of the test data, we can
achieve an average accuracy of 91.3% compared
with 88.8% of the state-of-the-art (error reduction
rate ?22%). Across the 8 languages we are sub-
stantially better at 7 and slightly worse at one. Our
method is reliable and could even be used to im-
prove the performance of a supervised POS tagger.
Currently, we are building the tagger and eval-
uating through several layers of mapping. Each
layer might introduce some noise which accumu-
lates and leads to a biased model. Moreover,
the tagset mappings are not available for many
resource-poor languages. We therefore also pro-
posed a method to automatically match between
tagsets based on a two-output maximum entropy
model. On the resource-poor language Mala-
gasy, we achieved the accuracy of 85.6% com-
pared with the state-of-the-art of 81.2% (Garrette
et al., 2013). Unlike their method, we didn?t use an
external dictionary but instead use a small amount
of parallel data.
In future work, we would like to improve the
performance of DPM by collecting more parallel
data. Duong et al. (2013a) pointed out that using
a different source language can greatly alter the
performance of the target language POS tagger.
We would like to experiment with different source
languages other than English. We assume that we
have 1,000 tokens for each language. Thus, for the
8 languages we considered we will have 8,000 an-
notated tokens. Currently, we treat each language
independently, however, it might also be interest-
ing to find some way to incorporate information
from multiple languages simultaneously to build
the tagger for a single target language.
Acknowledgments
We would like to thank Dan Garreette, Jason
Baldridge and Noah Smith for Malagasy and Kin-
yarwanda datasets. This work was supported by
the University of Melbourne and National ICT
Australia (NICTA). NICTA is funded by the Aus-
tralian Federal and Victoria State Governments,
and the Australian Research Council through the
ICT Centre of Excellence program. Dr Cohn is the
recipient of an Australian Research Council Fu-
ture Fellowship (project number FT130101105).
895
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceeding of
HLT-NAACL, pages 582?590.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
(ANLP ?00), pages 224?231, Seattle, Washington,
USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 575?584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013a. Increasing the quality and quan-
tity of source language data for Unsupervised Cross-
Lingual POS tagging. Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1243?1249. Asian Federation of
Natural Language Processing.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013b. Simpler unsupervised POS tagging
with bilingual projections. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
634?639. Association for Computational Linguis-
tics.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
pages 138?147, June.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. pages 583?592,
August.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start. In In Proc. ACL, pages
746?754.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?04),
pages 222?229, Barcelona, Spain, July.
Mark Johnson and Stefan Riezler. 2000. Exploit-
ing auxiliary distributions in stochastic unification-
based grammars. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
154?161.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79?86, Phuket, Thailand. AAMT.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach
to domain adaptation of a syntactic disambigua-
tion model. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 9?16.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
896
Computational Linguistics, pages 742?751, Gothen-
burg, Sweden, April.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS taggers (and other tools) for Indian lan-
guages: An experiment with Kannada using Telugu
resources. In Proceedings of IJCNLP workshop on
Cross Lingual Information Access: Computational
Linguistics and the Information Need of Multilin-
gual Societies. (CLIA 2011 at IJNCLP 2011), Chi-
ang Mai, Thailand, November.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn.
Res., 8:693?723, May.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller, and
Y. Singer a nd S.T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. Curran Associates, Inc.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1 (NAACL ?03), pages 173?180, Edmonton,
Canada.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In In
ACL International Conference Proceedings.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies, NAACL ?01, pages 1?8.
897
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 634?639,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Simpler unsupervised POS tagging with bilingual projections
Long Duong, 12 Paul Cook, 1 Steven Bird, 1 and Pavel Pecina2
1 Department of Computing and Information Systems, The University of Melbourne
2 Charles University in Prague, Czech Republic
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au,
sbird@unimelb.edu.au, pecina@ufal.mff.cuni.cz
Abstract
We present an unsupervised approach to
part-of-speech tagging based on projec-
tions of tags in a word-aligned bilingual
parallel corpus. In contrast to the exist-
ing state-of-the-art approach of Das and
Petrov, we have developed a substantially
simpler method by automatically identi-
fying ?good? training sentences from the
parallel corpus and applying self-training.
In experimental results on eight languages,
our method achieves state-of-the-art re-
sults.
1 Unsupervised part-of-speech tagging
Currently, part-of-speech (POS) taggers are avail-
able for many highly spoken and well-resourced
languages such as English, French, German, Ital-
ian, and Arabic. For example, Petrov et al (2012)
build supervised POS taggers for 22 languages us-
ing the TNT tagger (Brants, 2000), with an aver-
age accuracy of 95.2%. However, many widely-
spoken languages ? including Bengali, Javanese,
and Lahnda ? have little data manually labelled
for POS, limiting supervised approaches to POS
tagging for these languages.
However, with the growing quantity of text
available online, and in particular, multilingual
parallel texts from sources such as multilin-
gual websites, government documents and large
archives of human translations of books, news, and
so forth, unannotated parallel data is becoming
more widely available. This parallel data can be
exploited to bridge languages, and in particular,
transfer information from a highly-resourced lan-
guage to a lesser-resourced language, to build un-
supervised POS taggers.
In this paper, we propose an unsupervised ap-
proach to POS tagging in a similar vein to the
work of Das and Petrov (2011). In this approach,
a parallel corpus for a more-resourced language
having a POS tagger, and a lesser-resourced lan-
guage, is word-aligned. These alignments are ex-
ploited to infer an unsupervised tagger for the tar-
get language (i.e., a tagger not requiring manually-
labelled data in the target language). Our ap-
proach is substantially simpler than that of Das
and Petrov, the current state-of-the art, yet per-
forms comparably well.
2 Related work
There is a wealth of prior research on building un-
supervised POS taggers. Some approaches have
exploited similarities between typologically simi-
lar languages (e.g., Czech and Russian, or Telugu
and Kannada) to estimate the transition probabil-
ities for an HMM tagger for one language based
on a corpus for another language (e.g., Hana et al,
2004; Feldman et al, 2006; Reddy and Sharoff,
2011). Other approaches have simultaneously
tagged two languages based on alignments in a
parallel corpus (e.g., Snyder et al, 2008).
A number of studies have used tag projection
to copy tag information from a resource-rich to
a resource-poor language, based on word align-
ments in a parallel corpus. After alignment, the
resource-rich language is tagged, and tags are pro-
jected from the source language to the target lan-
guage based on the alignment (e.g., Yarowsky and
Ngai, 2001; Das and Petrov, 2011). Das and
Petrov (2011) achieved the current state-of-the-art
for unsupervised tagging by exploiting high con-
fidence alignments to copy tags from the source
language to the target language. Graph-based la-
bel propagation was used to automatically produce
more labelled training data. First, a graph was
constructed in which each vertex corresponds to
a unique trigram, and edge weights represent the
syntactic similarity between vertices. Labels were
then propagated by optimizing a convex function
to favor the same tags for closely related nodes
634
Model Coverage Accuracy
Many-to-1 alignments 88% 68%
1-to-1 alignments 68% 78%
1-to-1 alignments: Top 60k sents 91% 80%
Table 1: Token coverage and accuracy of many-
to-one and 1-to-1 alignments, as well as the top
60k sentences based on alignment score for 1-to-1
alignments, using directly-projected labels only.
while keeping a uniform tag distribution for un-
related nodes. A tag dictionary was then extracted
from the automatically labelled data, and this was
used to constrain a feature-based HMM tagger.
The method we propose here is simpler to that
of Das and Petrov in that it does not require con-
vex optimization for label propagation or a feature
based HMM, yet it achieves comparable results.
3 Tagset
Our tagger exploits the idea of projecting tag infor-
mation from a resource-rich to resource-poor lan-
guage. To facilitate this mapping, we adopt Petrov
et al?s (2012) twelve universal tags: NOUN,
VERB, ADJ, ADV, PRON (pronouns), DET (de-
terminers and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation), and X
(all other categories, e.g., foreign words, abbrevia-
tions). These twelve basic tags are common across
taggers for most languages.
Adopting a universal tagset avoids the need
to map between a variety of different, language-
specific tagsets. Furthermore, it makes it possi-
ble to apply unsupervised tagging methods to lan-
guages for which no tagset is available, such as
Telugu and Vietnamese.
4 A Simpler Unsupervised POS Tagger
Here we describe our proposed tagger. The key
idea is to maximize the amount of information
gleaned from the source language, while limit-
ing the amount of noise. We describe the seed
model and then explain how it is successively re-
fined through self-training and revision.
4.1 Seed Model
The first step is to construct a seed tagger from
directly-projected labels. Given a parallel corpus
for a source and target language, Algorithm 1 pro-
vides a method for building an unsupervised tag-
ger for the target language. In typical applications,
the source language would be a better-resourced
language having a tagger, while the target lan-
guage would be lesser-resourced, lacking a tagger
and large amounts of manually POS-labelled data.
Algorithm 1 Build seed model
1: Tag source side.
2: Word align the corpus with Giza++ and re-
move the many-to-one mappings.
3: Project tags from source to target using the re-
maining 1-to-1 alignments.
4: Select the top n sentences based on sentence
alignment score.
5: Estimate emission and transition probabilities.
6: Build seed tagger T.
We eliminate many-to-one alignments (Step 2).
Keeping these would give more POS-tagged to-
kens for the target side, but also introduce noise.
For example, suppose English and French were
the source and target language, respectively. In
this case alignments such as English laws (NNS)
to French les (DT) lois (NNS) would be expected
(Yarowsky and Ngai, 2001). However, in Step 3,
where tags are projected from the source to target
language, this would incorrectly tag French les as
NN. We build a French tagger based on English?
French data from the Europarl Corpus (Koehn,
2005). We also compare the accuracy and cov-
erage of the tags obtained through direct projec-
tion using the French Melt POS tagger (Denis and
Sagot, 2009). Table 1 confirms that the one-to-one
alignments indeed give higher accuracy but lower
coverage than the many-to-one alignments. At
this stage of the model we hypothesize that high-
confidence tags are important, and hence eliminate
the many-to-one alignments.
In Step 4, in an effort to again obtain higher
quality target language tags from direct projection,
we eliminate all but the top n sentences based on
their alignment scores, as provided by the aligner
via IBM model 3. We heuristically set this cutoff
to 60k to balance the accuracy and size of the seed
model.1 Returning to our preliminary English?
French experiments in Table 1, this process gives
improvements in both accuracy and coverage.2
1We considered values in the range 60?90k, but this
choice had little impact on the accuracy of the model.
2We also considered using all projected labels for the top
60k sentences, not just 1-to-1 alignments, but in preliminary
experiments this did not perform as well, possibly due to the
previously-observed problems with many-to-one alignments.
635
The number of parameters for the emission prob-
ability is |V | ? |T | where V is the vocabulary and
T is the tag set. The transition probability, on the
other hand, has only |T |3 parameters for the tri-
gram model we use. Because of this difference
in number of parameters, in step 5, we use dif-
ferent strategies to estimate the emission and tran-
sition probabilities. The emission probability is
estimated from all 60k selected sentences. How-
ever, for the transition probability, which has less
parameters, we again focus on ?better? sentences,
by estimating this probability from only those sen-
tences that have (1) token coverage > 90% (based
on direct projection of tags from the source lan-
guage), and (2) length > 4 tokens. These cri-
teria aim to identify longer, mostly-tagged sen-
tences, which we hypothesize are particularly use-
ful as training data. In the case of our preliminary
English?French experiments, roughly 62% of the
60k selected sentences meet these criteria and are
used to estimate the transition probability. For un-
aligned words, we simply assign a random POS
and very low probability, which does not substan-
tially affect transition probability estimates.
In Step 6 we build a tagger by feeding the es-
timated emission and transition probabilities into
the TNT tagger (Brants, 2000), an implementation
of a trigram HMM tagger.
4.2 Self training and revision
For self training and revision, we use the seed
model, along with the large number of target lan-
guage sentences available that have been partially
tagged through direct projection, in order to build
a more accurate tagger. Algorithm 2 describes
this process of self training and revision, and as-
sumes that the parallel source?target corpus has
been word aligned, with many-to-one alignments
removed, and that the sentences are sorted by
alignment score. In contrast to Algorithm 1, all
sentences are used, not just the 60k sentences with
the highest alignment scores.
We believe that sentence alignment score might
correspond to difficulty to tag. By sorting the sen-
tences by alignment score, sentences which are
more difficult to tag are tagged using a more ma-
ture model. Following Algorithm 1, we divide
sentences into blocks of 60k.
In step 3 the tagged block is revised by com-
paring the tags from the tagger with those ob-
tained through direct projection. Suppose source
Algorithm 2 Self training and revision
1: Divide target language sentences into blocks
of n sentences.
2: Tag the first block with the seed tagger.
3: Revise the tagged block.
4: Train a new tagger on the tagged block.
5: Add the previous tagger?s lexicon to the new
tagger.
6: Use the new tagger to tag the next block.
7: Goto 3 and repeat until all blocks are tagged.
language word wsi is aligned with target language
word wtj with probability p(wtj |wsi ), T si is the tag
for wsi using the tagger available for the source
language, and T tj is the tag for wtj using the tagger
learned for the target language. If p(wtj |wsi ) > S,
where S is a threshold which we heuristically set
to 0.7, we replace T tj by T si .
Self-training can suffer from over-fitting, in
which errors in the original model are repeated
and amplified in the new model (McClosky et al,
2006). To avoid this, we remove the tag of
any token that the model is uncertain of, i.e., if
p(wtj |wsi ) < S and T tj ?= T si then T tj = Null. So,
on the target side, aligned words have a tag from
direct projection or no tag, and unaligned words
have a tag assigned by our model.
Step 4 estimates the emission and transition
probabilities as in Algorithm 1. In Step 5, emis-
sion probabilities for lexical items in the previous
model, but missing from the current model, are
added to the current model. Later models therefore
take advantage of information from earlier mod-
els, and have wider coverage.
5 Experimental Results
Using parallel data from Europarl (Koehn, 2005)
we apply our method to build taggers for the same
eight target languages as Das and Petrov (2011)
? Danish, Dutch, German, Greek, Italian, Por-
tuguese, Spanish and Swedish ? with English as
the source language. Our training data (Europarl)
is a subset of the training data of Das and Petrov
(who also used the ODS United Nations dataset
which we were unable to obtain). The evaluation
metric and test data are the same as that used by
Das and Petrov. Our results are comparable to
theirs, although our system is penalized by having
less training data. We tag the source language with
the Stanford POS tagger (Toutanova et al, 2003).
636
Danish Dutch German Greek Italian Portuguese Spanish Swedish Average
Seed model 83.7 81.1 83.6 77.8 78.6 84.9 81.4 78.9 81.3
Self training + revision 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Table 2: Token-level POS tagging accuracy for our seed model, self training and revision, and the method
of Das and Petrov (2011). The best results on each language, and on average, are shown in bold.
0 5 10 15 20 25 30
Iteration
50
60
70
80
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
0 5 10 15 20 25 30
Iteration
70
75
80
85
90
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of
known tokens for Italian (left) and Dutch (right).
Table 2 shows results for our seed model, self
training and revision, and the results reported by
Das and Petrov. Self training and revision im-
prove the accuracy for every language over the
seed model, and gives an average improvement
of roughly two percentage points. The average
accuracy of self training and revision is on par
with that reported by Das and Petrov. On individ-
ual languages, self training and revision and the
method of Das and Petrov are split ? each per-
forms better on half of the cases. Interestingly, our
method achieves higher accuracies on Germanic
languages ? the family of our source language,
English?while Das and Petrov perform better on
Romance languages. This might be because our
model relies on alignments, which might be more
accurate for more-related languages, whereas Das
and Petrov additionally rely on label propagation.
Compared to Das and Petrov, our model per-
forms poorest on Italian, in terms of percentage
point difference in accuracy. Figure 1 (left panel)
shows accuracy, accuracy on known words, accu-
racy on unknown words, and proportion of known
tokens for each iteration of our model for Italian;
iteration 0 is the seed model, and iteration 31 is
the final model. Our model performs poorly on
unknown words as indicated by the low accuracy
on unknown words, and high accuracy on known
words compared to the overall accuracy. The poor
performance on unknown words is expected be-
cause we do not use any language-specific rules
to handle this case. Moreover, on average for the
final model, approximately 10% of the test data
tokens are unknown. One way to improve the per-
formance of our tagger might be to reduce the pro-
portion of unknown words by using a larger train-
ing corpus, as Das and Petrov did.
We examine the impact of self-training and re-
vision over training iterations. We find that for
all languages, accuracy rises quickly in the first
5?6 iterations, and then subsequently improves
only slightly. We exemplify this in Figure 1 (right
panel) for Dutch. (Findings are similar for other
languages.) Although accuracy does not increase
much in later iterations, they may still have some
benefit as the vocabulary size continues to grow.
6 Conclusion
We have proposed a method for unsupervised POS
tagging that performs on par with the current state-
of-the-art (Das and Petrov, 2011), but is substan-
tially less-sophisticated (specifically not requiring
convex optimization or a feature-based HMM).
The complexity of our algorithm is O(nlogn)
compared to O(n2) for that of Das and Petrov
637
(2011) where n is the size of training data.3 We
made our code are available for download.4
In future work we intend to consider using a
larger training corpus to reduce the proportion of
unknown tokens and improve accuracy. Given
the improvements of our model over that of Das
and Petrov on languages from the same family
as our source language, and the observation of
Snyder et al (2008) that a better tagger can be
learned from a more-closely related language, we
also plan to consider strategies for selecting an ap-
propriate source language for a given target lan-
guage. Using our final model with unsupervised
HMM methods might improve the final perfor-
mance too, i.e. use our final model as the ini-
tial state for HMM, then experiment with differ-
ent inference algorithms such as ExpectationMax-
imization (EM), Variational Bayers (VB) or Gibbs
sampling (GS).5 Gao and Johnson (2008) compare
EM, VB and GS for unsupervised English POS
tagging. In many cases, GS outperformed other
methods, thus we would like to try GS first for our
model.
7 Acknowledgements
This work is funded by Erasmus Mundus
European Masters Program in Language and
Communication Technologies (EM-LCT) and
by the Czech Science Foundation (grant no.
P103/12/G084). We would like to thank Prokopis
Prokopidis for providing us the Greek Treebank
and Antonia Marti for the Spanish CoNLL 06
dataset. Finally, we thank Siva Reddy and Span-
dana Gella for many discussions and suggestions.
References
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth con-
ference on Applied natural language processing
(ANLP ?00), pages 224?231. Seattle, Washing-
ton, USA.
Dipanjan Das and Slav Petrov. 2011. Unsu-
pervised part-of-speech tagging with bilingual
graph-based projections. In Proceedings of
3We re-implemented label propagation from Das and
Petrov (2011). It took over a day to complete this step on
an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but
only 15 minutes for our model.
4https://code.google.com/p/universal-tagger/
5We in fact have tried EM, but it did not help. The overall
performance dropped slightly. This might be because self-
training with revision already found the local maximal point.
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies - Volume 1 (ACL 2011), pages
600?609. Portland, Oregon, USA.
Pascal Denis and Beno??t Sagot. 2009. Coupling
an annotated corpus and a morphosyntactic lex-
icon for state-of-the-art POS tagging with less
human effort. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation, pages 721?736. Hong Kong,
China.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of
new morpho-syntactically annotated resources.
In Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?06), pages 549?554. Genoa, Italy.
Jianfeng Gao and Mark Johnson. 2008. A com-
parison of bayesian estimators for unsupervised
hidden markov model pos taggers. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08,
pages 344?352. Association for Computational
Linguistics, Stroudsburg, PA, USA.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP ?04), pages 222?229. Barcelona,
Spain.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Proceed-
ings of the Tenth Machine Translation Summit
(MT Summit X), pages 79?86. AAMT, Phuket,
Thailand.
David McClosky, Eugene Charniak, and Mark
Johnson. 2006. Effective self-training for pars-
ing. In Proceedings of the main conference on
Human Language Technology Conference of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ?06),
pages 152?159. New York, USA.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012. A universal part-of-speech tagset. In
Proceedings of the Eight International Confer-
ence on Language Resources and Evaluation
(LREC?12), pages 2089?2096. Istanbul, Turkey.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS Taggers (and other tools) for Indian
638
languages: An experiment with Kannada using
Telugu resources. In Proceedings of the IJC-
NLP 2011 workshop on Cross Lingual Infor-
mation Access: Computational Linguistics and
the Information Need of Multilingual Societies
(CLIA 2011). Chiang Mai, Thailand.
Benjamin Snyder, Tahira Naseem, Jacob Eisen-
stein, and Regina Barzilay. 2008. Unsupervised
multilingual learning for POS tagging. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP
?08), pages 1041?1050. Honolulu, Hawaii.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-
rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the
2003 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics on Human Language Technology - Vol-
ume 1 (NAACL ?03), pages 173?180. Edmon-
ton, Canada.
David Yarowsky and Grace Ngai. 2001. Induc-
ing multilingual POS taggers and NP bracketers
via robust projection across aligned corpora. In
Proceedings of the Second Meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technolo-
gies (NAACL ?01), pages 1?8. Pittsburgh, Penn-
sylvania, USA.
639
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 129?138,
Dublin, Ireland, August 23 2014.
Exploring Methods and Resources for Discriminating Similar Languages
Marco Lui
??
, Ned Letcher
?
, Oliver Adams
?
,
Long Duong
??
, Paul Cook
?
and Timothy Baldwin
??
?
Department of Computing and Information Systems
The University of Melbourne
?
NICTA Victoria
mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au,
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici-
pants to build an automatic language identification system to discriminate between 13 languages
in 6 groups of highly-similar languages (or national varieties of the same language). In this
paper, we describe the submissions made by team UniMelb-NLP, which took part in both the
closed and open categories. We present the text representations and modeling techniques used,
including cross-lingual POS tagging as well as fine-grained tags extracted from a deep grammar
of English, and discuss additional data we collected for the open submissions, utilizing custom-
built web corpora based on top-level domains as well as existing corpora.
1 Introduction
Language identification (LangID) is the problem of determining what natural language a document is
written in. Studies in the area often report high accuracy (Cavnar and Trenkle, 1994; Dunning, 1994;
Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accu-
racy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further
work is accurate discrimination between closely-related languages (Ljube?si?c et al., 2007; Tiedemann
and Ljube?si?c, 2012). The problem has been explored for specific groups of confusable languages, such
as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and
Ljube?si?c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre,
2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task
(Zampieri et al., 2014) was hosted at the VarDial workshop at COLING 2014, and brings together the
work on these various language groups by proposing a task on a single dataset containing text from 13
languages in 6 groups, drawn from a variety of news text datasets (Tan et al., 2014).
In this paper, we describe the entries made by team UniMelb NLP to the DSL shared task. We took
part in both the closed and the open categories, submitting to the main component (Groups A-E) as well
as the separate English component (Group F). For our closed submissions, we focused on comparing a
conventional LangID methodology based on individual words and language-indicative letter sequences
(Section 2.1) to a methodology that uses a de-lexicalized representation of language (Section 2.3). For
Groups A-E we use cross-lingual POS-tagger adaptation (Section 2.3.1) to convert the raw text to a
POS stream using a per-group tagger, and use n-grams of POS tags as our de-lexicalized representation.
For English, we also use a de-lexicalized representation based on lexical types extracted from a deep
grammar (Section 2.3.2), which can be thought of as a very fine-grained tagset. For the open submissions,
we constructed new web-based corpora using a standard methodology, targeting per-language top-level
domains (Section 2.4.2). We also compiled additional training data from existing corpora (Section 2.4.1).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
129
2 Overview
Our main focus was to explore novel methods and sources of training data for discriminating similar
languages. In this section, we describe techniques and text representations that we tested, as well as the
external data sources that we used to build language identifiers for this task.
2.1 Language-Indicative Byte Sequences
Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is
robust to variation in languages across different sources of text. The LD feature set can be thought of as
language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly
characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin
(2012) present langid.py,
1
an off-the-shelf LangID system that utilizes the LD feature set. In this
work, we re-train langid.py using the training data provided by the shared task organizers, and use
this as a baseline result representative of the state-of-the-art in LangID.
2.2 Hierarchical LangID
In LangID research to date, systems generally do not take into account any form of structure in the
class space. In this shared task, languages are explicitly grouped into 6 disjoint groups. We make use
of this structure by introducing a two-level LangID model. The first level implements a single group-
level classifier, which takes an input sentence and identifies the language group (A?F) that the sentence
is from. The output of this group-level classifier is used to select a corresponding per-group classifier,
that is trained only on data for languages in the group. This per-group classifier is applied to the input
sentence and the output thereof is the final label for the sentence.
2.3 De-Lexicalized Text Representation for DSL
One of the challenges in a machine learning approach to discriminating similar languages is to learn
differences between languages that are truly representative of the distinction between varieties, rather
than differences that are merely representative of peculiarities of the training data (Kilgarriff, 2001). One
possible confounding factor is the topicality of the training data ? if the data for each variety is drawn
from different datasets, it is possible that a classifier will simply learn the topical differences between
datasets. Diwersy et al. (2014) carried out a study of colligations in French varieties, where the variation
in the grammatical function of noun lemmas was studied across French-language newspapers from six
countries. In their initial analysis the found that the characteristic features of each country included the
name of the country and other country-specific proper nouns, which resulted in near 100% classification
accuracy but do not provide any insight into national varieties from a linguistic perspective.
One strategy that has been proposed to mitigate the effect of such topical differences is the use of
a de-lexicalized text representation (Lui and Cook, 2013). The de-lexicalization is achieved through
the use of a Part-Of-Speech tagger, which labels each word in a sentence according to its word class
(such as Noun, Verb, Adjective etc). De-lexicalized text representations through POS tagging were first
considered for native language identification (NLI), where they were used as a proxy for syntax in order
to capture certain types of grammatical errors (Wong and Dras, 2009). Syntactic structure is known to
vary across national dialects (Trudgill and Hannah, 2008), so Lui and Cook (2013) investigated POS plus
function word n-grams as a proxy for syntactic structure, and used this representation to build classifiers
to discriminate between Canadian, British and American English. They found that classifiers using such a
representation achieved above-baseline results, indicating some systematic differences between varieties
could be captured through the use of such a de-lexicalized representation. In this work, we explore this
idea further ? in particular, we examine (1) the applicability of de-lexicalized text representations to
other languages using automatically-induced crosslingual POS taggers, and (2) the difference in accuracy
for discriminating English varieties between representations based on a coarse-grained universal tagset
(Section 2.3.1) as compared to a very fine-grained tagset used in deep parsing (Section 2.3.2).
1
http://github.com/saffsd/langid.py
130
Sandy quit on Tuesday Sandy quit Tuesday
UT NOUN VERB ADP NOUN NOUN VERB NOUN
LTT n - pn v np
*
p np i-tmp n - c-dow n - pn v np
*
n - c-dow
British English American English
Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical
type tagset (LTT).
2.3.1 Crosslingual POS Tagging
A key issue in generating de-lexicalized text representations based on POS tags is the lack of availability
of POS taggers for many languages. While some languages have some tools available for POS tagging
(e.g. Treaties (Schmid, 1994) has parameter files for Spanish and Portuguese), the availability of POS
taggers is far from universal. To address this problem for the purposes of discriminating similar lan-
guages, we draw on previous work in unsupervised cross-lingual POS tagging (Duong et al., 2013) to
build a POS tagger for each group of languages, a method which we will refer to hereafter as ?UMPOS?.
UMPOS employs a 12-tag Universal Tagset introduced by Petrov et al. (2012), which consists of the
tags NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner or article), ADP
(preposition or postposition), NUM (numeral), CONJ (conjunction), PRT (particle), PUNCT (punctua-
tion), and X (all other categories, e.g., foreign words or abbreviations). These twelve basic tags constitute
a ?universal? tagset in that they can be used to describe the morphosyntax of any language at a coarse
level.
UMPOS generates POS taggers for new languages in an unsupervised fashion, by making use of
parallel data and an existing POS tagger. The input for UMPOS is: (1) parallel data between the source
and target languages; and (2) a supervised POS tagger for the source language. The output will be the
tagger for the target language. The parallel data acts as a bridge to transfer POS annotation information
from the source language to the target language.
The steps used in UMPOS are as follow. First, we collect parallel data which has English as the source
language, drawing from Europarl (Koehn, 2005) and EUbookshop (Skadin??s et al., 2014). UMPOS word-
aligns the parallel data using the Giza++ alignment tool (Och and Ney, 2003). The English side is POS-
tagged using the Stanford POS tagger (Toutanova et al., 2003), and the POS tags are then projected from
English to the target language based solely on one-to-one mappings. Using the sentence alignment score,
UMPOS ranks the ?goodness? of projected sentences and builds a seed model for the target language on
a subset of the parallel data. To further improve accuracy, UMPOS builds the final model by applying
self-training with revision to the rest of the data as follows: (1) the parallel corpus data is divided into
different blocks; (2) the first block is tagged using the seed model; (3) the block is revised based on
alignment confidence; (4) a new tagger is trained on the first block and then used to tag the second block.
This process continues until all blocks are tagged. In experiments on a set of 8 languages, Duong et al.
(2013) report accuracy of 83.4%, which is state-of-the-art for unsupervised POS tagging.
2.3.2 English Tagging Using ERG Lexical Types
Focusing specifically on language Group F ? British English and American English ? we leveraged
linguistic information from the analyses produced by the English Resource Grammar (ERG: Flickinger
(2002)), a broad-coverage, handcrafted grammar of English in the HPSG framework (Pollard and Sag,
1994) and developed within the DELPH-IN
2
research initiative. In particular, we extracted the lexical
types assigned to tokens by the parser for the best analysis of each input string. In accordance with
the heavily lexicalized nature of HPSG, lexical types are the primary means of distinguishing between
different morphosyntactic contexts in which a given lexical entry can occur. They can be thought of as
fine-grained POS tags, containing subcategorisation information in addition to part of speech informa-
tion, and semantic information in cases that it directly impacts on morphosyntax. The version of the
ERG we used (the ?1212? release) has almost 1000 lexical types.
Table 1 illustrates an example of the type of syntactic variation that can be captured with the finer-
2
http://www.delph-in.net
131
Group Language Code
Web Corpora Existing Corpora
TLD # words # datasets # words
A Bosnian bs .ba 817383 4 715602
A Croatian hr .hr 43307311 5 1536623
A Serbian sr .rs 1374787 4 1204684
B Indonesian id .id 23812382 3 564824
B Malaysian my .my 2596378 3 535221
C Czech cz .cz 17103140 8 2181486
C Slovakian sk .sk 17253001 8 2308083
D Brazilian Portuguese pt-BR .br 27369673 4 860065
D European Portuguese pt-PT .pt 22620401 8 2860321
E Argentine Spanish es-AR .ar 45913651 2 619500
E Peninsular Spanish es-ES .es 30965338 9 3458462
F British English en-GB .uk 20375047 1 523653
F American English en-US .us 21298230 1 527915
Table 2: Word count of training data used for open submissions.
grained lexical types, that would be missed with the coarse-grained universal tagset. In American En-
glish, both Sandy resigned on Tuesday and Sandy resigned on Tuesday are acceptable whereas British
English does not permit the omission of the preposition before dates. In the coarse-grained tagset, the
American English form results in a sequence VERB : NOUN, which is not particularly interesting as we
expect this to occur in both English varieties, whereas the fine-grained lexical types allow us to capture
the sequence v np
*
ntr : n - c-dow (verb followed by count noun [day of week]), which we expect
to see in American English but not in British English.
Since the ERG models a sharp notion of grammaticality, not all inputs receive an analysis ? whether
due to gaps in the coverage of the grammar or genuinely ungrammatical input. The ERG achieved
a coverage of 86% over the training data across both British English and American English. Sentences
which failed to parse were excluded from use as input into the classifier. However the inability to classify
any sentence which we cannot parse is unsatisfactory. We solved this problem by generating lexical type
features for sentences which failed to parse using the ERG-trained ?ubertagger of Dridan (2013), which
performs both tokenisation and supertagging of lexical types and improves parser efficiency by reducing
ambiguity in the input lattice to the parser.
2.4 External Corpora
The DSL shared task invited two categories of participation: (1) Closed, using only training data provided
by the organizers (Tan et al., 2014); and (2) Open, using any training data available to participants. To
participate in the latter category, we sourced additional training data through: (1) collection of data
relevant to this task from existing text corpora; and (2) automatic construction of web corpora. The
information about the additional training data is shown in Table 2.
2.4.1 Existing Corpora
We collected training data from a number of existing corpora, as shown in Table 3. Many of the cor-
pora that we used are part of OPUS (Tiedemann, 2012), which is a collection of sentence-aligned text
corpora commonly used for research in machine translation. The exceptions are: (1) debian, which
was constructed using translations of message strings from the Debian operating system,
3
; (2) BNC ?
the British National Corpus (Burnard, 2000); (3) OANC ? the open component of the Second Release
of the American National Corpus (Ide and Macleod, 2001), and (4) Reuters Corpus Volume 2 (RCV2);
4
a corpus of news stories by local reporters in 13 languages. We sampled approximately 19000 sentences
from each of the BNC and OANC, which we used as training data to generate ERG lextype features (Sec-
tion 2.3.2) for British English (en-GB) and American English (en-US), respectively. From RCV2 we
3
http://www.debian.org
4
http://trec.nist.gov/data/reuters/reuters.html
132
bs hr sr pt-PT pt-BR id my cz sk es-ES es-AR en-US en-GB
BNC X
debian X X X X X X X X X X X
ECB X X X X
EMEA X X X X
EUconst X X X X
Europarl X X X X
hrenWaC X
KDE4 X X X X X X X X X
KDEdoc X X X X
OANC X
OpenSubtitles X X X X X X X X X X
RCV2 X X
SETIMES2 X X X
Tatoeba X X
Table 3: Training data compiled from existing corpora.
used the Latin American Spanish news stories as a proxy for Argentine Spanish (es-AR). Note that, for
a given text source, we didn?t necessarily use data for all available languages. For example, debian
contains British English and American English translations, which we did not use.
2.4.2 Web Corpus Construction
Each existing corpus we describe in Section 2.4.1 provides incomplete coverage over the set of languages
in the shared task dataset. In order to have a resource that covers all the languages in the shared task
drawn from a single source, we constructed web corpora for each language. Our approach was strongly
inspired by the approach used to create ukWaC (Ferraresi et al., 2008), and the creation of each sub-
language?s corpus involved crawling the top level domains of the primary countries associated with
those sub-languages. Based on the findings of Cook and Hirst (2012), the assumption underlying this
approach is that text found in the top-level domains (TLDs) of those countries will primarily be of
the sub-language dominant in that country. For instance, we assume that Portuguese text found when
crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will
be primarily Brazilian Portuguese.
The process of creating a corpus for each sub-language involved translating a sample of 200 of the
original ukWaC queries into each language using Panlex (Baldwin et al., 2010).
5
These queries were then
submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining
results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended
them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a
Heritrix 3.1.1
6
instance with default settings other than constraining the crawled content to the relevant
TLD.
Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach,
only documents with a MIME type of HTML and size between 5k and 200k bytes were used. Jus-
text (Pomik?alek, 2011) was used to extract text from the selected documents. langid.py (Lui and
Baldwin, 2012) was then used to discard documents whose text was not in the relevant language or lan-
guage group. The corpus was then refined through deduplication. First, near-deduplication was done at
the paragraph level using Onion (Pomik?alek, 2011) with its default settings. Then, exact-match sentence-
level deduplication, ignoring whitespace and case, was applied.
3 Results and Discussion
Table 4 summarizes the runs submitted by team UniMelb NLP to the VarDial DSL shared task. We
submitted the maximum number of runs allowed, i.e. 3 closed runs and 3 open runs, to both the ?general?
Groups A?E subtask as well as the English-specific Group F subtask. We applied different methods to
Group F, as some of the tools (the ERG) and resources (BNC/OANC) were specific to English. For clarity
in discussion, we have labeled each of our runs according to a 3-letter code: the first letter indicates the
5
A sample of the queries was used because of time and resource limitations.
6
https://webarchive.jira.com/wiki/display/Heritrix
133
Run Description
Macro-avg F-Score
dev tst
Grp A-E closed
AC1 langid.py 13-way 0.822 0.817
AC2 langid.py per-group 0.923 0.918
AC3 POS features 0.683 0.671
Grp F closed
FC1 Lextype features 0.559 0.415
FC2 langid.py per-group 0.548 0.403
FC3 POS features 0.545 0.435
Grp A-E open
AO1 Ext Corpora (word-level model) 0.705 0.703
AO2 Web Corpora (word-level model) 0.771 0.767
AO3 5-way voting 0.881 0.878
Grp F open
FO1 Lextype features using BNC/OANC training data 0.491 0.572
FO2 Web Corpora (word-level model) 0.490 0.581
FO3 5-way voting 0.574 0.442
Table 4: Summary of the official runs submitted by UniMelbNLP. ?dev? indicates scores from our
internal testing on the development partition of the dataset.
subtask (A for Groups A?E, F for Group F), the second indicates Closed (?C?) or Open (?O?), and the
final digit indicates the run number.
AC1 represents a benchmark result based on the LangID system (Lui and Baldwin, 2012). We used
the training tools provided with langid.py to generate a new model using the training data provided
by the shared task organizers, noting that as only data from a single source is used, we are not able to
fully exploit the cross-domain feature selection (Lui and Baldwin, 2011) implemented by langid.py.
The macro-averaged F-score across groups is substantially lower than that on standard LangID datasets
(Lui and Baldwin, 2012).
AC2 and FC2 are a straightforward implementation of hierarchical LangID (Section 2.2), using
mostly-default settings of langid.py. A 6-way group-level classifier is trained, and well as 6 different
per-group classifiers. We increase the number of features selected per class (i.e. group or language) to
500 from the default of 300, to compensate for the smaller number of classes (langid.py off-the-
shelf supports 97 languages). In our internal testing on the provided development data, the group-level
classifier achieved 100% accuracy in classifying sentences at the group level, essentially reducing the
problem to within-group disambiguation. Despite being one of the simplest approaches, overall this
was our best-performing submission for Groups A?E. It also represents a substantial improvement on
AC1, further emphasizing the need to implement hierarchical LangID in order to attain high accuracy in
discriminating similar languages.
AC3 and FC3 are based solely on POS-tag sequences generated by UMPOS, and implement a hierar-
chical LangID approach similar to AC2/FC2. Each sentence in the training data is mapped to a POS-tag
sequence in the 12-tag universal tagset, using the per-group POS tagger for the language group. Each tag
was represented using a single character, allowing us to make use of langid.py to train 6 per-group
classifiers based on n-grams of POS-tags. We used n-grams of order 1?6, and selected 5000 top-ranked
sequences per-language. To classify test data, the same group-level classifier used in AC2 was used to
map sentences to language groups, and then the per-group POS tagger was applied to derive the corre-
sponding stream of POS tags for each sentence. The corresponding per-group classifier trained on POS
tag sequences was then applied to produce the final label for the sentence. For Groups A?E, we find that
134
bs hr sr id my cz sk
T 53.0 23.2 60.0 VHN 0.9 1.3 .1.1 1.0 1.2
TV 32.4 13.2 43.3 DHN 0.1 0.1 1.1. 2.0 2.2
NT 31.5 13.9 43.2 N.1. 12.1 3.1 1.1 4.0 4.4
TVN 24.8 9.8 34.3 N.N 63.3 48.0 .N.1 0.5 0.7
VT 19.4 6.1 27.1 .DNV 1.8 1.1 .C 39.0 33.5
TN 29.1 10.9 29.6 DH 1.7 2.1 .1.. 0.7 1.0
NTV 18.6 8.4 29.4 N.DN 3.2 2.0 .P 51.2 41.8
TVNN 16.8 6.9 23.7 VH 11.3 14.9 1. 14.0 13.9
NVT 11.2 2.9 15.5 PNV1 0.5 0.4 1.. 1.2 1.6
VTV 11.0 3.2 17.0 .1. 13.2 3.8 .R 44.0 30.0
pt-BR pt-PT es-AR es-ES en-GB en-US
X 3.4 2.8 .. 22.6 43.3 NNN 48.2 43.2
N.NN 22.2 15.3 N.. 16.4 31.7 HV 41.5 46.4
.NN 29.9 22.9 .P 52.2 68.3 NN 86.3 83.0
XN 0.4 0.4 P. 6.6 16.8 H 61.8 65.9
NNNN 6.2 3.2 D. 4.4 12.6 R 61.5 65.5
D 99.2 99.5 ..$ 0.0 0.0 RR 7.2 9.4
NNN 28.3 18.6 J.. 5.0 12.6 NNNN 21.7 18.5
.NNN 6.7 4.0 ..VV 0.9 5.2 .C 15.8 18.8
N.D 58.6 47.8 DN.. 4.2 11.0 ... 0.8 0.3
NX 0.8 0.5 .PD 24.5 36.3 N.C 11.3 13.6
Table 5: Top 10 POS features per-group by Information Gain, along with percentage of sentences in each
language in which the feature appears. The notation used is as follows: . = punctuation, J = adjective,
P = pronoun, R = adverb, C = conjunction, D = determiner/article, N = noun, 1 = numeral, H = pronoun,
T = particle, V = verb, and X = others
the POS-tag sequence features are not as effective as the character n-grams used in AC2. Nonetheless,
the results attained are above baseline, indicating that there are systematic differences between languages
in each group that can be captured by an unsupervised approach to POS-tagging using a coarse-grained
tagset. This extends the similar observation made by Lui and Cook (2013) on varieties of English, show-
ing that the same is true for the other language groups in this shared task. Also of interest is the higher
accuracy attained by the POS-tag features on Groups A?E (i.e. AC3) than on English (Group F, FC3).
The top-10 sequences per-group are presented in Table 5, where it can be seen that the sequences are
often slightly more common in one language in the group than the other language(s). One limitation of
the Information Gain based feature selection used in langid.py is that each feature is scored inde-
pendently, and each language receives a binarized score. This can be seen in the features selected for
Group A, where all the top-10 features selected involve particles (labelled T). Overall, this indicates that
Croatian (hr) appears to use particles much less frequently than Serbian (sr) or Bosnian (bs), which is
an intriguing finding. However, most of the top-10 features are redundant in that they all convey very
similar information.
Similar to FC3, a hierarchical LangID approach is used in FC1, in conjunction with per-group classi-
fiers based on a sequence of tags derived from the original sentence. The difference between the taggers
used for FC3 and FC1 is that the FC3 tagger utilizes the 12-tag universal tagset, whereas the FC1 tagger
uses the English-specific lexical types from the ERG (Section 2.3.2), a set of approximately 1000 tags.
There is hence a trade-off to be made between the degree of distinction between tags, and the relative
sparsity of the data ? having a larger tagset means that any given sequence of tags is proportionally less
likely to occur. On the basis of the results of FC1 and FC3 on the dev data, the lexical type features
marginally outperform the coarse-grained universal tagset. However, this result is made harder to inter-
pret by the mismatch between the dev and tst partitions of the shared task dataset. We will discuss
this issue in more detail below, in the context of examining the results on Group F for the open category.
In the open category, we focused primarily on the effect of using different sources of training data.
AO1 and AO2 both implement a hierarchical LangID approach, again using the group-level classifier
from AC2. For the per-group classifiers, runs AO1 and AO2 use a naive Bayes model on a word-level
representation, with feature selection by Information Gain. The difference between the two is that A01
uses samples from existing text corpora (Section 2.4.1), whereas A02 uses web corpora that we prepared
specifically for this shared task (Section 2.4.2). In terms of accuracy, both types of corpora perform
135
substantially better than baseline, indicating that at the word level, there are differences between the
language varieties that are consistent across the different corpus types. This result is complementary to
Cook and Hirst (2012), who found that web corpora from specific top-level domains were representative
of national varieties of English. AO2 (web corpora) outperforms AO1 (existing corpora), further high-
lighting the relevance of web corpora as a source of training data for discriminating similar languages.
However, our models trained on external data were not able to outperform the models trained on the
official training data for Groups A?E. A03 consists of a 5-way majority vote between results AC1, AC2,
AC3, AO1 and AO2. Including the predictions from the closed submissions substantially improves the
result with respect to AO1/AO2, but overall our best result for Groups A?E was obtained by run AC2.
For Group F, FO1 utilizes ERG lexical type features in the same manner as FC1, the difference being
that FC1 uses the shared task trn partition, whereas FO1 uses sentences sampled from existing corpora,
specifically BNC for en-GB and OANC for en-US. FO2 implements the same concept as AO2, namely a
word-level naive Bayes model trained using web corpora. For the Group F (i.e. English) subtask, this was
our best-performing submission overall. FO3 is a 5-way vote between FC1, FC2, FC3, FO1 and FO2,
similar to AO3. Notably, our Group F submissions based on the supplied training data all performed
substantially better on the dev partition of the shared task dataset than on the tst partition. The inverse
is true for our submissions based on external corpora, where all our entries performed substantially better
on the tst partition than on the dev partition. Furthermore, the differences are fairly large, particularly
since Group F is a binary classification task with a 50% baseline. This implies that, at least under our
models, the en-GB portion of the trn partition is a better model of the en-US portion of the tst partition
than the en-GB portion thereof. This is likely due to the manual intervention that was only carried out
on the test portion of the dataset (Zampieri et al., 2014).
Our Group F results appear to be inferior to previous work on discriminating English varieties (Lui
and Cook, 2013). However, there are a number of differences that make it difficult to compare the
results: Lui and Cook (2013) studied differences between Australian, British and Canadian English,
whereas the shared task focused on differences between British and American English. Lui and Cook
(2013) also draw on training data from a variety of domains (national corpora, web corpora and Twitter
messages), whereas the shared task used a dataset collected from newspaper texts (Tan et al., 2014).
Consistent with Cook and Hirst (2012) and Lui and Cook (2013), we found that web corpora appear to be
representative of national varieties, and consistent with Lui and Cook (2013) we found that de-lexicalized
representations of text are able to provide better than baseline discrimination between national varieties.
Overall, these results highlight the need for further research into discriminating between varieties of
English.
4 Conclusion
Discriminating between similar languages is an interesting sub-problem in language identification, and
the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater
detail. Our most successful methods implement straightforward hierarchical LangID, firstly identifying
the language group that a sentence belongs to, before identifying the specific language. We examined a
number of text representations for the per-group language identifiers, including a standard representation
for language identification based on language-indicative byte sequences, as well as with de-lexicalized
text representations. We found that the performance of de-lexicalized representations was above baseline,
however we were not able to fully investigate approaches to integrating predictions from lexicalized
and de-lexicalized text representations due to time constraints. We also found that when using external
corpora, web corpora constructed by scraping per-country top-level domains performed as well as (if
not better than) data collected from existing text corpora, supporting the hypothesis that web corpora
are representative of national varieties of respective languages. Overall, our best result was obtained by
applying two-level hierarchical LangID, firstly identifying the language group that a sentence belongs
to, and then disambiguating within each group. Our best result was achieved by applying an existing
LangID method (Lui and Baldwin, 2012) to both the group-level and the per-group classification tasks.
136
Acknowledgments
The authors wish to thank Li Wang, Rebecca Dridan and Bahar Salehi for their kind assistance with
this research. NICTA is funded by the Australian Government as represented by the Department of
Broadband, Communications and the Digital Economy and the Australian Research Council through the
ICT Centre of Excellence program.
References
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL HLT 2010), pages 229?237, Los Angeles, USA.
Timothy Baldwin, Jonathan Pool, and Susan M Colowick. 2010. Panlex and lextract: Translating all words of
all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics:
Demonstrations, pages 37?40, Beijing, China.
Marco Baroni and Silvia Bernardini. 2004. BootCaT: Bootstrapping corpora and terms from the Web. In Pro-
ceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004).
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford University
Computing Services.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the Third
Symposium on Document Analysis and Information Retrieval, pages 161?175, Las Vegas, USA.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties of
English? In Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages
281?293, Li`ege, Belgium.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A weakly supervised multivariate approach to the study
of language variation. In Benedikt Szmrecsanyi and Bernhard W?alchli, editors, Aggregating Dialectology,
Typology, and Register Analysis. Linguistic Variation in Text and Speech. De Gruyter, Berlin.
Rebecca Dridan. 2013. Ubertagging. Joint segmentation and supertagging for English. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1201?1212, Seattle, USA.
Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS 940-273, Computing Research
Laboratory, New Mexico State University.
Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013. Simpler unsupervised POS tagging with bilin-
gual projections. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 634?639.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Morocco.
Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering. CSLI Publi-
cations, Stanford, USA.
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of Analisi Statistica
dei Dati Testuali (JADT), pages 263?268, Rome, Italy.
Nancy Ide and Catherine Macleod. 2001. The American National Corpus: A standardized resource of American
English. In Proceedings of Corpus Linguistics 2001, pages 274?280, Lancaster, UK.
Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):97?133.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth
Machine Translation Summit (MT Summit X), pages 79?86, Phuket, Thailand.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identification : how to distinguish similar
languages ? In 29th International Conference on Information Technology Interfaces, pages 541?546.
Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceed-
ings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 553?561,
Chiang Mai, Thailand.
137
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages
25?30, Jeju, Republic of Korea.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013, pages 5?15, Brisbane, Australia.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.
European Language Resources Association (ELRA).
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Jan Pomik?alek. 2011. Removing Boilerplate and Duplicate Content from Web Corpora. Ph.D. thesis, Masaryk
University.
John M. Prager. 1999. Linguini: language identification for multilingual documents. In Proceedings of the 32nd
Annual Hawaii International Conference on Systems Sciences (HICSS-32), Maui, Hawaii.
Bali Ranaivo-Malancon. 2006. Automatic Identification of Close Languages - Case study : Malay and Indonesian.
ECTI Transaction on Computer and Information Technology, 2(2):126?134.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Conference
on New Methods in Natural Language Processing, Manchester, 1994.
Raivis Skadin??s, J?org Tiedemann, Roberts Rozis, and Daiga Deksne. 2014. Billions of parallel words for free:
Building and using the EU Bookshop corpus. In Proceedings of the 9th International Conference on Language
Resources and Evaluation (LREC-2014), Reykjavik, Iceland.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
W. J. Teahan. 2000. Text Classification and Segmentation Using Minimum Cross-Entropy. In Proceedings the 6th
International Conference ?Recherche dInformation Assistee par Ordinateur? (RIAO00), pages 943?961, Paris,
France.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 2619?
2634, Mumbai, India.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th International
Conference on Language Resources and Evaluation (LREC 2012), pages 2214?2218, Istanbul, Turkey.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL
?03), pages 173?180, Edmonton, Canada.
Peter Trudgill and Jean Hannah. 2008. International English: A guide to varieties of Standard English. Hodder
Education, London, UK, 5th edition.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proceed-
ings of the Australasian Language Technology Workshop 2009 (ALTW 2009), pages 53?61, Sydney, Australia.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS 2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN 2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and J?org Tiedemann. 2014. A report on the DSL shared task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial), Dublin, Ireland.
138
