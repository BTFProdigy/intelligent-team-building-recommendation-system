Using Syntactic Information to Extract Relevant Terms for Multi-Document
Summarization
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/ Juan del Rosal, 16 - 28040 Madrid - Spain
http://nlp.uned.es
Abstract
The identification of the key concepts in a set of
documents is a useful source of information for
several information access applications. We are
interested in its application to multi-document
summarization, both for the automatic genera-
tion of summaries and for interactive summa-
rization systems.
In this paper, we study whether the syntactic po-
sition of terms in the texts can be used to predict
which terms are good candidates as key con-
cepts. Our experiments show that a) distance
to the verb is highly correlated with the proba-
bility of a term being part of a key concept; b)
subject modifiers are the best syntactic locations
to find relevant terms; and c) in the task of auto-
matically finding key terms, the combination of
statistical term weights with shallow syntactic
information gives better results than statistical
measures alone.
1 Introduction
The fundamental question addressed in this article
is: can syntactic information be used to find the
key concepts of a set of documents? We will pro-
vide empirical answers to this question in a multi-
document summarization environment.
The identification of key terms out of a set of doc-
uments is a common problem in information access
applications and, in particular, in text summariza-
tion: a fragment containing one or more key con-
cepts can be a good candidate to be part of a sum-
mary.
In single-document summarization, key terms are
usually obtained from the document title or head-
ing (Edmundson, 1969; Preston, 1994; Kupiec et
al., 1995). In multi-document summarization, how-
ever, some processing is needed to identify key con-
cepts (Lin and Hovy, 2002; Kraaij et al, 2002;
Schlesinger et al, 2002). Most approaches are
based on statistical criteria.
Criteria to elaborate a manual summary depend,
by and large, on the user interpretation of both the
information need and the content of documents.
This is why this task has also been attempted from
an interactive perspective (Boguraev et al, 1998;
Buyukkokten et al, 1999; Neff and Cooper, 1999;
Jones et al, 2002; Leuski et al, 2003). A standard
feature of such interactive summarization assistants
is that they offer a list of relevant terms (automati-
cally extracted from the documents) which the user
may select to decide or refine the focus of the sum-
mary.
Our hypothesis is that the key concepts of a doc-
ument set will tend to appear in certain syntactic
functions along the sentences and clauses of the
texts. To confirm this hypothesis, we have used
a test bed with manually produced summaries to
study:
? which are the most likely syntactic functions
for the key concepts manually identified in the
document sets.
? whether this information can be used to auto-
matically extract the relevant terms from a set
of documents, as compared to standard statis-
tical term weights.
Our reference corpus is a set of 72 lists of key
concepts, manually elaborated by 9 subjects on
8 different topics, with 100 documents per topic.
It was built to study Information Synthesis tasks
(Amigo et al, 2004) and it is, to the best of
our knowledge, the multi-document summarization
testbed with a largest number of documents per
topic. This feature enables us to obtain reliable
statistics on term occurrences and prominent syn-
tactic functions.
The paper is organized as follows: in Section 2
we review the main approaches to the evaluation
of automatically extracted key concepts for summa-
rization. In Section 3 we describe the creation of the
reference corpus. In Section 4 we study the correla-
tion between key concepts and syntactic function in
texts, and in Section 5 we discuss the experimental
results of syntactic function as a predictor to extract
key concepts. Finally, in Section 6 we draw some
conclusions.
2 Evaluation of automatically extracted
key concepts
It is necessary, in the context of an interactive sum-
marization system, to measure the quality of the
terms suggested by the system, i.e., to what extent
they are related to the key topics of the document
set.
(Lin and Hovy, 1997) compared different strate-
gies to generate lists of relevant terms for summa-
rization using Topic Signatures. The evaluation was
extrinsic, comparing the quality of the summaries
generated by a system using different term lists as
input. The results, however, cannot be directly ex-
trapolated to interactive summarization systems, be-
cause the evaluation does not consider how informa-
tive terms are for a user.
From an interactive point of view, the evaluation
of term extraction approaches can be done, at least,
in two ways:
? Evaluating the summaries produced in the in-
teractive summarization process. This option
is difficult to implement (how do we evaluate
a human produced summary? What is the ref-
erence gold standard?) and, in any case, it is
too costly: every alternative approach would
require at least a few additional subjects per-
forming the summarization task.
? Comparing automatically generated term lists
with manually generated lists of key concepts.
For instance, (Jones et al, 2002) describes a
process of supervised learning of key concepts
from a training corpus of manually generated
lists of phrases associated to a single docu-
ment.
We will, therefore, use the second approach,
evaluating the quality of automatically generated
term lists by comparing them to lists of key con-
cepts which are generated by human subjects after a
multi-document summarization process.
3 Test bed: the ISCORPUS
We have created a reference test bed, the ISCOR-
PUS1 (Amigo et al, 2004) which contains 72 man-
ually generated reports summarizing the relevant in-
formation for a given topic contained in a large doc-
ument set.
For the creation of the corpus, nine subjects per-
formed a complex multi-document summarization
1Available at http://nlp.uned.es/ISCORPUS.
task for eight different topics and one hundred rele-
vant documents per topic. After creating each topic-
oriented summary, subjects were asked to make a
list of relevant concepts for the topic, in two cate-
gories: relevant entities (people, organizations, etc.)
and relevant factors (such as ?ethnic conflicts? as
the origin of a civil war) which play a key role in
the topic being summarized.
These are the relevant details of the ISCORPUS
test bed:
3.1 Document collection and topic set
We have used the Spanish CLEF 2001-2003 news
collection testbed (Peters et al, 2002), and selected
the eight topics with the largest number of docu-
ments manually judged as relevant from the CLEF
assessment pools. All the selected CLEF topics
have more than one hundred documents judged as
relevant by the CLEF assessors; for homogeneity,
we have restricted the task to the first 100 docu-
ments for each topic (using a chronological order).
This set of eight CLEF topics was found to have
two differentiated subsets: in six topics, it is neces-
sary to study how a situation evolves in time: the
importance of every event related to the topic can
only be established in relation with the others. The
invasion of Haiti by UN and USA troops is an ex-
ample of such kind of topics. We refer to them as
?Topic Tracking? (TT) topics, because they are suit-
able for such a task. The other two questions, how-
ever, resemble ?Information Extraction? (IE) tasks:
essentially, the user has to detect and describe in-
stances of a generic event (for instance, cases of
hunger strikes and campaigns against racism in Eu-
rope in this case); hence we will refer to them as IE
summaries.
3.2 Generation of manual summaries
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of summaries. All
subjects were given an in-place detailed description
of the task, in order to minimize divergent interpre-
tations. They were told they had to generate sum-
maries with a maximum of information about ev-
ery topic within a 50 sentence space limit, using a
maximum of 30 minutes per topic. The 50 sentence
limit can be temporarily exceeded and, once the 30
minutes have expired, the user can still remove sen-
tences from the summary until the sentence limit is
reached back.
3.3 Manual identification of key concepts
After summarizing every topic, the following ques-
tionnaire was filled in by users:
? Who are the main people involved in the topic?
? What are the main organizations participating in the topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e., a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
a topic about the invasion of Haiti by UN and USA
troops:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is generated
for each topic, joining all the answers given by the
nine subjects. These lists of key concepts constitute
the gold standard for all the experiments described
below.
3.4 Shallow parsing of documents
Documents are processed with a robust shallow
parser based in finite automata. The parser splits
sentences in chunks and assigns a label to every
chunk. The set of labels is:
? [N]: noun phrases, which correspond to
names or adjectives preceded by a determiner,
punctuation sign, or beginning of a sentence.
? [V]: verb forms.
? [Mod]: adverbial and prepositional phrases,
made up of noun phrases introduced by an ad-
verb or preposition. Note that this is the mech-
anism to express NP modifiers in Spanish (as
compared to English, where noun compound-
ing is equally frequent).
? [Sub]: words introducing new subordinate
clauses within a sentence (que, cuando, mien-
tras, etc.).
? [P]: Punctuation marks.
This is an example output of the chunker:
Previamente [Mod] ,[P]el presidente Bill Clinton [N] hab??a di-
cho [V] que [Sub] tenemos [V] la obligacion [N] de cambiar la
pol??tica estadounidense [Mod] que [Sub] no ha funcionado [V] en
Hait?? [Mod].[P]
Although the precision of the parser is limited,
the results are good enough for the statistical mea-
sures used in our experiments.
4 Distribution of key concepts in syntactic
structures
We have extracted empirical data to answer these
questions:
? Is the probability of finding a key concept cor-
related with the distance to the verb in a sen-
tence or clause?
? Is the probability of finding a key concept in a
noun phrase correlated with the syntactic func-
tion of the phrase (subject, object, etc.)?
? Within a noun phrase, where is it more likely
to find key concepts: in the noun phrase head,
or in the modifiers?
We have used certain properties of Spanish syn-
tax (such as being an SVO language) to decide
which noun phrases play a subject function, which
are the head and modifiers of a noun phrase, etc. For
instance, NP modifiers usually appear after the NP
head in Spanish, and the specification of a concept
is usually made from left to right.
4.1 Distribution of key concepts with verb
distance
Figure 1 shows, for every topic, the probability of
finding a word from the manual list of key con-
cepts in fixed distances from the verb of a sen-
tence. Stop words are not considered for computing
word distance. The broader line represents the aver-
age across topics, and the horizontal dashed line is
the average probability across all positions, i.e., the
probability that a word chosen at random belongs to
the list of key concepts.
The plot shows some clear tendencies in the data:
the probability gets higher when we get close to the
verb, falls abruptly after the verb, and then grows
steadily again. For TT topics, the probability of
finding relevant concepts immediately before the
verb is 56% larger than the average (0.39 before the
verb, versus 0.25 in any position). This is true not
only as an average, but also for all individual TT
topics. This can be an extremely valuable result: it
shows a direct correlation between the position of a
term in a sentence and the importance of the term
in the topic. Of course, this direct distance to the
verb should be adapted for languages with different
syntactic properties, and should be validated for dif-
ferent domains.
The behavior of TT and IE topics is substantially
different. IE topics have smaller probabilities over-
all, because there are less key concepts common to
all documents. For instance, if the topic is ?cases of
hunger strikes?, there is little in common between
Figure 1: Probability of finding key concepts at fixed distances from verb
all cases of hunger strikes found in the collection;
each case has its own relevant people and organiza-
tions, for instance. Users try to make abstraction of
individual cases to write key concepts, and then the
number of key concepts is smaller. The tendency
to have larger probabilities just before the verb and
smaller probabilities just after the verb, however,
can also be observed for IE topics.
Figure 2: Probability of finding key concepts in sub-
ject NPs versus other NPs
4.2 Key Concepts and Noun Phrase Syntactic
Function
We wanted also to confirm that it is more likely to
find a key concept in a subject noun phrase than
in general NPs. For this, we have split compound
sentences in chunks, separating subordinate clauses
([Sub] type chunks). Then we have extracted se-
quences with the pattern [N][Mod]*. We assume
that the sentence subject is a sequence [N][Mod]*
occurring immediately before the verb. For in-
stance:
El presidente [N] en funciones [Mod] de
Hait?? [Mod] ha afirmado [V] que [Sub]...
The rest of [N] and [Mod] chunks are consid-
ered as part of the sentence verb phrase. In a ma-
jority of cases, these assumptions lead to a correct
identification of the sentence subject. We do not
capture, however, subjects of subordinate sentences
or subjects appearing after the verb.
Figure 2 shows how the probability of finding a
key concept is always larger in sentence subjects.
This result supports the assumption in (Boguraev
et al, 1998), where noun phrases receive a higher
weight, as representative terms, if they are syntactic
subjects.
4.3 Distribution of key concepts within noun
phrases
Figure 3: Probability of finding key concepts in NP
head versus NP modifiers
For this analysis, we assume that, in
[N][Mod]* sequences identified as subjects,
[N] is the head and [Mod]* are the modifiers.
Figure 3 shows that the probability of finding a
key concept in the NP modifiers is always higher
than in the head (except for topic TT3, where it is
equal). This is not intuitive a priori; an examination
of the data reveals that the most characteristic con-
cepts for a topic tend to be in the complements: for
instance, in ?the president of Haiti?, ?Haiti? carries
more domain information than ?president?. This
seems to be the most common case in our news
collection. Of course, it cannot be guaranteed that
these results will hold in other domains.
5 Automatic Selection of Key Terms
We have shown that there is indeed a correlation be-
tween syntactic information and the possibility of
finding a key concept. Now, we want to explore
whether this syntactic information can effectively
be used for the automatic extraction of key concepts.
The problem of extracting key concepts for sum-
marization involves two related issues: a) What
kinds of terms should be considered as candidates?
and b) What is the optimal weighting criteria for
them?
There are several possible answers to the first
question. Previous work includes using noun
phrases (Boguraev et al, 1998; Jones et al, 2002),
words (Buyukkokten et al, 1999), n-grams (Leuski
et al, 2003; Lin and Hovy, 1997) or proper
nouns, multi-word terms and abbreviations (Neff
and Cooper, 1999).
Here we will focus, however, in finding appro-
priate weighting schemes on the set of candidate
terms. The most common approach in interactive
single-document summarization is using tf.idf mea-
sures (Jones et al, 2002; Buyukkokten et al, 1999;
Neff and Cooper, 1999), which favour terms which
are frequent in a document and infrequent across
the collection. In the iNeast system (Leuski et al,
2003), the identification of relevant terms is ori-
ented towards multi-document summarization, and
they use a likelihood ratio (Dunning, 1993) which
favours terms which are representative of the set of
documents as opposed to the full collection.
Other sources of information that have been used
as complementary measures consider, for instance,
the number of references of a concept (Boguraev
et al, 1998), its localization (Jones et al, 2002)
or the distribution of the term along the document
(Buyukkokten et al, 1999; Boguraev et al, 1998).
5.1 Experimental setup
A technical difficulty is that the key concepts in-
troduced by the users are intellectual elaborations,
which result in complex expressions which might
even not be present (literally) in the documents.
Hence, we will concentrate on extracting lists of
terms, checking whether these terms are part of
some key concept. We will assume that, once key
terms are found, it is possible to generate full nomi-
nal expressions using, for instance, phrase browsing
strategies (Pen?as et al, 2002).
We will then compare different weighting criteria
to select key terms, using two evaluation measures:
a recall measure saying how well manually selected
key concepts are covered by the automatically gen-
erated term list; and a noise measure counting the
number of terms which do not belong to any key
concept. An optimal list will reach maximum recall
with a minimum of noise. Formally:
R =
|Cl|
|C|
Noise = |Ln|
where C is the set of key concepts manually se-
lected by users; L is a (ranked) list of terms gen-
erated by some weighting schema; Ln is the subset
of terms in L which do not belong to any key con-
cept; and Cl is the subset of key concepts which are
represented by at least one term in the ranked list L.
Here is a (fictitious) example of how R and
Noise are computed:
C = {Haiti, reinstatement of democracy, UN and USA troops}
L = {Haiti, soldiers, UN, USA, attempt}
?
Cl = {Haiti, UN and USA troops} R = 2/3
Ln = {soldiers,attempt} Noise = 2
We will compare the following weighting strate-
gies:
TF The frequency of a word in the set of documents
is taken as a baseline measure.
Likelihood ratio This is taken from (Leuski et al,
2003) and used as a reference measure. We
have implemented the procedure described in
(Rayson and Garside, 2000) using unigrams
only.
OKAPImod We have also considered a measure
derived from Okapi and used in (Robertson et
al., 1992). We have adapted the measure to
consider the set of 100 documents as one single
document.
TFSYNTAX Using our first experimental result,
TFSYNTAX computes the weight of a term
as the number of times it appears preceding a
verb.
Figure 4: Comparison of weighting schemes to ex-
tract relevant terms
5.2 Results
Figure 4 draws Recall/Noise curves for all weight-
ing criteria. They all give similar results except our
TFSYNTAX measure, which performs better than
the others for TT topics. Note that the TFSYN-
TAX measure only considers 10% of the vocabu-
lary, which are the words immediately preceding
verbs in the texts.
In order to check whether this result is consistent
across topics (and not only the effect on an average)
we have compared recall for term lists of size 50 for
individual topics. We have selected 50 as a number
which is large enough to reach a good coverage and
permit additional filtering in an interactive summa-
rization process, such as the iNeast terminological
clustering described in (Leuski et al, 2003).
Figure 5 shows these results by topic. TFSYN-
TAX performs consistently better for all topics ex-
cept one of the IE topics, where the maximum like-
lihood measure is slightly better.
Apart from the fact that TFSYNTAX performs
better than all other methods, it is worth noticing
that sophisticated weighting mechanisms, such as
Okapi and the likelihood ratio, do not behave bet-
ter than a simple frequency count (TF).
6 Conclusions
The automatic extraction of relevant concepts for
a set of related documents is a part of many mod-
els of automatic or interactive summarization. In
this paper, we have analyzed the distribution of rel-
evant concepts across different syntactic functions,
and we have measured the usefulness of detecting
key terms to extract relevant concepts.
Our results suggest that the distribution of key
concepts in sentences is not uniform, having a max-
imum in positions immediately preceding the sen-
tence main verb, in noun phrases acting as subjects
and, more specifically, in the complements (rather
than the head) of noun phrases acting as subjects.
This evidence has been collected using a Spanish
news collection, and should be corroborated outside
the news domain and also adapted to be used for non
SVO languages.
We have also obtained empirical evidence that
statistical weights to select key terms can be im-
proved if we restrict candidate words to those which
precede the verb in some sentence. The combi-
nation of statistical measures and syntactic criteria
overcomes pure statistical weights, at least for TT
topics, where there is certain consistency in the key
concepts across documents.
Acknowledgments
This research has been partially supported by a re-
search grant of the Spanish Government (project
Hermes) and a research grant from UNED. We are
indebted to J. Cigarra?n who calculated the Okapi
weights used in this work.
References
E. Amigo, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. Information synthesis: an em-
pirical study. In Proceedings of the 42th Annual
Meeting of the ACL, Barcelona, July.
B. Boguraev, C. Kennedy, R. Bellamy, S. Brawer,
Y. Wong, and J. Swartz. 1998. Dynamic Presen-
tation of Document Content for Rapid On-line
Skimming. In Proceedings of the AAAI Spring
Figure 5: Comparison of weighting schemes by topic
1998 Symposium on Intelligent Text Summariza-
tion, Stanford, CA.
O. Buyukkokten, H. Garc??a-Molina, and
A. Paepcke. 1999. Seeing the Whole in
Parts: Text Summarization for Web Browsing
on Handheld Devices. In Proceedings of 10th
International WWW Conference.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1):61?74.
H. P. Edmundson. 1969. New Methods in Auto-
matic Extracting. Journal of the Association for
Computing Machinery, 16(2):264?285.
S. Jones, S. Lundy, and G. W. Paynter. 2002. In-
teractive Document Summarization Using Auto-
matically Extracted Keyphrases. In Proceedings
of the 35th Hawaii International Conference on
System Sciences, Big Island, Hawaii.
W. Kraaij, M. Spitters, and A. Hulth. 2002.
Headline Extraction based on a Combination of
Uni- and Multi-Document Summarization Tech-
niques. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalua-
tion, Philadelphia, PA, July.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of SI-
GIR?95.
A. Leuski, C. Y. Lin, and S. Stubblebine. 2003.
iNEATS: Interactive Multidocument Summariza-
tion. In Proceedings of the 4lst Annual Meeting
of the ACL (ACL 2003), Sapporo, Japan.
C.-Y. Lin and E.H. Hovy. 1997. Identifying Top-
ics by Position. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing
(ANLP), Washington, DC.
C. Lin and E. Hovy. 2002. NeATS in DUC
2002. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalu-
ation, Philadelphia, PA, July.
M. S. Neff and J. W. Cooper. 1999. ASHRAM: Ac-
tive Summarization and Markup. In Proceedings
of HICSS-32: Understanding Digital Documents.
A. Pen?as, F. Verdejo, and J. Gonzalo. 2002. Ter-
minology Retrieval: Towards a Synergy be-
tween Thesaurus and Free Text Searching. In IB-
ERAMIA 2002, pages 684?693, Sevilla, Spain.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
S. Preston, K.and Williams. 1994. Managing the
Information Overload. Physics in Business, June.
P. Rayson and R. Garside. 2000. Comparing Cor-
pora Using Frequency Profiling. In Proceedings
of the workshop on Comparing Corpora, pages
1?6, Honk Kong.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conference, pages 21?30.
J. D. Schlesinger, M. E. Okurowski, J. M. Conroy,
D. P. O?Leary, A. Taylor, J. Hobbs, and H. Wil-
son. 2002. Understanding Machine Performance
in the Context of Human Performance for Multi-
Document Summarization. In Proceedings of the
DUC 2002 Workshop on Multi-Document Sum-
marization Evaluation, Philadelphia, PA, July.
An Empirical Study of Information Synthesis Tasks
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,victor,anselmo,felisa}@lsi.uned.es
Abstract
This paper describes an empirical study of the ?In-
formation Synthesis? task, defined as the process of
(given a complex information need) extracting, or-
ganizing and inter-relating the pieces of information
contained in a set of relevant documents, in order to
obtain a comprehensive, non redundant report that
satisfies the information need.
Two main results are presented: a) the creation
of an Information Synthesis testbed with 72 reports
manually generated by nine subjects for eight com-
plex topics with 100 relevant documents each; and
b) an empirical comparison of similarity metrics be-
tween reports, under the hypothesis that the best
metric is the one that best distinguishes between
manual and automatically generated reports. A met-
ric based on key concepts overlap gives better re-
sults than metrics based on n-gram overlap (such as
ROUGE) or sentence overlap.
1 Introduction
A classical Information Retrieval (IR) system helps
the user finding relevant documents in a given text
collection. In most occasions, however, this is only
the first step towards fulfilling an information need.
The next steps consist of extracting, organizing and
relating the relevant pieces of information, in or-
der to obtain a comprehensive, non redundant report
that satisfies the information need.
In this paper, we will refer to this process as In-
formation Synthesis. It is normally understood as
an (intellectually challenging) human task, and per-
haps the Google Answer Service1 is the best gen-
eral purpose illustration of how it works. In this ser-
vice, users send complex queries which cannot be
answered simply by inspecting the first two or three
documents returned by a search engine. These are a
couple of real, representative examples:
a) I?m looking for information concerning the history of text
compression both before and with computers.
1http://answers.google.com
b) Provide an analysis on the future of web browsers, if
any.
Answers to such complex information needs are
provided by experts which, commonly, search the
Internet, select the best sources, and assemble the
most relevant pieces of information into a report,
organizing the most important facts and providing
additional web hyperlinks for further reading. This
Information Synthesis task is understood, in Google
Answers, as a human task for which a search engine
only provides the initial starting point. Our mid-
term goal is to develop computer assistants that help
users to accomplish Information Synthesis tasks.
From a Computational Linguistics point of view,
Information Synthesis can be seen as a kind of
topic-oriented, informative multi-document sum-
marization, where the goal is to produce a single
text as a compressed version of a set of documents
with a minimum loss of relevant information. Un-
like indicative summaries (which help to determine
whether a document is relevant to a particular topic),
informative summaries must be helpful to answer,
for instance, factual questions about the topic. In
the remainder of the paper, we will use the term
?reports? to refer to the summaries produced in an
Information Synthesis task, in order to distinguish
them from other kinds of summaries.
Topic-oriented multi-document summarization
has already been studied in other evaluation ini-
tiatives which provide testbeds to compare alterna-
tive approaches (Over, 2003; Goldstein et al, 2000;
Radev et al, 2000). Unfortunately, those stud-
ies have been restricted to very small summaries
(around 100 words) and small document sets (10-
20 documents). These are relevant summarization
tasks, but hardly representative of the Information
Synthesis problem we are focusing on.
The first goal of our work has been, therefore,
to create a suitable testbed that permits qualitative
and quantitative studies on the information synthe-
sis task. Section 2 describes the creation of such a
testbed, which includes the manual generation of 72
reports by nine different subjects across 8 complex
topics with 100 relevant documents per topic.
Using this testbed, our second goal has been to
compare alternative similarity metrics for the Infor-
mation Synthesis task. A good similarity metric
provides a way of evaluating Information Synthe-
sis systems (comparing their output with manually
generated reports), and should also shed some light
on the common properties of manually generated re-
ports. Our working hypothesis is that the best metric
will best distinguish between manual and automati-
cally generated reports.
We have compared several similarity metrics, in-
cluding a few baseline measures (based on docu-
ment, sentence and vocabulary overlap) and a state-
of-the-art measure to evaluate summarization sys-
tems, ROUGE (Lin and Hovy, 2003). We also intro-
duce another proximity measure based on key con-
cept overlap, which turns out to be substantially bet-
ter than ROUGE for a relevant class of topics.
Section 3 describes these metrics and the experi-
mental design to compare them; in Section 4, we an-
alyze the outcome of the experiment, and Section 5
discusses related work. Finally, Section 6 draws the
main conclusions of this work.
2 Creation of an Information Synthesis
testbed
We refer to Information Synthesis as the process
of generating a topic-oriented report from a non-
trivial amount of relevant, possibly interrelated doc-
uments. The first goal of our work is the generation
of a testbed (ISCORPUS) with manually produced
reports that serve as a starting point for further em-
pirical studies and evaluation of information synthe-
sis systems. This section describes how this testbed
has been built.
2.1 Document collection and topic set
The testbed must have a certain number of features
which, altogether, differentiate the task from current
multi-document summarization evaluations:
Complex information needs. Being Informa-
tion Synthesis a step which immediately follows a
document retrieval process, it seems natural to start
with standard IR topics as used in evaluation con-
ferences such as TREC2, CLEF3 or NTCIR4. The
title/description/narrative topics commonly used in
such evaluation exercises are specially well suited
for an Information Synthesis task: they are complex
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://research.nii.ac.jp/ntcir/
and well defined, unlike, for instance, typical web
queries.
We have selected the Spanish CLEF 2001-2003
news collection testbed (Peters et al, 2002), be-
cause Spanish is the native language of the subjects
recruited for the manual generation of reports. Out
of the CLEF topic set, we have chosen the eight
topics with the largest number of documents man-
ually judged as relevant from the assessment pools.
We have slightly reworded the topics to change the
document retrieval focus (?Find documents that...?)
into an information synthesis wording (?Generate a
report about...?). Table 1 shows the eight selected
topics.
C042: Generate a report about the invasion of Haiti by UN/US
soldiers.
C045: Generate a report about the main negotiators of the
Middle East peace treaty between Israel and Jordan, giving
detailed information on the treaty.
C047: What are the reasons for the military intervention of
Russia in Chechnya?
C048: Reasons for the withdrawal of United Nations (UN)
peace- keeping forces from Bosnia.
C050: Generate a report about the uprising of Indians in
Chiapas (Mexico).
C085: Generate a report about the operation ?Turquoise?, the
French humanitarian program in Rwanda.
C056: Generate a report about campaigns against racism in
Europe.
C080: Generate a report about hunger strikes attempted in
order to attract attention to a cause.
Table 1: Topic set
This set of eight CLEF topics has two differenti-
ated subsets: in a majority of cases (first six topics),
it is necessary to study how a situation evolves in
time; the importance of every event related to the
topic can only be established in relation with the
others. The invasion of Haiti by UN and USA troops
(C042) is an example of such a topic. We will refer
to them as ?Topic Tracking? (TT) reports, because
they resemble the kind of topics used in such task.
The last two questions (56 and 80), however, re-
semble Information Extraction tasks: essentially,
the user has to detect and describe instances of
a generic event (cases of hunger strikes and cam-
paigns against racism in Europe); hence we will re-
fer to them as ?IE? reports.
Topic tracking reports need a more elaborated
treatment of the information in the documents, and
therefore are more interesting from the point of view
of Information Synthesis. We have, however, de-
cided to keep the two IE topics; first, because they
also reflect a realistic synthesis task; and second, be-
cause they can provide contrastive information as
compared to TT reports.
Large document sets. All the selected CLEF
topics have more than one hundred documents
judged as relevant by the CLEF assessors. For ho-
mogeneity, we have restricted the task to the first
100 documents for each topic (using a chronologi-
cal order).
Complex reports. The elaboration of a com-
prehensive report requires more space than is al-
lowed in current multi-document summarization ex-
periences. We have established a maximum of fifty
sentences per summary, i.e., half a sentence per doc-
ument. This limit satisfies three conditions: a) it
is large enough to contain the essential information
about the topic, b) it requires a substantial compres-
sion effort from the user, and c) it avoids defaulting
to a ?first sentence? strategy by lazy (or tired) users,
because this strategy would double the maximum
size allowed.
We decided that the report generation would be
an extractive task, which consists of selecting sen-
tences from the documents. Obviously, a realistic
information synthesis process also involves rewrit-
ing and elaboration of the texts contained in the doc-
uments. Keeping the task extractive has, however,
two major advantages: first, it permits a direct com-
parison to automatic systems, which will typically
be extractive; and second, it is a simpler task which
produces less fatigue.
2.2 Generation of manual reports
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of reports. All
of them self-reported university degrees and a large
experience using search engines and performing in-
formation searches.
All subjects were given an in-place detailed de-
scription of the task in order to minimize divergent
interpretations. They were told that, in a first step,
they had to generate reports with a maximum of in-
formation about every topic within the fifty sentence
space limit. In a second step, which would take
place six months afterwards, they would be exam-
ined from each of the eight topics. The only docu-
mentation allowed during the exam would be the re-
ports generated in the first phase of the experiment.
Subjects scoring best would be rewarded.
These instructions had two practical effects: first,
the competitive setup was an extra motivation for
achieving better results. And second, users tried to
take advantage of all available space, and thus most
reports were close to the fifty sentences limit. The
time limit per topic was set to 30 minutes, which is
tight for the information synthesis task, but prevents
the effects of fatigue.
We implemented an interface to facilitate the gen-
eration of extractive reports. The system displays a
list with the titles of relevant documents in chrono-
logical order. Clicking on a title displays the full
document, where the user can select any sentence(s)
and add them to the final report. A different frame
displays the selected sentences (also in chronolog-
ical order), together with one bar indicating the re-
maining time and another bar indicating the remain-
ing space. The 50 sentence limit can be temporarily
exceeded and, when the 30 minute limit has been
reached, the user can still remove sentences from
the report until the sentence limit is reached back.
2.3 Questionnaires
After summarizing every topic, the following ques-
tionnaire was filled in by every user:
? Who are the main people involved in the topic?
? What are the main organizations participating in the
topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e. a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
the topic 42 about the invasion of Haiti by UN and
USA troops in 1994:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is gener-
ated for each topic, joining all the different answers.
Redundant concepts (e.g. ?war? and ?conflict?)
were inspected and collapsed by hand. These lists
of key concepts constitute the gold standard for the
similarity metric described in Section 3.2.5.
Besides identifying key concepts, users also filled
in the following questionnaire:
? Were you familiarized with the topic?
? Was it hard for you to elaborate the report?
? Did you miss the possibility of introducing annotations
or rewriting parts of the report by hand?
? Do you consider that you generated a good report?
? Are you tired?
Out of the answers provided by users, the most
remarkable facts are that:
? only in 6% of the cases the user missed ?a lot?
the possibility of rewriting/adding comments
to the topic. The fact that reports are made ex-
tractively did not seem to be a significant prob-
lem for our users.
? in 73% of the cases, the user was quite or very
satisfied about his summary.
These are indications that the practical con-
straints imposed on the task (time limit and extrac-
tive nature of the summaries) do not necessarily
compromise the representativeness of the testbed.
The time limit is very tight, but the temporal ar-
rangement of documents and their highly redundant
nature facilitates skipping repetitive material (some
pieces of news are discarded just by looking at the
title, without examining the content).
2.4 Generation of baseline reports
We have automatically generated baseline reports in
two steps:
? For every topic, we have produced 30 tentative
baseline reports using DUC style criteria:
? 18 summaries consist only of picking the
first sentence out of each document in 18
different document subsets. The subsets
are formed using different strategies, e.g.
the most relevant documents for the query
(according to the Inquery search engine),
one document per day, the first or last 50
documents in chronological order, etc.
? The other 12 summaries consist of a)
picking the first n sentences out of a set
of selected documents (with different val-
ues for n and different sets of documents)
and b) taking the full content of a few doc-
uments. In both cases, document sets are
formed with similar criteria as above.
? Out of these 30 baseline reports, we have se-
lected the 10 reports which have the highest
sentence overlap with the manual summaries.
The second step increases the quality of the base-
lines, making the task of differentiating manual and
baseline reports more challenging.
3 Comparison of similarity metrics
Formal aspects of a summary (or report), such
as legibility, grammatical correctness, informative-
ness, etc., can only be evaluated manually. How-
ever, automatic evaluation metrics can play a useful
role in the evaluation of how well the information
from the original sources is preserved (Mani, 2001).
Previous studies have shown that it is feasible to
evaluate the output of summarization systems au-
tomatically (Lin and Hovy, 2003). The process is
based in similarity metrics between texts. The first
step is to establish a (manual) reference summary,
and then the automatically generated summaries are
ranked according to their similarity to the reference
summary.
The challenge is, then, to define an appropriate
proximity metric for reports generated in the infor-
mation synthesis task.
3.1 How to compare similarity metrics without
human judgments? The QARLA
estimation
In tasks such as Machine Translation and Summa-
rization, the quality of a proximity metric is mea-
sured in terms of the correlation between the rank-
ing produced by the metric, and a reference ranking
produced by human judges. An optimal similarity
metric should produce the same ranking as human
judges.
In our case, acquiring human judgments about
the quality of the baseline reports is too costly, and
probably cannot be done reliably: a fine-grained
evaluation of 50-sentence reports summarizing sets
of 100 documents is a very complex task, which
would probably produce different rankings from
different judges.
We believe there is a cheaper and more robust
way of comparing similarity metrics without using
human assessments. We assume a simple hypothe-
sis: the best metric should be the one that best dis-
criminates between manual and automatically gen-
erated reports. In other words, a similarity metric
that cannot distinguish manual and automatic re-
ports cannot be a good metric. Then, all we need
is an estimation of how well a similarity metric sep-
arates manual and automatic reports. We propose
to use the probability that, given any manual report
Mref , any other manual report M is closer to Mref
than any other automatic report A:
QARLA(sim) = P (sim(M,Mref ) > sim(A,Mref ))
where M,Mref ?M, A ? A
where M is the set of manually generated re-
ports, A is the set of automatically generated re-
ports, and ?sim? is the similarity metric being eval-
uated.
We refer to this value as the QARLA5 estimation.
QARLA has two interesting features:
? No human assessments are needed to compute
QARLA. Only a set of manually produced
summaries and a set of automatic summaries,
for each topic considered. This reduces the
cost of creating the testbed and, in addition,
eliminates the possible bias introduced by hu-
man judges.
? It is easy to collect enough data to achieve sta-
tistically significant results. For instance, our
testbed provides 720 combinations per topic
to estimate QARLA probability (we have
nine manual plus ten automatic summaries per
topic).
A good QARLA value does not guarantee that
a similarity metric will produce the same rankings
as human judges, but a good similarity metric must
have a good QARLA value: it is unlikely that
a measure that cannot distinguish between manual
and automatic summaries can still produce high-
quality rankings of automatic summaries by com-
parison to manual reference summaries.
3.2 Similarity metrics
We have compared five different metrics using the
QARLA estimation. The first three are meant as
baselines; the fourth is the standard similarity met-
ric used to evaluate summaries (ROUGE); and the
last one, introduced in this paper, is based on the
overlapping of key concepts.
3.2.1 Baseline 1: Document co-selection metric
The following metric estimates the similarity of two
reports from the set of documents which are repre-
sented in both reports (i.e. at least one sentence in
each report belongs to the document).
DocSim(Mr,M) =
|Doc(Mr) ?Doc(M)|
|Doc(Mr)|
where Mr is the reference report, M a second re-
port and Doc(Mr), Doc(M) are the documents to
which the sentences in Mr,M belong to.
5Quality criterion for reports evaluation metrics
3.2.2 Baselines 2 and 3: Sentence co-selection
The more sentences in common between two re-
ports, the more similar their content will be. We can
measure Recall (how many sentences from the ref-
erence report are also in the contrastive report) and
Precision (how many sentences from the contrastive
report are also in the reference report):
SentenceSimR(Mr,M) =
|S(Mr) ? S(M)|
|S(Mr)|
SentenceSimP (Mr,M) =
|S(Mr) ? S(M)|
|S(M)|
where S(Mr), S(M) are the sets of sentences in
the reports Mr (reference) and M (contrastive).
3.2.3 Baseline 4: Perplexity
A language model is a probability distribution over
word sequences obtained from some training cor-
pora (see e.g. (Manning and Schutze, 1999)). Per-
plexity is a measure of the degree of surprise of a
text or corpus given a language model. In our case,
we build a language model LM(Mr) for the refer-
ence report Mr, and measure the perplexity of the
contrastive report M as compared to that language
model:
PerplexitySim(Mr,M) =
1
Perp(LM(Mr),M)
We have used the Good-Turing discount algo-
rithm to compute the language models (Clarkson
and Rosenfeld, 1997). Note that this is also a base-
line metric, because it only measures whether the
content of the contrastive report is compatible with
the reference report, but it does not consider the cov-
erage: a single sentence from the reference report
will have a low perplexity, even if it covers only a
small fraction of the whole report. This problem
is mitigated by the fact that we are comparing re-
ports of approximately the same size and without
repeated sentences.
3.2.4 ROUGE metric
The distance between two summaries can be estab-
lished as a function of their vocabulary (unigrams)
and how this vocabulary is used (n-grams). From
this point of view, some of the measures used in the
evaluation of Machine Translation systems, such as
BLEU (Papineni et al, 2002), have been imported
into the summarization task. BLEU is based in the
precision and n-gram co-ocurrence between an au-
tomatic translation and a reference manual transla-
tion.
(Lin and Hovy, 2003) tried to apply BLEU as
a measure to evaluate summaries, but the results
were not as good as in Machine Translation. In-
deed, some of the characteristics that define a good
translation are not related with the features of a good
summary; then Lin and Hovy proposed a recall-
based variation of BLEU, known as ROUGE. The
idea is the same: the quality of a proposed sum-
mary can be calculated as a function of the n-grams
in common between the units of a model summary.
The units can be sentences or discourse units:
ROUGEn =
?
C?{MU}
?
n-gram?C Countm
?
C?{MU}
?
n-gram?C Count
where MU is the set of model units, Countm is
the maximum number of n-grams co-ocurring in a
peer summary and a model unit, and Count is the
number of n-grams in the model unit. It has been
established that unigram and bigram based metrics
permit to create a ranking of automatic summaries
better (more similar to a human-produced ranking)
than n-grams with n > 2.
For our experiment, we have only considered un-
igrams (lemmatized words, excluding stop words),
which gives good results with standard summaries
(Lin and Hovy, 2003).
3.2.5 Key concepts metric
Two summaries generated by different subjects may
differ in the documents that contribute to the sum-
mary, in the sentences that are chosen, and even in
the information that they provide. In our Informa-
tion Synthesis settings, where topics are complex
and the number of documents to summarize is large,
it is likely to expect that similarity measures based
on document, sentence or n-gram overlap do not
give large similarity values between pairs of man-
ually generated summaries.
Our hypothesis is that two manual reports, even if
they differ in their information content, will have the
same (or very similar) key concepts; if this is true,
comparing the key concepts of two reports can be a
better similarity measure than the previous ones.
In order to measure the overlap of key concepts
between two reports, we create a vector ~kc for every
report, such that every element in the vector repre-
sents the frequency of a key concept in the report in
relation to the size of the report:
kc(M)i =
freq(Ci,M)
|words(M)|
being freq(Ci,M) the number of times the
key concept Ci appears in the report M , and
|words(M)| the number of words in the report.
The key concept similarity NICOS (Nuclear In-
formative Concept Similarity) between two reports
M and Mr can then be defined as the inverse of the
Euclidean distance between their associated concept
vectors:
NICOS(M,Mr) =
1
| ~kc(Mr)? ~kc(M)|
In our experiment, the dimensions of kc vectors
correspond to the list of key concepts provided by
our test subjects (see Section 2.3). This list is our
gold standard for every topic.
4 Experimental results
Figure 1 shows, for every topic (horizontal axis),
the QARLA estimation obtained for each similarity
metric, i.e., the probability of a manual report being
closer to other manual report than to an automatic
report. Table 2 shows the average QARLA measure
across all topics.
Metric TT topics IE topics
Perplexity 0.19 0.60
DocSim 0.20 0.34
SentenceSimR 0.29 0.52
SentenceSimP 0.38 0.57
ROUGE 0.54 0.53
NICOS 0.77 0.52
Table 2: Average QARLA
For the six TT topics, the key concept similarity
NICOS performs 43% better than ROUGE, and all
baselines give poor results (all their QARLA proba-
bilities are below chance, QARLA < 0.5). A non-
parametric Wilcoxon sign test confirms that the dif-
ference between NICOS and ROUGE is highly sig-
nificant (p < 0.005). This is an indication that the
Information Synthesis task, as we have defined it,
should not be studied as a standard summarization
problem. It also confirms our hypothesis that key
concepts tend to be stable across different users, and
may help to generate the reports.
The behavior of the two Information Extraction
(IE) topics is substantially different from TT topics.
While the ROUGE measure remains stable (0.53
versus 0.54), the key concept similarity is much
worse with IE topics (0.52 versus 0.77). On the
other hand, all baselines improve, and some of them
(SentenceSim precision and perplexity) give better
results than both ROUGE and NICOS.
Of course, no reliable conclusion can be obtained
from only two IE topics. But the observed differ-
ences suggest that TT and IE may need different
approaches, both to the automatic generation of re-
ports and to their evaluation.
Figure 1: Comparison of similarity metrics by topic
One possible reason for this different behavior is
that IE topics do not have a set of consistent key
concepts; every case of a hunger strike, for instance,
involves different people, organizations and places.
The average number of different key concepts is
18.7 for TT topics and 28.5 for IE topics, a differ-
ence that reveals less agreement between subjects,
supporting this argument.
5 Related work
Besides the measures included in our experiment,
there are other criteria to compare summaries which
could as well be tested for Information Synthesis:
Annotation of relevant sentences in a corpus.
(Khandelwal et al, 2001) propose a task, called
?Temporal Summarization?, that combines summa-
rization and topic tracking. The paper describes the
creation of an evaluation corpus in which the most
relevant sentences in a set of related news were an-
notated. Summaries are evaluated with a measure
called ?novel recall?, based in sentences selected by
a summarization system and sentences manually as-
sociated to events in the corpus. The agreement rate
between subjects in the identification of key events
and the sentence annotation does not correspond
with the agreement between reports that we have
obtained in our experiments. There are, at least, two
reasons to explain this:
? (Khandelwal et al, 2001) work on an average
of 43 documents, half the size of the topics in
our corpus.
? Although there are topics in both experiments,
the information needs in our testbed are more
complex (e.g. motivations for the invasion of
Chechnya)
Factoids. One of the problems in the evalua-
tion of summaries is the versatility of human lan-
guage. Two different summaries may contain the
same information. In (Halteren and Teufel, 2003),
the content of summaries is manually represented,
decomposing sentences in factoids or simple facts.
They also annotate the composition, generalization
and implication relations between extracted fac-
toids. The resulting measure is different from un-
igram based similarity. The main problem of fac-
toids, as compared to other metrics, is that they re-
quire a costly manual processing of the summaries
to be evaluated.
6 Conclusions
In this paper, we have reported an empirical study
of the ?Information Synthesis? task, defined as the
process of (given a complex information need) ex-
tracting, organizing and relating the pieces of infor-
mation contained in a set of relevant documents, in
order to obtain a comprehensive, non redundant re-
port that satisfies the information need.
We have obtained two main results:
? The creation of an Information Synthesis
testbed (ISCORPUS) with 72 reports manually
generated by 9 subjects for 8 complex topics
with 100 relevant documents each.
? The empirical comparison of candidate metrics
to estimate the similarity between reports.
Our empirical comparison uses a quantitative cri-
terion (the QARLA estimation) based on the hy-
pothesis that a good similarity metric will be able to
distinguish between manual and automatic reports.
According to this measure, we have found evidence
that the Information Synthesis task is not a standard
multi-document summarization problem: state-of-
the-art similarity metrics for summaries do not per-
form equally well with the reports in our testbed.
Our most interesting finding is that manually
generated reports tend to have the same key con-
cepts: a similarity metric based on overlapping key
concepts (NICOS) gives significantly better results
than metrics based on language models, n-gram co-
ocurrence and sentence overlapping. This is an in-
dication that detecting relevant key concepts is a
promising strategy in the process of generating re-
ports.
Our results, however, has also some intrinsic lim-
itations. Firstly, manually generated summaries are
extractive, which is good for comparison purposes,
but does not faithfully reflect a natural process of
human information synthesis. Another weakness is
the maximum time allowed per report: 30 minutes
seems too little to examine 100 documents and ex-
tract a decent report, but allowing more time would
have caused an excessive fatigue to users. Our vol-
unteers, however, reported a medium to high satis-
faction with the results of their work, and in some
occasions finished their task without reaching the
time limit.
ISCORPUS is available at:
http://nlp.uned.es/ISCORPUS
Acknowledgments
This research has been partially supported by a
grant of the Spanish Government, project HERMES
(TIC-2000-0335-C03-01). We are indebted to E.
Hovy for his comments on an earlier version of
this paper, and C. Y. Lin for his assistance with the
ROUGE measure. Thanks also to our volunteers for
their valuable cooperation.
References
P. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
toolkit. In Proceeding of Eurospeech ?97,
Rhodes, Greece.
J. Goldstein, V. O. Mittal, J. G. Carbonell, and
J. P. Callan. 2000. Creating and Evaluating
Multi-Document Sentence Extract Summaries.
In Proceedings of Ninth International Confer-
ences on Information Knowledge Management
(CIKM?00), pages 165?172, McLean, VA.
H. V. Halteren and S. Teufel. 2003. Examin-
ing the Consensus between Human Summaries:
Initial Experiments with Factoids Analysis. In
HLT/NAACL-2003 Workshop on Automatic Sum-
marization, Edmonton, Canada.
V. Khandelwal, R. Gupta, and J. Allan. 2001. An
Evaluation Corpus for Temporal Summarization.
In Proceedings of the First International Confer-
ence on Human Language Technology Research
(HLT 2001), Tolouse, France.
C. Lin and E. H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-ocurrence
Statistics. In Proceeding of the 2003 Language
Technology Conference (HLT-NAACL 2003), Ed-
monton, Canada.
I. Mani. 2001. Automatic Summarization, vol-
ume 3 of Natural Language Processing. John
Benjamins Publishing Company, Amster-
dam/Philadelphia.
C. D. Manning and H. Schutze. 1999. Foundations
of statistical natural language processing. MIT
Press, Cambridge Mass.
P. Over. 2003. Introduction to DUC-2003: An In-
trinsic Evaluation of Generic News Text Summa-
rization Systems. In Proceedings of Workshop on
Automatic Summarization (DUC 2003).
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?
318, Philadelphia.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
D. R. Radev, J. Hongyan, and M. Budzikowska.
2000. Centroid-Based Summarization of Mul-
tiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceed-
ings of the Workshop on Automatic Summariza-
tion at the 6th Applied Natural Language Pro-
cessing Conference and the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics, Seattle, WA, April.
