Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513?521,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Error-Driven Word-Character Hybrid Model
for Joint Chinese Word Segmentation and POS Tagging
Canasai Kruengkrai?? and Kiyotaka Uchimoto? and Jun?ichi Kazama?
Yiou Wang? and Kentaro Torisawa? and Hitoshi Isahara??
?Graduate School of Engineering, Kobe University
1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp
Abstract
In this paper, we present a discriminative
word-character hybrid model for joint Chi-
nese word segmentation and POS tagging.
Our word-character hybrid model offers
high performance since it can handle both
known and unknown words. We describe
our strategies that yield good balance for
learning the characteristics of known and
unknown words and propose an error-
driven policy that delivers such balance
by acquiring examples of unknown words
from particular errors in a training cor-
pus. We describe an efficient framework
for training our model based on the Mar-
gin Infused Relaxed Algorithm (MIRA),
evaluate our approach on the Penn Chinese
Treebank, and show that it achieves supe-
rior performance compared to the state-of-
the-art approaches reported in the litera-
ture.
1 Introduction
In Chinese, word segmentation and part-of-speech
(POS) tagging are indispensable steps for higher-
level NLP tasks. Word segmentation and POS tag-
ging results are required as inputs to other NLP
tasks, such as phrase chunking, dependency pars-
ing, and machine translation. Word segmenta-
tion and POS tagging in a joint process have re-
ceived much attention in recent research and have
shown improvements over a pipelined fashion (Ng
and Low, 2004; Nakagawa and Uchimoto, 2007;
Zhang and Clark, 2008; Jiang et al, 2008a; Jiang
et al, 2008b).
In joint word segmentation and the POS tag-
ging process, one serious problem is caused by
unknown words, which are defined as words that
are not found in a training corpus or in a sys-
tem?s word dictionary1. The word boundaries and
the POS tags of unknown words, which are very
difficult to identify, cause numerous errors. The
word-character hybrid model proposed by Naka-
gawa and Uchimoto (Nakagawa, 2004; Nakagawa
and Uchimoto, 2007) shows promising properties
for solving this problem. However, it suffers from
structural complexity. Nakagawa (2004) described
a training method based on a word-based Markov
model and a character-based maximum entropy
model that can be completed in a reasonable time.
However, this training method is limited by the
generatively-trained Markov model in which in-
formative features are hard to exploit.
In this paper, we overcome such limitations
concerning both efficiency and effectiveness. We
propose a new framework for training the word-
character hybrid model based on the Margin
Infused Relaxed Algorithm (MIRA) (Crammer,
2004; Crammer et al, 2005; McDonald, 2006).
We describe k-best decoding for our hybrid model
and design its loss function and the features appro-
priate for our task.
In our word-character hybrid model, allowing
the model to learn the characteristics of both
known and unknown words is crucial to achieve
optimal performance. Here, we describe our
strategies that yield good balance for learning
these two characteristics. We propose an error-
driven policy that delivers this balance by acquir-
ing examples of unknown words from particular
errors in a training corpus. We conducted our ex-
periments on Penn Chinese Treebank (Xia et al,
2000) and compared our approach with the best
previous approaches reported in the literature. Ex-
perimental results indicate that our approach can
achieve state-of-the-art performance.
1A system?s word dictionary usually consists of a word
list, and each word in the list has its own POS category. In
this paper, we constructed the system?s word dictionary from
a training corpus.
513
Figure 1: Lattice used in word-character hybrid model.
Tag Description
B Beginning character in a multi-character word
I Intermediate character in a multi-character word
E End character in a multi-character word
S Single-character word
Table 1: Position-of-character (POC) tags.
The paper proceeds as follows: Section 2 gives
background on the word-character hybrid model,
Section 3 describes our policies for correct path
selection, Section 4 presents our training method
based on MIRA, Section 5 shows our experimen-
tal results, Section 6 discusses related work, and
Section 7 concludes the paper.
2 Background
2.1 Problem formation
In joint word segmentation and the POS tag-
ging process, the task is to predict a path
of word hypotheses y = (y1, . . . , y#y) =
(?w1, p1?, . . . , ?w#y, p#y?) for a given character
sequence x = (c1, . . . , c#x), where w is a word,
p is its POS tag, and a ?#? symbol denotes the
number of elements in each variable. The goal of
our learning algorithm is to learn a mapping from
inputs (unsegmented sentences) x ? X to outputs
(segmented paths) y ? Y based on training sam-
ples of input-output pairs S = {(xt, yt)}Tt=1.
2.2 Search space representation
We represent the search space with a lattice based
on the word-character hybrid model (Nakagawa
and Uchimoto, 2007). In the hybrid model,
given an input sentence, a lattice that consists
of word-level and character-level nodes is con-
structed. Word-level nodes, which correspond to
words found in the system?s word dictionary, have
regular POS tags. Character-level nodes have spe-
cial tags where position-of-character (POC) and
POS tags are combined (Asahara, 2003; Naka-
gawa, 2004). POC tags indicate the word-internal
positions of the characters, as described in Table 1.
Figure 1 shows an example of a lattice for a Chi-
nese sentence: ? ? (Chongming is
China?s third largest island). Note that some nodes
and state transitions are not allowed. For example,
I and E nodes cannot occur at the beginning of the
lattice (marked with dashed boxes), and the transi-
tions from I to B nodes are also forbidden. These
nodes and transitions are ignored during the lattice
construction processing.
In the training phase, since several paths
(marked in bold) can correspond to the correct
analysis in the annotated corpus, we need to se-
lect one correct path yt as a reference for training.2
The next section describes our strategies for deal-
ing with this issue.
With this search space representation, we
can consistently handle unknown words with
character-level nodes. In other words, we use
word-level nodes to identify known words and
character-level nodes to identify unknown words.
In the testing phase, we can use a dynamic pro-
gramming algorithm to search for the most likely
path out of all candidate paths.
2A machine learning problem exists called structured
multi-label classification that allows training from multiple
correct paths. However, in this paper we limit our considera-
tion to structured single-label classification, which is simple
yet provides great performance.
514
3 Policies for correct path selection
In this section, we describe our strategies for se-
lecting the correct path yt in the training phase.
As shown in Figure 1, the paths marked in bold
can represent the correct annotation of the seg-
mented sentence. Ideally, we need to build a word-
character hybrid model that effectively learns the
characteristics of unknown words (with character-
level nodes) as well as those of known words (with
word-level nodes).
We can directly estimate the statistics of known
words from an annotated corpus where a sentence
is already segmented into words and assigned POS
tags. If we select the correct path yt that corre-
sponds to the annotated sentence, it will only con-
sist of word-level nodes that do not allow learning
for unknown words. We therefore need to choose
character-level nodes as correct nodes instead of
word-level nodes for some words. We expect that
those words could reflect unknown words in the
future.
Baayen and Sproat (1996) proposed that the
characteristics of infrequent words in a training
corpus resemble those of unknown words. Their
idea has proven effective for estimating the statis-
tics of unknown words in previous studies (Ratna-
parkhi, 1996; Nagata, 1999; Nakagawa, 2004).
We adopt Baayen and Sproat?s approach as
the baseline policy in our word-character hybrid
model. In the baseline policy, we first count the
frequencies of words3 in the training corpus. We
then collect infrequent words that appear less than
or equal to r times.4 If these infrequent words are
in the correct path, we use character-level nodes
to represent them, and hence the characteristics of
unknown words can be learned. For example, in
Figure 1 we select the character-level nodes of the
word ? ? (Chongming) as the correct nodes. As
a result, the correct path yt can contain both word-
level and character-level nodes (marked with as-
terisks (*)).
To discover more statistics of unknown words,
one might consider just increasing the threshold
value r to obtain more artificial unknown words.
However, our experimental results indicate that
our word-character hybrid model requires an ap-
propriate balance between known and artificial un-
3We consider a word and its POS tag a single entry.
4In our experiments, the optimal threshold value r is se-
lected by evaluating the performance of joint word segmen-
tation and POS tagging on the development set.
known words to achieve optimal performance.
We now describe our new approach to lever-
age additional examples of unknown words. In-
tuition suggests that even though the system can
handle some unknown words, many unidentified
unknown words remain that cannot be recovered
by the system; we wish to learn the characteristics
of such unidentified unknown words. We propose
the following simple scheme:
? Divide the training corpus into ten equal sets
and perform 10-fold cross validation to find
the errors.
? For each trial, train the word-character hybrid
model with the baseline policy (r = 1) us-
ing nine sets and estimate errors using the re-
maining validation set.
? Collect unidentified unknown words from
each validation set.
Several types of errors are produced by the
baseline model, but we only focus on those caused
by unidentified unknown words, which can be eas-
ily collected in the evaluation process. As de-
scribed later in Section 5.2, we measure the recall
on out-of-vocabulary (OOV) words. Here, we de-
fine unidentified unknown words as OOV words
in each validation set that cannot be recovered by
the system. After ten cross validation runs, we
get a list of the unidentified unknown words de-
rived from the whole training corpus. Note that
the unidentified unknown words in the cross val-
idation are not necessary to be infrequent words,
but some overlap may exist. Finally, we obtain the
artificial unknown words that combine the uniden-
tified unknown words in cross validation and in-
frequent words for learning unknown words. We
refer to this approach as the error-driven policy.
4 Training method
4.1 Discriminative online learning
Let Yt = {y1t , . . . , yKt } be a lattice consisting of
candidate paths for a given sentence xt. In the
word-character hybrid model, the lattice Yt can
contain more than 1000 nodes, depending on the
length of the sentence xt and the number of POS
tags in the corpus. Therefore, we require a learn-
ing algorithm that can efficiently handle large and
complex lattice structures.
Online learning is an attractive method for
the hybrid model since it quickly converges
515
Algorithm 1 Generic Online Learning Algorithm
Input: Training set S = {(xt, yt)}Tt=1
Output: Model weight vector w
1: w(0) = 0;v = 0; i = 0
2: for iter = 1 to N do
3: for t = 1 to T do
4: w(i+1) = update w(i) according to (xt, yt)
5: v = v +w(i+1)
6: i = i+ 1
7: end for
8: end for
9: w = v/(N ? T )
within a few iterations (McDonald, 2006). Algo-
rithm 1 outlines the generic online learning algo-
rithm (McDonald, 2006) used in our framework.
4.2 k-best MIRA
We focus on an online learning algorithm called
MIRA (Crammer, 2004), which has the de-
sired accuracy and scalability properties. MIRA
combines the advantages of margin-based and
perceptron-style learning with an optimization
scheme. In particular, we use a generalized ver-
sion of MIRA (Crammer et al, 2005; McDonald,
2006) that can incorporate k-best decoding in the
update procedure. To understand the concept of k-
best MIRA, we begin with a linear score function:
s(x, y;w) = ?w, f(x, y)? , (1)
where w is a weight vector and f is a feature rep-
resentation of an input x and an output y.
Learning a mapping between an input-output
pair corresponds to finding a weight vector w such
that the best scoring path of a given sentence is
the same as (or close to) the correct path. Given
a training example (xt, yt), MIRA tries to estab-
lish a margin between the score of the correct path
s(xt, yt;w) and the score of the best candidate
path s(xt, y?;w) based on the current weight vector
w that is proportional to a loss function L(yt, y?).
In each iteration, MIRA updates the weight vec-
tor w by keeping the norm of the change in the
weight vector as small as possible. With this
framework, we can formulate the optimization
problem as follows (McDonald, 2006):
w(i+1) = argminw?w ?w(i)? (2)
s.t. ?y? ? bestk(xt;w(i)) :
s(xt, yt;w)? s(xt, y?;w) ? L(yt, y?) ,
where bestk(xt;w(i)) ? Yt represents a set of top
k-best paths given the weight vector w(i). The
above quadratic programming (QP) problem can
be solved using Hildreth?s algorithm (Yair Cen-
sor, 1997). Replacing Eq. (2) into line 4 of Al-
gorithm 1, we obtain k-best MIRA.
The next question is how to efficiently gener-
ate bestk(xt;w(i)). In this paper, we apply a dy-
namic programming search (Nagata, 1994) to k-
best MIRA. The algorithm has two main search
steps: forward and backward. For the forward
search, we use Viterbi-style decoding to find the
best partial path and its score up to each node in
the lattice. For the backward search, we use A?-
style decoding to generate the top k-best paths. A
complete path is found when the backward search
reaches the beginning node of the lattice, and the
algorithm terminates when the number of gener-
ated paths equals k.
In summary, we use k-best MIRA to iteratively
update w(i). The final weight vector w is the av-
erage of the weight vectors after each iteration.
As reported in (Collins, 2002; McDonald et al,
2005), parameter averaging can effectively avoid
overfitting. For inference, we can use Viterbi-style
decoding to search for the most likely path y? for
a given sentence x where:
y? = argmax
y?Y
s(x, y;w) . (3)
4.3 Loss function
In conventional sequence labeling where the ob-
servation sequence (word) boundaries are fixed,
one can use the 0/1 loss to measure the errors of
a predicted path with respect to the correct path.
However, in our model, word boundaries vary
based on the considered path, resulting in a dif-
ferent numbers of output tokens. As a result, we
cannot directly use the 0/1 loss.
We instead compute the loss function through
false positives (FP ) and false negatives (FN ).
Here, FP means the number of output nodes that
are not in the correct path, and FN means the
number of nodes in the correct path that cannot
be recognized by the system. We define the loss
function by:
L(yt, y?) = FP + FN . (4)
This loss function can reflect how bad the pre-
dicted path y? is compared to the correct path yt.
A weighted loss function based on FP and FN
can be found in (Ganchev et al, 2007).
516
ID Template Condition
W0 ?w0? for word-level
W1 ?p0? nodes
W2 ?w0, p0?
W3 ?Length(w0), p0?
A0 ?AS(w0)? if w0 is a single-
A1 ?AS(w0), p0? character word
A2 ?AB(w0)? for word-level
A3 ?AB(w0), p0? nodes
A4 ?AE(w0)?
A5 ?AE(w0), p0?
A6 ?AB(w0), AE(w0)?
A7 ?AB(w0), AE(w0), p0?
T0 ?TS(w0)? if w0 is a single-
T1 ?TS(w0), p0? character word
T2 ?TB(w0)? for word-level
T3 ?TB(w0), p0? nodes
T4 ?TE(w0)?
T5 ?TE(w0), p0?
T6 ?TB(w0), TE(w0)?
T7 ?TB(w0), TE(w0), p0?
C0 ?cj?, j ? [?2, 2] ? p0 for character-
C1 ?cj , cj+1?, j ? [?2, 1] ? p0 level nodes
C2 ?c?1, c1? ? p0
C3 ?T (cj)?, j ? [?2, 2] ? p0
C4 ?T (cj), T (cj+1)?, j ? [?2, 1] ? p0
C5 ?T (c?1), T (c1)? ? p0
C6 ?c0, T (c0)? ? p0
Table 2: Unigram features.
4.4 Features
This section discusses the structure of f(x, y). We
broadly classify features into two categories: uni-
gram and bigram features. We design our feature
templates to capture various levels of information
about words and POS tags. Let us introduce some
notation. We write w?1 and w0 for the surface
forms of words, where subscripts ?1 and 0 in-
dicate the previous and current positions, respec-
tively. POS tags p?1 and p0 can be interpreted in
the same way. We denote the characters by cj .
Unigram features: Table 2 shows our unigram
features. Templates W0?W3 are basic word-level
unigram features, where Length(w0) denotes the
length of the word w0. Using just the surface
forms can overfit the training data and lead to poor
predictions on the test data. To alleviate this prob-
lem, we use two generalized features of the sur-
face forms. The first is the beginning and end
characters of the surface (A0?A7). For example,
?AB(w0)? denotes the beginning character of the
current word w0, and ?AB(w0), AE(w0)? denotes
the beginning and end characters in the word. The
second is the types of beginning and end charac-
ters of the surface (T0?T7). We define a set of
general character types, as shown in Table 4.
Templates C0?C6 are basic character-level un-
ID Template Condition
B0 ?w?1, w0? if w?1 and w0
B1 ?p?1, p0? are word-level
B2 ?w?1, p0? nodes
B3 ?p?1, w0?
B4 ?w?1, w0, p0?
B5 ?p?1, w0, p0?
B6 ?w?1, p?1, w0?
B7 ?w?1, p?1, p0?
B8 ?w?1, p?1, w0, p0?
B9 ?Length(w?1), p0?
TB0 ?TE(w?1)?
TB1 ?TE(w?1), p0?
TB2 ?TE(w?1), p?1, p0?
TB3 ?TE(w?1), TB(w0)?
TB4 ?TE(w?1), TB(w0), p0?
TB5 ?TE(w?1), p?1, TB(w0)?
TB6 ?TE(w?1), p?1, TB(w0), p0?
CB0 ?p?1, p0? otherwise
Table 3: Bigram features.
Character type Description
Space Space
Numeral Arabic and Chinese numerals
Symbol Symbols
Alphabet Alphabets
Chinese Chinese characters
Other Others
Table 4: Character types.
igram features taken from (Nakagawa, 2004).
These templates operate over a window of ?2
characters. The features include characters (C0),
pairs of characters (C1?C2), character types (C3),
and pairs of character types (C4?C5). In addi-
tion, we add pairs of characters and character types
(C6).
Bigram features: Table 3 shows our bigram
features. Templates B0-B9 are basic word-
level bigram features. These features aim to
capture all the possible combinations of word
and POS bigrams. Templates TB0-TB6 are the
types of characters for bigrams. For example,
?TE(w?1), TB(w0)? captures the change of char-
acter types from the end character in the previ-
ous word to the beginning character in the current
word.
Note that if one of the adjacent nodes is a
character-level node, we use the template CB0 that
represents POS bigrams. In our preliminary ex-
periments, we found that if we add more features
to non-word-level bigrams, the number of features
grows rapidly due to the dense connections be-
tween non-word-level nodes. However, these fea-
tures only slightly improve performance over us-
ing simple POS bigrams.
517
(a) Experiments on small training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270 3,046 75,169
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 32
OOV (word) 0.0987 (790/8,008)
OOV (word & POS) 0.1140 (913/8,008)
(b) Experiments on large training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270, 18,089 493,939
400-931,
1001-1151
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 35
OOV (word) 0.0347 (278/8,008)
OOV (word & POS) 0.0420 (336/8,008)
Table 5: Training, development, and test data
statistics on CTB 5.0 used in our experiments.
5 Experiments
5.1 Data sets
Previous studies on joint Chinese word segmen-
tation and POS tagging have used Penn Chinese
Treebank (CTB) (Xia et al, 2000) in experiments.
However, versions of CTB and experimental set-
tings vary across different studies.
In this paper, we used CTB 5.0 (LDC2005T01)
as our main corpus, defined the training, develop-
ment and test sets according to (Jiang et al, 2008a;
Jiang et al, 2008b), and designed our experiments
to explore the impact of the training corpus size on
our approach. Table 5 provides the statistics of our
experimental settings on the small and large train-
ing data. The out-of-vocabulary (OOV) is defined
as tokens in the test set that are not in the train-
ing set (Sproat and Emerson, 2003). Note that the
development set was only used for evaluating the
trained model to obtain the optimal values of tun-
able parameters.
5.2 Evaluation
We evaluated both word segmentation (Seg) and
joint word segmentation and POS tagging (Seg
& Tag). We used recall (R), precision (P ), and
F1 as evaluation metrics. Following (Sproat and
Emerson, 2003), we also measured the recall on
OOV (ROOV) tokens and in-vocabulary (RIV) to-
kens. These performance measures can be calcu-
lated as follows:
Recall (R) = # of correct tokens# of tokens in test data
Precision (P ) = # of correct tokens# of tokens in system output
F1 = 2 ?R ? PR+ P
ROOV = # of correct OOV tokens# of OOV tokens in test data
RIV = # of correct IV tokens# of IV tokens in test data
For Seg, a token is considered to be a cor-
rect one if the word boundary is correctly iden-
tified. For Seg & Tag, both the word boundary and
its POS tag have to be correctly identified to be
counted as a correct token.
5.3 Parameter estimation
Our model has three tunable parameters: the num-
ber of training iterations N ; the number of top
k-best paths; and the threshold r for infrequent
words. Since we were interested in finding an
optimal combination of word-level and character-
level nodes for training, we focused on tuning r.
We fixed N = 10 and k = 5 for all experiments.
For the baseline policy, we varied r in the range
of [1, 5] and found that setting r = 3 yielded the
best performance on the development set for both
the small and large training corpus experiments.
For the error-driven policy, we collected unidenti-
fied unknown words using 10-fold cross validation
on the training set, as previously described in Sec-
tion 3.
5.4 Impact of policies for correct path
selection
Table 6 shows the results of our word-character
hybrid model using the error-driven and baseline
policies. The third and fourth columns indicate the
numbers of known and artificial unknown words
in the training phase. The total number of words
is the same, but the different policies yield differ-
ent balances between the known and artificial un-
known words for learning the hybrid model. Op-
timal balances were selected using the develop-
ment set. The error-driven policy provides addi-
tional artificial unknown words in the training set.
The error-driven policy can improve ROOV as well
as maintain good RIV, resulting in overall F1 im-
provements.
518
(a) Experiments on small training corpus
# of words in training (75,169)
Eval type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 63,997 11,172 0.9587 0.9509 0.9548 0.7557 0.9809baseline 64,999 10,170 0.9572 0.9489 0.9530 0.7304 0.9820
Seg & Tag error-driven 63,997 11,172 0.8929 0.8857 0.8892 0.5444 0.9377baseline 64,999 10,170 0.8897 0.8820 0.8859 0.5246 0.9367
(b) Experiments on large training corpus
# of words in training (493,939)
Eval Type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 442,423 51,516 0.9829 0.9746 0.9787 0.7698 0.9906baseline 449,679 44,260 0.9821 0.9736 0.9779 0.7590 0.9902
Seg & Tag error-driven 442,423 51,516 0.9407 0.9328 0.9367 0.5982 0.9557baseline 449,679 44,260 0.9401 0.9319 0.9360 0.5952 0.9552
Table 6: Results of our word-character hybrid model using error-driven and baseline policies.
Method Seg Seg & Tag
Ours (error-driven) 0.9787 0.9367
Ours (baseline) 0.9779 0.9360
Jiang08a 0.9785 0.9341
Jiang08b 0.9774 0.9337
N&U07 0.9783 0.9332
Table 7: Comparison of F1 results with previous
studies on CTB 5.0.
Seg Seg & Tag
N&U07 Z&C08 Ours N&U07 Z&C08 Ours
Trial (base.) (base.)
1 0.9701 0.9721 0.9732 0.9262 0.9346 0.9358
2 0.9738 0.9762 0.9752 0.9318 0.9385 0.9380
3 0.9571 0.9594 0.9578 0.9023 0.9086 0.9067
4 0.9629 0.9592 0.9655 0.9132 0.9160 0.9223
5 0.9597 0.9606 0.9617 0.9132 0.9172 0.9187
6 0.9473 0.9456 0.9460 0.8823 0.8883 0.8885
7 0.9528 0.9500 0.9562 0.9003 0.9051 0.9076
8 0.9519 0.9512 0.9528 0.9002 0.9030 0.9062
9 0.9566 0.9479 0.9575 0.8996 0.9033 0.9052
10 0.9631 0.9645 0.9659 0.9154 0.9196 0.9225
Avg. 0.9595 0.9590 0.9611 0.9085 0.9134 0.9152
Table 8: Comparison of F1 results of our baseline
model with Nakagawa and Uchimoto (2007) and
Zhang and Clark (2008) on CTB 3.0.
Method Seg Seg & Tag
Ours (baseline) 0.9611 0.9152
Z&C08 0.9590 0.9134
N&U07 0.9595 0.9085
N&L04 0.9520 -
Table 9: Comparison of averaged F1 results (by
10-fold cross validation) with previous studies on
CTB 3.0.
5.5 Comparison with best prior approaches
In this section, we attempt to make meaning-
ful comparison with the best prior approaches re-
ported in the literature. Although most previous
studies used CTB, their versions of CTB and ex-
perimental settings are different, which compli-
cates comparison.
Ng and Low (2004) (N&L04) used CTB 3.0.
However, they just showed POS tagging results
on a per character basis, not on a per word basis.
Zhang and Clark (2008) (Z&C08) generated CTB
3.0 from CTB 4.0. Jiang et al (2008a; 2008b)
(Jiang08a, Jiang08b) used CTB 5.0. Shi and
Wang (2007) used CTB that was distributed in the
SIGHAN Bakeoff. Besides CTB, they also used
HowNet (Dong and Dong, 2006) to obtain seman-
tic class features. Zhang and Clark (2008) indi-
cated that their results cannot directly compare to
the results of Shi and Wang (2007) due to different
experimental settings.
We decided to follow the experimental settings
of Jiang et al (2008a; 2008b) on CTB 5.0 and
Zhang and Clark (2008) on CTB 4.0 since they
reported the best performances on joint word seg-
mentation and POS tagging using the training ma-
terials only derived from the corpora. The perfor-
mance scores of previous studies are directly taken
from their papers. We also conducted experiments
using the system implemented by Nakagawa and
Uchimoto (2007) (N&U07) for comparison.
Our experiment on the large training corpus is
identical to that of Jiang et al (Jiang et al, 2008a;
Jiang et al, 2008b). Table 7 compares the F1 re-
sults with previous studies on CTB 5.0. The result
of our error-driven model is superior to previous
reported results for both Seg and Seg & Tag, and
the result of our baseline model compares favor-
ably to the others.
Following Zhang and Clark (2008), we first
generated CTB 3.0 from CTB 4.0 using sentence
IDs 1?10364. We then divided CTB 3.0 into
ten equal sets and conducted 10-fold cross vali-
519
dation. Unfortunately, Zhang and Clark?s exper-
imental setting did not allow us to use our error-
driven policy since performing 10-fold cross val-
idation again on each main cross validation trial
is computationally too expensive. Therefore, we
used our baseline policy in this setting and fixed
r = 3 for all cross validation runs. Table 8 com-
pares the F1 results of our baseline model with
Nakagawa and Uchimoto (2007) and Zhang and
Clark (2008) on CTB 3.0. Table 9 shows a sum-
mary of averaged F1 results on CTB 3.0. Our
baseline model outperforms all prior approaches
for both Seg and Seg & Tag, and we hope that
our error-driven model can further improve perfor-
mance.
6 Related work
In this section, we discuss related approaches
based on several aspects of learning algorithms
and search space representation methods. Max-
imum entropy models are widely used for word
segmentation and POS tagging tasks (Uchimoto
et al, 2001; Ng and Low, 2004; Nakagawa,
2004; Nakagawa and Uchimoto, 2007) since they
only need moderate training times while they pro-
vide reasonable performance. Conditional random
fields (CRFs) (Lafferty et al, 2001) further im-
prove the performance (Kudo et al, 2004; Shi
and Wang, 2007) by performing whole-sequence
normalization to avoid label-bias and length-bias
problems. However, CRF-based algorithms typ-
ically require longer training times, and we ob-
served an infeasible convergence time for our hy-
brid model.
Online learning has recently gained popularity
for many NLP tasks since it performs comparably
or better than batch learning using shorter train-
ing times (McDonald, 2006). For example, a per-
ceptron algorithm is used for joint Chinese word
segmentation and POS tagging (Zhang and Clark,
2008; Jiang et al, 2008a; Jiang et al, 2008b).
Another potential algorithm is MIRA, which in-
tegrates the notion of the large-margin classifier
(Crammer, 2004). In this paper, we first intro-
duce MIRA to joint word segmentation and POS
tagging and show very encouraging results. With
regard to error-driven learning, Brill (1995) pro-
posed a transformation-based approach that ac-
quires a set of error-correcting rules by comparing
the outputs of an initial tagger with the correct an-
notations on a training corpus. Our approach does
not learn the error-correcting rules. We only aim to
capture the characteristics of unknown words and
augment their representatives.
As for search space representation, Ng and
Low (2004) found that for Chinese, the character-
based model yields better results than the word-
based model. Nakagawa and Uchimoto (2007)
provided empirical evidence that the character-
based model is not always better than the word-
based model. They proposed a hybrid approach
that exploits both the word-based and character-
based models. Our approach overcomes the limi-
tation of the original hybrid model by a discrimi-
native online learning algorithm for training.
7 Conclusion
In this paper, we presented a discriminative word-
character hybrid model for joint Chinese word
segmentation and POS tagging. Our approach
has two important advantages. The first is ro-
bust search space representation based on a hy-
brid model in which word-level and character-
level nodes are used to identify known and un-
known words, respectively. We introduced a sim-
ple scheme based on the error-driven concept to
effectively learn the characteristics of known and
unknown words from the training corpus. The sec-
ond is a discriminative online learning algorithm
based on MIRA that enables us to incorporate ar-
bitrary features to our hybrid model. Based on ex-
tensive comparisons, we showed that our approach
is superior to the existing approaches reported in
the literature. In future work, we plan to apply
our framework to other Asian languages, includ-
ing Thai and Japanese.
Acknowledgments
We would like to thank Tetsuji Nakagawa for his
helpful suggestions about the word-character hy-
brid model, Chen Wenliang for his technical assis-
tance with the Chinese processing, and the anony-
mous reviewers for their insightful comments.
References
Masayuki Asahara. 2003. Corpus-based Japanese
morphological analysis. Nara Institute of Science
and Technology, Doctor?s Thesis.
Harald Baayen and Richard Sproat. 1996. Estimat-
ing lexical priors for low-frequency morphologi-
cally ambiguous forms. Computational Linguistics,
22(2):155?166.
520
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Koby Crammer, Ryan McDonald, and Fernando
Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. In NIPS Workshop
on Learning With Structured Outputs.
Koby Crammer. 2004. Online Learning of Com-
plex Categorial Problems. Hebrew Univeristy of
Jerusalem, PhD Thesis.
Zhendong Dong and Qiang Dong. 2006. Hownet and
the Computation of Meaning. World Scientific.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/umass/chop biocreative ii systems. In Pro-
ceedings of the Second BioCreative Challenge Eval-
uation Workshop.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ryan McDonald, Femando Pereira, Kiril Ribarow, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523?530.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
University of Pennsylvania, PhD Thesis.
Masaki Nagata. 1994. A stochastic japanese mor-
phological analyzer using a forward-DP backward-
A* n-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201?207.
Masaki Nagata. 1999. A part of speech estimation
method for japanese unknown words using a statis-
tical model of morphology and context. In Proceed-
ings of ACL, pages 277?284.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sions.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level
information. In Proceedings of COLING, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, pages 277?284.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of IJ-
CAI.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing, pages 133?143.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of japanese using maximum entropy
aided by a dictionary. In Proceedings of EMNLP,
pages 91?99.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu dong Chiou, and
Shizhe Huang. 2000. Developing guidelines and
ensuring consistency for chinese text annotation. In
Proceedings of LREC.
Stavros A. Zenios Yair Censor. 1997. Parallel Op-
timization: Theory, Algorithms, and Applications.
Oxford University Press.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging on a single perceptron. In
Proceedings of ACL.
521
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73?83,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chen??, Jun?ichi Kazama?, Min Zhang?, Yoshimasa Tsuruoka??,
Yujie Zhang??, Yiou Wang?, Kentaro Torisawa? and Haizhou Li?
?Human Language Technology, Institute for Infocomm Research, Singapore
?National Institute of Information and Communications Technology (NICT), Japan
?School of Information Science, JAIST, Japan
?Beijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al, 2009; Zhao et al, 2009; Chen
et al, 2010). In bitext parsing, we can use the in-
formation based on ?bilingual constraints? (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al, 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
? We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al, 2009; Chen et al, 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
? We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al
(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
? Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
? We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!!
ROOT H hi hl d d h l f h f i h P Li!! e! g y!commen e !t e!resu ts!!o !!!t e!con erence!!!!w t !! eng
(a)
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!(b)Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle ??(de)/nominalizer? is placed after the
verb compound ???(peiyu)??(qilai)/cultivate?
to modify ???(jiqiao)/skill?. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of ???(jiqiao)/skill?.
The head may be ???(fahui)/demonstrate? or ??
?(peiyu)/cultivate?, as shown in Figure 2-(b) and
-(c), where (b) is correct.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiaoPN!!!!!!!VV!!!!!!!!!DT!!!!!!!!!!!!!!!NN!!!!!!!!!!!!!!!AD!!!!!!!!!!!!!!VV!!!!!!AD!!!!!!!VV!!!!VV DEC NN!!!!CC!!!NN(a)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(b)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(c)Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word ?that? is
a clue indicating the relative clause which shows the
relation between ?skill? and ?cultivate?, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word ?skill? corresponding to ???(jiqiao)/skill?
depends on the word ?demonstrate? corresponding
to ???(fahui)/demonstrate?, while the word ?cul-
tivate? corresponding to ???(peiyu)/cultivate? is a
grandchild of ?skill?. This is a positive evidence for
supporting ???(fahui)/demonstrate? as being the
head of ???(jiqiao)/skill?.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
He!hoped!that!all!the!athletes!would!!fully!demonstrate!the!strength!and!skill!that!they!cultivate!daily
Figure 3: Example of human translation
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word ?skills? corresponding
to ???(jiqiao)/skill? is a grandchild of the word
?play? corresponding to ???(fahui)/demonstrate?.
This is a positive evidence for supporting ??
?(fahui)/demonstrate? as being the head of ??
?(jiqiao)/skill?.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x, y) =
?
g?y
score(w, x, g) =
?
g?y
w ?f(x, g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x, g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the first- and second- order
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al, 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank.?
4.2 Bilingual constraint functions
In this paper, we focus on the first- and second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word ??
?(quanti)? depends on ????(yundongyuan)?
and word ?all? corresponding to ???(quanti)? de-
pends on word ?athletes? corresponding to ???
?(yundongyuan)?. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are ??(ta)?
and ???(xiwang)?, this time their corresponding
words ?He? and ?hope? do not have a direct depen-
dency relation. In this case, Fb2(rs2 : rt2)=?.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al, 2003). Figure 6
shows an example. The source word ???(jiqiao)?
depends on ???(fahui)? while its corresponding
word ?skills? indirectly depends on ?play? which
corresponds to ???(fahui)? via ?to?. In this case,
Fb2(rs2 : rt3)=+.
76
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 5: Example of bilingual constraints (2to2)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words ??
?(liliang)?, ??(he)?, and ???(jiqiao)? form a
parent-sibling structure, while their corresponding
words ?strength?, ?and?, and ?skills? also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3 : rt3)=+.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual reordering function: Fro
Huang et al (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [??(huitan), ??(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=?.
4.4 Original bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
?Fro?
?Fb2, Dir? ?Fb3, Dir?
?Fb2, Dir, Fro? ?Fb3, Dir, Fro?
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature ??Fb2(rs2 : rt3)=+, RIGHT ?? for
the second first-order feature template in Table 1.
5 Verified bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified constraint functions
5.1.1 Monolingual target subtrees
Chen et al (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b ht b h b k
ROOT!!He!!!!!bought!!!!!a!!!!book
e!!!!! oug oug t!! oo !
a book b ht b k!!!!!
(a) (b)
oug !!!a!!!!! oo !
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of ?all? and ?athletes? can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF .
5.1.3 Bilingual subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
? ? ? ?? ? ? ? ??ROOT! ?ta shi yi ming xuesheng
ROOT!!He!!!!!is!!!!!a!!!!!student He!!!!!is is!!!!!student
(a) (b)
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=?.
5.2 Verified bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features ?verified bilingual features?.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
??Fb2(rs2 : rt3) = +, Fvt(rt3) = MF,RIGHT ??
for the third first-order feature template and feature
??Fb2(rs2 : rt3)=+, Fvb(rbi23)=?, RIGHT ?? for
the fifth in Table 2.
First-order features Second-order features
?Fro?
?Fb2, Fvt(rtk)? ?Fb3, Fvt(rtk)?
?Fb2, Fvt(rtk), Dir? ?Fb3, Fvt(rtk), Dir?
?Fb2, Fvb(rbink)? ?Fb3, Fvb(rbink)?
?Fb2, Fvb(rbink), Dir? ?Fb3, Fvb(rbink), Dir?
?Fb2, Fro, Fvb(rbink)?
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al, 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool ?Penn2Malt?2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coder?s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al (2009) and Chen et al (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al, 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}?{?(de),?(le)} following the work
of Huang et al (2009).
To train an English parser, we used the PTB
(Marcus et al, 1993) in our experiments and the
tool ?Penn2Malt? to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al, 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al, 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al,
2009; Chen et al, 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemar?s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
U
A
S
Amount of training data (K)
Baseline1PAG1Baseline2PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the first- and second-order baseline systems,
and PAG1 and PAG2 refer to our first- and second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the first- and second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+STs 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI .
To achieve higher performance, we also added the
source subtree features (Chen et al, 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al (2009) and Chen et al (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877?886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21?29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17?24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222?
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49?56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 368?378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Why Question Answering using Sentiment Analysis and Word Classes
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ?
Takuya Kawada? Stijn De Saeger? Jun?ichi Kazama? Yiou Wang??
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? rovellia,? torisawa,? ch,? tkawada,?stijn,? kazama,??wangyiou}@nict.go.jp
Abstract
In this paper we explore the utility of sen-
timent analysis and semantic word classes
for improving why-question answering on a
large-scale web corpus. Our work is moti-
vated by the observation that a why-question
and its answer often follow the pattern that if
something undesirable happens, the reason is
also often something undesirable, and if some-
thing desirable happens, the reason is also of-
ten something desirable. To the best of our
knowledge, this is the first work that intro-
duces sentiment analysis to non-factoid ques-
tion answering. We combine this simple idea
with semantic word classes for ranking an-
swers to why-questions and show that on a set
of 850 why-questions our method gains 15.2%
improvement in precision at the top-1 answer
over a baseline state-of-the-art QA system that
achieved the best performance in a shared task
of Japanese non-factoid QA in NTCIR-6.
1 Introduction
Question Answering (QA) research for factoid ques-
tions has recently achieved great success as demon-
strated by IBM?s Watson at Jeopardy: its accuracy
has been reported to be around 85% on factoid ques-
tions (Ferrucci et al2010). Although recent shared
QA tasks (Voorhees, 2004; Pe?as et al2011; Fuku-
moto et al2007) have stimulated the research com-
munity to move beyond factoid QA, comparatively
little attention has been paid to QA for non-factoid
questions such as why questions and how to ques-
tions, and the performance of the state-of-art non-
factoid QA systems reported in the literature (Mu-
rata et al2007; Surdeanu et al2011; Verberne et
al., 2010) remains considerably lower than that of
factoid QA (i.e., 34% in MRR at top-150 results on
why-questions (Verberne et al2010)).
In this paper we explore the utility of sentiment
analysis (Pang et al2002; Turney, 2002; Nakagawa
et al2010) and semantic word classes for improv-
ing why-question answering (why-QA) on a large-
scale web corpus. The inspiration behind this work
is the observation that why-questions and their an-
swers often have the following tendency:
? if something undesirable happens, the reason is
often also something undesirable, and
? if something desirable happens, its reason is of-
ten also desirable.
Consider the following question Q1, and its an-
swer candidates A1-1 and A1-2.
? Q1: Why does cancer occur?
? A1-1: Carcinogens such as nitrosamine and
benzopyrene may increase the risk of cancer by
altering DNA in cells.
? A1-2: Maintaining a healthy weight may lower
the risk of various types of cancer.
Here A1-1 describes an undesirable event related to
cancer, while A1-2 suggests a desirable action for
its prevention. Our hypothesis suggests that A1-1
is more appropriate for answering Q1. If this hy-
pothesis holds, we can obtain a significant improve-
ment in performance on why-QA tasks by exploiting
the sentiment orientation1 of expressions obtainable
1 In this paper we denote the desirable/undesirable polar-
ity of an expression by the term ?sentiment orientation? instead
of ?semantic orientation? to avoid confusion with our different
notion of ?semantic word classes.?
368
by automatic sentiment analysis of questions and an-
swers.
A second observation motivating this work is that
there are often significant associations between the
lexico-semantic classes of words in a question and
those in its answer sentence. For instance, questions
concerning diseases like Q1 often have answers that
include references to specific semantic word classes
such as chemicals (like A1-1), viruses, body parts,
and so on. Capturing such statistical correlations be-
tween diseases and harmful substances may lead to
higher why-QA performance. For this purpose we
use classes of semantically similar words that were
automatically acquired from a large web corpus us-
ing an EM-based clustering method (Kazama and
Torisawa, 2008).
Another issue is that simply introducing the sen-
timent orientation of words or phrases in question
and answer sentences in a naive way is insufficient,
since answer candidate sentences may contain mul-
tiple sentiment expressions with different polarities
in answer candidates (i.e., about 33% of correct an-
swers had such multiple sentiment expressions with
different polarities in our test set). For example, if
A1-2 contained a second sentiment expression with
negative polarity like the example below,
?Trusting a specific food is not effective
for preventing cancer, but maintaining a
healthy weight may help lower the risk of
various types of cancer.?
both A1-1 and A1-2 would contain sentiment ex-
pressions with the same polarity as that of Q1. Thus,
it is difficult to expect that the sentiment orientation
alone will work well for recognizing A1-1 as a cor-
rect answer to Q1. To address this problem, we con-
sider the combination of sentiment polarity and the
contents of sentiment expressions associated with
the polarity in questions and their answer candidates
as well. To deal with the data sparseness problem
arising in using the content of sentiment expressions,
we developed a feature set that combines the polar-
ity and the semantic word classes effectively.
We exploit these two main ideas (concerned with
the sentiment orientation and the semantic classes
described so far) for training a supervised classi-
fier to rank answer candidates to why-questions.
Through a series of experiments on 850 Japanese
why-questions, we showed that the proposed seman-
tic features were effective in identifying correct an-
swers, and our proposed method obtained more than
15% improvement in precision of its top answer
(P@1) over our baseline, which achieved the best
performance in the non-factoid QA task in NTCIR-
6 (Murata et al2007). We also show that our
method can potentially perform with high precision
(64.8% in P@1) when answer candidates containing
at least one correct answer are given to our re-ranker.
2 Approach
Our proposed method is composed of answer re-
trieval and answer re-ranking. The first step, an-
swer retrieval, extracts a set of answer candidates to
a why-question from 600 million Japanese Web cor-
pus. The answer retrieval is our implementation of
the state-of-art method that has shown the best per-
formance in the shared task of Japanese non-factoid
QA in NTCIR-6 (Murata et al2007; Fukumoto et
al., 2007). The second step, answer re-ranking, is
the focus of this work.
2.1 Answer Retrieval
We use Solr2 to retrieve documents from a 600 mil-
lion Japanese Web page corpus3for a given why-
question. Let a set of content words in a why-
question be T = {t1, ? ? ? , tn}. Two boolean queries
for a why-question, ?t1 AND ? ? ? AND tn? and ?t1
OR ? ? ? OR tn,? are given to Solr and top-300 doc-
uments for each query are retrieved. Note that re-
trieved documents by each query have different cov-
erage and relevance to a given why-question. To
keep balance between the coverage and relevance of
retrieved documents, we use a set of retrieved doc-
uments by these two queries for obtaining answer
candidates. Each document in the result of docu-
ment retrieval is split into a set of answer candi-
dates consisting of five subsequent sentences4. Sub-
sequent answer candidates can share up to two sen-
tences to avoid errors due to wrong document seg-
mentation.
2 http://lucene.apache.org/solr
3 To the best of our knowledge, few Japanese non-factoid
QA systems in the literature have used such a large-scale cor-
pus.
4 The length of acceptable answer candidates for why-
QA in the literature ranges from one sentence to two para-
graphs (Fukumoto et al2007; Murata et al2007; Higashinaka
and Isozaki, 2008; Verberne et al2007; Verberne et al2010).
369
Answer candidate ac for question q is ranked
according to scoring function S(q, ac) given in
Eq. (1) (Murata et al2007). Murata et al2007)?s
method uses text search to look for answer candi-
dates containing terms from the question with ad-
ditional clue terms referring to ?reason? or ?cause.?
Following the original method we used riyuu (rea-
son), genin (cause) and youin (cause) as clue terms.
The top-20 answer candidates for each question are
passed on to the next step, which is answer re-
ranking. S(q, ac) assigns a score to answer candi-
dates like tf -idf , where 1/dist(t1, t2) functions like
tf and 1/df(t2) is idf for given terms t1 and t2 that
are shared by q and ac.
S(q, ac) = maxt1?T
?
t2?T
?? log(ts(t1, t2)) (1)
ts(t1, t2) =
N
2? dist(t1, t2)? df(t2)
Here T is a set of terms including nouns, verbs, and
adjectives in question q that appear in answer can-
didate ac. Note that the clue terms are added to T
if they exist in ac. N is the total number of docu-
ments (600 million), dist(t1, t2) represents the dis-
tance (the number of characters) between t1 and t2
in answer candidate ac, df(t) is the document fre-
quency of term t, and ? ? {0, 1} is an indicator,
where ? = 1 if ts(t1, t2) > 1, ? = 0 otherwise.
2.2 Answer Re-ranking
Our re-ranker is a supervised classifier (SVMs)
(Vapnik, 1995) that uses three types of feature
sets: features expressing morphological and syn-
tactic analysis of questions and answer candidates,
features representing semantic word classes appear-
ing in questions and answer candidates, and features
from sentiment analysis. All answer candidates of a
question are ranked in a descending order of their
score given by SVMs. We trained and tested the
re-ranker using 10-fold cross validation on a cor-
pus composed of 850 why-questions and their top-
20 answer candidates provided by the answer re-
trieval procedure in Section 2.1. The answer candi-
dates were manually annotated by three human an-
notators (not by the authors). Our corpus construc-
tion method is described in more detail in Section 4.
3 Features for Answer Re-ranking
This section describes our feature sets for answer
re-ranking: features expressing morphological and
syntactic analysis (MSA), features representing se-
mantic word class (SWC), and features indicat-
ing sentiment analysis (SA). MSA, which has been
widely used for re-ranking answers in the literature,
is used to identify associations between questions
and answers at the morpheme, word phrase, and syn-
tactic dependency levels. The other two feature sets
are proposed in this paper. SWC is devised for iden-
tifying semantic word class associations between
questions and answers. SA is used for identify-
ing sentiment orientation associations between ques-
tions and answers as well as expressing the combi-
nation of each sentiment expression and its polarity.
Table 1 summarizes the respective feature sets, each
of which is described in detail below.
3.1 Morphological and Syntactic Analysis
MSA including n-grams of morphemes, words, and
syntactic dependencies has been widely used for re-
ranking answers in non-factoid QA (Higashinaka
and Isozaki, 2008; Surdeanu et al2011; Verberne
et al2007; Verberne et al2010). We use MSA as
a baseline feature set in this work.
We represent all sentences in a question and
its answer candidate in three ways: morphemes,
word phrases (bunsetsu5) and syntactic dependency
chains. These are obtained using a morphological
analyzer6 and a dependency parser7. From each
question and answer candidate we extract n-grams
of morphemes, word phrases, and syntactic depen-
dencies, where n ranges from 1 to 3. Syntactic de-
pendency n-grams are defined as a syntactic depen-
dency chain containing n word phrases. Syntactic
dependency 1-grams coincide with word phrase 1-
grams, so they are ignored.
Table 1 defines four types of MSA (MSA1 to
MSA4). MSA1 is n-gram features from all sen-
tences in a question and its answer candidates and
distinguishes an n-gram feature found in a ques-
tion from that same feature found in answer candi-
dates. MSA2 contains n-grams found in the answer
5 A bunsetsu is a syntactic constituent composed of a content
word and several function words such as post-positions and case
markers. It is the smallest unit of syntactic analysis in Japanese.
6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
7 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
370
MSA1 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in a question and its answer candidate, where n ranges
from 1 to 3. n-grams in a question and those in an answer candidate are distinguished.
MSA2 MSA1?s n-grams in an answer candidate that contain a question term.
MSA3 MSA1?s n-grams that contain a clue term including riyuu (reason), genin (cause) and youin (cause). These n-grams in a question and
those in an answer candidate are distinguished.
MSA4 The ratio of the number of question terms in an answer candidate to the total number of question terms.
SWC1 Word class n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are distin-
guished.
SWC2 SWC1?s n-grams in an answer candidate whose original MSA1?s n-grams contain any question term.
SA@W1 Word polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W2 SA@W1?s n-grams in an answer candidate whose original MSA1 n-grams contain any question term.
SA@W3 Joint class-polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W4 SA@W3?s n-grams in an answer candidates whose original MSA1 n-grams contain any question term.
SA@P1 The indicator for polarity agreement between sentiment phrases, one in a question and the other in an answer candidate: 1 if any pair of
such sentiment phrases has polarity in agreement, 0 otherwise.
SA@P2 The phrase-polarity, positive or negative, of a pair of sentiment phrases for which the indicator in SA@P1 is 1.
SA@P3 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in sentiment phrases are coupled with their phrase-polarity,
where n ranges from 1 to 3. These n-grams in a question and those in an answer candidate are distinguished.
SA@P4 SA@P3?s n-grams in an answer candidates that contain a question term.
SA@P5 The ratio of the number of question terms in sentences that have sentiment phrases in answer candidates to the total number of question
terms.
SA@P6 Word class n-grams in sentiment phrases are coupled with phrase-polarity. These n-grams in a question and those in an answer candidate
are distinguished.
SA@P7 SA@P6?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P8 Joint class-polarity n-grams in sentiment phrases of a question and its answer candidate are coupled with phrase-polarity of the sentiment
phrases. These n-grams in a question and those in an answer candidate are distinguished.
SA@P9 SA@P8?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P10 A pair of SA@P6?s n-grams, one from sentiment phrases in a question and the other from those in an answer candidate, where the two
sentiment phrases should have the same sentiment orientation.
Table 1: Features used in training our re-ranker
candidates that themselves contain a term from the
question (e.g., ?types of cancer? in example A1-2).
MSA3 is the n-gram feature that contains one of the
clue terms used for answer retrieval (riyuu (reason),
genin (cause) or youin (cause)). Here too, n-grams
obtained from the questions and answer candidates
are distinguished. Finally, MSA4 is the percentage
of the question terms found in an answer candidate.
3.2 Semantic Word Class
Semantic word classes are sets of semantically simi-
lar words. We construct these semantic word classes
by using the noun clustering algorithm proposed in
Kazama and Torisawa (2008). The algorithm fol-
lows the distributional hypothesis, which states that
semantically similar words tend to appear in simi-
lar contexts (Harris, 1954). By treating syntactic de-
pendency relations between words as ?contexts,? the
clustering method defines a probabilistic model of
noun-verb dependencies with hidden classes as:
p(n, v, r) =
?
c
p(n|c)p(?v, r?|c)p(c) (2)
Here, n is a noun, v is a verb or noun on which n de-
pends via a grammatical relation r (post-positions in
Japanese), and c is a hidden class. Dependency rela-
tion frequencies were obtained from our 600-million
page web corpus, and model parameters p(n|c),
p(?v, r?|c) and p(c) were estimated using the EM
algorithm (Hofmann, 1999). We successfully clus-
tered 5.5 million nouns into 500 classes. For each
noun n, EM clustering estimates a probability dis-
tribution over hidden variables representing seman-
tic classes. From this distribution we obtained dis-
crete semantic word classes by assigning each noun
n to semantic class c = argmaxc? p(c?|n). The
resulting classes actually form clean semantic cat-
egories such as chemicals, nutrients, diseases and
conditions, in our examples of Q1 and Q2. The fol-
lowing are the top-10 words (English translation) ac-
cording to p(c|n) for these classes.
chemicals: acetylene, hydrogenation product,
phosphoric monoester, methacrylate, levoglu-
cosan, ammonium salt, halogenated organic
compound, benzonitrile, alkyne, nitrosamine
371
nutrients: glucide, carbonhydrate, mineral, salt,
sugar, water, fat, vitamin, nutrients, protein
diseases: pneumonia, neuritis, cancer, oral leuko-
plakia, pachymeningitis, acidosis, encephalitis,
abdominal injury, valvulitis, gingivitis
conditions: proficiency, decrepitude, deficiency,
impurity, abnormalities, floated, crisis, dis-
placement, condition, shortage
Semantic word class (SWC) features are used to
capture associations between semantic classes of
words in the question and those in the answer candi-
dates. For example:
? Q2: Why does rickets (Wdisease) occur in chil-
dren?
? A2: Deficiency (Wcondition) of vitamin D
(Wnutrients) can cause rickets (Wdisease).
Wcondition, Wdisease and Wnutrients represent se-
mantic word classes of conditions, diseases and nu-
trients, respectively. If this question-answer pair is
given to the classifier as a positive training sample,
we expect it to learn that if a disease name appears
in a question then, everything else being equal, an-
swers including nutrient names are more likely to be
correct. Note that in principle the same association
could be learned between word pairs, i.e., rickets and
vitamin D. However, we found that word level asso-
ciations are often too specific, and because of data
sparseness this knowledge cannot easily be general-
ized to unseen questions. This is our main motiva-
tion for introducing broad coverage semantic word
classes into the feature set.
We call the feature set with the word classes SWC
and use two types of SWC, as shown in Table 1. To
obtain the first type (SWC1), we convert all nouns
in the MSA1 n-grams into their respective word
classes, and keep all n-grams that contain at least
one word class. We call these features word class
n-grams. Again, word class n-grams obtained from
questions are distinguished from the ones in answer
candidates. For example, we extract ?Wdisease oc-
cur? as a word class 2-gram from Q2.
The second type of SWC, SWC2, represents word
class n-grams in an answer candidate, in which
question terms are replaced by their respective se-
mantic word classes. For example, Wdisease in word
class 2-gram ?cause Wdisease? from A2 is the se-
mantic word class of rickets, one of the question
terms. These features capture the correspondence
between semantic word classes in the question and
answer candidates.
3.3 Sentiment Analysis
Sentiment analysis (SA) features are classified into
word-polarity and phrase-polarity features. We use
opinion extraction tool8 and sentiment orientation
lexicon in the tool for these features.
3.3.1 Opinion Extraction Tool
Opinion extraction tool is a software, the im-
plementation of Nakagawa et al2010). It ex-
tracts linguistic expressions representing opinions
(henceforth, we call them sentiment phrases) from
a Japanese sentence and then identifies the polarity
of these sentiment phrases using machine learning
techniques. For example, rickets occur in Q2 and
Deficiency of vitamin D can cause rickets in A2 can
be identified as sentiment phrases with a negative
polarity. The tool identifies sentiment phrases and
their polarity by using polarities of words and de-
pendency subtrees as evidence, where these polari-
ties are given in a word polarity dictionary.
In this paper, we use a trained model and a word
polarity dictionary (containing about 35,000 entries)
distributed via the ALAGIN forum9 for our sen-
timent analysis. Table 2 shows the performance
of opinion extraction tool, precision (P), recall (R)
and F-value (F), in this setting (reported in the
Japanese homepage of this tool). In the evaluation of
sentiment-phrase extraction, an extracted sentiment
phrase is determined as correct if its head word is
the same as one in the gold standard. Polarity clas-
sification is evaluated under the condition that all of
the sentiment phrases are correctly extracted.
P R F
Sentiment-phrase extraction 0.602 0.408 0.486
Polarity classification (pos.) 0.873 0.893 0.883
Polarity classification (neg.) 0.866 0.842 0.854
Table 2: The performance of opinion extraction tool
3.3.2 Word Polarity (SA@W)
Polarities of words are identified by simply look-
ing up the word polarity dictionary of opinion ex-
8 Available at http://alaginrc.nict.go.jp/opinion/index_e.html
9 http://www.alagin.jp/index-e.html. Only the members of
the ALAGIN forum can access these resources.
372
traction tool. Word polarity features are used
for identifying associations between the polarity of
words in a question and that in a correct answer. For
example:
? Q2: Why does rickets (W?) occur in children?
? A2: Deficiency (W?) of vitamin D can cause
rickets (W?).
Here, W? represents negative word polarities. We
expect our classifier to learn from this question and
answer pair that if a word with negative polarity ap-
pears in a question then its correct answer is likely
to contain a negative polarity word as well.
SA@W1 and SA@W2 in Table 1 are sentiment
analysis features from word polarity n-grams, which
contain at least one word that has word polarities.
We obtain these n-grams by converting all nouns in
MSA1 n-grams into their word polarities through
dictionary lookup. For example, from Q2 in the
above example we extract ?W? occur? as a word
polarity 2-gram. SA@W1 is concerned with all
word polarity n-grams in questions and answer can-
didates. For SA@W2, we restrict word polarity
n-grams from SA@W1 to those whose original n-
gram include a question term.
Furthermore, word polarities are coupled with se-
mantic word classes so that our classifier can iden-
tify meaningful combinations of both. For example,
deficiency in A2 can be represented asW?condition by
its respective semantic word class and word polar-
ity, which allows for the representation of undesir-
able conditions. This in turn lets our system learn
meaningful correlations between words expressing
these kind of negative conditions and their connec-
tion to questions asking about diseases. SA@W3
and SA@W4 are features from this combination.
They are defined in the same way as SA@W1 and
SA@W2 except that word polarities are replaced
with the combination of semantic word classes and
word polarities. We call n-grams in SA@W3 and
SA@W4 joint (word) class-polarity n-grams.
3.3.3 Phrase Polarity (SA@P)
Opinion extraction tool is applied to question and
its answer candidate to identify sentiment phrases
and their phrase-polarities. In preliminary tests we
found that sentiment phrases do not help to iden-
tify correct answers if answer sentences including
the sentiment phrases do not have any term from the
question. So we restrict the target sentiment phrases
to those acquired from sentences containing at least
one question term. From these sentiment phrases we
extract three categories of features.
First, SA@P1 and SA@P2 are features concerned
with phrase-polarity agreement between sentiment
phrases in a question and its answer candidate. We
consider all possible pairs of sentiment phrases from
the question and answer. If any such pair agrees
in phrase-polarity, an indicator for the agreement
and its polarity in the agreement become features
SA@P1 and SA@P2, respectively.
Secondly, following the original hypothesis un-
derlying this paper, we assume that sentiment
phrases often represent the core part of the cor-
rect answer (e.g., A2 above) and it is important
to express the content of the sentiment phrases in
features. SA@P3 and SA@P4 were devised for
this purpose. SA@P3 represents this sentiment
phrase contents as n-grams of morphemes, words,
and syntactic dependencies of sentiment phrases,
together with their phrase-polarity. Furthermore,
SA@P4 is the subset of SA@P3 n-grams restricted
to those that include terms found in the question,
and SA@P5 indicates the percentage of sentiment
n-grams from the question that are found in a given
answer candidate.
Finally, features SA@P6 through SA@P9 use se-
mantic word classes to generalize the content fea-
tures mentioned above. These features consist of
word class n-grams and joint class-polarity n-grams
taken from sentiment phrases, together with their
phrase polarity. Similar to the definition of SA@P4,
for SA@P7 and SA@P9 we restrict ourselves to n-
grams containing a question term. SA@P10 repre-
sents the semantic content of two sentiment phrases
with the same sentiment orientation (one from a
question and the other from an answer candidate)
using word class n-grams, together with the phrase-
polarity in agreement.
4 Test Set
We prepared three sets of why-questions (QS1, QS2
and QS3) and used these questions to build two test
sets for our experiments.
Why-questions in QS1 are taken from the
Japanese version of Yahoo! Answers (called Ya-
hoo! Chiebukuro)10. We automatically extracted
10 We used ?Yahoo! Chiebukuro Data (2nd edition)? which is
373
questions consisting of a single sentence and con-
taining the interrogative naze (why), and our anno-
tators verified that these questions are meaningful
without further context. For example, they discarded
questions like ?Why doesn?t the WBC (world box-
ing council) make an objection to the WBC (World
baseball classic)?? (the object of the objection is
unclear) and ?Why do minors trade at the auction
even though it is disallowed by the rules? (informa-
tion about which auction is not provided).
Because questions in Yahoo! Answers are aimed
at human readers, users often ?set the stage? by giv-
ing lots of background information about their ques-
tion. This often leads to large stylistic differences
between the questions in Yahoo! Answers and those
typically posed to a QA system. We therefore cre-
ated a second set of why-questions, QS2, whose
style should be more appropriate for a QA system
(examples showing these differences are given in the
supplementary materials of this paper). Six human
annotators (not the authors) were asked to create
why-questions in their own words, keeping in mind
that the questions they create are for a QA system. In
addition, the annotators were asked to verify on the
Web that the questions they created ask about some
real event or phenomena. For example, a question
like ?Why does Mars appear blue?? is disallowed in
QS2 because ?Mars appears blue? is false. Note that
the correct answer to these questions does not have
to be either in our target corpus or in real-world Web
texts. These two sets of why-questions, QS1 and
QS2, are used to build a test set for evaluating our
proposed method.
Finally, QS3 contains why-questions that have at
least one answer in our target corpus (600 million
Japanese Web page corpus). For creating such why-
questions, four human annotators (not the authors)
were given a text passage composed of three contin-
uous sentences and asked to locate the reasons for
some event as described in this passage. Then they
created a why-question for which the description is a
correct answer. Because randomly selected passages
from our target corpus have little chance of generat-
ing good why-questions we extracted passages from
our target corpus that include at least one of the clue
terms used in our answer retrieval step (i.e. riyuu
(reason), genin (cause), or youin (cause)). This set-
provided by Yahoo Japan Corporation and contains 16 million
questions asked from April, 2004 to April 2009.
ting may not necessarily reflect a ?real world? dis-
tribution of why-questions, in which ideally a wide
range of people ask questions that may or may not
have an answer in our corpus. However, QS3 al-
lows us to evaluate our method under the idealized
conditions where we have a perfect answer retrieval
module whose answer candidates always contain at
least one correct answer (the source passage used
for creating the why-question). This setting allows
us to estimate the ideal-case performance of our
method. Under these circumstances we found that
our method achieves almost 65% precision in P@1,
which suggests that it can potentially perform with
high precision if the answer candidates given by the
answer retrieval module contain at least one correct
answer. This is the main purpose of QS3. Addition-
ally, we use QS3 for building training data, to check
whether questions that do not reflect the real-world
distribution of why-questions are useful for improv-
ing the system?s performance on ?real-world? ques-
tions (see Section 5.1).
In addition, we checked QS1, QS2 and QS3 for
questions having the same topic, to avoid the pos-
sibility that the distribution of questions is biased
towards certain topics. We manually extracted the
questions? topic words and randomly selected a sin-
gle representative question from all questions with
the same topic. For example, ?Why does Twitter
only allow 140 characters?? and ?Why is Twitter
so popular?? both have as topic Twitter. In the end
we obtained 250 questions in QS1, 250 questions in
QS2 and 350 questions in QS3.
For evaluation we prepared two test sets, Set1 and
Set2. Set1 contains question-answer pairs whose
questions are taken from QS1 and QS2. In our ex-
periment, we evaluate systems with 10-fold cross
validation on Set1. Set2 has question-answer pairs
whose questions are from QS3. Set2 is mainly used
for estimating estimate the ideal-case performance
of our method with a perfect answer retrieval mod-
ule. Furthermore Set2 is used as additional training
data in evaluating systems with 10-fold cross vali-
dation on Set1. We used our answer retrieval sys-
tem to obtain the top-20 answer candidates for each
question, and all question-answer (candidate) pairs
were checked by three annotators, where their inter-
rater agreement (Fleiss? kappa) was 0.634, indicat-
ing substantial agreement. Finally, correct answers
to each question were determined by majority vote.
374
Q1:???????????????????????????????????????????
(Why does the increase of greenhouse gases such as carbon dioxide in the atmosphere lead to a rise of ocean level?)
A1: .. ????????????????????????????????????????????????????????
??????????????????????????????????????? ... ???????????????????
???????????????????????
(The burning of fossil fuels contributes to the increase of atmospheric concentrations of greenhouse gases and this makes the atmosphere absorb more
thermal radiation. As a result, Earth?s average surface temperature increases. This is global warming. ... There are warnings that the increase of sea
water and melting of polar ice due to the global warming may cause sea-surface height to rise by 9?88 cm on average.
Q2:?????????????????????????????
(Why does hemoglobin deficiency cause lack of oxygen in the human body?)
A2:... ????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????..
(... Hemoglobin has an important role in the human body of carrying oxygen to the organs and transferring carbon dioxide back to the lungs, to be
dispensed from the organism. If the amount of hemoglobin produced by the body is insufficient due to iron deficiency, the amount of oxygen delivered
throughout the body decreases, causing oxygen deficiency. ... )
Table 3: Correct question-answer pairs in our test set
Table 3 shows a sample of correct question-answer
pairs in our test set. Please see the supplementary
materials of this paper for more examples.
Note that word and phrase polarities are not con-
sidered by the annotators in building our test sets
and these polarities are automatically identified us-
ing a word polarity dictionary and opinion extraction
tool. We confirmed that about 35% of questions and
40% of answer candidates had at least one sentiment
phrase by opinion extraction tool, and about 45% of
questions and 85% of answer candidates contained
at least one word having polarity by a word polarity
dictionary.
5 Experiments
We use TinySVM11 with a linear kernel for training
our re-ranker. Evaluation was done by P@1 (Pre-
cision of the top answer) and MAP (Mean Average
Precision). P@1 measures how many questions have
a correct top answer candidate. MAP, widely used in
evaluation of IR systems, measures the overall qual-
ity of the top-n answer candidates (n=20 in this ex-
periment) using the formula:
MAP =
1
|Q|
?
q?Q
?n
k=1(Prec(k)? rel(k))
|Aq|
(3)
Here Q is a set of why-questions, Aq is a set of cor-
rect answers to why-question q ? Q, Prec(k) is the
precision at cut-off k in the top-n answer candidates,
rel(k) is an indicator, 1 if the item at rank k is a cor-
rect answer in Aq, 0 otherwise.
We evaluated all systems using 10-fold cross val-
idation in two ways. In the first setting we per-
formed 10-fold cross validation on Set1. Set1 con-
11 http://chasen.org/?taku/software/TinySVM/
sists of 10,000 question-answer pairs (500 questions
with their 20 answer candidates), and was parti-
tioned into 10 subsamples such that the questions
in one subsample do not overlap with those of the
other subsamples. 9 subsamples (9,000 question-
answer pairs) were used as training data and the
remaining subsample (1,000 question-answer pairs)
was retained as test data. This experiment is called
CV(Set1). It shows the effect of answer re-ranking
when evaluating our proposed method with train-
ing data built with real world why-questions alone.
In the second setting, we used the same 10 sub-
samples of Set1 as in CV(Set1) and exploited Set2
(composed of 7,000 question-answer pairs) as ad-
ditional training data for 10-fold cross validation.
As a result, in each fold 16,000 question-answer
pairs (9,000 from Set1 and 7,000 from Set2) were
used as training data for re-rankers, and all systems
were evaluated on the remaining 1,000 question-
answer pair subsample from Set1. We call this set-
ting CV(Set1+Set2). It verifies whether training
data that does not necessarily reflect a real-world
distribution of why-questions can improve why-QA
performance on real-world questions.
5.1 Results
Table 4 shows the evaluation results of six different
systems. For each system, we represent the perfor-
mance in P@1 and MAP. B-QA is a system of our
answer retrieval and the other five re-rank top-20 an-
swer candidates using their own re-ranker.
B-QA: our answer retrieval system, our implemen-
tation of Murata et al2007).
B-Ranker: a system that has a re-ranker trained
with morphological and syntactic analysis
(MSA) features alone.
375
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
B-QA 0.222 (0.368) 0.270 (0.447) 0.222 (0.368) 0.270 (0.447)
B-Ranker 0.256 (0.424) 0.319 (0.528) 0.274 (0.454) 0.323 (0.535)
B-Ranker+CR 0.262 (0.434) 0.319 (0.528) 0.278 (0.460) 0.325 (0.538)
B-Ranker+WN 0.257 (0.425) 0.320 (0.530) 0.275 (0.455) 0.325 (0.538)
Proposed 0.336 (0.56) 0.377 (0.624) 0.374 (0.619) 0.391 (0.647)
UpperBound 0.604 (1) 0.604 (1) 0.604 (1) 0.604 (1)
Table 4: Comparison of systems
B-Ranker+CR: a system has a re-ranker trained
with our MSA features and the causal relation
(CR) features used in Higashinaka and Isozaki
(2008). The CR features include binary fea-
tures indicating whether an answer candidate
contains a causal relation pattern, which causal
relation pattern the answer candidate has, and
whether the question-answer pair contains a
causal relation instance ? cause in the answer,
effect in the question). We acquired causal
relation instances from our target corpus us-
ing the method from (De Saeger et al2009),
and exploited the top-100,000 causal relation
instances and the patterns that extracted them
for CR features. Note that these CR features
are introduced only for comparing our semantic
features with ones in Higashinaka and Isozaki
(2008) and they are not a part of our method.
B-Ranker+WN: its re-ranker is trained with our
MSA features and the WordNet features in Ver-
berne et al2010). The WordNet features in-
clude the percentage of the question terms and
their synonyms in WordNet synsets found in
an answer candidate and the semantic related-
ness score between a question and its answer
candidate, the average of the concept similar-
ity between each question term and all of the
answer terms by WordNet::Similarity (Peder-
sen et al2004). We used the Japanese Word-
Net 1.1 (Bond et al2009) for these WordNet
features. Note that the Japanese WordNet 1.1
has 93,834 Japanese words linked to 57,238
WordNet synsets, while the English WordNet
3.0 covers 155,287 words linked to 117,659
synsets. Due to this lower coverage, the Word-
Net features in Japanese may have a less power
for finding a correct answer than those in En-
glish used in Verberne et al2010).
Proposed: our proposed method. All of the MSA,
SWC and SA features are used for training our
re-ranker.
UpperBound: a system that ranks all n correct an-
swers as the top n results of the 20 answer can-
didates if there are any. This indicates the per-
formance upperbound in this experiment. The
relative performance of each system compared
to UpperBound is shown in parentheses.
The proposed method achieved the best perfor-
mance both in CV(Set1) and CV(Set1+Set2). Our
method shows a significant improvement (11.4?
15.2% in P@1 and 10.7?12.1% in MAP) over our
answer retrieval method, B-QA. Its improvement
over B-Ranker, B-Ranker+CR and B-Ranker+WN
(7.6?10% in P@1 and 5.7?6.6% in MAP) shows
the effectiveness of our proposed feature set over
the features used in previous works. Both B-
Ranker+CR and B-Ranker+WN did not show signif-
icant performance improvement over B-Ranker. At
least in our setting, the causal relation and WordNet
features did not prove effective. The performance
gap between B-Ranker and B-QA (3.4?5.2% in P@1
and 4.9?5.3% in MAP) suggests the effectiveness
of re-ranking. All systems consistently show better
performance in CV(Set1+Set2) than CV(Set1). This
suggests that training data built with why-questions
that does not reflect real-world distribution of why-
questions is useful in training re-rankers.
We investigate the contribution of each type of
features to the performance by removing one fea-
ture set from the all feature sets in training our re-
ranker. In this experiment, we split SA into SA@W
(features expressing words and their polarity) and
SA@P (features expressing phrases and their po-
larity) to investigate their contribution either. The
results are summarized in Table 5.
In Table 5, MSA+SWC+SA represents our pro-
posed method using all feature sets. The perfor-
mance gap between MSA+SWC+SA and the others
confirms that all the features contributed to a higher
376
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
SWC+SA 0.302 0.324 0.314 0.332
MSA+SWC 0.308 0.349 0.318 0.358
MSA+SA 0.300 0.352 0.314 0.364
MSA+SWC+SA@W 0.312 0.358 0.325 0.365
MSA+SWC+SA@P 0.323 0.369 0.358 0.384
MSA+SWC+SA 0.336 0.377 0.374 0.391
UpperBound 0.604 0.604 0.604 0.604
Table 5: Evaluation with different combination of feature
sets used in training our re-ranker
performance. The significant performance improve-
ment by SA (features from sentiment analysis) and
SWC (features from semantic word classes) (The
gap between MSA+SWC+SA and MSA+SWC was
2.8?6% and that between MSA+SWC+SA and
MSA+SA was 3.6%?6% in P@1) supports the hy-
pothesis for sentiment analysis and semantic word
classes in this paper.
Though the performance gap between
MSA+SWC+SA and MSA+SWC+SA@P
(1.3%?1.6% in P@1) shows that SA@W is
useful in training our re-ranker, we found that
MSA+SWC+SA@W made only 0.4?0.7% im-
provement over MSA+SWC. We believe that this
is mainly because SA@W and SWC are based on
semantic and sentiment information at the word
level, and these often capture a similar type of
information. For instance, disease names that are
grouped together into one class in SWC are typi-
cally classified as negative in SA@W. Therefore the
similarity in the information provided by SA@W
and SWC causes a classifier trained with both of
these features to obtain only a minor improvement
over a classifier using only one of the features.
To estimate the ideal-case performance of our
proposed method, we made another experiment by
using Set1 as training data for our re-ranker and
Set2 as test data for evaluating our proposed method.
Here, we assume a perfect answer retrieval module
that adds the source passage that was used for gener-
ating the original why-question in Set2 as a correct
answer to the set of existing answer candidates, giv-
ing 21 answer candidates. The performance of our
method in this setting was 64.8% in P@1 and 66.6%
in MAP. This evaluation result suggests that our re-
ranker can potentially perform with high precision
when at least one correct answer in answer candi-
dates is given by the answer retrieval module.
6 Related Work
In the QA literature, Higashinaka and Isozaki
(2008), Verberne et al2010), and Surdeanu et al
(2011) are closest to our work. The first two deal
with why-questions, the last with how-questions.
Similar to our method, they use machine learn-
ing techniques to re-rank answer candidates to non-
factoid questions based on various combinations of
syntactic, semantic and other statistical features such
as the density and frequency of question terms in the
answer candidates and patterns for causal relations
in the answer candidates. Especially for why-QA,
Higashinaka and Isozaki (2008) used causal relation
features and Verberne et al2010) exploited Word-
Net features as a kind of semantic features for train-
ing their re-ranker, where we used these features, re-
spectively, for B-Ranker+CR and B-Ranker+WN in
our experiment.
Our work differs from the above approaches in
that we propose semantic word classes and senti-
ment analysis as a new type of semantic features,
and show their usefulness in why-QA. Sentiment
analysis has been used before on the slightly un-
usual task of opinion question answering, where the
system is asked to answer subjective opinion ques-
tions (Stoyanov et al2005; Dang, 2008; Li et al
2009). To the best of our knowledge though, no pre-
vious work has systematically explored the use of
sentiment analysis in a general QA setting beyond
opinion questions.
7 Conclusion
In this paper, we have explored the utility of senti-
ment analysis and semantic word classes for ranking
answer candidates to why-questions. We proposed a
set of semantic features that exploit sentiment anal-
ysis and semantic word classes obtained from large-
scale noun clustering, and used them to train an an-
swer candidate re-ranker. Through a series of exper-
iments on 850 why-questions, we showed that the
proposed semantic features were effective in identi-
fying correct answers, and our proposed method ob-
tained more than 15% improvement in precision of
its top answer (P@1) over our baseline, a state-of-
the-art IR based QA system. We plan to use new se-
mantic knowledge such as semantic orientation, ex-
citatory or inhibitory, proposed in Hashimoto et al
(2012) for improving why-QA.
377
References
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka
Uchimoto, Takayuki Kuribayashi, and Kyoko Kan-
zaki. 2009. Enhancing the japanese wordnet. In Pro-
ceedings of the 7th Workshop on Asian Language Re-
sources, pages 1?8.
Hoa Tran Dang. 2008. Overview of the TAC 2008 opin-
ion question answering and summarization tasks. In
Proc. TAC 2008.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large scale
relation acquisition using class dependent patterns. In
Proc. of ICDM 2009, pages 764?769.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building Watson: An overview of the DeepQA
project. AI Magazine, 31(3):59?79.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th ques-
tion answering challenge (QAC-4) at NTCIR work-
shop 6. In Proc. of NTCIR-6.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excita-
tory or inhibitory: A new semantic orientation extracts
contradiction and causality from the web. In Proceed-
ings of EMNLP-CoNLL 2012.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-questions.
In Proc. of IJCNLP, pages 418?425.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?99, pages 50?57.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL-
08: HLT, pages 407?415.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, pages
737?745.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A system
for answering non-factoid Japanese questions by using
passage retrieval weighted based on type of answer. In
Proc. of NTCIR-6.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 786?794, Los An-
geles, California, June. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41.
Anselmo Pe?as, Eduard H. Hovy, Pamela Forner, ?lvaro
Rodrigo, Richard F. E. Sutcliffe, Corina Forascu, and
Caroline Sporleder. 2011. Overview of QA4MRE at
CLEF 2011: Question answering for machine reading
evaluation. In CLEF.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 923?
930.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 417?424.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Evaluating discourse-based an-
swer extraction for why-question answering. In SIGIR,
pages 735?736.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2010. What is not in the bag of words
for why-QA? Computational Linguistics, 36:229?
245.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
378
