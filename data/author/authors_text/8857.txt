Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 121?124,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Automatically Transcribed Dialogs to Learn User Models in a Spoken
Dialog System
Umar Syed
Department of Computer Science
Princeton University
Princeton, NJ 08540, USA
usyed@cs.princeton.edu
Jason D. Williams
Shannon Laboratory
AT&T Labs ? Research
Florham Park, NJ 07932, USA
jdw@research.att.com
Abstract
We use an EM algorithm to learn user mod-
els in a spoken dialog system. Our method
requires automatically transcribed (with ASR)
dialog corpora, plus a model of transcription
errors, but does not otherwise need any man-
ual transcription effort. We tested our method
on a voice-controlled telephone directory ap-
plication, and show that our learned models
better replicate the true distribution of user ac-
tions than those trained by simpler methods
and are very similar to user models estimated
from manually transcribed dialogs.
1 Introduction and Background
When designing a dialog manager for a spoken dia-
log system, we would ideally like to try different di-
alog management strategies on the actual user pop-
ulation that will be using the system, and select the
one that works best. However, users are typically un-
willing to endure this kind of experimentation. The
next-best approach is to build a model of user behav-
ior. That way we can experiment with the model as
much as we like without troubling actual users.
Of course, for these experiments to be useful,
a high-quality user model is needed. The usual
method of building a user model is to estimate it
from transcribed corpora of human-computer di-
alogs. However, manually transcribing dialogs is
expensive, and consequently these corpora are usu-
ally small and sparse. In this work, we propose a
method of building user models that does not oper-
ate on manually transcribed dialogs, but instead uses
dialogs that have been transcribed by an automatic
speech recognition (ASR) engine. Since this pro-
cess is error-prone, we cannot assume that the tran-
scripts will accurately reflect the users? true actions
and internal states. To handle this uncertainty, we
employ an EM algorithm that treats this information
as unobserved data. Although this approach does
not directly employ manually transcribed dialogs,
it does require a confusion model for the ASR en-
gine, which is estimated from manually transcribed
dialogs. The key benefit is that the number of manu-
ally transcribed dialogs required to estimate an ASR
confusion model is much smaller, and is fixed with
respect to the complexity of the user model.
Many works have estimated user models from
transcribed data (Georgila et al, 2006; Levin et al,
2000; Pietquin, 2004; Schatzmann et al, 2007). Our
work is novel in that we do not assume we have ac-
cess to the correct transcriptions at all, but rather
have a model of how errors are made. EM has pre-
viously been applied to estimation of user models:
(Schatzmann et al, 2007) cast the user?s internal
state as a complex hidden variable and estimate its
transitions using the true user actions with EM. Our
work employs EM to infer the model of user actions,
not the model of user goal evolution.
2 Method
Before we can estimate a user model, we must define
a larger model of human-computer dialogs, of which
the user model is just one component. In this section
we give a general description of our dialog model;
in Section 3 we instantiate the model for a voice-
controlled telephone directory.
We adopt a probabilistic dialog model (similar
121
to (Williams and Young, 2007)), depicted schemat-
ically as a graphical model in Figure 1. Follow-
ing the convention for graphical models, we use
directed edges to denote conditional dependencies
among the variables. In our dialog model, a dia-
log transcript x consists of an alternating sequence
of system actions and observed user actions: x =
(S0, A?0, S1, A?1, . . .). Here St denotes the systemaction, and A?t the output of the ASR engine whenapplied to the true user action At.
A dialog transcript x is generated by our model as
follows: At each time t, the system action is St andthe unobserved user state is Ut. The user state indi-cates the user?s hidden goal and relevant dialog his-
tory which, due to ASR confusions, is known with
certainty only to the user. Conditioned on (St, Ut),the user draws an unobserved action At from a dis-tribution Pr(At | St, Ut; ?) parameterized by an un-known parameter ?. For each user action At, theASR engine produces a hypothesis A?t of what theuser said, drawn from a distribution Pr(A?t | At),which is the ASR confusion model. The user state
Ut is updated to Ut+1 according to a deterministicdistribution Pr(Ut+1 | St+1, Ut, At, A?t). The sys-tem outputs the next system action St+1 accordingto its dialog management policy. Concretely, the val-
ues of St, Ut, At and A?t are all assumed to belongto finite sets, and so all the conditional distributions
in our model are multinomials. Hence ? is a vec-
tor that parameterizes the user model according to
Pr(At = a | St = s, Ut = u; ?) = ?asu.
The problem we are interested in is estimating ?
given the set of dialog transcripts X , Pr(A?t | At)and Pr(Ut+1 | St+1, Ut, At, A?t). Here, we assumethat Pr(A?t | At) is relatively straightforward to es-timate: for example, ASR models that rely a simple
confusion rate and uniform substitutions (which can
be estimated from small number of transcriptions)
have been used to train dialog systems which out-
perform traditional systems (Thomson et al, 2007).
Further, Pr(Ut+1 | St+1, Ut, At, A?t) is often deter-ministic and tracks dialog history relevant to action
selection ? for example, whether the system cor-
rectly or incorrectly confirms a slot value. Here we
assume that it can be easily hand-crafted.
Formally, given a set of dialog transcripts X , our
goal is find a set of parameters ?? that maximizes the
A?t
GFED@ABCAt
GFED@ABCUt ONMLHIJKUt+1
St St+1
OO
OO??
OO
!!D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
((QQ
Q
Q
Q
Q
Q
Q
Q
Q
Q
Q
//
OO
Figure 1: A probabilistic graphical model of a human-
computer dialog. The boxed variables are observed; the
circled variables are unobserved.
log-likelihood of the observed data, i.e.,
?? = arg max
?
log Pr(X | ?)
Unfortunately, directly computing ?? in this equa-
tion is intractable. However, we can efficiently ap-
proximate ?? via an expectation-maximization (EM)
procedure (Dempster et al, 1977). For a dialog tran-
script x, let y be the corresponding sequence of un-
observed values: y = (U0, A0, U1, A1, . . .). Let
Y be the set of all sequences of unobserved values
corresponding to the data set X . Given an estimate
?(t?1), a new estimate ?(t) is produced by
?(t) = arg max
?
EY
[
log Pr(X ,Y | ?)
??? X , ?(t?1)
]
The expectation in this equation is taken over all
possible values for Y . Both the expectation and its
maximization are easy to compute. This is because
our dialog model has a chain-like structure that
closely resembles an Hidden Markov Model, so a
forward-backward procedure can be employed (Ra-
biner, 1990). Under fairly mild conditions, the se-
quence ?(0), ?(1), . . . converges to a stationary point
estimate of ?? that is usually a local maximum.
3 Target Application
To test the method, we applied it to a voice-
controlled telephone directory. This system is cur-
rently in use in a large company with many thou-
sands of employees. Users call the directory system
and provide the name of a callee they wish to be
connected to. The system then requests additional
122
information from the user, such as the callee?s lo-
cation and type of phone (office, cell). Here is a
small fragment of a typical dialog with the system:
S0 = First and last name?
A0 = ?John Doe? [A?0 = Jane Roe ]
S1 = Jane Roe. Office or cell?
A1 = ?No, no, John Doe? [A?1 = No ]
S2 = First and last name?. . .
Because the telephone directory has many names,
the number of possible values for At, A?t, and Stis potentially very large. To control the size of the
model, we first assumed that the user?s intended
callee does not change during the call, which allows
us to group many user actions together into generic
placeholders e.g. At = FirstNameLastName.After doing this, there were a total of 13 possible
values for At and A?t, and 14 values for St.
The user state consists of three bits: one bit indi-
cating whether the system has correctly recognized
the callee?s name, one bit indicating whether the
system has correctly recognized the callee?s ?phone
type? (office or cell), and one bit indicating whether
the user has said the callee?s geographic location
(needed for disambiguation when several different
people share the same name). The deterministic dis-
tribution Pr(Ut+1 | St+1, Ut, At, A?t) simply updatesthe user state after each dialog turn in the obvious
way. For example, the ?name is correct? bit of Ut+1is set to 0 when St+1 is a confirmation of a namewhich doesn?t match At.
Recall that the user model is a multinomial distri-
bution Pr(At | St, Ut; ?) parameterized by a vector
?. Based on the number user actions, system actions,
and user states, ? is a vector of (13? 1)? 14? 8 =
1344 unknown parameters for our target application.
4 Experiments
We conducted two sets of experiments on the tele-
phone directory application, one using simulated
data, and the other using dialogs collected from ac-
tual users. Both sets of experiments assumed that all
the distributions in Figure 1, except the user model,
are known. The ASR confusion model was esti-
mated by transcribing 50 randomly chosen dialogs
from the training set in Section 4.2 and calculat-
ing the frequency with which the ASR engine rec-
ognized A?t such that A?t 6= At. The probabilities
Pr(A?t |At) were then constructed by assuming that,when the ASR engine makes an error recognizing a
user action, it substitutes another randomly chosen
action.
4.1 Simulated Data
Recall that, in our parameterization, the user model
is Pr(At = a | St = s, Ut = u; ?) = ?asu. Soin this set of experiments, we chose a reasonable,
hand-crafted value for ?, and then generated syn-
thetic dialogs by following the probabilistic process
depicted in Figure 1. In this way, we were able to
create synthetic training sets of varying sizes, as well
as a test set of 1000 dialogs. Each generated dialog
d in each training/test set consisted of a sequence of
values for all the observed and unobserved variables:
d = (S0, U0, A0, A?0, . . .).
For a training/test set D, let KDasu be the numberof times t, in all the dialogs in D, that At = a, St =
s, and Ut = u. Similarly, let K?Das be the number oftimes t that A?t = a and St = s.
For each training set D, we estimated ? using the
following three methods:
1. Manual: Let ? be the maximum likelihood
estimate using manually transcribed data, i.e.,
?asu = K
D
asuP
a KDasu
.
2. Automatic: Let ? be the maximum likelihood
estimate using automatically transcribed data,
i.e., ?asu = eKDasP
a
eKDas
. This approach ignores
transcription errors and assumes that user be-
havior depends only on the observed data.
3. EM: Let ? be the estimate produced by the EM
algorithm described in Section 2, which uses
the automatically transcribed data and the ASR
confusion model.
Now let D be the test set. We evaluated each user
model by calculating the normalized log-likelihood
of the model with respect to the true user actions in
D:
`(?) =
?
a,s,u KDasu log ?asu
|D|
`(?) is essentially a measure of how well the user
model parameterized by ? replicates the distribution
123
of user actions in the test set. The normalization is
to allow for easier comparison across data sets of
differing sizes.
We repeated this entire process (generating train-
ing and test sets, estimating and evaluating user
models) 50 times. The results presented in Figure
2 are the average of those 50 runs. They are also
compared to the normalized log-likelihood of the
?Truth?, which is the actual parameter ? used to gen-
erated the data.
The EM method has to estimate a larger number
of parameters than the Automatic method (1344 vs.
168). But as Figure 2 shows, after observing enough
dialogs, the EM method is able to leverage the hid-
den user state to learn a better model of user behav-
ior, with an average normalized log-likelihood that
falls about halfway between that of the models pro-
duced by the Automatic and Manual methods.
0 500 1000 1500?8
?7
?6
?5
?4
?3
Number of dialogs in training set
No
rm
aliz
ed 
log
?li
kel
iho
od
 
 
Truth
Manual
EM
Automatic
Figure 2: Normalized log-likelihood of each model
type with respect to the test set vs. size of training
set. Each data point is the average of 50 runs. For the
largest training set, the EM models had higher normal-
ized log-likelihood than the Automatic models in 48 out
of 50 runs.
4.2 Real Data
We tested the three estimation methods from the pre-
vious section on a data set of 461 real dialogs, which
we split into a training set of 315 dialogs and a test
set of 146 dialogs. All the dialogs were both man-
ually and automatically transcribed, so that each of
the three methods was applicable. The normalized
log-likelihood of each user model, with respect to
both the training and test set, is given in Table 1.
Since the output of the EM method depends on a
random choice of starting point ?(0), those results
were averaged over 50 runs.
Training Set `(?) Test Set `(?)
Manual -2.87 -3.73
EM -3.90 -4.33
Automatic -4.60 -5.80
Table 1: Normalized log-likelihood of each model type
with respect to the training set and the test set. The
EM values are the average of 50 runs. The EM models
had higher normalized log-likelihood than the Automatic
model in 50 out of 50 runs.
5 Conclusion
We have shown that user models can be estimated
from automatically transcribed dialog corpora by
modeling dialogs within a probabilistic framework
that accounts for transcription errors in a principled
way. This method may lead to many interesting fu-
ture applications, such as continuous learning of a
user model while the dialog system is on-line, en-
abling automatic adaptation.
References
AP Dempster, NM Laird, and DB Rubin. 1977. Maxi-
mum likelihood from incomplete data via the em algo-
rithm. J. Royal Stat. Soc., 39:1?38.
K Georgila, J Henderson, and O Lemon. 2006. User
simulation for spoken dialogue systems: Learning and
evaluation. In Proc ICSLP, Pittsburgh, USA.
E Levin, R Pieraccini, and W Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialogue strategies. IEEE Trans on Speech and Audio
Processing, 8(1):11?23.
O Pietquin. 2004. A framework for unsupervised learn-
ing of dialogue strategies. Ph.D. thesis, Faculty of En-
gineering, Mons (TCTS Lab), Belgium.
LR Rabiner, 1990. A tutorial on hidden Markov models
and selected applications in speech recognition, pages
267?296. Morgan Kaufmann Publishers, Inc.
J Schatzmann, B Thomson, and SJ Young. 2007. Sta-
tistical user simulation with a hidden agenda. In Proc
SIGDial, Antwerp, pages 273?282.
B Thomson, J Schatzmann, K Welhammer, H Ye, and
SJ Young. 2007. Training a real-world POMDP-based
dialog system. In Proc NAACL-HLT Workshop Bridg-
ing the Gap: Academic and Industrial Research in Di-
alog Technologies, Rochester, New York, USA, pages
9?17.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
124
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 1?4,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Demonstration of a POMDP Voice Dialer
Jason Williams
AT&T Labs ? Research, Shannon Laboratory
180 Park Ave., Florham Park, NJ 07932, USA
jdw@research.att.com
Abstract
This is a demonstration of a voice di-
aler, implemented as a partially observable
Markov decision process (POMDP). A real-
time graphical display shows the POMDP?s
probability distribution over different possi-
ble dialog states, and shows how system out-
put is generated and selected. The system
demonstrated here includes several recent ad-
vances, including an action selection mecha-
nism which unifies a hand-crafted controller
and reinforcement learning. The voice dialer
itself is in use today in AT&T Labs and re-
ceives daily calls.
1 Introduction
Partially observable Markov decision processes
(POMDPs) provide a principled formalism for plan-
ning under uncertainty, and past work has argued
that POMDPs are an attractive framework for build-
ing spoken dialog systems (Williams and Young,
2007a). POMDPs differ from conventional dialog
systems in two respects. First, rather than main-
taining a single hypotheses for the dialog state,
POMDPs maintain a probability distribution called
a belief state over many possible dialog states. A
distribution over a multiple dialog state hypothe-
ses adds inherent robustness, because even if an er-
ror is introduced into one dialog hypothesis, it can
later be discarded in favor of other, uncontaminated
dialog hypotheses. Second, POMDPs choose ac-
tions using an optimization process, in which a de-
veloper specifies high-level goals and the optimiza-
tion works out the detailed dialog plan. Because
of these innovations, POMDP-based dialog systems
have, in research settings, shown more resilience
to speech recognition errors, yielding shorter di-
alogs with higher task completion rates (Williams
and Young, 2007a; Williams and Young, 2007b).
Because POMDPs differ significantly from con-
ventional techniques, their operation can be difficult
to conceptualize. This demonstration provides an
accessible illustration of the operation of a state-of-
the-art POMDP-based dialog system. The system
itself is a voice dialer, which has been operational
for several months in AT&T Labs. The system in-
corporates several recent advances, including effi-
cient large-scale belief monitoring (akin to Young et
al., 2006), policy compression (Williams and Young,
2007b), and a hybrid hand-crafted/optimized dialog
manager (Williams, 2008). All of these elements
are depicted in a graphical display, which is updated
in real time, as a call is progressing. Whereas pre-
vious demonstrations of POMDP-based dialog sys-
tems have focused on showing the probability distri-
bution over dialog states (Young et al, 2007), this
demonstration adds new detail to convey how ac-
tions are chosen by the dialog manager.
In the remainder of this paper, Section 2 presents
the dialog system and explains how the POMDP ap-
proach has been applied. Then, section 3 explains
the graphical display which illustrates the operation
of the POMDP.
2 System description
This application demonstrated here is a voice dialer
application, which is accessible within the AT&T re-
search lab and receives daily calls. The dialer?s vo-
1
cabulary consists of 50,000 AT&T employees.
The dialog manager in the dialer is implemented
as a POMDP. In the POMDP approach, a distribu-
tion called a belief state is maintained over many
possible dialog states, and actions are chosen us-
ing reinforcement learning (Williams and Young,
2007a). In this application, a distribution is main-
tained over all of the employees? phone listings in
the dialer?s vocabulary, such as Jason Williams? of-
fice phone or Srinivas Bangalore?s cell phone. As
speech recognition results are received, this distri-
bution is updated using probability models of how
users are likely to respond to questions and how the
speech recognition process is likely to corrupt user
speech. The benefit of tracking this belief state is
that it synthesizes all of the ASR N-Best lists over
the whole dialog ? i.e., it makes the most possible
use of the information from the speech recognizer.
POMDPs then choose actions based on this be-
lief state using reinforcement learning (Sutton and
Barto, 1998). A developer writes a reward func-
tion which assigns a real number to each state/action
pair, and an optimization algorithm determines how
to choose actions in order to maximize the expected
sum of rewards. In other words, the optimization
performs planning and this allows a developer to
specify the trade-off to use between task comple-
tion and dialog length. In this system, a simple re-
ward function assigns -1 per system action plus +/-
20 for correctly/incorrectly transferring the caller at
the end of the call. Optimization was performed
roughly following (Williams and Young, 2007b), by
running dialogs in simulation.
Despite their theoretical elegance, applying a
POMDP to this spoken dialog system has presented
several interesting research challenges. First, scal-
ing the number of listings quickly prevents the be-
lief state from being updated in real-time, and here
we track a distribution over partitions, which is akin
to a beam search in ASR (Young et al, 2006). At
first, all listings are undifferentiated in a single mas-
ter partition. If a listing appears on the N-Best list,
it is separated into its own partition and tracked sep-
arately. If the number of partitions grows too large,
then low-probability partitions are folded back into
the master undifferentiated partition. This technique
allows a well-formed distribution to be maintained
over an arbitrary number of concepts in real-time.
Second, the optimization process which chooses
actions is also difficult to scale. To tackle this,
the so-called ?summary POMDP? has been adopted,
which performs optimization in a compressed space
(Williams and Young, 2007b). Actions are mapped
into clusters called mnemonics, and states are com-
pressed into state feature vectors. During opti-
mization, a set of template state feature vectors are
sampled, and values are computed for each action
mnemonic at each template state feature vector.
Finally, in the classical POMDP approach there is
no straightforward way to impose rules on system
behavior because the optimization algorithm con-
siders taking any action at any point. This makes
it impossible to impose design constraints or busi-
ness rules, and also needlessly re-discovers obvious
domain properties during optimization. In this sys-
tem, a hybrid POMDP/hand-crafted dialog manager
is used (Williams, 2008). The POMDP and con-
ventional dialog manager run in parallel; the con-
ventional dialog manager nominates a set of one or
more allowed actions, and the POMDP chooses the
optimal action from this set. This approach enables
rules to be imposed and allows prompts to easily be
made context-specific.
The POMDP dialer has been compared to a con-
vention version in dialog simulation, and improved
task completion from 92% to 97% while keeping di-
alog length relatively stable. The system has been
deployed in the lab and we are currently collecting
data to assess performance with real callers.
3 Demonstration
A browser-based graphical display has been created
which shows the operation of the POMDP dialer
in real time, shown in Figure 1. The page is up-
dated after the user speech has been processed, and
before the next system action has been played to
the user. The left-most column shows the system
prompt which was just played to the user, and the
N-Best list of recognized text strings, each with its
confidence score.
The center column shows the POMDP belief
state. Initially, all of the belief is held by the mas-
ter, undifferentiated partition, which is shown as a
green bar and always shown first. As names are rec-
ognized, they are tracked separately, and the top 10
2
Previous 
system action
N-Best 
recognition 
with 
confidence 
scores
POMDP belief 
state 
Features of the 
current dialog 
state
Allowed 
actions
Values of the 
allowed 
actions
Resulting 
system action, 
output to TTS
Figure 1: Overview of the graphical display. Contents are described in the text.
names are shown as blue bars, sorted by their belief.
If the system asks for the phone type (office or mo-
bile), then the bars sub-divide into a light blue (for
office) and dark blue (for mobile).
The right column shows how actions are selected.
The top area shows the features of the current state
used to choose actions. Red bars show the two con-
tinuous features: the belief in the most likely name
and most likely type of phone. Below that, three
discrete features are shown: how many phones are
available (none, one, or both); whether the most
likely name has been confirmed (yes or no); and
whether the most likely name is ambiguous (yes
or no). Below this, the allowed actions (i.e., those
which are nominated by the hand-crafted dialog
manager) are shown. Each action is preceded by the
action mnemonic, shown in bold. Below the allowed
actions, the action selection process is shown. The
values of the action mnemonic at the closest tem-
plate point are shown next to each action mnemonic.
Finally the text of this action, which is output to the
caller, is shown at the bottom of the right-hand col-
umn. Figure 2 shows the audio and video transcrip-
tion of an interaction with the demonstration.
4 Conclusion
This demonstration has shown the operation of a
POMDP-based dialog system, which incorporates
recent advances including efficient large-scale belief
monitoring, policy compression, and a unified hand-
crafted/optimized dialog manager. A graphical dis-
play shows the operation of the system in real-time,
as a call progresses, which helps make the POMDP
approach accessible to a non-specialist.
Acknowledgments
Thanks to Iker Arizmendi and Vincent Goffin for
help with the implementation.
References
R Sutton and A Barto. 1998. Reinforcement Learning:
an Introduction. MIT Press.
JD Williams and SJ Young. 2007a. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
JD Williams and SJ Young. 2007b. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
JD Williams. 2008. The best of both worlds: Unifying
conventional dialog systems and POMDPs. In (In sub-
mission).
SJ Young, JD Williams, J Schatzmann, MN Stuttle, and
K Weilhammer. 2006. The hidden information state
approach to dialogue management. Technical Re-
port CUED/F-INFENG/TR.544, Cambridge Univer-
sity Engineering Department.
SJ Young, J Schatzmann, B R M Thomson, KWeilham-
mer, and H Ye. 2007. The hidden information state
dialogue manager: A real-world POMDP-based sys-
tem. In Proc NAACL-HLT, Rochester, New York, USA.
3
Tr
an
sc
rip
t o
f a
ud
io
Sc
re
en
sh
ot
s 
of
 
gr
ap
hic
al 
dis
pla
y
S1
: S
or
ry,
 
firs
t a
nd
 
las
t n
am
e?
U1
: J
un
lan
Fe
ng
S1
: D
ial
ing
S1
: J
un
lan
Fe
ng
.
U1
: Y
es
S1
: F
irs
t a
nd
 
las
t n
am
e?
U1
: J
un
lan
Fe
ng
Fig
ur
e
2:
Th
e
de
m
on
str
at
ion
?
s
gr
ap
hic
al
dis
pla
yd
ur
ing
a
ca
ll.
Th
e
gr
ap
hic
al
dis
pla
yh
as
be
en
cr
op
pe
da
nd
re
-
ar
ra
ng
ed
for
re
ad
ab
ilit
y.
Th
e
ca
lle
r
sa
ys
?
Ju
nla
n
Fe
ng
?
tw
ice
,
an
da
lth
ou
gh
ea
ch
na
m
e
re
co
gn
itio
n
alo
ne
ca
rr
ies
a
low
co
nfid
en
ce
sc
or
e,
the
be
lie
fst
at
e
ag
gr
eg
at
es
thi
s
info
rm
at
ion
.
Th
is
no
ve
lb
eh
av
ior
en
ab
les
the
ca
llt
o
pr
og
re
ss
fas
te
r
tha
n
in
the
co
nv
en
tio
na
ls
yst
em
an
di
llu
str
at
es
on
e
be
ne
fito
fth
e
PO
MD
P
ap
pr
oa
ch
.
We
ha
ve
ob
se
rv
ed
se
ve
ra
lo
the
r
no
ve
ls
tra
teg
ies
no
tin
a
ba
se
lin
e
co
nv
en
tio
na
ld
ial
er
:
for
ex
am
ple
,
the
PO
MD
P-
ba
se
ds
yst
em
w
ill
co
nfir
m
a
ca
lle
e?
s
na
m
e
at
diff
er
en
tc
on
fide
nc
e
lev
els
de
pe
nd
ing
on
w
he
the
r
the
ca
lle
e
ha
s
a
ph
on
e
nu
m
be
r
lis
te
do
r
no
t;
an
du
se
s
ye
s/n
o
co
nfir
m
at
ion
qu
es
tio
ns
to
dis
am
big
ua
te
w
he
n
the
re
ar
e
tw
o
am
big
uo
us
ca
lle
es
.
4
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 1?8,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Applying POMDPs to Dialog Systems in the Troubleshooting Domain
Jason D. Williams
AT&T Labs ? Research
180 Park Ave, Building 103
Florham Park, NJ 07932
jdw@research.att.com
Abstract
This paper reports on progress applying
partially observable Markov decision pro-
cesses (POMDPs) to a commercial dia-
log domain: troubleshooting. In the trou-
bleshooting domain, a spoken dialog sys-
tem helps a user to fix a product such as
a failed DSL connection. Past work has
argued that a POMDP is a principled ap-
proach to building spoken dialog systems
in the simpler slot-filling domain; this pa-
per explains how the POMDPs formula-
tion can be extended to the more complex
troubleshooting domain. Results from di-
alog simulation verify that a POMDP out-
performs a handcrafted baseline.
1 Introduction
In the troubleshooting domain, a spoken dialog sys-
tem (SDS) helps a user to restore a malfunction-
ing product such as a DSL connection to a work-
ing state. Building dialog systems for this domain
presents several new challenges. First, the user may
make mistakes such as misinterpreting the meaning
of a status light or pressing the wrong button, so even
if no speech recognition errors are made, the user?s
response may be misleading. Next, in addition to the
speech recognizer, input is also received from run-
ning network tests such as pinging the user?s DSL
modem. Input from both sources may contain er-
rors, and a dialog system must cope with conflict-
ing information from two channels. In sum, the dia-
log system never knows the true state of the product
nor the user?s true actions, yet must still instruct the
user to successfully restore the product to a working
state.
Dialog models which explicitly model uncertainty
have been shown to significantly outperform base-
line models which do not, primarily because they
cope better with conflicting evidence introduced by
speech recognition errors (Roy et al, 2000; Zhang
et al, 2001; Williams and Young, 2007). However,
past work has been confined to slot-filling tasks and
has not tackled the troubleshooting domain. Con-
versely, dialog systems for troubleshooting in the
literature have not attempted to model uncertainty
directly (Grosz and Sidner, 1986; Lochbaum, 1998).
The contribution of this paper is to show how
to model a troubleshooting spoken dialog system
as a partially observable Markov decision process
(POMDP). We argue that past work in the gen-
eral troubleshooting literature represents simplifica-
tions or special cases of a POMDP, then we show
how a troubleshooting POMDP can be combined
with a dialog system POMDP to create a unified
framework that admits global optimization. Exper-
iments with simulated users show how the POMDP
formulation effectively balances diagnostic actions
(such as a network test) with communicative ac-
tions (such as giving the user instructions), and how
the POMDP formulation outperforms a hand-crafted
baseline both in terms of efficiency and task comple-
tion.
This paper is organized as follows. Section 2 re-
views POMDPs, the general troubleshooting prob-
lem, and POMDP-based spoken dialog systems;
section 3 explains how these two POMDPs can be
combined to model a troubleshooting spoken dialog
system; sections 4-5 present results from simulation;
and section 6 concludes.
2 Background
A POMDP is a model for control when there is un-
certainty in the effects of actions and in the state
1
of the environment. Formally, a POMDP P is de-
fined as a tuple P = (S,A,T,R,O,Z, ?, b0) where
S is a set of states s describing the environment with
s ? S; A is a set of actions a ? A which operate
on the environment; T defines a transition proba-
bility P (s?|s, a); R defines the expected (immedi-
ate, real-valued) reward r(s, a) ? <; O is a set
of observations o ? O which describe the state of
the environment; Z defines an observation proba-
bility P (o?|s?, a); ? is a geometric discount factor
0 ? ? ? 1; and b0 is an initial belief state, defined
below.
The POMDP operates as follows. At each time-
step, the environment is in some unobserved state
s. Since s is not known exactly, a distribution over
possible states called a belief state b is maintained
where b(s) indicates the probability of being in a
particular state s, with b0 denoting the initial belief
state. Based on b, a control algorithm (also called a
policy) selects an action a, receives a reward r, and
the environment transitions to (unobserved) state s?,
where s? depends only on s and a. The environment
then generates an observation o? which is dependent
on s? and a. At each time-step, b is updated as
b?(s?) = ? ? P (o?|s?, a)
?
s
P (s?|s, a)b(s) (1)
where ? is a normalization constant (Kaelbling et
al., 1998). The process of maintaining b at each
time step is called belief monitoring. The cumula-
tive, infinite-horizon, discounted reward is called the
return and written V =
??
t=0 ?tr(st, at), where st
and at indicate the state of the environment and the
action taken at time t, respectively. The goal of the
control algorithm is to choose actions that maximize
the expected return E[V ] given b and the POMDP
parameters P, and the process of searching for such
a control algorithm is called optimization.
2.1 Troubleshooting as a POMDP
The goal of the general (non-dialog) problem of au-
tomated troubleshooting is for a control algorithm to
fix a product by taking a sequence of diagnosis and
repair actions. Different actions have different relia-
bilities and different costs, and the aim is to find the
sequence that minimizes the total cost. Since the ac-
tions are not completely reliable, the true state of the
Timestep n Timestep n+1
x'
amy'
r'
x
amy
r
'
Figure 1: Influence diagram depiction of automated
troubleshooting. Round nodes represent random
variables, shaded nodes are unobservable and clear
nodes are observable. Arcs show conditional depen-
dence. Squares indicate actions, selected by the pol-
icy. Diamonds indicate real-valued rewards.
product can?t be known with certainty: for example,
an instrument may provide a faulty reading.
Formalizing this, a product has some hidden state
x, which is usually decomposed into components
x = (x1, x2, . . . , xn). A control algorithm takes
action am, which changes the state of x according
to P (x?|x, am). The product then produces an ob-
servation y according to P (y?|x?, am). Replacing
cost with reward, the control algorithm receives re-
ward r(x, am) and the goal is to find the sequence
of actions which maximizes the cumulative sum of
reward. When viewed in this way, automated trou-
bleshooting can be readily viewed as a POMDP
(Shakeri et al, 1997). Figure 1 shows the automated
troubleshooting task as an influence diagram.
Although POMDPs are an elegant model for trou-
bleshooting, they are also notoriously difficult to
optimize and much of the troubleshooting litera-
ture seeks appropriate constraints which render the
optimization tractable, such as assuming that each
action affects at most one product state compo-
nent, that actions have deterministic effects, and that
there is only fault present (Heckerman et al, 1995).
More recently, advances in the POMDP literature
have radically increased the scalability of optimiza-
tion algorithms: for example, Poupart optimizes a
substantial network troubleshooting problem cast as
a generic POMDP (Poupart and Boutilier, 2004).
Viewing troubleshooting as a generic POMDP in-
creases the scope of admissible troubleshooting
tasks, and as will be discussed in section 3, this view
also allows the uncertainty in the product state to be
explicitly modelled in a spoken dialog system.
2
2.2 Spoken dialog as a POMDP
Past work has argued that POMDPs represent a prin-
cipled approach to modelling (non-troubleshooting)
spoken dialog systems (Roy et al, 2000; Zhang et
al., 2001; Williams and Young, 2007). The intu-
ition is that a user?s goals and actions form the un-
observed state and the (possibly erroneous) ASR
result forms the observation. The SDS-POMDP
model (Williams and Young, 2007) formalizes this
by decomposing the POMDP state variable s into
three components, s = (su, au, d). The component
su gives the user?s goal, such as a complete travel
itinerary in a travel reservation task. The component
au gives the most recent user action (communicative
intent), such as stating a place the user would like to
travel to. Finally the component d records relevant
dialog history, such as the grounding status of a slot.
None of these components is observable directly by
the dialog system and the SDS-POMDP belief state
is formed of a distribution over these components
b(su, au, d). The POMDP action a corresponds to
the dialog system action am, such as asking the user
where they want to go to. Finally, the POMDP ob-
servation o is set to (a?u, c), where a?u is the hypoth-
esis of the user?s action (communicative intent) pro-
vided by the speech recognition and understanding
process, and c is the confidence score. Figure 2
shows the SDS-POMDP model as an influence di-
agram, and also shows the conditional dependencies
assumed in the SDS-POMDP model.
3 Troubleshooting SDS-POMDP model
In this section, we develop a statistical model of a
troubleshooting dialog system. The formulation be-
gins by taking the union of the state spaces of the
dialog POMDP and the troubleshooting POMDP,
(su, au, d, x), and making two modifications. First,
it is assumed that the user?s goal su is known and
constant (i.e., to fix the product), and as such does
not need to be included. Second, the user?s action
au is decomposed into two components: atsu denotes
troubleshooting actions that are directed toward the
product, such as turning a modem on or off, entering
a user name or just observing the status lights; and
acomu denotes communicative actions to the dialog
system such as saying ?green? or ?yes?. Reorder-
c, au
au
d
su
am
r
c, au
au
d'
su
am
r'
Timestep n Timestep n+1~ ~
'
''
' '
Figure 2: SDS-POMDP model shown as an influ-
ence diagram. The dotted box refers to all of the
(hidden) POMDP state components.
ing, the combined POMDP state has components:
s = (atsu , x, acomu , d). (2)
Next, the combined observation is formed of the
union of the observations from the dialog and trou-
bleshooting POMDPs:
o = (a?comu , c, y). (3)
Finally, since the POMDP may choose only one ac-
tion at each time-step, the POMDP action is simply
am.
Substituting eq. 2 into the POMDP
transition function P (s?|s, a) yields
P (atsu
?, x?, acomu ?, d?|atsu , x, acomu , d, am) and is
decomposed as follows. First, it is assumed that
the user?s troubleshooting action atsu ? depends only
on the system?s action am, the previous product
state x and the dialog history d. Next, it is as-
sumed that the product state x? depends only on
the previous product state x, and the most recent
user?s and dialog system?s troubleshooting actions
atsu ? and am. Further, the user?s communicative
action acomu ? depends only on the most recent user?s
troubleshooting action atsu ?, product state x?, dialog
history d and system action am. Finally, the dialog
history component d? is a function of the previous
dialog history d and the most recent user and dialog
system actions atsu ?, acomu ?, and am. With these
assumptions, the combined transition function is:
P (atsu
?, x?, acomu ?, d?|atsu , x, acomu , d, am) ?
P (atsu ?|x, d, am) ? P (x?|x, am, atsu ?)?
P (acomu ?|d, am, atsu ?, x?)?
P (d?|d, am, atsu ?, x?, acomu ?)
(4)
3
Timestep n Timestep n+1
x'
am
au   
y'
au     
r'
x
am
au  
y
au    
r
d'd
c,au
~com
com com'ts ts'
'
c,au
~com'
Figure 3: Influence diagram of a troubleshooting
spoken dialog system.
Substituting eq. 3 into the POMDP
observation function P (o?|s?, a) yields
P (a?comu ?, c?, y?|atsu ?, x?, acomu ?, d?, am). It is assumed
that the ASR hypothesis a?comu ? and confidence score
c? depend only on the user?s speech in acomu ? and
that the result of the troubleshooting test (conducted
by the dialog system) y? depends only on the state
of the product x? and the dialog system?s action am:
P (a?comu ?, c?, y?|atsu ?, x?, acomu ?, d?, am) ?
P (a?comu ?, c?|acomu ?) ? P (y|am, x?)
(5)
An influence diagram of the model is shown in Fig-
ure 3.
At runtime, a belief state (i.e., distribution)
is maintained over the POMDP state variables,
b(atsu , x, acomu , d). Based on this belief state the pol-
icy chooses an action am and receives observation
(a?comu ?, c?, y?). The belief state is updated by apply-
ing Eq 1, and the cycle repeats.
The user action models P (atsu ?|x, d, am) and
P (acomu ?|d, am, atsu ?, x?) indicate how users are
likely to respond in troubleshooting dialogs and can
be estimated from annotated dialog data. The prod-
uct models P (x?|x, am, atsu ?) and P (y?|am, x?) in-
dicate how user and dialog system actions change
the state of the product and the reliability of tests,
and these can be estimated by interviewing domain
experts or by examining logs of product perfor-
mance. As in the SDS-POMDP model, the di-
alog history model P (d?|d, am, acomu ?, x?, atsu ?) can
be handcrafted so as to incorporate features from
the dialog history which the dialog designer be-
lieves are important, such as appropriateness or no-
tions of grounding. The ASR confusion model
P (a?comu ?, c?|acomu ?) can be estimated from speech
recognition data or derived analytically. The re-
ward function can include distinct costs for differ-
ent diagnostic tests, dialog actions, and for success-
ful/unsuccessful task completion. It is not specified
explicitly here since it depends on the application.
4 Illustration: DSL-1
To illustrate the general framework, we first created
a very simple troubleshooting spoken dialog system
called DSL-1. Table 1 shows the values for all of
the variables. In DSL-1, the are just 2 possible prob-
lems: no-power and no-network.
The conditional probability tables composing the
model were handcrafted based on conversations
with troubleshooting experts and past experience
with spoken dialog systems. For example, the model
of user?s troubleshooting action assumes that the
user performs the correct action with p = 0.9,
doesn?t understand with p = 0.05, and performs an
incorrect action with p = 0.05. The model of the
user?s communicative action assumes that the user
provides correct (but possibly incomplete) informa-
tion with p = 0.9, and remains silent with p = 0.1.
The model of the product was designed such that
the user?s check-power and check-network actions
are always effective, but if power is restored there
may still be no-network with p = 0.2.
The model of the speech recognition and under-
standing process uses a concept error rate of 30%,
where errors are uniformly distributed, and no con-
fidence scores are used. For example, when the
user expresses the concept all-ok, it will be recog-
nized correctly 70% of the time, and will be mis-
recognized as no-power 5% of the time, as no-
network 5% of the time, etc. The model for y in-
dicates how reliable the ping action is, set with a
parameter perr: for example if perr = 0.1, the result
of a ping test will be incorrect 10% of the time. In
the experiments below, the value of perr is varied to
explore how the POMDP policy trades off between
the ping action and communicative actions.
The reward function provides +100 for taking
the end-call action when the connection is working,
?100 for taking the done action when the connec-
tion isn?t working, and ?1 for any communicative
or test action. The dialog continues until the dialog
4
Variable Values
atsu {check-power, check-network, observe, do-nothing, dont-understand}
State x {all-ok, no-power, no-network}
Components d {start, not-done, done}
acomu {no-power, no-network, power-ok, all-ok, silent, didnt-understand}
Observation a?comu (same set as acomu )
Components y {ping-ok, no-response}
Action am {ping, ask-working-ok, req-check-power, req-check-network, end-call}
Table 1: Variable values in the DSL-1 simple troubleshooting example.
system takes the done action, at which point the di-
alog is over.
4.1 Results
The POMDP was optimized using a standard algo-
rithm from the literature (Spaan and Vlassis, 2005).
This algorithm optimizes the policy at a discrete set
of belief points; as more points are added, the qual-
ity of the resulting policy improves at the expense
of more computation. We found that 300 belief
points achieved asymptotic performance. A model
was constructed for values of perr ranging from 0.0
to 0.5; each model was optimized and then evaluated
using 5000 simulated dialogs.
Results are shown in Figures 4 and 5. In each
figure the x-axis is the accuracy of the ping action:
perr = 0% indicates that the ping action is entirely
reliable and perr = 50% indicates that the ping ac-
tion returns useless noise. In Figure 4, the y-axis
shows average return, and in Figure 5, the solid line
shows the task completion rate and the dotted line
shows the average dialog length. The error bars in-
dicate the 95% confidence interval.
As the error rate for the ping action increases from
0% to 20%, the average dialog length increases from
5.1 turns to 6.5 turns, and the successful task com-
pletion rate falls from 100.0% to 98.9%. These fig-
ures then remain broadly constant from 20% to 50%.
In other words, as errors in the ping action increase,
dialogs become longer and occasionally the system
fails to fix the connection. Inspecting the dialog
transcripts showed that at perr = 0%, the policy
relies on the ping action to judge whether the con-
nection is working. As perr increases, the policy
decreasingly employs the ping diagnostic action in
favor of the ask-working-ok communicative action
until perr = 20%, at which point the ping action is
84
85
86
87
88
89
90
91
92
93
94
0% 5% 10
%
15
%
20
%
25
%
30
%
35
%
40
%
45
%
50
%
p err  (ping error rate)
Av
e
ra
ge
 
re
tu
rn
Figure 4: Error rate of the ping action vs. reward
gained per dialog. As the error rate of the ping ac-
tion is increased, performance declines until the er-
ror rate reaches 20%, at which point the system no
longer uses the ping action.
not used at all. At this point the planning process has
determined that the ping action doesn?t help produce
better dialogs than just interacting with the caller,
and the performance from 20% to 50% is constant.1
These experiments confirm that, for a very sim-
ple troubleshooting dialog system in simulation, the
POMDP approach is able to synthesize noisy infor-
mation gained from communicative and test actions
into one unified belief while the underlying, hidden
product state is changing. This is an important re-
sult because past work that has applied POMDPs
to dialog systems has employed a single modality
(communicative actions), and have largely had fixed
persistent state. Even so, this illustration is much
too small to be of practical use, and relies entirely
on hand-crafted models of the dynamics. In the next
section a model of realistic scale is presented with
transition dynamics estimated from real conversa-
1The variations in performance between 20% and 50% are
due to sampling in the optimization algorithm.
5
98.3%
98.7%
99.1%
99.5%
99.9%
0% 5% 10
%
15
%
20
%
25
%
30
%
35
%
40
%
45
%
50
%
p err  (ping error rate)
Ta
sk
 c
o
pm
le
tio
n
 
ra
te
 
(%
)
5
5.4
5.8
6.2
6.6
7
Av
e
ra
ge
 
di
a
lo
g 
le
n
gt
h 
(tu
rn
s)
Task completion rate Average dialog length 
Figure 5: Error rate of the ping action vs. success-
ful task completion rate and average dialog length.
The left y axis and the solid line show the task com-
pletion rate, and the right y axis and the dotted line
show the average dialog length in number of turns.
tional data.
5 Illustration: DSL-2
In this section we present a second POMDP-based
troubleshooting dialog system called DSL-2 which
captures many of the properties of a real-world
DSL troubleshooting task. Approximately 100 tele-
phone calls between (human) DSL support agents
and customers were monitored, and the observations
of these conversations guided creation of the dia-
log system, including typical problems, agent in-
structions, and user responses. The product state X
was decomposed into 19 components which track,
for example, whether there are any outages re-
ported, whether the DSL modem is switched on, and
whether the username has been entered correctly in
the DSL configuration. Seven of these components
can cause the connection to fail: (1) router pow-
ered off or crashed, (2) an upstream network crash,
(3) a service outage, (4-6) a wrong username, pass-
word, or connection type entered in the DSL modem
configuration, and (7) an unknown root cause which
can?t be fixed by the dialog system. Some of the
problems can only be identified or fixed by the dia-
log system (such as a service outage or an upstream
network crash), and the rest only by the user (such as
a router being off or wrong username entered). The
problems may occur in any combination: for exam-
ple, there may be a service outage while the user?s
password is entered incorrectly. The system action
set (Am) consisted of 18 actions such as asking the
user to turn the modem on, providing the correct
username, checking whether any outages have been
reported, and rebooting the upstream network inter-
face. The user?s troubleshooting action set Atsu con-
sisted of 12 actions such as turning the modem on
or off, opening the DSL configuration screen, enter-
ing a password, and attempting to surf to a website.
The user?s communicative action set Acomu consisted
of 11 actions such as saying the color of a light (e.g.,
?red? or ?green?), yes and no, back-channel, silence,
and an ?out-of-grammar? action which accounts for
user speech which cannot be recognized.
The conditional probability tables for each of the
product components were handcrafted based on in-
terviews with DSL technicians and are almost all
deterministic. For example, if the DSL modem
is powered on, the power light will always be on.
Next a subset of the agent/user telephone calls were
transcribed and annotated with simple dialog acts,
and from these the two user models were estimated.
Smoothing was applied so that the models allow for
the user to take any action at any point in the dia-
log. Concept recognition errors were generated with
p = 0.30, and confidence scores were drawn from
an exponential distribution such that (at an equal er-
ror rate confidence threshold) about half of the con-
cept errors could be identified. The reward func-
tion provides +100 for ending the dialog having cor-
rectly identified (and if possible resolved) the root
causes, ?100 for ending the dialog with unidenti-
fied or unresolved root causes, and ?1 for any other
action. If a dialog ran for more than 100 turns, it was
considered a failure and terminated.
We created a state-based dialog manager by hand
(called HC) which broadly reflects the agents? trou-
bleshooting practices and which serves as our base-
line. HC consisted of 19 dialog states, where each
state specified an action am to take (for example to
ask the user to turn the modem on), and observations
from the speech recognizer a?comu or troubleshooting
tests y may cause transitions between dialog states.
HC first asks the user to power cycle the modem,
then checks for outages and ?resets? the upstream
network interface, then verifies that the username,
password, and network type are configured correctly
on the router. After each step HC checks if the con-
nection is working by asking if the network light
is green, pinging the modem, then asking the user
6
POMDP HC HC(0)
CER 30% 30% 0%
N 500 500 500
TCR 96.1% 78.0% 88.6%
Length 19.9 76.5 48.5
Return 73.3 8.13 48.8
Table 2: Results for the POMDP and hand-crafted
dialog managers. CER is concept error rate; TCR is
task completion rate; Length is measured in turns.
to open a web browser; if any one of these tests
fails, troubleshooting resumes, and if they all suc-
ceed then HC ends the dialog. If an outage is de-
tected, HC says this and exits, and if the connection
still isn?t working at the end of the dialog then HC
escalates the call to a (human) technician. In general
when HC receives an unexpected answer or confi-
dence score below the equal-error rate threshold, it
treats this as a likely speech recognition error and
remains in the same dialog state.
Next, optimization was performed as described in
(Williams et al, 2005). This technique takes as in-
put a POMDP model and a state-based dialog con-
troller, and produces an improved dialog controller.
Space limitations prevent a full description here; the
intuition is that the algorithm uses the POMDP be-
lief state at runtime to ?rewire? the dialog controller
to achieve an improvement in reward. Because this
optimization algorithm improves a standard state-
based dialog controller (in this case the HC base-
line), it provides an indication of the value of adding
the POMDP machinery.
5.1 Results and discussion
First, 500 simulated dialogs were run with the
POMDP, and then 500 simulated dialogs were run
with the HC baseline controller. Finally, as a fur-
ther comparison, the ASR simulation was changed
so that no ASR errors were made, and HC was
run for 500 dialogs in this configuration, which we
call HC(0). Results are shown in Table 2. All of
the observed differences are statistically significant
(p  0.01).
In the presence of speech recognition errors, the
POMDP produces dialogs which are significantly
shorter and more successful than HC. Moreover, the
POMDP, which faced ASR errors, also outperforms
HC(0), which did not. Examination of the dialog
transcripts found that the main source of failure for
HC(0) was exceeding 100 turns. In other words,
quantitatively, the POMDP is both more robust to
ASR errors and (independent of ASR errors) more
efficient.
The dialog transcripts were inspected to deter-
mine qualitatively how the POMDP attained better
performance. An example is shown in Table 3. At
the start of the conversation, the belief (probability)
that the connection is working p(allOk) is 56% and
the belief that the power to the DSL modem is on
p(pwrOn) is 98.0% (these are 2 of the 19 compo-
nents in the product state x). As the dialog pro-
gresses, belief monitoring updates these to account
for the evidence received. For example, the unsuc-
cessful ping in S1 causes p(allOk) to drop from 56%
to 14%. The belief monitoring process also natu-
rally makes use of indirect evidence ? for example,
in U14 the user indicates the network light is ?red?:
since the network light will only be on if the power
to the DSL modem is on, this causes an increase in
the belief that the power is on, from 99.1% to 99.8%.
The key benefit of the POMDP approach is that
the dialog manager can exploit the belief state to
make better progress in the face of low-confidence
or even nonsensical replies, without sacrificing over-
all task completion. For example, in S1 through S9
the POMDP policy differs from the baseline con-
troller: the baseline controller would have ignored
the lower-confidence recognitions in U4 and U8, but
the POMDP policy moves ahead. When the policy
receives a nonsensical reply, for example in U6, it
reverts back to an earlier stage of the troubleshoot-
ing procedure it had previously skipped. This latter
behavior ensures that omitting steps to move faster
through the procedure doesn?t ultimately sacrifice
task completion.
6 Conclusions
This paper has shown how a spoken dialog system
for troubleshooting can be cast as a POMDP. The
troubleshooting domain has important differences to
past applications of the POMDP approach and the
two illustrations provided in this paper support our
claim that, at least in dialog simulation, the advan-
tages of POMDPs apply to this domain.
After finishing simulation experiments, we in-
7
stalled DSL-2 into a real dialog system, and found
that belief monitoring runs slower than real-time.
We subsequently developed a method to address
this, which we will report on separately in the fu-
ture, and are now preparing for a pilot study with
real users.
References
BJ Grosz and CL Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Lin-
guistics, 12(3):175?204.
D Heckerman, JS Breese, and K Rommelse. 1995.
Decision-theoretic troubleshooting. Communications
of the ACM, 38(3):49?57.
L Kaelbling, ML Littman, and AR Cassandra. 1998.
Planning and acting in partially observable stochastic
domains. Artificial Intelligence, 101.
KE Lochbaum. 1998. A collaborative planning model
of intentional structure. Computational Linguistics,
24(4):525?572.
P Poupart and C Boutilier. 2004. VDCBPI: an ap-
proximate scalable algorithm for large scale POMDPs.
In Proc Advances in Neural Information Processing
Systems 17 (NIPS), Vancouver, Canada, pages 1081?
1088.
N Roy, J Pineau, and S Thrun. 2000. Spoken dialog
management for robots. In Proc Association for Com-
putational Linguistics (ACL), Hong Kong.
M Shakeri, KR Pattipati, V Raghavan, A Patterson-Hine,
and DL Iverson. 1997. Multiple fault isolation in
redundant systems. Technical Report NCC2-5123,
NASA Ames.
MTJ Spaan and N Vlassis. 2005. Perseus: randomized
point-based value iteration for POMDPs. Journal of
Artificial Intelligence Research, 24:195?220.
JD Williams and SJ Young. 2007. Partially observable
markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
JD Williams, P Poupart, and SJ Young. 2005. Partially
observable Markov decision processes with continu-
ous observations for dialogue management. In Proc
SIGdial Workshop on Discourse and Dialogue, Lis-
bon.
B Zhang, Q Cai, J Mao, and B Guo. 2001. Planning
and acting under uncertainty: A new model for spoken
dialogue system. In Proc Conf on Uncertainty in Ar-
tificial Intelligence (UAI), Seattle, Washington, pages
572?579.
ID p(allOk) p(pwrOn) Transcript
*S1 56% 98.0% I?m going to try pinging your DSL modem from here.
S2 - - There was no response so it seems your connection isn?t working yet.
*S3 14% 96.1% Please turn the DSL modem on.
U4 - - ok [ok, 0.004]
*S5 30% 96.4% What color is the network light on the DSL modem?
U6 - - dontunderstand [no, 0.052]
*S7 30% 93.8% Please turn the DSL modem on.
U8 - - dontunderstand [ok, 0.454]
*S9 31% 99.1% I?m going to check if there are any outages reported, just a moment.
S10 - - There are no outages reported.
S11 39% 99.1% I?m going to try resetting your connection from here, just a moment.
S12 - - Ok, that has been done.
S13 48% 99.1% What color is the network light on the DSL modem?
U14 - - red [red, 0.678]
S15 3% 99.8% Please open the DSL configuration screen.
... ... ... ...
Table 3: Fragment of a conversation with the POMDP dialog manager. Asterisks (*) indicate transitions
not in the baseline dialog manager. p(allOk) shows the probability that DSL connectivity is working and
p(pwrOn) shows the probability that the power to the DSL modem is on, according to the POMDP belief
state. The simulated user?s communicative actions are shown in italics, followed by the [concept,
confidence score] produced by the ASR simulation.
8
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 132?135,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Estimating probability of correctness for ASR N-Best lists
Jason D. Williams and Suhrid Balakrishnan
AT&T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USA
{jdw,suhrid}@research.att.com
Abstract
For a spoken dialog system to make good
use of a speech recognition N-Best list, it is
essential to know how much trust to place
in each entry. This paper presents a method
for assigning a probability of correctness to
each of the items on the N-Best list, and to
the hypothesis that the correct answer is not
on the list. We find that both multinomial lo-
gistic regression and support vector machine
models yields meaningful, useful probabili-
ties across different tasks and operating con-
ditions.
1 Introduction
For spoken dialog systems, speech recognition er-
rors are common, and so identifying and reducing
dialog understanding errors is an important problem.
One source of potentially useful information is the
N-Best list output by the automatic speech recog-
nition (ASR) engine. The N-Best list contains N
ranked hypotheses for the user?s speech, where the
top entry is the engine?s best hypothesis. When the
top entry is incorrect, the correct entry is often con-
tained lower down in the N-Best list. For a dialog
system to make use of the N-Best list, it is useful to
estimate the probability of correctness for each en-
try, and the probability that the correct entry is not
on the list. This paper describes a way of assigning
these probabilities.
2 Background and related work
To begin, we formalize the problem. The user takes
a communicative action u, saying a phrase such as
?Coffee shops in Madison New Jersey?. Using a lan-
guage model g, the speech recognition engine pro-
cesses this audio and outputs an ordered list of N
hypotheses for u, u? = {u?1, . . . u?N}, N ? 2. To
the N-Best list we add the entry u??, where u = u??
indicates that u does not appear on the N-Best list.
The ASR engine also generates a set of K recog-
nition features f = [f1, . . . , fK ]. These features
might include properties of the lattice, word confu-
sion network, garbage model, etc. The aim of this
paper is to estimate a model which accurately as-
signs the N + 1 probabilities P (u = u?n|u?, f) for
n ? {?, 1, . . . , N} given u? and f . The model also
depends on the language model g, but we don?t in-
clude this conditioning in our notation for clarity.
In estimating these probabilities, we are most
concerned with the estimates being well-calibrated.
This means that the probability estimates we ob-
tain for events should accurately represent the em-
pirically observed proportions of those events. For
example, if 100 1-best recognitions are assigned a
probability of 60%, then approximately 60 of those
100 should in fact be the correct result.
Recent work proposed a generative model of the
N-Best list, P (u?, f |u) (Williams, 2008). The main
motivation for computing a generative model is
that it is a component of the update equation used
by several statistical approaches to spoken dialog
(Williams and Young, 2007). However, the diffi-
culty with a generative model is that it must estimate
a joint probability over all the features, f ; thus, mak-
ing use of many features becomes problematic. As
a result, discriminative approaches often yield bet-
ter results. In our work, we propose a discrimina-
tive approach and focus on estimating the probabil-
ities conditioned on the features. Additionally, un-
der some further fairly mild assumptions, by apply-
ing Bayes Rule our model can be shown equivalent
to the generative model required in the dialog state
update. This is a desirable property because dialog
systems using this re-statement have been shown to
work in practice (Young et al, 2009).
Much past work has assigned meaningful proba-
132
bilities to the top ASR hypothesis; the novelty here
is assigning probabilities to all the entries on the list.
Also, our task is different to N-Best list re-ranking,
which seeks to move more promising entries toward
the top of the list. Here we trust the ordering pro-
vided by the ASR engine, and only seek to assign
meaningful probabilities to the elements.
3 Model
Our task is to estimate P (u = u?n|u?, f) for n ?
{?, 1, . . . , N}. Ideally we could view each element
on the N-Best list as its own class and train an
(N+1)-class regression model. However this is dif-
ficult for two reasons. First, the number of classes is
variable: ASR results can have different N-Best list
lengths for different utterances. Second, we found
that the distribution of items on the N-Best list has
a very long tail, so it would be difficult to obtain
enough data to accurately estimate late position class
probabilities.
As a result, we model the probability P in two
stages: first, we train a (discriminative) model Pa to
assign probabilities to just three classes: u = u??,
u = u?1, and u ? u?2+, where u?2+ = {u?2, . . . , u?N}.
In the second stage, we use a separate probability
model Pb to distribute mass over the items in u?2+:
P (u?n = u|u?, f) = (1)
?
?
?
?
?
Pa(u = u?1|f) if n = 1,
Pa(u ? u?2+|f)Pb(u = u?n|f) if n > 1,
Pa(u = u??|f) if n = ?
To model Pa, multinomial logistic regression
(MLR) is a natural choice as it yields a well-
calibrated estimator for multi-class problems. Stan-
dard MLR can over-fit when there are many features
in comparison to the number of training examples;
to address this we use ridge regularized MLR in our
experiments below (Genkin et al, 2005).
An alternative to MLR is support vector machines
(SVMs). SVMs are typically formulated including
regularization; however, their output scores are gen-
erally not interpretable as probabilities. Thus for Pa,
we use an extension which re-scales SVM scores to
yield well-calibrated probabilities (Platt, 1999).
Our second stage model Pb, distributes mass
over the items in the tail of the N-best list (n ?
0%
20%
40%
60%
80%
100%
0% 20% 40% 60% 80% 100%
Cu
m
ul
at
ive
 p
ro
ba
bi
lity
Fractional position in N-Best list  (n/N) of correct entry
N-Best lists with N < 100 entries
N-Best lists with N >= 100 entries
All N-Best lists
Model (Beta distribution)
Figure 1: Empirical cumulative distribution of cor-
rect recognitions for N-Best lists, and the Beta dis-
tribution model for Pb on 1, 000 business search ut-
terances (Corpus 1 training set, from Section 4.)
{2, . . . , N}). In our exploratory analysis of N-Best
lists, we noticed a trend that facilitates modeling this
distribution. We observed that the distribution of the
fraction of the correction position n/N was rela-
tively invariant to N . For example, for both short
(N < 100) and long (N ? 100) lists, the proba-
bility that the answer was in the top half of the list
was very similar (see Figure 1). Thus, we chose a
continuous distribution in terms of the fractional po-
sition n/N as the underlying distribution in our sec-
ond stage model. Given the domain of the fractional
position [0, 1], we chose a Beta distribution. Our fi-
nal second stage model is then an appropriately dis-
cretized version of the underlying Beta, namely, Pb:
Pb(u = u?n|f) = Pb(u = u?n|N) =
Pbeta(
n? 1
N ? 1;?, ?) ? Pbeta(
n? 2
N ? 1;?, ?)
where Pbeta(x;?, ?) is the standard Beta cumula-
tive distribution function parametrized by ? and ?.
Figure 1 shows an illustration. In summary, our
method requires training the three-class regression
model Pa, and estimating the Beta distribution pa-
rameters ? and ?.
4 Data and experiments
We tested the method by applying it to three cor-
pora of utterances from dialog systems in the busi-
ness search domain. All utterances were from
133
Corpus WCN SVM MLR
1 -0.714 -0.697 -0.703
2 -0.251 -0.264 -0.222
3 -0.636 -0.605 -0.581
Table 1: Mean log-likelihoods on the portion of the
test set with the correct answer on the N-Best list.
None of the MLR nor SVM results differ signifi-
cantly from the WCN baseline at p < 0.02.2
users with real information needs. Corpus 1 con-
tained 2, 000 high-quality-audio utterances spoken
by customers using the Speak4It application, a
business search application which operates on mo-
bile devices, supporting queries containing a listing
name and optionally a location.1 Corpus 2 and 3
contained telephone-quality-audio utterances from
14, 000 calls to AT&T?s ?411? business directory
listing service. Corpus 2 contained locations (re-
sponses to ?Say a city and state?); corpus 3 con-
tained listing names (responses to ?OK what list-
ing??). Corpus 1 was split in half for training and
testing; corpora 2 and 3 were split into 10, 000 train-
ing and 4, 000 testing utterances.
We performed recognition using the Watson
speech recognition engine (Goffin et al, 2005), in
two configurations. Configuration A uses a sta-
tistical language model trained to recognize busi-
ness listings and optionally locations, and acous-
tic models for high-quality audio. Configuration B
uses a rule-based language model consisting of all
city/state pairs in the USA, and acoustic models for
telephone-quality audio. Configuration A was ap-
plied to corpora 1 and 3, and Configuration B was
applied to corpus 2. This experimental design is in-
tended to test our method on both rule-based and
statistical language models, as well as matched and
mis-matched acoustic and language model condi-
tions.
We used the following recognition features in f :
f1 is the posterior probability from the best path
through the word confusion network, f2 is the num-
ber of segments in the word confusion network,
f3 is the length of the N-Best list, f4 is the aver-
age per-frame difference in likelihood between the
1http://speak4it.com
22-tailed Wilcoxon Signed-Rank Test; 10-way partitioning.
Corpus WCN SVM MLR
1 -1.12 -0.882 -0.890
2 -0.821 -0.753 -0.734
3 -1.00 -0.820 -0.824
Table 2: Mean log-likelihoods on the complete test
set. All MLR and SVM results are significantly bet-
ter than the WCN baseline (p < 0.0054).2
highest-likelihood lattice path and a garbage model,
and f5 is the average per-frame difference in likeli-
hood between the highest-likelihood lattice path and
the maximum likelihood of that frame on any path
through the lattice. Features are standardized to the
range [?1, 1] and MLR and SVM hyperparameters
were fit by cross-validation on the training set. The
? and ? parameters were fit by maximum likelihood
on the training set.
We used the BMR toolkit for regularized multi-
nomial logistic regression (Genkin et al, 2005), and
the LIB-SVM toolkit for calibrated SVMs (Chang
and Lin, 2001).
We first measure average log-likelihood the mod-
els assign to the test sets. As a baseline, we use the
posterior probability estimated by the word confu-
sion network (WCN), which has been used in past
work for estimating likelihood of N-Best list entries
(Young et al, 2009). However, the WCN does not
assign probability to the u = u?? case ? indeed, this
is a limitation of using WCN posteriors. So we re-
ported two sets of results. In Table 1, we report the
average log-likelihood given that the correct result
is on the N-Best list (higher values, i.e., closer to
zero are better). This table includes only the items
in the test set for which the correct result appeared
on the N-Best list (that is, excluding the u = u??
cases). This table compares our models to WCNs
on the task for which the WCN is designed. On this
task, the MLR and SVM methods are competitive
with WCNs, but not significantly better.
In Table 2, we report average log-likelihood for
the entire test set. Here the WCNs use a fixed
prior for the u = u?? case, estimated on the training
sets (u = u?? class is always assigned 0.284; other
classes are assigned 1 ? 0.284 = 0.716 times the
WCN posterior). This table compares our models
to WCNs on the task for which our model is de-
signed. Here, the MLR and SVM models yielded
134
020
40
60
80
100
120
140
160
180
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Nu
m
be
r o
f e
nt
rie
s
Fr
ac
tio
n 
co
rre
ct
Regression-assigned probability
Perfect calibration (left axis)
MLR Calibration (left axis)
Number of entries (right axis)
Figure 2: Calibration and histogram of probabilities
assigned by MLR on corpus 1 (test set).
significantly better results than the WCN baseline.
We next investigated the calibration properties of
the models. Results for the MLR model on the
u = u?1 class from corpus 1 test set are shown in
Figure 2. This illustrates that the MLR model is rel-
atively well-calibrated and yields broadly distributed
probabilities. Results for the SVM were similar, and
are omitted for space.
Finally we investigated whether the models
yielded better accept/reject decisions than their in-
dividual features. Figure 3 shows the MLR model
a receiver operating characteristic (ROC) curve for
corpus 1 test set for the u = u?1 class. This con-
firms that the MLR model produces more accurate
accept/reject decisions than the individual features
alone. Results for the SVM were similar.
5 Conclusions
This paper has presented a method for assigning
useful, meaningful probabilities to elements on an
ASR N-Best list. Multinomial logistic regression
(MLR) and support vector machines (SVMs) have
been tested, and both produce significantly better
models than a word confusion network baseline, as
measured by average log likelihood. Further, the
models appear to be well-calibrated and yield a bet-
ter indication of correctness than any of its input fea-
tures individually.
In dialog systems, we are often more interested in
the concepts than specific words, so in future work,
we hope to assign probabilities to concepts. In the
0%
10%
20%
30%
40%
50%
0% 10% 20% 30% 40%
Tr
ue
 A
cc
ep
ts
False Accepts
MLR-assigned probability
n=1 posterior from word confusion network (f )
Average delta to best frame in lattice (f )
Average delta to garbage model (f )
1
5
4
Figure 3: ROC curve for MLR and the 3 most infor-
mative input features on corpus 1 (test set).
meantime, we are applying the method to our dialog
systems, to verify their usefulness in practice.
References
CC Chang and CJ Lin, 2001. LIBSVM: a library for sup-
port vector machines. http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
A Genkin, DD Lewis, and D Madigan, 2005.
BMR: Bayesian Multinomial Regression Soft-
ware. http://www.stat.rutgers.edu/
?madigan/BMR/.
V Goffin, C Allauzen, E Bocchieri, D Hakkani-Tur,
A Ljolje, S Parthasarathy, M Rahim, G Riccardi, and
M Saraclar. 2005. The AT&T Watson speech recog-
nizer. In Proc ICASSP, Philadelphia.
JC Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classifiers,
pages 61?74. MIT Press.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
JD Williams. 2008. Exploiting the ASR N-best by track-
ing multiple dialog state hypotheses. In Proc ICSLP,
Brisbane.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2009. The hidden information
state model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language. To appear.
135
Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learning-
based dialog management systems 
Jason D. Williams Steve Young 
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom 
{jdw30,sjy}@eng.cam.ac.uk 
 
Abstract 
This paper describes a method for ?boot-
strapping? a Reinforcement Learning-
based dialog manager using a Wizard-of-
Oz trial.  The state space and action set 
are discovered through the annotation, 
and an initial policy is generated using a 
Supervised Learning algorithm.  The 
method is tested and shown to create an 
initial policy which performs significantly 
better and with less effort than a hand-
crafted policy, and can be generated using 
a small number of dialogs. 
1 Introduction and motivation 
Recent work has successfully applied Rein-
forcement Learning (RL) to learning dialog strat-
egy from experience, typically formulating the 
problem as a Markov Decision Process (MDP).  
(Walker et al, 1998; Singh et al, 2002; Levin et 
al., 2000). Despite successes, several open 
questions remain, especially the issue of how to 
create (or ?bootstrap?) the initial system prior to 
data becoming available from on-line operation.  
This paper proceeds as follows.  Section 2 out-
lines the core elements of an MDP and issues re-
lated to applying an MDP to dialog management.  
Sections 3 and 4 detail a method for addressing 
these issues, and the procedure used to test the 
method, respectively.  Sections 5-7 present the re-
sults, a discussion, and conclusions, respectively.  
2 Background 
An MDP is composed of a state space, an action 
set, and a policy which maps each state to one ac-
tion.  Introducing a reward function allows us to 
create or refine the policy using RL.  (Sutton and 
Barto, 1998).  
When the MDP framework is applied to dialog 
management, the state space is usually constructed 
from vector components including information 
state, dialog history, recognition confidence, data-
base status, etc.  In most of the work to date both 
the state space and action set are hand selected, in 
part to ensure a limited state space, and to ensure 
training can proceed using a tractable number of 
dialogs.  However, hand selection becomes im-
practical as system size increases, and automatic 
generation/selection of these elements is currently 
an open problem, closely related to the problem of 
exponential state space size.  
3 A method for bootstrapping RL-based 
systems 
Here we propose a method for ?bootstrapping? an 
MDP-based system; specifically, we address the 
choice of the state representation and action set, 
and the creation of an initial policy. 
3.1 Step 1: Conduct Wizard-of-Oz dialogs 
The method commences with ?talking wizard? 
interactions in which either the wizard?s voice is 
disguised, or a Text-to-speech engine is used. We 
choose human/wizard rather than human/human 
dialogs as people behave differently toward (what 
they perceive to be) machines and other people as 
discussed in J?nsson and Dahlbick, 1988 and also 
validated in Moore and Browning, 1992. The dia-
log, including wizard?s interaction with back-end 
data sources is recorded and transcribed. 
3.2 Step 2: Exclude out-of-domain turns 
The wizard will likely handle a broader set of re-
quests than the system will ultimately be able to 
cover; thus some turns must be excluded.  Step 2 
begins by formulating a list of tasks which are to 
be included in the final system?s repertoire; turns 
dealing with tasks outside this repertoire are la-
beled out-of-domain (OOD) and excluded. 
This step takes an approach which is analogous 
to, but more simplistic than ?Dialogue Distilling? 
(Larsson et al, 2000) which changes, adds and re-
moves portions of turns or whole turns.  Here rules 
simply stipulate whether to keep a whole turn. 
3.3 Step 3: Enumerate action set and state 
space 
Next, the in-domain turns are annotated with dia-
log acts.  Based on these, an action set is enumer-
ated, and a set of state parameters and their 
possible values to form a vector describing the 
state space is determined, including: 
? Information state (e.g., departure-city, arri-
val-city) from the user and database. 
? The confidence/confirmation status of in-
formation state variables. 
? Expressed user goal and/or system goal. 
? Low-level turn information (e.g., yes/no re-
sponses, backchannel, ?thank you?, etc.). 
? Status of database interactions (e.g., when a 
form can be submitted or has been returned). 
A variety of dialog-act tagging taxonomies ex-
ist in the literature.  Here we avoid a tagging sys-
tem that relies on a stack or other recursive 
structure (for example, a goal or game stack) as it 
is not immediately clear how to represent a recur-
sive structure in a state space.  
In practice, many information state components 
are much less important than their corresponding 
confirmation status, and can be removed. 
Even with this reduction, the state space will be 
massive ? probably too large to ever visit all states.  
We propose using a parameterized value function -
- i.e., a value function that shares parameters 
across states (including states previously unob-
served).  One special case of this is state tying, in 
which a group of states share the same value func-
tion; an alternative is to use a Supervised Learning 
algorithm to estimate a value function. 
3.4 Step 4: Form an initial policy 
For each turn in the corpus, a vector is created rep-
resenting the current dialog state plus the subse-
quent wizard action.  Taking the action as the class 
variable, Supervised Learning (SL) is used to build 
a classifier which functions as the initial policy. 
Depending on the type of SL algorithm used, it 
may be possible to produce a prioritized list of ac-
tions rather than a single classification; in this case, 
this list can form an initial list of actions permitted 
in a given state. 
As noted by Levin et al (2000), supervised 
learning is not appropriate for optimizing dialog 
strategy because of the temporal/environmental 
nature of dialog.  Here we do not assert that the 
SL-learned policy will be optimal ? simply that it 
can be easily created, that it will be significantly 
better than random guessing, and better and 
cheaper to produce than creating a cursory hand-
crafted strategy. 
3.5 Limitations of the method 
This method has several obvious limitations: 
? Because a talking, perfect-hearing wizard is 
used, no/little account is taken of the recog-
nition errors to be expected with automated 
speech recognition (ASR).   
? Excluding too much in Step 2 may exclude 
actions or state components which would 
have ultimately produced a superior system. 
4 Experimental design 
The proposed approach has been tested using the 
Autoroute corpus of 166 dialogs, in which a talk-
ing wizard answered questions about driving direc-
tions in the UK (Moore and Browning, 1992).   
A small set of in-domain tasks was enumerated 
(e.g., gathering route details, outputting summary 
information about a route, disambiguation of place 
names, etc.), and turns which did not deal with 
these tasks were labeled OOD and excluded.  The 
latter included gathering the caller?s name and lo-
cation (?UserID?), the most common OOD type. 
The corpus was annotated using an XML 
schema to provide the following: 
? 15 information components were created 
(e.g., from, to, time, car-type).  
? Each information component was given a 
status: C (Confirmed), U (Unconfirmed), 
and NULL (Not known).   
? Up to 5 routes may be under discussion at 
once ? the state tracked the route under dis-
cussion (RUD), total number of routes (TR), 
and all information and status components 
for each route.  
? A component called flow tracked single-
turn dialog flow information from the caller 
(e.g., yes, no, thank-you, silence).  
? A component called goal tracked the (most 
recent) goal expressed by the user (e.g., 
plan-route, how-far).  Goal is empty 
unless explicitly set by the caller, and only 
one goal is tracked at a time.  No attempt is 
made to indicate if/when a goal has been 
satisfied.  
33 action types were identified, some of which 
take information slots as parameters (e.g., wh-
question, implicit-confirmation) . 
The corpus gave no indication of database in-
teractions other than what can be inferred from the 
dialog transcripts.  One common wizard action 
asked the caller to ?please wait? when the wizard 
was waiting for a database response.  To account 
for this, we provided an additional state component 
which indicated whether the database was working 
called db-request, which was set to true 
whenever the action taken was please-wait 
and false otherwise.  Other less common database 
interactions occurred when town names were am-
biguous or not found, and no attempt was made to 
incorporate this information into the state represen-
tation. 
The state space was constructed using only the 
status of the information slots (not the values); of 
the 15, 4 were occasionally expressed (e.g., day of 
the week) but not used to complete the transaction 
and were therefore excluded from the state space.   
Two turns of wizard action history were also in-
corporated.  This formulation of the state space 
leads to approximately 1033 distinct states. 
For evaluation of the method, a hand-crafted 
policy of 30 rules mapping states to actions was 
created by inspecting the dialogs.1 
5 Results 
Table 1 shows in-domain vs. out-of-domain wizard 
and caller turns.  Figures 1 through 4 show counts 
of flow values, goal values, action values, and state 
                                                          
1 It was not clear in what situations some of the actions should 
be used, so some (rare) actions were not covered by the rules. 
components, respectively.  The most common ac-
tion type was ?please-wait? (14.6% of actions). 
 
Turn 
type 
Total In  
domain 
OOD: 
User ID 
OOD: 
Other 
Wiz-
ard 
3155 
(100%)
2410 
(76.4%) 
594 
(18.8%) 
151 
(4.8%) 
Caller 2466 
(100%)
1713 
(69.5%) 
561 
(22.7%) 
192 
(7.8%) 
Table 1: In-domain and Out-of-domain (OOD) turns  
 
Criteria States Visits 
Visited only 
once 
1182  
(85.7%) 
1182  
(45.9%) 
Visited more 
than once 
without a con-
flict 
96  
(7.0%) 
353  
(13.7%) 
Visited more 
than once with 
conflict 
101 
(7.3%) 
1041  
(40.3%) 
TOTAL 1379  
(100%) 
2576  
(100%) 
Table 2: ?Conflicts? by state and visits 
 
Estimated action probabilities  Visits 
p(action taken | state) > p(any 
other action | state) 
774 (74.3%) 
p(action taken | state) = p(one 
or more other actions | state) > 
p(all remaining actions | state) 
119 (11.4%) 
p(action taken | state) < 
p(another action | state) 
148 (14.2%) 
TOTAL 1041 (100%) 
Table 3: Estimated probabilities in ?conflict? states 
 
Engine Class Precision 
Action-type only 72.7% jBNC 
Action-type & parameters 66.7% 
Action-type only 79.1% C4.5 
Action-type & parameters 72.9% 
Action-type only 58.4% Hand- 
craft Action-type & parameters 53.9% 
Table 4: Results from SL training and evaluation 
 
In some cases, the wizard took different actions 
in the same state; we labeled this situation a ?con-
flict.? Table 2 shows the number of distinct states 
that were encountered and, for states visited more 
than once, whether conflicting actions were se-
lected.  Of states with conflicts, Table 3 shows 
probabilities estimated from the corpus.  
The interaction data was then submitted to 2 SL 
pattern classifiers ? c4.5 using decision-trees 
(Quinlan, 1992) and jBNC using Na?ve Bayesians 
(Sacha, 2003).  Table 4 shows both algorithms? 10-
fold cross validation classification error rates 
classifying (1) the action type, and (2) the action 
type with parameters, as well as the results for the 
hand-crafted policy. 
Figure 5 show the 10-fold cross validation clas-
sification error rates for varying amounts of train-
ing data for the two SL algorithms classifying 
action-type and parameters. 
6 Discussion 
The majority of the data collected was ?usable?: 
although 26.7% of turns were excluded, 20.5% of 
these were due to a well-defined task not under 
study here (user identification), and only 6.1% fell 
outside of designated tasks.  That said, it may be 
desirable to impose a minimum threshold on how 
many times a flow, goal, or action must be ob-
served before adding it to the state space or action 
set given the ?long tails? of these elements.   
0
50
100
150
200
1 2 3 4 5 6 7 8 9 10 11 12
Flow component ID
D
ia
lo
gs
 c
on
ta
in
in
g 
Fl
ow
 ID
 
Figure 1: Dialogs containing flow components 
 
0
2
4
6
8
10
12
14
16
1 2 3 4 5 6 7 8 9 10 11 12 13
Goal component ID
D
ia
lo
gs
 c
on
ta
in
in
g 
G
oa
l
 
Figure 2: Dialogs containing goal components 
About half of the turns took place in states 
which were visited only once.  This confirms that 
massive amounts of data would be needed to ob-
serve all valid dialog states, and suggests dialogs 
do not confine themselves to familiar states. 
Within a given state, the wizard?s behavior is 
stochastic, occasionally deviating from an other-
wise static policy.  Some of this behavior results 
from database information not included in the cor-
pus and state space; in other cases, the wizard is 
making apparently random choices. 
 
0
50
100
150
200
1 5 9 13 17 21 25 29 33
Action ID
D
ia
lo
gs
 c
on
ta
in
in
g 
A
ct
io
n
 
Figure 3: Dialogs containing action types 
 
0
50
100
150
200
1 3 5 7 9 11 13
Component ID
D
ia
lo
gs
 c
on
ta
in
in
g
co
m
po
ne
nt
 
Figure 4: Dialogs containing information components 
 
Figure 5 implies that a relatively small number 
of dialogs (several hundred turns, or about 30-40 
dialogs) contain the vast majority of information 
relevant to SL algorithms ? less than expected.  
Correctly predicting the wizard?s action in 72.9% 
of turns is significantly better than the 58.4% cor-
rect prediction rate from the handcrafted policy. 
When a caller allows the system to retain initia-
tive, the policy learned by the c4.5 algorithm han-
dled enquiries about single trips perfectly.  Policy 
errors start to occur as the user takes more initia-
tive, entering less well observed states. 
Hand examination of a small number of mis-
classified actions indicate that about half of the 
actions were ?reasonable? ? e.g., including an extra 
item in a confirmation.  Hand examination also 
confirmed that the wizard?s non-deterministic be-
havior and lack of database information resulted in 
misclassifications. 
Other sources of mis-classifications derived 
primarily from under-account of the user?s goal 
and other deficiencies in the expressiveness of the 
state space. 
7 Conclusion & future work 
This work has proposed a method for determining 
many of the basic elements of a RL-based spoken 
dialog system with minimal input from dialog de-
signers using a ?talking wizard.?  The viability of 
the model has been tested with an existing corpus 
and shown to perform significantly better than a 
hand-crafted policy and with less effort to create. 
Future research will explore refining this ap-
proach vis-?-vis user goal, applying this method to 
actual RL-based systems and finding suitable 
methods for parameterized value functions 
References 
A. J?nsson and N. Dahlbick.  1988.  Talking to A Com-
puter is Not Like Talking To Your Best Friend. 
Proceedings of the Scandinavian Conference on 
ceedings of the Scandinavian Conference on 
Artificial Intelligence '88, pp. 53-68. 
Staffan Larsson, Arne J?nsson and Lena Santamarta.  
2000.  Using the process of distilling dialogues to 
understand dialogue systems. ICSLP 2000, Beijing. 
Ester Levin, Roberto Pieraccini and Wieland Eckert.  
2000.  A Stochastic Model of Human-Machine Inter-
action for Learning Dialogue Structures.  IEEE 
Trans on Speech and Audio Processing 8(1):11-23. 
R. K. Moore and S. R. Browning.  1992. Results of an 
exercise to collect ?genuine? spoken enquiries using 
Wizard of Oz techniques. Proc. of the Inst. of Acous-
tics. 
Ross Quinlan.  1992.  C4.5 Release 8. (Software pack-
age).  http://www.cse.unsw.edu.au/~quinlan/ 
Jarek P. Sacha.  2003.  jBNC version 1.0.  (Software 
package).  http://sourceforge.net/projects/jbnc/. 
Satinder Singh, Diane Litman, Michael Kearns, Marilyn 
Walker. 2002.  Optimizing Dialogue Management 
with Reinforcement Learning: Experiments with the 
NJFun System.  Journal of Artificial Intelligence Re-
search, vol 16, 105-133. 
Richard S. Sutton and Andrew G. Barto.  1998.  Rein-
forcement Learning: an Introduction.  The MIT 
Press, Cambridge, Massachusetts, USA. 
Marilyn A. Walker, Jeanne C. Fromer, Shrikanth Nara-
yanan.  1998.  Learning Optimal Dialogue Strate-
gies: A Case Study of a Spoken Dialogue Agent for 
Email.  Proc. 36th Annual Meeting of the ACM and 
17th Int?l Conf. on Comp. Linguistics, 1345--1352. 
 
 
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
0 500 1000 1500 2000 2500
Training examples (dialog turns)
Cl
as
sif
ic
at
io
n 
er
ro
rs
 (%
)
c4.5
Naive Bayes
 
Figure 5: Classification errors vs. training samples for action-type & parameters 
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 466?475,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative state tracking for spoken dialog systems
Angeliki Metallinou1?, Dan Bohus2, and Jason D. Williams2
1University of Southern California, Los Angeles, CA, USA
2Microsoft Research, Redmond, WA, USA
metallin@usc.edu dbohus@microsoft.com jason.williams@microsoft.com
Abstract
In spoken dialog systems, statistical state
tracking aims to improve robustness to
speech recognition errors by tracking a
posterior distribution over hidden dialog
states. Current approaches based on gener-
ative or discriminative models have differ-
ent but important shortcomings that limit
their accuracy. In this paper we discuss
these limitations and introduce a new ap-
proach for discriminative state tracking
that overcomes them by leveraging the
problem structure. An offline evaluation
with dialog data collected from real users
shows improvements in both state track-
ing accuracy and the quality of the pos-
terior probabilities. Features that encode
speech recognition error patterns are par-
ticularly helpful, and training requires rel-
atively few dialogs.
1 Introduction
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog
in a process called dialog state tracking. For ex-
ample, in a bus schedule information system, the
dialog state might indicate the user?s desired bus
route, origin, and destination. Dialog state track-
ing is difficult because automatic speech recog-
nition (ASR) and spoken language understand-
ing (SLU) errors are common, and can cause the
system to misunderstand the user?s needs. At
the same time, state tracking is crucial because
the system relies on the estimated dialog state to
choose actions ? for example, which bus schedule
information to present to the user.
The dialog state tracking problem can be for-
malized as follows (Figure 1). Each system turn
in the dialog is one datapoint. For each datapoint,
the input consists of three items: a set of K fea-
tures that describes the current dialog context, G
dialog state hypotheses, and for each dialog state
hypothesis, M features that describe that dialog
state hypothesis. The task is to assign a probabil-
ity distribution over the G dialog state hypotheses,
plus a meta-hypothesis which indicates that none
of the G hypotheses is correct.
Note that G varies across turns (datapoints) ?
for example, in the first turn of Figure 1, G = 3,
and in the second and third turns G = 5. Also
note that the dialog state tracker is not predicting
the contents of the dialog state hypotheses; the di-
alog state hypotheses contents are given by some
external process, and the task is to predict a proba-
bility distribution over them, where the probability
assigned to a hypothesis indicates the probability
that it is correct. It is a requirement that the G
hypotheses are disjoint; with the special ?every-
thing else? meta-hypothesis, exactly one hypoth-
esis is correct by construction. After the dialog
state tracker has output its distribution, this distri-
bution is passed to a separate, downstream process
that chooses what action to take next (e.g., how to
respond to the user).
Dialog state tracking can be seen an analogous
to assigning a probability distribution over items
on an ASR N-best list given speech input and the
recognition output, including the contents of the
N-best list. In this task, the general features de-
scribe the recognition overall (such as length of
utterance), and the hypothesis-specific features de-
scribe each N-best entry (such as decoder cost).
? Work done while at Microsoft Research
466
Another analogous task is assigning a probabil-
ity distribution over a set of URLs given a search
query and the URLs. Here, general features de-
scribe the whole set of results, e.g., number of
words in the query, and hypothesis-specific fea-
tures describe each URL, e.g., the fraction of
query words contained in page.
For dialog state tracking, most commercial sys-
tems use hand-crafted heuristics, selecting the
SLU result with the highest confidence score,
and discarding alternatives. In contrast, statisti-
cal approaches compute a posterior distribution
over many hypotheses for the dialog state. The
key insight is that dialog is a temporal process in
which correlations between turns can be harnessed
to overcome SLU errors. Statistical state track-
ing has been shown to improve task completion
in end-to-end spoken dialog systems (Bohus and
Rudnicky (2006); Young et al (2010); Thomson
and Young (2010)).
Two types of statistical state tracking ap-
proaches have been proposed. Generative ap-
proaches (Horvitz and Paek (1999); Williams and
Young (2007); Young et al (2010); Thomson and
Young (2010)) use generative models that capture
how the SLU results are generated from hidden
dialog states. These models can be used to track
an arbitrary number of state hypotheses, but can-
not easily incorporate large sets of potentially in-
formative features (e.g. from ASR, SLU, dialog
history), resulting in poor probability estimates.
As an illustration, in Figure 1, a generative model
might fail to assign the highest score to the correct
hypothesis (61C) after the second turn. In contrast,
discriminative approaches use conditional mod-
els, trained in a discriminative fashion (Bohus and
Rudnicky (2006)) to directly estimate the distribu-
tion over a set of state hypotheses based on a large
set of informative features. They generally pro-
duce more accurate distributions, but in their cur-
rent form they can only track a handful of state hy-
potheses. As a result, the correct hypothesis may
be discarded: for instance, in Figure 1, a discrim-
inative model might consider only the top 2 SLU
results, and thus fail to consider the correct 61C
hypothesis at all.
The main contribution of this paper is to de-
velop a new discriminative model for dialog state
tracking that can operate over an arbitrary number
of hypotheses and still compute accurate proba-
bility estimates. We also explore the relative im-
portance of different feature sets for this task, and
measure the amount of data required to reliably
train our model.
2 Data and experimental design
We use data from the public deployment of two
systems in the Spoken Dialog Challenge (Black
et al (2010)) which provide bus schedule infor-
mation for Pittsburgh, USA. The systems, DS1
and DS2, were fielded by AT&T, and are de-
scribed in Williams et al (2010) and Williams
(2012). Both systems followed a highly directed
flow, separately collecting 5 slots. All users were
asked for their bus route, origin, and destination;
then, they were optionally prompted for a date and
time. Each slot was explicitly or implicitly con-
firmed before collecting the next. At the end, bus
times were presented. The two systems differed in
acoustic models, confidence scoring model, state
tracking method and parameters, number of sup-
ported routes (8 vs 40, for DS1 and DS2 respec-
tively), presence of minor bugs, and user popu-
lation. These differences yield distinctions in the
distributions in the two corpora (Williams (2012)).
In both systems, a dialog state hypothesis con-
sists of a value of the user?s goal for a certain
slot: for example, a state hypothesis for the origin
slot might be ?carnegie mellon university?. The
number G of state hypotheses (e.g. slot values)
observed so far depends on the dialog, and turn
within that dialog. For instance, in Fig. 1, G pro-
gressively takes values 3, 5 and 5. Dialog state
hypotheses with identical contents (e.g., the same
bus route) are merged. The correctness of the SLU
results was manually labeled by professional an-
notators.
2.1 Experimental setup
To perform a comparative analysis of various state
tracking algorithms, we test them offline, i.e., by
re-running state tracking against the SLU results
from deployment. However, care must be taken:
when the improved state-tracker is installed into a
dialog system and used to drive action selection,
the distribution of the resulting dialog data (which
is an input for the state tracker) will change. In
other words, it is known a priori that the train
and test distributions will be mismatched. Hence,
when conducting offline experiments, if train and
test data were drawn from the same matched dis-
tribution, this may overstate performance.
467
Figure 1: Overview of dialog state tracking. In this example, the dialog state contains the user?s desired
bus route. At each turn, the system produces a spoken output. The user?s spoken response is processed
to extract a set of spoken language understanding (SLU) results, each with a local confidence score. A
set of G dialog state hypotheses is formed by considering all SLU results observed so far, including
the current turn and all previous turns. For each state hypothesis, a feature extractor produces a set of
M hypothesis-specific features, plus a single set of K general features that describes the current dialog
context. The dialog state tracker uses these features to produce a distribution over theG state hypotheses,
plus a meta-hypothesis rest which accounts for the possibility that none of the G hypotheses are correct.
dataset train set test set
MATCH1 half calls from DS2 remaining calls in DS2
MATCH2 half calls from DS1,
half from DS2
remaining calls from
DS1 and DS2
MISMATCH all calls from DS1 all calls from DS2
Table 1: Train-test data splits
To account for this effect, we explicitly study
train/test mismatch through three partitions of data
from DS1 and DS2 (see Table 1): MATCH1 con-
tains matched train/test data from the DS2 dataset;
MATCH2 contains matched train/test data from
both datasets; finally, MISMATCH contains mis-
matched train/test data. While the MISMATCH
condition may not identically replicate the mis-
match observed from deploying a new state tracker
online (since online characteristics depend on user
behavior) training on DS1 and testing on DS2 at
least ensures the presence of some real-world mis-
match.
We assess performance via two metrics: accu-
racy and L2 norm. Accuracy indicates whether the
state hypothesis with the highest assigned proba-
bility is correct, where rest is correct iff none of
the SLU results prior to the current turn include the
user?s goal. High accuracy is important as a dialog
system must ultimately commit to a single inter-
pretation of the user?s needs ? e.g., it must commit
to a route in order to provide bus timetable infor-
mation. In addition, the L2 norm (or Brier score,
Murphy (1973)) also captures how well calibrated
the output probabilities are, which is crucial to de-
cision theoretic methods for action selection. The
L2 norm is computed between the output poste-
rior and the ground-truth vector, which has 1 in
the position of the correct item and 0 elsewhere.
Both metrics are computed for each slot in each
turn, and reported by averaging across all turns
and slots.
468
2.2 Hand-crafted baseline state tracker
As a baseline, we construct a hand-crafted state
tracking rule that follows a strategy common in
commercial systems: it returns the SLU result
with the maximum confidence score, ignoring all
other hypotheses. Although this is very a simple
rule, it is very often effective. For example, if the
user says ?no? to an explicit confirmation or ?go
back? to an implicit confirmation, they are asked
the same question again, which gives an opportu-
nity for a higher confidence score. Of the G pos-
sible hypotheses for a slot, we denote the number
actually assigned a score by a model as G?, so in
this heuristic baseline G? = 1.
The performance of this baseline (BASELINE
in Table 3) is relatively strong because the top
SLU result is by far most likely to be correct, and
because the confidence score was already trained
with slot-specific speech data (Williams and Bal-
akrishnan (2009), Williams (2012)). However,
this simple rule can?t make use of SLU results on
the N-best list, or statistical priors; these limita-
tions motivate the use of statistical state trackers,
introduced next.
3 Generative state tracking
Generative state tracking approaches leverage
models that describe how SLU results are gener-
ated from a hidden dialog state, denoted g. The
user?s true (unobserved) action u is conditioned on
g and the system action a via a user action model
P (u|g, a), and also on the observed SLU result
u? via a model of how SLU results are generated
P (u?|u). Given a prior distribution b(g) and a re-
sult u?, an updated distribution b?(g) can be com-
puted by summing over all hidden user actions u:
b?(g) = ?
?
u
P (u?|u) ? P (u|g, a)b(g) (1)
where ? is a normalizing constant (Williams and
Young (2007)). Generative approaches model the
posterior over all possible dialog state hypotheses,
including those not observed in the SLU N-best
lists. In general this is computationally intractable
because the number of states is too large. One ap-
proach to scaling up is to group g into a few par-
titions, and to track only states suggested by ob-
served SLU results (Young et al (2010); Williams
(2010); Gas?ic? and Young (2011)). Another ap-
proach is to factor the components of a dialog
state, make assumptions about conditional inde-
pendence between the components, and apply ap-
proximate inference techniques such as loopy be-
lief propagation (Thomson and Young (2010)).
In deployment, DS1 and DS2 used the AT&T
Statistical Dialog Toolkit (ASDT) for dialog state
tracking (Williams (2010); AT&T Statistical Dia-
log Toolkit). ASDT implements a generative up-
date of the form of Eq 1, and uses partitions to
maintain tractability. Component models were
learned from dialog data from a different dia-
log system. A maximum of G? = 20 state hy-
potheses were tracked for each slot. The per-
formance (GENONLINE in Table 3), was worse
than BASELINE: an in-depth analysis attributed
this to the mismatch between train and test data
in the component models, and to the underlying
flawed assumption of eq. 1 that observations at
different turns are independent conditioned on the
dialog state ? in practice, confusions made by
speech recognition are highly correlated (Williams
(2012)).
For all datasets, we re-estimated the models on
the train set and re-ran generative tracking with
an unlimited number of partitions (i.e., G? = G);
see GENOFFLINE in Table 3. The re-estimated
tracker improved accuracy in MATCH conditions,
but degraded accuracy in the MISMATCH condi-
tion. This can be partly attributed to the difficulty
in estimating accurate initial priors b(g) for MIS-
MATCH, where the bus route, origin, and destina-
tion slot values in train and test systems differed
significantly.
4 Discriminative State Tracking:
Preliminaries and existing work
In contrast to generative models, discriminative
approaches to dialog state tracking directly predict
the correct state hypothesis by leveraging discrim-
inatively trained conditional models of the form
b(g) = P (g|f), where f are features extracted
from various sources, e.g. ASR, SLU, dialog his-
tory, etc. In this work we will use maximum en-
tropy models. We begin by briefly introducing
these models in the next subsection. We then de-
scribe the features used, and finally review exist-
ing discriminative approaches for state tracking
which serve as a starting point for the new ap-
proach we introduce in Section 5.
469
4.1 Maximum entropy models
The maximum entropy framework (Berger et al
(1996)) models the conditional probability distri-
bution of the label y given features x, p(y|x) via
an exponential model of the form:
P (y|x, ?) = exp(
?
i?I ?i?i(x, y))?
y?Y exp(
?
i?I ?i?i(x, y))
(2)
where ?i(x, y) are feature functions jointly de-
fined on features and labels, and ?i are the model
parameters. The training procedure optimizes the
parameters ?i to maximize the likelihood over the
data instances subject to regularization penalties.
In this work, we optimize the L1 penalty using a
cross-validation process on the train set, and we
use a fixed L2 penalty based on heuristic based on
the dataset size. The same optimization is used for
all models.
4.2 Features
Discriminative approaches for state tracking rely
on informative features to predict the correct di-
alog state. In this work we designed a set of
hypothesis-specific features that convey informa-
tion about the correctness of a particular state hy-
pothesis, and a set of general features that convey
information about the correctness of the rest meta-
hypothesis.
Hypothesis-specific features can be grouped
into 3 categories: base, history and confusion fea-
tures. Base features consider information about
the current turn, including rank of the current SLU
result (current hypothesis), the SLU result confi-
dence score(s) in the current N-best list, the differ-
ence between the current hypothesis score and the
best hypothesis score in the current N-best list, etc.
History features contain additional useful informa-
tion about past turns. Those include the number of
times an SLU result has been observed before, the
number of times an SLU result has been observed
before at a specific rank such as rank 1, the sum
and average of confidence scores of SLU results
across all past recognitions, the number of possi-
ble past user negations or confirmations of the cur-
rent SLU result etc.
Confusion features provide information about
likely ASR errors and confusability. Some recog-
nition results are more likely to be incorrect than
others ? background noise tends to trigger certain
results, especially short bus routes like ?p?. More-
over, similar sounding phrases are more likely to
be confused. The confusion features were com-
puted on a subset of the training data. For each
SLU result we computed the fraction of the time
that the result was correct, and the binomial 95%
confidence interval for that estimate. Those two
statistics were pre-computed for all SLU results
in the training data subset, and were stored in a
lookup table. At runtime, when an SLU hypoth-
esis is recognized, its statistics from this lookup
table are used as features. Similar statistics were
computed for prior probability of an SLU result
appearing on an N-best list, and prior probability
of SLU result appearance at specific rank positions
of an N-best list, prior probability of confusion be-
tween pairs of SLU results, and others.
General features provide aggregate information
about dialog history and SLU results, and are
shared across different SLU results of an N-best
list. For example, from the current turn, we use
the number of distinct SLU results, the entropy
of the confidence scores, the best path score of
the word confusion network, etc. We also include
features that contain aggregate information about
the sequence of all N-best lists up to the current
turn, such as the mean and variance of N-best list
lengths, the number of distinct SLU results ob-
served so far, the entropy of their corresponding
confidence scores, and others.
We denote the number of hypothesis-specific
features as M , and the number of general features
asK. K andM are each in the range of 100?200,
although M varies depending on whether history
and confusion features are included. For a given
dialog turn with G state hypotheses, there are a to-
tal of G ?M +K distinct features.
4.3 Fixed-length discriminative state
tracking
In past work, Bohus and Rudnicky (2006) intro-
duced discriminative state tracking, casting the
problem as standard multiclass classification. In
this setup, each turn constitutes one data instance.
Since in dialog state tracking the number of state
hypotheses varies across turns, Bohus and Rud-
nicky (2006) chose a subset of G? state hypothe-
ses to score. In this work we used a similar
setup, where we considered the top G1 SLU re-
sults from the current N-best list at turn t, and the
top G2 and G3 SLU results from the previous N-
best lists at turns t ? 1 and t ? 2. The problem
can then be formulated as multiclass classification
470
over G?+1 = G1+G2+G3+1 classes, where the
correct class indicates which of these hypotheses
(or rest) is correct. We experimented with differ-
ent values and found that G1 = 3, G2 = 2, and
G3 = 1 (G? = 6) yielded the best performance.
Feature functions are defined in the standard
way, with one feature function ? and weight ? for
each (feature,class) pair. Formally, ? of eq. 2 is
defined as ?i,j(x, y) = xi?(y, j), where ?(y, j) =
1 if y = j and 0 otherwise. i indexes over the
G?M +K features and j over the G? + 1 classes.1
The two-dimensional subscript i, j if used for clar-
ity of notation, but is otherwise identical in role to
the one-dimension subscript i in Eq 2. Figure 2 il-
lustrates the relationship between hypotheses and
weights.
Results are reported as DISCFIXED in Table 3.
In the MATCH conditions, performance is gener-
ally higher than the other baselines, particularly
when confusion features are included. In the MIS-
MATCH condition, performance is worse that the
BASELINE.
A strength of this approach is that it enables
features from every hypothesis to independently
affect every class. However, the total number
of feature functions (hence weights to learn) is
(G? + 1) ? (G?M +K), which increases quadrat-
ically with the number of hypotheses considered
G?. Although regularization can help avoid over-
fitting per se, it becomes a more challenging task
with more features. Learning weights for each
(feature,class) pair has the drawback that the ef-
fect of hypothesis-specific features such as confi-
dence have to be learned separately for every hy-
pothesis. Also, although we know in advance that
posteriors for a dialog state hypothesis are most
dependent on the features corresponding to that
hypothesis, in this approach the features from all
hypotheses are pooled together and the model is
left to discover these correspondences via learn-
ing. Furthermore, items lower down on the SLU
N-best list are much less likely to be correct: an
item at a very deep position (say 19) might never
be correct in the training data ? when this occurs,
it is unreasonable to expect posteriors to be esti-
mated accurately.
As a result of these issues, in practice G? is lim-
ited to being a small number ? here we found that
increasing G? > 6 degraded performance. Yet with
1Although in practice, maximum entropy model con-
straints render weights for one class redundant.
G? = 6, we found that in 10% of turns, the correct
state hypothesis was present but was being dis-
carded by the model, which substantially reduces
the upper-bound on tracker performance. In the
next section, we introduce a novel discriminative
state tracking approach that addresses the above
limitations, and enables jointly considering an ar-
bitrary number of state hypotheses, by exploiting
the structure inherent in the dialog state tracking
problem.
5 Dynamic discriminative state tracking
The key idea in the proposed approach is to use
feature functions that link hypothesis-specific fea-
tures to their corresponding dialog state hypoth-
esis. This approach makes it straightforward to
model relationships such as ?higher confidence for
an SLU result increases the probability of its cor-
responding state hypothesis being correct?. This
formulation also decouples the number of models
parameters (i.e. weights to learn) from the number
of hypotheses considered, allowing an arbitrary
number of dialog states hypotheses to be scored.
Figure 2: The DISCFIXED model is a traditional
maximum entropy model for classification. Every
feature in every hypothesis is linked to every hy-
pothesis, requiring (G?+ 1)(G?M +K) weights.
We begin by re-stating how features are in-
dexed. Recall each dialog state hypothesis has M
hypothesis-specific features; for each hypothesis,
we concatenate these M features with the K gen-
eral features, which are identical for all hypothe-
ses. For the meta-hypothesis rest, we again con-
catenateM+K features, where theM hypothesis-
specific features take special undefined values. We
write xgi to refer to the ith feature of hypothesis g,
where i ranges from 1 to M +K and g from 1 to
G+ 1.
471
Figure 3: The DISCDYN model presented in this
paper exploits the structure of the state tracking
problem. Features are linked to only their own
hypothesis, and weights are shared across all hy-
potheses, requiring M +K weights.
algorithm description
BASELINE simple hand-crafted rule
GENONLINE generative update, in deployed system
GENOFFLINE generative update, re-trained and run offline
DISCFIXED discr. fixed size multiclass (7 classes)
DISCDYN1 discr. joint dynamic estimation
DISCDYN2 discr. joint dynamic estimation, using indicator
encoding of ordinal features
DISCDYN3 discr. joint dynamic estimation, using indicator
encoding and ordinal-ordinal conjunctions
DISCIND discr. separate estimation
Table 2: Description of the various implemented
state tracking algorithms
The model is based on M + K feature func-
tions. However, unlike in traditional maximum
entropy models such as the fixed-position model
above, these features functions are dynamically
defined when presented with each turn. Specif-
ically, for a turn with G hypotheses, we define
?i(x, y = g) = xgi , where y ranges over the
set of possible dialog states G + 1 (and as above
i ? 1 . . .M +K). The feature function ?i is dy-
namic in that the domain of y ? i.e., the number of
dialog state hypotheses to score ? varies from turn
to turn. With feature functions defined this way,
standard maximum entropy optimization is then
applied to learn the corresponding set of M + K
weights, denoted ?i. Fig. 3 shows the relationship
of hypotheses and weights.
In practice, this formulation ? in which general
features are duplicated across every dialog state
hypothesis ? may require some additional feature
engineering: for every hypothesis g and general
feature i, the value of that general feature xgi will
be multiplied by the same weight ?i. The result
is that any setting of ?i affects all scores identi-
cally, with no net change to the resulting poste-
rior. Nonetheless, general features do contain use-
ful information for state tracking; to make use of
them, we add conjunctions (combinations) of gen-
eral and hypothesis-specific features.
We use 3 different feature variants. In DIS-
CDYN1, we use the original feature set, ignor-
ing the problem described above (so that the gen-
eral features contribute no information), result-
ing in M + K weights. DISCDYN2 adds indi-
cator encodings of the ordinal-valued hypothesis-
specific features. For example, rank is encoded
as a vector of boolean indicators, where the first
indicator is nonzero if rank = 1, the second is
nonzero if rank = 2, and the third if rank ?
3. This provides a more detailed encoding of
the ordinal-valued hypothesis-specific features, al-
though it still ignores information from the gen-
eral features. This encoding increases the number
of weights to learn to about 2(M +K).
Finally, DISCDYN3 extends DISCDYN2 by in-
cluding conjunctions of the ordinal-valued general
features with ordinal-valued hypothesis-specific
features. For example, if the 3-way hypothesis-
specific indicator feature for rank described above
were conjoined with a 4-way general indicator
feature for dialog state, the result would be an in-
dicator of dimension 3 ? 4 = 12. This expansion
results in approximately 10(M + K) weights to
learn in DISCDYN3.2
For comparison, we also estimated a simpler
alternative model, called DISCIND. This model
consists of 2 binary classifiers: the first one
scores each hypothesis in isolation, using the M
hypothesis-specific features for that hypothesis +
the K general features for that turn, and outputs a
(single) probability that the hypothesis is correct.
For this classifier, each hypothesis (not each turn)
defines a data instance. The second binary clas-
sifier takes the K general features, and outputs a
probability that the rest meta-hypothesis is correct.
For this second classifier, each turn defines one
data instance. The output of these two models is
then calibrated with isotonic regression (Zadrozny
and Elkan (2002)) and normalized to generate the
posterior over all hypotheses.
2We explored adding all possible conjunctions, including
real-valued features, but this increased memory and computa-
tional requirements dramatically without performance gains.
472
Metric Accuracy (larger numbers better) L2 (smaller numbers better)
Dataset MATCH1 MATCH2 MISMATCH MATCH1 MATCH2 MISMATCH
Features b bc bch b bc bch b bc bch b bc bch b bc bch b bc bch
BASELINE 61.5 61.5 61.5 63.4 63.4 63.4 62.5 62.5 62.5 27.1 27.1 27.1 25.5 25.5 25.5 27.3 27.3 27.3
GENONLINE 54.4 54.4 54.4 55.8 55.8 55.8 54.8 54.8 54.8 34.8 34.8 34.8 32.0 32.0 32.0 34.8 34.8 34.8
GENOFFLINE 57.1 57.1 57.1 60.1 60.1 60.1 51.8 51.8 51.8 37.6 37.6 37.6 33.4 33.4 33.4 42.0 42.0 42.0
DISCFIXED 61.9 66.7 65.3 63.6 69.7 68.8 59.1 61.9 59.3 27.2 23.6 24.4 25.8 21.9 22.4 28.9 27.8 27.8
DISCDYN1 62.0 70.9 71.1 64.4 72.4 72.9 59.4 61.8 62.3 26.3 21.3 20.9 25.0 20.4 20.1 27.7 26.3 25.9
DISCDYN2 62.6 71.3 71.5 65.7 72.1 72.2 61.9 63.2 63.1 26.3 21.4 21.2 24.4 20.5 20.4 26.9 25.8 25.4
DISCDYN3 63.6 70.1 70.9 65.9 72.1 70.7 60.7 62.1 62.9 26.2 21.5 21.4 24.3 20.6 20.7 27.1 25.9 26.1
DISCIND 62.4 69.8 70.5 63.4 71.5 71.8 59.9 63.3 62.2 26.7 23.3 22.5 25.7 21.8 20.7 28.4 27.3 28.8
Table 3: Performance of the different algorithms on each dataset using three feature combinations. Base
features are denoted as b, ASR/SLU confusion features as c and history features as h. Performance for
the feature combinations bh is omitted for space; it is between b and bc.
6 Results and discussion
The implemented state tracking methods are sum-
marized in Table 2, and our results are presented in
Table 3. These results suggest several conclusions.
First, discriminative approaches for state track-
ing broadly outperform generative methods. Since
discriminative methods incorporate many features
and are trained directly to optimize performance,
this is perhaps unsurprising for the MATCH con-
ditions. It is interesting that discriminative meth-
ods are also superior in the more realistic MIS-
MATCH setting, albeit with smaller gains. This
result suggests that discriminative methods have
good promise when deployed into real systems,
where mismatch between training and test distri-
butions is expected.
Second, the dynamic discriminative DISCDYN
models also outperformed the fixed-length dis-
criminative methods. This shows the benefit of
a model which can score every dialog state hy-
potheses, rather than a fixed subset. Third, the
three variants of the DISCDYN model, which pro-
gressively contain more detailed feature encoding
and conjunctions, perform similarly. This suggests
that a relatively simple encoding is sufficient to
achieve good performance, as the feature indica-
tors and conjunctions present in DISCDYN2 and
DISCDYN3 give only a small additional increase.
Among the discriminative models, the jointly-
optimized DISCDYN versions also slightly out-
perform the simpler, independently-optimized DI-
SCIND version. This is to be expected, for two rea-
sons: first, DISCIND is trained on a per-hypothesis
basis, while the DISCDYN models are trained on a
per-turn basis, which is the true performance met-
ric. For example, some turns have 1 hypothesis
and others have 100, but DISCIND training counts
all hypotheses equally. Second, model parameters
in DISCIND are trained independently of compet-
ing hypotheses. However, they should rather be
adjusted specifically so that the correct item re-
ceives a larger score than incorrect items ? not
merely to increase scores for correct items and de-
crease scores for incorrect items in isolation ? and
this is what is done in the DISCDYN models.
The analysis of various feature sets indicates
that the ASR/SLU error correlation (confusion)
features yield the largest improvement ? c.f. fea-
ture set bc compared to b in Table 3. The im-
provement is smallest for MISMATCH, which un-
derscores the challenges of mismatched train and
test conditions during a realistic runtime scenario.
Note, however, that we have constructed a highly
mismatched case where we train on DS1 (that sup-
ports just 8 routes) and test on DS2 (that supports
40 routes). Therefore, many route, origin and des-
tination slot values in the test data do not appear
in the training data. Hence, it is unsurprising that
the positive effect of confusion features would de-
crease.
While Table 3 shows performance measures av-
eraged across all turns, Table 4 breaks down per-
formance measures by slot, using the full feature
set bch and the realistic MISMATCH dataset. Re-
sults here show a large variation in performance
across the different slots. For the date and time
slots, there is an order of magnitude less data than
for the other slots; however performance for dates
is quite good, whereas times is rather poor. We
believe this is because the SLU confusion features
can be estimated well for slots with small cardinal-
ities (there are 7 possible values for the day), and
less well for slots with large cardinalities (there are
24 ? 60 = 1440 possible time values). This sug-
473
Accuracy (larger numbers better)
algorithms rout origin dest. date time
BASELINE 53.81 66.49 67.78 71.88 52.32
GENONLINE 50.02 54.11 59.05 75.78 35.02
GENOFFLINE 48.12 58.82 58.98 72.66 20.25
DISCFIXED 52.83 67.81 70.67 71.88 33.34
DISCDYN1 54.28 68.24 68.53 79.69 40.51
DISCDYN2 56.18 68.42 70.10 80.47 40.51
DISCDYN3 54.52 66.24 67.96 82.81 43.04
DISCIND 54.25 68.84 70.79 78.13 38.82
L2 metric (smaller numbers better)
algorithms route origin dest. date time
BASELINE 33.15 24.67 24.68 21.61 32.35
GENONLINE 35.50 35.10 31.13 19.86 52.58
GENOFFLINE 46.42 35.73 37.76 19.97 70.30
DISCFIXED 34.09 23.92 23.35 17.59 40.15
DISCDYN1 31.30 23.01 23.07 15.29 37.02
DISCDYN2 30.53 22.40 22.74 13.58 37.59
DISCDYN3 31.58 23.86 23.68 13.93 37.52
DISCIND 36.50 23.45 23.41 15.20 45.43
Table 4: Performance per slot on dataset MIS-
MATCH using the full feature set bch.
(a) MISMATCH dataset (b) MATCH2 dataset
Figure 4: Accuracy vs. amount of training data
gests that the amount of data required to estimate a
good model may depend on the cardinality of slot
values.
Finally, in Figure 4 we show how performance
varies with different amounts of training data for
the MATCH2 and MISMATCH datasets, where the
full training set size is approximately 5600 and
4400 turns, respectively. In both cases asymptotic
performance is reached after about 2000 turns, or
about 150 dialogs. This is particularly encour-
aging, as it suggests models could be learned or
adapted online with relatively little data, or could
even be individually tailored to particular users.
7 Conclusion and Future Work
Dialog state tracking is crucial to the successful
operation of spoken dialog systems. Recently de-
veloped statistical approaches are promising as
they fully utilize the dialog history, and can in-
corporate priors from past usage data. However,
existing methodologies are either limited in their
accuracy or their coverage, both of which hamper
performance.
In this paper, we have introduced a new model
for discriminative state tracking. The key idea is to
exploit the structure of the problem, in which each
dialog state hypothesis has features drawn from
the same set. In contrast to past approaches to dis-
criminative state tracking which required a num-
ber of parameters quadratic in the number of state
hypotheses, our approach uses a constant number
of parameters, invariant to the number of state hy-
potheses. This is a crucial property that enables
generalization and dealing with an unlimited num-
ber of hypotheses, overcoming a key limitation in
previous models.
We evaluated the proposed method and com-
pared it to existing generative and discrimina-
tive approaches on a corpus of real-world human-
computer dialogs chosen to include a mismatch
between training and test, as this will be found
in deployments. Results show that the proposed
model exceeds both the accuracy and probabil-
ity quality of all baselines when using the rich-
est feature set, which includes information about
common ASR confusions and dialog history. The
model can be trained efficiently, i.e. only about
150 training dialogs are necessary.
The next step is to incorporate this approach
into a deployed dialog system, and use the esti-
mated posterior over dialog states as input to the
action selection process. In future, we also hope
to explore unsupervised online adaptation, where
the trained model can be updated as test data is
processed.
Acknowledgments
We thank Patrick Nguyen for helpful discussions
regarding maximum entropy modeling and fea-
ture functions for handling structured and dynamic
output classification problems.
References
AT&T Statistical Dialog Toolkit. AT&T Statistical
Dialog Toolkit. http://www2.research.
att.com/sw/tools/asdt/, 2013.
Adam Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. A maximum entropy approach
to natural language processing. Computational
Linguistics, 22:39?71, 1996.
474
Alan W. Black, S. Burger, B. Langner, G. Par-
ent, and M. Eskenazi. Spoken dialog challenge
2010. In Proc. of Workshop on Spoken Lan-
guage Technologies (SLT), 2010.
Dan Bohus and Alex Rudnicky. A k hypothe-
ses + other belief updating model. In Proc.
of AAAI Workshop on Statistical and Empirical
Approaches to Spoken Dialog Systems, 2006.
Milica Gas?ic? and Steve Young. Effective handling
of dialogue state in the hidden information state
pomdp dialogue manager. ACM Transactions
on Speech and Language Processing, 7, 2011.
Eric Horvitz and Tim Paek. A computational ar-
chitecture for conversation. In Proc. of the 7th
Intl. Conf. on User Modeling, 1999.
Allan H Murphy. A new vector partition of the
probability score. Journal of Applied Meteorol-
ogy, 12:595?600, 1973.
Blaise Thomson and Steve Young. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588, 2010.
Jason D. Williams. Incremental partition recombi-
nation for efficient tracking of multiple dialogue
states. In Proc. of ICASSP, 2010.
Jason D. Williams. Challenges and opportuni-
ties for state tracking in statistical spoken dialog
systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Pro-
cessing, Special Issue on Advances in Spoken
Dialogue Systems and Mobile Interface, 6(8):
959?970, 2012.
Jason D. Williams and Suhrid Balakrishnan. Esti-
mating probability of correctness for asr n-best
lists. In Proc. SigDial Conference, 2009.
Jason D. Williams and Steve Young. Partially ob-
servable markov decision processes for spoken
dialog systems. Computer Speech and Lan-
guage, 21:393?422, 2007.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. Demonstration of AT&T Let?s Go: A
production-grade statistical spoken dialog sys-
tem. In Proc of Workshop on Spoken Language
Technologies (SLT), 2010.
Steve Young, Milica Gas?ic?, Simon Keizer,
Franc?ois Mairesse, Jost Schatzmann, Blaise
Thomson, and Kai Yu. The hidden informa-
tion state model: a practical framework for
POMDP-based spoken dialogue management.
Computer Speech and Language, 24(2):150?
174, 2010.
Bianca Zadrozny and Charles Elkan. Transform-
ing classifier scores into accurate multiclass
probability estimates. In Proc. of the eighth
ACM SIGKDD Intl. Conf on Knowledge Dis-
covery and Data mining, pages 694?699, 2002.
475
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 36?46,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Discovering Latent Structure in Task-Oriented Dialogues
Ke Zhai
?
Computer Science, University of Maryland
College Park, MD 20740
zhaike@cs.umd.edu
Jason D. Williams
Microsoft Research
Redmond, WA 98052
jason.williams@microsoft.com
Abstract
A key challenge for computational conver-
sation models is to discover latent struc-
ture in task-oriented dialogue, since it pro-
vides a basis for analysing, evaluating, and
building conversational systems. We pro-
pose three new unsupervised models to
discover latent structures in task-oriented
dialogues. Our methods synthesize hidden
Markov models (for underlying state) and
topic models (to connect words to states).
We apply them to two real, non-trivial
datasets: human-computer spoken dia-
logues in bus query service, and human-
human text-based chats from a live tech-
nical support service. We show that our
models extract meaningful state represen-
tations and dialogue structures consistent
with human annotations. Quantitatively,
we show our models achieve superior per-
formance on held-out log likelihood eval-
uation and an ordering task.
1 Introduction
Modeling human conversation is a fundamental
scientific pursuit. In addition to yielding ba-
sic insights into human communication, compu-
tational models of conversation underpin a host
of real-world applications, including interactive
dialogue systems (Young, 2006), dialogue sum-
marization (Murray et al, 2005; Daum?e III and
Marcu, 2006; Liu et al, 2010), and even medi-
cal applications such as diagnosis of psychological
conditions (DeVault et al, 2013).
Computational models of conversation can be
broadly divided into two genres: modeling and
control. Control is concerned with choosing ac-
tions in interactive settings?for example to maxi-
mize task completion?using reinforcement learn-
?
Work done at Microsoft Research.
ing (Levin et al, 2000), supervised learning (Hur-
tado et al, 2010), hand-crafted rules (Larsson and
Traum, 2000), or mixtures of these (Henderson
and Lemon, 2008). By contrast, modeling?the
genre of this paper?is concerned with inferring
a phenomena in an existing corpus, such as di-
alogue acts in two-party conversations (Stolcke
et al, 2000) or topic shifts in multi-party dia-
logues (Galley et al, 2003; Purver et al, 2006;
Hsueh et al, 2006; Banerjee and Rudnicky, 2006).
Many past works rely on supervised learning or
human annotations, which usually requires man-
ual labels and annotation guidelines (Jurafsky et
al., 1997). It constrains scaling the size of training
examples, and application domains. By contrast,
unsupervised methods operate only on the observ-
able signal (e.g. words) and are estimated with-
out labels or their attendant limitations (Crook et
al., 2009). They are particularly relevant because
conversation is a temporal process where models
are trained to infer a latent state which evolves as
the dialogue progresses (Bangalore et al, 2006;
Traum and Larsson, 2003).
Our basic approach is to assume that each ut-
terance in the conversation is in a latent state,
which has a causal effect on the words the conver-
sants produce. Inferring this model yields basic
insights into the structure of conversation and also
has broad practical benefits, for example, speech
recognition (Williams and Balakrishnan, 2009),
natural language generation (Rieser and Lemon,
2010), and new features for dialogue policy opti-
mization (Singh et al, 2002; Young, 2006).
There has been limited past work on unsuper-
vised methods for conversation modeling. Choti-
mongkol (2008) studies task-oriented conversa-
tion and proposed a model based on a hidden
Markov model (HMM). Ritter et al (2010) ex-
tends it by introducing additional word sources,
and applies to non-task-oriented conversations?
social interactions on Twitter, where the subjects
36
discussed are very diffuse. The additional word
sources capture the subjects, leaving the state-
specific models to express common dialogue flows
such as question/answer pairs.
In this paper, we retain the underlying HMM,
but assume words are emitted using topic models
(TM), exemplified by latent Dirichlet alocation
(Blei et al, 2003, LDA). LDA assumes each word
in an utterance is drawn from one of a set of latent
topics, where each topic is a multinomial distri-
bution over the vocabulary. The key idea is that
the set of topics is shared across all states, and
each state corresponds to a mixture of topics. We
propose three model variants that link topics and
states in different ways.
Sharing topics across states is an attractive
property in task-oriented dialogue, where a sin-
gle concept can be discussed at many points in a
dialogue, yet different topics often appear in pre-
dictable sequences. Compared to past works, the
decoupling of states and topics gives our mod-
els more expressive power and the potential to be
more data efficient. Empirically, we find that our
models outperform past approaches on two real-
world corpora of task-oriented dialogues.
This paper is organized as follows: Section 2 in-
troduces two task-oriented domains and corpora;
Section 3 details three new unsupervised genera-
tive models which combine HMMs and LDA and
efficient inference schemes; Section 4 evaluates
our models qualitatively and quantitatively, and fi-
nally conclude in Section 5.
2 Data
To test the generality of our models, we study two
very different datasets: a set of human-computer
spoken dialogues in quering bus timetable (Bus-
Time), and a set of human-human text-based dia-
logues in the technical support domain (TechSup-
port). In BusTime, the conversational structure is
known because the computer followed a determin-
istic program (Williams, 2012), making it possible
to directly compare an inferred model to ground
truth on this corpus.
1
In TechSupport, there is no
known flowchart,
2
making this a realistic applica-
tion of unsupervised methods.
1
Available for download at http://research.microsoft.
com/en-us/events/dstc/
2
Technical support human agents use many types of
documentation?mainly checklists and guidelines, but in
general, there are no flowcharts.
BusTime This corpus consists of logs of tele-
phone calls between a spoken dialogue system and
real bus users in Pittsburgh, USA (Black et al,
2010). For the user side, the words logged are the
words recognized by the automatic speech recog-
nizer. The vocabulary of the recognizer was con-
strained to the bus timetable task, so only words
known to the recognizer in advance are output.
Even so, the word error rate is approximately 30-
40%, due to the challenging audio conditions of
usage?with traffic noise and extraneous speech.
The system asked users sequentially for a bus
route, origin and destination, and optionally date
and time. The system confirmed low-confidence
speech recognition results. Due to the speech
recognition channel, system and user turns always
alternate. An example dialogue is given below:
System: Say a route like ?bus-route?, or say I?m not sure.
User: ?bus-route?.
System: I thought you said ?bus-route?, is that right?
User: Yes.
System: Say where?re you leaving from, like ?location?.
User: ?location?.
System: Okay, ?location?, where are you going to?
...
We discard dialogues with fewer than 20 ut-
terances. We also map all named entities (e.g.,
?downtown? and ?28X?) to their semantic types
(resp. ?location? and ?bus-route?) to reduce vo-
cabulary size. The corpus we use consists of ap-
proximately 850 dialogue sessions or 30, 000 ut-
terances. It contains 370, 000 tokens (words or se-
mantic types) with vocabulary size 250.
TechSupport This corpus consists of logs of
real web-based human-human text ?chat? con-
versations between clients and technical support
agents at a large corporation. Usually, clients and
agents first exchange names and contact informa-
tion; after that, dialogues are quite free-form, as
agents ask questions and suggest fixes. Most dia-
logues ultimately end when the client?s issue has
been resolved; some clients are provided with a
reference number for future follow-up. An exam-
ple dialogue is given below:
Agent: Welcome to the answer desk! My name is ?agent-
name?. How can I help you today?
Agent: May I have your name, email and phone no.?
Client: Hi, ?agent-name?. I recently installed new soft-
ware but I kept getting error, can you help me?
Agent: Sorry to hear that. Let me help you with that.
Agent: May I have your name, email and phone no.?
Client: The error code is ?error-code?.
Client: It appears every time when I launch it.
Client: Sure. My name is ?client-name?.
Client: My email and phone are ?email?, ?phone?.
Agent: Thanks, ?client-name?, please give me a minute.
37
s0
w
0,i
N
0
M
s
0
M
s
0
w
0,i
N
0
s
1
M
s
0
w
0,i
N
0
s
1
w
1,i
N
1
M
s
0
w
0,i
N
0
s
1
w
1,i
N
1
...
s
n
w
n,i
N
n
M
M
(a) LM-HMM
s
0
w
0,i
N
0
s
1
...
s
n
M
w
1,i
N
1
w
n,i
N
n
r
1,i
r
0,i
r
n,i?m
? E
?m
s
0
w
0,i
N
0
s
1
...
s
n
M
w
1,i
N
1
w
n,i
N
n
r
1,i
r
0,i
r
n,i
t
m
g
E
u
m
s
0
w
0,i
N
0
s
1
M
w
1,i
N
1
r
1,i
r
0,i
t
m
g
E
u
m
s
0
w
0,i
N
0
s
1
M
r
0,i
t
m
g
E
u
m
s
0
w
0,i
N
0
s
1
M
N
1
r
1,i
r
0,i
t
m
g
E
u
m
s
0
N
0
M
r
0,i
t
m
g
E
u
m
s
0
w
0,i
N
0
M
r
0,i
t
m
g
E
u
m
M
t
m
g
E
u
m
s
0
N
0
M
t
m
g
E
u
m
(b) LM-HMMS
Figure 1: Plate diagrams of baseline models, from
existing work (Chotimongkol, 2008; Ritter et al,
2010). Variable definitions are given in the text.
...
This data is less structured than BusTime;
clients? issues span software, hardware, network-
ing, and other topics. In addition, clients use com-
mon internet short-hand (e.g., ?thx?, ?gtg?, ?ppl?,
?hv?, etc), with mis-spellings (e.g., ?ofice?, ?off-
fice?, ?erorr?, etc). In addition, chats from the web
interface are segmented into turns when a user hits
?Enter? on a keyboard. Therefore, clients? input
and agents? responses do not necessarily alternate
consecutively, e.g., an a ent?s response may take
multiple turns as in the above example. Also, it
is unreasonable to group consecutive chats from
the same party to form a ?alternating? structure
like BusTime dataset due to the asynchronism of
different states. For instance, the second block
of client inputs clearly comes from two different
states which should not be merged together.
We discard dialogues with fewer than 30 utter-
ances. We map named entities to their semantic
types, apply stemming, and remove stop words.
3
The corpus we use contains approximately 2, 000
dialogue sessions or 80, 000 conversation utter-
ances. It consists of 770, 000 tokens, with a a vo-
cabulary size of 6, 600.
3 Latent Structure in Dialogues
In this work, our goal is to infer latent structure
presented in task-oriented conversation. We as-
sume that the structure can be encoded in a prob-
abilistic state transition diagram, where the dia-
logue is in one state at each utterance, and states
have a causal effect on the words observed. We as-
sume the boundaries between utterances are given,
which is trivial in many corpora.
The simplest formulation we consider is an
HMM where each state contains a unigram lan-
guage model (LM), proposed by Chotimongkol
(2008) for task-oriented dialogue and originally
3
We used regular expression to map named entities, and
Porter stemmer in NLTK to stem all tokens.
developed for discourse analysis by Barzilay and
Lee (2004). We call it LM-HMM as in Figure 1(a).
For a corpus of M dialogues, the m-th dialogue
contains n utterances, each of which contains N
n
words (we omit index m from terms because it
will be clear from context). At n-th utterance,
we assume the dialogue is in some latent state s
n
.
Words in n-th utterance w
n,1
, . . . , w
n,N
n
are gen-
erated (independently) according to the LM. When
an utte ance is complete, the next state is drawn
according to HMM, i.e., P (s
?
|s).
While LM-HMM captures the basic intuition of
conversation structure, it assumes words are con-
ditioned only on state. Ritter et al (2010) extends
LM-HMM to allow words to be emitted from two
additional sources: the topic of current dialogue
?, or a background LM ? shared across all dia-
logues. A multinomial pi indicates the expected
fraction of words from these three sources. For
every word in an utterance, first draw a source in-
dicator r from pi, and then generate the word from
the corresponding source. We call it LM-HMMS
(Figure 1(b)). Ritter et al (2010) finds these al-
ternate sources are important in non-task-oriented
domains, where events are diffuse and fleeting.
For example, Twitter exchanges often focus on a
particular event (labeled X), and follow patterns
like ?saw X last night??, ?X was amazing?. Here
X appears throughout the dialogue but does not
help to distinguish conversational states in social
media. We also explore similar variants.
In this paper, these two models form our base-
lines. For all models, we use Markov chain Monte
Carlo (MCMC) inference (Neal, 2000) to find la-
tent variables that best fit observed data. We also
assume symmetric Dirichlet priors on all multino-
mial distributions and apply collapsed Gibbs sam-
pling. In the rest of this section, we present our
models and their inference algorithms in turn.
3.1 TM-HMM
Our approach is to modify the emission probabil-
ities of states to be distributions over topics rather
than distributions over words. In other words, in-
stead of generating words via a LM, we generate
words from a topic model (TM), where each state
maps to a mixture of topics. The key benefit of this
additional layer of abstraction is to enable states
to express higher-level concepts through pooling
of topics across states. For example, topics might
be inferred for content like ?bus-route? or ?lo-
38
s0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
T
K
?t
? k
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
T
K
h
t
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
w
1,i
N
1
z
1,i
T
K
h
t
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
T
K
h
t
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
T
K
h
t
g
k
s
0
w
0,i
N
0
M
z
0,i
T
K
h
t
g
k
s
0
N
0
M
z
0,i
T
K
h
t
g
k
s
0
M
T
K
h
t
g
k
M
T
K
h
t
g
k
(a) TM-HMM
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
?m
?m
? k
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
1,i
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
w
1,i
N
1
z
1,i
K
r
1,i
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
K
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
M
z
0,i
K
r
0,i
h
m
u
m
g
k
s
0
N
0
M
z
0,i
K
r
0,i
h
m
u
m
g
k
s
0
N
0
M
z
0,i
K
h
m
u
m
g
k
s
0
M
K
h
m
u
m
g
k
M
K
h
m
u
m
g
k
M
K
g
k
(b) TM-HMMS
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
T
? m
? k
?t
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
w
1,i
N
1
z
1,i
K
r
1,i
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
1,i
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
K
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
M
z
0,i
K
r
0,i
T
h
m
g
k
u
t
s
0
N
0
M
z
0,i
K
r
0,i
T
h
m
g
k
u
t
s
0
N
0
M
z
0,i
K
T
h
m
g
k
u
t
s
0
M
K
T
h
m
g
k
u
t
M
K
T
h
m
g
k
u
t
M
K
T
g
k
u
t
(c) TM-HMMSS
Figure 2: Plate diagrams of proposed models. TM-HMM is an HMM with state-wise topic distributions.
TM-HMMS adds session-wise topic distribution and a source generator. TM-HMMSS adds a state-wise
source generator. Variable definitions are given in the text.
cations?; and other topics for dialogue acts, like
to ?ask? or ?confirm? information. States could
then be combinations of these, e.g., a state might
express ?ask bus route? or ?confirm location?.
This approach also decouples the number of top-
ics from the number of states. Throughout this pa-
per, we denote the number of topics as K and the
number of states as T . We index words, turns and
dialogues in the same ways as baseline models.
We develop three generative models. In the first
variant (TM-HMM, Figure 2(a)), we assume every
state s in HMM is as ociated with a distribution
over topics ?, and topics generate wordsw at each
utterance. The other two models allow words to
be generated from different sources (in addition to
states), akin to the LM-HMMS model.
TM-HMM generates a dialogue as following:
1: For each utterance n in that dialogue, sample
a state s
n
based on the previous state s
n?1
.
2: For each word in utterance n, first draw a
topic z from the state-specified distribution
over topics ?
s
n
conditioned on s
n
, then gener-
ate word w from the topic-specified distribu-
tion over vocabulary ?
z
based on z.
We assume ??s and ??s are drawn from corre-
sponding Dirichlet priors, as in LDA.
The posterior distributions of state assignment
s
n
and topic assignment z
n,i
are
p(s
n
|s
?n
, z,?,?) ? p(s
n
|s
?n
,?)
? p(z
n
|s, z
?n
,?), (1)
p(z
n,i
|s,w, z
?(n,i)
,?,?) ? p(z
n,i
|s, z
?(n,i)
,?)
? p(w
n,i
|s
n
,w
?(n,i)
, z,?),
where ?, ?, ? are symmetric Dirichlet priors on
state-wise topic distribution ?
t
?s, topic-wise word
distribution ?
t
?s and state transition multinomials,
respectively. All probabilities can be computed
using collapsed Gibbs sampler for LDA (Griffiths
and Steyvers, 2004) and HMM (Goldwater and
Griffiths, 2007). We iteratively sample all param-
eters until convergence.
3.2 TM-HMMS
TM-HMMS (Figure 2(b)) extends TM-HMM to al-
low words to be generated either from state LM
(as in LM-HMM), or a set of dialogue topics
(akin to LM-HMMS). Because task-oriented dia-
logues usually focus on a specific domain, a set
of words appears repeatedly throughout a given
dialogue. Therefore, the topic distribution is of-
ten stable throughout the entire dialogue, and
does not vary from turn to turn. For example,
in the troubleshooting domain, dialogues about
network connections, desktop productivity, and
anti-virus software could each map to different
session-wide topics. To express this, words in
the TM-HMMS model are generated either from
a dialogue-specific topic distribution, or from a
state-specific language model.
4
A distribution
over sources is sampled once at the beginning of
each dialogue and selects the expected fraction of
words generated from different sources.
The generative story for a dialogue session is:
1: At the beginning of each session, draw a dis-
tribution over topics ? and a distribution over
word sources ? .
2: For each utterance n in the conversation, draw
a state s
n
based on previous state s
n?1
.
3: For each word in utterance n, first choose a
word source r according to ? , and then de-
pending on r, generate a word w either from
the session-wide topic distribution ? or the
language model specified by the state s
n
.
4
Note that a TM-HMMS model with state-specific topic
models (instead of state-specific language models) would be
subsumed by TM-HMM, since one topic could be used as the
background topic in TM-HMMS.
39
Again, we impose Dirichlet priors on distributions
over topics ??s and distributions over words ??s
as in LDA. We also assume the distributions over
sources ? ?s are governed by a Beta distribution.
The session-wide topics is slightly different
from that used in LM-HMMS: LM-HMMS was de-
veloped for social chats on Twitter where topics
are very diffuse and unlikely to repeat; hence of-
ten unique to each dialogue. By contrast, our mod-
els are designed for task-oriented dialogues which
pertain to a given domain where topics are more
tightly clustered; thus, in TM-HMMS session-wide
topics are shared across the corpus.
The posterior distributions of state assignment
s
n
, word source r
n,i
and topic assignment z
n,i
are
p(s
n
|r, s
?n
,w,?,pi) ? p(s
n
|s
?n
,?)
? p(w
n
|r, s,pi),
p(r
n,i
|r
?(n,i)
, s,w,pi) ? p(r
n,i
|r
?(n,i)
,pi)
? p(w
n,i
|r, s,w
?(n,i)
, z,?), (2)
p(z
n,i
|r,w, z
?(n,i)
,?,?) ? p(z
n,i
|r, z
?(n,i)
,?)
? p(w
n,i
|r,w
?(n,i)
, z,?),
where pi is a symmetric Dirichlet prior on session-
wise word source distribution ?
m
?s, and other
symbols are defined above. All these probabilities
are Dirichlet-multinomial distributions and there-
fore can be computed efficiently.
3.3 TM-HMMSS
The TM-HMMSS (Figure 2(c)) model modifies
TM-HMMS to re-sample the distribution over
word sources ? at every utterance, instead of once
at the beginning of each session. This modifica-
tion allows the fraction of words drawn from the
session-wide topics to vary over the course of the
dialogue. This is attractive in task-oriented di-
alogue, where some sections of the dialogue al-
ways follow a similar script, regardless of session
topic?for example, the opening, closing, or ask-
ing the user if they will take a survey. To support
these patterns, TM-HMMSS conditions the source
generator distribution on the current state.
The generative story of TM-HMMSS is very
similar to TM-HMMS, except the distribution over
word sources ? ?s are sampled at every state. A
dialogue is generated as following:
1: For each session, draw a topic distribution ?.
2: For each utterance n in the conversation, draw
a state s
n
based on previous state s
n?1
, and
subsequently retrieve the state-specific distri-
bution over word sources ?
s
n
.
3: For each word in utterance n, first sample a
word source r according to ?
s
n
, and then de-
pending on r, generate a word w either from
the session-wide topic distribution ? or the
language model specified by the state s
n
.
As in TM-HMMS, we assume multinomial distri-
butions ??s and ??s are drawn from Dirichlet pri-
ors; and ? ?s are governed by Beta distributions.
The inference for TM-HMMSS is exactly same
as the inference for TM-HMMS, except the poste-
rior distributions over word source r
n,i
is now
p(r
n,i
|r
?(n,i)
, s,w,pi) ? p(r
n,i
|r
?(n,i)
, s
n
,pi)
? p(w
n,i
|r, s,w
?(n,i)
, z,?), (3)
where the first term is integrated over all sessions
and conditioned on the state assignment.
3.4 Supporting Multiple Parties
Since our primary focus is task-oriented dia-
logues between two parties, we assume every
word source is associated with two sets of LMs?
one for system/agent and another for user/client.
This configuration is similar to PolyLDA (Mimno
et al, 2009) or LinkLDA (Yano et al, 2009), such
that utterances from different parties are treated
as different languages or blog-post and comments
pairs. In this work, we implement all models un-
der this setting, but omit details in plate diagrams
for the sake of simplicity.
In settings where the agent and client always al-
ternate, each state emits both text before transi-
tioning to the next state. This is the case in the
BusTime dataset, where the spoken dialogue sys-
tem enforces strict turn-taking. In settings where
agents or client may produce more than one utter-
ance in a row, each state emits either agent text or
client text, then transitions to the next state. This
is the case in the TechSupport corpus, where either
conversant may send a message at any time.
3.5 Likelihood Estimation
To evaluate performance across different models,
we compute the likelihood on held-out test set.
For TM-HMM model, there are no local depen-
dencies, and we therefore compute the marginal
likelihood using the forward algorithm. However,
for TM-HMMS and TM-HMMSS models, the la-
tent topic distribution ? creates local dependen-
cies, rendering computation of marginal likeli-
40
hoods intractable. Hence, we use a Chib-style
estimator (Wallach et al, 2009). Although it is
computationally more expensive, it gives less bi-
ased approximation of marginal likelihood, even
for finite samples. This ensures likelihood mea-
surements are comparable across models.
4 Experiments
In this section, we examine the effectiveness of our
models. We first evaluate our models qualitatively
by exploring the inferred state diagram. We then
perform quantitative analysis with log likelihood
measurements and an ordering task on a held-out
test set. We train all models with 80% of the en-
tire dataset and use the rest for testing. We run
the Gibbs samplers for 1000 iterations and update
all hyper-parameters using slice sampling (Neal,
2003; Wallach, 2008) every 10 iterations. The
training likelihood suggest all models converge
within 500?800 iterations. For all Chib-style esti-
mators, we collect 100 samples along the Markov
chain to approximate the marginal likelihood.
4.1 Qualitative Evaluation
Figure 3 shows the state diagram for BusTime cor-
pus inferred by TM-HMM without any supervi-
sion.
5
Every dialogue is opened by asking the user
to say a bus route, or to say ?I?m not sure.? It then
transits to a state about location, e.g., origin and
destination. Both these two states may continue
to a confirmation step immediately after. After
verifying all the necessary information, the system
asks if the user wants ?the next few buses?.
6
Oth-
erwise, the system follows up with the user on the
particular date and time information. After system
reads out bus times, the user has options to ?re-
peat? or ask for subsequent schedules.
In addition, we also include the human-
annotated dialogue flow in Figure 4 for refer-
ence (Williams, 2012). It only illustrates the most
common design of system actions, without show-
ing edge cases. Comparing these two figures, the
dialogue flow inferred by our model along the
most probable path (highlighted in bold red in Fig-
ure 3) is consistent with underlying design. Fur-
thermore, our models are able to capture edge
cases?omitted for space?through a more gen-
eral and probabilistic fashion. In summary, our
5
Recall in BusTime, state transitions occur after each pair
of system/user utterances, so we display them synchronously.
6
The system was designed this way because most users
say ?yes? to this question, obviating the date and time.
models yield a very similar flowchart to the under-
lying design in a completely unsupervised way.
7
Figure 5 shows part of the flowchart for
the TechSupport corpus, generated by the TM-
HMMSS model.
8
A conversation usually starts
with a welcome message from a customer support
agent. Next, clients sometimes report a problem;
otherwise, the agent gathers the client?s identity.
After these preliminaries, the agent usually checks
the system version or platform settings. Then, in-
formation about the problem is exchanged, and a
cycle ensues where agents propose solutions, and
clients attempt them, reporting results. Usually,
a conversation loops among these states until ei-
ther the problem is resolved (as the case shown
in the figure) or the client is left with a reference
number for future follow-up (not shown due to
space limit). Although technical support is task-
oriented, the scope of possible issues is vast and
not prescribed. The table in Figure 5 lists the top
ranked words of selected topics?the categories
clients often report problems in. It illustrates that,
qualitatively, TM-HMMSS discovers both problem
categories and conversation structures on our data.
As one of the baseline model, we also include a
part of flowchart generated by LM-HMM model
with similar settings of T = 20 states. Illus-
trated by the highlighted states in 6, LM-HMM
model conflates interactions that commonly occur
at the beginning and end of a dialogue?i.e., ?ac-
knowledge agent? and ?resolve problem?, since
their underlying language models are likely to pro-
duce similar probability distributions over words.
By incorporating topic information, our proposed
models (e.g., TM-HMMSS in Figure 5) are able to
enforce the state transitions towards more frequent
flow patterns, which further helps to overcome the
weakness of language model.
4.2 Quantitative Evaluation
In this section, we evaluate our models using log
likelihood and an ordering task on a held-out test
set. Both evaluation metrics measure the predic-
tive power of a conversation model.
7
We considered various ways of making a quantitative
evaluation of the inferred state diagram, and proved difficult.
Rather than attempt to justify a particular sub-division of each
?design states?, we instead give several straightforward quan-
titative evaluations in the next section.
8
Recall in this corpus, state transitions occur after emit-
ting each agent or client utterances, which does not necessar-
ily alternate in a dialogue, so we display client request and
agent response separately.
41
state: ask for bus route
(route:0.14), (say:0.13), (<bus-route>:0.12), (not:0.10), 
(sure:0.10), (im:0.09), (a:0.08), (bus:0.07), (like:0.06), ...
e.g.: say a bus route like <bus-route> or say i am not sure
(<bus-route>:0.7), (the:0.07), (im:0.06), (not:
0.05), (sure:0.04), (route:0.02), (any:0.01), ...
e.g.: <bus-route>/im not sure
0.53
state: confirm low-confidence speech recognition results
(right:0.19), (is:0.19), (that:0.19), (<location>:0.12), (<bus-route>:
0.05), (i:0.04), (you:0.03), (said:0.03), (thought:0.03), (over:0.03), ...
e.g.: i thought you said (<bus-route>/<location>) is that right
(yes:0.45), (no:0.3), (yeah:0.12), (wrong:0.04), 
(correct:0.03), (back:0.02), (go:0.02), (nope:0.01), ...
e.g.: yes/no/yeah/wrong/correct/go back/nope
0.12
0.32
0.53
0.15
state: ask for locations
(you:0.1), (are:0.09), (where:0.08), (to:0.07), (say:0.06), (from:0.05), 
(leaving:0.05), (going:0.05), (<location>:0.05), (okay:0.04), ...
e.g.: (okay <location>) say where are you (going to/leaving from)
(<location>:0.84), (back:
0.05), (go:0.05), ...
e.g.: <location>
0.21
0.23
0.85
0.44
0.28
state: ask if user is traveling now
(say:0.8), (the:0.07), (you:0.07), (no:0.06), (yes:0.06), (do:
0.06), (want:0.06), (buses:0.05), (few:0.05), (next:0.04), ...
e.g.: do you want the next few buses say yes or no
(yes:0.5), (no:0.17), (yeah:0.16), (<bus-route>:
0.07), (back:0.04), (go:0.04), (nope:0.01), ...
e.g.: yes/no/yeah
0.31
state: read out bus timetables
(<location>:0.08), (at:0.05), (<time>:0.05), (next:0.05), (say:0.05), (from:0.04), (there:
0.04), (<bus-route>:0.04), (to:0.04), ...
e.g.: there is a <bus-route> from <location> to <location> at <time> say next or repeat
(next:0.4), (repeat:0.16), (over:0.11), (start:0.11), 
(previous:0.07), (go:0.06), (back:0.06), (goodbye:0.05), ...
e.g.: next/repeat/start over/previous
0.12
0.42
state: ask for date and time (optional)
(<time>:0.14), (<date>:0.1), (the:0.06), (or:0.05), (like:0.05), 
(say:0.05), (you:0.05), (want:0.05), (at:0.04), (depart:0.04), ...
e.g.: say the time you want to depart like <time>
(<time>:0.26), (<date>:0.14), (m:0.11), 
(depart:0.07), (a:0.07), (at:0.07), (by:0.03), ...
e.g.: depart (at/by) <time> a m <date>
0.55
Start	 ?
I	 ?heard	 ?61C,	 ?is	 ?that	 ?right?	 ?
Downtown,	 ?is	 ?that	 ?correct?	 ?
Did	 ?you	 ?just	 ?say	 ?Norwood?	 ?
Say	 ?just	 ?the	 ?day	 ?you	 ?want.	 ?
Say	 ?just	 ?the	 ??me	 ?you	 ?want.	 ?
I'm	 ?sorry,	 ?I	 ?can't	 ?find	 ?any	 ?bus	 ?
at	 ?all	 ?that	 ?run	 ?from	 ?Milton	 ?to	 ?
Norwell.	 ?I	 ?checked	 ?route	 ?61C	 ?
and	 ?I	 ?also	 ?checked	 ?all	 ?the	 ?
other	 ?bus	 ?routes	 ?I	 ?know	 ?too.	 ?
Repeat,	 ?next,	 ?previous	 ?
At	 ?11:45	 ?PM	 ?today,	 ?
there	 ?is	 ?a	 ?61	 ?C	 ?from	 ?
5th	 ?Ave	 ?and	 ?Main	 ?St	 ?
Canton,	 ?arriving	 ?2nd	 ?St	 ?
and	 ?Grant	 ?Ave	 ?in	 ?
Norwood	 ?at	 ?12:34	 ?AM.	 ?
Say	 ?a	 ?bus	 ?route,	 ?or	 ?
say	 ?I?m	 ?not	 ?sure.	 ?
Where	 ?are	 ?you	 ?leaving	 ?
from?	 ?(query	 ?database)	 ?
Where	 ?are	 ?you	 ?going	 ?
to?	 ?(query	 ?database)	 ?
Do	 ?you	 ?want	 ??mes	 ?for	 ?
the	 ?next	 ?few	 ?buses?	 ?
(query	 ?database)	 ?
Figure 3: (Upper) Part of the flowchart inferred on Bus-
Time, by TM-HMM model with K = 10 topics and
T = 10 states. The most probable path is highlighted,
which is consistent with the underlying design (Figure 4).
Cyan blocks are system actions and yellow blocks are
user responses. In every block, the upper cell shows the
top ranked words marginalized over all topics and the
lower cell shows some examples of that state. Transition
probability cut-off is 0.1. States are labelled manually.
Figure 4: (Left) Hand-crafted reference flowchart for
BusTime (Williams, 2012). Only the most common di-
alogue flows are displayed. System prompts shown are
example paraphrases. Edge cases are not included.
Log Likelihood The likelihood metric measures
the probability of generating the test set under a
specified model. As shown in Figure 7, our models
yield as good or better likelihood than LM-HMM
and LM-HMMS models on both datasets under all
settings. For our proposed models, TM-HMMS
and TM-HMMSS perform better than TM-HMM
on TechSupport, but not necessarily on BusTime.
In addition, we notice that the marginal benefit of
TM-HMMSS over TM-HMM is greater on Tech-
Support dataset, where each dialogue focuses on
one of many possible tasks. This coincides with
our belief that topics are more conversation de-
pendent and shared across the entire corpus in cus-
tomer support data?i.e., different clients in differ-
ent sessions might ask about similar issues.
Ordering Test Ritter et al (2010) proposes an
evaluation based on rank correlation coefficient,
which measures the degree of similarity between
any two orderings over sequential data. They use
Kendall?s ? as evaluation metric, which is based
on the agreement between pairwise orderings of
two sequences (Kendall, 1938). It ranges from ?1
to +1, where +1 indicates an identical ordering
and ?1 indicates a reverse ordering. The idea is
to generate all permutations of the utterances in
a dialogue (including true ordering), and compute
the log likelihood for each under the model. Then,
Kendall?s ? is computed between the most proba-
ble permutation and true ordering. The result is the
average of ? values for all dialogues in test corpus.
Ritter et al (2010) limits their dataset by choos-
ing Twitter dialogues containing 3 to 6 posts (ut-
terances), making it tractable to enumerate all per-
mutations. However, our datasets are much larger,
and enumerating all possible permutations of dia-
logues with more than 20 or 30 utterances is infea-
sible. Instead, we incrementally build up the per-
mutation set by adding one random permutation at
a time, and taking the most probable permutation
after each addition. If this process were continued
(intractably!) until all permutations are enumer-
ated, the true value of Kendall?s ? test would be
reached. In practice, the value appears to plateau
after a few dozen measurements.
We present our results in Figure 8. Our mod-
els consistently perform as good or better than
42
Agent: conversation opening + identity check
help, answer, desk, microsoft, may, 
<agent>, welcom, name, number, phone, ...
e.g.: welcome to microsoft answer desk, i'm 
<agent>, how can i help you, may i have 
your name?
Client: report problem
tri, get, comput, cant, window, message, 
error, problem, instal, say, ...
e.g.: get problem in windows, cant install 
on computer, it says error message
Agent: conversation closure
thank, answer, microsoft, desk, <client>, 
contact, help, chat, day, welcom, ...
e.g.: thank you for contacting microsoft answer 
desk, you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, <client>, check, 
give, moment, ok, wait, ...
e.g.: thank you, <client>, please give me 
a moment, wait for a minute, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, 
<prodkey>, one, problem, fine, ...
e.g.: ok, thanks, sure, <prodkey>, 
one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, <client>, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, <href>, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click <href> and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, dont, tri, 
download, get, say, ...
e.g.: ok, i am trying to download the code
Client: identity verification
<email>, <phoneno>, <client>, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is <client>
sure, <client>, <phoneno>, <email>
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
<agent>, ye(s), day, bye, ...
e.g.: yes, thank you <agent> so much 
for your great help, good day, bye
0.240581
0.20916
0.207256
0.145682
0.131558
0.122512
0.117276
0.108547
0.1291865
0.0798235
0.0764909
0.0893519
0.0801476
0.085454
0.077694
0.088172
0.0728498
0.083912
0.0917323
0.0888547
Agent: conversation opening + identity check
help, answer, desk, microsoft, may, #agent#, welcom, 
name, number, phone, ...
e.g.: welcome to microsoft answer desk, i'm #agent#, 
how can i help you, may i have your name?
Client: report problem
tri, get, comput, cant, window, message, error, 
problem, instal, say, ...
e.g.: i get this problem in windows, cant install 
on my computer, it says this error message
Agent: conversation closure
thank, answer, microsoft, desk, #client#, 
contact, help, chat, day, welcom, ...
e.g.: thank you for contacting microsoft answer 
desk, you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, #client#, check, 
give, moment, ok, wait, ...
e.g.: thank you, #client#, please give me a 
moment, wait for a minute, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, #prodkey#, 
one, problem, fine, ...
e.g.: ok, thanks, sure, #prodkey#, one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, #client#, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, #href#, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click #href# and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, 
dont, tri, download, get, say, ...
e.g.: ok, i am trying to 
download the code, yes
Client: identity verification
#email#, #phoneno#, #client#, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is #client#
sure, #client#, #phoneno#, #email#
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
#agent#, ye(s), day, bye, ...
e.g.: yes, thank you #agent# so much 
for your great help, good day, bye
0.0240581
0.020916
0.0207256
0.0145682
0.0131558
0.0122512
0.0117276
0.0108547
0.00891865
0.00798235
0.00764909
0.00593519
0.00801476
0.00455454
0.00377694
0.00568172
0.00328498
0.0053912
0.00617323
0.00588547
Agent: conversation opening + identity check
help, answer, desk, may, <agent-name>, 
welcom, name, number, phone, ...
e.g.: welcome to answer desk, i'm <agent-
name>, how can i help you, may i have 
your name?
Client: report problem
tri, get, comput, cant, window, message, 
error, problem, instal, say, ...
e.g.: get problem in windows, cant install 
on computer, it says error message
Agent: conversation closure
thank, answer, desk, <client-name>, contact, 
help, chat, day, welcom, ...
e.g.: thank you for contacting answer desk, 
you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, <client-name>, 
check, give, moment, ok, wait, ...
e.g.: thank you, <client-name>, please 
give me a moment, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, 
<prodkey>, one, problem, fine, ...
e.g.: ok, thanks, sure, <prodkey>, 
one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, <client-name>, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, <href>, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click <href> and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, dont, tri, 
download, get, say, ...
e.g.: ok, i am trying to download the code
Client: identity verification
<email>, <phone>, <client-name>, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is <client-name>
sure, <client-name>, <phone>, <email>
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
<agent-name>, ye(s), day, bye, ...
e.g.: great, thanks <agent-name> so 
much for your help, good day, bye
0.24
0.21
0.21
0.15
0.13
0.12
0.12
0.11
0.13
0.08
0.08
0.09
0.08
0.09
0.08
0.09
0.07
0.08
0.09
0.09
Agent: conversation opening + identity check
help, answer, desk, microsoft, may, <agent-
name>, welcom, name, number, phone, ...
e.g.: welcome to microsoft answer desk, i'm 
<agent-name>, how can i help you, may i 
have your name?
Client: report problem
tri, get, comput, cant, window, message, 
error, problem, instal, say, ...
e.g.: get problem in windows, cant install 
on computer, it says error message
Agent: conversation closure
thank, answer, microsoft, desk, <client-name>, 
contact, help, chat, day, welcom, ...
e.g.: thank you for contacting microsoft answer 
desk, you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, <client-name>, 
check, give, moment, ok, wait, ...
e.g.: thank you, <client-name>, please 
give me a moment, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, 
<prodkey>, one, problem, fine, ...
e.g.: ok, thanks, sure, <prodkey>, 
one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, <client-name>, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, <href>, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click <href> and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, dont, tri, 
download, get, say, ...
e.g.: ok, i am trying to download the code
Client: identity verification
<email>, <phone>, <client-name>, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is <client-name>
sure, <client-name>, <phone>, <email>
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
<agent-name>, ye(s), day, bye, ...
e.g.: great, thanks <agent-name> so 
much for your help, good day, bye
0.24
0.21
0.21
0.15
0.13
0.12
0.12
0.11
0.13
0.08
0.08
0.09
0.08
0.09
0.08
0.09
0.07
0.08
0.09
0.09
Topic Top Ranked Words
p
u
r
c
h
a
s
e
microsoft, store, purchas, able, get,
sir, order, site, mr, contact, mac, . . .
b
r
o
w
s
e
r
internet, explor, browser, ie, open,
websit, googl, download, click,
chrome, . . .
b
a
c
k
u
p
file, restor, system, comput, back,
folder, creat, option, dont, delet, . . .
b
o
o
t
comput, boot, mode, option, disc,
safe, recoveri, repair, back, clean,
cd, disk, . . .
u
p
d
a
t
e
updat, window, servic, instal, pack,
run, comput, download, check,
restart, inform, system, error, fix, . . .
n
e
t
w
o
r
k
connect, internet, printer, comput,
network, pc, print, access, wireless,
hp, cable, adapt, router, speed, . . .
a
n
t
i
-
v
i
r
u
s viru, scan, comput, remov, secur,
run, system, anti, essenti, infect, de-
fend, softwar, program, protect, an-
tiviru, malwar, . . .
h
a
r
d
w
a
r
e
driver, devic, drive, dvd, cd, hard-
war, issu, model, laptop, plug, soft-
ware, usb, . . .
w
i
n
d
o
w
s window, upgrad, 8, download, 7, in-
stal, bit, vista, pro, system, . . .
o
f
fi
c
e
offic, 2010, word, microsoft, home,
excel, version, 2007, student, docu-
ment, trial, 2013, . . .
o
u
t
l
o
o
k
outlook, account, email, mail, mi-
crosoft, com, live, password, profil,
contact, creat, server, access, . . .
l
i
c
e
n
s
e
key, product, activ, purchas, licens,
valid, verifi, id, disc, pro, grenuin,
. . .
f
a
c
i
l
i
t
y
window, 8, comput, instal, manufac-
tur, system, oem, 7, pc, hp, . . .
Figure 5: Part of flowchart (left) and topic table (right) on TechSupport dataset, generated by TM-HMMSS
model under settings ofK = 20 topics and T = 20 states. The topic table lists top ranked words in issues
discussed in the chats. Cyan blocks are system actions and yellow blocks are user responses. In every
block, the upper cell shows top ranked words, and the lower cell shows example string patterns of that
state. Transition probability cut-off is 0.05. States and topics are labelled manually.
Agent: conversation opening + identity check
answer, desk, help, <agent-name>, 
welcom, today, may, name, number, ...
e.g.: welcome to answer desk, i'm <agent-
name>, how can i help you, may i have 
your name, case/phone number, account?
Client: acknowledge agent / resolved problem
thank, ok, help, much, good, great, 
<agent-name>, day, appreci, bye, ...
e.g.: ok, thanks, great, <agent-name> 
appreciate your help, good day, bye
Agent: conversation closure
answer, desk, thank, contact, day, chat, 
great, session, com, help, ...
e.g.: thank you for contacting answer desk, 
you are welcome, have a nice day
Agent: acknowledge problem
issu, sorri, call, help, number, suport, 
concern, <client-name>, <phone>, best, ...
e.g.: sorry to hear that, let me help with 
your concern, <client-name>
Client: confirm identity
call, number, phone, case, <time>, would, 
<agent-name>, pleas, <phone>, time, ...
e.g.: <agent-time>, my phone number is 
<phone>. would you pleas call number...
Agent: conversation closure
anyth, els, welcom, help, <client-name>, 
today, assist, question, would, answer, ...
e.g.: you are welcome, anything else today 
i would help/assist you, <client-name>?
Agent: acknowledge identity
give, minut, pleas, check, let, 
thank, moment, 3, one, 5, ...
e.g.: thanks, one moment please, 
give me 3 minutes, let me check
Client: report problem
updat, window, install, <agent-name>, hello, 
error, get, problem, download, message, ...
e.g.: hello, <agent-name>, i get problem/error 
when install/update/download in windows
0.08
0.1
0.08
0.07
0.14
0.05
0.07
0.06
0.05
0.05
0.08
0.06
state 0 0
(0, answer) 0.14307
(0, desk) 0.140908
(0, help) 0.139542
(0, #agentname#) 0.122925
(0, microsoft) 0.121104
(0, welcom) 0.119283
(0, today) 0.0638546
(0, may) 0.0453027
(0, name) 0.0382462
(0, hello) 0.023564
state 0 1
(0, offic) 0.129144
(0, instal) 0.0691015
(0, 2013) 0.0479101
(0, purchas) 0.0395219
(0, 365) 0.0366522
(0, home) 0.0362107
(0, product) 0.0322373
(0, download) 0.0280432
(0, key) 0.0260565
(0, 2010) 0.024732
state 1 0
(1, offic) 0.211557
(1, instal) 0.0920019
(1, 2013) 0.057979
(1, 365) 0.0517066
(1, version) 0.0410626
(1, purchas) 0.028898
(1, home) 0.026237
(1, use) 0.0245264
(1, 2010) 0.0222455
(1, product) 0.0188242
state 1 1
(1, updat) 0.117013
(1, window) 0.0831741
(1, instal) 0.0484751
(1, #agentname#) 0.0456075
(1, hello) 0.0335632
(1, error) 0.0304088
(1, get) 0.0246734
(1, problem) 0.0212322
(1, download) 0.0209454
(1, messag) 0.0195116
state 3 0
(3, give) 0.0975515
(3, minut) 0.0942818
(3, pleas) 0.0906489
(3, check) 0.0884691
(3, let) 0.0703043
(3, thank) 0.0675796
(3, moment) 0.0523213
(3, 3) 0.0485067
(3, one) 0.0308869
(3, 5) 0.0305236
state 3 1
(3, window) 0.129853
(3, 7) 0.0926994
(3, home) 0.074722
(3, bit) 0.0595411
(3, premium) 0.0479556
(3, 64) 0.0463576
(3, vista) 0.0419631
(3, instal) 0.0351717
(3, servic) 0.0323752
(3, pack) 0.0303777
state 4 0
(4, support) 0.0786022
(4, premium) 0.0519191
(4, servic) 0.0504038
(4, #dollaramt#) 0.0501403
(4, issu) 0.0425636
(4, softwar) 0.0376882
(4, warranti) 0.035514
(4, day) 0.0312315
(4, 30) 0.0210854
(4, fix) 0.0206901
state 4 1
(4, pay) 0.0413008
(4, #dollaramt#) 0.0393345
(4, dont) 0.0325617
(4, fix) 0.0310324
(4, support) 0.0275368
(4, much) 0.0231672
(4, ok) 0.0216379
(4, cost) 0.0214194
(4, issu) 0.0192346
(4, money) 0.0192346
state 5 0
(5, thank) 0.313335
(5, #clientname#) 0.149405
(5, inform) 0.0777987
(5, wait) 0.0738004
(5, much) 0.0363618
(5, correct) 0.0243669
(5, patienc) 0.021459
(5, patient) 0.0200051
(5, #email#) 0.0156433
(5, card) 0.0138259
state 6 0
(6, comput) 0.0520198
(6, instal) 0.0475635
(6, system) 0.038489
(6, file) 0.0370306
(6, viru) 0.035248
(6, updat) 0.0345188
(6, program) 0.0290093
(6, run) 0.0215552
(6, tri) 0.0201778
(6, caus) 0.0179091
state 6 1
(6, window) 0.146467
(6, 8) 0.0713756
(6, instal) 0.0647403
(6, 7) 0.0538974
(6, comput) 0.0349627
(6, upgrad) 0.0216923
(6, laptop) 0.0199121
(6, new) 0.0189411
(6, ye) 0.0179701
(6, use) 0.0178082
state 7 0
(7, answer) 0.0739787
(7, microsoft) 0.0701565
(7, desk) 0.0603621
(7, thank) 0.0573362
(7, contact) 0.042764
(7, day) 0.0422066
(7, chat) 0.0359159
(7, great) 0.0321733
(7, session) 0.0280325
(7, com) 0.0226177
state 7 1
(7, call) 0.0779531
(7, number) 0.0643551
(7, phone) 0.0394806
(7, case) 0.0305258
(7, #time#) 0.0232293
(7, would) 0.0222343
(7, #agentname#) 0.0219027
(7, pleas) 0.021571
(7, #phoneno#) 0.0199127
(7, time) 0.0169278
state 8 0
(8, click) 0.105887
(8, right) 0.0329512
(8, pleas) 0.0285344
(8, start) 0.0279375
(8, type) 0.0272213
(8, open) 0.0236401
(8, press) 0.0205365
(8, window) 0.020059
(8, see) 0.0197009
(8, option) 0.0193427
state 8 1
(8, ok) 0.0348863
(8, click) 0.0326788
(8, internet) 0.0309129
(8, screen) 0.0262772
(8, open) 0.0238491
(8, see) 0.0236283
(8, dont) 0.0203172
(8, right) 0.0200964
(8, say) 0.0200964
(8, window) 0.0194342
state 9 1
(9, #email#) 0.244926
(9, #phoneno#) 0.240782
(9, #clientname#) 0.144641
(9, phone) 0.0414562
(9, number) 0.0298531
(9, ye) 0.0277811
(9, email) 0.0273667
(9, name) 0.0219795
(9, 4) 0.0136915
(9, cell) 0.0124484
state 10 0
(10, window) 0.155062
(10, instal) 0.0641206
(10, comput) 0.0618775
(10, 8) 0.0571107
(10, 7) 0.0501943
(10, system) 0.0422498
(10, use) 0.0337444
(10, oper) 0.0298189
(10, version) 0.0250522
(10, manufactur) 0.0186031
state 10 1
(10, updat) 0.0326156
(10, instal) 0.0297734
(10, comput) 0.0230421
(10, problem) 0.0224438
(10, ago) 0.0221446
(10, program) 0.0218454
(10, system) 0.0210975
(10, dont) 0.0209479
(10, restor) 0.0190033
(10, fix) 0.0185545
state 11 0
(11, number) 0.13071
(11, phone) 0.0902297
(11, email) 0.0720301
(11, may) 0.0627649
(11, case) 0.0584632
(11, address) 0.0581323
(11, pleas) 0.0576911
(11, name) 0.05361
(11, chat) 0.030447
(11, disconnect) 0.0212921
state 12 0
(12, issu) 0.0431294
(12, sorri) 0.0419008
(12, call) 0.0310888
(12, help) 0.0286316
(12, number) 0.0264201
(12, support) 0.0243314
(12, concern) 0.0229799
(12, #clientname#) 0.0218741
(12, #phoneno#) 0.0202769
(12, best) 0.0194169
state 12 1
(12, thank) 0.282447
(12, ok) 0.0864091
(12, help) 0.069528
(12, much) 0.0459309
(12, good) 0.0419375
(12, great) 0.0410299
(12, #agentname#) 0.0401223
(12, day) 0.0217892
(12, appreci) 0.0192479
(12, bye) 0.0179773
state 14 0
(14, comput) 0.0877508
(14, access) 0.061482
(14, remot) 0.0601575
(14, ok) 0.0432704
(14, let) 0.0359858
(14, connect) 0.0334472
(14, tri) 0.0309086
(14, issu) 0.0302464
(14, restart) 0.0281493
(14, check) 0.0224099
state 14 1
(14, ok) 0.436864
(14, ye) 0.106903
(14, thank) 0.101863
(14, pleas) 0.0313091
(14, sure) 0.0262695
(14, great) 0.0199037
(14, wait) 0.0191079
(14, oh) 0.0164555
(14, k) 0.015925
(14, let) 0.0148641
state 15 0
(15, #href#) 0.129779
(15, click) 0.118174
(15, link) 0.0862138
(15, pleas) 0.0784769
(15, code) 0.055727
(15, download) 0.0259771
(15, run) 0.0230297
(15, accept) 0.019806
(15, open) 0.0179639
(15, remot) 0.0166745
state 15 1
(15, ok) 0.100091
(15, link) 0.0744436
(15, click) 0.0557658
(15, download) 0.0376455
(15, code) 0.0292823
(15, dont) 0.0281672
(15, copi) 0.0245431
(15, #href#) 0.0242643
(15, page) 0.0237068
(15, past) 0.0237068
state 16 0
(16, anyth) 0.1478
(16, els) 0.146359
(16, welcom) 0.0844204
(16, help) 0.0792348
(16, #clientname#) 0.0754896
(16, today) 0.066847
(16, assist) 0.0518665
(16, question) 0.0233458
(16, would) 0.0193126
(16, answer) 0.0161437
state 19 0
(19, issu) 0.0736645
(19, troubleshoot) 0.0553393
(19, support) 0.0440807
(19, step) 0.0358164
(19, link) 0.0340198
(19, fix) 0.0339001
(19, help) 0.0325826
(19, resolv) 0.0285103
(19, advanc) 0.0240787
(19, option) 0.0230008
0.018599
0.0175358
0.0162186
0.015441
0.0149967
0.0147745
0.0147745
0.0143936
0.0138858
0.0127432
0.0122989
0.0113943
0.0107436
0.0106643
0.00910909
0.00833148
0.00761735
0.0072841
0.00674453
0.0063954
0.00631606
0.00625258
0.00609388
0.00590345
0.00579236
0.00576062
0.00569715
0.00568128
0.00518932
0.00509411
0.0050465
0.0048878
0.00453867
0.00439585
0.00412607
0.00407846
0.00403085
0.00403085
0.00382455
0.00377694
0.00366585
0.0035389
0.0035389
0.0035389
0.00349129
0.00342781
0.00326911
0.00326911
0.00325325
0.00318977
0.0031739
0.00307868
0.00299933
0.00296759
0.00284064
0.00277716
0.00276129
0.00266607
0.00266607
0.0026502
0.00261847
0.00255499
0.00255499
0.00250738
Figure 6: Part of flowchart on Tech-
Support dataset, generated by LM-HMM
model with T = 20 states. Cyan blocks
are system actions and yellow blocks are
user responses. In very block, the upper
cell shows the top ranked words, and the
lower cell shows example word sequences
or string patterns of that state. Transition
probability cut-off is 0.05. States are la-
belled manually. A poorly-inferred state
is highlighted, which seems to confla e
the ?acknowledge agent? and ?resolve
problem? states, a d TM-HMMSS model
has properly disentangled (Figure 5).
the baseline models. For BusTime data, all
models perform relatively well except LM-HMM
which only indicates weak correlations. TM-
HMM out-performs all other models under all set-
tings. This is also true for TechSupport dataset.
LM-HMMS, TM-HMMS and TM-HMMSS mod-
els perform considerably well on BusTime, but
not on TechSupport data. These three models al-
low words to be generated from additional sources
other than states. Although this improves log like-
lihood, it is possible these models encode less in-
formation about the state sequences, at least in
the more diffuse TechSupport data. In summary,
under both quantitative evaluation measures, our
models advance state-of-the-art, however which of
our models is best depends on the application.
43
K10 K20 K30
150
000200
000250
000
150
000200
000250
000
150
000200
000250
000
T10
T20
T30
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
model
ne
ga
tive
 log
 lik
elih
oo
d
K10 K20 K30
6e+
057e+
058e+
05
6e+
057e+
058e+
05
6e+
057e+
058e+
05
T10
T20
T30
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
model
ne
ga
tive
 log
 lik
elih
oo
d
Figure 7: Negative log likelihood on BusTime (upper) and TechSupport (lower) datasets (smaller is better)
under different settings of topics K and states T .
K10 K20 K30
0.00
.51.
0
0.00
.51.
0
0.00
.51.
0
T10
T20
T30
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100# of random permutations
av
era
ge 
ken
dal
l's t
au
model LM?HMM LM?HMMS TM?HMM TM?HMMS TM?HMMSS
K10 K20 K30
0.00
.51.
0
0.00
.51.
0
0.00
.51.
0
T10
T20
T30
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100# of random permutations
av
era
ge 
ken
dal
l's t
au
model LM?HMM LM?HMMS TM?HMM TM?HMMS TM?HMMSS
Figure 8: Average Kendall?s ? measure on BusTime (upper) and TechSupport (lower) datasets (larger is
better) against number of random permutations, under various settings of topics K and states T .
5 Conclusion and Future Work
We have presented three new unsupervised mod-
els to discover latent structures in task-oriented
dialogues. We evaluated on two very different
corpora?logs from spoken, human-computer dia-
logues about bus time, and logs of textual, human-
human dialogues about technical support. We
have shown our models yield superior perfor-
mance both qualitatively and quantitatively.
One possible avenue for future work is scala-
bility. Parallelization (Asuncion et al, 2012) or
online learning (Doucet et al, 2001) could signif-
icantly speed up inference. In addition to MCMC,
another class of inference method is variational
Bayesian analysis (Blei et al, 2003; Beal, 2003),
which is inherently easier to distribute (Zhai et al,
2012) and online update (Hoffman et al, 2010).
Acknowledgments
We would like to thank anonymous reviewers and
Jordan Boyd-Graber for their valuable comments.
We are also grateful to Alan Ritter and Bill Dolan
for their helpful discussions; and Kai (Anthony)
Lui for providing TechSupport dataset.
44
References
Arthur Asuncion, Padhraic Smyth, Max Welling,
David Newman, Ian Porteous, and Scott Triglia,
2012. Distributed Gibbs sampling for latent vari-
able models.
Satanjeev Banerjee and Alexander I Rudnicky. 2006.
A texttiling based approach to topic boundary detec-
tion in meetings. In INTERSPEECH.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of task-
driven human-human dialogs. In ACL, Stroudsburg,
PA, USA.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In NAACL, pages
113?120.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis.
Alan W Black, Susanne Burger, Alistair Conkie,
Helen Hastie, Simon Keizer, Nicolas Merigaud,
Gabriel Parent, Gabriel Schubiner, Blaise Thomson,
D. Williams, Kai Yu, Steve Young, and Maxine Es-
kenazi. 2010. Spoken dialog challenge 2010: Com-
parison of live and control test results. In SIGDIAL.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Ananlada Chotimongkol. 2008. Learning the Struc-
ture of Task-oriented Conversations from the Corpus
of In-domain Dialogs. Ph.D. thesis.
Nigel Crook, Ramn Granell, and Stephen G. Pulman.
2009. Unsupervised classification of dialogue acts
using a dirichlet process mixture model. In SIG-
DIAL.
Hal Daum?e III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In ACL-44: Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 305?312, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
David DeVault, Kallirroi Georgila, Ron Artstein, Fab-
rizio Morbini, David Traum, Stefan Scherer, Albert
Rizzo, and Louis-Philippe Morency. 2013. Verbal
indicators of psychological distress in interactive di-
alogue with a virtual human. In SIGDIAL.
Arnaud Doucet, Nando De Freitas, and Neil Gordon,
editors. 2001. Sequential Monte Carlo methods in
practice. Springer Texts in Statistics.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(Suppl 1):5228?5235.
James Henderson and Oliver Lemon. 2008. Mixture
model POMDPs for efficient handling of uncertainty
in dialogue management. In ACL.
Matthew Hoffman, David M. Blei, and Francis Bach.
2010. Online learning for latent Dirichlet alocation.
In NIPS.
Pei-yun Hsueh, Johanna D. Moore, and Steve Renals.
2006. Automatic segmentation of multiparty dia-
logue. In EACL.
Llu??s F. Hurtado, Joaquin Planells, Encarna Segarra,
Emilio Sanchis, and David Griol. 2010. A stochas-
tic finite-state transducer approach to spoken dialog
management. In INTERSPEECH.
Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual. Insti-
tute of Cognitive Science Technical Report, pages
97?02.
Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika Trust.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Natural Language
Engineering, 5(3/4):323?340.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialogue strategies. IEEE Trans
on Speech and Audio Processing, 8(1):11?23.
Jingjing Liu, Stephanie Seneff, and Victor Zue. 2010.
Dialogue-oriented review summary generation for
spoken dialogue recommendation systems. In
NAACL.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
European Conference on Speech Communication
and Technology.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249?
265.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
45
Matthew Purver, Konrad K?ording, Thomas L. Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In ACL.
Verena Rieser and Oliver Lemon. 2010. Natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. In EMNLP.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In
NAACL.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, September.
David R Traum and Staffan Larsson. 2003. The in-
formation state approach to dialogue management.
In Current and new directions in discourse and dia-
logue, pages 325?353.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Hanna M. Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Jason D. Williams and Suhrid Balakrishnan. 2009. Es-
timating probability of correctness for ASR N-best
lists. In SIGDIAL.
Jason D. Williams. 2012. Challenges and opportuni-
ties for state tracking in statistical spoken dialog sys-
tems: Results from two public deployments. Jour-
nal of Selected Topics in Signal Processing.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In NAACL, pages 477?485,
Stroudsburg, PA, USA. ACL.
Steve Young. 2006. Using POMDPs for dialog man-
agement. In Proceedings of the 1st IEEE/ACL Work-
shop on Spoken Language Technologies (SLT06).
Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational in-
ference in mapreduce. In WWW.
46
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 110?119,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Stability and Accuracy in Incremental Speech Recognition
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ
{selfridg,heemanp}@ohsu.edu {iker,jdw}@research.att.com
Abstract
Conventional speech recognition ap-
proaches usually wait until the user
has finished talking before returning a
recognition hypothesis. This results in
spoken dialogue systems that are unable
to react while the user is still speaking.
Incremental Speech Recognition (ISR),
where partial phrase results are returned
during user speech, has been used to
create more reactive systems. However,
ISR output is unstable and so prone to
revision as more speech is decoded. This
paper tackles the problem of stability
in ISR. We first present a method that
increases the stability and accuracy of
ISR output, without adding delay. Given
that some revisions are unavoidable,
we next present a pair of methods for
predicting the stability and accuracy of
ISR results. Taken together, we believe
these approaches give ISR more utility for
real spoken dialogue systems.
1 Introduction
Incremental Speech Recognition (ISR) enables a
spoken dialogue system (SDS) to react quicker
than when using conventional speech recogni-
tion approaches. Where conventional methods
only return a result after some indication of user
completion (for example, a short period of si-
lence), ISR returns partial phrase results while
the user is still speaking. Having access to a real-
time stream of user speech enables more natural
behavior by a SDS, and is a foundation for cre-
ating systems which take a more active role in
conversations.
Research by Fink et al(1998) and Skantze
& Schlangen (2009), among others, has demon-
strated the efficacy of ISR but has also drawn
attention to a significant obstacle to widespread
use: partial phrase results are generally unsta-
ble and so, as more speech is decoded, are prone
to revision. For example, the ISR component in
a bus information SDS may return the partial
?leaving from Hills?, where ?Hills? is a neigh-
borhood name. It may then return the revi-
sion ?leaving from Pittsburgh?, which the sys-
tem must handle gracefully. Given this propen-
sity to revise, a Stability Measure (SM) ? like-
lihood of a partial result remaining unchanged
compared to the final result ? is necessary for
optimal incremental system behavior. Further-
more, since a stable partial may still be inaccu-
rate, a Confidence Measure (CM) ? likelihood
of partial correctness ? is also necessary.
Effective ISR enables systems to participate in
more dynamic turn-taking. For instance, these
two measures would enable an SDS to identify
inaccurate recognition results while the user is
still speaking. The SDS could then interrupt
and prompt the user to start again. On the
other hand, ISR allows systems to handle pauses
gracefully. If the SDS recognizes that an utter-
ance is incomplete (though stable and accurate),
it could give the user more time to speak before
reacting.
We present two contributions specific to the
use of ISR. First, we characterize three ap-
proaches to ISR which make different trade-offs
between stability and the number of partials
generated. We then present a novel hybrid ap-
proach that combines their strengths to increase
110
stability without adding latency. However, even
with this method, some partial results are still
later revised. The second contribution of the
paper is to present a pair of methods which pre-
dict the stability and accuracy of each partial
result. These two measures are designed for use
in concert by dialogue systems, which must de-
cide whether to act on each partial result in real
time.
2 Background and Related Work
We now describe modern speech recognition
methodology, the production of partial phrase
results, and the advantages and deficiencies of
ISR. In this we seek only to provide a topical
foundation, and not a comprehensive review.
Most modern speech recognition engines use
Hidden-Markov Models and the Viterbi algo-
rithm to decode words from audio. Decod-
ing employs three models: an acoustic model,
which assigns probabilities to speech audio given
a phone; a lexicon, which specifies phone se-
quences for a word; and a language model, which
specifies the probability of a word sequence. The
aim of the decoding process is to find the N most
probable word sequences given the audio spoken
and these three models.
Two useful but different forms of language
models are commonly used in spoken dialogue
systems. A Rule-based Language Model (RLM)
specifies a list of valid sentences which may be
recognized, usually via expansion rules. By con-
trast, a Statistical Language Model (SLM) spec-
ifies a vocabulary of words, allowing arbitrary
sentences to be formed. Both models specify
probabilities over their respective sets ? RLMs
via whole-sentence probabilities, and SLMs via
probabilities of short word sequences called N-
grams. In an SLM, special word symbols are
used to represent the beginning and end of the
phrase, so the probability of beginning or ending
phrases with words can be modeled.
As speech frames are received, the recognizer
builds up a lattice which compactly describes the
probable sequences of words decoded from the
audio. In conventional turn-based speech recog-
nition, decoding continues until the user finishes
speaking. Once the user has finished, the engine
searches the lattice for the most probable word
sequence and returns this to the dialogue man-
ager. By contrast, in ISR the engine inspects
the lattice as it is being built, and returns partial
results to the dialogue manager as they become
available. A key issue for ISR is that partial
results may later be revised, because as more
speech is received and the lattice is extended, a
different path may become the most probable.
In other words, partial results are unstable in
the sense that they may later be revised. Note
that stability is not the same as accuracy: a par-
tial result may be accurate (correct so far) but
unstable, because it is later revised. Similarly, a
stable result may not be accurate.
In the literature, ISR has been proposed for
dialogue systems to enable them to engage in
more natural, human-like interactions. Stud-
ies have shown that incremental systems react
faster than non-incremental ones, and are well-
liked by users because of their naturalness (Aist
et al, 2007; Skantze and Schlangen, 2009). Aist
et al (2007) found that incremental speech
recognition yielded 20% faster task completion.
Moreover, adding ISR improved users? satisfac-
tion with the interaction; the authors attributed
this improvement to ?naturalness?: ?incremen-
tal systems are more like human-human con-
versation than their non-incremental counter-
parts.? Skantze & Schlangen (2009) observed a
similar trend, finding that an incremental sys-
tem was ?clearly preferred? since it ?was ex-
perienced as more pleasant and human-like?,
though it did not actually outperform the non-
incremental system in a number dictation task.
Some recent work has focused on incremen-
tal natural language understanding (NLU). De-
Vault et al (2009) showed that when using a
relatively small number of semantic possibili-
ties the correct interpretation could be predicted
by early incremental results. Schlangen et al
(2009) demonstrated that an incremental refer-
ence resolver could identify the correct reference
out of 12 more than 50% of the time. This
type of NLU can use context and other infor-
mation to be somewhat resilient to errors, and
word recognition inaccuracies may not yield a
111
change in understanding. In this paper we focus
on improving accuracy and stability at the word
level; we belief that improvements at the word
level are likely to improve performance at the
understanding level, although we do not evalu-
ate this here.
A number of researchers have described meth-
ods for evaluating and improving the stability of
ISR results (Baumann et al, 2009; Fink et al,
1998). Baumann, Atterer, & Schlangen spoke
directly to stability by comparing partial phrase
results against the ?final hypothesis produced
by the ASR?. They show that increasing the
amount of ?right context? ? the amount of
speech after the end of the putative partial result
? increases the stability of the partials. Fink et
al. (1998) also used a right context delay to de-
crease the word error rate of ISR results.
A key limitation of these past efforts to im-
prove stability is that adding right context nec-
essarily incurs delay, which degrades responsive-
ness and erodes the overall benefits of ISR. Fur-
thermore, past work has not addressed the prob-
lem of identifying which partials are likely to be
revised. In this paper, we tackle both of these
problems. We first present a method for im-
proving stability by considering features of the
lattice itself, without incurring the delay asso-
ciated with adding right context. Additionally,
since some partials will still be revised, we then
propose a method of scoring the stability of par-
tial speech recognition results.
3 Three approaches to ISR
We now describe three approaches to ISR: Ba-
sic, Terminal, and Immortal. Basic ISR simply
returns the most likely word sequence observed
after some number of speech frames has been de-
coded (in our case every 3 frames or 30ms). This
is the least restrictive approach, and we believe
is the method used by recent ISR research.
Terminal ISR, a more restrictive approach,
finds a partial result if the most likely path
through the (partially-decoded) lattice ends at
a terminal node in the language model. The in-
tuition is that if a partial result finishes a com-
plete phrase expected by the language model,
it is more likely to be stable. The meaning of
terminal is slightly different for rule-based lan-
guage models (RLMs) and statistical language
models (SLMs). For a rule-based grammar,
the terminal node is simply one that ends a
valid phrase (?Pittsburgh? in ?leaving from Pitts-
burgh?). For an SLM, a terminal node indicates
that the most likely successor state is the spe-
cial end-of-sentence symbol. In other words, in
an SLM Terminal partial result, the language
model assigns the highest probability to ending
the phrase.
A third method, Immortal ISR, is the most
restrictive method (Spohrer et al, 1980). If all
paths of the lattice come together into a node
? called an immortal node ? then the lattice
structure before that node will be unchanged by
any subsequent decoding. This structure guar-
antees that the best word sequence prior to an
immortal node is stable. Immortal ISR operates
identically for both RLMs and SLMs.1
To compare these approaches we evaluate
their performance. Utterances were extracted
from real calls to the Carnegie Mellon ?Lets
Go!? bus information system for Pittsburgh,
USA (Raux et al, 2005; Parent and Eskenazi,
2009). We chose this domain because this cor-
pus is publicly available, and this domain has
recently been used as a test bed for dialogue
systems (Black et al , 2010). The AT&T WAT-
SON speech recognition engine was used, modi-
fied to output partials as described above (Goffin
et al, 2005). We tested these three approaches
to ISR on three different recognition tasks. The
first two tasks used rule-based language models
(RLM), and the third used a statistical language
model (SLM).
The two rule-based language models were de-
veloped for AT&T ?Let?s Go? dialogue sys-
tem, prior to its deployment (Williams et al
, 2010). The first RLM (RLM1) consisted
1The choice of search beam size affects both accuracy
and the number of immortal nodes produced: a smaller
beams yields a sparser lattice with more immortal nodes
and lower accuracy; a larger beam yields a richer lattice
with fewer immortal nodes and higher accuracy. In this
work we used our recognizer?s default beam size, which
allows recognition to run in less than real time and yields
near-asymptotic accuracy for all experiments.
112
of street and neighborhood names, built from
the bus timetable database. The second RLM
(RLM2) consisted of just neighborhood names.
Utterances to test RLM1 and RLM2 were se-
lected from the corpus provided by Carnegie
Mellon to match the expected distribution of
speech at the dialogue states where RLM1 and
RLM2 would be used. RLM1 was evaluated on
a set of 7722 utterances, and RLM2 on 5411 ut-
terances. To simulate realistic use, both RLM
test sets were built so that 80% of utterances
are in-grammar, and 20% are out-of-grammar.
The SLM was a 3-gram trained on a set of 140K
utterances, and is tested on a set of 42620 ut-
terances.
In past work, Raux et al (2005) report word
error rates (WERs) of 60-68% on data from the
same dialogue system, though on a different set
of utterances. By comparison, our SLM yields
a WER of 35%, which gives us some confidence
that our overall recognition accuracy is compet-
itive, and that our results are relevant.
Table 1 provides a few statistics of the LMs
and test sets, including whole-utterance accu-
racy, computed using an exact string match.
Results are analyzed in two groups: All, where
all of the utterances are analyzed, and Multi-
Word (MW), where only utterances whose tran-
scribed speech (what was actually said) has
more than one word. Intuitively, these utter-
ances are where ISR would be most effective.
That said, ISR is beneficial for both short and
long utterances ? for example, ISR systems
can react faster to users regardless of utterance
length.
ISR was run using each of the three ap-
proaches (Basic, Terminal, Immortal) in each of
the three configurations (RLM1, RLM2, SLM).
The mean number of partials per utterance is
shown in Table 2. For all ISR methods, the more
flexible SLM produces more partials than the
RLMs. Also as expected, multi-word utterances
produce substantially more partials per utter-
ance than when looking at the entire utterance
set. The Basic approach produces nearly dou-
ble the number of partials than Terminal ISR
does, and Immortal ISR production highlights
its primary weakness: in many utterances, no
Table 1: Statistics for Recognition Tasks. In all ta-
bles, All refers to all utterances in a test set, and
MW refers to the subset of multi-word utterances in
a test set.
RLM1 RLM2 SLM
Num. Utts All 7722 5411 42620
Num. Utts MW 3213 1748 20396
Words/Utt All 1.7 1.5 2.3
Words/Utt MW 2.8 2.6 3.8
Utt. Acc. All. 50 % 60 % 62 %
Utt. Acc. MW 53 % 56 % 44 %
immortal nodes are found. Given this however,
immortal node occurrence is directly related to
the number of words, as indicted by the greater
number of immortal partials in multi-word ut-
terances.
Stability is assessed by comparing the partial
to the final recognition result. For simplicity, we
restrict our analysis to 1-Best hypotheses. If the
partial 1-Best hypothesis is a prefix (or full ex-
act match) of the final 1-Best hypothesis then it
is considered stable. For instance, if the partial
1-Best hypothesis is ?leaving from Forbes? then
it would be stable if the final 1-Best is ?leaving
from Forbes? or ?leaving from Forbes and Mur-
ray? but not if it is ?from Forbes and Murray? or
?leaving?. Accuracy is assessed similarly except
that the transcribed reference is used instead of
the final recognition result.
We report stability and accuracy in Table 3.
Immortal partials are excluded from stability
since they are guaranteed to be stable. The first
four rows report stability, and the second six
report accuracy. The results show that Termi-
nal Partials are relatively unstable, with 23%-
Table 2: Average Number of Partials per utterance
ISR Group RLM1 RLM2 SLM
Basic All 12.0 9.9 11.6MW 14.6 12.3 29.7
Terminal All 5.4 3.3 6.2MW 6.4 4.1 8.8
Immortal All 0.22 0.32 0.55MW 0.42 0.67 0.63
113
Table 3: Stability and Accuracy Percentages
ISR Group RLM1 RLM2 SLM
Stability
Basic All 10 % 11 % 7 %MW 14 % 15 % 9 %
Terminal All 23 % 31 % 37 %MW 20 % 28 % 36 %
Accuracy
Basic All 9 % 1 % 5 %MW 11 % 13 % 6 %
Terminal All 13 % 21 % 24 %MW 12 % 17 % 21 %
Immortal All 91 % 93 % 55 %MW 90 % 90 % 56 %
37% of partials being stable, and that their sta-
bility drops off when looking at multi-word ut-
terances. SLM stability seems to be somewhat
higher than that of the RLM. Basic partials
are even more unstable (about 10% of partials
are stable), with extremely low stability for the
SLM. Unlike Terminal ISR, their stability grows
when only multi-word utterances are analyzed,
though the maximum is still quite low.
The results also show that partials are always
less accurate than they are stable, indicating
that not all stable partials are accurate. Immor-
tal partials are rare, but when they are found,
they are much more accurate than Terminal or
Basic partials. The RLM accuracy is very high,
and we suspect that immortal nodes are corre-
lated with utterances which are easier to recog-
nize. Terminal ISR is far more accurate than
Basic ISR for all of the utterances, but its im-
provement declines for multi-word RLMs.
We have shown three types of ISR: Basic, Ter-
minal and Immortal ISR. While Basic and Ter-
minal ISR are both highly productive, Terminal
ISR is far more stable and accurate than Basic.
Furthermore, there are far more Basic partials
than Terminal partials, implying that the dia-
logue manager would have to handle more un-
stable and inaccurate partials more often. Given
this, Terminal ISR is a far better ?productive
ISR? than the Basic method. Taking produc-
tion and stability together, there is a double dis-
Table 4: Lattice-Aware ISR (LAISR) Example
1-best Partial Type
yew Terminal
sarah Terminal
baum Terminal
dallas Terminal
downtown Terminal
downtown Immortal
downtown pittsburgh Terminal
downtown pittsburgh Immortal
sociation between Terminal and Immortal ISR.
Terminal partials are over produced and rela-
tively unstable. Furthermore, they are even less
stable when the transcribed reference is greater
than one word. On the other hand, Immortal
partials are stable and quite accurate, but too
rare for use alone. By integrating the Immortal
Partials with the Terminal ones, we may be able
to increase the stability and accuracy overall.
4 Lattice-Aware ISR (LAISR)
We introduce Lattice-Aware ISR (LAISR ?
pronounced ?laser?), that integrates Terminal
and Immortal ISR by allowing both types of par-
tials to be found. The selection procedure works
by first checking for an Immortal partial. If one
is not found then it looks for a Terminal. Re-
dundant partials are returned when the partial
type changes. An example recognition is shown
in Table 4. Notice how the first four partials
are completely unstable. This is very common,
and suppressing this noise is one of the primary
benefits of using more right context. Basic ISR
has even more of this type of noise.
LAISR was evaluated on the three recogni-
tion tasks described above (see Table 5). The
first two rows show the average number of par-
tials per utterance for each task and utterance
group. Unsurprisingly, these numbers are quite
similar to Terminal ISR. The stability percent-
age of LAISR is shown in the second two rows.
For all the utterances, there appears to be a very
slight improvement when compared to Termi-
nal ISR in Table 3. The improvement increases
for MW utterances, with LAISR improving over
114
Table 5: Lattice-Aware ISR Stats
Partials per Utterance
RLM1 RLM2 SLM
All 5.6 3.5 6.7
MW 6.7 4.5 9.6
Stability Percentage
All 24 % 33 % 40 %
MW 24 % 35 % 41 %
Accuracy Percentage
All 15 % 23 % 26 %
MW 16 % 22 % 24 %
Terminal ISR by 4?7 percentage points. This
is primarily because there is a higher occur-
rence of Immortal partials as the utterance gets
longer. Accuracy is reported in the final two
rows. Like the previous ISR methods described,
the accuracy percentage is lower than the sta-
bility percentage. When compared to Terminal
ISR, LAISR accuracy is slightly higher, which
confirms the benefit of incorporating immortal
partials with their relatively high accuracy. To
be useful in practice, it is important to exam-
ine when in the utterance ISR results are be-
ing produced. For example, if most of the par-
tials are returned towards the end of utterances,
than ISR is of little value over standard turn-
based recognition. Figure 1 shows the percent
of partials returned from the start of speech to
the final partial for MW utterances using the
SLM. This figure shows that partials are re-
turned rather evenly over the duration of ut-
terances. For example, in the first 10% of dura-
tion of each utterance, about 10% of all partial
results are returned. Figure 1 also reports the
stability and accuracy of the partials returned.
These numbers grow as decoding progresses, but
shows that mid-utterance results do yield rea-
sonable accuracy: partials returned in the mid-
dle of utterances (50%-60% duration) have an
accuracy of near 30%, compared to final partials
47% percent.
For use in a real-time dialogue system, it is
also important to assess latency. Here we define
latency as the difference in (real-world) time be-
tween (1) when the recognizer receives the last
Figure 1: Percent of LAISR partials returned from
the start of detected speech to the final partial using
the SLM. The percentage of partials returned that
are stable/accurate are also shown.
frame of audio for a segment of speech, and (2)
when the partial that covers that segment of
speech is returned from the recognizer. Mea-
suring latencies of LAISR on each task, we find
that RLM1 has a median of 0.26 seconds and a
mean of 0.41s; RLM2 has a median of 0.60s and
a mean of 1.48s; and SLM has a median of 1.04s
and a mean of 2.10s. Since reducing latency
was not the focus on this work, no speed opti-
mizations have been made, and we believe that
straightforward optimization can reduce these
latencies. For example, on the SLM, simply
turning off N-Best processing reduces the me-
dian latency to 0.55s and the mean to 0.79s.
Human reaction time to speech is roughly 0.20
seconds (Fry, 1975), so even without optimiza-
tion the RLM latencies are not far off human
performance.
In sum, LAISR produces a steady stream
of partials with relatively low latency over the
course of recognition. LAISR has higher stabil-
ity and accuracy than Terminal ISR, but its par-
tials are still quite unstable and inaccurate. This
means that in practice, dialogue systems will
need to make important decisions about which
partials to use, and which to discard. This need
motivated us to devise techniques for predicting
when a partial is stable, and when it is accurate,
which we address next.
115
Table 6: Equal Error Rates: Significant improvements in bold. Basic at p < 0.016, Terminal at p < 0.002,
and LAISR at p < 0.00001
All Multi-Word
Stability Measure (SM) Equal Error Rate
RLM 1 RLM 2 SLM RLM 1 RLM 2 SLM
Basic WATSON Score 13.3 13.3 12.8 15.6 16.4 15.2Regression 10.7 11.3 12.3 13.2 15.2 15.1
Terminal WATSON Score 24.3 29.1 34.4 26.6 26.0 34.1Regression 19.7 26.5 26.5 23.0 24.3 24.7
LAISR WATSON Score 24.7 29.3 35.0 24.0 27.0 35.3Regression 19.2 25.6 25.0 18.4 23.3 22.7
Confidence Measure (CM) Equal Error Rate
Basic WATSON Score 11.3 11.7 9.9 14.1 14.0 11.6Regression 9.8 9.8 9.7 12.3 12.9 11.0
Terminal WATSON Score 15.1 21.1 30.6 15.7 17.4 29.3Regression 11.7 16.8 20.8 12.1 14.5 18.4
LAISR WATSON Score 15.8 21.8 32.3 18.4 19.5 31.8Regression 11.6 16.6 21.0 11.6 14.2 18.7
5 Stability and Confidence Measures
As seen in the previous section, partial speech
recognition results are often revised and inaccu-
rate. In order for a dialogue system to make
use of partial results, measures of both stability
and confidence are crucial. A Stability Measure
(SM) predicts whether the current partial is a
prefix or complete match of the final recogni-
tion result (regardless of whether the final result
is accurate). A Confidence Measure (CM) pre-
dicts whether the current partial is a prefix or
complete match of what the user actually said.
Both are useful in real systems: for example, if
a partial is likely stable but unlikely correct, the
system might interrupt the user and ask them
to start again.
We use logistic regression to learn separate
classifiers for SM and CM. Logistic regression is
appealing because it is well-calibrated, and has
shown good performance for whole-utterance
confidence measures (Williams and Balakrish-
nan, 2009). For this, we use the BXR pack-
age with default settings (Genkin et al, 2011).
For Terminal and Basic ISR we use 11 features:
the raw WATSON confidence score, the individ-
ual features which affect the confidence score,
the normalized cost, the normalized speech like-
lihood, the likelihoods of competing models,
the best path score of word confusion network
(WCN), the length of WCN, the worst probabil-
ity in the WCN, and the length of N-best list.
For LAISR, four additional features are used:
three binary indicators of whether the partial is
Terminal, Immortal or a Terminal following an
Immortal, and one which gives the percentage
of words in the hypothesis that are immortal.
We built stability and confidence measures for
Basic ISR, Terminal ISR, and LAISR. Each of
the three corpora (RLM1, RLM2, SLM) was di-
vided in half to form a train set and test set.
Regression models were trained on all utter-
ances in the train set. The resulting models were
then evaluated on both All and MW utterances.
As a baseline for both measures, we compare
to AT&T WATSON?s existing confidence score.
This score is used in numerous deployed com-
mercial applications, so we believe it is a fair
baseline. Although the existing confidence score
is designed to predict accuracy (not stability),
there is no other existing mechanism for pre-
dicting stability.
We first report ?equal error rate? for the mea-
sures (Table 6). Equal error rate (EER) is the
sum of false accepts and false rejects at the rejec-
116
Figure 2: True accept percentages for stability measure (a) and confidence measure (b), using a fixed false
accept rate of 5%. LAISR yields highest true accept rates, with p < 0.0001 in all cases.
(a) Stability measure (b) Confidence measure
tion threshold for which false accepts and false
rejects are equal. Equal error rate is a widely
used metric to evaluate the quality of scoring
models used for accept/reject decisions. A per-
fect scoring model would yield an EER of 0. For
statistical significance we use ?2 contingency ta-
bles with 1 degree of freedom. It is inappropri-
ate to compare EER across ISR methods, since
the total percentage of stable or accurate par-
tials significantly effects the EER. For example,
Basic ISR has relatively low EER, but this is
because it also has a relatively low number of
stable or accurate partials.
The top six rows of Table 6 show EER for the
Stability Measure (SM). The left three columns
show results on the entire test set (all utterances,
of any length). On the whole, the SM outper-
forms the WATSON confidence scores, and the
greatest improvement is a 10.0 point reduction
in EER for LAISR on the SLM task. The right
three columns show results on only multi-word
(MW) utterances. Performance is similar to the
entire test set, with a maximum EER reduction
of 12.6 percent. The SLM MW performance is
interesting, suggesting that it is easier to pre-
dict stability after at least one word has been
decoded, possibly due to higher probability of
immortal nodes occurring. This suggests there
would be benefit in combining our method with
past work that adds right-context, perhaps us-
ing more context early in the utterance. This
idea is left for future work.
The bottom six rows show results for the Con-
fidence Measure (CM). We see that that even
when comparing our CM against the WATSON
confidence scores, there is significant improve-
ment, with a maximum of 13.1 for LAISR in the
MW SLM task.
The consistent improvement shows that logis-
tic regression is an effective technique for learn-
ing confidence and stability measures. It is most
powerful when combined with LAISR, and only
slightly less so with Terminal. Furthermore,
though the gains are slight, it is also useful with
Basic ISR, which speaks to the generality of the
approach.
While equal error rate is useful for evaluating
discriminative ability, when building an actual
system a designer would be interested to know
how often the correct partial is accepted. To
evaluate this, we assumed a fixed false-accept
rate of 5%, and report the resulting percentage
of partials which are correctly accepted (true-
accepts). Results are shown in Figure 1. LAISR
accepts substantially more correct partials than
other methods, indicating that LAISR would be
more useful in practice. This result also shows
a synergy between LAISR and our regression-
based stability and confidence measures: not
only does LAISR improve the fraction of stable
117
and correct partials, but the regression is able
to identify them better than for Terminal ISR.
We believe this shows the usefulness of the ad-
ditional lattice features used by the regression
model built on LAISR results.
6 Discussion and Conclusion
The adoption of ISR is hindered by the num-
ber of revisions that most partials undergo. A
number of researchers have proposed the use of
right context to increase the stability of par-
tials. While this does increase stability, it mit-
igates the primary gain of ISR: getting a rela-
tively real-time stream of the user?s utterance.
We offer two methods to improve ISR function-
ality: the integration of low-occurring Immortal
partials with higher occurring Terminal partials
(LAISR), and the use of logistic regression to
learn stability and confidence measures.
We find that the integrative approach,
LAISR, outperforms Terminal ISR on three
recognition tasks for a bus timetable spoken dia-
logue system. When looking at utterances with
more than one word this difference becomes even
greater, and this performance increase is due to
the addition of immortal partials, which have
a higher occurrence in longer utterances. This
suggests that as dialogue systems are used to
process multi-phrasal utterances and have more
dynamic turn-taking interactions, immortal par-
tials will play an even larger roll in ISR and par-
tial stability will further improve.
The Stability and Confidence measures both
have lower Equal Error Rates than raw recog-
nition scores when classifying partials. The im-
provement is greatest for LAISR, which benefits
from additional features describing lattice struc-
ture. It also suggests that other incremental fea-
tures such as the length of right context could be
useful for predicting stability. The higher num-
ber of True Accept partials by LAISR indicates
that this method is more useful to a dialogue
manager than Basic or Terminal ISR. Even so,
for all ISR methods there are still more use-
ful stable partials than there are accurate ones.
This suggests that both of these measures are
important to the downstream dialogue manager.
For example, if the partial is predicted to be sta-
ble but not correct, than the agent could possi-
bly interrupt the user and ask them to begin
again.
There are a number of avenues for future
work. First, this paper has examined the word
level; however dialogue systems generally oper-
ate at the intention level. Not all changes at
the word level yield a change in the resulting
intention, so it would be interesting to apply
the confidence measure and stability measures
developed here to the (partial) intention level.
These measures could also be applied to later
stages of the pipeline ? for example, tracking
stability and confidence in the dialogue state re-
sulting from the current partial intention. Fea-
tures from the intention level and dialogue state
could be useful for these measures ? for instance,
indicating whether the current partial intention
is incompatible with the current dialogue state.
Another avenue for future work would be to
apply these techniques to non-dialogue real-time
ASR tasks, such as transcription of broadcast
news. Confidence and stability measures could
be used to determine whether/when/how to dis-
play recognized text to a viewer, or to inform
down-stream processes such as named entity ex-
traction or machine translation.
Of course, an important objective is to eval-
uate our Stability and Confidence Measures
with LAISR in an actual spoken dialogue sys-
tem. ISR completely restructures the conven-
tional turn-based dialogue manager, giving the
agent the opportunity to speak at any mo-
ment. The use of reinforcement learning to make
these turn-taking decisions has been shown in a
small simulated domain by Selfridge and Hee-
man (2010), and we believe this paper builds
a foundation for pursuing these ideas in a real
system.
Acknowledgments
Thanks to Vincent Goffin for help with this
work, and the anonymous reviewers for their
thoughtful suggestions and critique. We ac-
knowledge funding from the NSF under grant
IIS-0713698.
118
References
G. Aist, J. Allen, E. Campana, C. Gallo, S. Stoness,
Mary Swift, and Michael K. Tanenhaus. 2007. In-
cremental understanding in human-computer di-
alogue and experimental evidence for advantages
over nonincremental methods. In Proc. DECA-
LOG, pages 149?154.
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of
speech recognition for incremental systems. In
Proc. NAACL: HLT, pages 380?388.
A. Black, S. Burger, B. Langner, G. Parent, and
M. Eskenazi, 2010. Spoken dialog challenge 2010,
In Proc. Workshop on Spoken Language Technolo-
gies (SLT), Spoken Dialog Challenge 2010 Special
Session.
David DeVault, Kenji Sagae, and David Traum.
2009. Can i finish? learning when to respond to
incremental interpretation results in interactive di-
alogue. In Proc. SIGdial 2009 Conference, pages
11?20,
G.A. Fink, C. Schillo, F. Kummert, and G. Sagerer.
1998. Incremental speech recognition for multi-
modal interfaces. In Industrial Electronics Soci-
ety, 1998. IECON?98 volume 4, pages 2012?2017.
D.B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli.. Cortex volume 11, number 4,
page 355.
A. Genkin, L. Shenzhi, D. Madigan, and DD.
Lewis. 2011. Bayesian logistic regression.
http://www.bayesianregression.org.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-
Tur, A. Ljolje, S. Parthasarathy, M. Rahim,
G. Riccardi, and M. Saraclar. 2005. The AT&T
WATSON speech recognizer. In Proc. of ICASSP,
pages 1033?1036.
G. Parent and M. Eskenazi. 2009. Toward Better
Crowdsourced Transcription: Transcription of a
year of the Let?s Go Bus Information System Data.
Proc. of Interspeech 2005, Lisbon, Portugal.
A. Raux, B. Langner, D. Bohus, A.W. Black, and
M. Eskenazi. 2005. Lets go public! taking a spo-
ken dialog system to the real world. In Proc. of
Interspeech 2005.
D. Schlangen, T. Baumann, and M. Atterer. 2009.
Incremental reference resolution: The task, met-
rics for evaluation, and a Bayesian filtering model
that is sensitive to disfluencies. In Proc. SIGdial,
pages 30?37.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems.
In Proc. of ACL 2010, pages 177?185.
G. Skantze and D. Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proc.
EACL 2009, pages 745?753
J.C. Spohrer, PF Brown, PH Hochschild, and
JK Baker. 1980. Partial traceback in continuous
speech recognition. In Proc. of the IEEE Interna-
tional Conference on Cybernetics and Society.
J.D. Williams, I. Arizmendi and A. Conkie.
2010. Demonstration of AT&T ?Let?s Go?: A
production-grade statistical spoken dialog system.
In Proc Demonstration Session at IEEE Workshop
on Spoken Language Technology
J.D. Williams and S. Balakrishnan. 2009. Estimat-
ing probability of correctness for ASR N-Best lists.
In Proc. of SIGdial 2009, pages 132?135.
119
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 130?141,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Empirical Evaluation of a Statistical Dialog System in Public Use
Jason D. Williams
AT&T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USA
jdw@research.att.com
Abstract
This paper provides a first assessment of a sta-
tistical dialog system in public use. In our di-
alog system there are four main recognition
tasks, or slots ? bus route names, bus-stop lo-
cations, dates, and times. Whereas a conven-
tional system tracks a single value for each slot
? i.e., the speech recognizer?s top hypothesis
? our statistical system tracks a distribution
of many possible values over each slot. Past
work in lab studies has showed that this distri-
bution improves robustness to speech recog-
nition errors; but to our surprise, we found
the distribution yielded an increase in accu-
racy for only two of the four slots, and actu-
ally decreased accuracy in the other two. In
this paper, we identify root causes for these
differences in performance, including intrin-
sic properties of N-best lists, parameter set-
tings, and the quality of statistical models. We
synthesize our findings into a set of guidelines
which aim to assist researchers and practition-
ers employing statistical techniques in future
dialog systems.
1 Introduction
Over the past decade, researchers have worked to ap-
ply statistical techniques to spoken dialog systems,
and in controlled laboratory studies, statistical di-
alog systems have been shown to improve robust-
ness to errors compared to conventional approaches
(Henderson and Lemon, 2008; Young et al, 2010;
Thomson and Young, 2010). However, statistical
techniques have not yet been evaluated in a publicly
deployed system, and real users behave very differ-
ently to usability subjects (Raux et al, 2005; Ai et
al., 2008). So there is an important open question
whether statistical dialog systems improve perfor-
mance with real users.
This paper provides a first evaluation of a publi-
cally deployed statistical dialog system, AT&T Let?s
Go (Williams et al, 2010). AT&T Let?s Go pro-
vides bus times for Pittsburgh, and received approx-
imately 750 calls from real bus riders during the
2010 Spoken Dialog Challenge (Black et al, 2010).
AT&T Let?s Go is based on a publicly available
toolkit (Williams, 2010a) and achieved the highest
rates of successful task completion on real callers in
the challenge, so it provides a relevant exercise from
which to draw inferences.
AT&T Let?s Go collected four types of informa-
tion, or slots: bus route names, bus-stop names,
dates, and times. For each slot, we measured turn-
level accuracy of the deployed statistical system and
compared it to accuracy without application of the
statistical techniques (i.e., the top speech recogni-
tion result).
To our surprise, we found that statistical tech-
niques appeared to improve accuracy for only two of
the four slots, and decreased accuracy for the other
two. To investigate this, we considered four mech-
anisms by which statistical methods can differ from
the top speech recognition result. Analyzing the ef-
fects of each mechanism on each slot enables un-
derlying causes to be identified: for example, one
mechanism performed exceptionally well when its
statistical models was well matched to usage data,
but rather poorly when its model diverged from real
usage. We believe this analysis ? the focus of this
paper ? is relevant to researchers as well as practi-
130
tioners applying statistical techniques to production
systems.
In this paper, Section 2 reviews the operation of
statistical spoken dialog systems. Section 3 then
describes the AT&T Let?s Go dialog system. Sec-
tion 4 reports on overall accuracy, then analyzes the
underlying reasons for accuracy gains and losses.
Section 5 tackles how well error in the belief state
can be identified compared to speech recognition er-
rors. Section 6 concludes by summarizing lessons
learned.
2 Statistical dialog systems
Statistical dialog systems maintain a distribution
over a set of hidden dialog states. A dialog state
includes information not directly observable to the
dialog system, such as the user?s overall goal in the
dialog or the user?s true action (e.g., the user?s true
dialog act). For each dialog state s, a posterior prob-
ability of correctness called a belief is maintained
b(s). The set of hidden dialog states and their be-
liefs is collectively called the belief state, and up-
dating the belief state is called belief tracking. Here
we will present belief tracking at a level sufficient
for our purposes; for a more general treatment, see
(Williams and Young, 2007).
At the start of the dialog, the belief state is initial-
ized to a prior distribution b0(s). The system then
takes an action a, and the user takes an action in
response. The automatic speech recognizer (ASR)
then produces a ranked list of N hypotheses for the
user?s action, u = (u1, . . . , uN ), called an N-best
list. For each N-best list the ASR also produces a
distribution Pasr(u) which assigns a local, context-
independent probability of correctness to each item,
often called a confidence score. The belief state is
then updated:
b?(s) = k ?
?
u
Pasr(u)Pact(u|s, a)b(s) (1)
where Pact(u|s, a) is the probability of the user tak-
ing action u given the dialog is in hidden state s and
the system takes action a. k is a normalizing con-
stant.
In practice specialized techniques must be used to
compute Eq 1 in real-time. The system in this paper
uses incremental partition recombination (Williams,
2010b); alternatives include the Hidden Information
State (Young et al, 2010), Bayesian Update of Dia-
log States (Thomson and Young, 2010), and particle
filters (Williams, 2007). The details are not impor-
tant for this paper ? the key idea is that Eq 1 synthe-
sizes a prior distribution over dialog states together
with all of the ASR N-best lists and local confidence
scores to form a cumulative, whole-dialog poste-
rior probability distribution over all possible dialog
states, b(s).
In the system studied in this paper, slots are
queried separately, and an independent belief state is
maintained for each. Consequently, within each slot
user actions u and hidden states s are drawn from
the same set of slot values. Thus the top ASR result
u1 represents the ASR?s best hypothesis for the slot
value in the current utterance, whereas the top dia-
log state argmaxs b(s) = s? represents the belief
state?s best hypothesis for the slot value given all of
the ASR results so far, a prior over the slot values,
and models of user action likelihoods. The promise
of statistical dialog systems is that s? will (we hope!)
be correct more often than u1. In the next section,
we measure this in real dialogs.
3 AT&T Let?s Go
AT&T Let?s Go is a statistical dialog system that
provides bus timetable information for Pittsburgh,
USA. This system was created to demonstrate a
production-grade system built following practices
common in industry, but which incorporates two sta-
tistical techniques: belief tracking with the AT&T
Statistical Dialog Toolkit (Williams, 2010a), and
regression-based ASR confidence scores (Williams
and Balakrishnan, 2009).
As with most commercial dialog systems, AT&T
Let?s Go follows a highly directed flow, collecting
one slot at a time. There are four types of slots:
ROUTE, LOCATION, DATE, and TIME. The sys-
tem can only recognize values for the slot being
queried, plus a handful of global commands (?re-
peat?, ?go back?, ?start over?, ?goodbye?, etc.) ?
mixed initiative and over-completion were not sup-
ported. As mentioned above, an independent belief
state is maintained for each slot: this was an inten-
tional design decision made in order to use statistical
techniques within current commercial practices.
131
The system opens by asking the user to say a bus
ROUTE, or to say ?I?m not sure.? The system next
asks for the origin and destination LOCATIONs. The
system then asks if the caller wants times for the
?next few buses?; if not, the system asks for the
DATE then TIME in two separate questions. Finally
bus times are read out.
After requesting the value of a slot, the system re-
ceives an N-best list, assigns each item a confidence
score Pasr(u), and updates the belief in (only) that
slot using Eq 1. The top dialog hypothesis s? and
its belief b(s?) are used to determine which action
to take next, following a hand-crafted policy. This is
in contrast to a conventional dialog system, in which
the top ASR result and its confidence govern dialog
flow. Figure 6 shows the design of AT&T Let?s Go.
In the period July 16 ? August 16 2010, AT&T
Let?s Go received 742 calls, of which 670 had one
or more user utterances. These calls contained a
total of 8269 user utterances, of which 4085 were
in response to requests for one of the four slots.
(The remainder were responses to yes/no questions,
timetable navigation commands like ?next bus?,
etc.)
Our goal in this paper is to determine whether
tracking a distribution over multiple dialog states
improved turn-level accuracy compared to the top
ASR result. To measure this, we compare the accu-
racy of the top belief state and the top ASR result. A
transcriber listened to each utterance and marked the
top ASR hypothesis as correct if it was an exact lex-
ical or semantic match, or incorrect otherwise. The
same was then done for the top dialog hypothesis in
each turn.
Accuracy of the top ASR hypothesis and the top
belief state are shown in Table 1, which indicates
that belief monitoring improved accuracy for ROUTE
and DATE, but degraded accuracy for LOCATION and
TIME. We had hoped that belief tracking would im-
prove accuracy for all slots; seeing that it hadn?t
prompted us to investigate the underlying causes.
4 Belief tracking analysis
When an ASR result is provided to Eq 1 and a new
belief state is computed, the top dialog state hypoth-
esis s? may differ from top ASR result u1. For-
mally, these differences are simply the result of eval-
Slot ROUTE LOCATION DATE TIME
Utts 1520 2235 173 157
ASR 769 1326 124 80
correct 50.6% 59.3% 71.7% 51.0 %
Belief 799 1246 139 63
correct 52.6% 55.7% 80.3% 40.1%
Belief +30 -80 +15 -17
? ASR +2.0% -3.6% +8.7% -10.8%
Table 1: Accuracy of the top ASR result and top be-
lief state. LOCATION includes both origin and des-
tination utterances. Most callers requested the next
bus so few were asked for DATE and TIME.
uating this equation. However, intuitively there are
four mechanisms which cause differences, and each
difference can be explained by the action of one or
more mechanisms. These mechanisms are summa-
rized here; the appendix provides graphical illustra-
tions.1
? ASR re-ranking: When computing a con-
fidence score Pasr(u), it is possible that the
entry with the highest confidence u? =
argmaxu Pasr(u) will not be the first ASR re-
sult, u1 6= u?. In other words, if the confidence
score re-ranks the N-best list, this may cause s?
to differ from u1 (Figure 7).
? Prior re-ranking: Statistical techniques use a
prior probability for each possible dialog state
? in our system, each slot value ? b0(s). If an
item recognized lower-down on the N-best list
has a high prior, it can obtain the most belief,
causing s? to differ from u1 (Figure 8).
? Confidence aggregation: If the top belief
state s? has high belief, then subsequent low-
confidence recognitions which do not contain
s? will not dislodge s? from the top position,
causing s? to differ from u1 (Figure 9).
? N-best synthesis: If an item appears in two N-
best lists, but is not in the top ASR N-best posi-
tion in the latter recognition, it may still obtain
the highest belief, causing s? to differ from u1
(Figure 10).
1This taxonomy was developed for belief tracking over a
single slot. For systems which track joint beliefs over multiple
slots, additional mechanisms could be identified.
132
 

 

 

		

		
		



	


Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 275?279,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Integrating Incremental Speech Recognition and POMDP-based Dialogue
Systems
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams1
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ, USA
1Microsoft Research, Redmond, WA, USA
{selfridg,heemanp}@ohsu.edu
iker@research.att.com jason.williams@microsoft.com
Abstract
The goal of this paper is to present a first
step toward integrating Incremental Speech
Recognition (ISR) and Partially-Observable
Markov Decision Process (POMDP) based di-
alogue systems. The former provides sup-
port for advanced turn-taking behavior while
the other increases the semantic accuracy of
speech recognition results. We present an In-
cremental Interaction Manager that supports
the use of ISR with strictly turn-based dia-
logue managers. We then show that using
a POMDP-based dialogue manager with ISR
substantially improves the semantic accuracy
of the incremental results.
1 Introduction and Background
This paper builds toward integrating two distinct
lines of research in spoken dialogue systems: in-
cremental speech recognition (ISR) for input, and
Partially Observable Markov Decision Processes
(POMDPs) for dialogue management.
On the one hand, ISR improves on whole-
utterance speech recognition by streaming results to
the dialogue manager (DM) in real time (Baumann
et al, 2009; Skantze and Schlangen, 2009). ISR
is attractive because it enables sophisticated system
behavior such as interruption and back-channeling.
However, ISR output is particularly error-prone, and
often requires a specialized dialogue manager to be
written (Bu? and Schlangen, 2011; Schlangen and
Skantze, 2009).
1Work done while at AT&T Labs - Research
On the other hand, POMDP-based dialogue man-
agers improve on traditional approaches by (in part)
tracking a distribution over many possible dialogue
states, rather than just one, thereby improving ro-
bustness to speech recognition errors (Williams and
Young, 2007; Thomson and Young, 2010; Young
et al, 2010). The overall aim of combining these
two lines of research is to improve the robustness of
error-prone ISR output.
To our knowledge only one study to date has com-
bined ISR and POMDPs. Lu et al (2011) show
how 1-best ISR hypotheses can be used within a sin-
gle dialogue turn. This work is different than the
present paper, where we use N-Best lists of ISR re-
sults across multiple turns of a dialogue.
Specifically, this paper makes two contributions.
First, as a foundation, we introduce an Incremental
Interaction Manager (IIM) that enables ISR to be
used within the traditional turn-based dialogue man-
agement framework. The IIM confers many, but not
all, of the benefits of ISR without requiring mod-
ification to a traditional dialogue manager. Thus,
in theory, any existing dialogue system architecture
could use ISR with the addition of an IIM. Second,
we show that pairing our IIM with a POMDP-based
dialogue manager yields a substantial improvement
in accuracy for incremental recognition results at the
dialogue level.
The paper is organized as follows. Section 2 de-
scribes the IIM, section 3 describes the POMDP in-
tegration, sections 4 and 5 describe experiments and
results, and section 6 concludes.
275
Table 1: Example IIM operation. P = partial ISR result; A = dialogue action.
Original Copied
ISR IIM DM state DM state DM Action
Prompt: ?Where are you leaving from??
yew Rej. P 0 0 -
ridge Acc. P / Rej. A 0 0 ?I?m sorry...?
mckee Acc. P / Acc. A 0 1 ?Ok, Mckee...?
mckeesport Acc. P / Acc. A 0 2 ?Ok, Mckeesport..?
mckeesport center Acc. P / Rej. A 0 2 ?Ok, Mckeesport..?
Prompt: ?Ok, Mckeesport. Where are you going to??
pitt Acc. P / Rej. A 2 4 ?I?m sorry...?
pittsburgh Acc. P / Acc. A 2 5 ?Ok, Pittsburgh...?
2 Incremental Interaction manager
The Incremental Interaction Manager (IIM) medi-
ates communication between the incremental speech
recognizer and the DM. The key idea is that the
IIM evaluates potential dialogue moves by apply-
ing ISR results to temporary instances of the DM.
The IIM copies the current state of the DM, pro-
vides the copied DM with a recognition result, and
inspects the action that the copied DM would take.2
If the action does not sufficiently advance the dia-
logue (such as re-asking the same question), the ac-
tion is rejected and the copied DM is discarded. If
the action advances the dialogue (such as asking for
or providing new information), then that action is
immediately executed.
The system should gracefully handle revisions
following a premature action execution, and a copy-
ing procedure is a viable solution for any DM. When
a revision is received, a second copy of the original
DM is made and the new ISR result is passed to that
second copy; if that second copy takes an action that
advances the dialogue and is different from the ac-
tion generated by the first copy, then the first action
is terminated, the first copy of the DM is discarded,
the second action is initiated, and the second copy
assumes the position of the first copy. Additional
revisions can be handled by following the same pro-
cedure. Terminating a speech action and immedi-
ately starting another can be jarring (?Say a city /
Ok, Boston...?), which can be mitigated by preced-
2If the DM design does not force a state transition following
a result then the DM supplies the the action without copying.
ing actions with either a sound or simple silence (at
the expense of some response delay). Once recog-
nition is complete, the copied DM is installed as the
new original DM.
Many ISR results can be discarded before passing
them to the DM. First, only incremental results that
could correspond to complete user utterance are con-
sidered: incomplete results are discarded and never
passed to the DM. In addition, ISR results are of-
ten unstable, and it is undesirable to proceed with
an ISR result if it will very likely be revised. Thus
each candidate ISR result is scored for stability (Sel-
fridge et al, 2011) and results with scores below a
manually-set threshold are discarded.
Table 1 shows an example of the recognizer, the
IIM, and the DM. For sake of clarity, stability scores
are not shown. The system asks ?Where are you
leaving from?? and the user answers ?Mckeesport
Center.? The IIM receives five ISR results (called
partials), rejecting the first, yew, because its stabil-
ity score is too low (not shown). With the second,
ridge, it copies the DM, passes ridge to the copy,
and discards the action of the copied DM (also dis-
carded) because it does not advance the dialogue. It
accepts and begins to execute the action generated
by the third partial, mckee. The fourth partial revises
the action, and the fifth action is rejected since it is
the same. The original DM is then discarded and the
copied DM state is installed in its place.
Overall, the IIM enables a turn-based DM to en-
joy many of the benefits of ISR ? in particular, the
ability to make turn-taking decisions with a com-
plete account of the dialogue history.
276
3 Integrating ISR with a POMDP-based
dialogue manager
A (traditional) dialogue manager based on a partially
observable Markov decision process (POMDP DM)
tracks a probability distribution over multiple hid-
den dialogue states called a belief state (Williams
and Young, 2007).3 As such, POMDP DMs read-
ily make use of the entire ASR N-Best list, even
for low-confidence results ? the confidence level of
each N-Best list item contributes proportionally to
the probability of its corresponding hidden state.
It is straightforward to integrate ISR and a
POMDP DM using the IIM. Each item on the N-
Best list of an incremental result is assigned a confi-
dence score (Williams and Balakrishnan, 2009) and
passed to the POMDP DM as if it were a complete
result, triggering a belief state update. Note that this
approach is not predicting future user speech from
partial results (DeVault et al, 2009; Lu et al, 2011),
but rather (tentatively) assuming that partial results
are complete.
The key benefit is that a belief state generated
from an incremental result incorporates all of the
contextual information available to the system from
the start of the dialogue until the moment of that
incremental result. By comparison, an isolated in-
cremental result includes only information from the
current utterance. If the probability models in the
POMDP are estimated properly, belief states should
be more accurate than isolated incremental results.
4 Experimental design
For our experiments we used a corpus of 1037 calls
from real users to a single dialogue system that pro-
vides bus timetable information for Pittsburgh, PA
(a subsequent version of Williams (2011)). This di-
alogue system opened by asking the caller to say a
bus route number or ?I don?t know?; if the system
had insufficient confidence following recognition, it
repeated the question. We extracted the first 3 re-
sponses to the system?s bus route question. Often
the system did not need to ask 3 times; our exper-
imental set contained 1037 calls with one or more
attempts, 586 calls with two or more attempts, and
3It also uses reinforcement learning to choose actions, al-
though in this paper we are not concerned with this aspect.
356 calls with three or more attempts. These utter-
ances were all transcribed, and tagged for the bus
route they contained, if any: 25% contained neither
a route nor ?I don?t know?.
We ran incremental speech recognition on each
utterance using Lattice-Aware Incremental Speech
Recognition (Selfridge et al, 2011) on the AT&T
WATSONSM speech recognizer (Goffin et al, 2005)
with the same rule-based language models used in
the production system. On average, there were
5.78, 5.44, and 5.11 incremental results per utter-
ance (plus an utterance-final result) for the first, sec-
ond, and third attempts. For each incremental result,
we noted its time stamp and interpretation: correct,
if the interpretation was present and correct, other-
wise incorrect. Each incremental result included an
N-Best list, from which we determined oracle accu-
racy: correct if the correct interpretation was present
anywhere on the most recent ISR N-Best list, other-
wise incorrect.
Each incremental result was then passed to the
IIM and POMDP DM. The models in the POMDP
DM were estimated using data collected from a dif-
ferent (earlier) time period. When an incremental
result updated the belief state, the top hypothesis
for the route was extracted from the belief state and
scored for correctness. For utterances in the first at-
tempt, the belief state was initialized to its prior; for
subsequent attempts, it incorporated all of the prior
(whole-turn) utterances. In other words, each at-
tempt was begun assuming the belief state had been
running up to that point.
5 Results and Discussion
We present results by showing instantaneous seman-
tic accuracy for the raw incremental result (base-
line), the top belief state, and oracle. Instantaneous
semantic accuracy is shown with respect to the per-
cent of the total recognition time the partial is rec-
ognized at. An utterance is incorrect if it has no in-
cremental result before a certain percentage.
We show 2 sets of plots. Figure 1 shows only in-
cremental recognition results and excludes the end-
of-utterance (phrase) results; Figure 2 shows incre-
mental recognition results and includes phrase re-
sults. It is useful to view these separately since the
phrase result, having access to all the speech, is sub-
277
Figure 1: Instantaneous semantic accuracy of incremental results, excluding phrase-final results
Figure 2: Instantaneous semantic accuracy of incremental and phrase-final results
stantially more accurate than the incremental results.
Figure 1 shows that the POMDP is more accu-
rate than the raw incremental result (excluding end-
of-phrase results). Its performance gain is minimal
in attempt 1 because the belief is informed only by
the prior. In attempt 2 and 3, the gain is larger
since the belief also benefits from the previous at-
tempts. Since the top POMDP result in subsequent
attempts is sometimes already correct (because it
incorporates past recognitions), the POMDP some-
times meets and occasionally exceeds the oracle dur-
ing the early portions of attempts 2 and 3.
Figure 2 shows that when end-of-phrase recog-
nition results are included, the benefit of the belief
state is limited to the initial portions of the second
and third turns. This is because the POMDP mod-
els are not fit well to the data: the models were
estimated from an earlier version of the system,
with a different user base and different functionality.
Identifying and eliminating this type of mismatch
is an important issue and has been studied before
(Williams, 2011).
Taken as a whole, we find that using belief track-
ing increases the accuracy of partials by over 8%
(absolute) in some cases. Even though the final
phrase results of the 1-best list are more accurate
than the belief state, the POMDP shows better ac-
curacy on the volatile incremental results. As com-
pared to the whole utterance results, incremental re-
sults have lower 1-best accuracy, yet high oracle ac-
curacy. This combination is a natural fit with the
POMDPs belief state, which considers the whole N-
Best list, effectively re-ranking it by synthesizing in-
formation from dialogue history priors.
6 Conclusion
This paper has taken a step toward integrating ISR
and POMDP-based dialogue systems. The Incre-
mental Interaction Manager (IIM) enables a tradi-
tional turn-based DM to make use of incremental
results and enjoy many their benefits. When this
IIM is paired with a POMDP DM, the interpreta-
tion accuracy of incremental results improves sub-
stantially. In the future we hope to build on this work
by incorporating Reinforcement Learning into turn-
taking and dialogue action decisions.
Acknowledgments
Thanks to Vincent Goffin for help with this work,
and to the anonymous reviewers for their comments
and critique. We acknowledge funding from the
NSF under grant IIS-0713698.
278
References
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of speech
recognition for incremental systems. In Proc. NAACL:
HLT, pages 380?388. Association for Computational
Linguistics.
O. Bu? and D. Schlangen. 2011. Dium?an incremen-
tal dialogue manager that can produce self-corrections.
Proceedings of semdial.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can i finish? learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages 11?
20, London, UK, September. Association for Compu-
tational Linguistics.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T WATSON speech
recognizer. In Proceedings of ICASSP, pages 1033?
1036.
D. Lu, T. Nishimoto, and N. Minematsu. 2011. Decision
of response timing for incremental speech recogni-
tion with reinforcement learning. In Automatic Speech
Recognition and Understanding (ASRU), 2011 IEEE
Workshop on, pages 467?472. IEEE.
D. Schlangen and G. Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 710?718. Association for Computational
Linguistics.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2011. Stability and accuracy in incremen-
tal speech recognition. In Proceedings of the SIGdial
2011.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
745?753. Association for Computational Linguistics.
B. Thomson and S. Young. 2010. Bayesian update of di-
alogue state: A pomdp framework for spoken dialogue
systems. Computer Speech & Language, 24(4):562?
588.
J.D. Williams and S. Balakrishnan. 2009. Estimating
probability of correctness for asr n-best lists. In Proc
SIGDIAL, London, United Kingdom.
J.D. Williams and S. Young. 2007. Partially observable
markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393?422.
J.D. Williams. 2011. An empirical evaluation of a sta-
tistical dialog system in public use. In Proceedings of
the SIGDIAL 2011 Conference, pages 130?141. Asso-
ciation for Computational Linguistics.
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24(2):150?174.
279
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 23?24,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A belief tracking challenge task for spoken dialog systems
Jason D. Williams
Microsoft Research, Redmond, WA 98052 USA
jason.williams@microsoft.com
Abstract
Belief tracking is a promising technique for
adding robustness to spoken dialog systems,
but current research is fractured across differ-
ent teams, techniques, and domains. This pa-
per amplifies past informal discussions (Raux,
2011) to call for a belief tracking challenge
task, based on the Spoken dialog challenge
corpus (Black et al, 2011). Benefits, limita-
tions, evaluation design issues, and next steps
are presented.
1 Introduction and background
In dialog systems, belief tracking refers to maintain-
ing a distribution over multiple dialog states as a di-
alog progresses. Belief tracking is desirable because
it provides robustness to errors in speech recogni-
tion, which can be quite common.
This distribution can be modeled in a variety
of ways, including heuristic scores (Higashinaka et
al., 2003), Bayesian networks (Paek and Horvitz,
2000; Williams and Young, 2007), and discrimi-
native models (Bohus and Rudnicky, 2006). Tech-
niques have been fielded which scale to realisti-
cally sized dialog problems and operate in real time
(Young et al, 2009; Thomson and Young, 2010;
Williams, 2010; Mehta et al, 2010). In lab settings,
belief tracking has been shown to improve overall
system performance (Young et al, 2009; Thomson
and Young, 2010).
Despite this progress, there are still important un-
resolved issues. For example, a deployment with
real callers (Williams, 2011) found that belief track-
ing sometimes degraded performance due to model
mis-matches that are difficult to anticipate at train-
ing time. What is lacking is a careful comparison
of methods to determine their relative strengths, in
terms of generalization, sample efficiency, speed,
etc.
This position paper argues for a belief tracking
challenge task. A corpus of labeled dialogs and scor-
ing code would be released. Research teams would
enter one or more belief tracking algorithms, which
would be evaluated on a held-out test set.
2 Corpus
The Spoken dialog challenge corpus is an attractive
corpus for this challenge. It consists of phone calls
from real (not simulated) bus riders with real (not
imagined) information needs. There have been 2
rounds of the challenge (2010, and 2011-2012), with
3 systems in each round. The rounds differed in
scope and (probably) user population. A total of 3
different teams entered systems, using different dia-
log designs, speech recognizers, and audio output.
For each system in each round, 500-1500 dialogs
were logged. While it would be ideal if the corpus
included more complex interactions such as negotia-
tions, as a publicly available corpus it is unparalleled
in terms of size, realism, and system diversity.
There are limitations to a challenge based on this
corpus: it would not allow comparisons across do-
mains, nor for multi-modal or situated dialog. These
aspects could be left for a future challenge. An-
other possible objection is that off-line experiments
would not measure end-to-end impact on a real di-
alog system; however, we do know that good be-
lief tracking improves dialog performance (Young
23
et al, 2009; Thomson and Young, 2010; Williams,
2011), so characterizing and improving belief track-
ing seems a logical next step. Moreover, building an
end-to-end dialog system is a daunting task, out of
reach of many research teams without specific fund-
ing. A corpus-based challenge has a much lower
barrier to entry.
3 Evaluation issues
There are many (not one!) metrics to evaluate. It
is crucial to design these in advance and implement
them as computer programs for use during develop-
ment. Specific metrics could draw on the follow-
ing core concepts. Baseline accuracy measures the
speech recognition 1-best ? i.e., accuracy without
belief tracking. 1-best accuracy measures how of-
ten the belief tracker?s 1-best hypothesis is correct.
Mean reciprocal rank measures the quality of the
ordering of the belief state, ignoring the probabili-
ties used to order; log-likelihood measures the qual-
ity of the probabilities. ROC curves measure the
1-best discrimination of the belief tracker at differ-
ent false-accept rates, or at the equal error rate.
An important question is at which turns to assess
the accuracy of the belief in a slot. For example, ac-
curacy could be measured at every turn; every turn
after a slot is first mentioned; only turns where a slot
is mentioned; only turns where a slot appears in the
speech recognition result; and so on. Depending on
the evaluation metric, it may be necessary to anno-
tate dialogs for the user?s goal, which could be done
automatically or manually. Another issue is how to
automatically determine whether a belief state value
is correct at the semantic level.
A final question is how to divide the corpus into a
training and test set in a way that measures robust-
ness to the different conditions. Perhaps some of the
data from the second round (which has not yet been
released) could be held back for evaluation.
4 Next steps
The next step is to form a group of interested re-
searchers to work through the issues above, partic-
ularly for the preparation of the corpus and evalu-
ation methodology. Once this is documented and
agreed, code to perform the evaluation can be de-
veloped, and additional labelling (if needed) can be
started.
Acknowledgments
Thanks to Antoine Raux for advocating for this chal-
lenge task, and for helpful discussions. Thanks also
to Spoken Dialog Challenge organizers Alan Black
and Maxine Eskenazi.
References
AW W Black, S Burger, A Conkie, H Hastie, S Keizer,
O Lemon, N Merigaud, G Parent, G Schubiner,
B Thomson, JD Williams, K Yu, SJ Young, and M Es-
kenazi. 2011. Spoken dialog challenge 2010: Com-
parison of live and control test results. In Proc SIG-
dial Workshop on Discourse and Dialogue, Portland,
Oregon.
D Bohus and AI Rudnicky. 2006. A ?K hypotheses +
other? belief updating model. In Proc AAAI Workshop
on Statistical and Empirical Approaches for Spoken
Dialogue Systems, Boston.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken dia-
logue systems. In Proc ACL, Sapporo.
N Mehta, R Gupta, A Raux, D Ramachandran, and
S Krawczyk. 2010. Probabilistic ontology trees for
belief tracking in dialog systems. In Proc SIGdial
Workshop on Discourse and Dialogue, Tokyo, Japan.
T Paek and E Horvitz. 2000. Conversation as action
under uncertainty. In Proc Conf on Uncertainty in Ar-
tificial Intelligence (UAI), Stanford, California, pages
455?464.
A Raux. 2011. Informal meeting on a belief tracking
challenge at interspeech.
B Thomson and SJ Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
JD Williams. 2010. Incremental Partition Recom-
biantion for Efficient Tracking of Multiple Dialogue
States. In ICASSP, Dallas, TX.
JD Williams. 2011. An empirical evaluation of a statis-
tical dialog system in public use. In Proc SIGDIAL,
Portland, Oregon, USA.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2009. The hidden information
state model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language.
24
Proceedings of the SIGDIAL 2013 Conference, pages 384?393,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Continuously Predicting and Processing Barge-in
During a Live Spoken Dialogue Task
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams1
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ, USA
1Microsoft Research, Redmond, WA, USA
selfridg@ohsu.edu
Abstract
Barge-in enables the user to provide input
during system speech, facilitating a more
natural and efficient interaction. Stan-
dard methods generally focus on single-
stage barge-in detection, applying the di-
alogue policy irrespective of the barge-in
context. Unfortunately, this approach per-
forms poorly when used in challenging
environments. We propose and evaluate
a barge-in processing method that uses a
prediction strategy to continuously decide
whether to pause, continue, or resume the
prompt. This model has greater task suc-
cess and efficiency than the standard ap-
proach when evaluated in a public spoken
dialogue system.
Index Terms: spoken dialogue systems, barge-in
1 Introduction
Spoken dialogue systems (SDS) communicate
with users with spoken natural language; the op-
timal SDS being effective, efficient, and natural.
Allowing input during system speech, known as
barge-in, is one approach that designers use to
improve system performance. In the ideal use
case, the system detects user speech, switches off
the prompt, and then responds to the user?s utter-
ance. Dialogue efficiency improves, as the sys-
tem receives information prior to completing its
prompt, and the interaction becomes more natu-
ral, as the system demonstrates more human-like
turn-taking behavior. However, barge-in poses a
number of new challenges; the system must now
recognize and process input during its prompt that
may not be well-formed system directed speech.
This is a difficult task and standard barge-in ap-
proaches often stop the prompt for input that will
not be understood, subsequently initiating a clari-
fication sub-dialogue (?I?m sorry, I didn?t get that.
You can say...etc.?). This non-understood barge-in
(NUBI) could be from environmental noise, non-
system directed speech, poorly-formed system di-
rected speech, legitimate speech recognition diffi-
culties (such as acoustic model mismatch), or any
combination thereof.
This paper proposes and evaluates a barge-in
processing method that focuses on handling NU-
BIs. Our Prediction-based Barge-in Response
(PBR) model continuously predicts interpretation
success by applying adaptive thresholds to incre-
mental recognition results. In our view, predicting
whether the recognition will be understood has far
more utility than detecting whether the barge-in
is truly system directed speech as, for many do-
mains, we feel only understandable input has more
discourse importance than system speech. If the
input is predicted to be understood, the prompt is
paused. If it is predicted or found to be NUBI, the
prompt is resumed. Using this method, the sys-
tem may resume speaking before recognition is
complete and will never initiate a clarifying sub-
dialogue in response to a NUBI. The PBR model
was implemented in a public Lets Go! statistical
dialogue system (Raux et al, 2005), and we com-
pare it with a system using standard barge-in meth-
ods. We find the PBR model has a significantly
better task success rate and efficiency.
Table 1 illustrates the NUBI responses produced
by the standard barge-in (Baseline) and PBR mod-
els. After both prompts are paused, the standard
method initiates a clarifying sub-dialogue whereas
PBR resumes the prompt.
We first provide background on Incremental
Speech Recognition and describe the relevant re-
lated work on barge-in. We then detail the
Prediction-based Barge-in Response model?s op-
eration and motivation before presenting a whole-
call and component-wise analysis of the PBR
1Work done while at AT&T Labs - Research
384
Table 1: System response to Non-Understood Barge-In (NUBI)
Baseline Ok, sixty one <NUBI> Sorry, say a bus route like twenty eight x
PBR Ok, sixty one <NUBI> sixty one c. Where are you leaving from?
model. The paper concludes with a discussion of
our findings and implications for future SDS.
2 Background and Related Work
Incremental Speech Recognition: Incremental
Speech Recognition (ISR) provides the real-time
information critical to the PBR model?s continu-
ous predictions. ISR produces partial recognition
results (?partials?) until input ceases and the ?fi-
nal? recognition result is produced following some
silence. As partials have a tendency to be revised
as more audio is processed, stability measures are
used to predict whether a given partial hypothe-
sis will be present in the final recognition result
(McGraw and Gruenstein, 2012; Selfridge et al,
2011). Here, we use Lattice-Aware ISR, which
produces partials after a Voice Activity Detector
(VAD) indicates speech and limits them to be a
complete language model specified phrase or have
guaranteed stability (Selfridge et al, 2011).
Barge-In: Using the standard barge-in model,
the system stops the prompt if barge-in is detected
and applies the dialogue logic to the final recogni-
tion result. This approach assumes that the barge-
in context should not influence the dialogue pol-
icy, and most previous work on barge-in has fo-
cused on detection: distinguishing system directed
speech from other environmental sounds. Cur-
rently, these methods are either based on a VAD
(e.g. (Stro?m and Seneff, 2000)), ISR hypothe-
ses (Raux, 2008), or some combination (Rose and
Kim, 2003). Both approaches can lead to detection
errors: background speech will trigger the VAD,
and partial hypotheses are unreliable (Baumann et
al., 2009). To minimize this, many systems only
enable barge-in at certain points in the dialogue.
One challenge with the standard barge-in model
is that detection errors can initiate a clarifying sub-
dialogue to non-system directed input, as it is un-
likely that this input will be understood (Raux,
2008). Since this false barge-in, which in most
cases is background speech (e.g. the television), is
highly indicative of poor recognition performance
overall, the system?s errant clarifying response can
only further degrade user experience.
Strom and Seneff (2000) provide, to our knowl-
edge, the only mature work that proposed deviat-
ing from the dialogue policy when responding to
a barge-in recognition. Instead of initiating a clar-
ifying sub-dialogue, the system produced a filled-
pause disfluency (?umm?) and resumed the prompt
at the phrase boundary closest to the prompt?s sus-
pension point. However, this model only operated
at the final recognition level (as opposed the incre-
mental level) and, unfortunately, they provide no
evaluation of their approach. An explicit compar-
ison between the approaches described here and
the PBR model is found in Section 3.5.
3 Prediction-based Barge-in Response
The PBR model is characterized by three high-
level states: State 1 (Speaking Prediction), whose
goal is to pause the prompt if stability scores pre-
dict understanding; State 2 (Silent Prediction),
whose goal is to resume the prompt if stability
scores and the incremental recognition rate pre-
dict non-understanding; and State 3 (Completion),
which operates on the final recognition result, and
resumes the prompt unless the recognition is un-
derstood and the new speech act will advance the
dialogue. Here, we define ?advancing the dia-
logue? to be any speech act that does not start a
clarifying sub-dialogue indicating a NUBI. Tran-
sitions between State 1 and 2 are governed by
adaptive thresholds ? repeated resumptions sug-
gest the user is in a noisy environment, so each
resumption increases the threshold required to ad-
vance from State 1 to State 2 and decreases the
threshold required to advance from State 2 to State
1. A high-level comparison of the standard model
and our approach is shown in Figure 1; a complete
PBR state diagram is provided in the Appendix.
3.1 State 1: Speaking Prediction
In State 1, Speaking Prediction, the system is both
speaking and performing ISR. The system scores
each partial for stability, predicting the probability
that it will remain ?stable? ? i.e., will not be later
revised ? using a logistic regression model (Self-
ridge et al, 2011). This model uses a number of
features related to the recognizer?s generic confi-
dence score, the word confusion network, and lat-
tice characteristics. Table 2 shows partial results
385
Table 2: Background noise and User Speech ISR
Background Noise User Utterance
Partial Stab. Scr. Partial Stab. Scr.
one 0.134 six 0.396
two 0.193 sixty 0.542
six 0.127 fifty one 0.428
two 0.078 sixty one a 0.491
and stability scores for two example inputs: back-
ground noise on the left, and the user saying ?sixty
one a? on the right.
State 1 relies on the internal threshold param-
eter, T1. If a partial?s stability score falls below
T1, control remains in State 1 and the partial re-
sult is discarded. If a stability score meets T1, the
prompt is paused and control transitions to State 2.
T1 is initially set to 0 and is adapted as the dialogue
progresses. The adaptation procedure is described
below in Section 3.4. If a final recognition result
is received, control transitions directly to State 3.
Transitioning from State 1 to State 2 is only al-
lowed during the middle 80% of the prompt; oth-
erwise only transitions to State 3 are allowed.1
3.2 State 2: Silent Prediction
Upon entering State 2, Silent Prediction, the
prompt is paused and a timer is started. State 2 re-
quires continuous evidence (at least every T2 ms)
that the ISR is recognizing valid speech and each
time a partial result that meets T1 is received, the
timer is reset. If the timer reaches the time thresh-
old T2, the prompt is resumed and control returns
to State 1. T2 is initially set at 1.0 seconds and is
adapted as the dialogue progresses. Final recogni-
tion results trigger a transition to State 3.
The resumption prompt is constructed using the
temporal position of the VAD specified speech
start to find the percentage of the prompt that was
played up to that point. This percentage is then
reduced by 10% and used to create the resump-
tion prompt by finding the word that is closest to,
but not beyond, the modified percentage. White
space characters and punctuation are used to deter-
mine word boundaries for text-to-speech prompts,
whereas automatically generated word-alignments
are used for pre-recorded prompts.
1We hypothesized that people will rarely respond to the
current prompt during the first 10% of prompt time as over-
laps at the beginning of utterances are commonly initiative
conflicts (Yang and Heeman, 2010). Users may produce
early-onset utterances during the last 10% that should not
stop the prompt as it is not an ?intentional? barge-in.
Figure 1: The Standard Barge-in and PBR Models
3.3 State 3: Completion
State 3, Completion, is entered when a final recog-
nition result is received and determines whether
the current dialogue policy will advance the dia-
logue or not. Here, the PBR model relies on the
ability of the dialogue manager (DM) to produce a
speculative action without transitioning to the next
dialogue state. If the new action will not advance
the dialogue, it is discarded and the recognition
is NUBI. However, if it will advance the dialogue
then it is classified as an Understood Barge-In
(UBI). In the NUBI case, the system either contin-
ues speaking or resumes the current prompt (tran-
sitioning to State 1). In the UBI case, the system
initiates the new speech act after playing a short
reaction sound and the DM transitions to the next
dialogue state. This reaction sound precedes all
speech acts outside the barge-in context but is not
used for resumption or timeout prompts. Note that
by depending solely on the new speech act, our
model does not require access to the DM?s internal
understanding or confidence scoring components.
3.4 Threshold adjustments
States 1 and 2 contain parameters T1 and T2 that
are adapted to the user?s environment. T1 is the
stability threshold used in State 1 and State 2 that
controls how stable an utterance must be before
the prompt should be paused. In quiet environ-
ments ? where only the user?s speech produces
partial results ? a low threshold is desirable as
it enables near-immediate pauses in the prompt.
Conversely, noisy environments yield many spu-
rious partials that (in general) have much lower
stability scores, so a higher threshold is advan-
tageous. T2 is the timing threshold used to re-
sume the prompt during recognition in State 2. In
quiet environments, a higher threshold reduces the
chance that the system will resume its prompt dur-
ing a well-formed user speech. In noisy environ-
386
Figure 2: Example dialogue fragment of PBR Model
ments, a lower threshold allows the system to re-
sume quickly as the NUBI likelihood is greater.
Both T1 and T2 are dependent on the number of
system resumptions, as we view the action of re-
suming the prompt as an indication that the thresh-
old is not correct. With every resumption, the pa-
rameter R is incremented by 1 and, to account for
changing environments, R is decremented by 0.2
for every full prompt that is not paused until it
reaches 0. UsingR, T1 is computed by T1 = 0.17?
R, and T2 by T2 = argmax(0.1, 1? (0.1 ?R)).2
3.5 Method Discussion
The motivation behind the PBR model is both the-
oretical and practical. According to Selfridge and
Heeman (2010), turn-taking is best viewed as a
collaborative process where the turn assignment
should be determined by the importance of the
utterance. During barge-in, the system is speak-
ing and so should only yield the turn if the user?s
speech is more important than its own. For many
domains, we view non-understood input as less
important than the system?s prompt and so, in this
case, the system should not release the turn by
stopping the prompt and initiating a clarifying sub-
dialogue. On the practical side, there is a high
likelihood that non-advancing input is not system
directed, to which the system should neither con-
sume, in terms of belief state updating, nor re-
spond to, in terms of asking for clarification. In
the rare case of non-understood system directed
speech, the user can easily repeat their utterance.
Here, we note that in the event that the user is
backchanneling, the PBR model will behave cor-
rectly and not release the turn.
The PBR approach differs from standard barge-
in approaches in several respects. First, standard
barge-in stops the prompt (i.e., transitions from
State 1 to State 2) if either the VAD or the partial
hypothesis suggests that there is speech; our ap-
proach? using acoustic, language model, and lat-
tice features ? predicts whether the input is likely
to contain an interpretable recognition result. Sec-
2The threshold update values were determined empiri-
cally by the authors.
ond, standard barge-in uses a static threshold; our
approach uses dynamic thresholds that adapt to
the user?s acoustic environment. Parameter adjust-
ments are straightforward since our method auto-
matically classifies each barge-in as NUBI or UBI.
In practice, the prompt will be paused incorrectly
only a few times in a noisy environment, after
which the adaptive thresholds will prevent incor-
rect pauses at the expense of being less responsive
to true user speech. If the noise level decreases,
the thresholds will become more sensitive again,
enabling swifter responses. Finally, with the ex-
ception of Strom and Seneff, standard approaches
always discard the prompt; our approach can re-
sume the prompt if recognition is not understood
or is proceeding poorly, enabling the system to
resume speaking before recognition is complete.
Moreover, resumption yields a natural user expe-
rience as it often creates a repetition disfluency
(?Ok, sixty - sixty one c?), which are rarely no-
ticed by the listener (Martin and Strange, 1968).
An example dialogue fragment is shown in Fig-
ure 2, with the state transitions shown above. Note
the transition from State 2 to State 1, which is the
system resuming speech during recognition. This
recognition stream, produced by non-system di-
rected user speech, does not end until the user says
?repeat? for the last time.
4 Evaluation Results
The PBR model was evaluated during the Spoken
Dialog Challenge 2012-2013 in a live Lets Go!
bus information task. In this task, the public can
access bus schedule information during off hours
in Pittsburgh, PA via a telephonic interaction with
a dialogue system (Raux et al, 2005). The task
can be divided into five sub-tasks: route, origin,
destination, date/time, and bus schedules. The last
sub-task, bus schedules, provides information to
the user whereas the first four gather information.
We entered two systems using the same POMDP-
based DM (Williams, 2012). The first system, the
?Baseline?, used the standard barge-in model with
VAD barge-in detection and barge-in disabled in
387
Figure 3: Estimated success rate for the PBR and Baseline systems. Stars indicate p<0.018 with ?2 test.
a small number of dialogue states that appeared
problematic during initial testing. The second sys-
tem used the PBR model with an Incremental In-
teraction Manager (Selfridge et al, 2012) to pro-
duce speculative actions in State 3. The pub-
lic called both systems during the final weeks of
2011 and the start of 2012. The DM applied a lo-
gistic regression based confidence measure to de-
termine whether the recognition was understood.
Both systems used the AT&T WATSONSM speech
recognizer (Goffin et al, 2005) with the same
sub-task specific rule-based language models and
standard echo cancellation techniques. The beam
width was set to maximize accuracy while still
running faster than real-time. The PBR system
used a WATSON modification to output lattice-
aware partial results.
Call and barge-in statistics are shown in Table
3. Here, we define (potential) barge-in (some-
what imprecisely) as a full recognition that at
some point overlaps with the system prompt, as
determined by the call logs. We show the calls
with barge-in before the bus schedule sub-task was
reached (BI-BS) and the calls with barge-in during
any point of the call (BI All). Since the Baseline
system only enabled barge-in at specific points in
the dialogue, it has fewer instances of barge-in
(Total Barge-In) and fewer barge-in calls. Regret-
fully, due to logging issues with the PBR system,
recognition specific metrics such as Word Error
Rate and true/false barge-in rates are unavailable.
4.1 Estimated Success Rate
We begin by comparing the success rate and
efficiency between the Baseline and PBR sys-
Table 3: Baseline and PBR call/barge-in statistics.
Baseline PBR
Total Calls 1027 892
BI-BS 228 (23%) 345 (39%)
BI All 281 (27%) 483 (54%)
Total Barge-In 829 1388
tems. Since task success can be quite difficult to
measure, we use four increasingly stringent task
success definitions: Bus Times Reached (BTR),
where success is achieved if the call reaches the
bus schedule sub-task; List Navigation (List Nav.),
where success is achieved if the user says ??next?,
?previous?, or ?repeat? ? the intuition being that
if the user attempted to navigate the bus sched-
ule sub-task they were somewhat satisfied with
the system?s performance so far; and Immediate
Exit (BTR2Ex and ListNav2Ex), which further
constrains both of the previous definitions to only
calls that finish directly after the initial visit to the
bus times sub-task. Success rate for the defini-
tions were automatically computed (not manually
labeled). Figure 3 shows the success rate of the
PBR and Baseline systems for all four definitions
of success. It shows, from left to right, Barge-In,
No Barge-In (NBI), and All calls. Here we restrict
barge-in calls to those where barge-in occurred
prior to the bus schedule task being reached.
For the calls with barge-in, a ?2 test finds sig-
nificant differences between the PBR and Base-
line for all four task success definitions. However,
we also found significant differences in the NBI
calls. This was surprising since, when barge-in
is not triggered, both systems are ostensibly the
same. We speculate this could be due to the Base-
line?s barge-in enabling strategy: an environment
that triggers barge-in in the Baseline would always
trigger barge-in in the PBR model, whereas the
converse is not true as the Baseline only enabled
barge-in in some of the states. This means that
there is a potential mismatch when separating the
calls based on barge-in, and so the fairest compar-
ison is using All the calls. This is shown on the far
right of Figure 3. We find that, while the effect is
not as large, there are significant differences in the
success rate for the PBR model for the most and
least stringent success definition, and very strong
trends for the middle two definitions (p < 0.07 for
BTR2Ex and p < 0.054 for List Nav.). Taken as
a whole, we feel this offers compelling evidence
388
Figure 4: Seconds from beginning of dialogue to
reaching the Bus Schedule Information sub-task
that the PBR method is more effective: i.e. yields
higher task completion.
Next, we turn our attention to task efficiency.
For this, we report the amount of clock time from
the beginning of the call to when the Bus Schedule
sub-task was reached. Calls that do not reach this
sub-task are obviously excluded, and PBR times
are adjusted for the reaction sound (explained in
Section 3.3). Task efficiency is reported by cu-
mulative percentage in Figure 4. We find that,
while the NBI call times are nearly identical for
both systems, the PBR barge-in calls are much
faster than the Baseline calls. Here, we do not
feel the previously described mismatch is partic-
ularly problematic as all the calls reached the goal
state and the NBI are nearly identical. In fact, as
more NUBI should actually reduce efficiency, the
potential mismatch only strengthens the result.
Taken together, these results provide substantial
evidence that the PBR model is more effective and
more efficient than the Baseline. In order to ex-
plain PBR?s performance, we explore the effect of
prediction and resumption in isolation.
4.2 State 1: Speaking Prediction
State 1 is responsible for pausing the prompt, the
goal being to pause the prompt for UBI input and
not to pause the prompt for NUBI input. The
prompt is paused if a partial?s stability score meets
or exceeds the T1 threshold. We evaluate the ef-
ficacy of State 1 and T1 by analyzing the statis-
tics of NUBI/UBI input and Paused/Not Paused
(hereafter Continued) prompts. Since resuming
the prompt during recognition affects the recog-
nition outcome, we restrict our analysis to recog-
nitions that do not transition from State 2 back
to State 1. For comparison we show the overall
UBI/NUBI percentages for the Baseline and PBR
systems. This represents the recognition distri-
Table 4: Evaluation of T1, off-line PBR, and Base-
line VAD. For T1 we respectively (?-? split) show
the UBI/NUBI % that are Paused/Continued, the
Paused/Continued % that are UBI/NUBI, and the
percentage over all recognitions
T1 (%) VAD (%)
Paused Continued PBR BL
UBI 72-40-26 28-29-10 36 54
NUBI 61-60-39 39-71-25 64 46
bution for the live Baseline VAD detection and
off-line speculation for the PBR model. Recall
PBR does have VAD activation preceding partial
results and so the off-line PBR VAD shows how
the model would have behaved if it only used the
VAD for detection, as the Baseline does.
Table 4 provides a number of percentages, with
three micro-columns separated by dashes (?-?) for
T1. The first micro-column shows the percent-
age of UBI/NUBI that either Paused or Contin-
ued the prompt (sums to 100 horizontally). The
second micro-column shows the percentage of
Paused/Continued that are UBI/NUBI (sums to
100 vertically). The third micro-column shows
the percentage of each combination (e.g. UBI and
Paused) over all the barge-in recognitions. The
VAD columns show the percentage of UBI/NUBI
that (would) pause the prompt.
We first look at UBI/NUBI percentage that are
Paused/Continued (first micro-column): We find
that 72% of UBI are paused and 28% are Contin-
ued versus 61% of NUBI that are Paused with 39%
Continued. We now look at the Paused/Continued
percentage that are UBI/NUBI (second micro-
column): We find that 40% of Paused are UBI
and 60% are NUBI, whereas 29% of Continued
are UBI and 71% are NUBI. So, while T1 sus-
pends the prompt for the majority of NUBI (not
desirable, though expected since T1 starts at 0),
it has high precision when continuing the prompt.
This reduces the number of times that the prompt
is paused erroneously for NUBI while minimizing
incorrect (UBI) continues. This is clearly shown
by considering all of the recognitions (third micro-
column). We find that PBR erroneously paused
the prompt for 39% of recognitions, as opposed to
64% for the off-line PBR and 46% for the Base-
line. This came at the cost of reducing the number
of correct (UBI) pauses to 26% from 36% (off-line
PBR) and 54% (Baseline VAD).
The results show that the T1 threshold had
389
Figure 5: Secs from Speech Start to Final Result
modest success at discriminating UBI and NUBI;
while continuing the prompt had quite a high
precision for NUBI, the recall was substantially
lower. We note that, since erroneous pauses lead
to resumptions and erroneous continues still lead
to a new speech act, there is minimal cost to these
errors. Furthermore, in our view, reducing the per-
centage of recognitions that pause and resume the
prompt is more critical as these needlessly disrupt
the prompt. In this, T1 is clearly effective, reduc-
ing the percentage from 64% to 39%.
4.3 State 2: Silent Prediction
State 2 governs whether the prompt will remain
paused or be resumed during incremental recogni-
tion. This decision depends on the time parameter
T2, which should trigger resumptions for NUBIs.
Since the act of resuming the prompt during recog-
nition changes the outcome of the recognition, it
is impossible to evaluate how well T2 discrimi-
nated recognition results. However, we can evalu-
ate the effect of that resumption by comparing UBI
percentages between the PBR and Baseline sys-
tems. We first present evidence that T2 is most ac-
tive during longer recognitions, and then show that
longer Baseline recognitions have a lower UBI
percentage than longer PBR recognitions specif-
ically because of T2 resumptions. ?Recognitions?
refer to speech recognition results, with ?longer?
or ?shorter? referring to the clock time between
speech detection and the final recognition result.
We first report the PBR and Baseline response
and recognition time. We separate the PBR barge-
in recognitions into two groups: State 2?State 3,
where the system never transitions from State 2
to State 1, and State 2?State 1, where the sys-
tem resumes the prompt during recognition, tran-
sitioning from State 2 to State 1. The cumulative
percentages of the time from speech detection to
final recognition are shown in Figure 5. We find
that the State 2?State 3 recognitions are far faster
Figure 6: UBI % by minimum recognition time
than the Baseline recognitions, which in turn are
far faster than the State 2?State 1 recognitions.
The difference between PBR and Baseline recog-
nitions implies that T2 has greater activation dur-
ing longer recognitions. Given this, the overall
barge-in response time for PBR should be faster
than the Baseline (as the PBR system is resum-
ing where the Baseline is silent). Indeed this is
the case: the PBR system?s overall mean/median
response time is 1.58/1.53 seconds whereas Base-
line has a mean/median response time of 2.61/1.8
seconds.
The goal of T2 is for the system to resume when
recognition is proceeding poorly, and we have
shown that it is primarily being activated during
longer recognitions. If T2 is functioning properly,
recognition length should be inversely related to
recognition performance, and longer recognitions
should be less likely to be understood. Further-
more, if T2 resumption improves the user?s expe-
rience then longer PBR recognitions should per-
form better than Baseline recognitions of compa-
rable length. Figure 6 presents the UBI percent-
age by the minimum time for recognitions that
reach State 2. We find that, when all recogni-
tions are accounted for (0 second minimum), the
Baseline has a higher rate of UBI. However, as
recognition time increases the Baseline UBI per-
centage decreases (suggesting successful T2 func-
tioning) whereas the PBR UBI percentage actu-
ally increases. Since longer PBR recognitions are
dominated by T2 resumptions, we speculate this
improvement is driven by users repeating or initi-
ating new speech that leads to understanding suc-
cess, as the PBR system is responding where the
Baseline system is silent.
4.4 Resumption
The PBR model relies on resumption to recover
from poor recognitions, either produced in State 2
or State 3. Instead of a resumption, the Baseline
390
Figure 7: Sub-Task Abandonment Rate. NUBI is
different at p < 0.003
system initiates a clarifying sub-dialogue when a
barge-in recognition is not understood. We com-
pare these two behaviors using the call abandon-
ment rate ? the user hangs-up ? of sub-tasks
with and without NUBI. Here, we exclude the Bus
Schedule sub-task as it is the goal state.
Figure 7 shows the call abandonment rate for
sub-tasks that either have or do not have NUBI.
We find that there is a significant difference in
abandoned calls for NUBI sub-tasks between the
two systems (33% vs 48%, p < 0.003 using a ?2
test), but that there is no difference for the calls
that do not have NUBI (7.6% vs 8.4%). This re-
sult shows that prompt resumption is viewed far
more favorably by users than initiating a clarify-
ing sub-dialogue.
5 Discussion and Conclusion
The above results offer strong evidence that the
PBR model increases task success and efficiency,
and we found that all three states contribute to
the improved performance by creating a more ro-
bust, responsive, and natural interaction. T1 pre-
diction in State 1 reduced the number of spurious
prompt suspensions, T2 prediction in State 2 led to
improved understanding performance, and prompt
resumption (States 2 and 3) reduced the number of
abandoned calls.
An important feature of the Prediction-based
Barge-in Response model is that, while it lever-
ages incremental speech processing for barge-in
processing, it does not require an incremental di-
alogue manager to drive its behavior. Since the
model is also domain independent and does not
require access to internal dialogue manager com-
ponents, it can easily be incorporated into any ex-
isting dialogue system. However, one limitation of
the current model is that the prediction thresholds
are hand-crafted. We also believe that substan-
tial improvements can be made by explicitly at-
tempting to predict eventual understanding instead
of using the stability score and partial production
rate as a proxy. Furthermore, the PBR model does
not distinguish between the causes of the non-
understanding, specifically whether the input con-
tained in-domain user speech, out-of-domain user
speech, or background noise. This case is specifi-
cally applicable in domains where system and user
speech are in the same channel, such as interact-
ing via speaker phone. In this context, the system
should be able to initiate a clarifying sub-dialogue
and release the turn, as the system must be more
sensitive to the shared acoustic environment and
so its current prompt may be less important than
the user?s non-understood utterance.
The results challenge a potential assumption re-
garding barge-in: that barge-in indicates greater
user pro-activity and engagement with the task.
One of the striking findings was that dialogues
with barge-in are slower and less successful than
dialogues without barge-in. This suggests that,
for current systems, dialogues with barge-in are
more indicative of environmental difficulty than
user pro-activity. The superior performance of
the PBR model, which is explicitly resistant to
non-system directed speech, implies that domi-
nant barge-in models will have increasingly lim-
ited utility as spoken dialogue systems become
more prevalent and are used in increasingly dif-
ficult environments. Furthermore, within the con-
text of overall dialogue systems, the PBR model?s
performance emphasizes the importance of contin-
uous processing for future systems.
This paper has proposed and evaluated the
Prediction-based Barge-in Response model. This
model?s behavior is driven by continuously pre-
dicting whether a barge-in recognition will be un-
derstood successfully, and combines incremental
speech processing techniques with a prompt re-
sumption procedure. Using a live dialogue task
with real users, we evaluated this model against
the standard barge-in model and found that it led
to improved performance in both task success and
efficiency.
Acknowledgments
Many thanks to Vincent Goffin for help with this
work, and to the anonymous reviewers for their in-
sightful comments and critique. We acknowledge
funding from the NSF under grant IIS-0713698.
391
References
T. Baumann, M. Atterer, and D. Schlangen. 2009. As-
sessing and improving the performance of speech
recognition for incremental systems. In Proc.
NAACL: HLT, pages 380?388. Association for Com-
putational Linguistics.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T WATSON
speech recognizer. In Proceedings of ICASSP, pages
1033?1036.
James G Martin and Winifred Strange. 1968. The per-
ception of hesitation in spontaneous speech. Percep-
tion & Psychophysics, 3(6):427?438.
Ian McGraw and Alexander Gruenstein. 2012. Es-
timating word-stability during incremental speech
recognition. In in Proc. of Interspeech 2012.
A. Raux, B. Langner, D. Bohus, A.W. Black, and
M. Eskenazi. 2005. Lets go public! taking a spo-
ken dialog system to the real world. In in Proc. of
Interspeech 2005.
A. Raux. 2008. Flexible Turn-Taking for Spoken Dia-
log Systems. Ph.D. thesis, CMU.
Richard C Rose and Hong Kook Kim. 2003. A
hybrid barge-in procedure for more reliable turn-
taking in human-machine dialog systems. In Auto-
matic Speech Recognition and Understanding, 2003.
ASRU?03. 2003 IEEE Workshop on, pages 198?203.
IEEE.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems.
In Proc. of ACL 2010, pages 177?185. Association
for Computational Linguistics.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2011. Stability and accuracy in incre-
mental speech recognition. In Proceedings of the
SIGdial 2011.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2012. Integrating incremental speech
recognition and pomdp-based dialogue systems. In
Proceedings of the SIGdial 2012.
Nikko Stro?m and Stephanie Seneff. 2000. Intelligent
barge-in in conversational systems. Procedings of
ICSLP.
Jason D Williams. 2012. A critical analysis of two sta-
tistical spoken dialog systems in public use. In Spo-
ken Language Technology Workshop (SLT), 2012
IEEE, pages 55?60. IEEE.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue?. Computer Speech
Language, 24(2):175 ? 189.
392
A Appendix
This diagram represents the possible operating positions the Prediction-based Barge-in Response
model can be in. If the prompt is complete, the PBR model applies the dialogue policy to the final
recognition result and initiates the on-policy speech act. If the prompt was finished without being paused
it decrements R. In the latter case (barge-in), it operates using the three states as described in Section 2.
When a partial is recognized the Stability Score is computed and compared to the T1 threshold parame-
ter. If the score is below T1 the partial is discarded. Otherwise, if the model is in State 1 (the prompt is
on) the prompt is paused, a timer is started, and control transitions to State 2. If the model is in State 2
the timer is restarted. After transitioning to State 2, control only returns to State 1 if the timer exceeds
T2. At this time, the prompt is resumed and the resumption parameter R is incremented. Control im-
mediately transitions to State 3 if a final recognition result is received. The result is evaluated by the
dialogue manager, and the new speech act is returned. If the speech act indicates the recognition was not
understood successfully, the system either resumes (if in State 1) or continues (if in State 2). In the case
of resumption, R is incremented. If the new speech act indicates understanding success, the new speech
is immediately produced.
393
Proceedings of the SIGDIAL 2013 Conference, pages 404?413,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
The Dialog State Tracking Challenge
Jason Williams1, Antoine Raux2?, Deepak Ramachandran3?, and Alan Black4
1Microsoft Research, Redmond, WA, USA 2Lenovo Corporation, Santa Clara, CA, USA
3Nuance Communications, Mountain View, CA, USA 4Carnegie Mellon University, Pittsburgh, PA, USA
jason.williams@microsoft.com araux@lenovo.com deepak.ramachandran@nuance.com awb@cmu.edu
Abstract
In a spoken dialog system, dialog state
tracking deduces information about the
user?s goal as the dialog progresses, syn-
thesizing evidence such as dialog acts over
multiple turns with external data sources.
Recent approaches have been shown to
overcome ASR and SLU errors in some
applications. However, there are currently
no common testbeds or evaluation mea-
sures for this task, hampering progress.
The dialog state tracking challenge seeks
to address this by providing a heteroge-
neous corpus of 15K human-computer di-
alogs in a standard format, along with a
suite of 11 evaluation metrics. The chal-
lenge received a total of 27 entries from 9
research groups. The results show that the
suite of performance metrics cluster into 4
natural groups. Moreover, the dialog sys-
tems that benefit most from dialog state
tracking are those with less discriminative
speech recognition confidence scores. Fi-
nally, generalization is a key problem: in
2 of the 4 test sets, fewer than half of the
entries out-performed simple baselines.
1 Overview and motivation
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog
in a process called dialog state tracking (DST).
For example, in a bus schedule information sys-
tem, the dialog state might indicate the user?s de-
sired bus route, origin, and destination. Dialog
state tracking is difficult because automatic speech
?Most of the work for the challenge was performed when
the second and third authors were with Honda Research In-
stitute, Mountain View, CA, USA
recognition (ASR) and spoken language under-
standing (SLU) errors are common, and can cause
the system to misunderstand the user?s needs. At
the same time, state tracking is crucial because
the system relies on the estimated dialog state to
choose actions ? for example, which bus schedule
information to present to the user.
Most commercial systems use hand-crafted
heuristics for state tracking, selecting the SLU re-
sult with the highest confidence score, and dis-
carding alternatives. In contrast, statistical ap-
proaches compute scores for many hypotheses for
the dialog state (Figure 1). By exploiting correla-
tions between turns and information from external
data sources ? such as maps, bus timetables, or
models of past dialogs ? statistical approaches can
overcome some SLU errors.
Numerous techniques for dialog state tracking
have been proposed, including heuristic scores
(Higashinaka et al, 2003), Bayesian networks
(Paek and Horvitz, 2000; Williams and Young,
2007), kernel density estimators (Ma et al, 2012),
and discriminative models (Bohus and Rudnicky,
2006). Techniques have been fielded which scale
to realistically sized dialog problems and operate
in real time (Young et al, 2010; Thomson and
Young, 2010; Williams, 2010; Mehta et al, 2010).
In end-to-end dialog systems, dialog state tracking
has been shown to improve overall system perfor-
mance (Young et al, 2010; Thomson and Young,
2010).
Despite this progress, direct comparisons be-
tween methods have not been possible because
past studies use different domains and system
components, for speech recognition, spoken lan-
guage understanding, dialog control, etc. More-
over, there is little agreement on how to evaluate
dialog state tracking. Together these issues limit
progress in this research area.
The Dialog State Tracking Challenge (DSTC)
provides a first common testbed and evaluation
404
Figure 1: Overview of dialog state tracking. In this example, the dialog state contains the user?s desired
bus route. At each turn t, the system produces a spoken output. The user?s spoken response is processed
to extract a set of spoken language understanding (SLU) results, each with a local confidence score. A
set of Nt dialog state hypotheses is formed by considering all SLU results observed so far, including the
current turn and all previous turns. Here, N1 = 3 and N2 = 5. The dialog state tracker uses features of
the dialog context to produce a distribution over all Nt hypotheses and the meta-hypothesis that none of
them are correct.
suite for dialog state tracking. The DSTC orga-
nizers made available a public, heterogeneous cor-
pus of over 15K transcribed and labeled human-
computer dialogs. Nine teams entered the chal-
lenge, anonymously submitting a total of 27 dialog
state trackers.
This paper serves two roles. First, sections 2
and 3 provide an overview of the challenge, data,
and evaluation metrics, all of which will remain
publicly available to the community (DST, 2013).
Second, this paper summarizes the results of the
challenge, with an emphasis on gaining new in-
sights into the dialog state tracking problem, in
Section 4. Section 5 briefly concludes.
2 Challenge overview
2.1 Problem statement
First, we define the dialog state tracking problem.
A dialog state tracker takes as input all of the ob-
servable elements up to time t in a dialog, includ-
ing all of the results from the automatic speech
recognition (ASR) and spoken language under-
standing (SLU) components, and external knowl-
edge sources such as bus timetable databases and
models of past dialogs. It also takes as input a
set of Nt possible dialog state hypotheses, where
a hypothesis is an assignment of values to slots in
the system. The tracker outputs a probability dis-
tribution over the set of Nt hypotheses, and the
meta-hypothesis REST which indicates that none
of them are correct. The goal is to assign probabil-
ity 1.0 to the correct state, and 0.0 to other states.
Note that the set of dialog states is given. Also
note that Nt varies with t ? typically as the dia-
log progresses and more concepts are discussed,
the number of candidate hypotheses increases. An
example is given in Figure 1.
In this challenge, dialog states are generated in
the usual way, by enumerating all slots values that
have appeared in the SLU N-best lists or system
output up until the current turn. While this ap-
proach precludes a tracker assigning a score to an
405
SLU value that has not been observed, the cardi-
nality of the slots is generally large, so the likeli-
hood of a tracker correctly guessing a slot value
which hasn?t been observed anywhere in the input
or output is vanishingly small.
2.2 Challenge design
The dialog state tracking challenge studies this
problem as a corpus-based task ? i.e., dialog state
trackers are trained and tested on a static corpus
of dialogs, recorded from systems using a variety
of state tracking models and dialog managers. The
challenge task is to re-run state tracking on these
dialogs ? i.e., to take as input the runtime system
logs including the SLU results and system output,
and to output scores for dialog states formed from
the runtime SLU results. This corpus-based de-
sign was chosen because it allows different track-
ers to be evaluated on the same data, and because a
corpus-based task has a much lower barrier to en-
try for research groups than building an end-to-end
dialog system.
In practice of course, a state tracker will be used
in an end-to-end dialog system, and will drive ac-
tion selection, thereby affecting the distribution of
the dialog data the tracker experiences. In other
words, it is known in advance that the distribu-
tion in the training data and live data will be mis-
matched, although the nature and extent of the
mis-match are not known. Hence, unlike much
of supervised learning research, drawing train and
test data from the same distribution in offline ex-
periments may overstate performance. So in the
DSTC, train/test mis-match was explicitly created
by choosing test data to be from different dialog
systems.
2.3 Source data and challenge corpora
The DSTC uses data from the public deployment
of several systems in the Spoken Dialog Challenge
(SDC) (Black et al, 2010), provided by the Dialog
Research Center at Carnegie Mellon University. In
the SDC, telephone calls from real passengers of
the Port Authority of Allegheny County, who runs
city buses in Pittsburgh, were forwarded to dialog
systems built by different research groups. The
goal was to provide bus riders with bus timetable
information. For example, a caller might want
to find out the time of the next bus leaving from
Downtown to the airport.
The SDC received dialog systems from three
different research groups, here called Groups A,
B, and C. Each group used its own ASR, SLU,
and dialog manager. The dialog strategies across
groups varied considerably: for example, Groups
A and C used a mixed-initiative design, where the
system could recognize any concept at any turn,
but Group B used a directed design, where the
system asked for concepts sequentially and could
only recognize the concept being queried. Groups
trialled different system variants over a period of
almost 3 years. These variants differed in acoustic
and language models, confidence scoring model,
state tracking method and parameters, number of
supported bus routes, user population, and pres-
ence of minor bugs. Example dialogs from each
group are shown in the Appendix.
The dialog data was partitioned into 5 train-
ing corpora and 4 testing corpora (Table 1).
The partioning was intended to explore different
types of mis-match between the training and test
data. Specifically, the dialog system in TRAIN1A,
TRAIN1B, TRAIN1C, TRAIN2, and TEST1 are all
very similar, so TEST1 tests the case where there
is a large amount of similar data. TEST2 uses the
same ASR and SLU but a different dialog con-
troller, so tests the case where there is a large
amount of somewhat similar data. TEST3 is very
similar to TRAIN3 and tests the case where there
is a small amount of similar data. TEST4 uses a
completely different dialog system to any of the
training data.
2.4 Data preparation
The dialog system log data from all three groups
was converted to a common format, which
described SLU results and system output using
a uniform set of dialog acts. For example, the
system speech East Pittsburgh Bus Schedules.
Say a bus route, like 28X, or say I?m not sure.
was represented as hello(), request(route), exam-
ple(route=28x), example(route=dontknow). The
user ASR hypothesis the next 61c from oakland to
mckeesport transportation center was represented
as inform(time.rel=next), inform(route=61c),
inform(from.neighborhood=oakland), in-
form(to.desc=?mckeesport transportation
center?). In this domain there were a total
of 9 slots: the bus route, date, time, and three
components each for the origin and destination,
corresponding to streets, neighborhoods, and
points-of-interest like universities. For complete
details see (Williams et al, 2012).
406
TRAIN TEST
1A 1B 1C 2 3 1 2 3 4
Group A A A A B A A B C
Year(s) 2009 2009 2009 2010 2010 2011 2012 2011-2 2010
Dialogs 1013 1117 9502 643 688 715 750 1020 438
Turns/Dialog 14.7 13.3 14.5 14.5 12.6 14.1 14.5 13.0 10.9
Sys acts/turn 4.0 3.8 3.8 4.0 8.4 2.8 3.2 8.2 4.6
Av N-best len 21.7 22.3 21.9 22.4 2.9 21.2 20.5 5.0 3.2
Acts/N-best hyp 2.2 2.2 2.2 2.3 1.0 2.1 2.0 1.0 1.6
Slots/turn 44.0 46.5 45.6 49.0 2.1 41.4 36.9 4.3 3.5
Transcribed? yes yes yes yes yes yes yes yes yes
Labelled? yes no no yes yes yes yes yes yes
1-best WER 42.9% 41.1% 42.1% 58.2% 40.5% 57.9% 62.1% 48.1% 55.6%
1-best SLU Prec. 0.356 - - 0.303 0.560 0.252 0.275 0.470 0.334
1-best SLU Recall 0.522 - - 0.388 0.650 0.362 0.393 0.515 0.376
N-best SLU Recall 0.577 - - 0.485 0.738 0.456 0.492 0.634 0.413
Table 1: Summary of the datasets. One turn includes a system output and a user response. Slots are
named entity types such as bus route, origin neighborhood, date, time, etc. N-best SLU Recall indicates
the fraction of concepts which appear anywhere on the SLU N-best list.
Group B and C systems produced N-best lists
of ASR and SLU output, which were included in
the log files. Group A systems produced only 1-
best lists, so for Group A systems, recognition was
re-run with the Pocketsphinx speech recognizer
(Huggins-Daines et al, 2006) with N-best output
enabled, and the results were included in the log
files.
Some information in the raw system logs was
specific to a group. For example, Group B?s logs
included information about word confusion net-
works, but other groups did not. All of this infor-
mation was included in a ?system specific? sec-
tion of the log files. Group A logs contained about
40 system-specific name/value pairs per turn, and
Group B about 600 system-specific name/value
pairs per turn. Group C logs contained no system
specific data.
3 Labeling and evaluation design
The output of a dialog state tracker is a proba-
bility distribution over a set of given dialog state
hypotheses, plus the REST meta-hypothesis. To
evaluate this output, a label is needed for each di-
alog state hypothesis indicating its correctness.
In this task-oriented domain, we note that the
user enters the call with a specific goal in mind.
Further, when goal changes do occur, they are
usually explicitly marked: since all of the sys-
tems first collect slot values, and then provide bus
timetables, if the user wishes to change their goal,
they need to start over from the beginning. These
?start over? transitions are obvious in the logs.
This structure allows the correctness of each di-
alog state to be equated to the correctness of the
SLU items it contains. As a result, in the DSTC
we labeled the correctness of SLU hypotheses in
each turn, and then assumed these labels remain
valid until either the call ends, or until a ?start
over? event. Thus to produce the labels, the la-
beling task followed was to assign a correctness
value to every SLU hypothesis on the N-best list,
given a transcript of the words actually spoken in
the dialog up to the current turn.
To accomplish this, first all user speech was
transcribed. The TRAIN1 datasets had been tran-
scribed using crowd-sourcing in a prior project
(Parent and Eskenazi, 2010); the remainder were
transcribed by professionals. Then each SLU hy-
pothesis was labled as correct or incorrect. When a
transcription exactly and unambiguously matched
a recognized slot value, such as the bus route
?sixty one c?, labels were assigned automati-
cally. The remainder were assigned using crowd-
sourcing, where three workers were shown the true
words spoken and the recognized concept, and
asked to indicate if the recognized concept was
correct ? even if it did not match the recognized
words exactly. Workers were also shown dialog
407
history, which helps decipher the user?s meaning
when their speech was ambiguous. If the 3 work-
ers were not unanimous in their labels (about 4%
of all turns), the item was labeled manually by the
organizers. The REST meta-hypothesis was not
explicitly labeled; rather, it was deemed to be cor-
rect if none of the prior SLU results were labeled
as correct.
In this challenge, state tracking performance
was measured on each of the 9 slots separately,
and also on a joint dialog state consisting of all the
slots. So at each turn in the dialog, a tracker output
10 scored lists: one for each slot, plus a 10th list
where each dialog state contains values from all
slots. Scores were constrained to be in the range
[0, 1] and to sum to 1.
To evaluate tracker output, at each turn, each hy-
pothesis (including REST) on each of the 10 lists
was labeled as correct or incorrect by looking up
its corresponding SLU label(s). The scores and la-
bels over all of the dialogs were then compiled to
compute 11 metrics. Accuracy measures the per-
cent of turns where the top-ranked hypothesis is
correct. This indicates the correctness of the item
with the maximum score. L2 measures the L2 dis-
tance between the vector of scores, and a vector of
zeros with 1 in the position of the correct hypoth-
esis. This indicates the quality of all scores, when
the scores as viewed as probabilities.
AvgP measures the mean score of the first cor-
rect hypothesis. This indicates the quality of the
score assigned to the correct hypothesis, ignoring
the distribution of scores to incorrect hypotheses.
MRR measures the mean reciprocal rank of the
first correct hypothesis. This indicates the quality
of the ordering the scores produces (without nec-
essarily treating the scores as probabilities).
The remaining measures relate to receiver-
operating characteristic (ROC) curves, which
measure the discrimination of the score for the
highest-ranked state hypothesis. Two versions
of ROC are computed ? V1 and V2. V1 com-
putes correct-accepts (CA), false-accepts (FA),
and false-rejects (FR) as fractions of all utter-
ances, so for example
CA.V 1(s) = #CA(s)N (1)
where #CA(s) indicates the number of correctly
accepted states when only those states with score
? s are accepted, and N is the total number
of states in the sample. The V1 metrics are a
20%
30%
40%
50%
60%
70%
80%
90%
100%
sche
dule
2 ac
cura
cy fo
r all
 slot
s
Trackers Oracle Baseline0 Baseline1
train293% test175% test289% test348%train382% test438%
Figure 2: Schedule2 accuracy averaged over slots
for every tracker on every dataset. Percentages un-
der the datasets indicate the percent of the track-
ers which exceeded the performance of both base-
lines.
useful indication of overall performance because
they combine discrimination and overall accuracy
? i.e., the maximum CA.V 1(s) value is equal to
accuracy computed above.
V2 considers fractions of correctly classified ut-
terances, so for example
CA.V 2(s) = #CA(s)#CA(0) . (2)
The V2 metrics are useful because they measure
the discrimination of the scoring independently of
accuracy ? i.e., the maximum value of CA.V 2(s)
is always 1, regardless of accuracy.
From these ROC statistics, several met-
rics are computed. ROC.V1.EER computes
FA.V 1(s) where FA.V 1(s) = FR.V 1(s).
The metrics ROC.V1.CA05, ROC.V1.CA10,
and ROC.V1.CA20 compute CA.V 1(s) when
FA.V 1(s) = 0.05, 0.10, and 0.20 respec-
tively. ROC.V2.CA05, ROC.V2.CA10, and
ROC.V2.CA20 do the same using the V2 ver-
sions.
Apart from what to measure, there is currently
no standard that specifies when to measure ? i.e.,
which turns to include when computing each met-
ric. So for this challenge, a set of 3 schedules were
used. schedule1 includes every turn. schedule2
include turns where the target slot is either present
on the SLU N-best list, or where the target slot
is included in a system confirmation action ? i.e.,
where there is some observable new information
408
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0% 20% 40% 60% 80% 100%
True 
posit
ive ra
te
 
False positive rate  
test4test3test2test1
Figure 3: Receiver operating characteristc (ROC)
curve for SLU confidence scores of the 1-best hy-
pothesis in the test datasets. The SLU confidence
score in TEST3 is most discriminative; TEST1 and
TEST2 are the least discriminative.
about the target slot. schedule3 includes only the
last turn of a dialog.
In sum, for each tracker, one measurement is re-
ported for each test set (4), schedule (3), and met-
ric (11) for each of the 9 slots, the ?joint? slot, and
a weighted average of the individual slots (11), for
a total of 4 ? 3 ? 11 ? 11 = 1452 measurements per
tracker. In addition, each tracker reported average
latency per turn ? this ranged from 10ms to 1s.
3.1 Baseline trackers
For comparisons, two simple baselines were im-
plemented. The first (Baseline0) is a majority
class baseline that always guesses REST with
score 1. The second (Baseline1) follows simple
rules which are commonly used in spoken dialog
systems. It maintains a single hypothesis for each
slot. Its value is the SLU 1-best with the highest
confidence score observed so far, with score equal
to that SLU item?s confidence score.
4 Results and discussion
Logistically, the training data and labels, bus
timetable database, scoring scripts, and baseline
system were publicly released in late December
2012. The test data (without labels) was released
on 22 March 2013, and teams were given a week to
run their trackers and send results back to the orga-
nizers for evaluation. After the evaluation, the test
labels were published. Each team could enter up
to 5 trackers. For the evaluation, teams were asked
to process the test dialogs online ? i.e., to make a
1
3
5
7
9
11
13
15
17
19
accuracy l2 roc.v1_eer roc.v2_ca05
Average
 rank
 in tes
t datase
ts
Metric - schedule2 - weighted average over all slots
T3.E2T5.E1T5.E2T5.E5T6.E2T6.E3T6.E4T6.E5T9.E1
Figure 4: Average rank of top-performing trackers
for the four metrics identified in Figure 6. Rank-
ing was done using the given metric, schedule2,
and the weighted average of all slots. Tn.Em in-
dicates team n, entry m.
single pass over the data, as if the tracker were be-
ing run in deployment. Participation was open to
researchers at any institution, including the orga-
nizers and advisory board. To encourage partici-
pation, the organizers agreed not to identify par-
ticipants in publications, and there was no require-
ment to disclose how trackers were implemented.
9 teams entered the DSTC, submitting a total of
27 trackers. The raw output and all 1452 measure-
ments for each tracker (and the 2 baselines) are
available from the DSTC homepage (DST, 2013).
4.1 Analysis of trackers and datasets
We begin by looking at one illustrative metric,
schedule2 accuracy averaged over slots, which
measures the accuracy of the top dialog hypothe-
sis for every slot when it either appears on the SLU
N-best list or is confirmed by the system.1 Results
in Figure 2 show two key trends. First, relative
to the baselines, performance on the test data is
markedly lower than the training data. Comparing
TRAIN2 to TEST1/TEST2 and TRAIN3 to TEST3,
the relative gain over the baselines is much lower
on test data. Moreover, only 38% of trackers per-
formed better than a simple majority-class base-
line on TEST4, for which there was no matched
training data. These findings suggests that gen-
eralization is an important open issues for dialog
state trackers.
Second, Figure 2 indicates that the gains made
1Results using the joint dialog state are broadly similar,
and are omitted for space.
409
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
Trackers Baseline0 Baseline1
(a) TEST1
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(b) TEST2
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(c) TEST3
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(d) TEST4
Figure 5: Percent of highest-scored dialog state hypotheses which did not appear in the top-ranked SLU
position vs. schedule2 accuracy over all slots. Trackers ? including those with the highest accuracy ?
for TEST1 and TEST2 rarely assigned the highest score to an SLU hypothesis other than the top. All
trackers for TEST3 and TEST4 assigned the highest score to an SLU hypothesis other than the top in a
non-trivial percent of turns.
by the trackers over the baselines are larger
for Group A systems (TEST1 and TEST2) than
for Group B (TEST3) and C (TEST4) systems.
Whereas the baselines consider only the top SLU
hypothesis, statistical trackers can make use of
the entire N-best list, increasing recall ? compare
the 1-best and N-best SLU recall rates in Table 1.
However, Group A trackers almost never assigned
the highest score to an item below the top position
in the SLU N-best list. Rather, the larger gains for
Group A systems seem due to the relatively poor
discrimination of Group A?s SLU confidence score
(Figure 3): whereas the trackers use a multitude
of features to assign scores, the baselines rely en-
tirely on the SLU confidence for their scores, so
undiscriminative SLU confidence measures ham-
per baseline performance.
4.2 Analysis of metrics
This challenge makes it possible to study the em-
pirical differences among the evaluation metrics.
Intuitively, if the purpose of a metric is to order
a set of trackers from best to worst, then 2 met-
rics are similar if they yield a similar ordering over
trackers. Specifically, for every metricm, we have
a value x(m, d, s, t) where d is the dataset, and
s is the evaluation schedule, and t is the tracker.
We define r(m, d, s, t) as the rank of tracker t
when ordered using metric m, dataset d and eval-
uation schedule s. Using these ranks, we compute
Kendall?s Tau for every d, s, and pair of metrics
m1 and m2 (Kendall, 1938). We then compute the
average Kendall?s Tau for m1 and m2 by averag-
ing over all d and s.2
Results are in Figure 6. Here we see 4 natu-
ral clusters emerge: a cluster for correctness with
Accuracy, MRR, and the ROC.V1.CA measures; a
cluster for probability quality with L2 and Aver-
age score; and two clusters for score discrimina-
tion ? one with ROC.V1.EER and the other with
the three ROC.V2 metrics. This finding suggest
2A similar analysis over schedules showed that the differ-
ences in ranking for different schedules were smaller than for
metrics.
410
accuracy
mrr
roc_v1.ca05
roc_v1.ca10
roc_v1.ca20
roc.v1_eer
avgp
l2
roc.v2_ca05
roc.v2_ca05
roc.v2_ca05
Figure 6: Average divergence between rank orderings produced by different metrics. The size of a circle
at (x, y) is given by 1?? , where ? is the average Kendall?s Tau computed on the rank orderings produced
by methods x and y. Larger circles indicate dissimilar rankings; smaller circles indicate similar rankings;
missing circles indicate identical rankings. The red boxes indicate groups of metrics that yield similar
rankings.
that measuring one metric from each cluster will
contain nearly the same information as all 9 met-
rics. For example, one might report only Accu-
racy, L2, ROC.V1.EER, and ROC.V2.CA5.
Using these 4 metrics, we rank-ordered each
tracker, using schedule2 and a weighted average
of all slots. We then computed the average rank
across the 4 test sets. Finally we selected the set
of trackers with the top three average ranks for
each metric. Results in Figure 4 emphasize that
different trackers are tuned for different perfor-
mance measures, and the optimal tracking algo-
rithm depends crucially on the target performance
measure.
5 Conclusion
The dialog state tracking challenge has provided
the first common testbed for this task. The data,
evaluation tools, and baselines will continue to be
freely available to the research community (DST,
2013). The details of the trackers themselves will
be published at SIGDIAL 2013.
The results of the challenge show that the
suite of performance metrics cluster into 4 natural
groups. We also find that larger gains over conven-
tional rule-based baselines are present in dialog
systems where the speech recognition confidence
score has poor discrimination. Finally, we observe
substantial limitations on generalization: in mis-
matched conditions, around half of the trackers en-
tered did not exceed the performance of two sim-
ple baselines.
In future work, it should be verified that im-
provements in dialog state tracking lead to im-
provements in end-to-end dialog performance
(e.g., task completion, user satisfaction, etc.). In
addition, it would be interesting to study dialogs
where goal changes are more common.
Acknowledgements
The organizers thank the advisory board for their
valuable input on the design of the challenge:
Daniel Boies, Paul Crook, Maxine Eskenazi, Mil-
ica Gasic, Dilek Hakkani-Tur, Helen Hastie, Kee-
Eung Kim, Ian Lane, Sungjin Lee, Teruhisa Misu,
Olivier Pietquin, Joelle Pineau, Blaise Thomson,
David Traum, and Luke Zettlemoyer. The orga-
nizers also thank Ian Lane for his support for tran-
scription, and Microsoft and Honda Research In-
stitute USA for funding the challenge. Finally,
we thank the participants for making the challenge
successful.
411
References
AW Black, S Burger, B Langner, G Parent, and M Es-
kenazi. 2010. Spoken dialog challenge 2010. In
Proc SLT, Berkeley.
D Bohus and AI Rudnicky. 2006. A ?K hypotheses +
other? belief updating model. In Proc AAAI Work-
shop on Statistical and Empirical Approaches for
Spoken Dialogue Systems, Boston.
2013. Dialog State Tracking Challenge Home-
page. http://research.microsoft.com/
events/dstc/.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken di-
alogue systems. In Proc ACL, Sapporo.
D Huggins-Daines, M Kumar, A Chan, A W Black,
M Ravishankar, and A I Rudnicky. 2006. Pock-
etSphinx: A Free, Real-Time Continuous Speech
Recognition System for Hand-Held Devices. In
Proc ICASSP, Toulouse.
M Kendall. 1938. A new measure of rank correlation.
Biometrika, 30(1-2):81?89.
Y Ma, A Raux, D Ramachandran, and R Gupta. 2012.
Landmark-based location belief tracking in a spoken
dialog system. In Proc SigDial, Seoul.
N Mehta, R Gupta, A Raux, D Ramachandran, and
S Krawczyk. 2010. Probabilistic ontology trees for
belief tracking in dialog systems. In Proc SigDial,
Tokyo.
T Paek and E Horvitz. 2000. Conversation as ac-
tion under uncertainty. In Proc UAI, Stanford, pages
455?464.
G Parent and M Eskenazi. 2010. Toward Bet-
ter Crowdsourced Transcription: Transcription of a
Year of the Let?s Go Bus Information System Data.
In Proc SLT, Berkeley.
B Thomson and SJ Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams, A Raux, D Ramachandran, and
AW Black. 2012. Dialog state tracking challenge
handbook. Technical report, Microsoft Research.
JD Williams. 2010. Incremental partition recombina-
tion for efficient tracking of multiple dialogue states.
In Proc. of ICASSP.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The hid-
den information state model: a practical framework
for POMDP-based spoken dialogue management.
Computer Speech and Language, 24(2):150?174.
Appendix 1: Sample dialog excerpts
Below are transcripts of calls in the DSTC corpus.
The user?s side shows utterances as transcribed by
human annotators.
Group A
SYS: Welcome to the CMU Let?s Go bus informa-
tion system. To get help at any time, just say Help
or press zero. What can I do for you?
USR: 61c evening hours
SYS: For example, you can say, when is the next
28X from DOWNTOWN to THE AIRPORT? or
I?d like to go from MCKEESPORT to HOME-
STEAD tomorrow at 10 a.m.
USR: i?d like to go [noise]
SYS: Where are you leaving from?
USR: from Kennywood tomorrow at 6:30 PM
SYS: Leaving from MCKEE. Is this correct?
USR: yes
SYS: Okay What is your destination?
USR: downtown
SYS: Going to DOWNTOWN. Is this correct?
USR: yes
SYS: Right When do you want to travel?
USR: 6:30 PM
SYS: Leaving at 6 30 p.m.. Is this correct?
USR: [noise] yes
SYS: Alright Hold on. I?ll look that up. There is
a 71D leaving FIFTH AVENUE AT DARRAGH
MCKEE at 6 40 p.m.. It will arrive at FIFTH AV-
ENUE AT MARKET at 6 58 p.m.. To get more
information about buses related to this trip, you
can say, when is the next bus, or, when is the pre-
vious bus. To ask about a different trip, you can
say, start a new query. If you are finished, you can
say goodbye.
USR: start a new query
SYS: Okay, let?s start from the beginning.
USR: ...
Group B
SYS: East Pittsburgh Bus Schedules. Say a bus
route, like 28X, or say I?m not sure.
USR: 61A
SYS: Okay, 61A. To change, say go back. Where
are you leaving from?
USR: Downtown
SYS: Okay, downtown. You can always say go
back. And where are you going to?
USR: East Pittsburgh East Pittsburgh
SYS: Pittsburgh. Is that right?
412
USR: No
SYS: Oh, sorry, where are you going to?
USR: East Pittsburgh
SYS: Okay, East Pittsburgh. Do you want times
for the next few buses? Say yes or no.
USR: Yes
SYS: Ok, the next few buses. To change, say go
back. At 7:45 PM today, there is a 61 A, from
Fifth Avenue And Market Street, Downtown, ar-
riving Dynamo Alley And Beech Avenue, In East
Pittsburgh, at 8:42 PM. Say repeat, next, previous,
or start over.
Group C
SYS: Welcome to the Lets Go! bus information
system for Pittsburgh. Where would you like to
leave from?
USR: [noise]
SYS: Where do you wanna leave from?
USR: [noise] um I?m leaving from Forbes avenue
and Murray avenue
SYS: I?m sorry but I do not know any stops in the
moon neighborhood. The routes I currently cover
are the following: 28X, 54C, 56U, 61A, 61B, 61C,
61D and 64.
USR: um I need the scheduling information for the
61D
SYS: Where would you like to leave from?
USR: Forbes and Murray
SYS: Where do you wanna go?
USR: Greenfield
SYS: When are you going to take that bus?
USR: 61D
SYS: When are you going to take that bus?
USR: 61D
SYS: When would you like to travel? Say next bus
for the the next bus .
USR: next bus
SYS: The next bus from forbes avenue at murray
to greenfield is the 61D, leaving at 8 43 p.m. . It
arrives at murray avenue at loretta at 8 52 p.m. .
You may now say. next bus, previous bus, restart,
or goodbye.
USR: [noise] good-bye
SYS: Thank you goodbye
Appendix 2: Example log fragment
Following is a log from a call in TRAIN3. Someelements have been omitted for space. Logs areformatted in JavaScript Object Notation (JSON).
{
"session-id": "dt-201007221854-4808D",
"turns": [ {
"input": {
"asr-hyps": [
{ "asr-hyp": "sixty one b",
"score": 0.834314,
},
...
],
"slu-hyps": [
{ "slu-hyp": [
{"act": "inform",
"slots": [["route", "61b"]]},
],
"score": 0.834314,
},
...
],
"audio-file": "002.raw",
"start-time": 7.504
},
"output": {
"start-time": 0.078,
"transcript": "East Pittsburgh Bus
Schedules. Say a bus route, like
28X, or say I?m not sure.",
"dialog-acts": [
{ "act": "hello",
"slots": [] },
{ "act": "request",
"slots": [["route",null]] },
{ "act": "example",
"slots": [["route","28x"]] },
{ "act": "example",
"slots": [["route","dontknow"]] }
],
}
"system-specific": {
"wcn": {
"best_path_score": 0.9965,
"best_path": {
"word": ["sixty","one","b"],
"prob": [ 1.0,1.0,0.9965 ],
}
"network": [ ... ]
},
"nbest": [
{
"normSpeechLhood": -152.654,
"lastSpeechFrame": 266,
"numFrames": 354,
"udelta": -3.0280,
"speechLikelihood": -15876.0,
},
...
],
...
},
},
...
]
}
413
Proceedings of the SIGDIAL 2013 Conference, pages 433?441,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Multi-domain learning and generalization in dialog state tracking
Jason D. Williams
Microsoft Research, Redmond, WA, USA
jason.williams@microsoft.com
Abstract
Statistical approaches to dialog state track-
ing synthesize information across multi-
ple turns in the dialog, overcoming some
speech recognition errors. When training
a dialog state tracker, there is typically
only a small corpus of well-matched dia-
log data available. However, often there is
a large corpus of mis-matched but related
data ? perhaps pertaining to different se-
mantic concepts, or from a different dialog
system. It would be desirable to use this
related dialog data to supplement the small
corpus of well-matched dialog data. This
paper addresses this task as multi-domain
learning, presenting 3 methods which syn-
thesize data from different slots and differ-
ent dialog systems. Since deploying a new
dialog state tracker often changes the re-
sulting dialogs in ways that are difficult to
predict, we study how well each method
generalizes to unseen distributions of dia-
log data. Our main result is the finding that
a simple method for multi-domain learn-
ing substantially improves performance in
highly mis-matched conditions.
1 Introduction
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog in a
process called dialog state tracking. For example,
in a bus schedule information system, the dialog
state might indicate the user?s desired bus route,
origin, and destination. Dialog state tracking is
difficult because errors in automatic speech recog-
nition (ASR) and spoken language understanding
(SLU) are common, and can cause the system to
misunderstand the user?s needs. At the same time,
state tracking is crucial because the system relies
on the estimated dialog state to choose actions ?
for example, which bus schedule information to
present to the user.
Most commercial systems use hand-crafted
rules for state tracking, selecting the SLU result
with the highest confidence score observed so far,
and discarding alternatives. In contrast, statisti-
cal approaches compute a posterior distribution
over many hypotheses for the dialog state, and
in general these have been shown to be superior
(Horvitz and Paek, 1999; Williams and Young,
2007; Young et al, 2009; Thomson and Young,
2010; Bohus and Rudnicky, 2006; Metallinou et
al., 2013).
Unfortunately, when training a dialog state
tracker, there is rarely a large corpus of matched
data available. For example, a pilot version of the
system may be fielded in a controlled environment
to collect a small initial corpus. Yet there is of-
ten a large quantity of mis-matched dialog data
available. For example, dialog data might be avail-
able from another dialog system ? such as an ear-
lier version with a different recognizer, dialog con-
troller, and user population ? or from a related task
? such as searching for restaurants instead of ho-
tels.
In this paper, we tackle the general problem of
how to make use of disparate sources of data
when training a dialog state tracker. For exam-
ple, should a tracker for each slot be trained on
small sets of slot-specific data, or should data from
all slots be combined somehow? Can dialog data
from another system be used to build effective
tracker for a new system for which no data (yet)
exists? Once data from the new system is avail-
able, is the old data still useful?
These inter-related questions can be formalized
as multi-domain learning and generalization.
Multi-domain learning (MDL) refers to the task
of building a model ? here, a state tracker ? for
433
a target domain using training data from both the
target domain and a different but related domain.
Generalization refers to the ability of a model to
perform well in a domain unlike that seen in any
of the training data. Both multi-domain learning
and generalization are active research topics in the
machine learning community, with broad applica-
tions. (Joshi et al, 2012) provides a comparison of
popular methods on several (non-dialog) tasks, in-
cluding sentiment classification in on-line product
reviews.
In dialog state tracking, there are a variety of
properties that could be cast as a ?domain?. In this
paper, we explore two obvious domains: different
dialog systems, and different slots, where slots are
informational sub-units of the dialog state, such as
the origin, bus route, and departure time in a bus
timetables spoken dialog system. We apply sev-
eral methods for MDL across varied dialog sys-
tems, slots, and combinations of both. MDL is
attractive for dialog state tracking because the dis-
tribution across slots and systems is related but
not identical. For example, the ranges of speech
recognition confidence scores for two slots such as
bus route and date may be different, or one system
may use confirmations much more often than an-
other. Despite these differences, there are useful
patterns: regardless of the slot or system, higher
confidence scores and responses of ?yes? to con-
firmations provide more certainty. The hope is that
MDL can provide a principled way of using all
available data to maximize accuracy.
An important problem in dialog state tracking is
that deploying a new tracker into production will
produce a new distribution of dialog data that may
be unlike data observed at training time in ways
that are difficult to predict. As a result, it is impor-
tant to test the generalization of dialog state track-
ing models on data that differs from the training
distribution. In this paper, we evaluate each of the
MDL approaches on multiple held-out datasets,
ranging from well-matched to very mis-matched
? i.e., dialog data from the same dialog system, a
modified version of the dialog system, and a com-
pletely different dialog system.
We show that dialog data from multiple existing
systems can be used to build good state trackers
for a completely new system, and that a simple
form of MDL improves generalization substan-
tially. We also find that, if well-matched data from
that new system is available, the effect (positive or
negative) of MDL is slight. Since in practice the
level of mis-match can be difficult to predict, this
suggests that training with (a particular form of)
MDL is the safest approach.
This paper is organized as follows. Section 2
describes the algorithm used for state tracking and
the dialog data employed. Section 3 then intro-
duces methods for multi-domain learning. Section
4 presents results and Section 5 briefly concludes.
2 Preliminaries
We begin by describing the core model used for
dialog state tracking, and the source data. Both of
these will be important for the development of the
multi-domain learning methods in Section 3.
2.1 Dialog state tracking model
There are two dominant approaches to statisti-
cal methods for dialog state tracking. Genera-
tive approaches use generative models that capture
how the SLU results are generated from hidden
dialog states (Horvitz and Paek, 1999; Williams
and Young, 2007; Young et al, 2009; Thomson
and Young, 2010). In contrast, discriminative ap-
proaches use conditional models, trained in a dis-
criminative fashion to directly estimate the distri-
bution over a set of state hypotheses based on a
large set of informative features (Bohus and Rud-
nicky, 2006). Previous work has found that dis-
criminative approaches yield better performance
(Metallinou et al, 2013), so we base our experi-
ments on a discriminative model.
We will assume that each dialog state hypothe-
sis is described by a feature vector x, consisting of
|x| = X features. For example, a feature might be
the confidence score of the most recent recognition
result corresponding to the hypothesis. Features
can also be included which describe the current
dialog context, such as how many times the target
slot has been requested or confirmed. At a turn in
a dialog with index i, there are N(i) dialog state
hypotheses, each described by X features. We de-
note the concatenation of all N(i) feature vectors
as X(i), which has size XN(i).
The dialog state tracking task is to take as in-
put the complete feature vector X(i), and output a
distribution over the N(i) hypotheses, plus an ad-
ditional meta-hypothesis REST that indicates that
none of the hypotheses is correct. For training, la-
bels y(i) indicate which of the N(i) hypotheses is
correct, or else if none of them is correct. By con-
434
Feats/hyp
Group |X| |X?| Corpus Dialogs Mismatch to training data
A 90 54
643 TRAIN2 None ? same distribution
715 TEST1 Low
750 TEST2 Medium
B 90 316
1020 TRAIN3 None ? same distribution
438 TEST3 Low
C 90 0 TEST4 High
Table 1: Corpora used in this paper. |X| denotes the number of common features, and |X?| denotes the
number of system-specific features. The data in systems TEST1 and TEST3 has low mis-match to the
training data because they use very similar dialog managers as in TRAIN2 and TRAIN3, respectively.
The system in corpus TEST2 used a different dialog manager from TRAIN2, but the same set of system
actions, speech recognizer, and TTS, resulting in a medium level of mis-match. The system in corpus
TEST4 was completely different from any system in the training data. On average there were approxi-
mately 13 system turns and 13 user turns per dialog across all corpora. The TRAIN* corpora are used
for training, and the TEST* corpora are used for testing. Complete details of the corpora are given in
(Williams et al, 2013).
struction the hypotheses are disjoint; with the ad-
dition of the REST meta-hypothesis, exactly one
hypothesis is correct by construction. After the di-
alog state tracker has output its distribution, this
distribution is passed to a separate, downstream
process that chooses what action to take next (e.g.,
how to respond to the user).
Note that the dialog state tracker is not predict-
ing the contents of the dialog state hypotheses:
the dialog state hypotheses? contents and features
are given by some external process ? for exam-
ple, simply enumerating all SLU values observed
so far in the dialog. Rather, the task is to pre-
dict a probability distribution over the hypotheses,
where the probability assigned to a hypothesis in-
dicates the probability that it is correct.
In our previous work, we developed a
discriminatively-trained maximum-entropy model
for dialog state tracking (Metallinou et al, 2013).
The model estimates a single weight for each
feature in x; to keep learning tractable, these
weights are shared across all state hypotheses be-
ing scored. The model includes L1 and L2 regu-
larization. This model was found to out-perform
generative models, rule-based approaches typi-
cally used in industry, and competing discrimina-
tive approaches. The complete details are given
in (Metallinou et al, 2013) and are not crucial to
this paper, because the multi-domain learning ap-
proaches used here will not modify the learning
algorithm, but rather modify the features, as de-
scribed below.
2.2 Dialog data
We use dialog data and evaluation methods from
the Dialog State Tracking Challenge (Williams
et al, 2013; Williams et al, 2012). This data
comes from public deployments of dialog systems
which provide bus schedule information for Pitts-
burgh, USA. Three different research groups ? de-
noted Groups A, B, and C ? provided dialog sys-
tems. Each group used completely different sys-
tems, composed of different speech recognizers,
acoustic and language models, language under-
standing, dialog design, and text-to-speech. The
differences between systems from different groups
was substantial: for example, Group A and C
systems allowed users to provide any information
at any time, whereas Group B systems followed
a highly directed flow, separately collecting each
slot. In addition, Groups A and B fielded several
versions of their systems over a multi-year period
? these versions differed in various ways, such as
acoustic models, confidence scoring model, state
tracking method and parameters, number of sup-
ported bus routes, presence of minor bugs, and
user population. Differences across versions and
groups yielded differences in overall performance
and distributions in the data (Black et al, 2011;
Williams, 2012). Following the dialog state track-
ing challenge, we use these differences to test the
ability of dialog state tracking methods to gener-
alize to new, unseen distributions of dialog data.
Table 1 lists the groups, datasets, and the relative
435
match/mis-match between training and test data.
In this data, there are 9 slots: the bus route, date,
time, and three components each for the origin
and destination, roughly corresponding to streets,
neighborhoods, and points-of-interest like univer-
sities. In this paper we will build trackers that op-
erate on slots independently ? i.e., at each turn, a
total of 9 trackers will each output a ranked list of
dialog state hypotheses for its slot.1 The state hy-
potheses consist of all of the values for that slot ob-
served so far in the dialog ? either in an SLU result
or output by the system ? plus the meta-hypothesis
REST that represents the case that none of the ob-
served values is correct.
Each dialog state hypothesis is described by a
set of features extracted from the dialog data. The
Dialog State Tracking Challenge provides data
from all systems in a standard format, from which
we extracted 90 features per dialog state hypoth-
esis. We refer to these as common features, be-
cause they are available for all systems. We de-
note the concatenation of all common features for
all hypotheses at a given turn as XA, XB , or XC ,
subscripted based on the system from which they
were extracted. In addition, the challenge data
includes system-specific information. From the
Group A and B logs we extracted 54 and 316
system-specific features per hypothesis, respec-
tively. We denote the concatenation of all system-
specific features for all hypotheses at a given turn
as X?A or X?B , subscripted based on the system
from which they were extracted. Group C logs
provided no additional system-specific informa-
tion. Examples of features are provided in the Ap-
pendix.
3 Multi-domain learning methods
3.1 Models for multi-domain learning
In multi-domain learning (MDL), data instances
are of the form (X(i), y(i), d(i)), where X(i) are
features for instance i, y(i) is the label for instance
i, and d(i) is the domain of instance i, where there
are a total of D domains. The goal is to build a
good model for Pd(y|X) ? i.e., to predict the la-
bel of an instance given its features and domain.
A baseline model uses only data from domain d to
train Pd(y|X); MDL tackles the problem of how
to build models that use data from all domains to
improve on this baseline. In this paper, we con-
1For simplicity, in this paper we do not consider joint state
hypotheses, which include more than one slot.
sider the fully-supervised case, where all of the
training data has been labeled.
We explore four ways of constructing models.
First, in the IND baseline model, we build D sep-
arate models using only data from a single do-
main. Next, in the POOL model, the data from all
domains is simply pooled together into one large
corpus; the single model trained on this corpus is
used in all domains. Each feature vector is aug-
mented to include an indicator of the domain d(i)
from which it originated, as this has been found to
confer much of the benefit of more complex MDL
algorithms (Joshi et al, 2012). The POOL model
can be viewed as the simplest form of MDL.
Next, the MDL1 model employs a simple
but powerful method for MDL developed by
(Daume III, 2007). For each data instance, a syn-
thetic feature vector is formed with D + 1 blocks
of size |X|. Each block is set to all zeros, except
for block d(i) and block D + 1 which are both set
to X(i). For example, with D = 3 domains, the
synthetic feature vector for X(i) from domain 1
would be ?X(i),0,0,X(i)?, and for X(j) from do-
main 2 would be ?0,X(j),0,X(j)?, where 0 is a
vector of zeros of size |X|. This synthetic corpus
is then used to train a single model which is used
in any domain.
This approach has been found to be successful
on a variety of machine learning tasks, including
several NLP tasks (Daume III, 2007). To explain
the intuition, consider a single feature component
of X, X[k], which appears D + 1 times in the
synthetic feature vectors. For model estimation,
assume a standard loss function with a term that
penalizes classification errors, and a regularization
term that penalizes non-zero feature weights. Intu-
itively, if an individual scalar feature X[k] behaves
differently in the domains, the classifier will prefer
the per-domain copies, and assign a zero weight to
the final copy, reducing the error term of the loss
function, at the expense of a small increase in the
regularization term. On the other hand, if an indi-
vidual scalar feature X[k] behaves similarly across
domains, the model will prefer to assign a single
non-zero weight to the final copy and zeros to the
per-domain copies, as this will reduce the regular-
ization term in the loss function. In other words,
the classifier will prefer the shared copy when do-
ing so has little impact to accuracy ? i.e., the clas-
sifier chooses on a feature-by-feature basis when
to keep domains separate, and when to pool do-
436
Synthetic feature vector encoding for data from:
Method Target Slot Slot 1 Slot 2 ? ? ? Slot 9
SLOTIND
1 X1 not used ? ? ? not used
2 not used X2 ? ? ? not used
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
9 not used not used ? ? ? X9
SLOTPOOL all X1 X2 ? ? ? X3
SLOTMDL1 all X1,0, . . . ,0,X1 0,X2, . . . ,0,X2 ? ? ? 0,0, . . . ,X9,X9
SLOTMDL2
1 X1,0,X1 0,X2,X2 ? ? ? 0,X9,X9
2 0,X1,X1 X2,0,X2 ? ? ? 0,X9,X9
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
9 0,X1,X1 0,X2,X2 ? ? ? X9,0,X9
Table 2: Synthetic features constructed for each multi-domain learning method applied to slots. Here,
the subscript on X indicates the slot it describes.
mains.
When the number of domains D is large,
MDL1 can produce large, sparse synthetic feature
vectors, confounding training. MDL2 addresses
this by constructing D separate models; in model
d, data from all domains except d is pooled into
one meta-domain. Then the procedure in MDL1
is followed. For example, for model d = 1, in-
stances X(i) from domain d(i) = 1 is represented
as ?X(i),0,X(i)?; data from all other domains
d(i) 6= 1 is represented as ?0,X(i),X(i)?. This
synthetic data is then used to train a model for do-
main 1.
3.2 Application to dialog state tracking
In this study, we consider two orthogonal dimen-
sions of domain ? systems and slots ? and combi-
nations of the two.
Multi-domain learning across slots means
building a tracker for one slot using dialog data
pertaining to that slot, plus data pertaining to other
slots. In the experiments below, this is done by
treating each of the 9 slots as a domain and apply-
ing each of the four MDL methods above. Table 2
specifies the precise form of the synthetic feature
vectors for each method.
Multi-domain learning across systems means
building a tracker for one dialog system using dia-
log data collected with that system, plus data from
other dialog systems. Each of the two corpora in
the training data ? TRAIN2 from Group A and
TRAIN3 from Group B ? is treated as a domain.
Since only the common features are shared across
domains (i.e., systems), model complexity can be
reduced by building different models depending
on the target group ? the group the model will
be tested on ? and including system-specific fea-
tures only for the target group. For example, when
a model will be trained on data from Groups A
and B, then tested on data from Group A, we in-
clude common features from A and B but system-
specific features from only A. Table 3 specifies the
precise form of the synthetic feature vectors for
each method. Also, when MDL is applied across
systems, there are only 2 sources of training data,
so MDL2 is identical to MDL1 (and thus isn?t
shown in the results).
Applying multi-domain learning to both sys-
tems and slots is done by composing the two fea-
ture synthesis steps. This process is simple but can
increase the size of synthetic feature vectors by up
to an order of magnitude.
3.3 Evaluation method
In the experiments below, we train dialog state
trackers that output a scored list of dialog state
hypotheses for each slot at each turn in the dia-
log. For evaluation, we measure the fraction of
output lists where the top dialog state hypothesis
is correct. A dialog state hypothesis is correct if
it corresponds to a slot value which has been rec-
ognized correctly. The dialog state tracker may
include the meta-hypothesis REST among its hy-
potheses ? this meta-hypothesis is labeled as cor-
rect if no correct values have yet been recognized
for this slot.
Since most turns contain no information about
most slots, we limit evaluation to turns where new
information for a slot appears either in the speech
recognition output, or in the system output. For
437
Synthetic feature vector
encoding for data from:
Method Target group Group A Group B
SYSTEMIND A XA,X
?
A not used
B not used XB,X?B
SYSTEMIND-A C XA not used
SYSTEMIND-B C not used XB
SYSTEMPOOL
A XA,X?A XB,0
B XA,0 XB,X?B
C XA XB
SYSTEMMDL A XA,X
?
A,0,XA 0,0,XB,XB
B 0,0,XA,XA XB,X?B,0,XB
Table 3: Synthetic features constructed for each multi-domain learning method applied to systems. Here,
the subscript on X indicates the system it originated from. Asterisk super-scripts indicate system-specific
features, which are only included for the group the tracker will be tested on (i.e., the target group).
example, in turn i, if a system confirms a bus route,
and a date appears in the speech recognition out-
put, both of these slots in turn i will be included
when computing average accuracy. If the time slot
appears in neither the system output nor anywhere
in the speech recognition output of turn i, then the
time slot in turn i is excluded when computing av-
erage accuracy. The accuracy computation itself
was done by the scoring tool from the Dialog State
Tracking Challenge, using the schedule2 accuracy
metric for all slots (Williams et al, 2013; Williams
et al, 2012).
For comparison, we also report performance of
a simple rule-based tracker. For each slot, this
tracker scans over all values recognized so far in
the dialog, and returns the value which has been
recognized with the highest local SLU confidence
score.
4 Results
We first evaluated performance of multi-domain
learning in isolation, excluding the effects of gen-
eralization. To do this, we divided TRAIN2 and
TRAIN3 in half, using the first halves for train-
ing and the second halves for testing. This ex-
periment gives an indication of the performance of
multi-domain learning if conditions in deployment
match the training data.
Results are shown in Figure 1a-1b. Here, the
effects of multi-domain learning across systems
and slots is rather small, and inconsistent. For ex-
ample, pooling slot data yields best performance
on TRAIN3, and worst performance in TRAIN2.
Applying MDL across systems yields best perfor-
mance for TRAIN3, but not for TRAIN2. Overall,
when training and test data are very well-matched,
MDL has little effect.
Of course, in practice, training and test data will
not be well-matched, so we next evaluated per-
formance of multi-domain learning including the
effects of generalization. Here we trained using
the complete TRAIN2 and TRAIN3 corpora, and
tested on TEST1, TEST2, TEST3, and TEST4.
Results are shown in Figures 1c-1f. The dom-
inant trend is that, at high levels of mis-match as
in TEST3 and TEST4, simply pooling together all
available data yields a large increase in accuracy
compared to all other methods. The majority of
the increase is due to pooling across slots, though
pooling across systems yields a small additional
gain. This result echos past work, where pooling
data is often competitive with more sophisticated
methods for multi-domain learning (Joshi et al,
2012).
In our case, one possible reason for this result
is that simply pooling the data introduces a sort of
regularization: note that the models with SLOT-
POOL and SYSTEMPOOL have the highest ratio
of training data to model parameters. The MDL
methods also use all the data, but via their larger
synthetic feature vectors, they increase the number
of model parameters. The smaller model capacity
of the POOL models limit the ability to completely
fit the training data. This limitation can be a li-
ability for matched conditions ? see for example
Figure 1a ? but may help the model to generalize
438
72%
74%
76%
78%
80%
82%
84%
SystemInd SystemPool SystemMDL1
SlotInd SlotPool RuleTracker
SlotMDL1 SlotMDL2
(a) Evaluation on TRAIN2 (Group A), in which there is min-
imal mis-match between the training and test data.
66%
68%
70%
72%
74%
76%
78%
SystemInd SystemPool SystemMDL1
(b) Evaluation on TRAIN3 (Group B), in which there is min-
imal mis-match between the training and test data.
70%
72%
74%
76%
78%
80%
82%
SystemInd SystemPool SystemMDL1
(c) Evaluation on TEST1 (Group A), in which there is low
mis-match between the training and test data.
52%
54%
56%
58%
60%
62%
64%
SystemInd SystemPool SystemMDL1
(d) Evaluation on TEST3 (Group B), in which there is low
mis-match between the training and test data.
59%
61%
63%
65%
67%
69%
71%
SystemInd SystemPool SystemMDL1
(e) Evaluation on TEST2 (Group A), in which there is
medium mis-match between the training and test data.
58%
60%
62%
64%
66%
68%
70%
SystemInd-A SystemInd-B SystemPool
(f) Evaluation on TEST4 (Group C), in which there is high
mis-match between all of the training data and test data.
Figure 1: Average accuracy of different approaches to multi-domain learning in dialog state tracking.
Squares show SLOTIND, circles SLOTPOOL, unshaded diamonds SLOTMDL1, and shaded diamonds
SLOTMDL2. The solid line shows performance of a simple rule-based tracker, which is not trained on
data. In all plots, the vertical axis is shown on the same scale for comparability (12% from bottom to top),
and indicates average accuracy of the top dialog state (c.f., Section 3.3). In panels 1a and 1b, training is
done on the first halves of TRAIN2 and TRAIN3, and testing on the second halves. In the other panels,
training uses all of TRAIN2 and TRAIN3. In panel 1f, the categories for TEST4 ? for which there is no
in-domain data ? are different than the other panels.
439
in mis-matched conditions.
5 Conclusion
This paper has examined multi-domain learning
and generalization in dialog state tracking. Two
dimensions of domain have been studied ? learn-
ing across slots and learning across systems ? and
three simple methods for multi-domain learning
have been studied. By using corpora of real di-
alogs from the Dialog State Tracking Challenge,
generalization has been studied through varying
levels of mis-match between training and test data.
The results show that simply pooling together
data yields large benefits in highly mis-matched
conditions and has little effect in well-matched
conditions. In practice of course, the level of mis-
match a new tracker will produce is difficult to pre-
dict. So the safest strategy seems to be to always
pool together all available data.
There are a variety of issues to examine in future
work. First, the MDL methods used in this study
were chosen for their simplicity and versatility: by
augmenting features, no changes were required to
the learning method. There exist other methods of
MDL which do modify the learning, and in some
cases yield better performance. It would be inter-
esting to test them next, perhaps including meth-
ods that can construct deeper representations than
the maximum entropy model used here.
More broadly, this study has been limited to su-
pervised multi-domain learning, in which labeled
data from multiple domains is available at training
time. It would clearly be desirable to develop a
method for unsupervised adaptation, in which the
model is adjusted as the unlabeled test data is ex-
perienced.
For now, the contribution of this study is to pro-
vide at least an initial recommendation to prac-
titioners on how to best make use of disparate
sources of dialog data when building a statistical
dialog state tracker.
Acknowledgements
Thanks to Dan Bohus for making his machine
learning software available.
References
Alan W Black, Susanne Burger, Alistair Conkie, He-
len Hastie, Simon Keizer, Oliver Lemon, Nicolas
Merigaud, Gabriel Parent, Gabriel Schubiner, Blaise
Thomson, Jason D. Williams, Kai Yu, Steve Young,
and Maxine Eskenazi. 2011. Spoken dialog chal-
lenge 2010: Comparison of live and control test re-
sults. In Proc SIGdial Workshop on Discourse and
Dialogue, Portland, Oregon.
Dan Bohus and Alex Rudnicky. 2006. A ?K hypothe-
ses + other? belief updating model. In Proc Amer-
ican Association for Artificial Intelligence (AAAI)
Workshop on Statistical and Empirical Approaches
for Spoken Dialogue Systems, Boston.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Eric Horvitz and Tim Paek. 1999. A computational
architecture for conversation. In Proc 7th Interna-
tional Conference on User Modeling (UM), Banff,
Canada, pages 201?210.
Mahesh Joshi, Mark Dredze, William W Cohen, and
Carolyn Rose. 2012. Multi-domain learning: When
do domains matter? In Proc Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
Jeju, Korea.
Angeliki Metallinou, Dan Bohus, and Jason D.
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proc Association for
Computational Linguistics, Sofia.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Jason D Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan W Black. 2012. Dialog state
tracking challenge handbook. Technical report, Mi-
crosoft Research.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Submitted to SigDial 2013.
Jason D. Williams. 2012. Challenges and oppor-
tunities for state tracking in statistical spoken dia-
log systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Process-
ing, Special Issue on Advances in Spoken Dialogue
Systems and Mobile Interface, 6(8):959?970.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The hidden information state model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
440
Appendix
Example common features extracted for all systems
Number of times slot value has been observed in any previous speech recognition result
Whether the most recent speech recognition result includes this slot value
The highest rank on the speech recognition N-best list that this slot value has been observed
The number of times this slot has been requested by the system
Whether the system requested this slot in the current turn
The number of items on the current speech recognition N-best list
Whether confirmation for this slot has been attempted
If confirmation for this slot has been attempted, whether the user was recognized as saying ?yes?
The fraction of recognitions of this slot value in the training set which were correct
The fraction of dialogs in the training set in which the user requested this slot value
Example system-specific features extracted for Group A systems
Acoustic model score
Average word confidence score
Whether barge-in was triggered
Decoder score
Language model score
Maximum and minimum confidence score of any word
Estimated speaking rate
Estimated speaker gender (male/female)
Example system-specific features extracted for Group B systems
Score of best path through the word confusion network
Lowest score of any word on the best path through the word confusion network
Number of speech frames found
Decoder cost
Garbage model likelihood
Noise model likelihood
Average difference in decoder cost, per frame, between the best path and any path through the lattice
Whether barge-in was triggered
Table 4: Examples of features used for dialog state tracking. Group C logs provided no system-specific
information.
441
Proceedings of the SIGDIAL 2014 Conference, pages 263?272,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
The Second Dialog State Tracking Challenge
Matthew Henderson
1
, Blaise Thomson
1
and Jason Williams
2
1
Department of Engineering, University of Cambridge, U.K.
2
Microsoft Research, Redmond, WA, USA
mh521@eng.cam.ac.uk brmt2@eng.cam.ac.uk jason.williams@microsoft.com
Abstract
A spoken dialog system, while commu-
nicating with a user, must keep track of
what the user wants from the system at
each step. This process, termed dialog
state tracking, is essential for a success-
ful dialog system as it directly informs the
system?s actions. The first Dialog State
Tracking Challenge allowed for evalua-
tion of different dialog state tracking tech-
niques, providing common testbeds and
evaluation suites. This paper presents a
second challenge, which continues this
tradition and introduces some additional
features ? a new domain, changing user
goals and a richer dialog state. The chal-
lenge received 31 entries from 9 research
groups. The results suggest that while
large improvements on a competitive base-
line are possible, trackers are still prone
to degradation in mismatched conditions.
An investigation into ensemble learning
demonstrates the most accurate tracking
can be achieved by combining multiple
trackers.
1 Introduction
Spoken language provides a medium of communi-
cation that is natural to users as well as hands- and
eyes-free. Voice-based computer systems, called
spoken dialog systems, allow users to interact us-
ing speech to achieve a goal. Efficient operation of
a spoken dialog system requires a component that
can track what has happened in a dialog, incor-
porating system outputs, user speech and context
from previous turns. The building and evaluation
of these trackers is an important field of research
since the performance of dialog state tracking is
important for the final performance of a complete
system.
Until recently, it was difficult to compare ap-
proaches to state tracking because of the wide va-
riety of metrics and corpora used for evaluation.
The first dialog state tracking challenge (DSTC1)
attempted to overcome this by defining a challenge
task with standard test conditions, freely available
corpora and open access (Williams et al., 2013).
This paper presents the results of a second chal-
lenge, which continues in this tradition with the
inclusion of additional features relevant to the re-
search community.
Some key differences to the first challenge in-
clude:
? The domain is restaurant search instead of
bus timetable information. This provides par-
ticipants with a different category of interac-
tion where there is a database of matching en-
tities.
? Users? goals are permitted to change. In the
first challenge, the user was assumed to al-
ways want a specific bus journey. In this chal-
lenge the user?s goal can change. For exam-
ple, they may want a ?Chinese? restaurant at
the start of the dialog but change to wanting
?Italian? food by the end.
? The dialog state uses a richer representa-
tion than in DSTC1, including not only the
slot/value attributes of the user goal, but also
their search method, and what information
they wanted the system to read out.
As well as presenting the results of the different
state trackers, this paper attempts to obtain some
insights into research progress by analysing their
performance. This includes analyses of the predic-
tive power of performance on the development set,
the effects of tracking the dialog state using joint
distributions, and the correlation between 1-best
accuracy and overall quality of probability distri-
butions output by trackers. An evaluation of the
effects of ensemble learning is also performed.
The paper begins with an overview of the chal-
263
lenge in section 2. The labelling scheme and met-
rics used for evaluation are discussed in section 3
followed by a summary of the results of the chal-
lenge in section 4. An analysis of ensemble learn-
ing is presented in section 5. Section 6 concludes
the paper.
2 Challenge overview
2.1 Problem statement
This section defines the problem of dialog state
tracking as it is presented in the challenge. The
challenge evaluates state tracking for dialogs
where users search for restaurants by specifying
constraints, and may ask for information such as
the phone number. The dialog state is formu-
lated in a manner which is general to information
browsing tasks such as this.
Included with the data is an ontology
1
, which
gives details of all possible dialog states. The
ontology includes a list of attributes termed re-
questable slots which the user may request, such
as the food type or phone number. It also provides
a list of informable slots which are attributes that
may be provided as constraints. Each informable
slot has a set of possible values. Table 1 gives de-
tails on the ontology used in DSTC2.
The dialog state at each turn consists of three
components:
? The goal constraint for each informable slot.
This is either an assignment of a value from
the ontology which the user has specified as
a constraint, or is a special value ? either
Dontcare which means the user has no pref-
erence, or None which means the user is yet
to specify a valid goal for this slot.
? A set of requested slots, i.e. those slots
whose values have been requested by the
user, and should be informed by the system.
? An assignment of the current dialog search
method. This is one of
? by constraints, if the user is attempting
to issue a constraint,
? by alternatives, if the user is requesting
alternative suitable venues,
? by name, if the user is attempting to ask
about a specific venue by its name,
? finished, if the user wants to end the call
? or none otherwise.
Note that in DSTC1, the set of dialog states
1
Note that this ontology includes only the schema for di-
alog states and not the database entries
was dependent on the hypotheses given by a Spo-
ken Language Understanding component (SLU)
(Williams et al., 2013), whereas here the state is
labelled independently of any SLU (see section 3).
Appendix B gives an example dialog with the state
labelled at each turn.
A tracker must use information up to a given
turn in the dialog, and output a probability distri-
bution over dialog states for the turn. Trackers
output separately the distributions for goal con-
straints, requested slots and the method. They may
either report a joint distribution over the goal con-
straints, or supply marginal distributions and let
the joint goal constraint distribution be calculated
as a product of the marginals.
2.2 Challenge design
DSTC2 studies the problem of dialog state track-
ing as a corpus-based task, similar to DSTC1. The
challenge task is to re-run dialog state tracking
over a test corpus of dialogs.
A corpus-based challenge means all trackers
are evaluated on the same dialogs, allowing di-
rect comparison between trackers. There is also
no need for teams to expend time and money in
building an end-to-end system and getting users,
meaning a low barrier to entry.
When a tracker is deployed, it will inevitably al-
ter the performance of the dialog system it is part
of, relative to any previously collected dialogs. In
order to simulate this, and to penalise overfitting to
known conditions, evaluation dialogs in the chal-
lenge are drawn from dialogs with a dialog man-
ager which is not found in the training data.
2.3 Data
A large corpus of dialogs with various telephone-
based dialog systems was collected using Ama-
zon Mechanical Turk. The dialogs used in the
challenge come from 6 conditions; all combina-
tions of 3 dialog managers and 2 speech recognis-
ers. There are roughly 500 dialogs in each condi-
tion, of average length 7.88 turns from 184 unique
callers.
The 3 dialog managers are:
? DM-HC, a simple tracker maintaining a sin-
gle top dialog state, and a hand-crafted policy
? DM-POMDPHC, a dynamic Bayesian net-
work for tracking a distribution of dialog
states, and a hand-crafted policy
? DM-POMDP, the same tracking method as
DM-POMDPHC, with a policy learnt using
264
Slot Requestable
Informable
area yes
yes. 5 values; north,
south, east, west, centre
food yes
yes, 91 possible values
name yes
yes, 113 possible values
pricerange yes
yes, 3 possible values
addr yes
no
phone yes
no
postcode yes
no
signature yes
no
Table 1: Ontology used in DSTC2 for restaurant informa-
tion. Counts do not include the special Dontcare value.
POMDP reinforcement learning
The 2 speech recognisers are:
? ASR-degraded, speech recogniser with arti-
ficially degraded statistical acoustic models
? ASR-good, full speech recogniser optimised
for the domain
These give two acoustic conditions, the de-
graded model producing dialogs at higher error
rates. The degraded models simulate in-car con-
ditions and are described in Young et al. (2013).
The set of all calls with DM-POMDP, with both
speech recognition configurations, constitutes the
test set. All calls with the other two dialog man-
agers are used for the training and development
set. Specifically, the datasets are arranged as so:
? dstc2 train. Labelled dataset released in Oc-
tober 2013, with 1612 calls from DM-HC and
DM-POMDPHC, and both ASR conditions.
? dstc2 dev. Labelled dataset released at the
same time as dstc2 train, with 506 calls under
the same conditions as dstc2 train. No caller
in this set appears in dstc2 train.
? dstc2 test. Set used for evaluation. Released
unlabelled at the beginning of the evaluation
week. This consists of all 1117 dialogs with
DM-POMDP.
Paid Amazon Mechanical Turkers were as-
signed tasks and asked to call the dialog systems.
Callers were asked to find restaurants that matched
particular constraints on the slots area, pricerange
and food. To elicit more complex dialogs, includ-
ing changing goals (goals in DSTC1 were always
constant), the users were sometimes asked to find
more than one restaurant. In cases where a match-
ing restaurant did not exist they were required to
seek an alternative, for example finding an Indian
instead of an Italian restaurant.
A breakdown of the frequency of goal con-
straint changes is given in table 2, showing around
40% of all dialogs involved a change in goal con-
straint. The distribution of the goal constraints in
50
100
150
200
Figure 1: Histogram of values for the food constraint (ex-
cluding dontcare) in all data. The most frequent values are
Indian, Chinese, Italian and European.
Dataset
train dev test
area 2.9% 1.4% 3.8%
food 37.3% 34.0% 40.9%
name 0.0% 0.0% 0.0%
pricerange 1.7% 1.6% 3.1%
any 40.1% 37.0% 44.5%
Table 2: Percentage of dialogs which included a change in
the goal constraint for each informable (and any slot). Barely
any users asked for restaurants by name.
the data was reasonably uniform across the area
and pricerange slots, but was skewed for food as
shown in figure 1. The skew arises from the distri-
bution of the restaurants in the system?s database;
many food types have very few matching venues.
Recently, researchers have started using word
confusion networks for spoken language under-
standing (Henderson et al., 2012; T?ur et al., 2013).
Unfortunately, word confusion networks were not
logged at the time of collecting the dialog data. In
order to provide word confusion networks, ASR
was run offline in batch mode on each dialog us-
ing similar models as the live system. This gives
a second set of ASR results, labelled batch, which
not only includes ASR N -best lists (as in live re-
sults), but also word confusion networks.
For each dataset and speech recogniser, table 3
gives the Word Error Rate on the top ASR hypoth-
esis, and F-score for the top SLU hypothesis (cal-
culated as in Henderson et al. (2012)). Note the
batch ASR was always less accurate than the live.
Live Batch
Dataset ASR WER F-score WER
train
degraded 30.7% 72.4% 37.7%
good 22.4% 78.7% 25.5%
all 26.4% 75.7% 31.3%
dev
degraded 40.4% 67.3% 47.3%
good 25.2% 75.2% 30.0%
all 31.9% 71.6% 37.6%
test
degraded 33.6% 70.0% 41.1%
good 23.5% 77.8% 27.1%
all 28.7% 73.8% 34.3%
Table 3: Word Error Rate on the top hypothesis, and F-score
on top SLU hypothesis.
265
3 Labelling and evaluation
The output of each tracker is a distribution over
dialog states for each turn, as explained in section
2.1. To allow evaluation of the tracker output, the
single correct dialog state at each turn is labelled.
Labelling of the dialog state is facilitated by first
labelling each user utterance with its semantic rep-
resentation, in the dialog act format described in
Henderson et al. (2013) (some example seman-
tic representations are given in appendix B). The
semantic labelling was achieved by first crowd-
sourcing the transcription of the audio to text.
Next a semantic decoder was run over the tran-
scriptions, and the authors corrected the decoder?s
results by hand. Given the sequence of machine
actions and user actions, both represented seman-
tically, the true dialog state is computed determin-
istically using a simple set of rules.
Recall the dialog state is composed of multiple
components; the goal constraint for each slot, the
requested slots, and the method. Each of these
is evaluated separately, by comparing the tracker
output to the correct label. The joint over the goal
constraints is evaluated in the same way, where the
tracker may either explicitly enumerate and score
its joint hypotheses, or let the joint be computed as
the product of the distributions over the slots.
A bank of metrics which look at the tracker out-
put and the correct labels are calculated in the eval-
uation. These metrics are a slightly expanded set
of those calculated in DSTC1.
Denote an example probability distribution
given by a tracker as p and the correct label to be
i, so we have that the probability reported to the
correct hypothesis is p
i
, and
?
j
p
j
= 1.
Accuracy measures the fraction of turns where
the top hypothesis is correct, i.e. where i =
argmax
j
p
j
. AvgP, average probability, mea-
sures the mean score of the correct hypothesis, p
i
.
This gives some idea of the quality of the score
given to the correct hypothesis, ignoring the rest
of the distribution. Neglogp is the mean nega-
tive logarithm of the score given to the correct hy-
pothesis, ? logp
i
. Sometimes called the negative
log likelihood, this is a standard score in machine
learning tasks. MRR is the mean reciprocal rank
of the top hypothesis, i.e.
1
1+k
where j
k
= i and
p
j
0
? p
j
1
? . . .. This metric measures the qual-
ity of the ranking, without necessarily treating the
scores as probabilities. L2 measures the square
of the l
2
norm between the distribution and the
correct label, indicating quality of the whole re-
ported distribution. It is calculated for one turn
as (1 ? p
i
)
2
+
?
j 6=i
p
2
j
. Two metrics, Update
precision and Update accuracy measure the ac-
curacy and precision of updates to the top scoring
hypothesis from one turn to the next. For more
details, see Higashinaka et al. (2004), which finds
these metrics to be highly correlated with dialog
success in their data.
Finally there is a set of measures relating to
the receiver operating characteristic (ROC) curves,
which measure the discrimination of the scores for
the highest-ranked hypotheses. Two versions of
ROC are computed, V1 and V2. V1 computes
correct-accepts (CA), false accepts (FA) and false-
rejects (FR) as fractions of all utterances. The
V2 metrics consider fractions of correctly classi-
fied utterances, meaning the values always reach
100% regardless of the accuracy. V2 metrics mea-
sure discrimination independently of the accuracy,
and are therefore only comparable between track-
ers with similar accuracies.
Several metrics are computed from the ROC
statistics. ROC V1 EER computes the false ac-
ceptance rate at the point where false-accepts are
equal to false-rejects. ROC V1 CA05, ROC V1
CA10, ROCV1 CA20 and ROCV2 CA05, ROC
V2 CA10, ROC V2 CA20, compute the correct
acceptance rates for both versions of ROC at false-
acceptance rates 0.05, 0.10, and 0.20.
Two schedules are used to decide which turns to
include when computing each metric. Schedule 1
includes every turn. Schedule 2 only includes a
turn if any SLU hypothesis up to and including the
turn contains some information about the compo-
nent of the dialog state in question, or if the correct
label is not None. E.g. for a goal constraint, this is
whether the slot has appeared with a value in any
SLU hypothesis, an affirm/negate act has appeared
after a system confirmation of the slot, or the user
has in fact informed the slot regardless of the SLU.
The data is labelled using two schemes. The
first, scheme A, is considered the standard la-
belling of the dialog state. Under this scheme,
each component of the state is defined as the most
recently asserted value given by the user. The
None value is used to indicate that a value is yet
to be given. Appendix B demonstrates labelling
under scheme A.
A second labelling scheme, scheme B, is in-
cluded in the evaluation, where labels are prop-
266
agated backwards through the dialog. This la-
belling scheme is designed to assess whether a
tracker is able to predict a user?s intention be-
fore it has been stated. Under scheme B, the la-
bel at a current turn for a particular component of
the dialog state is considered to be the next value
which the user settles on, and is reset in the case
of goal constraints if the slot value pair is given in
a canthelp act by the system (i.e. the system has
informed that this constraint is not satisfiable).
3.1 Featured metrics
All combinations of metrics, state components,
schedules and labelling schemes give rise to 815
total metrics calculated per tracker in evaluation.
Although each may have its particular motiva-
tion, many of the metrics will be highly corre-
lated. From the results of DSTC1 it was found
the metrics could be roughly split into 3 indepen-
dent groups; one measuring 1-best quality (e.g.
Acc), another measuring probability calibration
(e.g. L2), and the last measuring discrimination
(e.g. ROC metrics) (Williams et al., 2013).
By selecting a representative from each of these
groups, the following were chosen as featured
metrics:
? Accuracy, schedule 2, scheme A
? L2 norm, schedule 2, scheme A
? ROC V2 CA 5, schedule 2, scheme A
Accuracy is a particularly important measure
for dialog management techniques which only
consider the top dialog state hypothesis at each
turn, while L2 is of more importance when mul-
tiple dialog states are considered in action selec-
tion. Note that the ROC metric is only compara-
ble among systems operating at similar accuracies,
and while L2 should be minimised, Accuracy and
the ROC metric should be maximised.
Each of these, calculated for joint goal con-
straints, search method and combined re-
quested slots, gives 9 metrics altogether which
participants were advised to focus on optimizing.
3.2 Baseline trackers
Three baseline trackers were entered in the chal-
lenge, under the ID ?team0?. Source code for
all the baseline systems is available on the DSTC
website
2
. The first, ?team0.entry0?, follows sim-
ple rules commonly used in spoken dialog sys-
tems. It gives a single hypothesis for each slot,
2
http://camdial.org/
?
mh521/dstc/
whose value is the top scoring suggestion so far in
the dialog. Note that this tracker does not account
well for goal constraint changes; the hypothesised
value for a slot will only change if a new value
occurs with a higher confidence.
The focus baseline, ?team0.entry1?, includes a
simple model of changing goal constraints. Be-
liefs are updated for the goal constraint s = v, at
turn t, P (s = v), using the rule:
P (s = v)
t
= q
t
P (s = v)
t?1
+ SLU (s = v)
t
where 0 ? SLU(s = v)
t
? 1 is the evidence
for s = v given by the SLU in turn t, and q
t
=
?
v
?
SLU(s = v
?
)
t
? 1.
Another baseline tracker, based on the tracker
presented in Wang and Lemon (2013) is included
in the evaluation, labelled ?team0.entry2?. This
tracker uses a selection of domain independent
rules to update the beliefs, similar to the focus
baseline. One rule uses a learnt parameter called
the noise adjustment, to adjust the SLU scores.
Full details of this and all baseline trackers are pro-
vided on the DSTC website.
Finally, an oracle tracker is included under the
label ?team0.entry3?. This reports the correct la-
bel with score 1 for each component of the dialog
state, but only if it has been suggested in the dialog
so far by the SLU. This gives an upper-bound for
the performance of a tracker which uses only the
SLU and its suggested hypotheses.
4 Results
Altogether 9 research teams participated in the
challenge. Each team could submit a maximum of
5 trackers, and 31 trackers were submitted in total.
Teams are identified by anonymous team numbers
team1-9, and baseline systems are grouped under
team0. Appendix A gives the results on the fea-
tured metrics for each entry submitted to the chal-
lenge. The full results, including tracker output,
details of each tracker and scripts to run the evalu-
ation are available on the DSTC2 website.
The table in appendix A specifies which of the
inputs available were used for each tracker- from
live ASR, live SLU and batch ASR. This facil-
itates comparisons between systems which used
the same information.
A variety of techniques were used in the sub-
mitted trackers. Some participants provided short
synopses, which are available in the download
from the DSTC2 website. Full details on the track-
ers themselves are published at SIGdial 2014.
267
For the ?requested slot? task, some trackers out-
performed the oracle tracker. This was possible
because trackers could guess a slot was requested
using dialog context, even if there was no mention
of it in the SLU output.
Participants were asked to report the results of
their trackers on the dstcs2 dev development set.
Figure 2 gives some insight into how well perfor-
mance on the development set predicted perfor-
mance on the test set. Metrics are reported as per-
centage improvement relative to the focus base-
line to normalise for the difficulty of the datasets;
in general trackers achieved higher accuracies on
the test set than on development. Figure 2 shows
that the development set provided reasonable pre-
dictions, though in all cases improvement rel-
ative to the baseline was overestimated, some-
times drastically. This suggests that approaches to
tracking have trouble with generalisation, under-
performing in the mismatched conditions of the
test set which used an unseen dialog manager.
Joint Goal Constraint Accuracy
0.3 0.2 0.1 0.1
team1entry0team2entry1team3entry0team4entry0team5entry4team6entry2team7entry0team8entry1team9entry0
Joint Goal Constraint L2team1entry0team2entry1team3entry0team4entry0team5entry4team6entry2team7entry0team8entry1team9entry0
0.2 0.2 0.4 0.6
Figure 2: Performance relative to the focus baseline (per-
centage increase) for dev set (white) and test set (grey). Top
entry for each team chosen based on joint goal constraint ac-
curacy. A lower L2 score is better.
Recall from section 2, trackers could output
joint distributions for goal constraints, or simply
output one distribution for each slot and allow the
joint to be calculated as the product. Two teams,
team2 and team8, opted to output a joint distribu-
tion for some of their entries. Figure 3 compares
performance on the test set for these trackers be-
tween the joint distributions they reported, and the
joint calculated as the product. The entries from
team2 were able to show an increase in the accu-
racy of the top joint goal constraint hypotheses,
but seemingly at a cost in terms of the L2 score.
Conversely the entries from team8, though oper-
ating at lower performance than the focus base-
line, were able to show an improvement in L2 at a
slight loss in accuracy. These results suggest that a
tracking method is yet to be proposed which can,
at least on this data, improve both accuracy and
the L2 score of tracker output by reporting joint
predictions of goal constraints.
Accuracy
team0entry2team2entry0team2entry1team2entry2team2entry3team2entry4team8entry0team8entry1team8entry2team8entry3
0.70 0.72 0.74 0.76 0.78
0.03% 1.34%0.44%-0.11%0.20%-0.30%-0.04%
-0.09%-0.04%-0.09%
L2team0entry2
team2entry0
team2entry1
team2entry3
team2entry4
team8entry0
team8entry1
team8entry2
team8entry3
team8entry4
0.4 0.5 0.6 0.7
team0entry2team2entry0team2entry1team2entry2team2entry3team2entry4team8entry0team8entry1team8entry2team8entry3
0.03% 38.21%52.17%0.22% 23.05% -2.00%-1.73%-1.69%-1.79%-1.75%
Figure 3: Influence of reporting a full joint distribution.
White bar shows test set performance computing the goal
constraints as a product of independent marginals; dark bar is
performance with a full joint distribution. All entries which
reported a full joint are shown. A lower L2 score is better.
It is of interest to investigate the correlation be-
tween accuracy and L2. Figure 4 plots these met-
rics for each tracker on joint goal constraints. We
see that in general a lower L2 score correlates with
a higher accuracy, but there are examples of high
accuracy trackers which do poorly in terms of L2.
This further justifies the reporting of these as two
separate featured metrics.
0.50 0.55 0.60 0.65 0.70 0.75 0.800.3
0.4
0.5
0.6
0.7
0.8 team2entry0
team2entry1
team4entry0team2entry3
focus baseline, team0entry2
Accuracy
L2
Figure 4: Scatterplot of joint goal constraint accuracy and
joint goal constraint L2 for each entry. Plotted line is least-
squares linear regression, L2 = 1.53? 1.43Accuracy
268
Joint goal Method Requested
Tracker Acc. L2 Acc. L2 Acc. L2
Single best entry 0.784 0.346 0.950 0.082 0.978 0.035
Score averaging: top 2 entries 0.787 0.364- 0.945- 0.083 0.976 0.039-
Score averaging: top 5 entries 0.777 0.347 0.945 0.089- 0.976 0.038
Score averaging: top 10 entries 0.760- 0.364- 0.934- 0.108- 0.967- 0.056-
Score averaging: all entries 0.765- 0.362- 0.934- 0.103- 0.971- 0.052-
Stacking: top 2 entries 0.789 0.322+ 0.949 0.085- 0.977 0.040-
Stacking: top 5 entries 0.795+ 0.315+ 0.949 0.084 0.978 0.037
Stacking: top 10 entries 0.796+ 0.312+ 0.949 0.083 0.979 0.035
Stacking: all entries 0.798+ 0.308+ 0.950 0.083 0.980 0.034
Table 4: Accuracy and L2 for Joint goal constraint, Method, and Requested slots for the single best tracker (by accuracy) in
DSTC2, and various ensemble methods. ?Top N entries? means the N entries with highest accuracies from distinct teams, where
the baselines are included as a team. +/- indicates statistically significantly better/worse than the single best entry (p < 0.01),
computed with McNemar?s test for accuracy and the paired t-test for L2, both with Bonferroni correction for repeated tests.
5 Ensemble learning
The dialog state tracking challenge provides an
opportunity to study ensemble learning ? i.e. syn-
thesizing the output of many trackers to improve
performance beyond any single tracker. Here we
consider two forms of ensemble learning: score
averaging and stacking.
In score averaging, the final score of a class is
computed as the mean of the scores output by all
trackers for that class. One of score averaging?s
strengths is that it requires no additional training
data beyond that used to train the constituent track-
ers. If each tracker?s output is correct more than
half the time, and if the errors made by trackers are
not correlated, then score averaging is guaranteed
to improve performance (since the majority vote
will be correct in the limit). In (Lee and Eskenazi,
2013), score averaging (there called ?system com-
bination?) has been applied to combine the output
of four dialog state trackers. To help decorrelate
errors, constituent trackers were trained on differ-
ent subsets of data, and used different machine
learning methods. The relative error rate reduction
was 5.1% on the test set.
The second approach to ensemble learning is
stacking (Wolpert, 1992). In stacking, the scores
output by the constituent classifiers are fed to a
new classifier that makes a final prediction. In
other words, the output of each constituent classi-
fier is viewed as a feature, and the new final classi-
fier can learn the correlations and error patterns of
each. For this reason, stacking often outperforms
score averaging, particularly when errors are cor-
related. However, stacking requires a validation
set for training the final classifier. In DSTC2, we
only have access to trackers? output on the test set.
Therefore, to estimate the performance of stack-
ing, we perform cross-validation on the test set:
the test set is divided into two folds. First, fold 1
is used for training the final classifier, and fold 2
is used for testing. Then the process is reversed.
The two test outputs are then concatenated. Note
that models are never trained and tested on the
same data. A maximum entropy model (maxent) is
used (details in (Metallinou et al., 2013)), which is
common practice for stacking classifiers. In addi-
tion, maxent was found to yield best performance
in DSTC1 (Lee and Eskenazi, 2013).
Table 4 reports accuracy and L2 for goal con-
straints, search method, and requested slots. For
each ensemble method and each quantity (column)
the table gives results for combining the top track-
ers from 2 or 5 distinct teams, for combining the
top tracker from each team, and combining all
trackers (including the baselines as a team). For
example, the joint goal constraint ensemble with
the top 2 entries was built from team2.entry1 &
team4.entry0, and the method ensemble with the
top 2 entries from team2.entry4 & team4.entry0.
Table 4 shows two interesting trends. The first
is that score averaging does not improve perfor-
mance, and performance declines as more track-
ers are combined, yielding a statistically signifi-
cant decrease across all metrics. This suggests that
the errors of the different trackers are correlated,
which is unsurprising since they were trained on
the same data. On the other hand, stacking yields
a statistically significant improvement in accuracy
for goal constraints, and doesn?t degrade accuracy
for the search method and requested slots. For
stacking, the trend is that adding more trackers in-
creases performance ? for example, combining the
best tracker from every team improves goal con-
straint accuracy from 78.4% to 79.8%.
For completeness, we note that the additional
data could alternatively be used to improve the ac-
curacy of a constituent classifier; given the con-
straints of the challenge, we can?t assess the mag-
269
nitude of that improvement, so it is an open ques-
tion whether stacking is the best use of additional
data. Also, the training and test conditions of
the final stacking classifier are not mis-matched,
whereas in practice they would be. Nonethe-
less, this result does suggest that, if additional
data is available, stacking can be used to success-
fully combine multiple trackers and achieve per-
formance better than the single best tracker.
6 Conclusions
DSTC2 continues the tradition of DSTC1 by pro-
viding a common testbed for dialog state track-
ing, introducing some additional features relevant
to the research community? specifically a new
domain, changing user goals and a richer dialog
state. The data, evaluation scripts, and baseline
trackers will remain available and open to the re-
search community online.
Results from the previous challenge motivated
the selection of a few metrics as featured met-
rics, which facilitate comparisons between track-
ers. Analysis of the performance on the matched
development set and the mismatched test set sug-
gests that there still appears to be limitations on
generalisation, as found in DSTC1. The results
also suggest there are limitations in exploiting cor-
relations between slots, with few teams exploiting
joint distributions and the effects of doing so being
mixed. Investigating ensemble learning demon-
strates the effectiveness of combining tracker out-
puts. Ensemble learning exploits the strengths of
individual trackers to provide better quality output
than any constituent tracker in the group.
A follow up challenge, DSTC3, will present
the problem of adapting to a new domain with
very few example dialogs. Future work should
also verify that improvements in dialog state track-
ing translate to improvements in end-to-end dia-
log system performance. In this challenge, paid
subjects were used as users with real information
needs were not available. However, differences
between these two user groups have been shown
(Raux et al., 2005), so future studies should also
test on real users.
Acknowledgements
The authors thank the advisory committee for
their valuable input: Paul Crook, Maxine Eske-
nazi, Milica Ga?si?c, Helen Hastie, Kee-Eung Kim,
Sungjin Lee, Oliver Lemon, Olivier Pietquin,
Joelle Pineau, Deepak Ramachandran, Brian
Strope and Steve Young. The authors also thank
Zhuoran Wang for providing a baseline tracker,
and DJ Kim, Sungjin Lee & David Traum for com-
ments on evaluation metrics. Finally, thanks to
SIGdial for their endorsement, and to the partic-
ipants for making the challenge a success.
References
Matthew Henderson, Milica Ga?si?c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012. IEEE.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2013. Dialog State Tracking Challenge
2 & 3 Handbook. camdial.org/?mh521/dstc/.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2004. Evaluat-
ing discourse understanding in spoken dialogue sys-
tems. ACM Trans. Speech Lang. Process., Novem-
ber.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference.
Angeliki Metallinou, Dan Bohus, and Jason D.
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proc Association for
Computational Linguistics, Sofia.
Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Let?s go public!
Taking a spoken dialog system to the real world.
G?okhan T?ur, Anoop Deoras, and Dilek Hakkani-T?ur.
2013. Semantic parsing using word confusion net-
works with conditional random fields. In INTER-
SPEECH.
Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dia-
log state tracking challenge: On the believability of
observed information. In Proceedings of the SIG-
DIAL 2013 Conference.
Jason Williams, Antoine Raux, Deepak Ramachadran,
and Alan Black. 2013. The Dialog State Track-
ing Challenge. In Proceedings of the SIGDIAL 2013
Conference, Metz, France, August.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241?259.
Steve Young, Catherine Breslin, Milica Ga?si?c,
Matthew Henderson, Dongho Kim, Martin Szum-
mer, Blaise Thomson, Pirros Tsiakoulis, and Eli
Tzirkel Hancock. 2013. Evaluation of Statistical
POMDP-based Dialogue Systems in Noisy Environ-
ment. In Proceedings of IWSDS, Napa, USA, Jan-
uary.
270
Appendix A: Featured results of evaluation
Tracker Inputs Joint Goal Constraints Search Method Requested Slots
team entry
Live
ASR
Live
SLU
Batch
ASR
Acc L2 ROC Acc L2 ROC Acc L2 ROC
0* 0 X 0.619 0.738 0.000 0.879 0.209 0.000 0.884 0.196 0.000
1 X 0.719 0.464 0.000 0.867 0.210 0.349 0.879 0.206 0.000
2 X 0.711 0.466 0.000 0.897 0.158 0.000 0.884 0.201 0.000
3 X
?
0.850 0.300 0.000 0.986 0.028 0.000 0.957 0.086 0.000
1 0 X 0.601 0.649 0.064 0.904 0.155 0.187 0.960 0.073 0.000
1 X 0.596 0.671 0.036 0.877 0.204 0.397 0.957 0.081 0.000
2 0 X X 0.775 0.758 0.063 0.944 0.092 0.306 0.954 0.073 0.383
1 X X X 0.784 0.735 0.065 0.947 0.087 0.355 0.957 0.068 0.446
2 X 0.668 0.505 0.249 0.944 0.095 0.499 0.972 0.043 0.300
3 X X X 0.771 0.354 0.313 0.947 0.093 0.294 0.941 0.090 0.262
4 X X X 0.773 0.467 0.140 0.950 0.082 0.351 0.968 0.050 0.497
3 0 X 0.729 0.452 0.000 0.878 0.210 0.000 0.889 0.188 0.000
4 0 X 0.768 0.346 0.365 0.940 0.095 0.452 0.978 0.035 0.525
1 X 0.746 0.381 0.383 0.939 0.097 0.423 0.977 0.038 0.490
2 X 0.742 0.387 0.345 0.922 0.124 0.447 0.957 0.069 0.340
3 X 0.737 0.406 0.321 0.922 0.125 0.406 0.957 0.073 0.385
5 0 X X 0.686 0.628 0.000 0.889 0.221 0.000 0.868 0.264 0.000
1 X X 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.000
2 X X 0.637 0.726 0.000 0.927 0.147 0.000 0.974 0.053 0.000
3 X X 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.000
4 X X 0.695 0.610 0.000 0.927 0.147 0.000 0.974 0.053 0.000
6 0 X 0.713 0.461 0.100 0.865 0.228 0.199 0.932 0.118 0.057
1 X 0.707 0.447 0.223 0.871 0.211 0.290 0.947 0.093 0.218
2 X 0.718 0.437 0.207 0.871 0.210 0.287 0.951 0.085 0.225
7 0 X 0.750 0.416 0.081 0.936 0.105 0.237 0.970 0.056 0.000
1 X 0.739 0.428 0.159 0.921 0.161 0.554 0.970 0.056 0.000
2 X 0.750 0.416 0.081 0.929 0.117 0.379 0.971 0.054 0.000
3 X 0.725 0.432 0.105 0.936 0.105 0.237 0.972 0.047 0.000
4 X 0.735 0.433 0.086 0.910 0.140 0.280 0.946 0.089 0.190
8 0 X 0.692 0.505 0.071 0.899 0.153 0.000 0.935 0.106 0.000
1 X 0.699 0.498 0.067 0.899 0.153 0.000 0.939 0.101 0.000
2 X 0.698 0.504 0.067 0.899 0.153 0.000 0.939 0.101 0.000
3 X 0.697 0.501 0.068 0.899 0.153 0.000 0.939 0.101 0.000
4 X 0.697 0.508 0.068 0.899 0.153 0.000 0.939 0.101 0.000
9 0 X 0.499 0.760 0.000 0.857 0.229 0.000 0.905 0.149 0.000
* The entries under team0 are the baseline systems mentioned in section 3.2.
?
team0.entry3 is the
oracle tracker, which uses the labels on the test set and limits itself to hypotheses suggested by the live
SLU.
The top score in each column is indicated by bold-type. The ROC metric is only comparable for trackers
operating at a similar accuracy, and so the highest values are not indicated.
271
Appendix B: Sample dialog, labels, and tracker output
S:
U:
Which part of town?
The north uh area
0.2 inform(food=north_african) area=north
method=byconstraints
requested=()
0.1 inform(area=north)
0.2 food=north_african
0.1 area=north
request(area)
inform(area=north)
0.9 byconstraints
0.1 none
0.0 phone
0.0 address
Actual input and output SLU hypotheses and scores Labels Example tracker output Correct?
S:
U:
Which part of town?
A cheap place in 
the north
inform(area=north, 
pricerange=cheap)
0.8 inform(area=north),
inform(pricerange=cheap)
area=north
pricerange=cheap
method=byconstraints
requested=()
0.1 inform(area=north)
0.7 area=north
pricerange=cheap
0.1 area=north
food=north_african
request(area)
0.9 byconstraints
0.1 none
0.0 phone
0.0 address
S:
U:
Clown caf? is a cheap 
restaurant in the 
north part of town.
Do you have any 
others l ike that, 
maybe in the south 
part of town?
reqalts(area=south)
0.7 reqalts(area=south) area=south
pricerange=cheap
method=byalternatives
requested=()
0.2 reqmore()
0.8 area=south
pricerange=cheap
0.1 area=north
pricerange=cheap
0.6 byalternatives
0.2 byconstraints
0.0 phone
0.0 address
S:
U:
Galleria is a cheap 
restaurant in the 
south.
What is their phone 
number and 
address?
request(phone), 
request(address)
0.6 request(phone) area=south
pricerange=cheap
method=byalternatives
requested= (phone, 
address)
0.2 request(phone),
request(address)
0.9 area=south
pricerange=cheap
0.1 area=north
pricerange=cheap
0.5 byconstraints 
0.4 byalternatives
0.8 phone
0.3 address
0.1 request(address)
0.7 ()
0.2 ()
0.1 ()
0.0 ()
Example dialog illustrating DSTC2 data, labels, and evaluation procedure. The left column shows the
actual system output and user input. The second column shows two SLU N-Best hypothesis and their
scores. In practice, up to 10 SLU N-Best hypotheses are output. In the right 3 columns, the three shaded
regions correspond to the three components of the dialog state output by a tracker at each turn. The blue
region corresponds to the user?s joint goal constraint; the red region to the user?s search method; and
the yellow region to the slots requested by the user. For space, only 2 of the 5 methods and 2 of the
8 requestable slots are shown. The third column shows the label (correct output) for each component.
The fourth column shows example tracker output for each of these three quantities, and the fifth column
indicates correctness. A goal constraint is correct if it exactly matches the label. Therefore, 0 or 1 of
the output goal constraints is correct, and all the others are incorrect. Accuracy is determined by the
correctness of the goal constraint with the highest tracker score. For search method, exactly one method
is correct at each turn, so correctness is determined by comparing the maximum scoring method to the
label. For requested slots, each slot can be requested (or not) in the same turn, so each requestable slot
is separately marked as correct or incorrect. The quantity requested.all averages the correctness of all
requested slots.
272
Proceedings of the SIGDIAL 2014 Conference, pages 282?291,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Web-style ranking and SLU combination for dialog state tracking
Jason D. Williams
Microsoft Research, Redmond, WA, USA
jason.williams@microsoft.com
Abstract
In spoken dialog systems, statistical state
tracking aims to improve robustness to
speech recognition errors by tracking a
posterior distribution over hidden dialog
states. This paper introduces two novel
methods for this task. First, we explain
how state tracking is structurally simi-
lar to web-style ranking, enabling ma-
ture, powerful ranking algorithms to be ap-
plied. Second, we show how to use mul-
tiple spoken language understanding en-
gines (SLUs) in state tracking ? multiple
SLUs can expand the set of dialog states
being tracked, and give more information
about each, thereby increasing both recall
and precision of state tracking. We eval-
uate on the second Dialog State Tracking
Challenge; together these two techniques
yield highest accuracy in 2 of 3 tasks, in-
cluding the most difficult and general task.
1 Introduction
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog in
a process called dialog state tracking (Williams
et al., 2013; Henderson et al., 2014). For exam-
ple, in a restaurant search application, the dialog
state might indicate that the user is looking for an
inexpensive restaurant in the center of town. Di-
alog state tracking is difficult because errors in
automatic speech recognition (ASR) and spoken
language understanding (SLU) are common, and
can cause the system to misunderstand the user?s
needs. At the same time, state tracking is crucial
because the system relies on the estimated dia-
log state to choose actions ? for example, which
restaurants to present to the user.
Historically, commercial systems have used
hand-crafted rules for state tracking, selecting the
SLU result with the highest confidence score ob-
served so far, and discarding alternatives. In con-
trast, statistical approaches compute a posterior
distribution over many hypotheses for the dialog
state, and in general these have been shown to be
superior (Horvitz and Paek, 1999; Williams and
Young, 2007; Young et al., 2009; Thomson and
Young, 2010; Bohus and Rudnicky, 2006; Met-
allinou et al., 2013; Williams et al., 2013).
This paper makes two contributions to the task
of statistical dialog state tracking. First, we show
how to cast dialog state tracking as web-style rank-
ing. Each dialog state can be viewed as a doc-
ument, and each dialog turn can be viewed as a
search instance. The benefit of this construction is
that it enables a rich literature of powerful rank-
ing algorithms to be applied. For example, the
ranker we apply constructs a forest of decision
trees, which ? unlike existing work ? automat-
ically encodes conjunctions of low-level features.
Conjunctions are attractive in dialog state tracking
where relationships exist between low-level con-
cepts like grounding and confidence score.
The second contribution is to incorporate the
output of multiple spoken language understanding
engines (SLUs) into dialog state tracking. Using
more than one SLU can increase the number of di-
alog states being tracked, improving the chances
of discovering the correct one. Moreover, addi-
tional SLUs supply more features, such as seman-
tic confidence scores, improving accuracy.
This paper is organized as follows. First, sec-
tion 2 states the problem formally and covers re-
lated work. Section 3 then lays out the data, fea-
tures, and experimental design. Section 4 applies
web-style ranking, and section 5 covers the usage
of multiple SLUs. Section 6 extends the types of
tracking tasks, section 7 compares performance to
other entries in DSTC2, and section 8 briefly con-
282
cludes.
2 Background
Statistical dialog state tracking can be formalized
as follows. At each turn in the dialog, the state
tracker maintains a set X of dialog state hypothe-
ses X = {x
1
, x
2
, . . . , x
N
}. Each state hypothesis
corresponds to a possible true state of the dialog.
The posterior of a state x
i
at a certain turn in the
dialog is denoted P (x
i
).
Based on this posterior, the system takes an ac-
tion a, the user provides an utterance in reply,
and an automatic speech recognizer (ASR) con-
verts the user?s utterance into words. Since speech
recognition is an error prone process, the speech
recognizer outputs weighted alternatives, for ex-
ample an N-best list or a word-confusion network.
A spoken language understanding engine (SLU)
then converts the ASR output into a meaning rep-
resentation U for the user?s utterance, where U
can contain alternatives for the user?s meaning,
U = {u
1
, . . . , u
L
}.
The state tracker then updates its internal state.
This is done in three stages. First, a hand-written
function G ingests the system?s last action s, the
meaning representation U , and the current set of
states X , and yields a new set of possible states,
X
?
= G(s, U,X), where we denote the members
of X
?
as {x
?
1
, x
?
2
, . . . , x
?
N
?
}. The number of ele-
ments in X
?
may be different than X , and typi-
cally the number of states increases as the dialog
progresses, i.e. N
?
> N . In this work, G simply
takes the Cartesian product of X and U . Second,
for each new state hypothesis x
?
i
, a vector of J fea-
tures is extracted, ?(x
?
i
) = [?
1
(x
?
i
), . . . , ?
J
(x
?
i
)].
In the third stage, a scoring process takes all of the
features for all of the new dialog states and scores
them to produce the new distribution over dialog
states, P
?
(x
?
i
). This new distribution is used to
choose another system action, and the whole pro-
cess repeats.
Most early work cast dialog state tracking as a
generative model in which hidden user goals gen-
erate observations in the form of SLU hypothe-
ses (Horvitz and Paek, 1999; Williams and Young,
2007; Young et al., 2009; Thomson and Young,
2010). More recently, discriminatively trained di-
rect models have been applied, and two studies
on dialog data from two publicly deployed dialog
systems suggest direct models yield better perfor-
mance (Williams, 2012; Zilka et al., 2013). The
methods introduced in this paper also use discrim-
inative techniques.
One of the first approaches to direct models for
dialog state tracking was to consider a small, fixed
number of states and then apply a multinomial
classifier (Bohus and Rudnicky, 2006). Since a
multinomial classifier can make effective use of
more features than a generative model, this ap-
proach improves precision, but can decrease recall
by only considering a small number of states (e.g.
5 states). Another discriminative approach is to
score each state using a binary model, then some-
how combine the binary scores to form a distribu-
tion ? see, for example (Henderson et al., 2013b)
which used a binary neural network. This ap-
proach scales to many states, but unlike a multi-
nomial classifier, each binary classifier isn?t aware
of its competitors, reducing accuracy. Also, when
training a binary model in the conventional way,
the training criteria is mis-matched, since the clas-
sifier is trained per hypothesis per timestep, but is
evaluated only once per timestep.
Maximum entropy (maxent) models have been
proposed which provide the strengths of both of
these approaches (Metallinou et al., 2013). The
probability of a dialog hypothesis x
i
being correct
(y = i) is computed as:
P (y = i|X,?) =
exp(
?
j?J
?
j
?
j
(x
i
))
?
x?X
exp(
?
j?J
?
j
?
j
(x))
.
(1)
Maximum entropy models yielded top perfor-
mance in the first dialog state tracking challenge
(Lee and Eskenazi, 2013). In this paper, we use
maxent models as a baseline.
A key limitation with linear (and log-linear)
models such as maximum entropy models is that
they do not automatically build conjunctions of
features. Conjunctions express conditional combi-
nations of features such as whether the system at-
tempted to confirm x and if ?yes? was recognized
and if the confidence score of ?yes? is high. Con-
junctions are important in dialog state tracking be-
cause they are often more discriminative than indi-
vidual features. Moreover, in linear models for di-
alog state tracking, one weight is learned per fea-
ture (equation 1) (Metallinou et al., 2013). As a re-
sult, if a feature takes the same value for every dia-
log hypothesis at a given timestep, its contribution
to every hypothesis will be the same, and it will
therefore have no effect on the ranking. For exam-
ple, features describing the current system action
283
are identical for all state hypotheses. Concretely,
if ?
j
(x
i
) = c for all i, then changing c causes no
change in P (y = i|X,?) for all i.
Past work has shown that conjunctions improve
dialog state tracking (Metallinou et al., 2013; Lee,
2013). However, past work has added conjunction
by hand, and this doesn?t scale: the number of pos-
sible conjunctions increases exponentially in the
number of terms in the conjunction, and it?s diffi-
cult to predict in advance which conjunctions will
be useful. This paper introduces algorithms from
web-style ranking as a mechanism for automati-
cally building feature conjunctions.
In this paper we also use score averaging, a
well-known machine learning technique for com-
bining the output of several models, where each
output class takes the average score assigned by
all the models. Under certain assumptions ? most
importantly that errors are made independently ?
score averaging is guaranteed to exceed the perfor-
mance of the best single model. Score averaging
has been applied to dialog state tracking in previ-
ous work (Lee and Eskenazi, 2013). Here we use
score averaging to maximize data use in cascaded
models, and as a hedge against unlucky parameter
settings.
3 Preliminaries
In this paper we use data and evaluation metrics
from the second dialog state tracking challenge
(DSTC2) (Henderson et al., 2014; Henderson et
al., 2013a). Dialogs in DSTC2 are in the restau-
rant search domain. Users can search for restau-
rants in multiple ways, including via constraints,
or by name. The system can offer restaurants that
match, confirm user input, ask for additional con-
straints, etc.
There are three components to the hidden di-
alog state: user?s goal, search method, and re-
quested slots. The user?s goal specifies the
user?s search constraints, and consists of 4 slots:
area, pricerange, foodtype, and name. The num-
ber of values for the slots ranges from 4 to
113.
1
In DSTC2, trackers output scored lists
for each slot, and also a scored list of joint hy-
potheses. For example, at a given timestep in
a given dialog, three joint goal hypothesis might
be (area=west,food=italian), (area=west), and (),
where () means the user hasn?t specified any con-
straints yet. Since tracking the joint user goal is
1
Including a special ?don?t care? value.
the most general and most difficult task, we?ll fo-
cus on this first, and return to the other tasks in
section 6.
3.1 User goal features
For features, we broadly follow past work (Lee
and Eskenazi, 2013; Lee, 2013; Metallinou et al.,
2013). For a hypothesis x
i
, for each slot the fea-
tures encode 253 low-level quantities, such as:
whether the slot value appears in this hypothesis;
how many times the slot value has been observed;
whether the slot value has been observed in this
turn; functions of recognition metrics such as con-
fidence score and position on N-best list; goal pri-
ors and confusion probabilities estimated on train-
ing data (Williams, 2012; Metallinou et al., 2013);
results of confirmation attempts (?Italian food, is
that right??); output of the four rule-based base-
line trackers; and the system act and its relation to
the goal?s slot value (e.g., whether the system act
mentions this slot value).
Of these 253 features for each slot, 119 are the
same for all values of that slot in a given turn,
such as which system acts were observed in this
turn. For these, we add 238 conjunctions with
slot-specific features like confidence score, which
makes these features useful to our maxent base-
line. This results in a total of 253+238 = 491 fea-
tures per slot. The features for each of the 4 slots
are concatenated together to yield 491 ? 4 = 1964
features per joint hypothesis.
3.2 Evaluation metrics
In DSTC2, there are 3 primary metrics for eval-
uation ? accuracy of the top-scored hypothesis,
the L2 probability quality, and an ROC measure-
ment. The ROC measurement is only meaningful
when compared across systems with similar accu-
racy; since our variants differ in accuracy, we omit
ROC. However, note that all of the metrics, includ-
ing ROC, for our final entries on the development
set and test set are available for public download
from the DSTC2 website.
2
The DSTC2 corpus consists of three partitions:
train, development, and test. Throughout sections
4-6, we report accuracy by training on the train-
ing set, and report accuracy on the development
set and test set. The development set was avail-
able during development of the models, whereas
the test set was not.
2
camdial.org/
?
mh521/dstc/
284
3.3 Baselines
We first compare to the four rule-based trackers
provided by DSTC2. These were carefully de-
signed by other research groups, and earlier ver-
sions of them scored very well in the first DSTC
(Wang and Lemon, 2013). In each column in Ta-
bles 2 and 3, we report the best result from any
rule-based tracker. We also compare to a maxent
model as in Eq 1. Our implementation includes
L1 and L2 regularization which was automatically
tuned via cross-validation.
4 Web-style ranking
The ranking task is to order a set of N docu-
ments by relevance given a query. The input
to a ranker is a query Q and set of documents
X = {D
1
, . . . , D
N
}, where each document is de-
scribed in terms of features of that document and
the query ?(D
i
, Q). The output is a score for
each document, where the highest score indicates
the most relevant document. The overall objective
is to order the documents by relevance, given the
query. Training data indicates the relevance of ex-
ample query/document pairs. Training labels are
provided by judges, and relevance is typically de-
scribed in terms of several levels, such as ?excel-
lent?, ?good?, ?fair?, and ?not relevant?.
The application of ranking to dialog state track-
ing is straightforward: instead of ranking features
of documents and queries ?(D
i
, Q), we rank fea-
tures of dialog states ?(X
i
). For labeling, the cor-
rect dialog state is ?relevant? and all other states
are ?not relevant?.
Like dialog state tracking, ranking tasks often
have features which are constant over all docu-
ments ? particularly features of the query. This is
one reason why ranking algorithms have incorpo-
rated methods for automatically building conjunc-
tions. The specific algorithm we use here is lamb-
daMART (Wu et al., 2010; Burges, 2010). Lamb-
daMART is a mature, scalable ranking algorithm:
it has underpinned the winning entry in a commu-
nity ranking challenge task (Chapelle and Chang,
2011), and is the foundation of the ranker in the
Bing search engine. LambdaMART constructs a
forest of M decision trees, where each tree con-
sists of binary branches on features, and the leaf
nodes are real values. Each binary branch speci-
fies a threshold to apply to a single feature. For a
forest of M trees, the score of a dialog state x is
F (x) =
M
?
m=1
?
m
f
m
(x) (2)
where ?
m
is the weight of treem and f
m
(x) is the
value of the leaf node obtained by evaluating de-
cision tree m by features [?
1
(x), . . . , ?
J
(x)]. The
training objective is to maximize ranking quality,
which here means one-best accuracy. The deci-
sion trees are learned by regularized gradient de-
scent, where trees are added successively to im-
prove ranking quality ? in our case, to maximize
how often the correct dialog state is ranked first.
The number of trees to create and the number of
leaves per tree are tuning parameters. Through
cross-validation, we found that 500 decision trees
each with 32 leaves were the best settings. We use
the same set of 1964 features for lambdaMART as
was used for the maxent baseline.
Results are shown in row 3 of table 2 under
?Joint goal?. Ranking outperforms both baselines
on both the development and training set. This re-
sult illustrates that automatically-constructed con-
junctions do indeed improve accuracy in dialog
state tracking. An example of a single tree com-
puted by lambdaMART is shown in Appendix
A. The complexity of this tree suggests that hu-
man designers would find it difficult to specify a
tractable set of good conjunction features.
5 Multiple SLU engines
As described in the introduction, dialog state
tracking typically proceeds in three stages: enu-
meration of the set of dialog states to score, fea-
ture extraction, and scoring. Incorporating the out-
put of multiple SLUs requires changing the first
two steps. Continuing with notation from sec-
tion 2, with a single SLU output U , the enumer-
ation step is X
?
= G(s, U,X) ? recall that U
is a set of SLU hypotheses from an SLU engine.
With multiple SLU engines we have K SLU out-
puts U
1
, . . . , U
K
, and the enumeration step is thus
X
?
= G(s, U
1
, . . . , U
K
, X). In our implementa-
tion, we simply take the union of all concepts on
all SLU N-best lists and enumerate states as in the
single SLU case ? i.e., the Cartesian product of di-
alog states X with concepts on the SLU output.
The feature extraction step is modified to out-
put features derived from all of the SLU engines.
Concretely, if a feature ?
j
(x) includes informa-
tion from an SLU engine (such as confidence score
285
or position on the N-best list), it is duplicated K
times ? i.e., once for each SLU engine. Additional
binary features are added to encode whether each
SLU engine has output the slot value of this dialog
state. This allows for the situation that a slot value
is not output by all SLU engines, in which case
its confidence score, N-best list position, etc. will
not be present from some SLU engines. Using two
SLU engines on our data increases the number of
features per joint goal from 1964 to 3140.
5.1 SLU Engines
We built two new SLU engines, broadly following
(Henderson et al., 2012). Both consist of many
binary classifiers. In the first engine SLU1, a
binary classifier is estimated for each slot/value
pair, and predicts the presence of that slot/value
pair in the utterance. Similarly, a binary classi-
fier is estimated for each user dialog act. Input
features are word n-grams from the ASR N-best
list. We only considered n-grams which were ob-
served at least c times in the training data; infre-
quent n-grams were mapped to a special UNK
feature. For binary classification we used deci-
sion trees, which marginally outperformed logis-
tic regression, SVMs, and deep neural networks.
Through cross-validation we set n = 2 and c = 2
? i.e., uni-grams and bi-grams which appear at
least twice in the training data.
At runtime, the top SLU output on the N-best
list is formed by taking the most likely combina-
tion of all the binary classifiers; the second SLU
output is formed by taking the second most likely
combination of all the binary classifiers; and so on,
where only valid SLU combinations are consid-
ered. For example, the ?bye? dialog act takes no
arguments, so if ?bye? and ?food=italian? were the
most likely combination, this combination would
be skipped. Scores are formed by taking the prod-
uct of all the binary classifiers, with some smooth-
ing.
The second SLU engine SLU2 is identical ex-
cept that it also includes features from the word
confusion network. Specifically, each word (uni-
gram) appearing in the word confusion network is
a feature. Bi-gram confusion network features did
not improve performance.
If we train a new SLU engine and a ranker on
the same data, this will introduce unwanted bias.
Therefore, we divided the training data in half, and
use the first half for training the SLU, and the sec-
ond for training the ranker. Table 1 shows several
evaluation metrics for each SLU engine, includ-
ing the SLU included in the corpus, which we de-
note SLU0. SLU precision, recall, and F-measure
are computed on the top hypotheses. Item cross-
entropy (ICE) (Thomson et al., 2008) measures the
quality of the scores for all the items on the SLU
N-best list. Table 1 also shows joint goal accu-
racy by using SLU0, SLU1, or SLU2, for either a
rule-based baseline or the ranking model. Over-
all, our SLU engines performed better on isolated
SLU metrics, but did not yield better state tracking
performance when used instead of the SLU results
in the corpus.
5.2 Results with multiple SLU engines
Table 2, rows 4 and 7 show that an improvement
in performance does results from using 2 SLU en-
gines. In rows 4 and 7, the additional SLU en-
gine is trained on the first half of the data, and the
ranker is trained on the second half ? we call this
arrangement Fold A. To maximize use of the data,
it?s possible to train a second SLU/ranker pair by
inverting the training data ? i.e., train a second
SLU on the second half, and a second ranker (us-
ing the second SLU) on the first half. We call this
arrangement Fold B. These two configurations can
be combined by running both trackers on test data,
then averaging their scores. We call this arrange-
ment Fold AB. If a hypothesis is output by only
one configuration, it is assumed the other configu-
ration output a zero score.
Table 2, rows 5 and 8 show that the fold AB con-
figuration yields an additional performance gain.
5.3 Model averaging
A small further improvement is possible by aver-
aging across multiple models (rankers) with dif-
ferent parameter settings. Since all of the mod-
els will be estimated on the same data, this is un-
likely to make a large improvement, but it can
hedge against an unlucky parameter setting, since
the performance after averaging is usually close to
the maximum.
To test this, we trained a second pair of rank-
ing models, with a different number of leaves per
tree (8 instead of 32). We then applied this sec-
ond model, and averaged the scores between the
two variants. Results are in Table 2, rows 6 and
9. Averaging scores across two parameter settings
generally results in performance equal to or better
than the maximum of the two models.
286
Dev set Test set
SLU SLU Metrics Goal track. acc. SLU Metrics Goal track. acc.
source Prec. Recall F-meas. ICE Rules Ranking Prec. Recall F-meas. ICE Rules Ranking
SLU0 0.883 0.666 0.759 2.185 0.623 0.666 0.900 0.691 0.782 1.955 0.719 0.739
SLU1 0.818 0.729 0.771 2.189 0.598 0.637 0.846 0.762 0.802 1.943 0.667 0.709
SLU2 0.844 0.742 0.789 2.098 0.605 0.658 0.870 0.777 0.821 1.845 0.685 0.734
Table 1: Performance of three SLU engines. SLU0 is the DSTC2 corpus; SLU1 is our engine with
uni-grams and bi-grams of ASR results in the corpus; and SLU2 is SLU1 with the addition of unigram
features from the word confusion network. Precision, Recall, F-measure, and ICE evaluate the quality
of the SLU output, not state tracking. ?ICE? is item-wise cross entropy ? smaller numbers are better
(Thomson et al., 2008). ?Rules? indicates dialog state tracking accuracy for user joint goals by running
the rule-based baseline tracker on the indicated SLU (alone); ?Ranking? indicates joint goal accuracy of
running a ranker trained on the indicated SLU (alone). For training, goal tracking results use the ?Fold
A? configuration (c.f. Section 5.2).
5.4 Joint goal tracking summary
The overall process used to train the joint goal
tracker is summarized in Appendix B. For joint
goal tracking, web-style ranking and multiple
SLUs both yield improvements in accuracy on the
development and test sets, with the improvement
associated with multiple SLUs being larger. We
also observe that ranking produces relatively poor
L2 results. This can be attributed to its training
objective, which explicitly maximizes 1-best ac-
curacy without regard to the distribution of the
scores. This is in contrast to maxent models which
explicitly minimize the L2 loss. We examined the
distribution of scores, and qualitatively the ranker
is usually placing less mass on its top guess than
maxent, and spreading more mass out among other
(usually wrong) entries. We return to this in the
future work section.
6 Fixed-size state components
DSTC2 consists of three tracking tasks: in addi-
tion to the user?s goal, the user?s search method
and which slots they requested to hear were also
tracked. These other two tasks were comparatively
simpler because their domains are of a small, fixed
size. Thus classical machine learning methods can
be applied ? i.e., ranking is not directly applicable
to tracking the method and required slots. How-
ever, applying multiple SLU engines is still appli-
cable.
The search method specifies how the user
wants to search. There are 5 values: by-constraints
such as area=west,food=italian, by-name such as
?royal spice?, by-alternatives as in ?do you have
any others like that??, finished when the user is
done as in ?thanks goodbye?, and none when the
method can?t be determined. At each turn, ex-
actly one of the 5 methods is active, so we view
the method component as a standard multinomial
classification task. For features, we use the score
for each method output by each of the 4 rule-
based baselines, and whether each of the methods
is available according to the SLU results observed
so far. We also take conjunctions for each method
with: whether each system dialog act is present in
the current turn, or has ever been used, and what
slots they mentioned; and whether each slot has
appeared in the SLU results from this turn, or any
turn. In total there are 640 features for the method
classifier (when using one SLU engine).
The requested slots are the pieces of informa-
tion the user wants to hear in that turn. The user
can request to hear a restaurant?s area, food-type,
name, price-range, address, phone number, post-
code, and/or signature dish. The user can ask to
hear any combination of slots in a turn ? e.g., ?tell
me their address and phone number?. Therefore
we view each requested slot as a binary classifica-
tion task, and estimate 8 binary classifiers, one for
each requestable slot. Each requested slot takes
as features: whether the slot could logically be re-
quested at this turn in the dialog; whether the SLU
output contained a ?request? act and which slot
was requested; the score output by each of the 4
rule-based baselines; whether each system dialog
act is present in the current turn, or has ever been
used, and what slots they mentioned; and whether
each slot has appeared in the SLU results from this
turn, or any turn. For each requestable slot?s bi-
nary classifier, this results in 187 features (with
one SLU engine).
For each of these tasks, we applied a maxent
287
Joint goal Search method Requested slot
Model Dev. set Test set Dev. set Test set Dev. set Test set
Row SLU Fold Model comb. Acc. L2 Acc. L2 Acc. L2 Acc. L2 Acc. L2 Acc. L2
1 0 ? rules N 0.623 0.601 0.719 0.464 0.860 0.217 0.897 0.158 0.903 0.155 0.884 0.196
2 0 all maxent N 0.649 0.532 0.692 0.480 0.890 0.177 0.909 0.143 0.952 0.078 0.967 0.054
3 0 all * N 0.666 0.739 0.739 0.721 ? ? ? ? ? ? ? ?
4 0+1 A * N 0.686 0.770 0.757 0.766 0.912 0.144 0.936 0.104 0.960 0.062 0.976 0.039
5 0+1 AB * N 0.697 0.749 0.769 0.748 0.913 0.135 0.938 0.097 0.962 0.060 0.978 0.037
6 0+1 AB ** Y 0.699 0.766 0.770 0.766 0.916 0.135 0.943 0.091 0.964 0.059 0.978 0.036
7 0+2 A * N 0.697 0.731 0.765 0.727 0.910 0.146 0.939 0.099 0.966 0.058 0.979 0.037
8 0+2 AB * N 0.711 0.725 0.778 0.721 0.913 0.133 0.943 0.092 0.967 0.058 0.980 0.033
9 0+2 AB ** Y 0.710 0.742 0.781 0.739 0.915 0.132 0.948 0.085 0.967 0.057 0.980 0.033
Table 2: Summary of accuracy and L2 for the three tracking tasks, trained on the ?train? set. In rows
marked (*), joint goal accuracy used ranking, and the other two tasks used maxent. In rows marked (**),
several model classes/parameter settings were used and combined with score averaging.
model; results for this and the best rule-based
baseline are in the rows 1 and 2 of Table 2. We
tried applying decision trees, but this did not im-
prove performance (not shown) as it did for goal
tracking. Note that in the goal tracking task,
one weight is learned for each feature for any
class (goal), whereas in standard multiclass and bi-
nary classification, one weight is learned for each
feature,class pair.
3
Perhaps decision trees were
not effective in increasing accuracy for method
and requested slots because, compared to joint
goal tracking, some conjunctions are implicitly in-
cluded in linear models.
We then added a second SLU engine in the same
manner as for goal tracking. This increased the
number of features for the method task from 640
to 840, and from 187 to 217 for each binary re-
quested slot classifier. Results are shown in Ta-
ble 2; rows 4 and 7 show results with one fold,
and rows 5 and 8 show results with both folds.
Finally, we considered alternate model forms for
each classifier, and then combined them with score
averaging. For the method task, we used a sec-
ond maximum entropy model with different regu-
larization weights, and a multi-class decision tree.
For the requested slot binary classifiers, we added
a neural network classifier. As above, score av-
eraging across different model classes can yield
small gains (rows 6 and 9).
Overall, as with goal tracking, adding a sec-
ond SLU engine resulted in a substantial increase
in accuracy. Unlike goal tracking which used
a ranker, the standard classification models used
here are explicitly optimized for L2 performance
and as a result achieved very good L2 perfor-
mance.
3
Plus a constant term per class.
7 Blind evaluation results
When preparing final entries for the DSTC2 blind
evaluation, we not longer needed a separate devel-
opment set, so our final models are trained on the
combined training and development sets. In the
DSTC2 results, we are team2. Our entry 0 and 1
use the process described above, including score
averaging across multiple models. Entry0 used
SLU0+1, and entry1 used SLU0+2. Entry3 used
a maxent model on SLU0+2, but without model
averaging since its parameters are set with cross-
validation.
4
Results are summarized in Table 3. For accu-
racy for the joint goal and method tasks, our en-
tries had highest accuracy. After the evaluation,
we learned that we were the only team to use fea-
tures from the word confusion network (WCN).
Comparing our entry0, which does not use WCN
features, to the other teams shows that, given the
same input data, our entries were still best for the
joint goal and method tasks.
The blind evaluation results give a final oppor-
tunity to compare the maxent model with the rank-
ing model: entry1 and entry3 both use SLU0+2,
and score an identical set of dialog states using
identical features. Joint goal accuracy is better for
the ranking model. However, as noted above, L2
performance for the ranking model was substan-
tially worse than for the maxent model.
After the blind evaluation, we realized that we
had inadvertently omitted a key feature from the
?requested? binary classifiers ? whether the ?re-
quest? dialog act appeared in the SLU results.
4
The other entries team2.entry2 and team2.entry4 are not
described in this paper. In brief, entry2 was based on a recur-
rent neural network, and entry4 was a combination of entries
1, 2, and 3.
288
Goal Method Requested Requested*
model Acc. L2 Acc. L2 Acc. L2 Acc. L2
Best baseline 0.719 0.464 0.897 0.158 0.884 0.196 0.884 0.196
Best DSTC2 result from another team 0.768 0.346 0.940 0.095 0.978 0.035 0.978 0.035
SLU0+1, AB, model comb. (entry0) 0.775 0.758 0.944 0.092 0.954 0.073 0.977 0.037
SLU0+2, AB, model comb. (entry1) 0.784 0.735 0.947 0.087 0.957 0.068 0.980 0.034
SLU0+2, AB, maxent (entry3) 0.771 0.354 0.947 0.093 0.941 0.090 0.979 0.040
Table 3: Final DSTC2 evaluation results, training on the combined ?train? and ?development? sets. In
the results, we are team2. ?Model comb.? indicates score averaging over several model instances. For the
?requested? task, our entry in DSTC2 inadvertently omitted a key feature, which decreased performance
significantly. ?Requested*? columns indicate results with this feature included. They were computed
after the blind evaluation and are not part of the official DSTC2 results.
Therefore table 3 shows results with and without
this feature. With the inclusion of this feature, the
requested classifiers also achieved best accuracy
and L2 scores, although we note that this is not
part of the official DSTC2 results. (The results in
the preceding sections of this paper included this
feature.)
8 Conclusion
This paper has introduced two new methods for
dialog state tracking. First, we have shown how
to apply web-style ranking for scoring dialog state
hypotheses. Ranking is attractive because it can
construct a forest of decision trees which compute
feature conjunctions, and because it optimizes di-
rectly for 1-best accuracy. Second, we have in-
troduced the usage of multiple SLU engines. Us-
ing additional SLU engines is attractive because it
both adds more possible dialog states to score (in-
creasing recall), and adds features which help to
discriminate the best states (increasing precision).
In experiments, using multiple SLU engines im-
proved performance on all three of the tasks in the
second dialog state tracking challenge. Maximum
entropy models scored best in the previous dia-
log state tracking challenge; here we showed that
web-style ranking improved accuracy over max-
ent when using either a single or multiple SLU
engines. Thus, the two methods introduced here
are additive: they each yield gains separately, and
further gains in combination.
Comparing to other systems in the DSTC2 eval-
uation, these two techniques yielded highest accu-
racy in DSTC2 for 2 of 3 tasks. If we include a
feature accidentally omitted from the third task,
our methods yield highest accuracy for all three
tasks. This experience highlights the importance
of the manual task of extracting a set of informa-
tive features. Also, ranking improved accuracy,
but yielded poor probability quality. For rank-
ing, the L2 performance of ranking was among
the worst in DSTC2. By contrast, for the method
task, where standard classification could be ap-
plied, our entry yielded best L2 performance. The
relative importance of L2 vs. accuracy in dialog
state tracking is an open question.
In future work, we plan to investigate how to
improve the L2 performance of ranking. One ap-
proach is to train a maxent model on the output of
the ranker. On the test set, this yields an improve-
ment in L2 score from 0.735 to 0.587, and simply
clamping ranker?s best guess to 1.0 and all others
to 0.0 improves L2 to 0.431. This is a start, but
not competitive with the best result in DSTC2 of
0.346. Also, techniques which avoid the extrac-
tion of manual features altogether would be ideal,
particularly in light of experiences here.
Even so, for the difficult and general task of
user goal tracking, the techniques here yielded a
relative error rate reduction of 23% over the best
baseline, and exceeded the accuracy of any other
tracker in the second dialog state tracking chal-
lenge.
Acknowledgements
Thanks to Dan Bohus for making his maxent soft-
ware available, to Andrzej Pastusiak for helpful
guidance with lambdaMART, and to Geoff Zweig
for several helpful conversations.
References
Dan Bohus and Alex Rudnicky. 2006. A ?K hypothe-
ses + other? belief updating model. In Proc Amer-
ican Association for Artificial Intelligence (AAAI)
Workshop on Statistical and Empirical Approaches
for Spoken Dialogue Systems, Boston.
289
Christopher J.C. Burges. 2010. From ranknet to lamb-
darank to lambdamart: An overview. Technical Re-
port MSR-TR-2010-82, Microsoft Research.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learning
to rank challenge. JMLR Workshop and Conference
Proceedings, 14:1?24.
Matthew Henderson, Milica Gasic, Blaise Thomson,
Pirros Tsiakoulis, Kai Yu, and Steve Young. 2012.
Discriminative spoken language understanding us-
ing word confusion networks. In Proc IEEE Work-
shop on Spoken Language Technologies (SLT), Mi-
ami, Florida, USA.
Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2013a. Handbook for the dialog state
tracking challenge 2 & 3. Technical report, Cam-
bridge University.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013b. Deep neural network approach for
the dialog state tracking challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 467?471,
Metz, France, August. Association for Computa-
tional Linguistics.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
Eric Horvitz and Tim Paek. 1999. A computational
architecture for conversation. In Proc 7th Interna-
tional Conference on User Modeling (UM), Banff,
Canada, pages 201?210.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414?422, Metz, France, August. Association for
Computational Linguistics.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the
SIGDIAL 2013 Conference, pages 442?451, Metz,
France, August. Association for Computational Lin-
guistics.
Angeliki Metallinou, Dan Bohus, and Jason D.
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proc Association for
Computational Linguistics, Sofia.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
B Thomson, K Yu, M Gasic, S Keizer, F Mairesse,
J Schatzmann, and S Young. 2008. Evaluating
semantic-level confidence scores with multiple hy-
potheses. In Proc Intl Conf on Spoken Language
Processing (ICSLP), Brisbane, Australia.
Zhuoran Wang and Oliver Lemon. 2013. A sim-
ple and generic belief tracking mechanism for the
dialog state tracking challenge: On the believabil-
ity of observed information. In Proceedings of the
SIGDIAL 2013 Conference, pages 423?432, Metz,
France, August. Association for Computational Lin-
guistics.
Jason D Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proc SIGdial Workshop on Dis-
course and Dialogue, Metz, France.
Jason D. Williams. 2012. Challenges and oppor-
tunities for state tracking in statistical spoken dia-
log systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Process-
ing, Special Issue on Advances in Spoken Dialogue
Systems and Mobile Interface, 6(8):959?970.
Qiang Wu, Christopher J. C. Burges, Krysta M. Svore,
and Jianfeng Gao. 2010. Adapting boosting for in-
formation retrieval measures. Journal of Informa-
tion Retrieval, 13(3):254?270.
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The hidden information state model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Lukas Zilka, David Marek, Matej Korvas, and Filip Ju-
rcicek. 2013. Comparison of bayesian discrimina-
tive and generative models for dialogue state track-
ing. In Proceedings of the SIGDIAL 2013 Confer-
ence, pages 452?456, Metz, France, August. Asso-
ciation for Computational Linguistics.
Appendix A: Example decision tree
Figure 1 shows an example decision tree gener-
ated by lambdaMART. Note how the tree is able to
combine features across different slots ? for exam-
ple, following the right-most path tests the scores
of 3 different slots. Also, note how generally more
positive evidence leads to higher scores.
Appendix B: Schematic of approach
Figure 2 shows a schematic diagram of our
overall approach for training the state tracker
(team2.entry0).
290
area.baseline1.score 
> 0.45 ?
food.baseline1.score 
> 0.52 ?
# values recognized for 
area > 1 and food value 
empty?
price.baseline1.score 
> 0.61 ?+0.31-0.08
area.best_conf_score > 
0.01
+0.50Has system grounded price?-0.32 Is price value empty?
+0.48+0.33-0.36 -0.47
YN
YN
YN
YNN Y
NY
N Y
Figure 1: Appendix A: Example decision tree with 8 leaves generated by lambdaMART. Each non-
terminal node contains a binary test; each terminal node contains a real value that linearly contributes to
the score of the dialog state being evaluated. ?baseline1? refers to the output of one of the rule-based
baseline trackers, used in this classifier as an input feature.
Final tracker 
outputAll train
Data, SLU0
All test data,
SLU0+1 / B
All test data, 
SLU0+1 / A
Score
averaging
2nd half train, 
SLU0+1 / A
1st half train, 
SLU0+1 / B
Train, Fold A, 
SLU0
SLU 1
Fold A
SLU 1
Fold B
Train, Fold B, 
SLU0
Tracker ouput
SLU0+1 / A / II
Tracker ouput
SLU0+1 / A / I
Tracker ouput
SLU0+1 / B / II
Tracker ouput
SLU0+1 / B / I
Ranker, 
params II
Ranker, 
params I 
Ranker, 
params II
Ranker, 
params I
All test data, 
SLU 0
Figure 2: Appendix B: Schematic diagram of our overall approach for training the state tracker, using
SLU1 (team2.entry0). Cylinders represent data, rectangles are models, and scripts are tracker output.
Solid arrows are steps done at training time, and dotted arrows are steps done at test time. Approach for
SLU2 (team2.entry1) is identical except that additional features are used in training the SLU models.
291
