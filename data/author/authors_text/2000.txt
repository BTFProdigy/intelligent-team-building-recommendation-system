Topological Dependency Trees:
A Constraint-Based Account of Linear Precedence
Denys Duchier
Programming Systems Lab
Universita?t des Saarlandes, Geb. 45
Postfach 15 11 50
66041 Saarbru?cken, Germany
duchier@ps.uni-sb.de
Ralph Debusmann
Computational Linguistics
Universita?t des Saarlandes, Geb. 17
Postfach 15 11 50
66041 Saarbru?cken, Germany
rade@coli.uni-sb.de
Abstract
We describe a new framework for de-
pendency grammar, with a modular de-
composition of immediate dependency
and linear precedence. Our approach
distinguishes two orthogonal yet mutu-
ally constraining structures: a syntactic
dependency tree and a topological de-
pendency tree. The syntax tree is non-
projective and even non-ordered, while
the topological tree is projective and
partially ordered.
1 Introduction
Linear precedence in so-called free word order
languages remains challenging for modern gram-
mar formalisms. To address this issue, we pro-
pose a new framework for dependency gram-
mar which supports the modular decomposition
of immediate dependency and linear precedence.
Duchier (1999) formulated a constraint-based ax-
iomatization of dependency parsing which char-
acterized well-formed syntax trees but ignored is-
sues of word order. In this article, we develop a
complementary approach dedicated to the treat-
ment of linear precedence.
Our framework distinguishes two orthogonal,
yet mutually constraining structures: a syntactic
dependency tree (ID tree) and a topological de-
pendency tree (LP tree). While edges of the ID
tree are labeled by syntactic roles, those of the
LP tree are labeled by topological fields (Bech,
1955). The shape of the LP tree is a flattening of
the ID tree?s obtained by allowing nodes to ?climb
up? to land in an appropriate field at a host node
where that field is available. Our theory of ID/LP
trees is formulated in terms of (a) lexicalized con-
straints and (b) principles governing e.g. climbing
conditions.
In Section 2 we discuss the difficulties pre-
sented by discontinuous constructions in free
word order languages, and briefly touch on the
limitations of Reape?s (1994) popular theory of
?word order domains?. In Section 3 we introduce
the concept of topological dependency tree. In
Section 4 we outline the formal framework for
our theory of ID/LP trees. Finally, in Section 5
we illustrate our approach with an account of the
word-order phenomena in the verbal complex of
German verb final sentences.
2 Discontinuous Constructions
In free word order languages, discontinuous con-
structions occur frequently. German, for example,
is subject to scrambling and partial extraposition.
In typical phrase structure based analyses, such
phenomena lead to e.g. discontinuous VPs:
(1) (dass)
(that)
einen
a
Mann
manacc
Maria
Marianom
zu
to
lieben
love
versucht
tries
whose natural syntax tree exhibits crossing edges:
S
NP V
VP
NP V
DET N
(dass) einen Mann Maria zu lieben versucht
Since this is classically disallowed, discontinu-
ous constituents must often be handled indirectly
through grammar extensions such as traces.
Reape (1994) proposed the theory of word or-
der domains which became quite popular in the
HPSG community and inspired others such as
Mu?ller (1999) and Kathol (2000). Reape distin-
guished two orthogonal tree structures: (a) the un-
ordered syntax tree, (b) the totally ordered tree of
word order domains. The latter is obtained from
the syntax tree by flattening using the operation
of domain union to produce arbitrary interleav-
ings. The boolean feature [??] of each node con-
trols whether it must be flattened out or not. In-
finitives in canonical position are assigned [?+]:
(dass)
S
NP
Maria
VP[?+]
NP[??]
DET
einen
N
Mann
V
zu lieben
V
versucht
Thus, the above licenses the following tree of
word order domains:
(dass)
S
NP
DET
einen
N
Mann
NP
Maria
V
zu lieben
V
versucht
Extraposed infinitives are assigned [??]:
(dass)
S
NP
Maria
V
versucht
VP[??]
NP
DET
einen
N
Mann
V
zu lieben
As a consequence, Reape?s theory correctly pre-
dicts scrambling (2,3) and full extraposition (4),
but cannot handle the partial extraposition in (5):
(2) (dass) Maria einen Mann zu lieben versucht
(3) (dass) einen Mann Maria zu lieben versucht
(4) (dass) Maria versucht, einen Mann zu lieben
(5) (dass) Maria einen Mann versucht, zu lieben
3 Topological Dependency Trees
Our approach is based on dependency grammar.
We also propose to distinguish two structures: (a)
a tree of syntactic dependencies, (b) a tree of topo-
logical dependencies. The syntax tree (ID tree) is
unordered and non-projective (i.e. it admits cross-
ing edges). For display purposes, we pick an ar-
bitrary linear arrangement:
(dass) Maria einen Mann zu lieben versucht
det
objec
t z
uvinfsubject
The topological tree (LP tree) is partially ordered
and projective:
(dass) Maria einen Mann zu lieben versucht
n
d
n v
vdf
mfmf vc
Its edge labels are called (external) fields and are
totally ordered: df ? mf ? vc. This induces a
linear precedence among the daughters of a node
in the LP tree. This precedence is partial because
daughters with the same label may be freely per-
muted.
In order to obtain a linearization of a LP tree,
it is also necessary to position each node with
respect to its daughters. For this reason, each
node is also assigned an internal field (d, n, or v)
shown above on the vertical pseudo-edges. The
set of internal and external fields is totally or-
dered: d ? df ? n ? mf ? vc ? v
Like Reape, our LP tree is a flattened version of
the ID tree (Reape, 1994; Uszkoreit, 1987), but
the flattening doesn?t happen by ?unioning up?;
rather, we allow each individual daughter to climb
up to find an appropriate landing place. This idea
is reminiscent of GB, but, as we shall see, pro-
ceeds rather differently.
4 Formal Framework
The framework underlying both ID and LP trees
is the configuration of labeled trees under valency
(and other) constraints. Consider a finite set L
of edge labels, a finite set V of nodes, and E ?
V ? V ? L a finite set of directed labeled edges,
such that (V,E) forms a tree. We write w???`w?
for an edge labeled ` from w to w?. We define the
`-daughters `(w) of w ? V as follows:
`(w) = {w? ? V | w???`w? ? E}
We write L? for the set of valency specifications ?`
defined by the following abstract syntax:
?` ::= ` | `? | `? (` ? L)
A valency is a subset of L?. The tree (V,E) satis-
fies the valency assignment valency : V ? 2L? if
for all w ? V and all ` ? L:
` ? valency(w) ? |`(w)| = 1
`? ? valency(w) ? |`(w)| ? 1
`? ? valency(w) ? |`(w)| ? 0
otherwise ? |`(w)| = 0
4.1 ID Trees
An ID tree (V,EID, lex, cat, valencyID) consists
of a tree (V,EID) with EID ? V ? V ?R, where
the set R of edge labels (Figure 1) represents syn-
tactic roles such as subject or vinf (bare infinitive
argument). lex : V ? Lexicon assigns a lexi-
cal entry to each node. An illustrative Lexicon is
displayed in Figure 1 where the 2 features cats
and valencyID of concern to ID trees are grouped
under table heading ?Syntax?. Finally, cat and
valencyID assign a category and an R? valency to
each node w ? V and must satisfy:
cat(w) ? lex(w).cats
valencyID(w) = lex(w).valencyID
(V,EID) must satisfy the valencyID assignment as
described earlier. For example the lexical entry
for versucht specifies (Figure 1):
valencyID(versucht) = {subject, zuvinf}
Furthermore, (V,EID) must also satisfy the
edge constraints stipulated by the grammar
(see Figure 1). For example, for an edge
w?????det w? to be licensed, w? must be assigned
category det and both w and w? must be assigned
the same agreement.1
4.2 LP Trees
An LP tree (V,ELP, lex, valencyLP, fieldext, fieldint)
consists of a tree (V,ELP) with ELP ?
V ? V ? Fext, where the set Fext of edge
labels represents topological fields (Bech, 1955):
df the determiner field, mf the ?Mittelfeld?, vc
1Issues of agreement will not be further considered in this
paper.
the verbal complement field, xf the extraposition
field. Features of lexical entries relevant to LP
trees are grouped under table heading ?Topology?
in Figure 1. valencyLP assigns a F?ext valency
to each node and is subject to the lexicalized
constraint:
valencyLP(w) = lex(w).valencyLP
(V,ELP) must satisfy the valencyLP assignment
as described earlier. For example, the lexical en-
try for zu lieben2 specifies:
valencyLP(zu lieben2) = {mf?, xf?}
which permits 0 or more mf edges and at most
one xf edge; we say that it offers fields mf and xf.
Unlike the ID tree, the LP tree must be projective.
The grammar stipulates a total order on Fext,
thus inducing a partial linear precedence on each
node?s daughters. This order is partial because
all daughters in the same field may be freely per-
muted: our account of scrambling rests on free
permutations within the mf field. In order to ob-
tain a linearization of the LP tree, it is necessary
to specify the position of a node with respect to its
daughters. For this reason each node is assigned
an internal field in Fint. The set Fext ? Fint is to-
tally ordered:
d ? df ? n ? mf ? vc ? v ? xf
In what (external) field a node may land and
what internal field it may be assigned is deter-
mined by assignments fieldext : V ? Fext and
fieldint : V ? Fint which are subject to the lexi-
calized constraints:
fieldext(w) ? lex(w).fieldext
fieldint(w) ? lex(w).fieldint
For example, zu lieben1 may only land in field vc
(canonical position), and zu lieben2 only in xf (ex-
traposed position). The LP tree must satisfy:
w???`w? ? ELP ? ` = fieldext(w?)
Thus, whether an edge w???`w? is licensed de-
pends both on valencyLP(w) and on fieldext(w?).
In other words: w must offer field ` and w? must
accept it.
For an edge w???`w? in the ID tree, we say that
w is the head of w?. For a similar edge in the LP
Grammar Symbols
C = {det , n, vfin, vinf , vpast, zuvinf} (Categories)
R = {det, subject, object, vinf, vpast, zuvinf} (Syntactic Roles)
Fext = {df, mf, vc, xf} (External Topological Fields)
Fint = {d, n, v} (Internal Topological Fields)
d ? df ? n ? mf ? vc ? v ? xf (Topological Ordering)
Edge Constraints
w?????????det w? ? cat(w?) = det ? agr(w) = agr(w?)
w?????????subject w? ? cat(w?) = n ? agr(w) = agr(w?) ? NOM
w?????????object w? ? cat(w?) = n ? agr(w?) ? ACC
w?????????vinf w? ? cat(w?) = vinf
w?????????vpast w? ? cat(w?) = vpast
w?????????zuvinf w? ? cat(w?) = zuvinf
Lexicon
Word Syntax Topology
cats valencyID fieldint fieldext valencyLP
einen {det} {} {d} {df} {}
Mann {n} {det} {n} {mf} {df?}
Maria {n} {} {n} {mf} {}
lieben {vinf} {object?} {v} {vc} {}
geliebt {vpast} {object?} {v} {vc} {}
ko?nnen1 {vinf} {vinf} {v} {vc} {vc?}
ko?nnen2 {vinf , vpast} {vinf} {v} {xf} {mf?, vc?, xf?}
wird {vfin} {subject, vinf} {v} {vc} {mf?, vc?, xf?}
haben {vinf} {vpast} {v} {xf} {mf?, vc?, xf?}
hat {vinf} {subject, vpast} {v} {vc} {mf?, vc?, xf?}
zu lieben1 {zuvinf} {object?} {v} {vc} {}
zu lieben2 {zuvinf} {object?} {v} {xf} {mf?, xf?}
versucht {vfin} {subject, zuvinf} {v} {vc} {mf?, vc?, xf?}
Figure 1: Grammar Fragment
tree, we say that w is the host of w? or that w?
lands on w. The shape of the LP tree is a flat-
tened version of the ID tree which is obtained by
allowing nodes to climb up subject to the follow-
ing principles:
Principle 1 a node must land on a transitive
head2
Principle 2 it may not climb through a barrier
We will not elaborate the notion of barrier which
is beyond the scope of this article, but, for exam-
ple, a noun will prevent a determiner from climb-
ing through it, and finite verbs are typically gen-
eral barriers.
2This is Bro?cker?s terminology and means a node in the
transitive closure of the head relation.
Principle 3 a node must land on, or climb higher
than, its head
Subject to these principles, a node w? may climb
up to any host w which offers a field licensed by
fieldext(w?).
Definition. An ID/ LP analysis is a tuple (V,
EID, ELP, lex, cat, valencyID, valencyLP, fieldext,
fieldint) such that (V,EID, lex, cat, valencyID) is
an ID tree and (V,ELP, lex, valencyLP, fieldext,
fieldint) is an LP tree and all principles are sat-
isfied.
Our approach has points of similarity with
(Bro?ker, 1999) but eschews modal logic in fa-
vor of a simpler and arguably more perspicuous
constraint-based formulation. It is also related
to the lifting rules of (Kahane et al, 1998), but
where they choose to stipulate rules that license
liftings, we opt instead for placing constraints on
otherwise unrestricted climbing.
5 German Verbal Phenomena
We now illustrate our theory by applying it to the
treatment of word order phenomena in the verbal
complex of German verb final sentences. We as-
sume the grammar and lexicon shown in Figure 1.
These are intended purely for didactic purposes
and we extend for them no claim of linguistic ad-
equacy.
5.1 VP Extraposition
Control verbs like versuchen or versprechen al-
low their zu-infinitival complement to be option-
ally extraposed. This phenomenon is also known
as optional coherence.
(6) (dass) Maria einen Mann zu lieben versucht
(7) (dass) Maria versucht, einen Mann zu lieben
Both examples share the following ID tree:
(dass) Maria einen Mann zu lieben versucht
det
objec
t z
uvinfsubject
Optional extraposition is handled by having two
lexical entries for zu lieben. One requires it to
land in canonical position:
fieldext(zu lieben1) = {vc}
the other requires it to be extraposed:
fieldext(zu lieben2) = {xf}
In the canonical case, zu lieben1 does not offer
field mf and einen Mann must climb to the finite
verb:
(dass) Maria einen Mann zu lieben versucht
n
d
n v
vdf
mfmf vc
In the extraposed case, zu lieben2 itself offers
field mf:
(dass) Maria versucht einen Mann zu lieben
n
v
d
n
v
mf
df
mf
xf
5.2 Partial VP Extraposition
In example (8), the zu-infinitive zu lieben is extra-
posed to the right of its governing verb versucht,
but its nominal complement einen Mann remains
in the Mittelfeld:
(8) (dass) Maria einen Mann versucht, zu lieben
In our account, Mann is restricted to land in an mf
field which both extraposed zu lieben2 and finite
verb versucht offer. In example (8) the nominal
complement simply climbed up to the finite verb:
(dass) Maria einen Mann versucht zu lieben
n
d
n
v
v
mf
df
mf xf
5.3 Obligatory Head-final Placement
Verb clusters are typically head-final in German:
non-finite verbs precede their verbal heads.
(9) (dass)
(that)
Maria
Marianom
einen
a
Mann
manacc
lieben
love
wird
will
(10)*(dass) Maria einen Mann wird lieben
The ID tree for (9) is:
(dass) Maria einen Mann lieben wird
subject
det
obje
ct
vinf
The lexical entry for the bare infinitive lieben re-
quires it to land in a vc field:
fieldext(lieben) = {vc}
therefore only the following LP tree is licensed:3
(dass) Maria einen Mann lieben wird
n
d
n v
v
mf
df
mf vc
where mf ? vc ? v, and subject and ob-
ject, both in field mf, remain mutually unordered.
Thus we correctly license (9) and reject (10).
5.4 Optional Auxiliary Flip
In an auxiliary flip construction (Hinrichs and
Nakazawa, 1994), the verbal complement of an
auxiliary verb, such as haben or werden, follows
rather than precedes its head. Only a certain class
of bare infinitive verbs can land in extraposed po-
sition. As we illustrated above, main verbs do
not belong to this class; however, modals such as
ko?nnen do, and may land in either canonical (11)
or in extraposed (12) position. This behavior is
called ?optional auxiliary flip?.
(11) (dass)
(that)
Maria
Maria
einen
a
Mann
man
lieben
love
ko?nnen
can
wird
will
(that) Maria will be able to love a man
(12) (dass) Maria einen Mann wird lieben ko?nnen
Both examples share the following ID tree:
(dass) Maria einen Mann wird lieben ko?nnen
subject
det
object
vinf
vinf
Our grammar fragment describes optional auxil-
iary flip constructions in two steps:
? wird offers both vc and xf fields:
valencyID(wird) = {mf?, vc?, xf?}
? ko?nnen has two lexical entries, one canonical
and one extraposed:
fieldext(ko?nnen1) = {vc}
fieldext(ko?nnen2) = {xf}
3It is important to notice that there is no spurious ambi-
guity concerning the topological placement of Mann: lieben
in canonical position does not offer field mf; therefore Mann
must climb to the finite verb.
Thus we correctly account for examples (11) and
(12) with the following LP trees:
(dass) Maria einen Mann lieben ko?nnen wird
n
d
n
v
v
v
mf
df
mf
vc
vc
(dass) Maria einen Mann wird lieben ko?nnen
n
d
n
v
v
v
mf
df
mf
vc
xf
The astute reader will have noticed that other LP
trees are licensed for the earlier ID tree: they are
considered in the section below.
5.5 V-Projection Raising
This phenomenon related to auxiliary flip de-
scribes the case where non-verbal material is in-
terspersed in the verb cluster:
(13) (dass) Maria wird einen Mann lieben ko?nnen
(14)*(dass) Maria lieben einen Mann ko?nnen wird
(15)*(dass) Maria lieben ko?nnen einen Mann wird
The ID tree remains as before. The NP einen
Mann must land in a mf field. lieben is in canon-
ical position and thus does not offer mf, but
both extraposed ko?nnen2 and finite verb wird do.
Whereas in (12), the NP climbed up to wird, in
(13) it climbs only up to ko?nnen.
(dass) Maria wird einen Mann lieben ko?nnen
n
v
d
n v
v
mf
df
mf vc
xf
(14) is ruled out because ko?nnen must be in the
vc of wird, therefore lieben must be in the vc
of ko?nnen, and einen Mann must be in the mf of
wird. Therefore, einen Mann must precede both
lieben and ko?nnen. Similarly for (15).
5.6 Intermediate Placement
The Zwischenstellung construction describes
cases where the auxiliary has been flipped but its
verbal argument remains in the Mittelfeld. These
are the remaining linearizations predicted by our
theory for the running example started above:
(16) (dass) Maria einen Mann lieben wird ko?nnen
(17) (dass) einen Mann Maria lieben wird ko?nnen
where lieben has climbed up to the finite verb.
5.7 Obligatory Auxiliary Flip
Substitute infinitives (Ersatzinfinitiv) are further
examples of extraposed verbal forms. A sub-
stitute infinitive exhibits bare infinitival inflec-
tion, yet acts as a complement of the perfectizer
haben, which syntactically requires a past partici-
ple. Only modals, AcI-verbs such as sehen and
lassen, and the verb helfen can appear in substi-
tute infinitival inflection.
A substitute infinitive cannot land in canonical
position; it must be extraposed: an auxiliary flip
involving a substitute infinitive is called an ?oblig-
atory auxiliary flip?.
(18) (dass)
(that)
Maria
Maria
einen
a
Mann
man
hat
has
lieben
love
ko?nnen
can
(that) Maria was able to love a man
(19) (dass) Maria hat einen Mann lieben ko?nnen
(20)*(dass) Maria einen Mann lieben ko?nnen hat
These examples share the ID tree:
(dass) Maria einen Mann hat lieben ko?nnen
subject
det
object
xvinf
vinf
hat subcategorizes for a verb in past participle in-
flection because:
valencyID(hat) = {subject, vpast}
and the edge constraint for w??????vpast w? requires:
cat(w?) = vpast
This is satisfied by ko?nnen2 which insists on being
extraposed, thus ruling (20) out:
fieldext(ko?nnen2) = {xf}
Example (18) has LP tree:
(dass) Maria einen Mann hat lieben ko?nnen
n
d
n
v
v
v
mf
df
mf xf
vc
In (18) einen Mann climbs up to hat, while in (19)
it only climbs up to ko?nnen.
5.8 Double Auxiliary Flip
Double auxiliary flip constructions occur when
an auxiliary is an argument of another auxiliary.
Each extraposed verb form offers both vc and mf:
thus there are more opportunities for verbal and
nominal arguments to climb to.
(21) (dass) Maria wird haben einen Mann lieben
ko?nnen
(that) Maria will have been able to love a man
(22) (dass) Maria einen Mann wird haben lieben
ko?nnen
(23) (dass) Maria wird einen Mann lieben haben
ko?nnen
(24) (dass) Maria einen Mann wird lieben haben
ko?nnen
(25) (dass) Maria einen Mann lieben wird haben
ko?nnen
These examples have ID tree:
Maria einen Mann wird haben lieben ko?nnen
subject
det
object
vinf
vinf
vpast
and (22) obtains LP tree:
Maria einen Mann wird haben lieben ko?nnen
n
d
n
v
v
v
v
mf
df
mf xf
vc
xf
5.9 Obligatory Coherence
Certain verbs like scheint require their argument
to appear in canonical (or coherent) position.
(26) (dass)
(that)
Maria
Maria
einen
a
Mann
man
zu
to
lieben
love
scheint
seems
(that) Maria seems to love a man
(27)*(dass) Maria einen Mann scheint, zu lieben
Obligatory coherence may be enforced with the
following constraint principle: if w is an obliga-
tory coherence verb and w? is its verbal argument,
then w? must land in w?s vc field. Like barri-
ers, the expression of this principle in our gram-
matical formalism falls outside the scope of the
present article and remains the subject of active
research.4
6 Conclusions
In this article, we described a treatment of lin-
ear precedence that extends the constraint-based
framework for dependency grammar proposed by
Duchier (1999). We distinguished two orthogo-
nal, yet mutually constraining tree structures: un-
ordered, non-projective ID trees which capture
purely syntactic dependencies, and ordered, pro-
jective LP trees which capture topological depen-
dencies. Our theory is formulated in terms of (a)
lexicalized constraints and (b) principles which
govern ?climbing? conditions.
We illustrated this theory with an application to
the treatment of word order phenomena in the ver-
bal complex of German verb final sentences, and
demonstrated that these traditionally challenging
phenomena emerge naturally from our simple and
elegant account.
Although we provided here an account spe-
cific to German, our framework intentionally per-
mits the definition of arbitrary language-specific
topologies. Whether this proves linguistically ad-
equate in practice needs to be substantiated in fu-
ture research.
Characteristic of our approach is that the for-
mal presentation defines valid analyses as the so-
lutions of a constraint satisfaction problem which
is amenable to efficient processing through con-
straint propagation. A prototype was imple-
mented in Mozart/Oz and supports a parsing
4we also thank an anonymous reviewer for pointing out
that our grammar fragment does not permit intraposition
mode as well as a mode generating all licensed
linearizations for a given input. It was used to
prepare all examples in this article.
While the preliminary results presented here
are encouraging and demonstrate the potential of
our approach to linear precedence, much work re-
mains to be done to extend its coverage and to
arrive at a cohesive and comprehensive grammar
formalism.
References
Gunnar Bech. 1955. Studien u?ber das deutsche Ver-
bum infinitum. 2nd unrevised edition published
1983 by Max Niemeyer Verlag, Tu?bingen (Linguis-
tische Arbeiten 139).
Norbert Bro?ker. 1999. Eine Dependenzgrammatik
zur Kopplung heterogener Wissensquellen. Lin-
guistische Arbeiten 405. Max Niemeyer Verlag,
Tu?bingen/FRG.
Denys Duchier. 1999. Axiomatizing dependency
parsing using set constraints. In Sixth Meeting on
the Mathematics of Language, Orlando/FL, July.
Erhard Hinrichs and Tsuneko Nakazawa. 1994. Lin-
earizing AUXs in German verbal complexes. In
Nerbonne et al (Nerbonne et al, 1994), pages 11?
37.
Sylvain Kahane, Alexis Nasr, and Owen Rambow.
1998. Pseudo-projectivity: a polynomially parsable
non-projective dependency grammar. In Proc.
ACL/COLING?98, pages 646?52, Montre?al.
Andreas Kathol. 2000. Linear Syntax. Oxford Uni-
versity Press.
Igor Melc?uk. 1988. Dependency Syntax: Theory and
Practice. The SUNY Press, Albany, N.Y.
Stefan Mu?ller. 1999. Deutsche Syntax deklara-
tiv. Head-Driven Phrase Structure Grammar fu?r
das Deutsche. Linguistische Arbeiten 394. Max
Niemeyer Verlag, Tu?bingen/FRG.
John Nerbonne, Klaus Netter, and Carl Pollard, edi-
tors. 1994. German in Head-Driven Phrase Struc-
ture Grammar. CSLI, Stanford/CA.
Mike Reape. 1994. Domain union and word order
variation in German. In Nerbonne et al (Nerbonne
et al, 1994), pages 151?197.
Hans Uszkoreit. 1987. Word Order and Constituent
Structure in German. CSLI, Stanford/CA.
A Relational Syntax-Semantics Interface Based on Dependency Grammar
Ralph Debusmann Denys Duchier? Alexander Koller Marco Kuhlmann Gert Smolka Stefan Thater
Saarland University, Saarbr?cken, Germany ?LORIA, Nancy, France
{rade|kuhlmann|smolka}@ps.uni-sb.de, duchier@loria.fr, {koller|stth}@coli.uni-sb.de
Abstract
We propose a syntax-semantics interface that
realises the mapping between syntax and se-
mantics as a relation and does not make func-
tionality assumptions in either direction. This
interface is stated in terms of Extensible De-
pendency Grammar (XDG), a grammar formal-
ism we newly specify. XDG?s constraint-based
parser supports the concurrent flow of informa-
tion between any two levels of linguistic rep-
resentation, even when only partial analyses are
available. This generalises the concept of under-
specification.
1 Introduction
A key assumption of traditional syntax-semantics
interfaces, starting with (Montague, 1974), is that
the mapping from syntax to semantics is functional,
i. e. that once we know the syntactic structure of a
sentence, we can deterministically compute its se-
mantics.
Unfortunately, this assumption is typically not
justified. Ambiguities such as of quantifier scope
or pronominal reference are genuine semantic am-
biguities; that is, even a syntactically unambigu-
ous sentence can have multiple semantic readings.
Conversely, a common situation in natural language
generation is that one semantic representation can
be verbalised in multiple ways. This means that the
relation between syntax and semantics is not func-
tional at all, but rather a true m-to-n relation.
There is a variety of approaches in the litera-
ture on syntax-semantics interfaces for coping with
this situation, but none of them is completely sat-
isfactory. One way is to recast semantic ambiguity
as syntactic ambiguity by compiling semantic dis-
tinctions into the syntax (Montague, 1974; Steed-
man, 1999; Moortgat, 2002). This restores function-
ality, but comes at the price of an artificial blow-
up of syntactic ambiguity. A second approach is to
assume a non-deterministic mapping from syntax
to semantics as in generative grammar (Chomsky,
1965), but it is not always obvious how to reverse
the relation, e. g. for generation. For LFG, the oper-
ation of functional uncertaintainty allows for a re-
stricted form of relationality (Kaplan and Maxwell
III, 1988). Finally, underspecification (Egg et al,
2001; Gupta and Lamping, 1998; Copestake et al,
2004) introduces a new level of representation,
which can be computed functionally from a syntac-
tic analysis and encapsulates semantic ambiguity in
a way that supports the enumeration of all semantic
readings by need.
In this paper, we introduce a completely rela-
tional syntax-semantics interface, building upon the
underspecification approach. We assume a set of
linguistic dimensions, such as (syntactic) immedi-
ate dominance and predicate-argument structure; a
grammatical analysis is a tuple with one component
for each dimension, and a grammar describes a set
of such tuples. While we make no a priori function-
ality assumptions about the relation of the linguistic
dimensions, functional mappings can be obtained
as a special case. We formalise our syntax-seman-
tics interface using Extensible Dependency Gram-
mar (XDG), a new grammar formalism which gen-
eralises earlier work on Topological Dependency
Grammar (Duchier and Debusmann, 2001).
The relational syntax-semantics interface is sup-
ported by a parser for XDG based on constraint pro-
gramming. The crucial feature of this parser is that
it supports the concurrent flow of possibly partial in-
formation between any two dimensions: once addi-
tional information becomes available on one dimen-
sion, it can be propagated to any other dimension.
Grammaticality conditions and preferences (e. g. se-
lectional restrictions) can be specified on their nat-
ural level of representation, and inferences on each
dimension can help reduce ambiguity on the oth-
ers. This generalises the idea of underspecifica-
tion, which aims to represent and reduce ambiguity
through inferences on a single dimension only.
The structure of this paper is as follows: in Sec-
tion 2, we give the general ideas behind XDG, its
formal definition, and an overview of the constraint-
based parser. In Section 3, we present the relational
syntax-semantics interface, and go through exam-
ples that illustrate its operation. Section 4 shows
how the semantics side of our syntax-semantics in-
terface can be precisely related to mainstream se-
mantics research. We summarise our results and
point to further work in Section 5.
2 Extensible Dependency Grammar
This section presents Extensible Dependency
Grammar (XDG), a description-based formalism
for dependency grammar. XDG generalizes previ-
ous work on Topological Dependency Grammar
(Duchier and Debusmann, 2001), which focussed
on word order phenomena in German.
2.1 XDG in a Nutshell
XDG is a description language over finite labelled
graphs. It is able to talk about two kinds of con-
straints on these structures: The lexicon of an XDG
grammar describes properties local to individual
nodes, such as valency. The grammar?s principles
express constraints global to the graph as a whole,
such as treeness. Well-formed analyses are graphs
that satisfy all constraints.
An XDG grammar allows the characterisation
of linguistic structure along several dimensions of
description. Each dimension contains a separate
graph, but all these graphs share the same set of
nodes. Lexicon entries synchronise dimensions by
specifying the properties of a node on all dimen-
sions at once. Principles can either apply to a single
dimension (one-dimensional), or constrain the rela-
tion of several dimensions (multi-dimensional).
Consider the example in Fig. 1, which shows an
analysis for a sentence of English along two dimen-
sions of description, immediate dominance (ID) and
linear precedence (LP). The principles of the under-
lying grammar require both dimensions to be trees,
and the LP tree to be a ?flattened? version of the ID
tree, in the sense that whenever a node v is a tran-
sitive successor of a node u in the LP tree, it must
also be a transitive successor of u in the ID tree. The
given lexicon specifies the potential incoming and
required outgoing edges for each word on both di-
mensions. The word does, for example, accepts no
incoming edges on either dimension and must there-
fore be at the root of both the ID and the LP tree. It is
required to have outgoing edges to a subject (subj)
and a verb base form (vbse) in the ID tree, needs
fillers for a subject (sf) and a verb complement field
(vcf) in the LP tree, and offers an optional field for
topicalised material (tf). All these constraints are
satisfied by the analysis, which is thus well-formed.
2.2 Formalisation
Formally, an XDG grammar is built up of dimen-
sions, principles, and a lexicon, and characterises a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
s
u
b
j
v
b
s
e
o
b
j
what does John eat
s
f
v
c
f
what does John eat
t
f
word inID outID inLP outLP
what {obj?} {} {tf?} {}
does {} {subj,vbse} {} {tf?,sf,vcf}
John {subj?,obj?} {} {sf?,of?} {}
eat {vbse?} {obj} {vcf?} {}
Figure 1: XDG analysis of ?what does John eat?
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments (or
lexical entries). A D-structure, representing an anal-
ysis on dimension D, is a triple (V,E,F) of a set V
of nodes, a set E ?V ?V ?Lab of directed labelled
edges, and an assignment F : V ? (Fea ? Val) of
lexical entries to nodes. V and E form a graph. We
write StrD for the set of all possible D-structures.
The principles characterise subsets of StrD that have
further dimension-specific properties, such as being
a tree, satisfying assigned valencies, etc. We assume
that the elements of Pri are finite representations of
such subsets, but do not go into details here; some
examples are shown in Section 3.2.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Str1??? ??Strn
where all dimensions share the same set of nodes V .
Multi-dimensional principles work just like one-
dimensional principles, except that they specify
subsets of Ana, i. e. couplings between dimensions
(e. g. the flattening principle between ID and LP in
Section 2.1). The lexicon Lex ? Lex1 ? ?? ? ? Lexn
constrains all dimensions at once. An XDG analysis
is licenced by Lex iff (F1(w), . . . ,Fn(w)) ? Lex for
every node w ?V .
In order to compute analyses for a given input, we
model it as a set of input constraints (Inp), which
again specify a subset of Ana. The parsing prob-
lem for XDG is then to find elements of Ana that
are licenced by Lex and consistent with Inp and
Pri. Note that the term ?parsing problem? is tradi-
tionally used only for inputs that are sequences of
words, but we can easily represent surface realisa-
tion as a ?parsing? problem in which Inp specifies a
semantic dimension; in this case, a ?parser? would
compute analyses that contain syntactic dimensions
from which we can read off a surface sentence.
2.3 Constraint Solver
The parsing problem of XDG has a natural read-
ing as a constraint satisfaction problem (CSP) (Apt,
2003) on finite sets of integers; well-formed anal-
yses correspond to the solutions of this problem.
The transformation, whose details we omit due to
lack of space, closely follows previous work on ax-
iomatising dependency parsing (Duchier, 2003) and
includes the use of the selection constraint to effi-
ciently handle lexical ambiguity.
We have implemented a constraint solver for
this CSP using the Mozart/Oz programming system
(Smolka, 1995; Mozart Consortium, 2004). This
solver does a search for a satisfying variable assign-
ment. After each case distinction (distribution), it
performs simple inferences that restrict the ranges
of the finite set variables and thus reduce the size
of the search tree (propagation). The successful
leaves of the search tree correspond to XDG anal-
yses, whereas the inner nodes correspond to partial
analyses. In these cases, the current constraints are
too weak to specify a complete analysis, but they
already express that some edges or feature values
must be present, and that others are excluded. Partial
analyses will play an important role in Section 3.3.
Because propagation operates on all dimensions
concurrently, the constraint solver can frequently
infer information about one dimension from infor-
mation on another, if there is a multi-dimensional
principle linking the two dimensions. These infer-
ences take place while the constraint problem is be-
ing solved, and they can often be drawn before the
solver commits to any single solution.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an NP-
complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential, but the average-case complex-
ity for the hand-crafted grammars we experimented
with is often better than this result suggests. We
hope there are useful fragments of XDG that would
guarantee polynomial worst-case complexity.
3 A Relational Syntax-Semantics Interface
Now that we have the formal and processing frame-
works in place, we can define a relational syntax-
semantics interface for XDG. We will first show
how we encode semantics within the XDG frame-
work. Then we will present an example grammar
(including some principle definitions), and finally
go through an example that shows how the rela-
tionality of the interface, combined with the con-
currency of the constraint solver, supports the flow
of information between different dimensions.
3.1 Representing Meaning
We represent meaning within XDG on two dimen-
sions: one for predicate-argument structure (PA),
every student reads a book
s
u
b
j
d
e
t
o
b
j
d
e
t
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
i. ID-tree ii. PA-structure
s
every student reads a book
s
r
r
s
every student reads a book
s
r
r
iii. scope trees
Figure 2: Two analyses for the sentence ?every stu-
dent reads a book.?
and one for scope (SC). The function of the PA di-
mension is to abstract over syntactic idiosyncrasies
such as active-passive alternations or dative shifts,
and to make certain semantic dependencies e. g. in
control constructions explicit; it deals with concepts
such as agent and patient, rather than subject and ob-
ject. The purpose of the SC dimension is to reflect
the structure of a logical formula that would repre-
sent the semantics, in terms of scope and restriction.
We will make this connection explicit in Section 4.
In addition, we assume an ID dimension as above.
We do not include an LP dimension only for ease of
presentation; it could be added completely orthogo-
nally to the three dimensions we consider here.
While one ID structure will typically correspond
to one PA structure, each PA structure will typically
be consistent with multiple SC structures, because
of scope ambiguities. For instance, Fig. 2 shows the
unique ID and PA structures for the sentence ?Ev-
ery student reads a book.? These structures (and the
input sentence) are consistent with the two possi-
ble SC-structures shown in (iii). Assuming a David-
sonian event semantics, the two SC trees (together
with the PA-structure) represent the two readings of
the sentence:
? ?e.?x.student(x) ??y.book(y)? read(e,x,y)
? ?e.?y.book(y)??x.student(x) ? read(e,x,y)
3.2 A Grammar for a Fragment of English
The lexicon for an XDG grammar for a small frag-
ment of English using the ID, PA, and SC dimensions
is shown in Fig. 3. Each row in the table specifies a
(unique) lexical entry for each part of speech (deter-
miner, common noun, proper noun, transitive verb
and preposition); there is no lexical ambiguity in
this grammar. Each column specifies a feature. The
meaning of the features will be explained together
inID outID inPA outPA inSC outSC
DET {subj?,obj?,pcomp?} {det!} {ag?,pat?,arg?} {quant!} {r?,s?,a?} {r!,s!}
CN {det?} {prep?} {quant?} {mod?} {r?,s?,a?} {}
PN {subj?,obj?,pcomp?} {prep?} {ag?,pat?,arg?} {mod?} {r?,s?,a?} {r?,s!}
TV {} {subj!,obj!,prep?} {} {ag!,pat!, instr?} {r?,s?,a?} {}
PREP {prep?} {pcomp!} {mod?, instr?} {arg!} {r?,s?,a?} {a!}
link codom contradom
DET {quant 7? {det}} {quant 7? {r}} {}
CN,PN {mod 7? {prep}} {} {mod 7? {a}}
TV {ag 7? {subj},pat 7? {obj}, instr 7? {prep}} {} {ag 7? {s},pat 7? {s}, instr 7? {a}}
PREP {arg 7? {pcomp}} {} {arg 7? {s}}
Figure 3: The example grammar fragment
with the principles that use them.
The ID dimension uses the edge labels LabID =
{det,subj,obj,prep,pcomp} resp. for determined
common noun,1 subject, object, preposition, and
complement of a preposition. The PA dimension
uses LabPA = {ag,pat,arg,quant,mod, instr}, resp.
for agent, patient, argument of a modifier, common
noun pertaining to a quantifier, modifier, and instru-
ment; and SC uses LabSC = {r,s,a} resp. for restric-
tion and scope of a quantifier, and for an argument.
The grammar also contains three one-dimen-
sional principles (tree, dag, and valency), and
three multi-dimensional principles (linking, co-
dominance, and contra-dominance).
Tree and dag principles. The tree principle re-
stricts ID and SC structures to be trees, and the
dag principle restricts PA structures to be directed
acyclic graphs.
Valency principle. The valency principle, which
we use on all dimensions, states that the incom-
ing and outgoing edges of each node must obey the
specifications of the in and out features. The possi-
ble values for each feature ind and outd are subsets
of Labd ? {!,?,?}. `! specifies a mandatory edge
with label `, `? an optional one, and `? zero or more.
Linking principle. The linking principle for di-
mensions d1,d2 constrains how dependents on d1
may be realised on d2. It assumes a feature linkd1,d2
whose values are functions that map labels from
Labd1 to sets of labels from Labd2 , and is specified
by the following implication:
v
l
?d1 v
? ? ?l? ? linkd1,d2(v)(l) : v
l?
?d2 v
?
Our grammar uses this principle with the link fea-
ture to constrain the realisations of PA-dependents in
the ID dimension. In Fig. 2, the agent (ag) of reads
must be realised as the subject (subj), i. e.
1We assume on all dimensions that determiners are the
heads of common nouns. This makes for a simpler relationship
between the syntactic and semantic dimensions.
reads ag?PA every ? reads
subj
? ID every
Similarly for the patient and the object. There
is no instrument dependent in the example, so this
part of the link feature is not used. An ergative verb
would use a link feature where the subject realises
the patient; Control and raising phenomena can also
be modelled, but we cannot present this here.
Co-dominance principle. The co-dominance
principle for d1,d2 relates edges in d1 to dominance
relations in the same direction in d2. It assumes a
feature codomd1,d2 mapping labels in Labd1 to sets
of labels in Labd2 and is specified as
v
l
?d1 v
? ? ?l? ? codomd1,d2(v)(l) : v
l?
???d2v
?
Our grammar uses the co-dominance principle on
dimension PA and SC to express, e. g., that the
propositional contribution of a noun must end up in
the restriction of its determiner. For example, for the
determiner every of Fig. 2 we have:
every quant? PA student ? every
r
???SCstudent
Contra-dominance principle. The contra-domi-
nance principle is symmetric to the co-dominance
principle, and relates edges in d1 to dominance
edges into the opposite direction in d2. It assumes
a feature contradomd1,d2 mapping labels of Labd1 to
sets of labels from Labd2 and is specified as
v
l
?d1 v
? ?
?l? ? contradomd1,d2(v)(l) : v?
l?
???d2v
Our grammar uses the contra-dominance principle
on dimensions PA and SC to express, e. g., that pred-
icates must end up in the scope of the quantifiers
whose variables they refer to. Thus, for the transi-
tive verb reads of Fig. 2, we have:
reads ag?PA every ? every
s
???SCreads
reads pat?PA a ? a
s
???SCreads
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
p
r
e
p
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
i
n
s
t
r
a
s
Mary saw a student with a book
a
g
p
a
t
a
r
g
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
m
o
d
p
r
e
p
a
i. Partial analysis
ii. verb attachment
iii. noun attachment
ID
PA
SC
Figure 4: Partial description (left) and two solutions (right) for ?Mary saw a student with a book.?
3.3 Syntax-Semantics Interaction
It is important to note at this point that the syntax-
semantics interface we have defined is indeed re-
lational. Each principle declaratively specifies a set
of admissible analyses, i. e. a relation between the
structures for the different dimensions, and the anal-
yses that the complete grammar judges grammatical
are simply those that satisfy all principles. The role
of the lexicon is to provide the feature values which
parameterise the principles defined above.
The constraint solver complements this relation-
ality by supporting the use of the principles to move
information between any two dimensions. If, say,
the left-hand side of the linking principle is found to
be satisfied for dimension d1, a propagator will infer
the right-hand side and add it to dimension d2. Con-
versely, if the solver finds that the right-hand side
must be false for d2, the negation of the left-hand
side is inferred for d1. By letting principles interact
concurrently, we can make some very powerful in-
ferences, as we will demonstrate with the example
sentence ?Mary saw a student with a book,? some
partial analyses for which are shown in Fig. 4.
Column (i) in the figure shows the state after the
constraint solver finishes its initial propagation, at
the root of the search tree. Even at this point, the va-
lency and treeness principles have conspired to es-
tablish an almost complete ID-structure. By the link-
ing principle, the PA-structure has been determined
similarly closely. The SC-structure is still mostly un-
determined, but by the co- and contra-dominance
principles, the solver has already established that
some nodes must dominate others: A dotted edge
with label s in the picture means that the solver
knows there must be a path between these two nodes
which starts with an s-edge. In other words, the
solver has computed a large amount of semantic in-
formation from an incomplete syntactic analysis.
Now imagine some external source tells us that
with is a mod-child of student on PA, i. e. the anal-
ysis in (iii). This information could come e. g. from
a statistical model of selectional preferences, which
will judge this edge much more probable than an
instr-edge from the verb to the preposition (ii).
Adding this edge will trigger additional inferences
through the linking principle, which can now infer
that with is a prep-child of student on ID. In the other
direction, the solver will infer more dominances on
SC. This means that semantic information can be
used to disambiguate syntactic ambiguities, and se-
mantic information such as selectional preferences
can be stated on their natural level of representation,
rather than be forced into the ID dimension directly.
Similarly, the introduction of new edges on SC
could trigger a similar reasoning process which
would infer new PA-edges, and thus indirectly also
new ID-edges. Such new edges on SC could come
from inferences with world or discourse knowledge
(Koller and Niehren, 2000), scope preferences, or
interactions with information structure (Duchier and
Kruijff, 2003).
4 Traditional Semantics
Our syntax-semantics interface represents seman-
tic information as graphs on the PA and SC dimen-
sions. While this looks like a radical departure from
traditional semantic formalisms, we consider these
graphs simply an alternative way of presenting more
traditional representations. We devote the rest of the
paper to demonstrating that a pair of a PA and a SC
structure can be interpreted as a Montague-style for-
mula, and that a partial analysis on these two di-
mensions can be seen as an underspecified semantic
description.
4.1 Montague-style Interpretation
In order to extract a standard type-theoretic expres-
sion from an XDG analysis, we assign each node v
two semantic values: a lexical value L(v) represent-
ing the semantics of v itself, and a phrasal value
P(v) representing the semantics of the entire SC-
subtree rooted at v. We use the SC-structure to de-
termine functor-argument relationships, and the PA-
structure to establish variable binding.
We assume that nodes for determiners and proper
names introduce unique individual variables (?in-
dices?). Below we will write ??v?? to refer to the in-
dex of the node v, and we write ?` to refer to the
node which is the `-child of the current node in the
appropriate dimension (PA or SC). The semantic lex-
icon is defined as follows; ?L(w)? should be read as
?L(v), where v is a node for the word w?.
L(a) = ?P?Q?e.?x(P(x)?Q(x)(e))
L(book) = book?
L(with) = ?P?x.(with?(???arg??)(x)?P(x))
L(reads) = read?(???pat??)(???ag??)
Lexical values for other determiners, common
nouns, and proper names are defined analogously.
Note that we do not formally distinguish event
variables from individual variables. In particular,
L(with) can be applied to either nouns or verbs,
which both have type ?e, t?.
We assume that no node in the SC-tree has more
than one child with the same edge label (which our
grammar guarantees), and write n(`1, . . . , `k) to in-
dicate that the node n has SC-children over the edge
labels `1, . . . , `k. The phrasal value for n is defined
(in the most complex case) as follows:
P(n(r,s)) = L(n)(P(?r))(? ??n??.P(?s))
This rule implements Montague?s rule of quan-
tification (Montague, 1974); note that ? ??n?? is a
binder for the variable ??n??. Nodes that have no
s-children are simply functionally applied to the
phrasal semantics of their children (if any).
By way of example, consider the left-hand SC-
structure in Fig. 2. If we identify each node by the
word it stands for, we get the following phrasal
@
@
every
?
@
@
a
?
@
@
read var
var
student
book
r
s
s
r
every student reads a book
r
r
s
s
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
Figure 5: A partial SC-structure and its correspond-
ing CLLS description.
value for the root of the tree:
L(a)(L(book))(?x.L(every)(L(student)
(?y.read?(y)(x)))),
where we write x for ??a?? and y for ??every??. The
arguments of read? are x and y because every and
a are the arg and pat children of reads on the PA-
structure. After replacing the lexical values by their
definitions and beta-reduction, we obtain the fa-
miliar representation for this semantic reading, as
shown in Section 3.1.
4.2 Underspecification
It is straightforward to extend this extraction of
type-theoretic formulas from fully specified XDG
analyses to an extraction of underspecified seman-
tic descriptions from partial XDG analyses. We will
briefly demonstrate this here for descriptions in the
CLLS framework (Egg et al, 2001), which sup-
ports this most easily. Other underspecification for-
malisms could be used too.
Consider the partial SC-structure in Fig. 5, which
could be derived by the constraint solver for the
sentence from Fig. 2. We can obtain a CLLS con-
straint from it by first assigning to each node of
the SC-structure a lexical value, which is now a part
of the CLLS constraint (indicated by the dotted el-
lipses). Because student and book are known to be r-
daughters of every and a on SC, we plug their CLLS
constraints into the r-holes of their mothers? con-
straints. Because we know that reads must be dom-
inated by the s-children of the determiners, we add
the two (dotted) dominance edges to the constraint.
Finally, variable binding is represented by the bind-
ing constraints drawn as dashed arrows, and can be
derived from PA exactly as above.
5 Conclusion
In this paper, we have shown how to build a fully re-
lational syntax-semantics interface based on XDG.
This new grammar formalism offers the grammar
developer the possibility to represent different kinds
of linguistic information on separate dimensions
that can be represented as graphs. Any two dimen-
sions can be linked by multi-dimensional principles,
which mutually constrain the graphs on the two di-
mensions. We have shown that a parser based on
concurrent constraint programming is capable of in-
ferences that restrict ambiguity on one dimension
based on newly available information on another.
Because the interface we have presented makes
no assumption that any dimension is more ?basic?
than another, there is no conceptual difference be-
tween parsing and generation. If the input is the sur-
face sentence, the solver will use this information
to compute the semantic dimensions; if the input is
the semantics, the solver will compute the syntactic
dimensions, and therefore a surface sentence. This
means that we get bidirectional grammars for free.
While the solver is reasonably efficient for many
(hand-crafted) grammars, it is an important goal
for the future to ensure that it can handle large-
scale grammars imported from e.g. XTAG (XTAG
Research Group, 2001) or induced from treebanks.
One way in which we hope to achieve this is to iden-
tify fragments of XDG with provably polynomial
parsing algorithms, and which contain most useful
grammars. Such grammars would probably have to
specify word orders that are not completely free,
and we would have to control the combinatorics
of the different dimensions (Maxwell and Kaplan,
1993). One interesting question is also whether dif-
ferent dimensions can be compiled into a single di-
mension, which might improve efficiency in some
cases, and also sidestep the monostratal vs. multi-
stratal distinction.
The crucial ingredient of XDG that make rela-
tional syntax-semantics processing possible are the
declaratively specified principles. So far, we have
only given some examples for principle specifi-
cations; while they could all be written as Horn
clauses, we have not committed to any particular
representation formalism. The development of such
a representation formalism will of course be ex-
tremely important once we have experimented with
more powerful grammars and have a stable intuition
about what principles are needed.
At that point, it would also be highly interest-
ing to define a (logic) formalism that generalises
both XDG and dominance constraints, a fragment of
CLLS. Such a formalism would make it possible to
take over the interface presented here, but use dom-
inance constraints directly on the semantics dimen-
sions, rather than via the encoding into PA and SC
dimensions. The extraction process of Section 4.2
could then be recast as a principle.
Acknowledgements
We thank Markus Egg for many fruitful discussions
about this paper.
References
K. Apt. 2003. Principles of Constraint Programming.
Cambridge University Press.
N. Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2004. Minimal recursion semantics. an introduction.
Journal of Language and Computation. To appear.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In ACL 2001, Toulouse.
D. Duchier and G.-J. M. Kruijff. 2003. Information
structure in topological dependency grammar. In
EACL 2003.
D. Duchier. 2003. Configuration of labeled trees un-
der lexicalized constraints and principles. Research
on Language and Computation, 1(3?4):307?336.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language,
and Information, 10:457?485.
V. Gupta and J. Lamping. 1998. Efficient linear logic
meaning assembly. In COLING/ACL 1998.
R. M. Kaplan and J. T. Maxwell III. 1988. An algorithm
for functional uncertainty. In COLING 1988, pages
297?302, Budapest/HUN.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
COLING-2000, Saarbr?cken.
A. Koller and K. Striegnitz. 2002. Generation as depen-
dency parsing. In ACL 2002, Philadelphia/USA.
J. T. Maxwell and R. M. Kaplan. 1993. The interface
between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
R. Montague. 1974. The proper treatment of quantifica-
tion in ordinary english. In Richard Thomason, editor,
Formal Philosophy. Selected Papers of Richard Mon-
tague, pages 247?271. Yale University Press, New
Haven and London.
M. Moortgat. 2002. Categorial grammar and formal se-
mantics. In Encyclopedia of Cognitive Science. Na-
ture Publishing Group, MacMillan. To appear.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
G. Smolka. 1995. The Oz Programming Model. In
Computer Science Today, Lecture Notes in Computer
Science, vol. 1000, pages 324?343. Springer-Verlag.
M. Steedman. 1999. Alternating quantifier scope in
CCG. In Proc. 37th ACL, pages 301?308.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for english. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
Extensible Dependency Grammar: A New Methodology
Ralph Debusmann
Programming Systems Lab
Saarland University
Postfach 15 11 50
66041 Saarbru?cken
Germany
rade@ps.uni-sb.de
Denys Duchier
?Equipe Calligramme
LORIA ? UMR 7503
Campus Scientifique, B. P. 239
54506 Vand?uvre le`s Nancy CEDEX
France
duchier@loria.fr
Geert-Jan M. Kruijff
Computational Linguistics
Saarland University
Postfach 15 11 50
66041 Saarbru?cken
Germany
gj@coli.uni-sb.de
Abstract
This paper introduces the new grammar formalism
of Extensible Dependency Grammar (XDG), and
emphasizes the benefits of its methodology of ex-
plaining complex phenomena by interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. This has the potential to increase
modularity with respect to linguistic description and
grammar engineering, and to facilitate concurrent
processing and the treatment of ambiguity.
1 Introduction
We introduce the new grammar formalism of Exten-
sible Dependency Grammar (XDG). In XDG, com-
plex phenomena arise out of the interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. In this paper, we point out how
this novel methodology positions XDG in between
multi-stratal approaches like LFG (Bresnan and Ka-
plan, 1982) and MTT (Mel?c?uk, 1988), see also
(Kahane, 2002), and mono-stratal ones like HPSG
(Pollard and Sag, 1994), attempting to combine
their benefits and avoid their problems.
It is the division of linguistic analyses into dif-
ferent dimensions which makes XDG multi-stratal.
On the other, XDG is mono-stratal in that its princi-
ples interact to constrain all dimensions simultane-
ously. XDG combines the benefits of these two po-
sitions, and attempts to circumvent their problems.
From multi-stratal approaches, XDG adopts a high
degree of modularity, both with respect to linguis-
tic description as well as for grammar engineering.
This also facilitates the statement of cross-linguistic
generalizations. XDG avoids the problem of placing
too high a burden on the interfaces, and allows in-
teractions between all and not only adjacent dimen-
sions. From mono-stratal approaches, XDG adopts
a high degree of integration, facilitating concurrent
processing and the treatment of ambiguity. At the
same time, XDG does not lose its modularity.
XDG is a descendant of Topological Depen-
dency Grammar (TDG) (Duchier and Debusmann,
2001), pushing the underlying methodology further
by generalizing it in two aspects:
? number of dimensions: two in TDG (ID and
LP), arbitrary many in XDG
? set of principles: fixed in TDG, extensible
principle library in XDG
The structure of this paper is as follows: In ?2, we
introduce XDG and the XDG solver used for pars-
ing and generation. In ?3, we introduce a number
of XDG principles informally, before making use of
them in an idealized example grammar in ?4. In ?5
we argue why XDG has the potential to be an im-
provement over multi-stratal and mono-stratal ap-
proaches, before we conclude in ?6.
2 Extensible Dependency Grammar
In this section, we introduce XDG formally and
mention briefly the constraint-based XDG solver for
parsing and generation.
2.1 Formalization
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principles, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments
called lexical entries. An analysis on dimension
D is a triple (V,E,F) of a set V of nodes, a set
E ? V ?V ?Lab of directed labeled edges, and an
assignment F : V ? (Fea ? Val) of lexical entries
to nodes. V and E form a graph. We write AnaD for
the set of all possible analyses on dimension D. The
principles characterize subsets of AnaD. We assume
that the elements of Pri are finite representations of
such subsets.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Ana1 ? ?? ? ?
Anan where all dimensions share the same set of
nodes V . We call a dimension of a grammar gram-
mar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex ? Lex1??? ??Lexn con-
strains all dimensions at once, thereby synchroniz-
ing them. An XDG analysis is licensed by Lex iff
(F1(v), . . . ,Fn(v)) ? Lex for every node v ?V .
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are licensed
by Lex, and consistent with Inp and Pri. The input
constraints determine whether XDG solving is to be
used for parsing or generation. For parsing, they
specify a sequence of words, and for generation, a
multiset of semantic literals.
2.2 Solver
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of integers,
where well-formed analyses correspond to the solu-
tions of the CSP (Duchier, 2003). We have imple-
mented an XDG solver using the Mozart-Oz pro-
gramming system.
XDG solving operates on all dimensions concur-
rently. This means that the solver can infer informa-
tion about one dimension from information on an-
other, if there is either a multi-dimensional principle
linking the two dimensions, or by the synchroniza-
tion induced by the lexical entries. For instance, not
only can syntactic information trigger inferences in
syntax, but also vice versa.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an
NP-complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential. The average-case complexity
of many smaller-scale grammars that we have ex-
perimented with seems polynomial, but it remains
to be seen whether we can scale this up to large-
scale grammars.
3 Principles
The well-formedness conditions of XDG analy-
ses are stipulated by principles. Principles are
parametrizable, e.g. by the dimensions on which
they are applied, or by lexical features. They can
be lexicalized or non-lexicalized, and can be one-
dimensional or multi-dimensional. Principles are
taken from an extensible principle library, and we
introduce some of the most important principles in
the following.
3.1 Tree principle
tree(i) The analysis on dimension i must be a tree.
The tree principle is non-lexicalized and
parametrized by the dimension i.
3.2 Dag principle
dag(i) The analysis on dimension i must be a di-
rected acyclic graph.
The dag principle is non-lexicalized and
parametrized by the dimension i.
3.3 Valency principle
valency(i, ini,outi) All nodes on dimension i must
satisfy their in and out specifications.
The valency principle is lexicalized and serves
to lexically describe dependency graphs. It is
parametrized by the dimension i, the in specification
ini and the out specification outi. For each node, ini
stipulates the licensed incoming edges, and outi the
licensed outgoing edges.
In the example grammar lexicon part in Figure 1
below, the in specification is inID and outID is the
out specification on the ID dimension. For the com-
mon noun Roman, the in specification licenses zero
or one incoming edges labeled subj, and zero or one
incoming edges labeled obj ({subj?,obj?}), i.e. it
can be either a subject or an object. The out specifi-
cation requires precisely one outgoing edge labeled
det ({det!}), i.e. it requires a determiner.
3.4 Government principle
government(i,casesi,governi) All edges in dimen-
sion i must satisfy the government specification of
the mother.
The government principle is lexicalized. Its pur-
pose is to constrain the case feature of a depen-
dent.1 It is parametrized by the dimension i, the
cases specification casesi and the government spec-
ification govern. cases assigns to each word a set of
possible cases, and govern a mapping from labels to
sets of cases.
In Figure 1, the cases specification for the deter-
miner den is {acc} (i.e. it can only be accusative).
By its government specification, the finite verb ver-
sucht requires its subject to exhibit nominative case
(subj 7? {nom}).
3.5 Agreement principle
agreement(i,casesi,agreei) All edges in dimension
i must satisfy the agreement specification of the
mother.
1We restrict ourselves to the case feature only for simplicity.
In a fully-fledged grammar, the government principle would be
used to constrain also other morphological aspects like number,
person and gender.
The agreement principle is lexicalized. Its pur-
pose is to enforce the case agreement of a daugh-
ter.2 It is parametrized by dimension i, the lexical
cases specification casesi, assigning to each word a
set of possible cases, and the agreement specifica-
tion agreei, assigning to each word a set of labels.
As an example, in Figure 1, the agreement spec-
ification for the common noun Roman is {det}, i.e.
the case of the common noun must agree with its
determiner.
3.6 Order principle.
order(i,oni,?i) On dimension i, 1) each node must
satisfy its node labels specification, 2) the order of
the daughters of each node must be compatible with
?i, and 3) the node itself must be ordered correctly
with respect to its daughters (using its node label).
The order principle is lexicalized. It is
parametrized by the dimension i, the node labels
specification oni mapping each node to set of labels
from Labi, and the total order ?i on Labi.
Assuming the node labels specification given in
Figure 2, and the total order in (5), the tree in (11)
satisfies the order principle.3 For instance for the
node versucht: 1) The node label of versucht is lbf,
satisfying the node labels specification. 2) The order
of the daughters Roman (under the edge labeled vf),
Peter (mf) and lesen (rbf) is compatible with the
total order prescribing vf ? mf ? rbf. 3) The node
versucht itself is ordered correctly with respect to its
daughters (the total order prescribes vf ? lbf ?mf).
3.7 Projectivity principle
projectivity(i) The analysis on dimension i must be
projective.
The projectivity principle is non-lexicalized. Its
purpose is to exclude non-projective analyses.4 It is
parametrized by dimension i.
3.8 Climbing principle
climbing(i, j) The graph on dimension i must be
flatter than the graph on dimension j.
The climbing principle is non-lexicalized and
two-dimensional. It is parametrized by the two di-
mensions i and j.
For instance, the tree in (11) is flatter than the
corresponding tree in (10). This concept was intro-
duced as lifting in (Kahane et al, 1998).
2Again, we restrict ourselves to case for simplicity.
3The node labels are defined in (2) below.
4The projectivity principle of course only makes sense in
combination with the order principle.
3.9 Linking principle
linking(i, j, linki, j) All edges on dimension i must
satisfy the linking specification of the mother.
The linking principle is lexicalized and two-
dimensional. It is parametrized by the two dimen-
sions i and j, and by the linking specification linki, j ,
mapping labels from Labi to sets of labels from
Lab j . Its purpose is to specify how dependents on
dimension i are realized by (or linked to) dependents
on dimension j.
In the lexicon part in Figure 3, the linking spec-
ification for the transitive verb lesen requires that
its agent on the PA dimension must be realized by a
subject (ag 7? {subj}), and the patient by an object
(pat 7? {obj}).
The linking principle is oriented. Symmetric
linking could be gained simply by using the linking
principle twice (in both directions).
4 Example grammar
In this section, we elucidate XDG with an example
grammar fragment for German. With it, we demon-
strate three aspects of the methodology of XDG:
? How complex phenomena such as topicaliza-
tion and control arise by the interaction of sim-
ple principles on different dimensions of lin-
guistic description.
? How the high degree of integration helps to re-
duce ambiguity.
? How the high degree of modularity facilitates
the statement of cross-linguistic generaliza-
tions.
Note that this grammar fragment is an idealized ex-
ample, and does not make any claims about XDG as
a grammar theory. Its purpose is solely to substan-
tiate our points about XDG as a framework. More-
over, the grammar is fully lexicalized for simplicity.
However, XDG of course allows the grammar writer
to formulate lexical abstractions using inheritance
(like in HPSG) or crossings (Candito, 1996).
4.1 Dimensions
The grammar fragment make use of two dimen-
sions: Immediate Dominance (ID) and Linear
Precedence (LP). The models on the ID dimension
are unordered, syntactic dependency trees whose
edge labels correspond to syntactic functions like
subject and object. On the LP dimension, the mod-
els are ordered, projective topological dependency
trees whose edge labels are topological fields like
Vorfeld and Mittelfeld.
4.2 Labels
The set LabID of labels on the ID dimension is:
LabID = {det,subj,obj,vinf ,part} (1)
These correspond resp. to determiner, subject, ob-
ject, infinitive verbal complement, and particle.
The set LabLP of labels on the LP dimension is:
LabLP = {detf,nounf,vf, lbf,mf,partf, rbf} (2)
Corresponding resp. to determiner field, noun field,
Vorfeld, left bracket field, Mittelfeld, particle field,
and right bracket field.
4.3 Principles
On the ID dimension, we make use of the following
one-dimensional principles:
tree(ID)
valency(ID, inID,outID)
government(ID,casesID,governID)
agreement(ID,casesID,agreeID)
(3)
The LP dimension uses the following principles:
tree(LP)
valency(LP, inLP,outLP)
order(LP,onLP,?LP)
projectivity(LP)
(4)
where the total order ?LP is defined as:
detf ? nounf ? vf ? lbf ? mf ? partf ? rbf (5)
We make use of the following multi-dimensional
principles:
climbing(LP, ID)
linking(LP, ID) (6)
4.4 Lexicon
We split the lexicon into two parts. The ID and LP
parts are displayed resp. in Figure 15 and Figure 2.
The LP part includes also the linking specification
for the LP,ID-application of the linking principle.6
4.5 Government and agreement
Our first example is the following sentence:
Peter versucht einen Roman zu lesen.
Peter tries aacc novel to read.
Peter tries to read a novel.
(7)
We display the ID analysis of the sentence below:
.
Peter versucht einen Roman zu lesen
subj vinf
partobj
det
(8)
5Here, stands for ?don?t care?, this means e.g. for the verb
versucht that it has unspecified case.
6We do not make use of the linking specification for the
German grammar fragment (the mappings are all empty), but
we will do so as we switch to Dutch in ?4.8 below.
Here, Peter is the subject of versucht. lesen is the in-
finitival verbal complement of versucht, zu the parti-
cle of lesen, and Roman the object of lesen. Finally,
einen is the determiner of Roman.
Under our example grammar, the sentence is un-
ambiguous, i.e. the given ID tree is the only possible
one. Other ID trees are ruled out by the interaction
of the principles on the ID dimension. For instance,
the government and agreement principles conspire
to rule out the reading where Roman is the subject of
versucht (and Peter the object). How? By the agree-
ment principle, Roman must be accusative, since it
agrees with its accusative determiner einen. By the
government principle, the subject of versucht must
be nominative, and the object of lesen accusative.
Thus Roman, by virtue of being accusative, cannot
become the subject of versucht. The only other op-
tion for it is to become the object of lesen. Conse-
quently, Peter, which is unspecified for case, must
become the subject of versuchen (versuchen must
have a subject by the valency principle).
4.6 Topicalization
Our second example is a case of topicalization,
where the object has moved into the Vorfeld, to the
left of the finite verb:
Einen Roman versucht Peter zu lesen. (9)
Here is the ID tree and the LP tree analysis:
.
Einen Roman versucht Peter zu lesen
subj vinf
partobj
det
(10)
.
Einen Roman versucht Peter zu lesen
detf
nounf
lbf
nounf
partf
rbf
vf
detf
mf rbf
part
f
(11)
The ID tree analysis is the same as before, except
that the words are shown in different positions. In
the LP tree, Roman is in the Vorfeld of versucht, Pe-
ter in the Mittelfeld, and lesen in the right bracket
field. versucht itself is (by its node label) in the left
bracket field. Moreover, Einen is in the determiner
field of Roman, and zu in the particle field of lesen.
Again, this is an example demonstrating how
complex phenomena (here: topicalization) are ex-
plained by the interaction of simple principles. Top-
icalization does not have to explicitly taken care of,
it is rather a consequence of the interacting princi-
ples. Here, the valency, projectivity and climbing
inID outID casesID governID agreeID
den {det?} {} {acc} {} {}
Roman {subj?,obj?} {det!} {nom,dat,acc} {} {det}
Peter {subj?,obj?} {} {nom,dat,acc} {} {}
versucht {} {subj!,vinf!} {subj 7? {nom}} {}
zu {part?} {} {} {}
lesen {vinf?} {obj!} {obj 7? {acc}} {}
Figure 1: Lexicon for the example grammar fragment, ID part
inLP outLP onLP linkLP,ID
den {detf?} {} {detf} {}
Roman {vf?,mf?} {detf!} {nounf} {}
Peter {vf?,mf?} {} {nounf} {}
versucht {} {vf?,mf?, rbf?} {lbf} {}
zu {partf?} {} {partf} {}
lesen {rbf?} {} {rbf} {}
Figure 2: Lexicon for the example grammar fragment, LP part
principles conspire to bring about the ?climbing up?
of the NP Einen Roman from being the daughter of
lesen in the ID tree to being the daughter of versucht
in the LP tree: The out specification of lesen does
not license any outgoing edge. Hence, Roman must
become the daughter of another node. The only pos-
sibility is versucht. The determiner Einen must then
also ?climb up? because Roman is its only possi-
ble mother. The result is an LP tree which is flat-
ter with respect to the ID tree. The LP tree is also
projective. If it were not be flatter, then it would
be non-projective, and ruled out by the projectivity
principle.
4.7 Negative example
Our third example is a negative example, i.e. an un-
grammatical sentence:
?Peter einen Roman versucht zu lesen. (12)
This example is perfectly legal on the unordered ID
dimension, but has no model on the LP dimension.
Why? Because by its LP out specification, the finite
verb versucht allows only one dependent to the left
of it (in its Vorfeld), and here we have two. The
interesting aspect of this example is that although
we can find a well-formed ID tree for it, this ID tree
is never actually generated. The interactions of the
principles, viz. here of the principles on the LP di-
mension, rule out the sentence before any full ID
analysis has been found.
4.8 From German to Dutch
For the fourth example, we switch from German to
Dutch. We will show how to use the lexicon to con-
cisely capture an important cross-linguistic general-
ization. We keep the same grammar as before, but
with two changes, arising from the lesser degree of
inflection and the higher reliance on word order in
Dutch:
? The determiner een is not case-marked but
can be either nominative, dative or accusative:
casesID = {nom,dat,acc}.
? The Vorfeld of the finite verb probeert cannot
be occupied by an object (but only by an ob-
ject): linkLP,ID = {vf 7? {subj}}.7
Now to the example, a Dutch translation of (7):
Peter probeert een roman te lezen.
Peter tries a novel to read.
Peter tries to read a novel.
(13)
We get only one analysis on the ID dimension,
where Peter is the subject and roman the object.
An analysis where Peter is the object of lezen and
roman the subject of probeert is impossible, as in
the German example. The difference is, however,
how this analysis is excluded. In German, the ac-
cusative inflection of the determiner einen triggered
the agreement and the government principle to rule
it out. In Dutch, the determiner is not inflected.
The unwanted analysis is excluded on the grounds
of word order instead: By the linking principle, the
Vorfeld of probeert must be filled by a subject, and
not by an object. That means that Peter in the Vor-
feld (to the left of probeert) must be a subject, and
consequently, the only other choice for roman is that
it becomes the object of lezen.
4.9 Predicate-Argument Structure
Going towards semantics, we extend the grammar
with another dimension, Predicate-Argument Struc-
ture (PA), where the models are not trees but di-
rected acyclic graphs (dags), to model re-entrancies
7Of course, this is an idealized assumption. In fact, given
the right stress, the Dutch Vorfeld can be filled by objects.
e.g. caused by control constructions. Thanks to the
modularity of XDG, the PA part of the grammar is
the same for German and Dutch.
The set LabPA of labels on the PA dimension is:
LabPA = {ag,pat,prop} (14)
Corresponding resp. to agent, patient and proposi-
tion.
The PA dimension uses the following one-
dimensional principles:
dag(PA)
valency(PA, inPA,outPA) (15)
Note that we re-use the valency principle again, as
we did on the ID and LP dimensions.
And also the following multi-dimensional princi-
ples:
climbing(ID, PA)
linking(PA, ID) (16)
Here, we re-use the climbing and linking princi-
ples. That is, we state that the ID tree is flatter
than the corresponding PA dag. This captures rais-
ing and control, where arguments of embedded infi-
nite verbs can ?climb up? and become arguments of
a raising or control verb, in the same way as syntac-
tic arguments can ?climb up? from ID to LP. We use
the linking principle to specify how semantic argu-
ments are to be realized syntactically (e.g. the agent
as a subject etc.). We display the PA part of the lex-
icon in Figure 3.8
Here is an example PA dag analysis of example
sentence (7):
.
Peter versucht einen Roman zu lesen
ag
prop
patag
(17)
Here, Peter is the agent of versucht, and also the
agent of lesen. Furthermore, lesen is a proposition
dependent of versucht, and Roman is the patient of
lesen.
Notice that the PA dag is indeed a dag and not a
tree since Peter has two incoming edges: It is simul-
taneously the agent of versucht and of lesen. This
is enforced by by the valency principle: Both ver-
sucht and lesen require an agent. Peter is the only
word which can be the agent of both, because it is a
subject and the agents of versucht and lesen must
be subjects by the linking principle. The climb-
ing principle ensures that predicate arguments can
8Notice that we specify linking lexically, allowing us to
capture deviations from the typical linking patterns. Still, we
can also accommodate linking generalizations using lexical ab-
stractions.
be ?raised? on the ID structure with respect to the
PA structure. Again, this example demonstrates that
XDG is able to reduce a complex phenomenon such
as control to the interaction of per se fairly simple
principles such as valency, climbing and linking.
5 Comparison
This section includes a more in-depth comparison
of XDG with purely multi- and mono-stratal ap-
proaches.
Contrary to multi-stratal approaches like LFG or
MTT, XDG is more integrated. For one, it places
a lighter burden the interfaces between the dimen-
sions. In LFG for instance, the ? -mapping from c-
structure to f-structure is rather specific, and has to
be specifically adapted to new c-structures, e.g. in
order to handle a new construction with a different
word order. That is, not only the grammar rules for
the c-structure need to be adapted, but also the inter-
face between c- and f-structure. In XDG, complex
phenomena arise out of the interaction of simple,
maximally general principles. To accommodate the
new construction, the grammar would ideally only
need to be adapted on the word order dimension.
Furthermore, XDG allows interactions of rela-
tional constraints between all dimensions, not only
between adjacent ones (like c- and f-structure),
and in all directions. For one, this gets us bi-
directionality for free. Secondly, the interactions
of XDG have the potential to help greatly in reduc-
ing ambiguity. In multi-stratal approaches, ambigu-
ity must be duplicated throughout the system. E.g.
suppose there are two candidate c-structures in LFG
parsing, but one is ill-formed semantically. Then
they can only be ruled out after duplicating the am-
biguity on the f-structure, and then filtering out the
ill-formed structure on the semantic ? -structure. In
XDG on the other hand, the semantic principles can
rule out the ill-formed analysis much earlier, typ-
ically on the basis of a partial syntactic analysis.
Thus, ill-formed analyses are never duplicated.
Contrary to mono-stratal ones, XDG is more
modular. For one, as (Oliva et al, 1999) note,
mono-stratal approaches like HPSG usually give
precedence to the syntactic tree structure, while
putting the description of other aspects of the anal-
ysis on the secondary level only, by means of fea-
tures spread over the nodes of the tree. As a result,
it becomes a hard task to modularize grammars. Be-
cause syntax is privileged, the phenomena ascribing
to semantics cannot be described independently, and
whenever the syntax part of the grammar changes,
the semantics part needs to be adapted. In XDG, no
dimension is privileged to another. Semantic phe-
inPA outPA linkPA,ID
den {} {} {}
Roman {ag?,pat?} {} {}
Peter {ag?,pat?} {} {}
versucht {} {ag!,prop!} {ag 7? {subj},prop 7? {vinf}}
zu {} {} {}
lesen {prop?} {ag!,pat!} {ag 7? {subj},pat 7? {obj}}
Figure 3: Lexicon of the example grammar fragment, PA part
nomena can be described much more independently
from syntax. This facilitates grammar engineering,
and also the statement of cross-linguistic general-
izations. Assuming that the semantics part of a
grammar stay invariant for most natural languages,
in order to accommodate a new language, ideally
only the syntactic parts would need to be changed.
6 Conclusion
In this paper, we introduced the XDG grammar
framework, and emphasized that its new methodol-
ogy places it in between the extremes of multi- and
mono-stratal approaches. By means of an idealized
example grammar, we demonstrated how complex
phenomena are explained as arising from the in-
teraction of simple principles on numerous dimen-
sions of linguistic description. On the one hand, this
methodology has the potential to modularize lin-
guistic description and grammar engineering, and
to facilitate the statement of linguistic generaliza-
tions. On the other hand, as XDG is a inherently
concurrent architecture, inferences from any dimen-
sion can help reduce the ambiguity on others.
XDG is a new grammar formalism, and still has
many open issues. Firstly, we need to continue work
on XDG as a framework. Here, one important goal
is to find out what criteria we can give to restrict the
principles. Secondly, we need to evolve the XDG
grammar theory, and in particular the XDG syntax-
semantics interface. Thirdly, for practical use, we
need to improve our knowledge about XDG solv-
ing (i.e. parsing and generation). So far, our only
good results are for smaller-scale handwritten gram-
mars, and we have not good results yet for larger-
scale grammars induced from treebanks (NEGRA,
PDT) or converted from other grammar formalisms
(XTAG). Finally, we need to incorporate statistics
into the picture, e.g. to guide the search for solu-
tions, in the vein of (Dienes et al, 2003).
References
Joan Bresnan and Ronald Kaplan. 1982. Lexical-
functional grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 173?281. The MIT Press, Cam-
bridge/USA.
Marie-He`le?ne Candito. 1996. A principle-based hi-
erarchical representation of LTAG. In Proceed-
ings of COLING 1996, Kopenhagen/DEN.
Peter Dienes, Alexander Koller, and Marco
Kuhlmann. 2003. Statistical A* Dependency
Parsing. In Prospects and Advances in the Syn-
tax/Semantics Interface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001.
Topological dependency trees: A constraint-
based account of linear precedence. In Proceed-
ings of ACL 2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled
trees under lexicalized constraints and principles.
Research on Language and Computation, 1(3?
4):307?336.
Sylvain Kahane, Alexis Nasr, and Owen Ram-
bow. 1998. Pseudo-projectivity: a polynomi-
ally parsable non-projective dependency gram-
mar. In 36th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 1998),
Montre?al/CAN.
Sylvain Kahane. 2002. Grammaire d?Unification
Sens-Texte: Vers un mode`le mathe?matique ar-
ticule? de la langue. Universite? Paris 7. Docu-
ment de synthe`se de l?habilitation a` diriger les
recherches.
Alexander Koller and Kristina Striegnitz. 2002.
Generation as dependency parsing. In Proceed-
ings of ACL 2002, Philadelphia/USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Karel Oliva, M. Andrew Moshier, and Sabine
Lehmann. 1999. Grammar engineering for the
next millennium. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium
1999 ?Closing the Millennium?, Beijing/CHI.
Tsinghua University Press.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago/USA.
219
220
221
222
223
224
225
226
XMG: eXtensible MetaGrammar
Beno??t Crabbe??
INRIA - Universite? Paris 7
Denys Duchier??
LIFO - Universite? d?Orle?ans
Claire Gardent?
CNRS - LORIA, Nancy
Joseph Le Roux?
LIPN - Universite? Paris Nord
Yannick Parmentier?
LIFO - Universite? d?Orle?ans
In this article, we introduce eXtensible MetaGrammar (XMG), a framework for specifying
tree-based grammars such as Feature-Based Lexicalized Tree-Adjoining Grammars (FB-LTAG)
and Interaction Grammars (IG). We argue that XMG displays three features that facilitate
both grammar writing and a fast prototyping of tree-based grammars. Firstly, XMG is fully
declarative. For instance, it permits a declarative treatment of diathesis that markedly departs
from the procedural lexical rules often used to specify tree-based grammars. Secondly, the XMG
language has a high notational expressivity in that it supports multiple linguistic dimensions,
inheritance, and a sophisticated treatment of identifiers. Thirdly, XMG is extensible in that its
computational architecture facilitates the extension to other linguistic formalisms. We explain
how this architecture naturally supports the design of three linguistic formalisms, namely,
FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show
how it permits a straightforward integration of additional mechanisms such as linguistic and
formal principles. To further illustrate the declarativity, notational expressivity, and extensibility
of XMG, we describe the methodology used to specify an FB-LTAG for French augmented with a
? UFR de Linguistique, Universite? Paris Diderot-Paris 7, Case 7003, 2, F-75205 Paris Cedex 13, France.
E-mail: bcrabbe@linguist.jussieu.fr.
?? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: denys.duchier@univ-orleans.fr.
? Laboratoire LORIA - CNRS, Projet Synalp, Ba?timent B, BP 239, Campus Scientifique, F-54506
Vand?uvre-Le`s-Nancy Cedex, France. E-mail: gardent@loria.fr.
? Laboratoire d?Informatique de Paris Nord, UMR CNRS 7030, Institut Galile?e - Universite? Paris-Nord, 99,
avenue Jean-Baptiste Cle?ment, F-93430 Villetaneuse, E-mail: leroux@univ-paris13.fr.
? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: yannick.parmentier@univ-orleans.fr.
Submission received: 27 March 2009; revised version received: 2 July 2012; accepted for publication:
11 August 2012.
doi:10.1162/COLI a 00144
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
unification-based compositional semantics. This illustrates both how XMG facilitates the
modeling of the tree fragment hierarchies required to specify tree-based grammars and of a
syntax/semantics interface between semantic representations and syntactic trees. Finally, we
briefly report on several grammars for French, English, and German that were implemented
using XMG and compare XMG with other existing grammar specification frameworks for
tree-based grammars.
1. Introduction
In the late 1980s and early 1990s, many grammar engineering environments were
developed to support the specification of large computational grammars for natural
language. One may, for instance, cite XLE (Kaplan and Newman 1997) for specifying
Lexical-Functional Grammars (LFG), LKB (Copestake and Flickinger 2000) for speci-
fying Head-driven Phrase Structure Grammars (HPSG), and DOTCCG (Baldridge
et al 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such
environments usually rely on (i) a formal language used to describe a target com-
putational grammar, and (ii) a processor for this language, which aims at generating
the actual described grammar (and potentially at checking it, e.g., by feeding it to
a parser).
Although these environments were tailored for specific grammar formalisms, they
share a number of features. Firstly, they are expressive enough to characterize subsets
of natural language. Following Shieber (1984), we call this feature weak completeness.
Secondly, they are notationally expressive enough to relatively easily formalize important
theoretical notions. Thirdly, they are rigorous, that is, the semantics of their underlying
language is well defined and understood. Additionally, for an environment to be useful
in practice, it should be simple to use (by a linguist), and make it possible to detect errors
in the described target grammar.
If we consider a particular type of computational grammar, namely, tree-based
grammars?that is, grammars where the basic units are trees (or tree descriptions) of
arbitrary depth, such as Tree-Adjoining Grammar (TAG; Joshi, Levy, and Takahashi
1975), D-Tree Grammar (DTG; Rambow, Vijay-Shanker, and Weir 1995), Tree Description
Grammars (TDG; Kallmeyer 1999) or Interaction Grammars (IG; Perrier 2000)?
environments sharing all of the listed features are lacking. As we shall see in Section 7
of this article, there have been some proposals for grammar engineering environments
for tree-based grammar (e.g., Candito 1996; Xia, Palmer, and Vijay-Shanker 1999,
but these lack notational expressivity. This is partly due to the fact that tree-based
formalisms offer an extended domain of locality where one can encode constraints
between remote syntactic constituents. If one wants to define such constraints while
giving a modular and incremental specification of the grammar, one needs a high level
of notational expressivity, as we shall see throughout the article (and especially in
Section 4).
In this article, we present XMG (eXtensible MetaGrammar), a framework for
specifying tree-based grammars. Focusing mostly on Feature-Based Lexicalized Tree-
Adjoining Grammars (FB-LTAG) (but using Interaction Grammars [IG] and Multi-
Component Tree-Adjoining Grammars [MC-TAG] to illustrate flexibility), we argue that
XMG departs from other existing computational frameworks for designing tree-based
grammars in three main ways:
 First, XMG is a declarative language. In other words, grammaticality is
defined in an order-independent fashion by a set of well-formedness
592
Crabbe? et al XMG: eXtensible MetaGrammar
constraints rather than by procedures. In particular, XMG permits a
fully declarative treatment of diathesis that markedly departs from the
procedural rules (called meta-rules or lexical rules) previously used to
specify tree-based grammars.
 Second, XMG is notationally expressive. The XMG language supports full
disjunction and conjunction of grammatical units, a modular treatment
of multiple linguistic dimensions, multiple inheritance of units, and a
sophisticated treatment of identifiers. We illustrate XMG?s notational
expressivity by showing (i) how it facilitates the modeling of the tree
fragment hierarchies required to specify tree-based grammars and (ii) how
it permits a natural modeling of the syntax/semantics interface between
semantic representations and syntactic trees as can be used in FB-LTAG.
 Third, XMG is extensible in that its computational architecture facilitates
(i) the integration of an arbitrary number of linguistic dimensions (syntax,
semantics, etc.), (ii) the modeling of different grammar formalisms
(FB-LTAG, MC-TAG, IG), and (iii) the specification of general linguistic
principles (e.g., clitic ordering in French).
The article is structured as follows. Section 2 starts by giving a brief introduction
to FB-LTAG, the grammar formalism we used to illustrate most of XMG?s features. The
next three sections then go on to discuss and illustrate XMG?s three main features?
namely, declarativity, notational expressivity, and flexibility. In Section 3, we focus
on declarativity and show how XMG?s generalized disjunction permits a declarative
encoding of diathesis. We then contrast the XMG approach with the procedural methods
previously resorted to for specifying FB-LTAG. Section 4 addresses notational expressiv-
ity. We present the syntax of XMG and show how the sophisticated identifier handling
it supports or permits a natural treatment (i) of identifiers in tree based hierarchies
and (ii) of the unification-based syntax/semantics interface often used in FB-LTAG. In
Section 5, we concentrate on extensibility. We first describe the operational semantics
of XMG and the architecture of the XMG compiler. We then show how these facilitate
the adaptation of the basic XMG language to (i) different grammar formalisms (IG,
MC-TAG, FB-LTAG), (ii) the integration of specific linguistic principles such as clitic
ordering constraints, and (iii) the specification of an arbitrary number of linguistic
dimensions. In Section 6, we illustrate the usage of XMG by presenting an XMG
specification for the verbal fragment of a large scale FB-LTAG for French augmented
with a unification-based semantics. We also briefly describe the various other tree-
based grammars implemented using XMG. Section 7 discusses the limitations of other
approaches to the formal specification of tree-based grammars, and Section 8 concludes
with pointers for further research.
2. Tree-Adjoining Grammar
A Tree-Adjoining Grammar (TAG) consists of a set of auxiliary or initial elementary
trees and of two tree composition operations, namely, substitution and adjunction.
Initial trees are trees whose leaves are either substitution nodes (marked with ?) or
terminal symbols (words). Auxiliary trees are distinguished by a foot node (marked
with ) whose category must be the same as that of the root node. Substitution inserts a
tree onto a substitution node of some other tree and adjunction inserts an auxiliary tree
593
Computational Linguistics Volume 39, Number 3
N
Marie
Mary
V
V
a
has
V
S
N? V
vu
seen
N?
N
Jean
John
??
S
N
Marie
Mary
V
V
a
has
V
vu
seen
N
Jean
John
Figure 1
Sample derivation of Marie a vu Jean ?Mary has seen John? in a TAG.
into a tree. Figure 1 shows a toy TAG generating the sentence Marie a vu Jean ?Mary has
seen John? and sketches its derivation.1
Among existing variants of TAG, one commonly used in practice is Lexical-
ized FB-LTAG (Vijay-Shanker and Joshi 1988). A lexicalized TAG is such that each
elementary tree has at least one leaf labeled with a lexical item (word), whereas in
an FB-LTAG, tree nodes are additionally decorated with two feature structures (called
top and bottom). These feature structures are unified during derivation as follows. On
substitution, the top features of the substitution node are unified with the top features of
the root node of the tree being substituted in. On adjunction, the top features of the root
of the auxiliary tree are unified with the top features of the node where adjunction takes
place; and the bottom features of the foot node of the auxiliary tree are unified with the
bottom features of the node where adjunction takes place. At the end of a derivation,
the top and bottom feature structures of all nodes in the derived tree are unified.
Implementation of Tree-Adjoining Grammars. Most existing implementations of TAGs fol-
low the three-layer architecture adopted for the XTAG grammar (XTAG Research Group
2001), a feature-based lexicalized TAG for English. Thus the grammar consists of (i) a
set of so-called tree schemas (i.e., elementary trees having a leaf node labeled with a
 referring to where to anchor lexical items2), (ii) a morphological lexicon associating
words with lemmas, and (iii) a syntactic lexicon associating lemmas with tree schemas
(these are gathered into families according to syntactic properties, such as the sub-
categorization frame for verbs). Figure 2 shows some of the tree schemas associated
with transitive verbs in the XTAG grammar. The tree corresponds (a) to a declarative
sentence, (b) to a WH-question on the subject, (c) to a passive clause with a BY-agent,
and (d) to a passive clause with a WH-object. As can be seen, each tree schema contains
an anchor node (marked with ). During parsing this anchor node can be replaced by
any word morphologically related to a lemma listed in the syntactic lexicon as anchor-
ing the transitive tree family.
This concept of tree family allows us to share structural information (tree schemas)
between words having common syntactic properties (e.g., sub-categorization frames).
There still remains a large redundancy within the grammar because many elementary
tree schemas share common subtrees (large coverage TAGs usually consist of hun-
dreds, sometimes thousands, of tree schemas). An important issue when specifying
1 The elementary trees displayed in this article conform to Abeille? (2002), that is, we reject the use of a VP
constituent in French.
2 As mentioned earlier, we describe lexicalized TAG, thus every tree schema has to contain at least one
anchor (node labeled ).
594
Crabbe? et al XMG: eXtensible MetaGrammar
(a) (b)
Sr
NP0 ? VP
V NP1 ?
Sq
NP0 ?
[
wh +
]
[]
Sr
NPNA

VP
V NP1 ?
(c) (d)
Sr [][
mode 3
]
NP1 ? VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Sq
NP1 ?
[
wh +
]
[]
Sr [][
mode 3
]
NPNA

VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Figure 2
Some tree schemas for English transitive verbs.
such grammars is thus structure sharing. Being able to share structural information is
necessary not only for a faster grammar development, but also for an easier grammar
maintenance (modifications to be applied to the tree schemas would be restricted to
shared structures). In the next section, we will see how XMG declarativity can be
efficiently used to factorize TAGs. In addition, Section 4 will show how XMG notational
expressivity facilitates the specification of another commonly used tree sharing device,
namely, inheritance hierarchies of tree fragments.
Extending TAG with a Unification-Based Semantics. To extend FB-LTAG with a compo-
sitional semantics, Gardent and Kallmeyer (2003) propose to associate each elementary
tree with a flat semantic representation. For instance, in Figure 3, the trees3 for John, runs,
and often are associated with the semantics l0:name(j,john), l1:run(e,s), and l2:often(x),
respectively. Importantly, the arguments of semantic functors are represented by uni-
fication variables which occur both in the semantic representation of this functor and
on some nodes of the associated syntactic tree. Thus in Figure 3, the semantic index s
occurring in the semantic representation of runs also occurs on the subject substitution
node of the associated elementary tree. The value of semantic arguments is then deter-
mined by the unifications resulting from adjunction and substitution. For instance, the
semantic index s in the tree for runs is unified during substitution with the semantic
index j labeling the root node of the tree for John. As a result, the semantics of John often
runs is {l0:name(j,john), l1:run(e,j), l2:often(e)}.
Gardent and Kallmeyer?s (2003) proposal was applied to various semantic phe-
nomena (Kallmeyer and Romero 2004a, 2004b, 2008). Its implementation, however,
3 Cx/Cx abbreviate a node with category C and a top/bottom feature structure including the feature-value
pair { index : x}.
595
Computational Linguistics Volume 39, Number 3
NPj
John
l0:name(j,john)
Sg
NP?s VPgf
Vfe
runs
l1:run(e,s)
VPx
often VP*x
l2:often(x)
? l0:name(j,john), l1:run(e,j), l2:often(e)
Figure 3
A toy lexicalized FTAG with unification-based semantics (l0, l1, l2, e, and j are constants and
s, f, g, x are unification variables).
relies on having a computational framework that associates syntactic trees with flat
semantic formulae while allowing for shared variables between trees and formulae. In
the following sections, we will show how XMG notational expressivity makes it pos-
sible to specify an FB-LTAG equipped with a unification-based semantics.
3. Declarativity
In this section, we show how a phenomenon which is often handled in a procedural
way by existing approaches can be provided with a declarative specification in XMG.
Concretely, we show how XMG supports a declarative account of diathesis that avoids
the drawbacks of lexical rules (e.g., information erasing). We start by presenting the
lexical rule approach. We then contrast it with the XMG account.
3.1 Capturing Diathesis Using Lexical Rules
Following Flickinger (1987), redundancy among grammatical descriptions is often han-
dled using two devices: an inheritance hierarchy and a set of lexical rules. Whereas
the inheritance hierarchy permits us to encode the sharing of common substructures,
lexical rules (sometimes called meta-rules) permit us to capture relationships between
trees by deriving new trees from already specified ones. For instance, passive trees will
be derived from active ones.
Although Flickinger?s (1987) approach was developed for HPSGs, several similar
approaches have been put forward for FB-LTAG (Vijay-Shanker and Schabes 1992;
Becker 1993; Evans, Gazdar, and Weir 1995; XTAG Research Group 2001). One important
drawback of these approaches, however, is that they are procedural in that the order in
which lexical rules apply matters. For instance, consider again the set of trees given
in Figure 2. In the meta-rule representation scheme adopted by Becker (1993), the base
tree (a) would be specified in the inheritance hierarchy grouping all base trees, and
the derived trees (b, c, d) would be generated by applying one or more meta-rules on
this base tree. Figure 4 sketches these meta-rules. The left-hand side of the meta-rule
is a matching pattern replaced with the right-hand side of the meta-rule in the newly
generated tree. Symbol ??? denotes a meta-variable whose matching subtree in the input
is substituted in place of the variable in the output tree. Given these, the tree family in
Figure 2 is generated as follows: (b) and (c) are generated by application to the base
tree (a) of the Wh-Subject and Passive meta-rules, respectively. Further, (d) is generated
by applying first, the Wh-Subject meta-rule and second, the Passive meta-rule to the
base tree.
596
Crabbe? et al XMG: eXtensible MetaGrammar
Passive meta-rule Wh-Subject meta-rule
Sr
?1 NP? VP
V ?2 NP?
? Sr [][
mode 3
]
?2 NP? VP
[
mode 3
]
?
?
mode 2
passive 1
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
?1 NP?
Sr
?2NP? ? ?1
? Sq
?2NP? ?
[
wh +
]
[]
Sr
NP?NA

?1
Figure 4
Simplified meta-rules for passive and wh-subject extraction.
More generally a meta-rule is a procedural device that, given a tree instance,
generates a new tree instance by adding, suppressing (hence possibly substituting)
information in grammatical units. Prolo (2002) defines a set of meta-rules that can
be used to specify a large FB-LTAG for English. Given an ordered set of meta-rules,
however, there is no guarantee that the trees they derive are linguistically appropriate
and that the derivation process terminates. Thus, to ensure termination and consistency,
Prolo needs to additionally provide rule ordering schemes (expressed as automata).
3.2 XMG: Capturing Diathesis Using Disjunction
XMG provides an alternative account for describing tree sets such as that of Figure 2
without lexical rules and without the related ordering constraints. In essence, the
approach consists of enumerating trees by combining tree fragments using conjunction
and disjunction.
More specifically, the tree set given in Figure 2 can be generated by combining
some of the tree fragments sketched in Figure 5 using the following conjunctions and
disjunctions:4
Subject ? CanonicalSubject ? Wh-NP-Subject (1)
ActiveTransitiveVerb ? Subject ? ActiveVerb ? CanonicalObject (2)
PassiveTransitiveVerb ? Subject ? PassiveVerb ? CanonicalByObject (3)
TransitiveVerb ? ActiveTransitiveVerb ? PassiveTransitiveVerb (4)
The first clause (Subject) groups together two subtrees representing the possi-
ble realizations of a subject (canonical and wh). The next two clauses define a tree
set for active and passive transitive verbs, respectively. The last clause defines the
TransitiveVerb family as a disjunction of the two verb forms (passive or active). In sum,
the TransitiveVerb clause defines the tree set sketched in Figure 2 as a disjunction of
conjunctions of tree fragments.
One of the issues of meta-rules reported by Prolo (2002) is the handling of feature
equations. For a number of cases (including subject relativization in passive trees),
4 For now, let us consider that the tree fragments are combined in order to produce minimal trees by
merging nodes whose categories (and features) unify. In the next section, we will see how to precisely
control node identification using either node variables or node constraints.
597
Computational Linguistics Volume 39, Number 3
Canonical Subject ? Wh-NP-Subject ? Canonical Object ? Wh-NP-Object ?
Sr
NP? VP
Sq
NP?
[
wh +
]
[]
Sr
NPNA

VP
VP
V NP?
Sq
NP?
[
wh +
]
[]
Sr
VP NPNA

Canonical By Object ? Wh By Object ? Active Verb ? Passive Verb ?
VP
V PP
P
by
NP?
VP
PP
P
by
NP?
V
Sr
VP
V
Sr[]
[
mode 3
]
VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
Figure 5
Tree fragments.
ad hoc meta-rules are needed, for a unified tree transformation cannot be defined. In
a declarative approach such as the one here, dealing with feature equations can be
done relatively easily. Let us imagine that we now want to extend the trees of Figure 2
with feature equations for subject?number agreement. We can for instance do so by
defining the following tree fragment (the dashed line indicates that the VP node can be
a descendant, not only a daughter, of the S node):5
SubjAgreement ? S
NP?
[
num 1
]
[
num 1
]
VP
[
num 1
]
[
num 1
]
Then we extend the definition of Subject as follows:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (5)
If we want to get further with the description of transitive verbs, for instance by
taking into account wh-objects and by-objects, this can be done as follows. We first
define the elementary fragments Wh-NP-Object and Wh-By-Object (see Figure 5), and
then define the following additional combinations:6
ActiveTransitiveVerb ? CanonicalSubject ? ActiveVerb ? Wh-Np-Object (6)
PassiveTransitiveVerb ? CanonicalSubject ? PassiveVerb ? Wh-By-Object (7)
5 Note that in XMG, it is not mandatory to define any tree structure inside SubjAgreement. We could define
independent NP and VP nodes, and associate them with variables, say n1 and n2. n1 and n2 would then
be exported and reused directly in the classes CanonicalSubject and Wh-NP-Subject, respectively.
6 Note that these clauses only consider canonical subjects to avoid having both a Wh-subject and a
Wh-object. This is not entirely satisfactory, as we would prefer to define a single abstraction over objects
(as was done for subjects) and use it wherever possible. There would then be another mechanism to
capture this exception and cause the invalid combination to fail (that is, the resulting tree description not
to have any model). Such a mechanism exists in XMG, and is called linguistic principle (see Section 5).
598
Crabbe? et al XMG: eXtensible MetaGrammar
Evans, Gazdar, and Weir (1995) argue for the necessity of using lexical rules for
grammatical description based on two arguments: (i) morphology is irregular and has
to be handled by a non-monotonic device and (ii) erasing rules such as the agentless
passive (John eats an apple / An apple is eaten ) are needed to erase an argument from
the canonical base tree. Neither of these arguments holds here, however: The first
argument because we describe tree schema hence lexical and morphological issues are
ruled out; the second because agentless passive and, more generally, argument erasing
constructions can simply be defined by an additional clause such as:
AgentlessPassiveTransitiveVerb ? Subject ? PassiveVerb (8)
To summarize, using a declarative language to specify a tree-based grammar offers
an adequate level of control on the structures being described while avoiding having
to deal with ordering and termination issues. It facilitates grammar design and mainte-
nance, by providing an abstract view on grammar trees, uniquely made of monotonic
(no information removal) combinations of tree fragments.
4. Notational Expressivity
We now focus on notational expressivity and show how XMG supports a direct
encoding of (i) distinct linguistic dimensions (here syntax, semantics and the syntax/
semantics interface) and (ii) the various types of coreferences7 that arise in the devel-
opment of tree-based grammars.
The syntax of the XMG language can be formally defined as follows.
Class ::= NameC1,...,Ckx1,...,xn ? Content (9)
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
(10)
SYN ::=
n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
(11)
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM (12)
DYN ::= ? f1 : v1,...,fn : vn ? (13)
Here and in what follows, we use the following notational conventions. Ci denote
variables over class names; xi, x, and y are variables ranging over tree nodes or feature
values; ni refer to node variables; f, fi are features and v, vi and feature values (constants
or variables); li, hj, p, and Ei are variables over semantic labels, semantic holes, predi-
cates, and predicate arguments in flat semantic formulae, respectively.8 [ ] are used to
associate a node variable with some feature constraint. ( ) are used to associate a node
variable with some property constraint (e.g., node colors, see Section 5). ci and cvi denote
7 By coreference, we mean the sharing of information between distinct elementary fragments of the
grammar specification.
8 See Gardent and Kallmeyer (2003) for a detailed introduction to flat semantics.
599
Computational Linguistics Volume 39, Number 3
a property constraint and a property constraint value, respectively. Ci.y denotes the y
variable declared in class Ci and = is unification; ? and ? denote linear precedence and
immediate dominance relations between nodes. Finally, +, ? represent the transitive and
transitive-reflexive closure of a relation, respectively.
The first two clauses of the formal definition here specify XMG classes and how they
combine. The next three clauses define the languages supported for describing three lin-
guistic dimensions, namely, syntax (SYN), semantics (SEM), and the syntax/semantics
interface (called DYN for dynamic interface). We now discuss each of these in more
detail starting bottom?up with the three linguistic dimensions and ending with the
control language that permits us to combine basic linguistic units into bigger ones.
SYN. The XMG formalism for syntax (copied here for convenience) is a tree description
logic similar to that proposed by Vijay-Shanker and Schabes (1992) and Rogers and
Vijay-Shanker (1994) to describe tree-based grammars.
SYN ::= n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
It includes tree node variables, feature names, feature values, and feature variables.
Tree node variables can be related by equality (node identification), precedence (imme-
diate or non-immediate), and dominance (immediate or non-immediate). Tree nodes
can also be labeled with feature structures of depth 2, that is, sets of feature/value
pairs where feature values are either variables, constants (e.g., syntactic category), or
non-recursive feature structure (e.g., top and bottom feature structures).
Here is a graphical illustration of how tree logic formulae can be used to describe
tree fragments: The depicted tree fragment is a model satisfying the given formula.
n1 ? n2 ? n1 ? n3 ? n2 ? n3
? n1[cat : S] ? n2(mark : subst) [cat : NP] ? n3[cat : VP]
S
NP? VP
One distinguishing feature of the XMG tree language is the introduction of node
constraints (n1(c : cv)) that generalize Muskens and Krahmer?s (1998) use of positive
and negative node markings. Concretely, node constraints are attribute-value matri-
ces, which contain information to be used when solving tree descriptions to produce
grammar trees. In other words, node constraints are used to further restrict the set
of models satisfying a tree description. As an example of node constraint, consider
node annotations in FB-LTAG (foot node, substitution node, null-adjunction, etc.). Such
annotations can be used as node constraints to allow the description solver to apply
well-formedness constraints (e.g., there is at most one foot node).
Another interesting feature of XMG concerns the inclusion of the dot operator,
which permits us to identify variables across classes in cases where name sharing cannot
be resorted to. When a variable y is declared in a class C, the latter being instantiated
within a class D, y can be accessed from D by C.y (the identifier y still being available
in D?s namespace).
600
Crabbe? et al XMG: eXtensible MetaGrammar
SEM. The semantic dimension supports a direct encoding of the flat semantic formulae
used by Gardent and Kallmeyer (2003):
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM
where li : p(E1,..., En) represents a predicate p with label li and arguments E1,..., En and
li ? hj is a scope constraint between label li and scope hj. Expressions (predicate argu-
ments Ei) can refer to semantic holes, constants (atomic values), or unification variables
(written x, y hereafter).
For instance, the following flat semantic formula can be used to underspecify the
meaning of the sentence ?Every dog chases a cat?:
l0 : ?(x, h1, h2) ? l1 ? h1 ? l1 : Dog(x) ? l2 ? h2 ? l2 : Chase(x, y)
? l3 : ?(y, h3, h4) ? l4 ? h3 ? l4 : Cat(y) ? l2 ? h4
(14)
This formula denotes the following two first-order logic formulae, thereby describing
the two possibles readings of this sentence.9
l0 : ?(x, l1, l3) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l2) ? l4 : Cat(y) (15)
l0 : ?(x, l1, l2) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l0) ? l4 : Cat(y) (16)
DYN. The DYN dimension generalizes Kinyon?s hypertag (Kinyon 2000) which is
unified whenever two tree fragments are combined. Similarly, in XMG the DYN
dimension is a feature structure that is unified whenever two XMG classes are com-
bined through inheritance or through conjunction (see the discussion on XMG control
language, subsequently).
For instance, the following constraints ensure a coreference between the index I
occurring in the syntactic dimension and the argument X occurring in the semantic
dimension (indexsubject and arg1 are feature names, and E, I, X, and V local unification
variables).
C1 ? Node [idx : I] ? ?indexsubject : I? (17)
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X? (18)
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V? (19)
More generally, the DYN dimension permits us to unify nodes and feature values
that belong to distinct classes and dimensions, and are thus often not related within
the inheritance hierarchy. As we shall see in Section 6, the DYN dimension permits
a modular account of the syntax/semantics interface in which linking constraints can
be stipulated separately and reused to specify the various diatheses.
In other words, the DYN feature structure allows us to extend the scope of some
specific variables so that they can be unified with variables (or values) introduced
in some other classes of the metagrammar. This concept of scope extension can be
compared with that of hook in Copestake, Lascarides, and Flickinger (2001).
9 For more details on the interpretation of flat semantics and on its association with a grammar of natural
language, see Gardent (2008).
601
Computational Linguistics Volume 39, Number 3
Control language. The linguistic units (named Content here) defined by the linguist can
be abstracted and combined as follows:
Class ::= NameC1,...,Ckx1,...,xn ? Content
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
The first clause states that the linguistic information encoded in Content is abstracted in
a class named Name and that this class inherits classes C1,..., Ck and exports variables
x1,..., xn. That is, XMG allows for abstraction, inheritance, and variable exports. By
default, variables (referring to nodes and feature values) are local to a class. Export
statements extend the scope of a variable to all sub-classes, however. An exported
variable can also be accessed from outside its class in case of class instantiation (using
the dot operator introduced earlier in this section). The second clause states that an
XMG class consists of a syntactic, a semantic, and a dynamic description (each of them
possibly empty), and that XMG classes can be combined by conjunction and disjunc-
tion and reused through class instantiation. The notation ?SYN, SEM, DYN? represents
simultaneous contributions (possibly empty) to all three dimensions.10
The XMG control language differs from other frameworks used to specify tree-
based grammars (Vijay-Shanker and Schabes 1992; Xia et al 1998; Candito 1999)
in two main ways. First, it supports generalized conjunctions and disjunctions of
classes. As shown in Section 3, this permits us, inter alia, a declarative treatment of
diathesis.
Second, it allows for both local and exported variables. As mentioned in Section 3, a
common way to share structure within a tree-based grammar is to define an inheritance
hierarchy of either tree fragments (Evans, Gazdar, and Weir 1995) or tree descriptions
(Vijay-Shanker and Schabes 1992; Candito 1996; Xia 2001). When considering an FB-
LTAG augmented with unification semantics, the hierarchy will additionally contain
semantic representations and/or tuples made of tree fragments and semantic represen-
tations. In all cases, the question arises of how to handle identifiers across classes and,
more specifically, how to share them.
In Candito?s (1996) approach, tree nodes are referred to using constants so that
multiple occurrences of the same node constant refer to the same node. As pointed out
in Gardent and Parmentier (2006), global names have several non-trivial shortcomings.
First, they complicate grammar writing in that the grammar writer must remember the
names used and their intended interpretation. Second, they fail to support multiple uses
of the same class within one class. For instance, in French, some verbs sub-categorize
for two prepositional phrases (PP). A natural way of deriving the tree for such verbs
would be to combine a verbal tree fragment with two instances of a PP fragment. If,
however, the nodes in the PP fragment are labeled with global names, then the two
occurrences of these nodes will be identified thereby blocking the production of the
appropriate tree.11
A less restrictive treatment of identifiers is proposed by Vijay-Shanker and Schabes
(1992), where each tree description can be associated with a set of declared node
variables and subsets of these node variables can be referred to by descriptions in the
10 Although formally precise, this notation can be cumbersome. In the interest of legibility we adopt
throughout the convention that SYN stands for ?SYN, , ?, SEM for ? , SEM, ?, and DYN for ? , , DYN?.
11 An analogous situation may arise in English with ditransitive verbs requiring two direct objects.
602
Crabbe? et al XMG: eXtensible MetaGrammar
hierarchy that inherit from the description in which these node variables were declared.
For instance, if entity A in the hierarchy declares such a special node variable X and B
inherits from A, then X can be referred to in B using the notation A.X.12
XMG generalizes Vijay-Shanker and Schabes?s (1992) approach by integrating an
export mechanism that can be used to extend the scope of a given identifier (node
or feature value variable) to classes that inherit from the exporting class. Thus if
class B inherits from class A and class A exports variable X, then X is visible in B
and its reuse forces identity. If B inherits from several classes and two (or more) of
these inherited classes export the same variable name X, then X is not directly visible
from B. It can be accessed though using the dot operator. First A is identified with a
local variable (e.g., T = A), then T.X can be used to refer to the variable X exported
by A.
To summarize, XMG allows for local variables to be exported to sub-classes as well
as for prefixed variables?that is, variables that are prefixed (using the dot operator)
with a reference to the class in which they are declared. In this way, the pitfalls in-
troduced by global names are avoided while providing enough expressivity to handle
variable coreference (via the definition of variable namespaces). Section 6 will further
illustrate the use of the various coreference devices made available by XMG showing
how they concretely facilitate grammar writing.
Let us finally illustrate variable handling with XMG in the example of Figure 2.
Recall that we define the trees of Figure 2 as the conjunctions and disjunctions of some
tree fragments of Figure 5, such as:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (20)
CanonicalSubject can be defined as a tree description formula as follows (only variables
n2 and n3 are exported):
CanonicalSubjectn2,n3 ?
n1 ? n2 ? n1[cat : S] ? n2(mark : subst) [cat : NP]?
n1 ? n3 ? n3[cat : VP] ? n2 ? n3
(21)
The class Wh-NP-Subject is defined accordingly (i.e., by means of a slightly more
complex tree description formula using the n2 and n3 variable identifiers to refer to
the nodes involved in subject agreement). The class SubjAgreement is defined slightly
differently (we do not impose any tree relation between the node concerned with
number agreement):
SubjAgreementn1,n2 ?
n1 [[top : [num : x]] [bot : [num : x]]]?
n2 [[top : [num : x]] [bot : [num : x]]]
(22)
12 In fact, the notation used by Vijay-Shanker and Schabes (1992) is attr:X with attr an attribute variable
ranging over a finite set of attributes, to indicate special node variables that scope outside their class; and
attr(A) to refer to such variables from outside the entity in which they were declared. We use a different
notation here to enforce consistency with the XMG notation.
603
Computational Linguistics Volume 39, Number 3
We can then explicitly control the way the fragments combine as follows:
Subject ?
C1 = SubjAgreementn1,n2 ?
C2 = ( CanonicalSubjectn2,n3 ? Wh-NP-Subjectn2,n3 ) ?
C1.n1 = C2.n2 ? C1.n2 = C2.n3
(23)
In this example, we see how to constrain, via variable export and unification, some
given syntactic nodes to be labeled with feature structures defined somewhere else in
the metagrammar. We use XMG?s flexible management of variable scope to deal with
node coreference. Compared with previous approaches on metagrammars such as those
of Candito (1996), Xia (2001), having the possibility of handling neither only global nor
only local variables, offers a high level of expressivity along with a precise control on
the structures being described.
5. Extensibility
A third distinguishing feature of XMG is extensibility. XMG is extensible in that
(i) dimensions can be added and (ii) each dimension can be associated with its own
interpreter. In order to support an arbitrary number of dimensions, XMG relies on a
device permitting the accumulation of an arbitrary number of types of literals, namely,
Extensible Definite Clause Grammar (EDCG) (Van Roy 1990). Once literals are accumu-
lated according to their type (i.e., each type of literals is accumulated separately), they
can be fed to dedicated interpreters. Because each of these sets of literals represents
formulas of a description language, these interpreters are solvers whose role is to
compute models satisfying the accumulated formulas.
Via this concept of separated dimensions, XMG allows us (i) to describe different
levels of language (not only syntax, but also semantics and potentially morphology,13
etc.), and (ii) to define linguistic principles (well-formedness constraints to be applied on
the structures being described). These principles depend either on the dimension (e.g.,
scope constraints in flat semantics), the target formalism (e.g. cooccurrence predicate-
arguments in FB-LTAG), or the natural language (e.g., clitic ordering in Romance lan-
guages) being described.
In what follows, we start by showing how XMG handles dimensions independently
from each other introducing EDCG (Section 5.1). We then summarize the architecture
of the XMG system (Section 5.2). We finally show how different solvers can be used
to implement various constraints on each of these dimensions (Section 5.3). In partic-
ular, we discuss three kinds of extensions implemented in XMG: extension to several
grammar formalisms, integration of explicit linguistic generalizations, and inclusion of
color-based node marking to facilitate grammar writing.
5.1 XMG: Accumulating and Interpreting an Arbitrary Number of Descriptions
Accumulating (tree) descriptions. First, let us notice that XMG is nothing other than a logic
language a` la Prolog (Duchier, Parmentier, and Petitjean 2012). More precisely, an XMG
13 Recently, XMG has been used to describe the morphology of verbs in Ikota, a Bantu language spoken in
Gabon (Duchier, Parmentier, and Petitjean 2012).
604
Crabbe? et al XMG: eXtensible MetaGrammar
specification is a collection of Horn clauses, which contribute a declarative description
of what a computational tree grammar is.
Logic Program XMG Metagrammar
Clause ::= Head ? Body
Body ::= Fact | Head |
Body ? Body |
Body ? Body
Query ::= Head
Class ::= Name ? Content
Content ::= Description | Name |
Content ? Content |
Content ? Content
Axiom ::= Name
Recall that the descriptions handled by XMG are in fact tuples of the form
?SYN, SEM, DYN?. An XMG class can thus describe, in a non-exclusive way, any of these
three levels of description. If one wants to add another level of description (i.e., another
dimension), one needs to extend the arity of this tuple. Before discussing this, let us first
see how such tuples are processed by XMG.
As mentioned earlier, XMG?s control language is comparable to Horn clauses.
A common way to represent Horn clauses is by using Definite Clause Grammar
(DCG) (Pereira and Warren 1980). Concretely, a DCG is a rewriting system (namely, a
context-free grammar), where the symbols of the rewriting rules are equipped with
pairs of unification variables (these are usually called difference list or accumulator)
(Blackburn, Bos, and Striegnitz 2006, page 100). As an illustration, consider the follow-
ing toy example.
s --> np,vp. np --> det,n.
vp --> v,np. vp --> v.
det --> [the]. det --> [a].
n --> [cat]. n --> [mouse].
v --> [eats].
The string language described by this DCG can be obtained by submitting the query
s(X,[]) where X is a unification variable to be bound with lists of facts (these being the
sentences belonging to the string language). As we can easily see, this language contains
the sentences ?a cat eats,? ?the cat eats,? ?a mouse eats,? ?the mouse eats,? ?a cat eats a
mouse,? ?a mouse eats a cat,? and so on.
Similarly, we can represent XMG classes as DCG clauses. For instance, the combina-
tions of syntactic fragments given in relations (1)?(4) can be rewritten as DCG clauses
as follows:
subject --> canonicalSubject.
subject --> whNpSubject.
activeTransitiveVerb --> subject, activeVerb, canonicalObject.
passiveTransitiveVerb --> subject, passiveVerb, canonicalByObject.
transitiveVerb --> activeTransitiveVerb.
transitiveVerb --> passiveTransitiveVerb.
Disjunctions (e.g., the subject specification) translate to multiple clauses with iden-
tical heads and conjunctions (e.g., activeTransitiveVerb) to a clause body.
In our case, the terminal symbols of the underlying DCG are not just facts, but
tuples of descriptions. In other words, the DCG clause whose head is canonicalSubject
is associated with a tuple of the following form (the dots have to be replaced with
605
Computational Linguistics Volume 39, Number 3
adequate descriptions, these can contain unification variables, whose scope is by default
local to the clause):
canonicalSubject --> [desc(syn(...),sem(...),dyn(...))].
In order to allow for an extension of XMG to an arbitrary number of dimensions,
instead of compiling XMG classes into a DCG whose accumulator stores tuples with
a fixed arity, these classes are compiled into an EDCG (Van Roy 1990). EDCG are DCG
with multiple accumulators. In XMG, each dimension is thus allocated a dedicated
accumulator in the underlying EDCG.
Note that although the content of the various dimensions is accumulated separately,
dimensions may nevertheless share information either via local unification variables
(if the XMG class defines several dimensions locally), via exported unification vari-
ables (in case of class instantiation or inheritance), or via the shared unification variables
supported by the DYN dimension.
At the end of the EDCG execution, we obtain, for each axiom of the metagrammar
(i.e., for each class name to be valuated), a list of description formulas per accumulator.
These lists are grouped together into a tuple of lists of the following form (N is the
number of dimensions, and consequently of accumulators):
desc(accu1(L1),accu2(L2), ... ,accuN(LN))
Each element (i.e., list Li) of such a tuple is a complete description of a given dimension,
where shared variables have been unified (via unification with backtracking).
Solving (tree) descriptions. As illustrated earlier, interpreting XMG?s control language in
terms of an EDCG yields tuples whose arity is the number of dimensions defined by
the linguist, that is, triples of the form ?SYN, SEM, DYN? if syntax, semantics, and the
dynamic interface are described.
For each dimension D, XMG includes a constraint solver SD that computes the set of
minimal models MD = SD(dD) satisfying the description (dD) of that dimension. In other
words, each dimension is interpreted separately by a specific solver. For instance, the
syntactic dimension is handled by a tree description solver that produces, for a given
tree description, the set of trees satisfying that description, whereas the solver for the
semantic dimension simply outputs the flat semantic representation (list of semantic
literals) built by the EDCG through accumulation.
Note that, although solvers are distinct, the models computed in each dimension
may nonetheless be coupled through shared variables. In that case, these variables can
constrain the models computed by the respective solvers. For instance, shared variables
can be used for the syntactic tree description solver to be parametrized by some value
coming from the semantic input description. Note that the output of the solving process
is a Cartesian product of the sets of minimal models of each solver. As a consequence,
the worst case complexity of metagrammar compilation is that of the various solvers
associated with relevant dimensions.
In addition to having separate solvers for each dimension, the constraint-solving
approach used in XMG permits us to modularize a given solver by combining different
principles. Each such principle enforces specific constraints on the models satisfying
the description of a given dimension. For instance, for the syntactic dimension of an
FB-LTAG, a set of principles is used to enforce that the structures produced by the
compiler are trees, and that these conform to the FB-LTAG formalism (e.g., there is no
tree having two foot nodes).
606
Crabbe? et al XMG: eXtensible MetaGrammar
5.2 Architecture
The XMG compiler14 consists of the following three modules:
 A compiler that parses XMG?s concrete syntax and compiles XMG classes
into clauses of an EDCG.
 A virtual machine (VM), which interprets EDCG. This VM performs
the accumulation of dimensions along with scope management and
identifiers resolution. This VM is basically a unification engine equipped
with backtracking, and which is extended to support EDCG. Although its
architecture is inspired by the Warren Abstract Machine (A??t-Kaci 1991),
it uses structure-sharing to represent and unify prolog terms, and, given
a query on a class, processes the conjunctions, disjunctions, inheritance,
and export statements related to that class to produce its full definition,
namely, a tree description for the SYN dimension, a flat semantic formula
for the SEM dimension, and a feature structure for the DYN dimension.
 A constraint-solving phase that produces for each dimension the minimal
models satisfying the input description as unfolded by the preceding
two steps.
As already mentioned, the first part is extensible in that new linguistic dimensions
can be added by specifying additional dedicated accumulators to the underlying EDCG.
The second part is a unification engine that interprets EDCG while performing both term
unification and polarized unification (i.e., unification of polarized feature structures, as
defined by Perrier [2000], and discussed in Section 5.3.1). This extended unification is
the reason why XMG does not merely recourse to an existing Prolog engine to process
EDCG, but relies on a specific VM instead.
The third part is completely modular in that various constraint solvers can be
plugged in depending on the requirements set by the dimensions used, and the chosen
grammatical framework. For instance, the SYN dimension is solved in terms of tree
models, and the SEM dimension is solved in terms of underspecified flat semantic
formulae (i.e., the input semantics remains untouched modulo the unification of its
shared variables).
Importantly, these additional solvers can be ?turned on/off? (via a primitive of the
XMG language) so that, for instance, the same processor can be used to compile an
XMG specification for an FB-LTAG using linguistic principles such as those defined in
the next section (i.e., clitic ordering principle) or not.
5.3 Three Extensions of XMG
We now show (i) how the modular architecture of the XMG compiler permits us
to specify grammars for several tree-based linguistic formalisms; (ii) how it can be
extended to enforce language specific constraints on the syntactic trees; and (iii) how
additional formal constraints (namely node marking) can be integrated to simplify node
identifications (and consequently grammar writing).
14 The XMG compiler is open source software released under the terms of the CeCILL GPL-compliant
licence. See http://sourcesup.renater.fr/xmg.
607
Computational Linguistics Volume 39, Number 3
Eq
Up
Down
Left
Right
Figure 6
Partition of the nodes of tree models.
5.3.1 TAG, MC-TAG, and IG: Producing Trees, Tree Sets, or Tree Descriptions. XMG in-
tegrates a generic tree solver that computes minimal tree models from tree descrip-
tion logic formulae built on the language SYN introduced in Section 4. This solver
integrates the dominance solving technique proposed by Duchier and Niehren (2000)
and can be summarized as follows. A minimal tree model is described in terms of
the relative positions of its nodes. For each node n in a minimal tree model T, the
set of all the nodes of T can be partitioned in five subsets, depending on their po-
sition relative to n. Hence, for each node variable n appearing in a tree description,
it is first associated with an integer (called node id). We then define the five sets
of node ids (i.e., sets of integers) Downn, Upn, Leftn, Rightn, and Eqn referring to the
ids of the nodes located below, above, on the left, on the right, or identified with n,
respectively (see Figure 6). Note that we require that these sets are a partition of all
node ids.
Using this set-based representation of a model, we translate each node relation
from the input formula (built on the tree description language introduced in Section 4)
into constraints on the sets of node ids that must hold in a valid model. For instance,
the sub-formula n1 ?+ n2, which states that node n1 strictly precedes node n2, is
translated into:
n1 ?+ n2 ? EqDownn1 ? Leftn2 ? EqDownn2 ? Rightn1?
Rightn2 ? Rightn1 ? Leftn1 ? Leftn2
(24)
where15 EqDownx = Eqx unionmulti Downx for x ? {n1, n2}. In other words, in a valid minimal
tree model, the set of nodes below or equal to n1 is included in the set of nodes (strictly)
on the left of n2, the set of nodes below or equal to n2 is included in the set of nodes
(strictly) on the right of n1, the set of nodes on the right of n2 is included in the set of
nodes on the right of n1, and finally the set of nodes on the left of n1 is included in the
set of nodes on the left of n2.
Once all input relations are translated into set constraints, the solver uses standard
Constraint Satisfaction techniques (e.g., a first-fail exploration of the search tree) to find a
set of consistent partitions. Finally, the nodes of the models are obtained by considering
nodes with distinct Eqn.
15 unionmulti represents disjoint union.
608
Crabbe? et al XMG: eXtensible MetaGrammar
FB-LTAG trees. To support the specification of FB-LTAG trees, the XMG compiler extends
the generic tree solver described here with a set of constraints ensuring that the trees are
well-formed TAG trees. In effect, these constraints require the trees to be linear ordered
trees with appropriate decorations. Each node must be labeled with a syntactic category.
Leaf nodes are either terminal, foot, or substitution nodes. There is at most one foot
node per tree and the category of the foot node must be identical to that of the root
node. Finally, each tree must have at least one leaf node that is an anchor.
MCTAG tree sets. Where FB-LTAG consists of trees, MC-TAG (Weir 1988) consists of sets
of trees. To support the specification of MC-TAG, the sole extension needed concerns
node variables that are not dominated by any other node variable in the tree description.
Whereas for FB-LTAG, these are taken to denote either the same root node or nodes that
are connected to some other node (i.e., uniqueness of the root), for MC-TAG they can
be treated as distinct nodes, thereby allowing for models that are sets of trees rather
than trees (Parmentier et al 2007). In other words, the only modification brought to the
tree description solver is that, in MC-TAG mode, it does not enforce the uniqueness of
a root node in a model.
IG polarized tree descriptions. IG (Perrier 2000) consist of tree descriptions whose node
variables are labeled with polarized feature structures. A polarized feature structure is
a set of polarized feature triples (f, p, v) where f and v are standard features and feature
values, respectively, and p is a polarity value in {?,?,=,?}. Polarities are used to
guide parsing in that a valid derivation structure must neutralize polarities.
To support an XMG encoding of IG, two extensions are introduced, namely, (i) the
ability to output tree descriptions rather than trees, and (ii) the ability to write polarized
feature structures. The first extension is trivially realized by specifying a description
solver that ensures that any output description has at least one tree model. For the
second point, the SYN language is extended to define polarized feature structures and
the unification engine to support unification of polarized features (for instance, a ?
feature will unify with a neutral (=) feature to yield a ? polarized feature value triple).
5.3.2 Adding Specific Linguistic Constraints: The Case of Clitics. XMG can be extended
to support specific constraints on tree descriptions (e.g., constraints on node linear
order), which make it possible to describe linguistic-dependent phenomena, such as,
for instance, clitic ordering in French, at a meta-level (i.e., within the metagrammar).
According to Perlmutter (1970), clitics are subject to two hard constraints. First,
they appear in front of the verb in a fixed order according to their rank (Exam-
ples 25a and 25b).16 Second, two different clitics in front of the verb cannot have the
same rank (Example 25c).
(25) a. Jean le3 lui4 donne.
?John gives it to him.?
b. *Jean lui4 le3 donne.
*?John gives to him it.?
c. *Jean le3 la3 donne.
*?John gives it it.?
16 In (Examples 25a?c), the numbers on the clitics indicate their rank.
609
Computational Linguistics Volume 39, Number 3
S
N? ?+ V?
?
V?
Cl?3 ?+ V
?
V?
Cl?4 ?+ V
?
S
V?
V
?
S
N? V?
Cl?3 Cl?4 V
S
N? V?
Cl?4 Cl?3 V
Figure 7
Clitic ordering in French.
To support a direct encoding of Perlmutter?s observation, XMG includes both a
node uniqueness principle and a node ordering principle. The latter allows us to label
nodes with some property (let us call it rank) whose value is an integer (for instance,
one can define a node as n1(rank : 2)[cat : Cl]). When solving tree descriptions, XMG
further requires that in a valid tree model, (i) there are no two nodes with the same
rank and (ii) sibling nodes labeled with a rank are linearly ordered according to their
rank.
Accordingly, in the French grammar of Crabbe? (2005), each node labeled with a clitic
category is also labeled with a numerical node property representing its rank.17 XMG
ordering principle then ensures that the ill-formed tree crossed out in Figure 7 is not
produced. Note that in Figure 7, every type of clitic is defined locally (i.e., in a separate
class), and that the interactions between these local definitions are handled by XMG
using this rank principle, to produce only one valid description (pictured to the right of
the arrow).
That is, XMG ordering constraints permit a simple, declarative encoding of the
interaction between clitics. This again contrasts with systems based on lexical rules. As
noted by Perlmutter (1970), if clitics are assumed to be moved by transformations, then
the order in which lexical rules apply this movement must be specified.
To implement the uniqueness principle, one needs to express the fact that in a valid
model ?, there is only one node having a given property p (i.e., a parameter of the
constraint, here the value of the rank node property). This can be done by introducing,
for each node n of the description, a Boolean variable pn indicating whether the node
denoting n in the model has this property or not (i.e., are there two nodes of identical
rank?). Then, if we call V?p the set of integers referring to nodes having the property p in
a model, we have: pn ? (Eqn ? V
?
p ) = ?. Finally, if we represent pn being true with 1 and
pn being false with 0,18 and we sum pn for each n in the model, we have that in a valid
model this sum is strictly lower than 2:
?
n?? pn < 2.
To implement the ordering principle, one needs to express the fact that in a valid
model ?, two sibling nodes n1 and n2 having a given property p of type integer and
of values p1 and p2, respectively, are such that the linear precedence between these
nodes conform to the natural order between p1 and p2. This can be done by first
introducing, for each pair of nodes n, m of the description, a Boolean variable bn,m
indicating whether they have the same ancestors: bn,m ? (Upn ? Upm) = (Upn ? Upm).
For each pair of nodes that do so, we check whether they both have the property p,
17 Recall that node properties are features whose values are used by the tree description solver in order to
restrict the set of valid models. These properties may not appear in the trees produced from the input
metagrammar. For instance, the rank property is not part of the FB-LTAG formalism, and thus does not
appear in the FB-LTAG elementary trees produced by XMG.
18 These integer representations are usually called reified constraints.
610
Crabbe? et al XMG: eXtensible MetaGrammar
and if this is the case, we add to the input description a strict precedence constraint on
these nodes according to their respective values of the property p:19
bn,m ? (pn < pm) ? n ?+ m (26)
bn,m ? (pm < pn) ? m ?+ n (27)
5.3.3 Adding Color Constraints to Facilitate Grammar Writing. To further ease grammar
development, XMG supports a node coloring mechanism that permits nameless node
identification (Crabbe? and Duchier 2004), reminiscent of the polarity-based node iden-
tification first proposed by Muskens and Krahmer (1998) and later used by Duchier
and Thater (1999) and Perrier (2000). Such a mechanism offers an alternative to explicit
node identification using equations between node variables. The idea is to label node
variables with a color property, whose value (either red, black, or white) can trigger
node identifications.
This mechanism is another parameter of the tree solver. When in use, the valid
tree models must satisfy some color constraints, namely, they must only have red or
black nodes (no remaining white nodes; these have to be identified with some black
nodes). As shown in the following table, node identification must observe the following
constraints: A white node must be identified with a black node; a red node cannot be
identified with any other node; and a black node may be identified with one or more
white nodes.20
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
We now briefly describe how the constraint solver sketched in Section 5.3.1 was
extended to support colors. As mentioned previously, in valid models all white nodes
are identified with a black node (at most one black node per white node). Consequently,
there is a bijection from the red and black nodes of the tree description to the nodes of
the model. In order to take this bijection into account, we add a node variable RBn to
the five sets already associated with a node variable n from Section 5.1. RBn denotes
either n if n is a black or red node, or the black node identified with n if n is a white
node. Note that all the node variables must be colored: the set of node variables in a
tree description can then be partitioned into three sets: Red, Black, and White. Basically,
we know that, for all nodes n, RBn ? Eqn (this is what the bijection is about). Again
we translate color information into constraints on node sets (these constraints help the
generic tree solver by reducing the ambiguity for the Eqn sets):
n ? Red ? (n = RBn) ? (Eqn = {n}) (28)
n ? Black ? (n = RBn) ? (Eqn\{n} ? White) (29)
n ? White ? (RBn ? Black) ? (Eqn ? Black = {RBn}) (30)
19 In fact, rather than adding strict precedence constraints to the tree description, we directly add to the
solver their equivalent set constraints on Eq, Up, Left, Right, Down, introduced earlier.
20 In other words, node colors can be seen as information on node saturation.
611
Computational Linguistics Volume 39, Number 3
Node coloring offers an alternative to complex namespace management. The main
advantage of this particular identification mechanism is its economy: Not only is there
no longer any need to remember node identifiers, there is in fact no need to choose a
name for node variables.
It is worth stressing that the XMG node identification process is reduced to a
constraint-solving problem and so it is not a sequential process. Thus the criticisms
leveled by Cohen-Sygal and Wintner (2007, 2009) against non-associative constraints
on node unification do not apply.
Briefly, in their work, Cohen-Sygal and Wintner (2007, 2009) showed that any
polarity-based tree description formalism is not associative. In other words, when
describing trees in terms of combinations of polarized structures, the order in which
the structures are combined matters (i.e., the output structures depend on the combi-
nation order). This feature makes such formalisms not appropriate for a modular and
collaborative grammar engineering, such as that of Cohen-Sygal and Wintner (2011) for
Unification Grammar.
In the XMG case, when using node colors, the tree description solver does not
rely on any specific fragment combination order. It computes all possible combination
orders. In this context, the grammar designer cannot think in terms of sequences of node
identifications. This would lead to tree overgeneration.
Again, it is important to remember that tree solving computes any valid tree model,
independently of any specific sequence of node identifications (all valid node identifica-
tions are computed). In this context, non-associativity of color-based node identification
is not an issue, but rather a feature, as it allows for a compact description of a large
number of node identifications (and thus of tree structures).
6. Writing Grammars with XMG
In this section, we first provide a detailed example showing how XMG can be used to
specify the verbal trees of a large FB-LTAG for French extended with unification-based
semantics. We then give a brief description of several large- and middle-scale grammars
that were implemented using XMG.
6.1 SEMTAG: A large FB-LTAG for French Covering Syntax and Semantics
We now outline the XMG specification for the verbal trees of SEMTAG, a large FB-LTAG
for French. This specification further illustrates how the various features of XMG (e.g.,
combined use of disjunction and conjunction, node colors) permit us to specify compact
and declarative grammar descriptions. We first discuss the syntactic dimension (SYN).
We then go on to show how the semantic dimension (SEM) and the syntax/semantic
interface (DYN) are specified.
6.1.1 The Syntactic Dimension. The methodology used to implement the verbal fragment
of SEMTAG can be summarized as follows. First, tree fragments are defined that rep-
resent either a possible realization of a verb argument or a possible realization of the
verb. The verbal elementary TAG trees of SEMTAG are then defined by appropriately
combining these tree fragments.
To maximize structure sharing, we work with four levels of abstraction. First, basic
tree fragments describing verb or verb argument realizations are defined. Second, gram-
matical functions are defined as disjunctions of argument realizations. Third, verbal
diathesis alternatives are defined as conjunctions of verb realizations and grammatical
612
Crabbe? et al XMG: eXtensible MetaGrammar
CanonSubj ?
S?W
N??R V?W CanonObj ?
S?W
V?W N??R
CanonIndirObj ?
S?W
V?W PP?R
P?R
a`?R
N??R
CanonByObj ?
S?W
V?W PP?R
P?R
par?R
N??R
RelatSubj ?
N?R
N?R S?W
N??R V?W WhObj ?
S?R
N??R S?W
V?W
WhByObj ?
S?R
PP?R
P?R
par?R
N??R
S?W
WhIndirObj ?
S?R
PP?R
P?R
a`?R
N??R
S?W
ActiveVerbForm?
S?B
V?B PassiveVerbForm?
S?B
V?B
V??B V?B
Figure 8
Elementary tree fragments used as building blocks of the grammar (nodes are colored to control
their identification when blocks are combined).
functions. Fourth, diathesis alternatives are gathered into tree families. In the next
paragraphs, we explain each of these levels in more detail.
Tree fragments. Tree fragments are the basic building blocks used to define SEMTAG.
These are the units that are shared and reused in the definition of many elementary
trees. For instance, the fragment for a canonical subject will be used by all FB-LTAG
elementary trees involving a canonical subject.
As mentioned earlier, to specify the verbal elementary trees of SEMTAG, we begin
by defining tree fragments which describe the possible syntactic realizations of the verb
arguments and of the verb itself. Figure 8 provides some illustrative examples of these
fragments. Here and in the following, we omit the feature structures decorating the trees
to facilitate reading.21
To further factorize information and facilitate grammar maintenance, the basic tree
fragments are organized in an inheritance hierarchy.22 Figure 9 shows a partial view of
21 See Crabbe? (2005) for a complete description of SEMTAG tree fragments, including feature structures.
22 Recall from Section 4 that inheritance is used to share namespaces. Thus, (node or feature) variables
introduced in a given class C can be directly reused in the sub-classes of C.
613
Computational Linguistics Volume 39, Number 3
VerbalArgument
CanonSubj CanonCompl
CanonObj CanPP
CanonIndirObj CanonByObj
Wh
WhObj WhPP
WhIndirObj WhByObj
RelatSubj
Figure 9
Organization of elementary fragments in an inheritance hierarchy.
this hierarchy illustrating how the tree fragments for argument realization depicted in
Figure 8 are organized to maximize the sharing of common information. The hierarchy
classifies the verbal arguments depicted in Figure 8 into four categories:
1. The canonical subject is a noun realized in front of the verb.
2. Canonical complements occur after the verb. The canonical object is a
noun phrase whereas prepositional complements are introduced by
specific prepositions, namely, a` for the canonical indirect object and
par for the canonical by object.
3. Wh-arguments (or questioned arguments) occur in front of a sentence
headed by a verb. A Wh-object is an extracted noun whereas questioned
prepositional objects are extracted prepositional phrases that are
introduced by a specific preposition.
4. Finally, the relativized subject is a relative pronoun realized in front
of the sentence. Extracted subjects in French cannot be realized at an
unbounded distance from the predicate.
Syntactic functions. The second level of abstraction uses syntactic function names such
as Subject and Object to group together alternative ways in which a given syntactic
function can be realized. For instance, if we make the simplifying assumption that the
possible argument realizations are limited to those given in Figure 8, the Subject, Object,
ByObject, and IndirectObject classes would be defined as follows.23
Subject ? CanonSubj ? RelatSubj (31)
Object ? CanonObj ? WhObj (32)
ByObject ? CanonByObj ? WhByObj (33)
IndirectObject ? CanonIndirObj ? WhIndirObj (34)
That is, we define the Subject class as an abstraction for talking about the set of tree
fragments that represent the possible realizations of a subject argument?namely, in
23 Note that, when these abstractions will be combined to describe for instance transitive verbs, the
combination of WhObj with WhByObj will be ruled out by using a uniqueness principle such as
introduced in Section 5.
614
Crabbe? et al XMG: eXtensible MetaGrammar
our restricted example, canonical and relativized subject. Thus, the simplified Subject
class defined in Equation (31) characterizes contexts such as the following:
(35) a. Jean mange. (canonical subject)
?John eats.?
b. Le garc?on qui mange (relativized subject)
?The boy who eats?
Similarly, the IndirectObject class abstracts over the realization of an argument intro-
duced by the preposition a` to the right of the verb (CanonIndirObj) or realized in
extracted position (possibly realized at an unbounded distance from the predicate) as
illustrated by the following examples:
(36) a. Jean parle a` Marie. (canonical indirect object)
?John talks to Mary.?
b. A` qui Jean parle-t-il ? (wh indirect object)
?To whom is John talking ??
c. A` qui Pierre croit-il que Jean parle ? (wh indirect object)
?To whom Peter thinks that John talks ??
This way of grouping tree fragments is reminiscent of the informal classification of
French syntactic functions presented by Iordanskaja and Mel?c?uk (2009) whereby each
syntactic function is associated with a set of possible syntactic constructions.
Diathesis alternations. In this third level, we take advantage of the abstractions defined
in the previous level to represent diathesis alternations. Again, we are interested here
in describing alternatives. Diathesis alternations are those alternations of mapping
between arguments and syntactic functions such as for instance the active/passive
alternation. In a diathesis alternation, the actual form of the verb constrains the way
predicate arguments are realized in syntax. Thus, in the following example, it is con-
sidered that both Examples (37a) and (37b) are alternative realizations of a predicate
argument structure such as send(John, a letter).
(37) a. Jean envoie une lettre.
?John sends a letter.?
b. Une lettre est envoye?e par Jean.
?A letter is sent by John.?
The active/passive diathesis alternation captures the fact that if the verb is in the
active form, its two arguments are realized by a subject and an object whereas if the
verb is in the passive form, then the arguments consist of a subject and a by-object.
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
(38)
Finally a traditional case of ?erasing,?24 such as the agentless passive (or passive
without agent) can be expressed in our language by adding an additional alternative
24 It is often argued that a language of grammatical representation must be equipped with an ?erasing
device? like lexical rules because of phenomena such as the passive without agent. In this framework it
turns out that this kind of device is not needed because we do not grant any special status to base trees.
615
Computational Linguistics Volume 39, Number 3
where the by-object or agentive complement is not expressed. Thus Equation (39) is an
augmentation of (38) where we have added the agentless passive alternative (indicated
in boldface).
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
? (Subject ? PassiveVerbForm)
(39)
This methodology can be further augmented to implement an actual linking in the
manner of Bresnan and Zaenen (1990). For the so-called erasing cases, one can map the
?erased? predicative argument to an empty realization in syntax. We refer the reader to
Crabbe? (2005) for further details.
Tree families. Finally, tree families are defined?that is, sets of trees capturing alternative
realizations of a given verb type (i.e., sub-categorization frame). Continuing with the
simplified example presented so far, we can for instance define the tree family for
verbs taking a nominal subject, a nominal object, and an indirect nominal object (i.e.,
ditransitive verbs) as follows:
DitransitiveFamily ? TransitiveDiathesis ? IndirectObject (40)
The trees generated for such a family will, among others, handle the following
contexts:25
(41) a. Jean offre des fleurs a` Marie.
?John offers flowers to Mary.?
b. A` quelle fille Jean offre-t-il des fleurs ?
?To which girl does John offer flowers ??
c. Le garc?on qui offre des fleurs a` Marie.
?The boy who offers flowers to Mary.?
d. Quelles fleurs le garc?on offre-t-il a` Marie ?
?Which flowers does the boy offer to Mary ??
e. Les fleurs sont offertes par Jean a` Marie.
?The flowers are offered by John to Mary.?
f. Par quel garc?on les fleurs sont-elles offertes a` Marie ?
?By which boy are the flowers offered to Mary ??
It is straightforward to extend the grammar with new families. Thus, for instance,
Equation (42) shows how to define the transitive family (for verbs taking a nominal
subject and a nominal object) and Equation (43), the intransitive one (alternatives of a
verb sub-categorizing for a nominal subject).
TransitiveFamily ? TransitiveDiathesis (42)
IntransitiveFamily ? Subject ? ActiveVerbForm (43)
25 Note that number and gender agreements are dealt with using coreferences between features labeling
syntactic nodes, see Crabbe? (2005).
616
Crabbe? et al XMG: eXtensible MetaGrammar
Similarly, tree families for non-verbal predicates (adjectives, nouns) can be defined
using the abstraction over grammatical functions defined for verbs. For instance, the ex-
amples in (44a?44b) can be captured using the adjectival trees defined in Equations (46)
and (47), respectively, where Subject extends the definition of subject given above with a
Wh-subject, PredAdj combines a subject tree fragment with a tree fragment describing a
predicative adjective, and PredAdjAObj extends a PredAdj tree fragment with a canonical
a`-object.
(44) a. Jean est attentif. Qui est attentif ? L?homme qui est attentif
?John is mindful. Who is mindful ? The man who is mindful?
b. Jean est attentif a` Marie. Qui est attentif a` Marie ? L?homme qui est attentif a`
Marie
?John is mindful of Mary. Who is mindful of Mary ? The man who is mindful
of Mary?
Subject ? CanonSubj ? RelatSubj ? WhSubj (45)
PredAdj ? Subject ? AdjectivalForm (46)
PredAdjAObj ? PredAdj ? CanonAObj (47)
6.1.2 The Semantic Dimension and the Syntax/Semantic Interface. We now show how to
extend the XMG specification presented in the previous section to integrate a
unification-based compositional semantics. Three main changes need to be carried out:
1. Each elementary tree must be associated with a semantic formula. This is
done using the SEM dimension.
2. The nodes of elementary trees must be labeled with the appropriate
semantic indices. This involves introducing the correct attribute-value pair
in the correct feature structure (top or bottom) on the appropriate node.
3. Syntax and semantics need to be synchronized?that is, variable sharing
between semantic formulae and tree indices need to be enforced. To this
end we use the DYN dimension.
Informing the semantic dimension. To associate each elementary tree with a formula rep-
resenting the meaning of the words potentially anchoring that tree, we use the SEM
dimension to specify a semantic schema. For instance, the TransitiveFamily class defined
in Equation (42) for verbs taking two nominal arguments is extended as follows:
TransitiveFamily ? TransitiveDiathesis ? BinaryRel (48)
where TransitiveDiathesis is the XMG class defined in Equation (39) to describe the set of
trees associated with transitive verbs and BinaryRel the class describing the following
semantic schema:
L : P(E) ? L : Theta1(E, X) ? L : Theta2(E, Y) (49)
In this semantic schema, P, Theta1, and Theta2 are unification variables that become
ground when the tree is anchored with a specific word. For instance, P, Theta1, and
Theta2 are instantiated to eat, agent, and patient, respectively, when the anchor is ate (these
617
Computational Linguistics Volume 39, Number 3
pieces of information?predicate, thematic roles?are associated with lemmas, located
in the syntactic lexicon, and unified with adequate semantic variables via anchoring
equations). Further, X, Y, E, L are unification variables representing semantic arguments.
As illustrated in Figure 3, these become ground during (or after) derivation as a side
effect of the substitutions and adjunctions taking place when trees are combined. It
is worth noting that by combining semantic schemas with diathesis classes, one such
specification assigns the specified semantic schema to many trees, namely, all the trees
described by the corresponding diathesis class. In this way, the assignment of semantic
formulae to trees is relatively economical. Indeed in SEMTAG, roughly 6,000 trees are
assigned a semantic schema using a total of 75 schema calls.
Co-indexing trees and formulae indices. Assuming that tree nodes are appropriately deco-
rated with semantic indices by the specification scheme described in the next paragraph,
we now show how to enforce the correct mapping between syntactic and semantic
arguments. This is done in two steps.
First, we define a set of interface constraints of the form ?indexF : V, argi : V? which
are used to enforce the identification of the semantic index (indexF) labeling a given tree
node with grammatical function F (e.g., F := subject) with the index (argi) representing
the i-th argument in a semantic schema. For instance, the following constraints ensure
a subject/arg1 mapping, that is, a coreference between the index labeling a subject node
and the index representing the first argument of a semantic schema:
C1 ? Node [idx : I] ? ?indexsubject : I?
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X?
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V?
(50)
Given such interface constraints, we refine the diathesis definitions so as to ensure the
correct bindings. For instance, the specification in Equation (38) is modified to:
TransitiveDiathesis ? TransitiveActive ? TransitivePassive
TransitiveActive ? (SubjectArg1 ? ObjectArg2?
Subject ? ActiveVerbForm ? Object)
(51)
and the passive diathesis is specified as:
TransitivePassive ? (SubjectArg2 ? ByObjectArg1?
Subject ? PassiveVerbForm ? ByObject)
(52)
Labeling tree nodes with semantic indices. This scheme relies on the assumption that tree
nodes are appropriately labeled with semantic indices (e.g., the subject node must be
labeled with a semantic index) and that these indices are appropriately named (arg1
must denote the parameter representing the first argument of a binary relation and
indexsubject the value of the index feature on a subject node). As suggested by Gardent
(2007), a complete semantic labeling of a TAG with the semantic features necessary
618
Crabbe? et al XMG: eXtensible MetaGrammar
to enrich this TAG with the unification-based compositional semantics sketched in the
previous section can be obtained by applying the following labeling principles:26
Argument labeling: In trees associated with semantic functors, each argument node
is labeled with a semantic index27 named after the grammatical function of the
argument node (e.g., indexsubject for a subject node).
Controller/Controllee: In trees associated with control verbs, the semantic index of the
controller is identified with the value of the controlled index occurring on the
sentential argument node.
Anchor projection: The anchor node projects its index up to its maximal projection.
Foot projection: A foot node projects its index up to the root.28
As we shall now see, XMG permits a fairly direct encoding of these principles.
The Argument Labeling principle states that, in the tree associated with a syntactic
functor (e.g., a verb), each node representing a syntactic argument (e.g., the subject
node) should be labeled with a semantic index named after the grammatical function of
that node (e.g., indexsubject).29
To specify this labeling, we define for each grammatical function Function ?
{Subject, Object, ByObject, IndirectObject, . . . }, a semantic class FunctionSem which as-
sociates with an (exported) node variable called FunctionNode the feature value pair
[index : I] and a DYN constraint of the form ?indexFunction : I?. For instance, the class
SubjectSem associates the node SubjectNode with the feature value pair [index : I] and
the DYN constraint ?indexsubject : I?.
SubjectSem ? SubjectNode [index : I] ? ?indexsubject : I? (53)
Additionally, in the tree fragments describing the possible realizations of the grammat-
ical functions, the (exported) variable denoting the argument node is systematically
named ArgNode.
Finally, we modify the specification of the realizations of the grammatical functions
to import the appropriate semantic class and identify ArgNode and FunctionNode. For
instance, the Subject specification given above is changed to:
Subject ? SubjectSem ? ArgNode = SubjectNode ?
(CanonSubj ? RelatSubj ? WhSubj)
(54)
26 The principles required to handle quantification are omitted. We refer the reader to Gardent (2007) for a
more extensive presentation of how semantics is implemented using XMG.
27 For simplicity, we only mention indices. To be complete, however, labels should also be used.
28 The foot projection principle only applies to foot nodes that are not argument nodes (i.e., to modifiee
nodes).
29 In other words, this argument labeling principle defines an explicit and normalized reference to any
realization of a semantic argument. Following FB-LTAG predicate?argument co-occurrence principle
(Abeille?, Candito, and Kinyon 1999), we know that any elementary tree includes a leaf node for each
realized semantic argument of its anchor. This principle thus holds in any FB-LTAG. Its implementation,
however, is closely related to the architecture of the metagrammar; here we benefit from the fact that
verbal arguments are described in dedicated classes to reach a high degree of factorization.
619
Computational Linguistics Volume 39, Number 3
E3
E2
E2
E1
E2
E1
E1
E
E1
E
E1
E
? E? ? E? ? E?
Depth 3 Depth 2 Depth 1
SE2E1
VPE1E
?VE?
ActiveVerbForm
Figure 10
Anchor/Foot projection.
As a result, all ArgNode nodes in the tree descriptions associated with a subject realiza-
tion are labeled with an index feature I whose global name is indexsubject.
Value sharing between the semantic index of the controller (e.g., the subject of
the control verb) and that of the controllee (e.g., the empty subject of the infinitival
complement) is enforced using linking constraints between the semantic index labeling
the controller node and that labeling the sentential argument node of the control verb.
Control verb definitions then import the appropriate (object or subject control) linking
constraint.
The anchor (respectively, foot) projection principle stipulates the projection of
semantic indices from the anchor (respectively, foot) node up to the maximal projection
(respectively, root). Concretely, this means that the top and bottom features of the nodes
located on this path between the anchor (respectively, foot) and the maximal projection
(respectively, root) all include an index feature whose value is shared between adjacent
nodes (see variables Ei in Figure 10).30 Once the top and bottom structures are unified,
so are the semantic indices along this path (modulo expected adjunctions realized on
the projection).
To implement these principles, we define a set of anchor projection classes
{Depth1, Depth2, Depth3} as illustrated in Figure 10. We then ?glue? these projection
skeletons onto the relevant syntactic trees by importing the skeletons in the syntactic
tree description and explicitly identifying the anchor node of the semantic projection
classes with the anchor or foot node of these syntactic tree descriptions. Because the
models must be trees, the nodes dominating the anchor node of the projection class
will deterministically be identified with those dominating the anchor or foot node of
the trees being combined with. For instance, for verbs, the class specifying the verbal
spine (e.g., ActiveVerbForm, see Figure 10) equates the anchor node of the verbal spine
with that of the projection skeleton. As a result, the verb projects its index up to
the root.
6.1.3 Some Figures About SEMTAG. As mentioned previously, SEMTAG is a large FB-LTAG
for French equipped with semantics (Gardent 2008); it extends the purely syntactic
FTAG of Crabbe? (2005) with a unification based compositional semantics as described
by Gardent and Kallmeyer (2003).31 The syntactic FTAG in essence implements Abeille??s
(2002) proposal for an FB-LTAG-based modeling of French syntax. FTAG contains
around 6,000 elementary trees built from 293 XMG classes and covers some 40 basic
30 For sake of brevity, we write E2E1 for [bot : [index : E1] top : [index : E2]]. ? ? refers to the anchor / foot.31 FTAG and SEMTAG are freely available under the terms of the GPL-compliant CeCILL license, the former
at https://sourcesup.renater.fr/scm/viewvc.php/trunk/METAGRAMMARS/FrenchTAG/?root=xmg, and
the latter on request.
620
Crabbe? et al XMG: eXtensible MetaGrammar
verbal sub-categorization frames. For each of these frames, FTAG defines a set of
argument alternations (active, passive, middle, neuter, reflexivization, impersonal,
passive impersonal) and of argument realizations (cliticization, extraction, omission,
permutations, etc.) possible for this frame. Predicative (adjectival, nominal, and
prepositional) and light verb constructions are also covered as well as some common
sub-categorizing noun and adjective constructions. Basic descriptions are provided for
the remaining constructions namely, adverbs, determiners, and prepositions.
FTAG and SEMTAG were both evaluated on the Test Suite for Natural Language Pro-
cessing (TSNLP) (Lehmann et al 1996), using a lexicon designed specifically on the test
suite, hence reducing lexical ambiguity (Crabbe? 2005; Parmentier 2007). This test suite
focuses on difficult syntactical phenomena, providing grammatical and ungrammatical
sentences. These competence grammars accept 76% of the grammatical items, reject 83%
of the ungrammatical items, and have an average ambiguity of 1.64 parses per sentence.
To give an idea of the compilation time, under architectures made of a 2-Ghz processor
with 1 Gb of RAM, it takes XMG 10 minutes to compile the whole SEMTAG (recall that
there is no semantic description solving, hence the compilation times between FTAG
and SEMTAG do not differ).32
Note that SEMTAG can be used for assigning semantic representations to sentences
when combined with an FB-LTAG parser and a semantic construction module as de-
scribed by Gardent and Parmentier (2005, 2007).33 Conversely, it can be used to verbalize
the meaning denoted by a given semantic representation when coupled with the GenI
surface realizer described by Gardent and Kow (2007).
6.2 Other Grammars Designed with XMG
XMG has been used mainly to design FB-LTAG and IG for French or English. More
recently, it has also been used to design a FB-LTAG for Vietnamese and a TreeTuple
MC-TAG for German. We now briefly describe each of these resources.
SemXTAG. The English grammar, SEMXTAG (Alahverdzhieva 2008), reimplements the
FB-LTAG developed for English at the University of Pennsylvania (XTAG Research
Group 2001) and extends it with a unification-based semantics. It contains 1,017 trees
and covers the syntactic fragment of XTAG, namely, auxiliaries, copula, raising and
small clause constructions, topicalization, relative clauses, infinitives, gerunds, pas-
sives, adjuncts, ditransitives (and datives), ergatives, it-clefts, wh-clefts, PRO con-
structions, noun?noun modification, extraposition, determiner sequences, genitives,
negation, noun?verb contractions, sentential adjuncts, imperatives, and resultatives.
The grammar was tested on a handbuilt test-suite of 998 sentences illustrating the
various syntactic constructions meant to be covered by the grammar. All sentences in
the test suite can be parsed using the grammar.
FrenchIG. The extended XMG framework was used to design a core IG for French
consisting of 2,059 tree descriptions compiled out of 448 classes (Perrier 2007). The
resulting grammar is lexicalized, and its coverage was evaluated using the previously
mentioned TSNLP. The French IG accepts 88% of the grammatical sentences and rejects
32 As a comparison, about one hour was needed by Candito?s (1999) compiler to produce a French FB-LTAG
containing about 1,000 tree schemas.
33 As an alternative way to parse FB-LTAG grammars equipped with flat semantics such as those produced
by XMG, one can use the Tu?bingen Linguistic Parsing Architecture (TuLiPA) (Kallmeyer et al 2010).
621
Computational Linguistics Volume 39, Number 3
85% of the ungrammatical sentences, although the current version of the French IG
does not yet cover all the syntactic phenomena presented in the test suite (for example,
causative and superlative constructions).
Vietnamese TAG. The XMG language was used by Le Hong, N?Guyen, and Roussanaly
(2008) to produce a core FB-LTAG for Vietnamese. Their work is rather a proof of con-
cept than a large-scale implementation. They focused on Vietnamese?s categorization
frames, and were able to produce a TAG covering the following frames: intransitive
(tree family N0V), transitive with a nominal complement (N0VN1), transitive with a
clausal complement (N0VS1), transitive with modal complement (N0V0V1), ditransi-
tive (N0VN1N2), ditransitive with a preposition (N0VN1ON2), ditransitive with a ver-
bal complement (N0V0N1V1), ditransitive with an adjectival complement (N0VN1A),
movement verbs with a nominal complement (N0V0V1N1), movement verbs with an
adjectival complement (N0V0AV1), and movement ditransitive (N0V0N1V1N2).
GerTT. Another XMG-based grammar corresponds to the German MC-TAG of
Kallmeyer et al (2008). This grammar, called GerTT, is in fact an MC-TAG with
Tree Tuples (Lichte 2007). This variant of MCTAG has been designed to model free
word order phenomena. This is done by imposing node sharing constraints on MCTAG
derivations (Kallmeyer 2005). GerTT covers phenomena such as scrambling, coherent
constructions, relative clauses, embedded questions, copula verbs, complementized
sentences, verbs with various sub-categorization frames, nouns, prepositions, determin-
ers, adjectives, and partly includes semantics. It is made of 103 tree tuples, compiled
from 109 classes.
7. Related Work
We now compare XMG with existing environments for designing tree-based grammars
and briefly report on the grammars designed with these systems.
7.1 Environments for Designing Tree-Based Grammars
Candito?s Metagrammar Compiler. The concept of metagrammar was introduced by
Candito (1996). In her paper, Candito presented a compiler for abstract specifications
of FB-LTAG trees (the so-called metagrammars). Such specifications are based on three
dimensions, each of them being encoded in a separate inheritance hierarchy of linguistic
descriptions. Dimension 1 describes canonical sub-categorization frames (e.g., transitive),
the Dimension 2 describes redistributions of syntactic functions (e.g., active to passive),
and Dimension 3 the tree descriptions corresponding to the realizations of the syntactic
functions defined in Dimension 2. This three-dimensional metagrammatical description
is then processed by a compiler to compute FB-LTAG tree schemas. In essence, these
tree schemas are produced by associating a canonical sub-categorization frame (Dimen-
sion 1) with a compatible redistribution schema (Dimension 2), and with exactly one
function realization (Dimension 3) for each function required by the sub-categorization
frame.
Candito?s (1996, 1999) approach improves on previous proposals by Vijay-Shanker
and Schabes (1992) and Evans, Gazdar, and Weir (1995) in that it provides a linguistically
principled basis for structuring the inheritance hierarchy. As shown in Section 6.1,
622
Crabbe? et al XMG: eXtensible MetaGrammar
the XMG definition of SEMTAG uses similar principles. Candito?s approach differs,
however, from the XMG account in several important ways:
 Much of the linguistic knowledge used to determine which classes to
combine is hard-coded in the compiler (unlike in XMG, there is no explicit
control on class combinations). In other words, there is no clear separation
between the linguistic knowledge needed to specify a high-level FB-LTAG
description and the algorithm used to compile an actual FB-LTAG from
this description. This makes grammar extension and maintenance by
linguists extremely difficult.
 As in Vijay-Shanker and Schabes (1992) Evans, Gazdar, and Weir (1995),
the linguistic description is non-monotonic in that some erasing classes
are used to remove information introduced by other dimensions
(e.g., agentless passive).
 The approach fails to provide an easy means to state exceptions. These
are usually encoded in the compiling algorithm.
 The tree description language used to specify classes in Dimension 3
relies on global node variables. Thus, two variables with identical names
introduced in different classes are expected to refer to the same tree node.
As argued in Section 4, this makes it hard to design large-scale
metagrammars.
The LexOrg system. An approach similar to Candito?s was presented by Xia et al
(1998), Xia (2001), and Xia, Palmer, and Vijay-Shanker (2005, 2010). As in Candito?s
approach, a TAG abstract specification relies on a three-dimensional description made
of, namely, sub-categorization frames, blocks, and lexical redistribution rules. To com-
pile this specification into a TAG, the system selects a canonical sub-categorization
frame, and applies some lexical redistribution rules to derive new frames and finally
select blocks corresponding to the resulting frames. These blocks contain tree descrip-
tions using the logic of Rogers and Vijay-Shanker (1994).
LexOrg suffers from similar limitations as Candito?s compiler. Much of the lin-
guistic knowledge is embedded in the compiling algorithm, making it difficult for
linguists to extend the grammar description and to handle exceptions. Unlike in Can-
dito?s framework, the tree description language uses local node variables and lets the
tree description solver determine node identifications. Although this avoids having to
memorize node names, this requires that the descriptions be constrained enough to
impose the required node identifications and prevent the unwanted ones. In practice,
this again complicates grammar writing. In contrast, XMG provides an intermediate
solution which, by combining local variables with export declarations, avoids having to
memorize too many node variable names (only those local to the relevant sub-hierarchy
need memorizing) while allowing for explicit node identification.
The Metagrammar Compiler of Gaiffe, Crabbe?, and Roussanaly. Gaiffe, Crabbe?, and
Roussanaly (2002) proposed a compiler for FB-LTAG that aims to remedy both the lack
of a clear separation between linguistic information and compilation algorithm, and
the lack of explicit control on the class combinations prevalent in Candito (1996), Xia
et al (1998), and Xia (2001). In their approach, the linguistic specification consists of
a single inheritance hierarchy of classes, each class containing a tree description. The
623
Computational Linguistics Volume 39, Number 3
description logic used is similar to Candito?s. That is, global node names are used. To
trigger class combinations, classes are labeled with two types of information: needs and
resources. The compiler selects all final classes of the hierarchy, performs all possible
combinations, and only keeps those combinations that neutralize the stated needs
and resources. The tree descriptions contained in these neutral combinations are then
solved to produce the expected trees.
Although this approach implements a clear separation between linguistic informa-
tion and compilation algorithm, the fully automatic derivation of FB-LTAG trees from
the inheritance hierarchy makes it difficult in practice to control overgeneration. In
contrast, XMG?s explicit definitions of class combinations by conjunction, disjunction,
and inheritance makes it easier to control the tree set that will be generated by the
compiler from the grammar specification. Additionally, the issues raised by global
variables remain (no way to instantiate twice a given class, and cumbersome definition
of variables in large metagrammars).
The MGCOMP System. More recently, Villemonte de la Clergerie (2005, 2010) proposed a
compiler for FB-LTAG that aims at preserving a high degree of factorization in both the
abstract grammar specification and the grammar which is compiled from it. Thus, the
MGCOMP system does not compute FB-LTAG elementary trees, but factorized trees.
In MGCOMP, like in Gaiffe, Crabbe?, and Roussanaly?s (2002) approach, a meta-
grammar consists of a single hierarchy of classes. The classes are labeled with needs and
resources, and final classes of the hierarchy are combined to compute tree descriptions.
The main differences with Gaiffe, Crabbe?, and Roussanaly (2002), lies in the fact that
(i) a description can include new factorizing operators, such as repetition (Kleene-star
operator), shuffling (interleaving of nodes), optionality, and disjunctions; and (ii) it offers
namespaces to specify the scope of variables. MGCOMP?s extended tree descriptions
are not completely solved by the compiler. Rather, it compiles underspecified trees (also
called factorized trees). With this approach, a large grammar is much smaller in terms of
number of grammatical structures than a classical FB-LTAG. As a result, the grammars it
compiles are only compatible with the DyALog parsing environment (Villemonte de La
Clergerie 2005). And, because the linguist designs factorized trees and not actual TAG
trees, debugging the metagrammar becomes harder.
7.2 Resources Built Using Candito, Xia, and De La Clergerie?s Systems
Candito?s system has been used by Candito (1999) herself to design a core FB-LTAG
for French and Italian, and later by Barrier (2006) to design a FB-LTAG for adjectives
in French. Xia?s system (LexOrg) has been used to semi-automatically generate XTAG
(Xia 2001). De La Clergerie?s system (MGCOMP) has been used to design a grammar
for French named FRMG (FRench MetaGrammar) (Villemonte de la Clergerie 2010).
FRMG makes use of MGCOMP?s factorizing operators (e.g., shuffling operator), thus
producing not sensu stricto a FB-LTAG, but a factorized FB-LTAG. FRMG is freely
available, contains 207 factorized trees (having optional branches, etc.) built from 279
metagrammatical classes, and covers 95% of the TSNLP.
8. Conclusion
In this article, we presented the eXtensible MetaGrammar framework and argued that,
contrary to other existing grammar writing environments for tree-based grammar,
624
Crabbe? et al XMG: eXtensible MetaGrammar
XMG is declarative, extensible, and notationally expressive. We believe that these fea-
tures make XMG particularly appropriate for a fast prototyping of the kind of deep
tree-based grammars that are used in applications requiring high precision in gram-
mar modeling (e.g., language teaching, man/machine dialogue systems, data-to-text
generation).
The XMG language is documented on-line, and its compiler is open source soft-
ware, freely available under the terms of the GPL-compliant CeCILL license.34 Many
grammars designed with XMG (FB-LTAG and IG for French and English, TT-MCTAG
for German) are also open-source and available on-line.35
Future research will focus on extensibility. So far, XMG has been used to design tree-
based grammars for different languages. We plan to extend XMG to handle other types
of formalisms36 such as dependency grammars, and to support dimensions other than
syntax and semantics such as for instance, phonology or morphology. As mentioned
here, XMG offers a modular architecture, making it possible to extend it relatively easily.
Nonetheless, in its current state, such extensions imply modifying XMG?s code. We are
exploring new extensions of the formalism, which would allow the linguist to dynam-
ically define her/his metagrammar formalism (e.g., which principles or descriptions to
use) depending on the target formalism.
Another interesting question concerns cross-language grammar engineering. So far,
the metagrammar allows for dealing with structural redundancy. As pointed out by
Kinyon et al (2006), a metagrammar can be used to capture generalizations across
languages and is surely worth further investigating.
Finally, we plan to extend XMG with features borrowed from Integrated De-
velopment Environments (IDE) for programming languages. Designing a grammar
is, in some respect, similar to programming an application. Grammar environments
should benefit from the same tools as those used for the development of applications
(incremental compilation, debugger, etc.).
Acknowledgments
We are grateful to the three anonymous
reviewers for their valuable comments.
Any remaining errors are ours.
References
Abeille?, A. 2002. Une grammaire e?lectronique
du franc?ais. CNRS Editions.
Abeille?, A., M. Candito, and A. Kinyon. 1999.
Ftag: current status and parsing scheme.
In Proceedings of Vextal ?99, pages 283?292,
Venice.
A??t-Kaci, Hassan. 1991. Warren?s Abstract
Machine: A Tutorial Reconstruction. MIT
Press, Cambridge, MA.
Alahverdzhieva, Katya. 2008. XTAG using
XMG. Masters thesis, Nancy Universite?.
Baldridge, Jason, Sudipta Chatterjee,
Alexis Palmer, and Ben Wing. 2007.
DotCCG and VisCCG: Wiki and
programming paradigms for improved
grammar engineering with OpenCCG.
In Tracy Holloway King and Emily M.
Bender, editors, Proceedings of the Grammar
Engineering Across Framework Workshop
(GEAF 07). CSLI, Stanford, CA, pages 5?25.
Barrier, Se?bastien. 2006. Une me?tagrammaire
pour les noms pre?dicatifs du franc?ais :
de?veloppement et expe?rimentations pour les
grammaires TAG. Ph.D. thesis, Universite?
Paris 7.
Becker, Tilman. 1993. HyTAG: A New Type
of Tree Adjoining Grammars for Hybrid
Syntactic Representation of Free Word Order
Language. Ph.D. thesis, Universita?t des
Saarlandes.
34 See https://sourcesup.renater.fr/xmg.
35 The French TAG and French and English IG are available on XMG?s website, and the German TreeTuple
MC-TAG is available at http://www.sfs.uni-tuebingen.de/emmy/res.html.
36 Preliminary work on cross-framework grammar engineering has been realized by Cle?ment and Kinyon
(2003), who used Gaiffe et al?s compiler to produce both a TAG and a LFG from a given metagrammar.
625
Computational Linguistics Volume 39, Number 3
Blackburn, Patrick, Johan Bos, and Kristina
Striegnitz. 2006. Learn Prolog Now!,
volume 7 of Texts in Computing. College
Publications, London.
Bresnan, Joan and Annie Zaenen. 1990. Deep
unaccusitivity in LFG. In K. Dziwirek,
P. Farell, and E. Mejias-Bikandi, editors,
Grammatical Relations: A Cross-Theoretical
Perspective. CSLI publications, Stanford,
CA, pages 45?57.
Candito, Marie. 1996. A principle-based
hierarchical representation of LTAGs.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 194?199, Copenhagen.
Candito, Marie. 1999. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques
lexicalise?es : application au franc?ais et a`
l?italien. Ph.D. thesis, Universite? Paris 7.
Cle?ment, Lionel and Alexandra Kinyon.
2003. Generating parallel multilingual
lfg-tag grammars from a metagrammar.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 184?191, Sapporo.
Cohen-Sygal, Yael and Shuly Wintner. 2007.
The Non-Associativity of Polarized
Tree-Based Grammars. In Proceedings of the
Eighth International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLing-2007), pages 208?217, Mexico City.
Cohen-Sygal, Yael and Shuly Wintner. 2009.
Associative grammar combination operators
for tree-based grammars. Journal of Logic,
Language and Information, 18(3):293?316.
Cohen-Sygal, Yael and Shuly Wintner. 2011.
Towards modular development of typed
unification grammars. Computational
Linguistics, 37(1):29?74.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
Second Conference on Language Resources and
Evaluation (LREC-2000), Athens.
Copestake, Ann, Alex Lascarides, and Dan
Flickinger. 2001. An algebra for semantic
construction in constraint-based
grammars. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 140?147, Toulouse.
Crabbe?, Benoit. 2005. Repre?sentation
informatique de grammaires fortement
lexicalise?es : Application a` la grammaire
d?arbres adjoints. Ph.D. thesis, Universite?
Nancy 2.
Crabbe?, Beno??t and Denys Duchier. 2004.
Metagrammar redux. In Proceedings of the
Workshop on Constraint Solving for Language
Processing (CSLP 2004), pages 32?47,
Copenhagen.
Duchier, Denys, Brunelle Magnana Ekoukou,
Yannick Parmentier, Simon Petitjean, and
Emmanuel Schang. 2012. Describing
morphologically-rich languages using
metagrammars: A look at verbs in Ikota.
In Workshop on ?Language Technology for
Normalisation of Less-resourced Languages,?
8th SALTMIL Workshop on Minority
Languages and 4th Workshop on African
Language Technology, International
Conference on Language Resources and
Evaluation, LREC 2012, pages 55?60,
Istanbul.
Duchier, Denys and Joachim Niehren. 2000.
Dominance constraints with set operators.
In John W. Lloyd, Vero?nica Dahl, Ulrich
Furbach, Manfred Kerber, Kung-Kiu Lau,
Catuscia Palamidessi, Lu??s Moniz Pereira,
Yehoshua Sagiv, and Peter J. Stuckey,
editors, Proceedings of the First International
Conference on Computational Logic,
volume 1861 of Lecture Notes in Computer
Science. Springer, Berlin, pages 326?341.
Duchier, Denys, Yannick Parmentier, and
Simon Petitjean. 2012. Metagrammars as
logic programs. In International Conference
on Logical Aspects of Computational
Linguistics (LACL 2012). Proceedings of
the Demo Session, pages 1?4, Nantes.
Duchier, Denys and Stefan Thater. 1999.
Parsing with tree descriptions: A
constraint-based approach. In Proceedings
of the Sixth International Workshop on
Natural Language Understanding and Logic
Programming (NLULP?99), pages 17?32,
Las Cruces, NM.
Evans, Roger, Gerald Gazdar, and David
Weir. 1995. Encoding lexicalized tree
adjoining grammars with a nonmonotonic
inheritance hierarchy. In Proceedings of the
33rd Annual Meeting of the Association for
Computational Linguistics, pages 77?84,
Cambridge, MA.
Flickinger, Daniel. 1987. Lexical Rules in the
Hierarchical Lexicon. Ph.D. thesis, Stanford
University.
Gaiffe, Bertrand, Beno??t Crabbe?, and Azim
Roussanaly. 2002. A new metagrammar
compiler. In Proceedings of the Sixth
International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6),
pages 101?108, Venice.
Gardent, Claire. 2007. Tree adjoining
grammar, semantic calculi and labelling
invariants. In Proceedings of the International
Workshop on Computational Semantics
(IWCS), Tilburg.
626
Crabbe? et al XMG: eXtensible MetaGrammar
Gardent, Claire. 2008. Integrating a
unification-based semantics in a large
scale lexicalised tree adjoininig grammar
for French. In Proceedings of the 22nd
International Conference on Computational
Linguistics (COLING?08), pages 249?256,
Manchester.
Gardent, Claire and Laura Kallmeyer. 2003.
Semantic construction in feature-based
tree adjoining grammar. In Proceedings of
the 10th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 123?130, Budapest.
Gardent, Claire and Eric Kow. 2007. A
symbolic approach to near-deterministic
surface realisation using tree adjoining
grammar. In 45th Annual Meeting of the
Association for Computational Linguistics,
pages 328?335, Prague.
Gardent, Claire and Yannick Parmentier.
2005. Large scale semantic construction for
tree adjoining grammars. In Proceedings
of the Fifth International Conference
on Logical Aspects of Computational
Linguistics (LACL?05), pages 131?146,
Bordeaux.
Gardent, Claire and Yannick Parmentier.
2006. Coreference Handling in XMG.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006) Main Conference Poster Sessions,
pages 247?254, Sydney.
Gardent, Claire and Yannick Parmentier.
2007. SemTAG: A platform for specifying
tree adjoining grammars and performing
TAG-based semantic construction. In
Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions,
pages 13?16, Prague.
Iordanskaja, Lidija and Igor Mel?c?uk, 2009.
Establishing an inventory of surface?
syntactic relations: valence-controlled
surface-dependents of the verb in French.
In A. Polgue`re and I. A. Mel?duk, editors,
Dependency in Linguistic Description.
John Benjamins, Amsterdam,
pages 151?234.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Sciences,
10(1):136?163.
Kallmeyer, Laura. 1999. Tree Description
Grammars and Underspecified
Representations. Ph.D. thesis,
Universita?t Tu?bingen.
Kallmeyer, Laura. 2005. Tree-local
multicomponent tree-adjoining grammars
with shared nodes. Computational
Linguistics, 31(2):187?226.
Kallmeyer, Laura, Timm Lichte, Wolfgang
Maier, Yannick Parmentier, and Johannes
Dellert. 2008. Developing a TT-MCTAG for
German with an RCG-based parser. In
Proceedings of the Sixth Language Resources
and Evaluation Conference (LREC),
pages 782?789, Marrakech.
Kallmeyer, Laura, Wolfgang Maier, Yannick
Parmentier, and Johannes Dellert. 2010.
TuLiPA?Parsing extensions of TAG with
range concatenation grammars. Bulletin of
the Polish Academy of Sciences: Technical
Sciences, 58(3):377?392.
Kallmeyer, Laura and Maribel Romero.
2004a. LTAG semantics for questions.
In Proceedings of 7th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+7), pages 186?193,
Vancouver.
Kallmeyer, Laura and Maribel Romero.
2004b. LTAG semantics with semantic
unification. In Proceedings of 7th
International Workshop on Tree-Adjoining
Grammar and Related Formalisms (TAG+7),
page 155?162, Vancouver.
Kallmeyer, Laura and Maribel Romero.
2008. Scope and situation binding in
LTAG using semantic unification.
Research on Language and Computation,
6(1):3?52.
Kaplan, Ronald and Paula Newman. 1997.
Lexical resource reconciliation in the Xerox
linguistic environment. In Proceedings
of the ACL Workshop on Computational
Environments for Grammar Development
and Linguistic Engineering, pages 54?61,
Madrid.
Kinyon, Alexandra. 2000. Hypertags.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING?00), pages 446?452, Saarbru?cken.
Kinyon, Alexandra, Owen Rambow, Tatjana
Scheffler, SinWon Yoon, and Aravind K.
Joshi. 2006. The metagrammar goes
multilingual: A cross-linguistic look at
the v2-phenomenon. In Proceedings of
the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms,
pages 17?24, Sydney.
Le Hong, Phuong, Thi-Min-Huyen
N?Guyen, and Azim Roussanaly. 2008.
A metagrammar for Vietnamese. In
Proceedings of the 9th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+9), Tu?bingen.
627
Computational Linguistics Volume 39, Number 3
Lehmann, Sabine, Stephan Oepen, Sylvie
Regnier-Prost, Klaus Netter, Veronika Lux,
Judith Klein, Kirsten Falkedal, Frederik
Fouvry, Dominique Estival, Eva Dauphin,
Herve? Compagnion, Judith Baur, Lorna
Balkan, and Doug Arnold. 1996. TSNLP?
Test suites for natural language processing.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 711?716, Copenhagen.
Lichte, Timm. 2007. An MCTAG with tuples
for coherent constructions in German.
In Proceedings of the 12th Conference on
Formal Grammar (FG 2007), 12 pages,
Dublin.
Muskens, Reinhard and Emiel Krahmer.
1998. Description theory, LTAGs and
Underspecified Semantics. In Fourth
International Workshop on Tree Adjoining
Grammars and Related Frameworks,
pages 112?115, Philadelphia, PA.
Parmentier, Yannick. 2007. SemTAG: une
plate-forme pour le calcul se?mantique a` partir
de Grammaires d?Arbres Adjoints. Ph.D.
thesis, Universite? Henri Poincare? - Nancy.
Parmentier, Yannick, Laura Kallmeyer, Timm
Lichte, and Wolfgang Maier. 2007. XMG:
eXtending MetaGrammars to MCTAG. In
Proceedings of the Workshop on High-Level
Syntactic Formalisms, 14th Conference on
Natural Language Processing (TALN?2007),
pages 473?482, Toulouse.
Pereira, Fernando and David Warren. 1980.
Definite clause grammars for language
analysis?A survey of the formalism
and a comparison to augmented
transition networks. Artificial
Intelligence, 13:231?278.
Perlmutter, David. 1970. Surface structure
constraints in syntax. Linguistic Inquiry,
1:187?255.
Perrier, Guy. 2000. Interaction grammars.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 600?606,
Saarbru?cken.
Perrier, Guy. 2007. A French interaction
grammar. In Proceedings of the 6th
Conference on Recent Advances in Natural
Language Processing (RANLP 2007),
pages 463?467, Borovets.
Prolo, Carlos A. 2002. Generating the
XTAG English grammar using metarules.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING?2002), pages 814?820, Taipei.
Rambow, Owen, K. Vijay-Shanker, and
David Weir. 1995. D-tree grammars.
In Proceedings of the 33th Meeting of the
Association for Computational Linguistics,
pages 151?158, Cambridge, MA.
Rogers, James and K. Vijay-Shanker. 1994.
Obtaining trees from their descriptions:
An application to tree-adjoining
grammars. Computational Intelligence,
10:401?421.
Shieber, Stuart M. 1984. The design of a
computer language for linguistic
information. In Proceedings of the Tenth
International Conference on Computational
Linguistics, pages 362?366, Stanford, CA.
Van Roy, Peter. 1990. Extended DCG
notation: A tool for applicative
programming in prolog. Technical
Report UCB/CSD 90/583, University
of California, Berkeley.
Vijay-Shanker, K. and Aravind K. Joshi.
1988. Feature structures based tree
adjoining grammars. In Proceedings
of the 12th Conference on Computational
Linguistics (COLING?88), pages 714?719,
Budapest.
Vijay-Shanker, K. and Yves Schabes. 1992.
Structure sharing in lexicalized tree
adjoining grammars. In Proceedings
of the 14th International Conference on
Computational Linguistics (COLING?92),
pages 205?212, Nantes.
Villemonte de La Clergerie, E?ric. 2005.
DyALog: a tabular logic programming
based environment for NLP. In Proceedings
of 2nd International Workshop on Constraint
Solving and Language Processing (CSLP?05),
pages 18?33, Barcelona.
Villemonte de la Clergerie, E?ric. 2010.
Building factorized TAGs with
meta-grammars. In Proceedings of
the 10th International Workshop on
Tree-Adjoining Grammar and Related
Formalisms (TAG+10), pages 111?118,
New Haven, CT.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Xia, Fei. 2001. Automatic Grammar Generation
from Two Different Perspectives. Ph.D. thesis,
University of Pennsylvania.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 1999. Toward semi-automating
grammar development. In Proceedings of
the 5th Natural Language Processing Pacific
Rim Symposium (NLPRS-99), pages 96?101,
Beijing.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 2005. Automatically generating
tree adjoining grammars from abstract
specifications. Journal of Computational
Intelligence, 21(3):246?287.
628
Crabbe? et al XMG: eXtensible MetaGrammar
Xia, Fei, Martha Palmer, and
K. Vijay-Shanker. 2010. Developing
tree-adjoining grammars with lexical
descriptions. In Srinivas Bangalore and
Aravind Joshi, editors, Supertagging:
Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press,
Cambridge, MA, pages 73?110.
Xia, Fei, Martha Palmer, K. Vijay-Shanker,
and Joseph Rosenzweig. 1998. Consistent
grammar development using partial-tree
descriptions for LTAGs. In Proceedings
of the 4th International Workshop on
Tree Adjoining Grammar and Related
Formalisms (TAG+ 1998), pages 180?183,
Philadelphia, PA.
XTAG Research Group. 2001. A lexicalized
tree adjoining grammar for English.
Technical Report IRCS-01-03, IRCS,
University of Pennsylvania.
629

