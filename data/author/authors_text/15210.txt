Proceedings of NAACL-HLT 2013, pages 709?714,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distributional semantic models for the evaluation of disordered language
Masoud Rouhizadeh?, Emily Prud?hommeaux?, Brian Roark?, Jan van Santen?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{rouhizad,vansantj}@ohsu.edu, {emilypx,roarkbr}@gmail.com
Abstract
Atypical semantic and pragmatic expression is
frequently reported in the language of children
with autism. Although this atypicality often
manifests itself in the use of unusual or un-
expected words and phrases, the rate of use
of such unexpected words is rarely directly
measured or quantified. In this paper, we
use distributional semantic models to automat-
ically identify unexpected words in narrative
retellings by children with autism. The classi-
fication of unexpected words is sufficiently ac-
curate to distinguish the retellings of children
with autism from those with typical develop-
ment. These techniques demonstrate the po-
tential of applying automated language anal-
ysis techniques to clinically elicited language
data for diagnostic purposes.
1 Introduction
Autism spectrum disorder (ASD) is a neurodevelop-
mental disorder characterized by impaired commu-
nication and social behavior. Although the symp-
toms of ASD are numerous and varied, atypical
and idiosyncratic language has been one of the
core symptoms observed in verbal individuals with
autism since Kanner first assigned a name to the
disorder (Kanner, 1943). Atypical language cur-
rently serves as a diagnostic criterion in many of the
most widely used diagnostic instruments for ASD
(Lord et al, 2002; Rutter et al, 2003), and the phe-
nomenon is especially marked in the areas of seman-
tics and pragmatics (Tager-Flusberg, 2001; Volden
and Lord, 1991).
Because structured language assessment tools are
not always sensitive to the particular atypical seman-
tic and pragmatic expression associated with ASD,
measures of atypical language are often drawn from
spontaneous language samples. Expert manual an-
notation and analysis of spontaneous language in
young people with ASD has revealed that children
and young adults with autism include significantly
more bizarre and irrelevant content (Loveland et al,
1990; Losh and Capps, 2003) in their narratives and
more abrupt topic changes (Lam et al, 2012) in
their conversations than their language-matched typ-
ically developing peers. Most normed clinical in-
struments for analyzing children?s spontaneous lan-
guage, however, focus on syntactic measures and
developmental milestones related to the acquisition
of vocabulary and syntactic structures. Measures of
semantic and pragmatic atypicality in spontaneous
language are rarely directly measured. Instead, the
degree of language atypicality is often determined
via subjective parental reports (e.g., asking a par-
ent whether their child has ever used odd phrases
(Rutter et al, 2003)) or general impressions dur-
ing clinical examination (e.g., rating the child?s de-
gree of ?stereotyped or idiosyncratic use of words or
phrases? on a four-point scale (Lord et al, 2002)).
This has led to a lack of reliable and objective infor-
mation about the frequency of atypical language use
and its precise nature in ASD.
In this study, we attempt to automatically detect
instances of contextually atypical language in spon-
taneous speech at the lexical level in order to quan-
tify its prevalence in the ASD population. We first
determine manually the off-topic, surprising, or in-
709
appropriate words in a set of narrative retellings
elicited in a clinical setting from children with ASD
and typical development. We then apply word rank-
ing methods and distributional semantic modeling to
these narrative retellings in order to automatically
identify these unexpected words. The results indi-
cate not only that children with ASD do in fact pro-
duce more semantically unexpected and inappropri-
ate words in their narratives than typically develop-
ing children but also that our automated methods
for identifying these words are accurate enough to
serve as an adequate substitute for manual annota-
tion. Although unexpected off-topic word use is just
one example of the atypical language observed in
ASD, the work presented here highlights the poten-
tial of computational language evaluation and analy-
sis methods for improving our understanding of the
linguistic deficits associated with ASD.
2 Data
Participants in this study included 37 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was diag-
nosed via clinical consensus according to the DSM-
IV-TR criteria (American Psychiatric Association,
2000) and the established threshold scores on two
diagnostic instruments: the Autism Diagnostic Ob-
servation Schedule (ADOS) (Lord et al, 2002), a
semi-structured series of activities designed to allow
an examiner to observe behaviors associated with
autism; and the Social Communication Question-
naire (SCQ) (Rutter et al, 2003), a parental ques-
tionnaire. None of the children in this study met
the criteria for a language impairment, and there
were no significant between-group differences in
age (mean=6.4) or full-scale IQ (mean=114).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Korkman
et al, 1998), a large and comprehensive battery of
tasks that test neurocognitive functioning in chil-
dren. The NEPSY Narrative Memory (NNM) sub-
test is a narrative retelling test in which the subject
listens to a brief narrative, excerpts of which are
shown in Figure 1, and then must retell the narra-
tive to the examiner. The NNM was administered
to each participant in the study, and each partici-
pant?s retelling was recorded and transcribed. Us-
ing Amazon?s Mechanical Turk, we also collected
a large corpus of retellings from neurotypical adults,
who listened to a recording of the story and provided
written retellings. We describe how this corpus was
used in Section 3, below.
Two annotators, blind to the diagnosis of the ex-
perimental subjects, identified every word in each
retelling transcript that was unexpected or inappro-
priate given the larger context of the story. For in-
stance, in the sentence T-rex could smell things, both
T-rex and smell were marked as unexpected, since
there is no mention of either concept in the story. In
a seemingly more appropriate sentence, the boy sat
up off the bridge, the word bridge is considered un-
expected since the boy is trapped up in a tree rather
than on a bridge.
3 Methods
We start with the expectation that different retellings
of the same source narrative will share a common
vocabulary and semantic space. The presence of
words outside of this vocabulary or semantic space
in a retelling may indicate that the speaker has
strayed from the topic of the story. Our approach for
automatically identifying these unexpected words
relies on the ranking of words according to the
strength of their association with the target topic of
the corpus. The word association scores used in the
Figure 1: Excerpts from the NNM narrative.
Jim was a boy whose best friend was Pepper. Pepper was a big black dog. [...] Near Jim?s house was a
very tall oak tree with branches so high that he couldn?t reach them. Jim always wanted to climb that tree,
so one day he took a ladder from home and carried it to the oak tree. He climbed up [...] When he started
to get down, his foot slipped, his shoe fell off, and the ladder fell to the ground. [...] Pepper sat below the
tree and barked. Suddenly Pepper took Jim?s shoe in his mouth and ran away. [...] Pepper took the shoe to
Anna, Jim?s sister. He barked and barked. Finally, Anna understood that Jim was in trouble. She followed
Pepper to the tree where Jim was stuck. Anna put the ladder up and rescued Jim.
710
ranking are informed by the frequency of a word
in the child?s retelling relative to the frequency of
that word in other retellings in the larger corpus of
retellings. These association measures are similar
to those developed for the information retrieval task
of topic modeling, in which the goal is to identify
topic-specific words ? i.e., words that appear fre-
quently in only a subset of documents ? in order
to cluster together documents about a similar topic.
Details about how these scores are calculated and in-
terpreted are provided in the following sections.
The pipeline for determining the set of unusual
words in each retelling begins by calculating word
association scores, described below, for each word
in each retelling and ranking the words according to
these scores. A threshold over these scores is de-
termined for each child using leave-one-out cross
validation in order to select a set of potentially un-
expected words. This set of potential unexpected
words is then filtered using two external resources
that allow us to eliminate words that were not used
in other retellings but are likely to be semantically
related to topic of the narrative. This final set of
words is evaluated against the set of manually iden-
tified words in order determine the accuracy of our
unexpected word classification.
3.1 Word association measures
Before calculating the word association measures,
we tokenize, downcase, and stem (Porter, 1980) the
transcripts and remove all punctuation. We then use
two association measures to score each word in each
child?s retelling: tf-idf, the term frequency-inverse
document frequency measure (Salton and Buckley,
1988), and the log odds ratio (van Rijsbergen et al,
1981). We use the following formulation to calcu-
late tf-idf for each child?s retelling i and each word
in that retelling j, where cij is the count of word j
in retelling i; fj is the number of retellings from the
full corpus of child and adult retellings containing
that word j; and D is the total number of retellings
in the full corpus (Manning et al, 2008):
tf-idfij =
{
(1 + log cij) log Dfj if cij ? 1
0 otherwise
The log odds ratio, another association measure
used in information retrieval and extraction tasks, is
the ratio between the odds of a particular word, j,
appearing in a child?s retelling, i, as estimated us-
ing its relative frequency in that retelling, and the
odds of that word appearing in all other retellings,
again estimated using its relative frequency in all
other retellings. Letting the probability of a word
appearing in a retelling be p1 and the probability of
that word appearing in all other retellings be p2, we
can express the odds ratio as follows:
odds ratio =
odds(p1)
odds(p2)
=
p1/(1? p1)
p2/(1? p2)
A large tf-idf or log odds score indicates that the
word j is very specific to the retelling i, which in
turn suggests that the word might be unexpected or
inappropriate in the larger context of the NNM nar-
rative. Thus we expect that the words with higher as-
sociation measure scores are likely to be the words
that were manually identified as unexpected in the
context of the NNM narrative.
3.2 Application of word association measures
As previously mentioned, both of these word associ-
ation measures are used in information retrieval (IR)
to cluster together documents about a similar target
topic. In IR, words that appear only in a subset of
documents from a large and varied corpus of docu-
ments will have high word association scores, and
the documents containing those words will likely be
focused on the same topic. In our task, however,
we have a single cluster of documents focused on
a single topic: the NNM narrative. Topic-specific
words ought to occur much more frequently than
other words. As a result, words with high tf-idf and
log odds scores are likely to be those unrelated to
the topic of the NNM story. If a child veers away
from the topic of the NNM story and uses words that
do not occur frequently in the retellings produced
by neurotypical speakers, his retellings will contain
more words with high word association scores. We
predict that this set of high-scoring words is likely to
overlap significantly with the set of words identified
by the manual annotators as unexpected or off-topic.
Applying these word association scoring ap-
proaches to each word in each child?s retelling yields
a list of words from each retelling ranked in order of
decreasing tf-idf or log odds score. We use cross-
validation to determine, for each measure, the op-
711
erating point that maximizes the unexpected word
identification accuracy in terms of F-measure. For
each child, the threshold is found using the data from
all of the other children. This threshold is then ap-
plied to the ranked word list of the held-out child.
All words above this threshold are potential unex-
pected words, while all words below this threshold
are considered to be expected and appropriate in the
context of the NNM narrative. Table 1 shows the
recall, precision, and F-measure using the two word
association measures discussed here. We see that
these two techniques result in high recall at the ex-
pense of precision. The next stage in the pipeline is
therefore to use external resources to eliminate any
semantically appropriate words from the set of po-
tentially unexpected or inappropriate words gener-
ated via thresholding on the tf-idf or log odds score.
3.3 Filtering with external resources
Recall that the corpus of retellings used to gener-
ate the word association measures described above,
is very small. It is therefore quite possible that a
child may have used an entirely appropriate word
that by chance was never used by another child or
one of the neurotypical adults. One way of increas-
ing the lexical coverage of the corpus of retellings
is through semantic expansion using an external re-
source. For each word in the set of potential un-
expected words, we located the WordNet synset for
that word (Fellbaum, 1998). If any of the WordNet
synonyms of the potentially unexpected word was
present in the source narrative or in one of the adult
retellings, that word was removed from the set of
unexpected words.
In the final step, we used the CHILDES corpus
of transcripts of children?s conversational speech
(MacWhinney, 2000) to generate topic estimates for
each remaining potentially unexpected word. For
each of these words, we located every utterance in
the CHILDES corpus containing that potentially un-
expected word. We then measured the association
of that word with every other open-class word that
appeared in an utterance with that word using the
log likelihood ratio (Dunning, 1993). The 20 words
from the CHILDES corpus with the highest log like-
lihood ratio (i.e., the words most strongly associ-
ated with the potentially unexpected word), were as-
sumed to collectively represent a particular topic. If
more than two of the words in the vector of words
representing this topic were also present in the NNM
source narrative or the adult retellings, the word that
generated that topic was eliminated from the set of
unexpected words.
We note that the optimized threshold described
in Section 3.2, above, is determined after filtering.
There is therefore potentially a different threshold
for each condition tested, and hence we do not nec-
essarily expect precision to increase and recall to
decrease after filtering. Rather, since the threshold
is selected in order to optimize F-measure, we ex-
pect that if the filtering is effective, F-measure will
increase with each additional filtering condition ap-
plied.
4 Results
We evaluated the performance of our two word rank-
ing techniques, both individually and combined by
taking either the maximum of the two measures or
the sum, against the set of manually annotations de-
scribed in Section 2. In addition, we report the re-
sults of applying these word ranking techniques in
combination with the two filtering techniques. We
compare these results with a simple baseline method
in which every word used in a retelling that is never
used in another retelling is considered to be unex-
pected. Table 1 shows the precision, accuracy, and
F-measure of these approaches. We see that all of
the more sophisticated unexpected word identifica-
tion approaches outperform the baseline by a wide
margin, and that tf-idf and log odds perform compa-
rably under the condition without filtering and both
filtering conditions. Filtering improves F-measure
under both word ranking schemes, and combining
the two measures results in further improvements
under both filtering conditions. Although apply-
ing topic-estimate filtering yields the highest preci-
sion, the simple WordNet-based approach results in
the highest F-measure and a reasonable balance be-
tween precision and recall.
Recall that the purpose of identifying these un-
expected words was to determine whether children
with ASD produce unexpected and inappropriate
words at a higher rate than children with typical de-
velopment. This appears to be true in our manu-
ally annotated data. On average, 7.5% of the words
712
Unexpected word identification method P R F1
Baseline 46.3 74.0 57.0
TF-IDF 72.1 79.5 75.6
Log-odds 70.5 79.5 74.7
Sum(TF-IDF, Log-odds) 72.2 83.3 77.4
Max(TF-IDF, Log-odds) 69.9 83.3 76.0
TF-IDF+WordNet 83.8 80.5 82.1
Log-odds+WordNet 82.1 83.1 82.6
Sum(TF-IDF, Log-odds)+WordNet 84.2 83.1 83.7
Max(TF-IDF, Log-odds)+WordNet 83.3 84.4 83.9
TF-IDF+WordNet+topic 85.7 77.9 81.7
Log-odds+WordNet+topic 83.8 80.5 82.1
Sum(TF-IDF, Log-odds)+WordNet+topic 86.1 80.5 83.2
Max(TF-IDF, Log-odds)+WordNet+topic 85.1 81.8 83.4
Table 1: Accuracy of unexpected word identification.
types produced by children with ASD were marked
as unexpected, while only 2.5% of words produced
by children with TD were marked as unexpected, a
significant difference (p < 0.01, using a one-tailed
t-test). This significant between-group difference
in rate of unexpected word use holds even when
using the automated methods of unexpected word
identification, with the best performing unexpected
word identification method estimating a mean of
6.6% in the ASD group and 2.5% in the TD group
(p < 0.01).
5 Conclusions and future work
The automated methods presented here for rank-
ing and filtering words according to their distribu-
tions in different corpora, which are adapted from
techniques originally developed for topic modeling
in the context of information retrieval and extrac-
tion tasks, demonstrate the utility of automated ap-
proaches for the analysis of semantics and pragmat-
ics. We were able to use these methods to iden-
tify unexpected or inappropriate words with high
enough accuracy to replicate the patterns of unex-
pected word use manually observed in our two di-
agnostic groups. This work underscores the poten-
tial of automated techniques for improving our un-
derstanding of the prevalence and diagnostic utility
of linguistic features associated with ASD and other
communication and language disorders.
In future work, we plan to use a development set
to determine the optimal number of topical words
to select during the topic estimate filtering stage of
the pipeline in order to maintain improvements in
precision without a loss in recall. We would also
like to investigate using part-of-speech, word sense,
and parse information to improve our approaches
for both semantic expansion and topic estimation.
Although the rate of unexpected word use alone is
unlikely to provide sufficient power to classify the
two diagnostic groups investigated here, we expect
that it can serve as one feature in an array of fea-
tures that capture the broad range of semantic and
pragmatic atypicalities observed in the spoken lan-
guage of children with autism. Finally, we plan to
apply these same methods to identify the confabula-
tions and topic shifts often observed in the narrative
retellings of the elderly with neurodegenerative con-
ditions.
Acknowledgments
This work was supported in part by NSF
Grant #BCS-0826654, and NIH NIDCD grant
#1R01DC012033-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
713
Christian Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic language
deficits in children with high-functioning autism: De-
picting the phenotype using the pragmatic rating scale.
Research in Autism Spectrum Disorders, 6:792797.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability
in high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental Dis-
orders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tunali.
1990. Narrative story telling in autism and down?s
syndrome. British Journal of Developmental Psychol-
ogy, 8(1):9?23.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval. Cambridge University Press.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Helen Tager-Flusberg. 2001. Understanding the lan-
guage and communicative impairments in autism. In-
ternational Review of Research in Mental Retardation,
23:185?205.
C.J. van Rijsbergen, D.J. Harper, and M.F. Porter. 1981.
The selection of good search terms. Information Pro-
cessing and Management, 17(2):77?91.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
714
Collecting Semantic Data by Mechanical Turk for the Lexical
Knowledge Resource of a Text-to-Picture Generating System
Masoud Rouhizadeh* Margit Bowler* Richard Sproat*Bob Coyne**
*Center for Spoken Language Understanding, Oregon Health and Science University
**Department of Computer Science, Columbia University
Abstract
WordsEye is a system for automatically converting natural language text into 3D scenes repre-
senting the meaning of that text. At the core of WordsEye is the Scenario-Based Lexical Knowledge
Resource (SBLR), a unified knowledge base and representational system for expressing lexical and
real-world knowledge needed to depict scenes from text. To enrich a portion of the SBLR, we need to
fill out some contextual information about its objects, including information about their typical parts,
typical locations and typical objects located near them. This paper explores our proposed method-
ology to achieve this goal. First we try to collect some semantic information by using Amazon?s
Mechanical Turk (AMT). Then, we manually filter and classify the collected data and finally, we
compare the manual results with the output of some automatic filtration techniques which use several
WordNet similarity and corpus association measures.
1 Introduction
WordsEye (Coyne and Sproat, 2001), (Coyne et al, 2010) is a system for automatically converting natural
language text into 3D scenes representing the meaning of that text. A version of WordsEye has been
tested online (www.wordseye.com) with several thousand real-world users. The system works by first
parsing each input sentence into a dependency structure. These dependency structures are then processed
to resolve anaphora and other coreferences. The lexical items and dependency links are then converted
to semantic nodes and roles drawing on lexical valence patterns and other information in the Scenario-
Based Lexical Knowledge Resource (SBLR) (Coyne et al, 2010). The resulting semantic relations are
then converted to a final set of graphical constraints representing the position, orientation, size, color,
texture, and poses of objects in the scene. Finally, the scene is composed from these constraints and
rendered in OpenGL (http://www.opengl.org).
The SBLR is the core of the text-to-scene conversion mechanism. It is a unified knowledge base and
representational system for expressing lexical and real-world knowledge needed to depict scenes from
text. The SBLR will ultimately include information on the semantic categories of words; the semantic
relations between predicates (verbs, nouns, adjectives, and prepositions) and their arguments; the types
of arguments different predicates typically take; additional contextual knowledge about the visual scenes
various events and activities occur in; and the relationship between this linguistic information and the 3D
objects in our objects library.
To enrich a portion of the SBLR we need to fill out some contextual information about several
hundred objects in WordsEye?s database, including information about their typical parts, typical location
and typical objects nearby them. Such information can in principle be extracted from online corpora
(e.g. Sproat (2001)), but such data is invariably noisy and requires hand editing. Furthermore, precisely
because much of the information is common sense it is rarely explicitly stated in text. Ontologies of
common sense information such as Cyc are effectively useless for extracting such information.
This paper explores our proposed methodology to achieve this goal. First we try to collect some
semantic information by Amazon?s Mechanical Turk (AMT). Then, we manually filter and classify the
380
collected data and finally, we compare the manual results with the output of some automatic filtration
techniques which use WN similarity and corpus association measures.
2 Data collection from Amazon?s Mechanical Turk
Amazon?s Mechanical Turk is an online marketplace that provides a way to pay people small amounts
of money to perform tasks that are simple for humans but difficult for computers. Examples of these
Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing
feedback on the relevance of results for a search query. The highly accurate, cheap and efficient results
of several NLP tasks (Callison-Burch and Dredze, 2010) have encouraged us to explore using AMT.
We designed three separate tasks to collect information about typical nearby objects, typical location
and typical parts of the objects of our library. For task 1, we asked the workers to name 10 common
objects that they might typically find around or near a given object. For task 2, we asked the workers to
name 10 locations in which they might typically find a given object and in task 3, we asked the workers
to list 10 parts of a given object. Given that some objects might not consist of 10 parts, (i.e, they are
very simple objects), we wanted the worker to name as many parts as possible. We collected 17,200
responses from the AMT tasks and paid $106.90 overall for completion of the three tasks. Table 1 shows
a summary of the AMT tasks, payments, and completion time.
Task TW UI AA RPA EHR ACT
Objects 342 6850 2? $0.05 $1.54 5
Locations 342 6850 2? $0.05 $1.26 5
Parts 245 3500 1? $0.07 $2.29 5
TW: Number of Target Words; UI: Number of User Inputs; AA: Average Time Per Assignment;
RPA: Reward Per Assignment; EHR: Effective Hourly Rate; ACT: Approximate Completion Days
Table 1: Summary of AMT tasks, payments and the completion time
The data that we collected in this step was in raw format. The next step was filtering out undesirable
data entered by the workers and mapping it into entities and relations contained within the SBLR.
3 Manual filtering and classifying the data
Data collected from AMT tasks was manually filtered via removal of undesirable target item-response
item pairs and classified via definition of the relations between the remaining target item-response item
pairs. Response items given in their plural form were lemmatized to the singular form of the word.
A total of 34 relations were defined within the Amazon Mechanical Turk data. Defining relations was
completed manually and determined by pragmatic cues about the relationship held between the target
item-response item pair. Restricting AMT workers to those within the United States ensured that actions
or items which might differ in their typically found location by cultural or geographical context were
restricted to the location(s) generally agreed upon by English speakers within the United States.
Generic, widely applicable relations were used in the general case for all sets of Mechanical Turk data
(e.g. the containment relation containing.r was used for generic instances of containment; the next-to.r
relation was used for target item-response item pairs for which the orientation of the items with respect to
one another was not a defining characteristic of their relationship). Finer distinctions were made within
these generic relations, e.g. habitat.r and residence.r within the overarching containment relation, which
specified that the relation held between two items was that of habitat or residence, respectively. More
semantically explicit relations were used for target item-response item pairs that tended to occur in more
specific relations. Specific relations of this type included those spatial relations from the following target
item-response item-relation triples:
javelin - dirt - embedded-in.r
binoculars - case - true-containing.r
381
Another subsection of relations included functional relations such as those within the following
triples:
harmonica - hand - human-grip.r
earmuffs - head - wearing.r
Relation labels for meronymic (part-whole) relations were based off of already defined part-whole
classifications (Priss, 1996).
3.1 Data and results for each AMT task
Target item-response item pairs were usually rejected for misinterpretation of the potentially ambiguous
target item (e.g. misinterpreting mobile as a cell phone rather than as a decorative hanging structure,
prompting mobile - ear as an object-nearby object pair). Target item-response item pairs were also dis-
carded if the interpretation of the target item, though viable, was not contained within the SBLR library.
This was especially prevalent in instances where the target item was a plant or animal (e.g. crawfish)
that could be interpreted as either a live plant/animal or as food. With the exception of mushroom, the
SBLR does not contain the edible interpretation of these nouns; in the object-nearby object task, target
item-response item pairs such as crawfish - plate were discarded.
In the object-location task, the most common relation labels were derivatives of the generic spatial
containment relation. The containing.r relation accounted for 38.01% of all labeled target-response pairs;
habitat.r accounted for 11.02%, and on-surface.r accounted for 10.6%.
In the part-whole task, AMT workers provided responses that were predominantly labeled by part-
whole relations. When AMT responses were not relevant for part-whole relations, they tended to fall
under the generic containment relation. The object-part.r relation accounted for 79.12% of all labeled
target-response pairs; stuff-object.r accounted for 16.33%, and containing.r accounted for 1.48%.
As with the part-whole task, responses in the nearby objects task that were not relevant for the next-
to.r relation usually fell under the generic spatial containment relation. In the object-nearby object task,
the next-to.r relation was the most frequently utilized relation label, accounting for 75.66% of all target-
response pairs labeled. The on-surface.r relation was the second most common relation, with 5.69%,
and containing.r accounted for 4.44% of all labeled target-response pairs
4 Automatic filtering undesirable data
Manual processing of the data is a time-consuming and expensive approach. As a result, we are inves-
tigating different automatic techniques to filter out the undesirable responses from AMT, using current
manually annotated data as a gold standard for evaluation of automatic approaches.
4.1 WordNet Similarity measures
In the first approach, we computed some lexical similarity scores for the target and the response items
based on the followingWN similarity measures. (It should be noted that not all of the target and responses
were present in WN. For such words, we used their nearest hypernyms).
WN Path Distance Similarity between each target word and each received response for that target
word. This score denotes how similar two word senses are, based on the shortest path that connects
the senses in the is-a (hypernym/hypnoym) taxonomy. We selected the maximum similarity score of
different senses of the target and the respond words.
Resnik Similarity between each target word and each of the received responses for that target word.
This score denotes how similar the two word senses are, based on the Information Content (IC) of the
Least Common Subsumer (most specific ancestor node) (Resnik, 1999).
The Average Pairwise Similarity Score which is computed based on WN path distance similarity
score. If we assumeW1,W2...Wn to be n responses for target word T; and Sij to be theWN path distance
similarity between Wi and Wj , then the average pairwise similarity score for Wi will be Si1+Si2+...+Sinn .
This will provide us the average similarity of each response (i.e Wi) with the other responses (i.e. Wj
382
so that i6= j). In this way we will reward the responses that are more semantically related to each other
(regardless of their similarity to the target word).
The WN Matrix Similarity which is a bag of words similarity matrix based on WN path distance
similarities. For target word T we have the following similarity matrix:
1 + S12 + ...+ S1n
S21 + 1 + ...+ S2n
...
Sn1 + Sn2 + ...+ Snn
In this matrix row i is the similarity vector of Wi represented as ~Vi = [Si1 + Si2 + ...+ Sin]. We
use cosine similarity to calculate the similarity measure of two words. So, the similarity measure of
Wi and Wj is the cosine of ~Vi and ~Vj and is computed by CSij = cos(?) = Vi.Vj||Vi||.||Vj || . Then the WN
matrix similarity score of Wi will be CSi1+CSi2+...+CSinn . The more two words are semantically related
to similar set of words, the higher cosine similarity they will have. If a word is related to many different
words in the set, it will obtain higher WN matrix similarity score.
4.2 Corpus association measures
The next approach for filtering the raw data was finding association measures of target-response pairs
using Google?s 1-trillion 5-gram web corpus (LDC2006T13), by counting the frequency of each target
and response word in unigram and bigram portions of the corpus and then the number of times the two
words co-occur within a +/- 4-word window in the 5-gram portion of the corpus. We also computed the
sentential co-occurrences of each target-response pair (i.e. the number of sentences in which the target
or the response words appear and the number of sentences in which both words occur together) on the
English Gigaword corpus (LDC2007T07) which is a 1 billion word corpus of articles marked up from
English press texts (mainly the New York Times). Based on these counts, we used log-likelihood and
log-odds ratio (Dunning, 1993) to compute the association between the two words.
4.3 Discussion and evaluation of automatic filtaration techniques
The collected responses of each AMT task were ranked separately by each of the above similarity and
association measures. We classify the ranked responses into ?keep? (higher-scoring) and ?reject? (lower-
scoring) classes by defining a specific threshold for each list. Then we evaluated the accuracy of each
filtration approach by computing their precision and recall on correct ?keep? items (see table 2). In this
table the baseline score shows the accuracy of the responses of each AMT task before using automatic
filtration techniques. It should be added that collecting data by using AMT is rather cheap and fast, so
we are more interested in higher precision (achieving highly accurate data) than higher recall. Lower
recall means we lose some data, which is not too expensive to collect.
Baseline Log-likelihood Log-odds WN Path Dist sim Resnik sim WN Pairwise sim WN Matrix sim
Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec
LOC 0.5527 1.0 0.7832 0.6690 0.7851 0.6684 0.5624 0.9724 0.5674 0.9784 0.6115 0.3657 0.4832 1.0
PAR 0.7887 1.0 0.7921 0.4523 0.8321 0.5022 0.8073 1.0 0.8234 1.0 0.9045 0.2859 0.9010 0.2516
OBJ 0.8934 1.0 0.9015 1.0 0.9286 0.9144 0.9123 1.0 0.9185 1.0 0.9855 0.3215 0.8925 1.0
Table 2: The accuracy of automatic filtering approaches
As can be seen in table 2, within the object-location data set, we gained the best precision (0.7832) by
using log-odds with relatively high recall (0.6690). Target-response pairs that were approved or rejected
contrary to automatic predictions were due primarily to the specificity of the response location.
In the part-whole task, the best precision (0.9010) was achieved by using WN matrix similarities
but again we lost a noticeable portion of data (recall= 0.2516). Rejected target-response pairs from the
higher-scoring part-whole set were often due to responses that named attributes, rather than parts, of
the target item (e.g. croissant - flaky). Many responses were too general (e.g, gong - material). Many
target-response pairs would have fallen under the next-to.r relation rather than any of the meronymic
383
relations. The majority of the approved target-response pairs from the lower-scoring part-whole set were
due to obvious, ?common sense responses that would usually be inferred rather than explicitly stated,
particularly body parts (e.g, bunny - brain).
The baseline accuracy of the nearby objects task is quite high (precision=0.8934, recall=1.0), and
we gain the best precision by using WN average pairwise similarity (0.9855) by removing lower-scoring
part of AMT responses (recall=0.3215). The high precision in all automatic techniques is due primarily
to the fact that the open-ended nature of the task resulted in a large number of target-response pairs that,
while not pertinent to the next-to.r relation, could be labeled by other relations. Again, the open-ended
nature of the nearby objects task resulted in the lowest percentage of rejected high-scoring pairs.
5 Conclusions
In this paper, we investigated the use Amazon?s Mechanical Turk for collecting semantic information for
a portion of our lexical knowledge resource. Manual evaluation of the AMT responses (baseline results
in table 2) confirms that we can collect highly accurate data in a cheap and efficient way by using AMT.
The accuracy of automatic filtration techniques sounds promising as we were able to filter out some
undesirable data, most of the time without loosing so much of collected responses.
Overall, we have shown a method which is very good in collecting semantic information and some
other methods which are very good at filtering out word pairs that are undesirable in this particular context
(i.e locations, nearby object and parts of our object library). This approach seems to have the potential
to be extended for more contexts. For the future work, we are planning to apply this methodology to
collect semantic information about action verbs, such as information about the locations of the action,
the participants, their relation to each other, the background objects and so on.
References
Callison-Burch, C. and M. Dredze (2010). Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, Los Angeles, CA, USA, pp. 1?12.
Coyne, B., O. Rambow, J. Hirschberg, and R. Sproat (2010). Frame semantics in text-to-scene gen-
eration. In R. Setchi, I. Jordanov, R. Howlett, and L. Jain (Eds.), Knowledge-Based and Intelligent
Information and Engineering Systems, Volume 6279 of Lecture Notes in Computer Science, pp. 375?
384. Springer Berlin / Heidelberg.
Coyne, B. and R. Sproat (2001). Wordseye: An automatic text-to-scene conversion system. In Proceed-
ings of the 28th annual conference on Computer graphics and interactive techniques, Los Angeles,
CA, USA, pp. 487? 496.
Coyne, B., R. Sproat, and J. Hirschberg (2010). Spatial relations in text-to-scene conversion. In Compu-
tational Models of Spatial Language Interpretation, Workshop at Spatial Cognition 2010, Mt. Hood,
OR, USA, pp. 9?16.
Dunning, T. E. (1993). Accurate methods for the statistics of surprise and coincidence. Computational
Linguistics 19(1), 61?74.
Priss, U. (1996). Classification of meronymy by methods of relational concept analysis. In Online
proceedings of the 1996 Midwest Artificial Intelligence Conference, Bloomington, IN, USA.
Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application
to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 95?130.
Sproat, R. (2001). Inferring the environment in a text-to-scene conversion system. In Proceedings of The
First International Conference on Knowledge Capture, Victoria, BC, Canada, pp. 147?154.
384
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 46?50,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Detecting linguistic idiosyncratic interests in autism
using distributional semantic models
Masoud Rouhizadeh
?
, Emily Prud?hommeaux
?
, Jan van Santen
?
, Richard Sproat
?
?
Center for Spoken Language Understanding, Oregon Health & Science University
?
Center for Language Sciences, University of Rochester
?
Google, Inc.
{rouhizad,vansantj}@ohsu.edu, emilypx@gmail.com, rws@xoba.com
Abstract
Children with autism spectrum disorder
often exhibit idiosyncratic patterns of be-
haviors and interests. In this paper, we fo-
cus on measuring the presence of idiosyn-
cratic interests at the linguistic level in
children with autism using distributional
semantic models. We model the semantic
space of children?s narratives by calculat-
ing pairwise word overlap, and we com-
pare the overlap found within and across
diagnostic groups. We find that the words
used by children with typical development
tend to be used by other children with typ-
ical development, while the words used
by children with autism overlap less with
those used by children with typical devel-
opment and even less with those used by
other children with autism. These findings
suggest that children with autism are veer-
ing not only away from the topic of the
target narrative but also in idiosyncratic
semantic directions potentially defined by
their individual topics of interest.
1 Introduction
Autism spectrum disorder (ASD) is a neurode-
velopmental disorder characterized by impaired
communication and social behavior. One of the
core deficits associated with ASD is an intense
preoccupation with a restricted set of interests
(American Psychiatric Association, 2000; Amer-
ican Psychiatric Association, 2013), which can of-
ten be observed in an individual?s tendency to per-
severate on specific, idiosyncratic topics of con-
versation. Because this symptom is explicitly
mentioned among the diagnostic criteria for ASD
used in the DSM-IV and DSM-5, many diagnos-
tic instruments (Lord et al., 2002; Rutter et al.,
2003) require a qualitative assessment of this phe-
nomenon. Instances of perseveration on a partic-
ular topic in the spontaneous spoken language of
children with ASD, however, are not typically ex-
plicitly counted in a clinical setting, making com-
parisons with typically developing children diffi-
cult to quantify.
Expert manual analysis of conversations and
narratives of individuals with ASD has shown that
children and teenagers with autism include signif-
icantly more bizarre and irrelevant content in their
narratives (Loveland et al., 1990; Losh and Capps,
2003) and introduce more abrupt topic changes in
their conversations (Lam et al., 2012) than their
typically developing peers. Automatic detection
of poor topic maintenance has also been explored
using techniques originally developed for infor-
mation extraction (Rouhizadeh et al., 2013). There
has been little work, however, in annotating the
precise direction of the departure from a target
topic. Thus, it is not clear whether children with
ASD are instigating similar topic changes or pur-
suing idiosyncratic directions in their narratives
and conversations consistent with their restricted
interests.
In this paper, we attempt to automatically iden-
tify topic changes and idiosyncratic interests ex-
pressed in the language of children with ASD
by measuring the semantic similarity of narrative
retellings produced by children with and without
ASD. We first use word overlap measures to cal-
culate the semantic similarity between every pos-
sible pair of narratives. We then build three pair-
wise comparison matrices: one comparing pairs of
typically developing (TD) children; one compar-
ing pairs of children with ASD; and a third com-
46
paring pairs consisting of one child with ASD and
one child with TD. We calculate the significance
of the differences between the pairs in the three
matrices using the Monte Carlo method to shuffle
the diagnosis label of each child.
We find that TD children share the greatest
word overlap with one another, while children
with ASD have significantly less word overlap
with TD children and even less word overlap with
other ASD children. These results indicate that
TD children tend to adhere to the target topic in
the narrative retellings, while children with ASD
often stray from the target topic. Furthermore,
the fact that the word choices of an individual
child with ASD seem not to resemble the word
choices of other children with ASD suggests that
when a child with ASD chooses to abandon the
target topic, he or she does so in an idiosyncratic
way. Although these results are only indirect in-
dications of the presence of restricted interests,
the work presented here highlights the potential of
computational language analysis methods for im-
proving our understanding of the social and lin-
guistic deficits associated with the disorder.
2 Data
Participants in this study included 39 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was di-
agnosed via clinical consensus according to the
DSM-IV-TR criteria (American Psychiatric Asso-
ciation, 2000) and the established threshold scores
on two diagnostic instruments: the Autism Di-
agnostic Observation Schedule (ADOS) (Lord et
al., 2002), a semi-structured series of activities de-
signed to allow an examiner to observe behaviors
associated with autism; and the Social Communi-
cation Questionnaire (SCQ) (Rutter et al., 2003),
a parental questionnaire. None of the children
in this study met the criteria for a language im-
pairment, and there were no significant between-
group differences in age (mean=6.3) or full-scale
IQ (mean=115.5).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Kork-
man et al., 1998), a large and comprehensive bat-
tery of tasks that test neurocognitive functioning in
children. The NEPSY Narrative Memory (NNM)
subtest is a narrative retelling test in which the sub-
ject listens to a brief narrative about a boy and his
dog and then must retell the narrative to the ex-
aminer. Under standard administration, the NNM
free recall score is calculated by counting how
many from a set of 17 story elements were used
in a retelling. Following the free recall portion of
the test is the cued recall task, in which the ex-
aminer then asks the subject to provide answers to
questions about all of the story elements that were
omitted in the retelling.
The NNM was administered to each participant
in the study, and each participant?s retelling was
recorded and transcribed. The responses for the
cued recall portion of the subtest were not in-
cluded in this work presented here. There was no
significant difference between the two diagnostic
groups in the standard NNM free recall score.
3 Methods
We expect that two different retellings of the same
source will lie in the same lexico-semantic space.
As a result, they should include high percentage
of overlapping words. When a pair of retellings
has a low word overlap measure, it could be that
one or both retellings include intrusions from un-
related topics. An alternative explanation is that
the subjects recalled a non-overlapping set of story
elements or simply a small set of story elements.
However, since we did not find any significant dif-
ference between the TD and ASD groups in the
standard narrative recall score, we infer that a low
percentage of word overlap indicates a difference
in topic between the two retellings.
3.1 Word overlap measures
In order to calculate the similarity between a pair
of narratives i and j, we use type and token over-
lap measures based on the Jaccard similarity coef-
ficient. Token similarity is defined as the size of
intersection of the words (i.e., the actual number
of tokens in common) in narratives i and j relative
to the size of the union of the words in the two
narratives (i.e., summing over all tokens in both
narratives, the maximum number of instances of
that token in either narrative). Type similarity is
defined as the size of intersection of the types (i.e.,
unique words) in narratives i and j relative to the
size of the union of the types in the two narratives.
For instance, for the following set of words i and
j:
i = {a, b, c, d, c}
j = {a, c, e, c, a, a},
the token intersection is equal to {a, c, c} and
47
Group Means
TD.TD TD.ASD ASD.ASD
Type Overlap .23 .17 .13
Token Overlap .19 .14 .11
Table 3: Word overlap pairwise group means
the token union is {a, a, a, c, c, b, e, d}. The token
overlap similarity between the two sets i and j is
therefore 3/8. The type intersection of i and j is
equal to {a, c} and the type union is {a, c, b, e, d},
yielding a type overlap similarity of 2/5.
3.2 Pairwise similarity matrix
We next build a similarity matrix for the type and
token overlap measures, comparing every possi-
ble pair of children. Every child in the TD and
ASD groups is compared to the children in his own
group (TD.TD and ASD.ASD), as well as the chil-
dren in the other group (TD.ASD). The pairwise
similarity matrix is diagonally symmetrical, and
we thus consider only the top right section of the
matrix above the diagonal in our analysis.
3.3 Monte Carlo permutation
Since we may not have enough information to
make an assumption that the pairwise similarity
measures of all children are from a particular dis-
tribution, we utilize a non-parametric procedure,
the Monte Carlo permutation approach, which is
widely used in non-standard significance testing
situations.
Given the three sub-matrices in the similarity
matrix described above (TD.TD, TD.ASD, and
ASD.ASD), we first calculate for each pair of sub-
matrices (e.g., TD.TD vs ASD.ASD) three statis-
tics that compare all cells in one submatrix with
the cells in other submatrices: the difference be-
tween the means, t-statistics (using the Welch
Two Sample t-test), and w-statistics (using the
Wilcoxon rank sum test). We label these observed
values observed-mean, observed-t, and observed-
w. We next take a large random sample with re-
placement from all possible permutations of the
data by shuffling the diagnosis labels of the chil-
dren 1000 times, and then calculate each of the
three above statistics for each shuffle. Finally, we
determine the number of times the observed values
exceed the values generated by the 1000 shuffles.
4 Results
The comparison of the group means of each of
the three sub-matrices described in Section 3.2
show that TD children have the greatest overlap
with each other; children with ASD have less
word overlap with TD children than TD children
have with one another and even less word over-
lap with other ASD children. The group means
of both type and token overlap are summarized
in Table 3. In addition, examples of overlapping
and non-overlapping terms between the groups are
provided in Tables 1 and 2 respectively.
The level plot of the pairwise token overlap
is shown in figure 1. We see that the TD.TD
sub-matrix has the lightest color, indicating higher
overlap, followed by TD.ASD. The ASD.ASD
submatrix has the darkest color, indicating low
word overlap.
In the next step, we determine the significance
of the group mean differences. As described in
Section 3.3, using the Monte Carlo permutation to
test the significance of the following comparisons:
TD.TD vs ASD.ASD, TD.TD vs TD.ASD, and
TD.ASD vs ASD.ASD. The results of these signif-
Group Top 10 overlapping words
TD.TD shoe, tree, climb, ladder, fall, Pepper, Jim, dog, sister, branch
TD.ASD shoe, tree, Jim, climb, dog, ladder, Pepper, fall, branch, sister
ASD.ASD shoe, tree, Jim, dog, climb, Pepper, ladder, branch, boy, run
Table 1: Top 10 overlapping words between the groups
Group Examples of non-overlapping words
TD.TD coconut, couch, jew, lie, picture, spike, stuff, t-rex, tight, watch
TD.ASD arm, bottom, cousin, doctor, eat, fruit, giant, meat, push, sense
ASD.ASD bite, bridge, crunch, donut, gadget, lizard, microphone, sell, table, vision
Table 2: Examples of non-overlapping words between the groups
48
??
??
??
Figure 1: Level plot of the pairwise token overlap
(lighter colors indicate higher overlap)
icance tests are summarized in table 4, and in all
cases the differences are significant at p < 0.05.
5 Conclusions and future work
The methods presented for comparing the lexical
choices made by children with and without ASD
while generating a narrative retelling demonstrate
the utility of language analysis for revealing diag-
nostically interesting information. The low rates
of word overlap between retellings produced by
children with ASD and those produced by typi-
cally developing children suggest that the children
with ASD are having difficulty maintaining the
target topic. Furthermore, the low overlap between
pairs of children with ASD suggests that children
with ASD are not straying from the topic in sim-
ilar ways but are instead exploring topics that are
of idiosyncratic interest.
These findings can be potentially used for
diagnostic purposes in combinations of other
applications of speech and language process-
ing for automated narrative retelling assessment
(Lehr et al., 2013), detection of off-topic words
(Rouhizadeh et al., 2013), and pragmatic deficits
(Prud?hommeaux and Rouhizadeh, 2012). From a
clinical standpoint, diagnostic measures utilizing
these methods for automated evaluation of disor-
dered language could be very useful in diagnosis
and planning interventions.
One major focus of our future work will be to
manually annotate the narrative retellings used in
this study to determine the frequency of topic de-
partures and the nature of these departures. Given
the vocabulary differences seen here, we expect
to find not only that children with ASD are aban-
doning the topic of the source narrative more fre-
quently than children with typical development
but also that the topics they choose to pursue are
related to their own individual specific interests.
A second area we hope to explore is the use
of external resources, such as WordNet, to ex-
pand the set of terms used to calculate word over-
lap. It is perfectly reasonable to expect that people
will use synonyms and paraphrases in their narra-
tive retellings. It is therefore possible that chil-
dren with autism are discussing the appropriate
topic but choosing unusual words within that topic
space in their retellings, which could be consis-
tent with the type of atypical language often ob-
served in children with ASD. By considering se-
mantic overlap rather than simple word overlap,
we may be able to distinguish instances of atypical
language from true examples of poor topic main-
tenance.
Third, we are also interested in applying the
analysis described above to a set of retellings from
seniors with and without mild cognitive impair-
ment, a frequent precursor to dementia. Like chil-
dren with ASD, seniors with dementia are also
more likely to include irrelevant information in
overlap statistic
p-values
TD.TD vs ASD.ASD TD.TD vs TD.ASD TD.ASD vs ASD.ASD
Type Overlap
Means .004 .042 .008
t.test .009 .012 .008
Wilcoxon test .004 .002 .002
Token Overlap
Means .012 .034 .028
t.test .014 .022 .022
Wilcoxon test .012 .002 .002
Table 4: Monte Carlo significance test results
49
their narrative retellings. These intrusions, how-
ever, are often informed by real-world knowledge,
and thus may not result in a decrease in measures
of word overlap with narratives produced by unim-
paired individuals.
Finally, we plan to apply our methods to the out-
put of an automatic speech recognition (ASR) sys-
tem rather than manual transcripts. Although the
ASR output is likely to contain many errors, the
fact that our methods focus on content words may
make them robust to the sorts of function word
recognition errors typically produced by ASR sys-
tems.
Acknowledgments
This work was supported in part by NSF grant
#BCS-0826654, and NIH NIDCD grants #R01-
DC007129 and #1R01DC012033-01. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of
the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washing-
ton, DC.
American Psychiatric Association. 2013. Diagnostic
and statistical manual of mental disorders (5th ed.).
American Psychiatric Publishing, Washington, DC.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological as-
sessment. The Psychological Corporation, San An-
tonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic lan-
guage deficits in children with high-functioning
autism: Depicting the phenotype using the prag-
matic rating scale. Research in Autism Spectrum
Disorders, 6(2):792?797.
Maider Lehr, Izhak Shafran, Emily Prud?hommeaux,
and Brian Roark. 2013. Discriminative joint model-
ing of lexical variation and acoustic confusion for
automated narrative retelling assessment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability in
high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental
Disorders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tu-
nali. 1990. Narrative story telling in autism and
down?s syndrome. British Journal of Developmen-
tal Psychology, 8(1):9?23.
Emily Prud?hommeaux and Masoud Rouhizadeh.
2012. Automatic detection of pragmatic deficits
in children with autism. In Proceedings of the
3rd Workshop on Child, Computer and Interaction
(WOCCI).
Masoud Rouhizadeh, Emily Prud?hommeaux, Brian
Roark, and Jan van Santen. 2013. Distributional
semantic models for the evaluation of disordered
language. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
50
