Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 833?840
Manchester, August 2008
Prediction of Maximal Projection for Semantic Role Labeling
Weiwei Sun
?
, Zhifang Sui
Institute of Computational Linguistics
Peking University
Beijing, 100871, China
{ws, szf}@pku.edu.cn
Haifeng Wang
Toshiba (China) R&D Center
501, Tower W2, Oriental Plaza
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
In Semantic Role Labeling (SRL), argu-
ments are usually limited in a syntax sub-
tree. It is reasonable to label arguments lo-
cally in such a sub-tree rather than a whole
tree. To identify active region of argu-
ments, this paper models Maximal Pro-
jection (MP), which is a concept in D-
structure from the projection principle of
the Principle and Parameters theory. This
paper makes a new definition of MP in S-
structure and proposes two methods to pre-
dict it: the anchor group approach and the
single anchor approach. The anchor group
approach achieves an accuracy of 87.75%
and the single anchor approach achieves
83.63%. Experimental results also indicate
that the prediction of MP improves seman-
tic role labeling.
1 Introduction
Semantic Role Labeling (SRL) has gained the in-
terest of many researchers in the last few years.
SRL consists of recognizing arguments involved
by predicates of a given sentence and labeling their
semantic types. As a well defined task of shallow
semantic parsing, SRL has a variety of applications
in many kinds of NLP tasks.
A variety of approaches has been proposed
for the different characteristics of SRL. More re-
cent approaches have involved calibrating features
(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;
?
This work was partial completed while this author was at
Toshiba (China) R&D Center.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Pradhan et al, 2005), analyzing the complex input
? syntax trees (Moschitti, 2004; Liu and Sarkar,
2007), exploiting the complicated output ? the
predicate-structure (Toutanova et al, 2005), as
well as capturing paradigmatic relations between
predicates (Gordon and Swanson, 2007).
In prior SRL methods, role candidates are ex-
tracted from a whole syntax tree. Though sev-
eral pruning algorithms have been raised (Xue and
Palmer, 2004), the policies are all in global style.
In this paper, a statistical analysis of Penn Prop-
Bank indicates that arguments are limited in a local
syntax sub-tree rather than a whole one. Prior SRL
methods do not take such locality into account and
seek roles in a wider area. The neglect of local-
ity of arguments may cause labeling errors such
as constituents outside active region of arguments
may be falsely recognized as roles.
This paper uses insights from generative lin-
guistics to guide the solution of locality of argu-
ments. In particular, Maximal Projection (MP)
which dominates
1
active region of arguments ac-
cording to the projection principle of principle and
parameters. Two methods, the anchor group ap-
proach and the single anchor approach, are pro-
posed to find the active sub-tree which is rooted by
MP and covers all roles. The solutions put forward
in this paper borrow ideas from NP-movement
principle in generative linguistics and are in statis-
tical flavor. The anchor group approach achieves
an accuracy of 87.75%, and the single anchor ap-
proach achieves 83.63%. Though the accuracy is
lower, the single anchor approach fits SRL better.
1
Dominate is an concept in X-bar theory are modeled. As-
suming ? and ? are two nodes in a syntax tree: ? dominates
? means ? is ancestor of ?.
833
Figure 1: A sentence from WSJ test corpus of CoNLL-2005 shared task
2 Maximal Projection and Its
Government of Arguments
2.1 Maximal Projection
Principle and parameters theory is a framework of
generative grammar. X-bar theory, as a module
of principle and parameters, restricts context-free
phrase structure rules as follows:
1. a phrase always contains a head of the same
type, i.e. NPs Ns, VPs Vs, PPs Ps, etc.
2. XP(X?) ? specifier X?
3. X??X complement(s)
These structural properties are conventionally rep-
resented as shown in figure 2.
Figure 2: X-bar structure
X is the head of the phrase XP. X? and XP(X?)
are called projections of X. The head is also called
the zero projection. X-bar structure is integrated
with the properties of lexical items via the Projec-
tion Principle of principle and parameters. This
principle is summed up as the properties of lexi-
cal information project onto the syntax of the sen-
tence. For instance:
? Sue likes Picasso
? *Sue likes
The subcategorization frame of the lexical item
like [ ,NP] ensures that the verb is followed by an
NP and the second sentence is of ungrammatical
form.
Maximal Projection (MP) is the constituent
which is projected to the highest level of an X-bar
structure from lexical entities and is therefore the
top node XP of the X-bar structure.
Take figure 1 for instance, S is the MP of the
predicate come. Though the syntax tree is not in D-
structure (deep structure), the S-structure (surface
structure) headed by come is similar to its genuine
D-structure. In a latter part of this section, a spe-
cific definition of MP in S-structure will be given
for application.
2.2 MP Limits Active Region of Arguments
MP holds all lexical properties of heads. In partic-
ular, the MP of a predicate holds predicate struc-
ture information and the constituents out of its do-
main cannot occupy argument positions. ?-theory
and government are two modules of principle and
parameters. They both suggest that the possi-
ble positions of semantic roles are in the sub-tree
rooted by MP.
834
Concerning assignment of semantic roles to
constituents, ?-theory suggests that semantic roles
are assigned by predicates to their sisters (Chom-
sky, 1986). Furthermore, in a X-bar theory, com-
plements are assigned semantic roles by the pred-
icate and specifiers get roles from the V?. In both
situations the process of roles assignment is in sis-
terhood condition and limited in the sub-structure
which is dominated by the MP. Only constituents
under MP can get semantic roles. The Case As-
signment Principle also points out: Case is as-
signed under government (Chomsky, 1981). Take
figure 1 for instance, only NP-1 and PP-2 can get
semantic roles of the head come.
From generative linguists? point, MP limits sub-
tree of arguments. Therefore, finding the MP is
equivalent to finding the active region of predicate
structure.
2.3 Definition of MP in S-structure
Though a clear enough definition of MP in D-
structure has been previously illustrated, it is still
necessary to define a specific one in S-structure
for application, especially for automatic parsing
which are not exactly correct. This paper de-
fines MP in S-structure (hereinafter denote MP
for short) as following: for every predicate p in the
syntax tree T , there exists one and only one MP
mp s.t.
1. mp dominates all arguments of p;
2. all descendent nodes of mp don?t satisfy the
former condition.
Due to its different characteristics from argu-
ments, adjunct-like arguments are excluded from
the set of arguments in generative grammar and
many other linguistic theories. For this reason, this
paper does not take them into account.
For gold syntax tree, there exists a one-to-one
mapping between arguments and nodes of syn-
tax trees, whereas automatic syntactic parsing con-
tains no such mapping. This paper do not take
arguments which cannot get corresponding con-
stituents into account to reduce the influence of au-
tomatic parsing error.
Take the sentence of figure 1 to illustrate our
definition of MP: S is MP of come since NP-1 and
PP-2 are arguments of it. There is no node map-
ping to the argument Wall Street professionals in
the parsing tree. Instead of covering argument?s
fragments, we simply take it PP-4 as MP.
2.4 Using MP Information in SRL
The boundaries of a predicate structure are two
word positions of the sentence. It is difficult to
model these two words. On the contrary, MP, as
one ancestor of predicate, has a clear-cut meaning
and is ideal for modeling. In this paper, the pol-
icy to predict MP rather than two word positions is
carried out to deal with locality of arguments.
Automatic prediction of MP can be viewed as a
preprocessing especially a pruning preprocessing
for SRL. Given a sentence and its parsing, SRL
systems can take seeking the active sub-tree rooted
by MP as the first step. Then SRL systems can
work on the shrunk syntax tree, and follow-up la-
beling processes can be in a various form. Most
of previous SRL methods still work without spe-
cial processing. Take figure 1 for example: when
labeling include, as the MP is PP-4, just NP-7 will
be extracted as argument candidate.
3 Analysis of Locality of Arguments
Principle and parameters suggests that MP bounds
arguments. Additionally, a statistical analysis
shows that possible positions of arguments are lim-
ited in a narrow region of syntax tree. An opposite
experiment also shows that MP information is use-
ful for SRL.
3.1 Data and Baseline System
In this paper, CoNLL-2005 SRL shared task
data (Carreras and M`arquez, 2005) is used as cor-
pus. The data consists of the Wall Street Jour-
nal (WSJ) part of the Penn TreeBank with infor-
mation on predicate argument structures extracted
from the PropBank corpus. In addition, the test
set of the shared task includes three sections of the
Brown corpus. Statistical analysis is based on sec-
tion 02-21 of WSJ. Experiments are conducted on
WSJ and Brown corpus. As defined by the shared
task, section 02-21 of PropBank are used for train-
ing models while section 23 and Brown corpus are
used for test. In terms of syntax information, we
use Charniak parser for POS tagging and full pars-
ing.
A majority of prior SRL approaches formulate
the SRL propblem as a multi-class classification
propblem. Generally speaking, these SRL ap-
proaches use a two-stage architecture: i) argument
identification; ii) argument classification, to solve
the task as a derivation of Gildea and Jurafsky?s
pioneer work (Gildea and Jurafsky, 2002). UIUC
835
Precision Recall F
?=1
Arg0 86.28% 87.01% 86.64
Arg1 79.37% 75.06% 77.15
Arg2 69.48% 62.97% 66.07
Arg3 69.01% 56.65% 62.22
Arg4 72.64% 75.49% 74.04
Table 1: SRL performance of UIUC SRLer
Precision Recall F
?=1
Arg0 91.84% 89.98% 90.90
Arg1 81.73% 75.93% 78.72
Arg2 69.86% 63.06% 66.29
Arg3 71.13% 58.38% 64.13
Arg4 73.08% 74.51% 73.79
Table 2: SRL performance of UIUC SRLer using
information of gold MP
Semantic Role Labeler
2
(UIUC SRLer) is a state-
of-the-art SRL system that based on the champion
system of CoNLL-2005 shared task (Carreras and
M`arquez, 2005). It is utilized as a baseline system
in this paper. The system participated in CoNLL-
2005 is based on several syntactic parsing results.
However, experiments of this paper just use the
best parsing result from Charniak parser. Param-
eters for training SRL models are the same as de-
scribed in (Koomen, 2005).
3.2 Active Region of Arguments
According to a statistical analysis, the average
depth from a target predicate to the root of a syntax
tree is 5.03, and the average depth from a predicate
to MP is just 3.12. This means about 40% of an-
cestors of a predicate do not dominate arguments
directly. In addition, the quantity of leaves in syn-
tax tree is another measure to analyze the domain.
On average, a syntax tree covers 28.51 leaves, and
MP dominates only 18.19. Roughly speaking, only
about 60% of words are valid for semantic roles.
Statistics of corpora leads to the following conclu-
sion: arguments which are assigned semantic roles
are in a local region of a whole syntax tree.
3.3 Typical Errors Caused by Neglect of
Locality of Arguments
The neglect of the locality of arguments in prior
SRL methods shows that it may cause errors.
Some constituents outside active region of argu-
ments may be falsely labeled as roles especially for
those being arguments of other predicates. A sta-
tistical analysis shows 20.62% of falsely labeled
arguments are constituents out of MP domain in
labeling results of UIUC SRLer. Take figure 1 for
instance, UIUC SRLer makes a mistake when la-
beling NP-1 which is Arg1 of the predicate come
for the target include; it labels Arg0 to NP. In fact,
the active region of include is the sub-tree rooted
2
http://l2r.cs.uiuc.edu/ cogcomp/srl-demo.php
by PP-4. Since NP-1 is an argument of another
predicate, some static properties of NP-1 make it
confusing as an argument.
3.4 SRL under Gold MP
If MP has been found before labeling semantic
roles, the set of role candidates will be shrunk,
and the capability to identify semantic roles may
be improved. An opposite experiment verifies this
idea. In the first experiment, UIUC SRLer is re-
trained as a baseline. For comparison, during the
second experiment, syntax sub-trees dominated by
gold MP are used as syntactic information. Both
training and test data are preprocessed with gold
MP information. That is to say we use pruned data
for training, and test is conducted on pruned syntax
sub-trees.
Table 1 and 2 show that except for Arg4, all ar-
guments get improved labeling performance, espe-
cially Arg0. Since arguments except for Arg0 are
realized as objects on the heel of predicate in most
case, the information of MP is not so useful for
them as Arg0. The experiment suggests that high
performance prediction of MP can improve SRL.
4 Prediction of MP
Conforming to government and ?-theory, MP is
not too difficult to predict in D-structure. Unfor-
tunately, sentences being looked at are in their sur-
face form and region of arguments has been ex-
panded. Simple rules alone are not adequate for
finding MP owing to a variety of movement be-
tween D-structure and S-structure. This paper de-
signs two data driven algorithms based on move-
ment principles for prediction of MP.
4.1 NP-movement and Prediction of MP
4.1.1 NP-movement in Principle and
Parameters
The relationship between D-structure and S-
structure is movement: S-structure equals D-
836
structure plus movement. NP-movement prin-
ciple in principle and parameters indicates that
noun phrases only move from A-positions (argu-
ment position) which have been assigned roles
to A-positions which have not, leaving an NP-
trace. On account of ?-theory and government, A-
positions are nodes m-commanded
3
by predicates
in D-structure. In NP-movement, arguments move
to positions which are C-commanded
4
by target
predicate and m-commanded by other predicates.
Broadly speaking, A-positions are C-commanded
by predicates after NP-movement. The key of the
well-known pruning algorithm raised in (Xue and
Palmer, 2004) is extracting sisters of ancestors as
role candidates. Those candidate nodes are all C-
commanders of a predicate. NP-movement can
give an explanation why the algorithm works.
4.1.2 Definition of Argument Anchor
To capture the characteristics of A-positions, we
make definition of A-anchor as following. For ev-
ery predicate p in the syntax tree T , denote A the
set of C-commanders of p:
? a left-A-anchor satisfies:
1. left-A-anchor belongs to A;
2. left-A-anchor is a noun phrase (includ-
ing NNS, NNP, etc.) or simple declara-
tive clause (S);
3. left-A-anchor is on the left hand of p.
? a right-A-anchor satisfies:
1. right-A-anchor belongs to A;
2. right-A-anchor is a noun phrase (includ-
ing NNS, NNP, etc.);
3. right-A-anchor is on the right hand of p.
Take figure 1 for example, NP-1, NP-4 and NP-
6 are left-A-anchors of include, and no right-A-
anchor. There is a close link between A-position
and the A-anchor that we defined, since A-anchors
occupy A-positions.
4.1.3 Anchor Model for Prediction of MP
Parents of A-anchors and first branching ances-
tor of the predicate can cover 96.25% of MP and
the number of those ancestors is 2.78 times of the
3
M-command is an concept in X-bar syntax. Assuming
? and ? are two nodes in a syntax tree: ? m-commands ?
means ? C-commands ? and the MP of ? dominates ?
4
C-command is an concept in X-bar theory. Assuming ?
and ? are two nodes in a syntax tree: ? C-commands ? means
every parent of ? is ancestor of ?.
number of MP. The number of all ancestors is 6.65
times. The data suggests that taking only these
kinds of ancestors as MP candidates can shrink the
candidate set with a relatively small loss.
4.2 Anchor Group Approach
MP is one ancestor of a predicate. An natural ap-
proach to predict MP is searching the set of all
ancestors. This idea encounters the difficulty that
there are too many ancestors. In order to reduce
the noise brought by non-anchors? parents, the an-
chor group approach prunes away useless ances-
tors which are neither parents of A-anchors nor
first branching node upon predicate from MP can-
didate set. Then the algorithm scores all candidates
and chooses the MP in argmax flavor. Formally,
we denote the set of MP candidates C and the score
function S(.).
m?p = argmax
c?C
S(mp|c)
Probability function is chosen as score func-
tion in this paper. In estimating of the probability
P (MP |C), log-linear model is used. This model is
often called maximum entropy model in research
of NLP. Let the set {1,-1} denotes whether a con-
stituent is MP and ?(c, {?1, 1}) ? R
s
denotes
a feature map from a constituent and the possible
class to the vector space R
s
. Formally, the model
of our system is defined as:
m?p = argmax
c?C
e
<?(c,1),?>
e
<?(c,1),?>
+e
<?(c,0),?>
The algorithm is also described in pseudo code
as following.
Ancestor Algorithm:
1: collect parents of anchors and the first
branching ancestor, denote them set C
2: for every c ? C
3: calculate P (mp|c)
4: return c? that gets the maximal P (mp|c)
4.2.1 Features
We use some features to represent various as-
pects of the syntactic structure as well as lexical
information. The features are listed as follows:
Path The path features are similar to the path
feature which is designed by (Gildea and Jurafsky,
2002).A path is a sequential collection of phrase
tags. There are two kinds of path features here: one
is from target predicate through to the candidate;
the other is from the candidate to the root of the
syntax tree. For include in the sentence of figure 1,
the first kind of path of PP-2 is VBG+PP+NP+PP
and the second is PP+VP+S.
837
C-commander Thread As well as path features,
C-commander threads are other features which
reflect aspects of the syntactic structures. C-
commander thread features are sequential contain-
ers of constituents which C-command the target
predicate. We design three kinds of C-commander
threads: 1) down thread collects C-commanders
from the anchor to the target predicate; 2) up
thread collects C-commanders from the anchor to
the left/right most C-commander; 3) full thread
collects all C-commanders in the left/right direc-
tion from the target predicate. Direction is depen-
dent on the type of the anchor - left or right anchor.
Considering the grammatical characteristics of
phrase, we make an equivalence between such
phrase types:
? JJ, JJR, JJS, ADJP
? NN, NNP, NNS, NNPS, NAC, NX, NP
Besides the equivalent constituents, we discard
these types of phrases:
? MD, RB, RBS, RBR, ADVP
For include in figure 1, the up thread of
NP-4 is VBG+,+NP+NP; the down thread
is NP+IN+VBD+NP; the full thread is
VBG+,+NP+NP+IN+VBD+NP.
The phrase type of candidate is an important fea-
ture for prediction
Candidate of MP. We also select the rank num-
ber of the current candidate and the number of all
candidates as features. For the former example,
the two features for PP-2 are 2 and 3, since NP-
4 is the second left-A-anchor and there are three
A-anchors of include.
Anchor Features of anchor include the head
word of the anchor, the boundary words and their
POS, and the number of the words in the anchor.
Those features are clues of judgment of whether
the anchor?s position is an A-position.
Forward predicate For the former example, the
forward predicate of NP-4 is come. The features
include the predicate itself, the Levin class and the
SCF of the predicate.
predicate Features of predicate include lemma,
Levin class, POS and SCF of the predicate.
Figure 3: Flow diagram of the single anchor ap-
proach
Formal Subject An anchor may be formal sub-
ject. Take It is easy to say the specialist is not do-
ing his job for example, the formal subject will be
recognized as anchor of do. We use a heuristic rule
to extract this feature: if the first NP C-commander
of the anchor is ?it? and the left word of predicate
is ?to?, the value of this feature is 1; otherwise 0.
The Maximal Length of C-commanders Con-
stituent which consists of many words may be a
barrier between the predicate and an A-position.
For the former example, if the target predicate is
include, this feature of NP-1 is 2, since the largest
constituent NP-4 is made up of two words.
4.3 Single Anchor Approach
Among all A-anchors, the right most left-A-anchor
such as NP-6 of include in figure 1 is the most im-
portant one for MP prediction. The parent of this
kind of left-A-anchor is the MP of the predicate,
obtaining a high probability of 84.59%. The single
anchor approach is designed based on right most
left-A-anchor. The key of this approach is an ac-
tion prediction that when right most left-A-anchor
is found, the algorithm predicts next action to re-
turn which node of syntax tree as MP. There is
a label set of three types for learning ? here, up,
down. After action is predicted, several simple
rules are executed as post process of this predic-
tion: i) if there is no left-A-anchor, return the root
of the whole syntax tree as MP; ii)if the predicted
label is here, return the parent of right most left-
A-anchor; iii) if the predicted label is down, return
838
Prediction Accuracy
Corpus Action MP
WSJ ? 87.75%
Brown ? 88.84%
Table 3: Accuracy of the anchor group ap-
proach
Prediction Accuracy
Corpus Action MP
WSJ 88.45% 83.63%
Brown 90.10% 85.70%
Table 4: Accuracy of the single anchor ap-
proach
Precision Recall F
?=1
Arg0 86.23% 87.90% 87.06
Arg1 80.21% 74.79% 77.41
Arg2 70.09% 62.70% 66.19
Arg3 71.74% 57.23% 63.67
Arg4 74.76% 75.49% 75.12
Table 5: SRL performance of UIUC SRLer us-
ing information of predicted MP; the anchor
group approach; WSJ test corpus
Precision Recall F
?=1
Arg0 87.03% 87.59% 87.31
Arg1 80.24% 74.77% 77.41
Arg2 70.35% 63.06% 66.51
Arg3 71.43% 57.80% 63.90
Arg4 73.33% 75.49% 74.40
Table 6: SRL performance of UIUC SRLer us-
ing information of predicted MP; the single an-
chor approach; WSJ test corpus
the first branching node upon the predicate; iv) if
the predicted label is up, return the root. The ac-
tion prediction also uses maximum entropy model.
Figure 3 is the flow diagram of the single anchor
approach. Features for this approach are similar
to the former method. Features of the verb which
is between the anchor and the predicate are added,
including the verb itself and the Levin class of that
verb.
5 Experiments and Results
Experiment data and toolkit have been illustrated
in section 3. Maxent
5
, a maximum entropy model-
ing toolkit, is used as a classifier in the experiments
of MP prediction.
5.1 Experiments of Prediction of MP
The results are reported for both the anchor group
approach and the single anchor approach. Table 3
summaries the accuracy results of MP prediction
for the anchor group approach; table 4 summaries
results of both action prediction and MP prediction
for the single anchor approach. Both the anchor
group approach and the single anchor approach
have better prediction performance in Brown test
set, though the models are trained on WSJ cor-
pus. These results illustrate that anchor approaches
which are based on suitable linguistic theories have
robust performance and overcome limitations of
training corpus.
5
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
5.2 Experiments of SRL Using MP Prediction
Like the experiments in the end of section 3, we
perform similar experiments under predicted MP.
Both training and test corpus make use of predicted
MP information. It is an empirical tactic that pre-
dicted information of maximal projection, instead
of gold information, is chosen for a training set.
Experiments suggest predicted information is bet-
ter. Table 5 is SRL performance using the anchor
group approach to predict MP; Table 6 is SRL per-
formance using the single anchor approach.
Compared with table 1 on page 4, table 5 and
table 6 both indicate the predicted MP can help to
label semantic roles. However, there is an interest-
ing phenomenon. Even though the anchor group
approach achieves a higher performance of MP,
the single anchor approach is more helpful to SRL.
18.56% of falsely labeled arguments are out of MP
domain using the single anchor approach to predict
MP, compared to 20.62% of the baseline system.
In order to test robustness of the contribution
of MP prediction to SRL, another opposite exper-
iment is performed using the test set from Brown
corpus. Table 7 is the SRL performance of UIUC
SRLer on Brown test set. Table 8 is the corre-
sponding performance using MP information pre-
dicted by the single anchor approach. Comparison
between table 7 and table 8 indicates the approach
of MP prediction proposed in this paper adapts to
other genres of corpora.
Capability of labeling Arg0 gets significant im-
provement. Subject selection rule, a part of the-
839
Precision Recall F
?=1
Arg0 82.88% 85.51% 84.17
Arg1 66.30% 63.17% 64.70
Arg2 50.00% 45.58% 47.69
Arg3 0.00% 0.00% 0.00
Arg4 60.00% 20.00% 30.00
Table 7: SRL performance of UIUC SRLer;
Brown test corpus
Precision Recall F
?=1
Arg0 83.85% 86.22% 85.02
Arg1 66.67% 63.02% 64.79
Arg2 50.38% 44.90% 47.48
Arg3 0.00% 0.00% 0.00
Arg4 60.00% 20.00% 30.00
Table 8: SRL performance of UIUC SRLer us-
ing information of predicted MP; the single an-
chor approach; Brown test corpus
matic hierarchy theory, states that the argument
that the highest role (i.e. proto-agent, Arg0 in
PropBank) is the subject. This means that Arg0 is
usually realized as a constituent preceding a predi-
cate and has a long distance from the predicate. As
a solution of finding active region of arguments,
MP prediction is helpful to shrink the searching
range of arguments preceding the predicate. From
this point, we give a rough explanation why exper-
iment results for Arg0 are better.
6 Conclusion
Inspired by the locality phenomenon that argu-
ments are usually limited in a syntax sub-tree, this
paper proposed to label semantic roles locally in
the active region arguments dominated by maximal
projection, which is a concept in D-structure from
the projection principle of the principle and param-
eters theory. Statistical analysis showed that MP
information was helpful to avoid errors in SRL,
such as falsely recognizing constituents outside ac-
tive region as arguments. To adapt the projection
concept to label semantic roles, this paper defined
MP in S-structure and proposed two methods to
predict MP, namely the anchor group approach and
the single anchor approach. Both approaches were
based on NP-movement principle of principle and
parameters. Experimental results indicated that
our MP prediction methods improved SRL.
Acknowlegements
The work is supported by the National Natu-
ral Science Foundation of China under Grants
No. 60503071, 863 the National High Technol-
ogy Research and Development Program of China
under Grants No.2006AA01Z144, 973 Natural
Basic Research Program of China under Grants
No.2004CB318102.
References
Carreras, Xavier and Llu??s M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: semantic role
labeling. In Proceedings of Conference on Natural
Language Learning.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Chomsky, Noam. 1986. Barriers. MIT Press, Barriers.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computional Linguis-
tics, 28(3):245?288.
Gordon, Andrew and Reid Swanson. 2007. Generaliz-
ing Semantic Role Annotations Across Syntactically
Similar Verbs. In Proceedings of Conference on As-
sociation for Computational Linguistics.
Koomen, Peter, Vasina Punyakanok, Dan Roth and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of Conference on Natural Language Learn-
ing.
Liu, Yudong and Anoop Sarkar. 2004. Experimen-
tal Evaluation of LTAG-Based Features for Semantic
Role Labeling. In Proceedings of Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning.
Mocshitti, Alessandro. 2004. A Study on Convolu-
tion Kernels for Shallow Semantic Parsing. In Pro-
ceedings of Conference on Association for Compu-
tational Linguistics.
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. In Proceedings of Conference
on Association for Computational Linguistics.
Toutanova, Kristina, Aria Haghighi and Christopher
Manning. 2005. Joint Learning Improves Seman-
tic Role Labeling. In Proceedings of Conference on
Association for Computational Linguistics.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
840
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1475?1483,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Chinese Semantic Role Labeling with Shallow Parsing
Weiwei Sun and Zhifang Sui and Meng Wang and Xin Wang
Institute of Computational Linguistics
Peking University
Key Laboratory of Computational Linguistics
Ministry of Education, China
weiwsun@gmail.com;{szf,wm}@pku.edu.cn;xinwang.cpku@gmail.com;
Abstract
Most existing systems for Chinese Seman-
tic Role Labeling (SRL) make use of full
syntactic parses. In this paper, we evalu-
ate SRL methods that take partial parses as
inputs. We first extend the study on Chi-
nese shallow parsing presented in (Chen
et al, 2006) by raising a set of addi-
tional features. On the basis of our shal-
low parser, we implement SRL systems
which cast SRL as the classification of
syntactic chunks with IOB2 representation
for semantic roles (i.e. semantic chunks).
Two labeling strategies are presented: 1)
directly tagging semantic chunks in one-
stage, and 2) identifying argument bound-
aries as a chunking task and labeling their
semantic types as a classification task. For
both methods, we present encouraging re-
sults, achieving significant improvements
over the best reported SRL performance
in the literature. Additionally, we put
forward a rule-based algorithm to auto-
matically acquire Chinese verb formation,
which is empirically shown to enhance
SRL.
1 Introduction
In the last few years, there has been an increas-
ing interest in Semantic Role Labeling (SRL) on
several languages, which consists of recognizing
arguments involved by predicates of a given sen-
tence and labeling their semantic types. Nearly
all previous Chinese SRL research took full syn-
tactic parsing as a necessary pre-processing step,
such as (Sun and Jurafsky, 2004; Xue, 2008; Ding
and Chang, 2008). Many features are extracted to
encode the complex syntactic information. In En-
glish SRL research, there have been some attempts
at relaxing the necessity of using full syntactic
parses; better understanding of SRL with shallow
parsing is achieved by CoNLL-2004 shared task
(Carreras and M`arquez, 2004). However, it is still
unknown how these methods perform on other lan-
guages, such as Chinese.
To date, the best SRL performance reported on
the Chinese Proposition Bank (CPB) corresponds
to a F-measure is 92.0, when using the hand-
crafted parse trees from Chinese Penn Treebank
(CTB). This performance drops to 71.9 when a
real parser is used instead
1
(Xue, 2008). Com-
paratively, the best English SRL results reported
drops from 91.2 (Pradhan et al, 2008) to 80.56
(Surdeanu et al, 2007). These results suggest that
as still in its infancy stage, Chinese full parsing
acts as a central bottleneck that severely limits our
ability to solve Chinese SRL. On the contrary, Chi-
nese shallow parsing has gained a promising re-
sult (Chen et al, 2006); hence it is an alternative
choice for Chinese SRL.
This paper addresses the Chinese SRL problem
on the basis of shallow syntactic information at
the level of phrase chunks. We first extend the
study on Chinese chunking presented in (Chen et
al., 2006) by raising a set of additional features.
The new set of features yield improvement over
the strong chunking system described in (Chen et
al., 2006). On the basis of our shallow parser, we
implement lightweight systems which solve SRL
as a sequence labeling problem. This is accom-
plished by casting SRL as the classification of syn-
tactic chunks (e.g. NP-chunk) into one of semantic
labels with IOB2 representation (?). With respect
to the labeling strategy, we distinguish two differ-
ent approaches. The first one directly recognizes
semantic roles by an IOB-type sequence tagging.
The second approach divides the problem into two
independent subtasks: 1) Argument Identification
(AI) and 2) Semantic Role Classification (SRC).
1
This F-measure is evaluated on the basis of hand-crafted
word segmentation and POS tagging.
1475
A Chinese word consists of one or more char-
acters, and each character, in most cases, is a mor-
pheme. The problem of how the words are con-
structed from morphemes, known as word for-
mation, is very important for a majority of Chi-
nese language processing tasks. To capture Chi-
nese verb formation information, we introduce a
rule-based algorithm with a number of heuristics.
Experimental results indicate that word formation
features can help both shallow parsing and SRL.
We present encouraging SRL results on CPB
2
.
The best F-measure performance (74.12) with
gold segmentation and POS tagging can be
achieved by the first method. This result yield
significant improvement over the best reported
SRL performance (71.9) in the literature (Xue,
2008). The best recall performance (71.50) can be
achieved by the second method. This result is also
much higher than the best reported recall (65.6) in
(Xue, 2008).
2 Related Work
Previous work on Chinese SRL mainly focused on
how to implement SRL methods which are suc-
cessful on English, such as (Sun and Jurafsky,
2004; Xue and Palmer, 2005; Xue, 2008; Ding
and Chang, 2008). Sun and Jurafsky (2004) did
the preliminary work on Chinese SRL without
any large semantically annotated corpus of Chi-
nese. Their experiments were evaluated only on
ten specified verbs with a small collection of Chi-
nese sentences. This work made the first attempt
on Chinese SRL and produced promising results.
After the CPB was built, (Xue and Palmer, 2005)
and (Xue, 2008) have produced more complete
and systematic research on Chinese SRL. Ding
and Chang (2008) divided SRC into two sub-tasks
in sequence. Under the hierarchical architecture,
each argument should first be determined whether
it is a core argument or an adjunct, and then be
classified into fine-grained categories. Chen et
al. (2008) introduced an application of transduc-
tive SVM in Chinese SRL. Because their experi-
ments took hand-crafted syntactic trees as input,
how transductive SVMs perform in Chinese SRL
in realistic situations is still unknown.
Most existing systems for automatic Chinese
SRL make use of a full syntactic parse of the sen-
tence in order to define argument boundaries and
2
Our system is available at
http://code.google.com/p/csrler/
to extract relevant information for training clas-
sifiers to disambiguate between role labels. On
the contrary, in English SRL research, there have
been some attempts at relaxing the necessity of us-
ing syntactic information derived from full parse
trees. For example, Hacioglu and Ward (2003)
considered SRL as a chunking task; Pradhan et
al. (2005) introduced a new procedure to incor-
porate SRL results predicted respectively on full
and shallow syntactic parses. Previous work on
English suggests that even good labeling perfor-
mance has been achieved by full parse based SRL
systems, partial parse based SRL systems can still
enhance their performance. Though better under-
standing of SRL with shallow parsing on English
is achieved by CoNLL-2004 shared task (Carreras
and M`arquez, 2004), little is known about how
these SRL methods perform on Chinese.
3 Chinese Shallow Parsing
There have been some research on Chinese shal-
low parsing, and a variety of chunk defini-
tions have been proposed. However, most of
these studies did not provide sufficient detail.
In our system, we use chunk definition pre-
sented in (Chen et al, 2006), which provided
a chunk extraction tool. The tool to extract
chunks from CTB was developed by modify-
ing the English tool used in CoNLL-2000 shared
task, Chunklink
3
, and is publicly available at
http://www.nlplab.cn/chenwl/chunking.html. The
definition of syntactic chunks is illustrated in Line
CH in Figure 1. For example, ?????/the in-
surance company?, consisting of two nouns, is a
noun phrase.
With IOB2 representation (Ramshaw and Mar-
cus, 1995), the problem of Chinese chunking can
be regarded as a sequence labeling task. In this
paper, we first implement the chunking method
described in (Chen et al, 2006) as a strong base-
line. To conveniently illustrate, we denote a word
in focus with a fixed window w
?2
w
?1
ww
+1
w
+2
,
where w is current token. The baseline features
includes:
? Uni-gram word/POS tag feature: w
?2
, w
?1
,
w, w
+1
, w
+2
;
? Bi-gram word/POS tag feature: w
?2
w
?1
,
w
?1
w, w w
+1
, w
+1
w
+2
;
3
http://ilk.uvt.nl/team/sabine/chunklink/chunklink 2-2-
2000 for conll.pl
1476
WORD: ?? ?? ?? ?? ? ? ?? ?? ?? ?? ??
POS: [P] [NT] [NN NN] [AD] [P] [NR] [NN] [VP] [NN NN]
CH: [PP NP] [NP] [ADVP] [PP NP NP ] [VP] [NP]
M1: B-A* I-A*
4
B-A0 B-AM-ADV B-A2 I-A2 I-A2 B-V B-A1
M2-AI: B-A I-A B-A B-A B-A I-A I-A B-V B-A
M2-SRC: AM-TMP A0 AM-ADV A2 Rel A1
Until now, the insurance company has provided insurance services for the Sanxia Project.
Figure 1: An example from Chinese PropBank.
That means 18 features are used to represent a
given token. For instance, the bi-gram Word fea-
tures at 5th word position (???/company?) in
Figure 1 are ?? ???, ??? ???, ??? ??,
?? ??.
To improve shallow parsing, we raised an addi-
tional set of features. We will discuss these fea-
tures in section 5.
4 SRL with Shallow Parsing
The CPB is a project to add predicate-argument
relations to the syntactic trees of the CTB. Similar
to English PropBank, the semantic arguments of a
predicate are labeled with a contiguous sequence
of integers, in the form of AN (i.e. ArgN ); the ad-
juncts are annotated as such with the label AM (i.e.
ArgM) followed by a secondary tag that represents
the semantic classification of the adjunct. The as-
signment of argument labels is illustrated in Figure
1, where the predicate is the verb ???/provide?.
For example, the noun phrase ?????/the in-
surance company? is labeled as A0, meaning that it
is the proto-Agent of ??; the preposition phrase
?????/until now? is labeled as AM-TMP, in-
dicating a temporal component.
4.1 System Architecture
SRL is a complex task which has to be decom-
posed into a number of simpler decisions and tag-
ging schemes in order to be addressed by learn-
ing techniques. Regarding the labeling strategy,
we can distinguish at least two different strategies.
The first one consists of performing role identifi-
cation directly as IOB-type sequence tagging. The
second approach consists of dividing the problem
into two independent subtasks.
4
The semantic chunk labels here are B-AM-TMP and I-
AM-TMP. Limited to the document length, we cannot put all
detailed chunk labels in one line in Figure 1.
4.1.1 One-stage Strategy
In the one-stage strategy, on the basis of syntac-
tic chunks, we define semantic chunks which do
not overlap nor embed using IOB2 representation.
Syntactic chunks outside a chunk receive the tag
O. For syntactic chunks forming a chunk of type
A*, the first chunk receives the B-A* tag (Begin),
and the remaining ones receive the tag I-A* (In-
side). Then a SRL system can work directly by
using sequence tagging techinique. Since the se-
mantic annotation in the PropBank corpus does
not have any embedded structure, there is no loss
of information in this representation. The line M1
in Figure 1 illustrates this semantic chunk defini-
tion.
4.1.2 Two-stage Strategy
In the two-stage architecture, we divide Chinese
SRL into two subtasks: 1) semantic chunking for
AI, in which the argument boundaries are pre-
dicted, and 2) classification for SRC, in which the
already recognized arguments are assigned role la-
bels. In the first stage, we define semantic chunks
B-A which means begin of an argument and I-A
which means inside of an argument. In the second
stage, we solve SRC problem as a multi-class clas-
sification. The lines M2-AI and M2-SRC in Fig-
ure 1 illustrate this two-stage architecture. For ex-
ample, the noun phrase ?????/the insurance
company? is proto-Agent, and thus should be la-
beled as B-A in the AI chunking phase, and then
be tagged as A0. The phrase ??????/for the
Sanxia Project? consists of three chunks, which
should be labeled as B-A, I-A, and I-A respectively
in the AI chunking phase, then these three chunks
as a whole argument should be recognized as A2.
4.1.3 Chunk-by-Chunk
There is also another semantic chunk definition,
where the basic components of a semantic chunk
are words rather than syntactic chunks. A good
election for this problem is chunk-by-chunk pro-
1477
cessing instead of word-by-word. The motivation
is twofold: 1) phrase boundaries are almost always
consistent with argument boundaries; 2) chunk-
by-chunk processing is computationally less ex-
pensive and allows systems to explore a relatively
larger context. This paper performs a chunk-by-
chunk processing, but admitting a processing by
words within the target verb chunks.
4.2 Features
Most of the feature templates are ?standard?,
which have been used in previous SRL research.
We give a brief description of ?standard? features,
but explain our new features in detail.
5
4.2.1 Features for Semantic Chunking
In the semantic chunking tasks, i.e. the one-stage
method and the first step in the two-stage method,
we use the same set of features. The features
are extracted from three types of elements: syn-
tactic chunks, target verbs, links between chunks
and target verbs. They are formed making use
of words, POS tags and chunks of the sentence.
Xue (2008) put forward a rough verb classifica-
tion where verb classes are automatically derived
from the frame files, which are verb lexicon for
the CPB annotation. This kind of verb class in-
formation has been shown very useful for Chinese
SRL. Our system also includes this feature. In our
experiments, we represent a verb in two dimen-
sions: 1) number of arguments, and 2) number of
framesets. For example, a verb may belong to the
class ?C1C2,? which means that this verb has two
framesets, with the first frameset having one argu-
ment and the second having two arguments.
To conveniently illustrate, we de-
note a token chunk with a fixed context
w
i?1
[
c
k
w
i
...w
h
...w
j
]w
j+1
, where w
h
is the
head word of this chunk c
k
. The complete list of
features is listed here.
Extraction on Syntactic Chunks
Chunk type: c
k
.
Length: the number of words in a chunk.
Head word/POS tag. The rules described in
(Sun and Jurafsky, 2004) are used to extract head
word.
IOB chunk tag of head word: chunk tag of head
word with IOB2 representation (e.g. B-NP, I-NP).
5
The source code of our system also provides lots of com-
ments for implementation of all features.
Chunk words/POS tags context. Chunk con-
text includes one word before and one word after:
w
i?1
and w
j+1
.
POS tag chain: sequential containers of each
word?s POS tag: w
i
... w
j
. For example, this fea-
ture for ?????? is ?NN NN?.
Position: the position of the phrase with respect
to the predicate. It has three values as before, after
and here.
Extraction on Target Verbs Given a target verb
w
v
and its context, we extract the following fea-
tures.
Predicate, its POS tag, and its verb class.
Predicate IOB chunk tag context: the chain of
IOB2 chunk tags centered at the predicate within
a window of size -2/+2.
Predicate POS tag context: the POS tags of
the words that immediately precede and follow the
predicate.
Number of predicates: the number of predicates
in the sentence.
Extraction on Links To capture syntactic prop-
erties of links between the chunks and the verbs,
we use the following features.
Path: a flat path is defined as a chain of base
phrases between the token and the predicate. At
both ends, the chain is terminated with the POS
tags of the predicate and the headword of the to-
ken.
Distance: we have two notions of distance. The
first is the distance of the token from the predicate
as a number of base phrases, and the second is the
same distance as the number of VP chunks.
Combining Features We also combine above
features as some new features.
Conjunctions of position and head word, tar-
get verb, and verb class, including: position w
h
,
position w
v
, position w
h
w
v
, position class,
and position w
h
class.
Conjunctions of position and POS tag of
head word, target verb, and verb class, in-
cluding: position w
h
w
v
, position w
h
, and
position w
h
class.
4.2.2 Features for SRC
In the SRC stage of the two-stage method, dif-
ferent from previous work, our system only uses
word-based features, i.e. features extracted from
words and POS tags, to represent a given argu-
ment. Experiments show that a good semantic
1478
role classifier can be trained by using only word-
based features. To gather all argument position
information predicted in AI stage, we design a
coarse frame feature, which is a sequential collec-
tion of arguments. So far, we do not know the
detailed semantic type of each argument, and we
use XP as each item in the frame. To distinguish
the argument in focus, we use a special symbol
to indicate the corresponding frame item. For in-
stance, the Frame feature for argument ???
? is XP+XP+XP+XP+V+!XP, where !XP means
that it is the argument in focus.
Denote 1) a given argument
w
i?2
w
i?1
[w
i
w
i+1
...w
j?1
w
j
]w
j+1
w
j+2
, and
2) a given predicate w
v
. The features for SRC are
listed as follows.
Words/POS tags context of arguments: the con-
tents and POS tags of the following words: w
i
,
w
i?1
, w
i?2
, w
i+1
, w
i+2
, w
j
, w
j+1
, w
j?1
, w
j?2
,
w
j+1
, w
j+2
; the POS tags of the following words:
w
i+1
, w
i+2
, w
j+1
, w
j+2
.
Token Position.
Predicate, its POS, and its verb class.
Coarse Frame.
Combining features: conjunctions of bound-
ary words, including w
i?1
w
j+1
and w
i?2
w
j+2
;
conjunction of POS tags of boundary words, in-
cluding w
i?1
w
j+1
and w
i?2
w
j+2
; conjunction
of token position, boundary words, and predi-
cate word, including position w
i
w
j
, w
i
w
j
w
v
;
position w
i
w
j
w
v
; conjunction of token posi-
tion, boundary words? POS tags, and predicate
word, also including position w
i
w
j
, w
i
w
j
w
v
;
position w
i
w
j
w
v
; conjunction of predicate and
frame; conjunction of target verb class and frame;
conjunction of boundary words? POS tags, and
predicate word.
5 Automatic Chinese Verb Formation
Analyzing
5.1 Introduction to Chinese Word Formation
Chinese words consist of one or more charac-
ters, and each character, in most cases, is a mor-
pheme which is the smallest meaningful unit of
the language. According to the number of mor-
phemes, the words can be grouped into two sets,
simple words (consisting of one morpheme) and
compound words (consisting of two morphemes
or more). There are 9 kinds of word formation in
Chinese compound words, and table 1 shows the
detail with examples. Note that, attributive-head
and complementarity are not for Chinese verbs.
Types Examples
reduplication ??(look)??(think)
affixation ??(intensify)??(feel)
subject-verb ??(hear)??(dictate)
verb-object ??(quit smoking)
??(haircut)
verb-complement ??(inform)??(plant)
verb-result ??(exceed)??(boil)
adverbial-head ??(retreat)??(misuse)
coordinate ??(cherish)??(chase)
attributive-head* ??(rumor)??(hospital)
complementarity* ??(paper)??(horse)
Table 1: Example Words with Formation
The internal structure of a word constraints its
external grammatical behavior, and the formation
of a verb can provide very important information
for Chinese SRL. Take ???/exceed? as an ex-
ample, the two characters are both verbal mor-
phemes, and the character ??? means ?pass? and
the character ??? with the meaning of ?over?
shows the complement of the action of ???. In
this word, ??? is usually collocated with an ob-
ject, and hence a Patient role should comes af-
ter the verb ????. Note that, the verb ???,
however, is unlikely to have an object. Take ??
?/haircut? as another example, the first charac-
ter ??? is a verbal morpheme with the meaning
of ?cut? and the second character ??? is a nomi-
nal morpheme with the meaning of ?hair?. In this
word, ??? acts as the object of ???, and the word
???? is unlikely to have an Patient any more in
the sentential context.
5.2 Verb Formation Analyzing Method
To automatically analyze verb formation, we in-
troduce a rule-based algorithm. Pseudo code in
Algorithm 1 illustrates our algorithm. This algo-
rithm takes three string (one or more Chinese char-
acters) sets as lexicon knowledge:
? adverbial suffix set A: strings in A are usu-
ally realized as the modifier in a adverbial-
head type word, e.g. ?/not, ?/not,
?/always,?/both,?/all.
? object head setO: strings inO are usually re-
alized as the head in a verb-object type word,
e.g. ?/change,?/get,?/talk,?/send.
1479
Algorithm 1: Verb Formation Analyzing.
Data: adverbial suffix set A, object head set
O, complement suffix set C
input : word W = c
1
...c
n
and its POS P
output: head character h, adverbial character
a, complement character c, object
character o
begin
h = c = a = o = null;
if n = 4 and c
1
= c
3
and c
2
= c
4
then
return Verb formation of W
?
= c
1
c
3
;
else if n = 3 and c
2
= c
3
then
h = c
1
, c = c
2
;
else if n = 2 and c
1
= c
2
then
h = c
1
;
else if n = 1 then
h = c
1
;
else if c
n
? C and c
n?1
c
n
? C and
P=?VV? then
h = c
1
, c = c
n
/c
n?1
c
n
;
else if c
1
? A then
a = c
1
, h = c
2
...c
n
;
else if c
1
? O and P=?VV? then
h = c
1
, o = c
2
...c
n
;
end
? complement suffix set C: strings in C are
usually realized as complement in a verb-
complement type word: e.g. ?/out, ?/in,
?/finish,?/come,??/not.
Note that, to date there is no word formation
annotation corpus, so direct evaluation of our rule-
based algorithm is impossible. This paper makes
task-oriented evaluation which measures improve-
ments in SRL.
5.3 Using Word Formation Information to
improve Shallow Parsing
The majority of Chinese nouns are of type
attributive-head. This means that for most nouns
the last character provides very important infor-
mation indicating the head of the noun. For ex-
ample, the word formations of ???/peach?, ??
?/willow? and ????/boxtree? (three different
kinds of trees), are attributive-head and they have
the same head word ??/tree?. While for verbs, the
majority are of three types: verb-object, coordi-
nate and adverbial-head. For example, words ??
?/enlarge?, ???/make more drastic? and ??
?/accelerate? have the same head ??/add?. The
head morpheme is very useful in alleviating the
data sparseness in word level. However, for any
given word, it is very hard to accurately find the
head. In the shallow paring experiments, we use
a very simple rule to get a pseudo head character:
1) extracting the last word for a noun, and 2) ex-
tracting the first word for a verb. The new features
include:
Pattern 1: conjunction of pseudo head of w
i?1
and POS tags of w
i?1
and w
i
.
Pattern 2: conjunction of pseudo head of w
i
and
POS tags of w
i?1
and w
i
.
Pattern 3: conjunction of length/POS tags of
w
i?1
, w
i
, w
i+1
.
5.4 Using Verb Formation Information to
improve SRL
We use some new verb formation features to im-
prove our SRL system. The new features are listed
as follows. The first four are used in semantic
chunking task, and all are used in SRC task.
First/last characters.
Word length.
Conjunction of word length and first/last char-
acter.
Conjunction of token position and first/last
character.
The head string of a verb (e.g. ??? in ????).
The adverbial string of a verb (e.g. ??? in ??
??).
The complement string of a verb (e.g. ??? in
????).
The object string of a verb (e.g. ??? in ??
??).
6 Results and Discussion
6.1 Experimental Setting
6.1.1 Data
Experiments in previous work are mainly based on
CPB and CTB, but the experimental data prepar-
ing procedure does not seem consistent. For ex-
ample, the sum of each semantic role reported in
(Ding and Chang, 2008) is extremely smaller than
the corresponding occurrence statistics in origi-
nal data files in CPB. In this paper, we mod-
ify CoNLL-2005 shared task software
6
to pro-
cess CPB and CTB. In our experiments, we use
the CPB 1.0 and CTB 5.0. The data is divided
into three parts: files from chtb 081 to chtb 899
are used as training set; files from chtb 041 to
6
http://www.lsi.upc.edu/?srlconll/soft.html
1480
chtb 080 as development set; files from chtb 001
to chtb 040, and chtb 900 to chtb 931 as test set.
The data setting is the same as (Xue, 2008). The
results were evaluated for precision, recall and F-
measure numbers using the srl-eval.pl script pro-
vided by CoNLL-2005 shared task.
6.1.2 Classifier
For both syntactic and semantic chunking, we
used TinySVM along with YamCha
7
(Kudo and
Matsumoto, 2000; Kudo and Matsumoto, 2001).
In the chunking experiments, all SVM classifiers
were realized with a polynomial kernel of de-
gree 2. Pair-wise strategy is used to solve multi-
class classification problem. For the SRC ex-
periments, we use a linear SVM classifier, along
with One-Vs-All approach for multi-class classifi-
cation. SVM
lin
8
, a fast linear SVM solvers, is used
for supervised learning. l
2
-SVM-MFN (modified
finite newton) method is used to solve the opti-
mization problem (Keerthi and DeCoste, 2005).
6.2 Shallow Parsing Performance
P(%) R(%) F
?=1
Baseline 93.54 93.00 93.27
Ours 93.83 93.39 93.61
Table 2: Shallow parsing performance
Table 2 summarizes the overall shallow pars-
ing performance on test set. The first line shows
the performance of baseline. Comparing the best
system performance 94.13 F-measure of CoNLL
2000 shared task (Syntactic Chunking on English),
we can see Chinese shallow parsing has reached
a comparable result, tough the comparison of nu-
meric performance is not very fair, because of dif-
ferent languages, different chunk definition, dif-
ferent training data sizes, etc.. The second line
Ours shows the performance when new features
are added, from which we can see the word for-
mation based features can help shallow parsing.
Table 3 shows the detailed performance of noun
phrase (NP) and verb phrase (VP), which make up
most of phrase chunks in Chinese. Our new fea-
tures help NP more, whereas the effect of new fea-
tures for VP is not significant. That is in part be-
cause most VP chunk recognition error is caused
by long dependency, where word formation fea-
7
http://chasen.org/?taku/index.html.en
8
http://people.cs.uchicago.edu/?vikass/svmlin.html
P(%) R(%) F
?=1
NP(Baseline) 90.84 90.05 90.44
NP(Ours) 91.42 90.78 91.10
VP(Baseline) 94.44 94.55 94.50
VP(Ours) 94.65 94.74 94.69
Table 3: Performance of NP-chunk and VP-chunk
tures do not work. Take the sentences below for
example:
1. [
V P
??????]? (Therefore (we)
achieve victory.)
2. [
ADV P
??] [
V P
????] ?????
????? (Therefore the major changes
have not been met before.)
The contexts of the word ???/therefore? in the
two sentences are similar, where ???? is fol-
lowed by verbal components. In the second sen-
tence, the word ???/therefore? will be correctly
recognized as an adverbial phrase unless classifier
knows the following component is a clause. Un-
fortunately, word formation features cannot sup-
ply this kind of information.
6.3 SRL Performance
P(%) R(%) A(%) F
?=1
(Xue, 2008) 79.5 65.6 ? 71.9
M1? 79.02 69.12 ? 73.74
M1+ 79.25 69.61 ? 74.12
M2?/AI 80.34 75.11 ? 77.63
M2+/AI 80.01 75.15 ? 77.51
M2?/SRC ? ? 92.57 ?
M2+wf/SRC ? ? 93.25 ?
M2+/SRC ? ? 93.42 ?
M2?AI+SRC 76.48 71.50 ? 73.90
Table 4: Overall SRL performance of different
methods
Table 4 lists the overall SRL performance num-
bers on test set using different methods mentioned
earlier; these results are based on features com-
puted from gold standard segmentation and POS
tagging, but automatic recognized chunks, which
is parsed by our improved shallow parsing sys-
tem. For the AI and the whole SRL tasks, we
report the precision (P), recall (R) and the F
?=1
-
measure scores, and for the SRC task we report
the classification accuracy (A). The first line (Xue,
1481
2008) shows the SRL performance reported in
(Xue, 2008). To the authors? knowledge, this re-
sult is best SRL performance in the literature. Line
2 and 3 shows the performance of the one-stage
systems: 1) Line M1? is the performance without
word formation features; 2) Line M1+ is the per-
formance when verb formation features are added.
Line 4 to 8 shows the performance of the two-stage
systems: 1) Line M2?/AI and M2+/AI shows the
performance of AI phase without and within word
formation features respectively; 2) Line M2?/SRC
shows the SRC performance with trivial word-
based features (i.e. frame features and verb forma-
tion features are not used); 3) Line M2+wf/SRC is
the improved SRC performance when coarse verb
formation features are added; 4) Line M2+/SRC
is the SRC performance with all features; 5) Line
M2?AI+SRC shows the performance of SRL sys-
tem, which uses baseline features to identify argu-
ments, and use all features to classify arguments.
6.4 Discussion
The results summarized in Table 4 indicate that
according to the-state-of-the-art in Chinese pars-
ing, SRL systems based on shallow parsing out-
performs the ones based on full parsing. Com-
parison between one-stage strategy and two-stage
strategy indicates 1) that there is no significant dif-
ference in the F-measure; and 2) that two-stage
strategy method can achieve higher recall while
one-stage strategy method can achieve higher pre-
cision. Both the one-stage strategy and two-stage
strategy methods yield significant improvements
over the best reported SRL performance in the lit-
erature, especially in terms of recall performance.
Comparison SRL performance with full parses
and partial parses indicates that both models have
strong and weak points. The full parse based
method can implement high precision SRL sys-
tems, while the partial parse based methods can
implement high recall SRL systems. This is fur-
ther justification for combination strategies that
combine these independent SRL models.
Generally, Table 4 shows that verb formation
features can enhance Chinese SRL, especially for
fine-grained role classification. The effect of word
formation in formation in both shallow parsing
and SRL suggests that automatic word formation
analyzing is very important for Chinese language
processing. The rule-based algorithm is just a pre-
liminary study on this new topic, which requires
Num of words P (%) R (%) F
?=1
Length = 1 84.69% 75.48% 79.82
Length = 2 82.14% 74.21% 77.97
Length = 3 75.43% 63.98% 69.24
Length = 4 75.71% 65.63% 70.32
Length = 5 72.46% 64.38% 68.18
Length = 6 72.97% 66.21% 69.43
Length = 7 77.03% 67.65% 72.04
Length = 8 74.39% 57.28% 64.72
Length = 9 66.67% 51.16% 57.89
Length = 10 68.08% 58.28% 62.80
Length = 11+ 67.40% 57.71% 62.18
Table 5: SRL performance with arguments of dif-
ferent length
more research effort.
Though our SRC module does not use any pars-
ing information, our system can achieve 93.42%
accuracy, comparing the best gold parse based re-
sult 94.68% in the literature. This result suggests
that Chinese SRC system, even without parsing,
can reach a considerable good performance. The
main reason is that in Chinese, arguments with dif-
ferent semantic types have discriminative bound-
ary words, which can be extracted without pars-
ing. It is very clear that the main bottleneck for
Chinese SRL is to accurately identify arguments
rather than to disambiguate their detailed seman-
tic types.
Table 5 summarizes the labeling performance
for argument of different length. It is not surpris-
ing that arguments are more and more difficult to
rightly recognize as the increase of their length.
But the performance decline slows up when the
length of arguments is larger than 10. In other
words, some of the arguments that are composed
of many words can still be rightly identified. The
main reason for this point is that these arguments
usually have clear collocation words locating at ar-
gument boundaries. Take the sentences below for
example,
3. ??[A1 . . . . . .?] (including ... etc.)
the object of the verb ???/include? has a defi-
nite collocation word ??/etc.?, and therefore this
object is easy to be recognized as a A1.
7 Conclusion
In this paper, we discuss Chinese SRL on the ba-
sis of partial syntactic structure. Our systems ad-
vance the state-of-the-art in Chinese SRL. We first
1482
extend the study on Chinese shallow parsing and
implement a good shallow parser. On the ba-
sis of partial parses, SRL are formulated as a se-
quence labeling problem, performing IOB2 deci-
sions on the syntactic chunks of the sentence. We
exploit a wide variety of features based on words,
POS tags, and partial syntax. Additionally, we
discuss a language special problem, i.e. Chinese
word formation. Experimental results show that
coarse word formation information can help shal-
low parsing, especially for NP-chunk recognition.
A rule-based algorithm is put forward to automat-
ically acquire Chinese verb formation, which is
empirically shown to enhance SRL.
Acknowledgments
This work is supported by NSFC Project
60873156, 863 High Technology Project of
China 2006AA01Z144 and the Project of Toshiba
(China) R&D Center.
We would like to thank Weiwei Ding for his
good advice on this research.
We would also like to thank the anonymous re-
viewers for their helpful comments.
References
Xavier Carreras and Llu??s M`arquez. 2004. Introduc-
tion to the conll-2004 shared task: Semantic role
labeling. In Hwee Tou Ng and Ellen Riloff, edi-
tors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learn-
ing (CoNLL-2004), pages 89?97, Boston, Mas-
sachusetts, USA, May 6 - May 7. Association for
Computational Linguistics.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 97?104, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Yaodong Chen, Ting Wang, Huowang Chen, and Xis-
han Xu. 2008. Semantic role labeling of Chinese
using transductive svm and semantic heuristics. In
Proceedings of the Third International Joint Confer-
ence on Natural Language Processing: Volume-II.
Weiwei Ding and Baobao Chang. 2008. Improv-
ing Chinese semantic role classification with hier-
archical feature selection strategy. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 324?333, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support
vector machines. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 25?27, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
S. Sathiya Keerthi and Dennis DeCoste. 2005. A mod-
ified finite newton method for fast solution of large
scale linear svms. J. Mach. Learn. Res., 6:341?361.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of the 2nd workshop on Learning language in
logic and the 4th conference on Computational natu-
ral language learning, pages 142?144, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL ?01: Sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1?8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 217?220, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Sameer S. Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards robust semantic role labeling.
Comput. Linguist., 34(2):289?310.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Pro-
ceedings of the 3rd ACL/SIGDAT Workshop on Very
Large Corpora, Cambridge, Massachusetts, USA,
pages 82?94.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantc parsing of Chinese. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings.
Mihai Surdeanu, Llu??s M`arquez, Xavier Carreras, and
Pere Comas. 2007. Combination strategies for se-
mantic role labeling. J. Artif. Intell. Res. (JAIR),
29:105?151.
Nianwen Xue and Martha Palmer. 2005. Automatic
semantic role labeling for Chinese verbs. In in Pro-
ceedings of the 19th International Joint Conference
on Artificial Intelligence, page 2005.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
1483
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 253?256,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Prediction of Thematic Rank for Structured Semantic Role Labeling
Weiwei Sun and Zhifang Sui and Meng Wang
Institute of Computational Linguistics
Peking University
Key Laboratory of Computational Linguistics
Ministry of Education, China
weiwsun@gmail.com;{wm,szf}@pku.edu.cn
Abstract
In Semantic Role Labeling (SRL), it is rea-
sonable to globally assign semantic roles
due to strong dependencies among argu-
ments. Some relations between arguments
significantly characterize the structural in-
formation of argument structure. In this
paper, we concentrate on thematic hierar-
chy that is a rank relation restricting syn-
tactic realization of arguments. A log-
linear model is proposed to accurately
identify thematic rank between two argu-
ments. To import structural information,
we employ re-ranking technique to incor-
porate thematic rank relations into local
semantic role classification results. Exper-
imental results show that automatic pre-
diction of thematic hierarchy can help se-
mantic role classification.
1 Introduction
In Semantic Role Labeling (SRL), it is evident that
the arguments in one sentence are highly corre-
lated. For example, a predicate will have no more
than one Agent in most cases. It is reasonable to
label one argument while taking into account other
arguments. More structural information of all ar-
guments should be encoded in SRL approaches.
This paper explores structural information of
predicate-argument structure from the perspec-
tive of rank relations between arguments. The-
matic hierarchy theory argues that there exists a
language independent rank of possible semantic
roles, which establishes priority among arguments
with respect to their syntactic realization (Levin
and Hovav, 2005). This construct has been widely
implicated in linguistic phenomena, such as in the
subject selection rule of Fillmore?s Case Grammar
(1968): ?If there is an A [=Agent], it becomes the
subject; otherwise, if there is an I [=Instrument],
it becomes the subject; otherwise, the subject is
the O [=Object, i.e., Patient/Theme]?. This rule
implicitly establishes precedence relations among
semantic roles mentioned and can be simplified to:
Agent  Instrument  Patient/Theme
Emerging from a range of more basic semantic
properties of the ranked semantic roles, thematic
hierarchies can help to construct mapping from se-
mantics to syntax. It is therefore an appealing op-
tion for argument structure analysis. For example,
if the the rank of argument a
i
is shown higher than
a
j
, then the assignment [a
i
=Patient, a
j
=Agent] is
illegal, since the role Agent is the highest role.
We test the hypothesis that thematic rank be-
tween arguments can be accurately detected by
using syntax clues. In this paper, the concept
?thematic rank? between two arguments a
i
and a
j
means the relationship that a
i
is prior to a
j
or a
j
is
prior to a
i
. Assigning different labels to different
relations between a
i
and a
j
, we formulate predic-
tion of thematic rank between two arguments as a
multi-class classification task. A log-linear model
is put forward for classification. Experiments on
CoNLL-2005 data show that this approach can
get an good performance, achieving 96.42% ac-
curacy on gold parsing data and 95.14% accuracy
on Charniak automatic parsing data.
Most existing SRL systems divide this task into
two subtasks: Argument Identification (AI) and
Semantic Role Classification (SRC). To add struc-
tural information to a local SRL approach, we in-
corporate thematic hierarchy relations into local
classification results using re-ranking technique
in the SRC stage. Two re-ranking approaches,
1) hard constraint re-ranking and 2) soft con-
straint re-ranking, are proposed to filter out un-
like global semantic role assignment. Experiments
on CoNLL-2005 data indicate that our method
can yield significant improvement over a state-of-
the-art SRC baseline, achieving 0.93% and 1.32%
253
absolute accuracy improvements on hand-crafted
and automatic parsing data.
2 Prediction of Thematic Rank
2.1 Ranking Arguments in PropBank
There are two main problems in modeling the-
matic hierarchy for SRL on PropBank. On the one
hand, there is no consistent meaning of the core
roles (i.e. Arg0-5/ArgA). On the other hand, there
is no consensus over hierarchies of the roles in the
thematic hierarchy. For example, the Patient occu-
pies the second highest hierarchy in some linguis-
tic theories but the lowest in some other theories
(Levin and Hovav, 2005).
In this paper, the proto-role theory (Dowty,
1991) is taken into account to rank PropBank argu-
ments, partially resolving the two problems above.
There are three key points in our solution. First,
the rank of Arg0 is the highest. The Agent is al-
most without exception the highest role in pro-
posed hierarchies. Though PropBank defines se-
mantic roles on a verb by verb basis, for a particu-
lar verb, Arg0 is generally the argument exhibit-
ing features of a prototypical Agent while Arg1
is a prototypical Patient or Theme (Palmer et al,
2005). As being the proto-Agent, the rank of Arg0
is higher than other numbered arguments. Second,
the rank of the Arg1 is second highest or lowest.
Both hierarchy of Arg1 are tested and discussed in
section 4. Third, we do not rank other arguments.
Two sets of roles closely correspond to num-
bered arguments: 1) referenced arguments and 2)
continuation arguments. To adapt the relation to
help these two kinds of arguments, the equivalence
relation is divided into several sub-categories. In
summary, relations of two arguments a
i
and a
j
in
this paper include: 1) a
i
 a
j
: a
i
is higher than
a
j
, 2) a
i
? a
j
: a
i
is lower than a
j
, 3) a
i
ARa
j
: a
j
is the referenced argument of a
i
, 4) a
i
RAa
j
: a
i
is
the referenced argument of a
j
, 5) a
i
ACa
j
: a
j
is
the continuation argument of a
i
, 6) a
i
CAa
j
: a
i
is
the continuation argument of a
j
, 7) a
i
= a
j
: a
i
and a
j
are labeled as the same role label, and 8)
a
i
? a
j
: a
i
and a
j
are labeled as the Arg2-5, but
not in the same type.
2.2 Prediction Method
Assigning different labels to possible rank be-
tween two arguments a
i
and a
j
, such as labeling
a
i
 a
j
as ??, identification of thematic rank
can be formulated as a classification problem. De-
lemma, POS Tag, voice, and SCF of predicate
categories, position of two arguments; rewrite
rules expanding subroots of two arguments
content and POS tags of the boundary words
and head words
category path from the predicate to candidate
arguments
single character category path from the
predicate to candidate arguments
conjunction of categories, position, head
words, POS of head words
category and single character category path
from the first argument to the second argument
Table 1: Features for thematic rank identification.
note the set of relationsR. Formally, given a score
function S
TH
: A?A?R 7? R, the relation r is
recognized in argmax flavor:
r? = r
?
(a
i
, a
j
) = argmax
r?R
S
TH
(a
i
, a
j
, r)
A probability function is chosen as the score func-
tion and the log-linear model is used to estimate
the probability:
S
TH
(a
i
, a
j
, r) =
exp{?(a
i
, a
j
, r) ?w}
?
r?R
exp{?(a
i
, a
j
, r) ?w}
where ? is the feature map and w is the param-
eter vector to learn. Note that the model pre-
dicts the rank of a
i
and a
j
through calculating
S
TH
(a
i
, a
j
, r) rather than S
TH
(a
j
, a
i
, r), where
a
i
precedes a
j
. In other words, the position infor-
mation is implicitly encoded in the model rather
than explicitly as a feature.
The system extracts a number of features to rep-
resent various aspects of the syntactic structure of
a pair of arguments. All features are listed in Table
1. The Path features are designed as a sequential
collection of phrase tags by (Gildea and Jurafsky,
2002). We also use Single Character Category
Path, in which each phrase tag is clustered to a cat-
egory defined by its first character (Pradhan et al,
2005). To characterize the relation between two
constituents, we combine features of the two indi-
vidual arguments as new features (i.e. conjunction
features). For example, if the category of the first
argument is NP and the category of the second is S,
then the conjunction of category feature is NP-S.
3 Re-ranking Models for SRC
Toutanova et al (2008) empirically showed that
global information is important for SRL and that
254
structured solutions outperform local semantic
role classifiers. Punyakanok et al (2008) raised an
inference procedure with integer linear program-
ming model, which also showed promising results.
Identifying relations among arguments can pro-
vide structural information for SRL. Take the sen-
tence ?[
Arg0
She] [
V
addressed] [
Arg1
her hus-
band] [
ArgM?MNR
with her favorite nickname].?
for example, if the thematic rank of she and her
husband is predicted as that she is higher than her
husband, then her husband should not be assigned
the highest role.
To incorporate the relation information to lo-
cal classification results, we employ re-ranking ap-
proach. Assuming that the local semantic classi-
fier can produce a list of labeling results, our sys-
tem then attempts to pick one from this list accord-
ing to the predicted ranks. Two different polices
are implemented: 1) hard constraint re-ranking,
and 2) soft constraint re-ranking.
Hard Constraint Re-ranking The one picked
up must be strictly in accordance with the ranks.
If the rank prediction result shows the rank of ar-
gument a
i
is higher than a
j
, then role assignments
such as [a
i
=Patient and a
j
=Agent] will be elim-
inated. Formally, the score function of a global
semantic role assignment is:
S(a, s) =
?
i
S
l
(a
i
, s
i
)
?
i,j,i<j
I(r
?
(a
i
, a
j
), r(s
i
, s
j
))
where the function S
l
locally scores an argument;
r
?
: A ? A 7? R is to predict hierarchy of two
arguments; r : S ? S 7? R is to point out the the-
matic hierarchy of two semantic roles. For exam-
ple, r(Agent, Patient) = ?  ?. I : R ?R 7?
{0, 1} is identity function.
In some cases, there is no role assignment sat-
isfies all predicted relations because of prediction
mistakes. For example, if the hierarchy detec-
tion result of a = (a
1
, a
2
, a
3
) is (r
?
(a
1
, a
2
) =
, r
?
(a
2
, a
3
) =, r
?
(a
1
, a
3
) =?), there will be no
legal role assignment. In these cases, our system
returns local SRL results.
Soft Constraint Re-ranking In this approach,
the predicted confidence score of relations is
added as factor items to the score function of the
semantic role assignment. Formally, the score
function in soft constraint re-ranking is:
S(a, s) =
?
i
S
l
(a
i
, s
i
)
?
i,j,i<j
S
TH
(a
i
, a
j
, r(s
i
, s
j
))
4 Experiments
4.1 Experimental Settings
We evaluated our system using the CoNLL-2005
shared task data. Hierarchy labels for experimen-
tal corpora are automatically set according to the
definition of relation labels described in section
2.1. Charniak parser (Charniak, 2000) is used for
POS tagging and full parsing. UIUC Semantic
Role Labeler
1
is a state-of-the-art SRL system. Its
argument classification module is used as a strong
local semantic role classifier. This module is re-
trained in our SRC experiments, using parameters
described in (Koomen et al, 2005). Experiments
of SRC in this paper are all based on good ar-
gument boundaries which can filter out the noise
raised by argument identification stage.
4.2 Which Hierarchy Is Better?
Detection SRL (S) SRL (G)
Baseline ? 94.77% ?
A 94.65% 95.44% 96.89%
A & P? 95.62% 95.07% 96.39%
A & P? 94.09% 95.13% 97.22%
Table 2: Accuracy on different hierarchies
Table 2 summarizes the performance of the-
matic rank prediction and SRC on different the-
matic hierarchies. All experiments are tested on
development corpus. The first row shows the per-
formance of the local sematic role classifier. The
second to the forth rows show the performance
based on three ranking approach. A means that
the rank of Agent is the highest; P? means that the
rank of Patient is the second highest; P? means
that the rank of the Patient is the lowest. Col-
umn SRL(S) shows SRC performance based on
soft constraint re-ranking approach, and column
SRL(G) shows SRC performance based on gold
hierarchies. The data shows that the third the-
matic hierarchy fits SRL best, but is harder to
learn. Compared with P?, P? is more suitable for
SRL. In the following SRC experiments, we use
the first hierarchy because it is most helpful when
predicted relations are used.
4.3 Results And Improvement Analysis
Table 3 summarizes the precision, recall, and F-
measure of this task. The second column is fre-
quency of relations in the test data, which can be
1
http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php
255
seen as a simple baseline. Moreover, another natu-
ral baseline system can predict hierarchies accord-
ing to the roles classified by local classifier. For
example, if the a
i
is labeled as Arg0 and a
j
is la-
beled as Arg2, then the relation is predicted as .
The third column BL shows the F-measure of this
baseline. It is clear that our approach significantly
outperforms the two baselines.
Rel Freq. BL P(%) R(%) F
 57.40 94.79 97.13 98.33 97.73
? 9.70 51.23 98.52 97.24 97.88
? 23.05 13.41 94.49 93.59 94.04
= 0.33 19.57 93.75 71.43 81.08
AR 5.55 95.43 99.15 99.72 99.44
AC 3.85 78.40 87.77 82.04 84.81
CA 0.16 30.77 83.33 50.00 62.50
All ? 75.75 96.42
Table 3: Thematic rank prediction performance
Table 4 summarizes overall accuracy of SRC.
Baseline performance is the overall accuracy of
the local classifier. We can see that our re-ranking
methods can yield significant improvemnts over
the baseline.
Gold Charniak
Baseline 95.14% 94.12%
Hard 95.71% 94.74%
Soft 96.07% 95.44%
Table 4: Overall SRC accuracy.
Hierarchy prediction and re-ranking can be
viewed as modification for local classification re-
sults with structural information. Take the sen-
tence ?[Some ?circuit breakers? installed after the
October 1987] crash failed [their first test].? for
example, where phrases ?Some ... 1987? and
?their ... test? are two arguments. The table be-
low shows the local classification result (column
Score(L)) and the rank prediction result (column
Score(H)). The baseline system falsely assigns
roles as Arg0+Arg1, the rank relation of which is
. Taking into account rank prediction result that
relation ? gets a extremely high probability, our
system returns Arg1+Arg2 as SRL result.
Assignment Score(L) Score(H)
Arg0+Arg1 78.97%? 82.30% :0.02%
Arg1+Arg2 14.25%? 11.93% ?:99.98%
5 Conclusion and Future Work
Inspired by thematic hierarchy theory, this paper
concentrates on thematic hierarchy relation which
characterize the structural information for SRL.
The prediction of thematic rank is formulated as
a classification problem and a log-linear model
is proposed to solve this problem. To improve
SRC, we employ re-ranking technique to incorpo-
rate thematic rank information into the local se-
mantic role classifier. Experimental results show
that our methods can construct high-performance
thematic rank detector and that identification of ar-
guments? relations can significantly improve SRC.
Acknowledgments
This work is supported by NSFC Project
60873156, 863 High Technology Project of
China 2006AA01Z144 and the project of Toshiba
(China) Co., Ltd. R&D Center.
References
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL-00.
David R. Dowty. 1991. Thematic proto-roles and ar-
gument selection. Language, 67:547?619.
Charles Fillmore. 1968. The case for case. In Em-
mon Bach and Richard Harms, editors, Universals
in Linguistic Theory, pages 1?90. Holt, Rinehart and
Winston, New York, New York.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245?288.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Pro-
ceedings of the CoNLL-2005, pages 181?184, June.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Research Surveys in Linguistics.
Cambridge University Press, New York.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argu-
ment classification. In Machine Learning.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Comput. Linguist.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist.
256
Coling 2010: Poster Volume, pages 1211?1219,
Beijing, August 2010
Word-Based and Character-Based Word Segmentation Models:
Comparison and Combination
Weiwei Sun
Department of Computational Linguistics, Saarland University
German Research Center for Artificial Intelligence (DFKI)
wsun@coli.uni-saarland.de
Abstract
We present a theoretical and empirical
comparative analysis of the two domi-
nant categories of approaches in Chinese
word segmentation: word-based models
and character-based models. We show
that, in spite of similar performance over-
all, the two models produce different dis-
tribution of segmentation errors, in a way
that can be explained by theoretical prop-
erties of the two models. The analysis is
further exploited to improve segmentation
accuracy by integrating a word-based seg-
menter and a character-based segmenter.
A Bootstrap Aggregating model is pro-
posed. By letting multiple segmenters
vote, our model improves segmentation
consistently on the four different data sets
from the second SIGHAN bakeoff.
1 Introduction
To find the basic language units, i.e. words,
segmentation is a necessary initial step for Chi-
nese language processing. There are two domi-
nant models for Chinese word segmentation. The
first one is what we call ?word-based? approach,
where the basic predicting units are words them-
selves. This kind of segmenters sequentially
decides whether the local sequence of charac-
ters make up a word. This word-by-word ap-
proach ranges from naive maximum matching
(Chen and Liu, 1992) to complex solution based
on semi-Markov conditional random fields (CRF)
(Andrew, 2006). The second is ?character-based?
approach, where basic processing units are char-
acters which compose words. Segmentation is
formulated as a classification problem to predict
whether a character locates at the beginning of,
inside or at the end of a word. This character-
by-character method was first proposed in (Xue,
2003), and a number of sequence labeling algo-
rithms have been exploited.
This paper is concerned with the behavior of
different segmentation models in general. We
present a theoretical and empirical comparative
analysis of the two dominant approaches. The-
oretically, these approaches are different. The
word-based models do prediction on a dynamic
sequence of possible words, while character-
based models on a static character sequence. The
former models have a stronger ability to represent
word token features for disambiguation, while the
latter models can better induce a word from its in-
ternal structure. For empirical analysis, we im-
plement two segmenters, both using the Passive-
Aggressive algorithm (Crammer et al, 2006) to
estimate parameters. Our experiments indicate
that despite similar performance in terms of over-
all F-score, the two models produce different
types of errors, in a way that can be explained by
theoretical properties. We will present a detailed
analysis that reveals important differences of the
two methods in Sec. 4.
The two types of approaches exhibit differ-
ent behaviors, and each segmentation model has
strengths and weaknesses. We further consider in-
tegrating word-based and character-based models
in order to exploit their complementary strengths
and thereby improve segmentation accuracy be-
yond what is possible by either model in isola-
tion. We present a Bootstrap Aggregating model
to combine multiple segmentation systems. By
1211
letting multiple segmenters vote, our combination
model improves accuracy consistently on all the
four different segmentation data sets from the sec-
ond SIGHAN bakeoff. We also compare our inte-
grating system to the state-of-the-art segmentation
systems. Our system obtains the highest reported
F-scores on three data sets.
2 Two Methods for Word Segmentation
First of all, we distinguish two kinds of ?words?:
(1) Words in dictionary are word types; (2) Words
in sentences are word tokens. The goal of word
segmentation is to identify word tokens in a run-
ning text, where a large dictionary (i.e. list of
word types) and annotated corpora may be avail-
able. From the view of token, we divide segmen-
tation models into two main categories: word-
based models and character-based models. There
are two key points of a segmentation model: (1)
How to decide whether a local sequence of char-
acters is a word? (2) How to do disambiguation if
ambiguous segmentation occurs? For each model,
we separately discuss the strategies for word pre-
diction and segmentation disambiguation.
2.1 Word-Based Approach
It may be the most natural idea for segmentation
to find word tokens one by one. This kind of
segmenters read the input sentences from left to
right, predict whether current piece of continu-
ous characters is a word token. After one word
is found, segmenters move on and search for next
possible word. There are different strategies for
the word prediction and disambiguation problems.
Take for example maximum matching, which was
a popular algorithm at the early stage of research
(Chen and Liu, 1992). For word prediction, if a
sequence of characters appears in a dictionary, it
is taken as a word candidate. For segmentation
disambiguation, if more than one word types are
matched, the algorithm chooses the longest one.
In the last several years, machine learning tech-
niques are employed to improve word-based seg-
mentation, where the above two problems are
solved in a uniform model. Given a sequence of
characters c ? Cn (n is the number of characters),
denote a segmented sequence of words w ? Wm
(m is the number of words, i.e. m varies with w),
and a function GEN that enumerates a set of seg-
mentation candidates GEN(c) for c. In general,
a segmenter solves the following ?argmax? prob-
lem:
w? = arg max
w?GEN(c)
?>?(c,w) (1)
= arg max
w?GEN(c)
?>
|w|?
i=1
?(c, w[1:i]) (2)
where ? and ? are global and local feature maps
and ? is the parameter vector to learn. The inner
product ?>?(c, w[1:i]) can been seen as the con-
fidence score of whether wi is a word. The dis-
ambiguation takes into account confidence score
of each word, by using the sum of local scores
as its criteria. Markov assumption is neces-
sary for computation, so ? is usually defined on
a limited history. Perceptron and semi-Markov
CRFs were used to estimate ? in previous work
(Zhang and Clark, 2007; Andrew, 2006).
2.2 Character-Based Approach
Most previous data-driven segmentation solutions
took an alternative, character-based view. This ap-
proach observes that by classifying characters as
different positions in words, segmentation can be
treated as a sequence labeling problem, assigning
labels to the characters in a sentence indicating
whether a character ci is a single character word
(S) or the begin (B), middle (I) or end (E) of a
multi-character word. For word prediction, word
tokens are inferred based on the character classes.
The main difficulty of this model is character am-
biguity that most Chinese characters can occur in
different positions within different words. Linear
models are also popular for character disambigua-
tion (i.e. segmentation disambiguation). Denote
a sequence of character labels y ? Yn, a linear
model is defined as:
y? = arg max
y?Y |c|
?>?(c,y) (3)
= arg max
y?Y |c|
?>
|c|?
i=1
?(c, y[1:i]) (4)
Note that local feature map ? is defined only
on the sequence of characters and their labels.
1212
Several discriminative models have been ex-
ploited for parameter estimation, including per-
ceptron, CRFs, and discriminative latent variable
CRFs (Jiang et al, 2009; Tseng, 2005; Sun et al,
2009b).
2.3 Theoretical Comparison
Theoretically, the two types of models are differ-
ent. We compare them from four aspects.
2.3.1 Internal Structure of Words
Chinese words have internal structures. In most
cases, Chinese character is a morpheme which
is the smallest meaningful unit of the language.
Though we cannot exactly infer the meaning of a
word from its character components, the character
structure is still meaningful. Partially characteriz-
ing the internal structures of words, one advantage
of character-based models is the ability to induce
new words. E.g., character ?/person? is usually
used as a suffix meaning ?one kind of people?. If
a segmenter never sees ?/worker? in train-
ing data, it may still rightly recognize this word
by analyzing the prefix ?/work? with label BI
and the suffix ?? with label E. In contrast, cur-
rent word-based models only utilize the weighted
features as word prediction criteria, and thus word
formation information is not well explored. For
more details about Chinese word fomation, see
(Sun et al, 2009a).
2.3.2 Linearity and Nonlinearity
A majority of structured prediction models are
linear models in the sense that the score func-
tions are linear combination of parameters. Both
previous solutions for word-based and character-
based systems utilize linear models. However,
both ?linear? models incur nonlinearity to some
extent. In general, a sequence classification it-
self involves nonlinearity in a way that the features
of current token usually encode previous state in-
formation which is linear combination of features
of previous tokens. The interested readers may
consult (Liang et al, 2008) for preliminary dis-
cussion about the nonlinearity in structured mod-
els. This kind of nonlinearity exists in both word-
based and character-based models. In addition, in
most character-based models, a word should take
a S label or start with a B label, end with E label,
and only have I label inside. This inductive way
for word prediction actually behaves nonlinearly.
2.3.3 Dynamic Tokens or Static Tokens
Since word-based models take the sum of part
score of each individual word token, it increases
the upper bound of the whole score to segment
more words. As a result, word-based segmenter
tends to segment words into smaller pieces. A dif-
ficult case occurs when a word token w consists
of some word types which could be separated as
words on their own. In such cases a word-based
segmenter more easily splits the word into indi-
vidual words. For example, in the phrase ?
/4300 /meter (4300 meters)?, the numeral
?? consists of two individual numeral
types ? (4000)? and ?(300)?. A word-
based segmenter more easily made a mistake to
segment two word tokens. This phenomenon is
very common in named entities.
2.3.4 Word Token or Word Type Features
In character-based models, features are usually
defined by the character information in the neigh-
boring n-character window. Despite a large set
of valuable features that could be expressed, it is
slightly less natural to encode predicted word to-
ken information. On the contrary, taking words
as dynamic tokens, it is very easy to define word
token features in a word-based model. Word-
based segmenters hence have greater representa-
tional power. Despite of the lack of word token
representation ability, character-based segmenters
can use word type features by looking up a dic-
tionary. For example, if a local sequence of char-
acters following current token matches a word in
a dictionary; these word types can be used as fea-
tures. If a string matches a word type, it has a very
high probability (ca. 90%) to be a word token.
So word type features are good approximation of
word token features.
3 Baseline Systems
For empirical analysis, we implement segmenters
in word-based and character-based architectures
respectively. We introduce them from three as-
pects: basic models, parameter estimation and
feature selection.
1213
Algorithm 1: The PA learning procedure.
input : Data {(xt,yt), t = 1, 2, ..., n}
Initialize: w ? (0, ..., 0)1
for I = 1, 2, ... do2
for t = 1, ..., n do3
Predict: y?t =4
arg maxy?GEN(xt) w>?(xt,y)
Suffer loss: lt = ?(yt,y?t ) +5
w>?(xt,y?t )? w>?(xt,yt)
Set: ?t = lt||?(xt,y?t )??(xt,yt)||2+0.5C6
Update:7
w ? w + ?t(?(xt,yt)??(xt,y?t ))
end8
end9
3.1 Models
For both word-based and character-based seg-
menters, we use linear models introduced in the
section above. We use a first order Markov
models for training and testing. In particu-
lar, for word-based segmenter, the local feature
map ?(c, w[1:i]) is defined only on c, wi?1 and
wi, and thereby Eq. 2 is defined as w? =
arg maxw?GEN(c) ?>
?|w|
i=1 ?(c, wi?1, wi). This
model has a first-order Semi-Markov structure.
For decoding, Zhang and Clark (2007) used a
beam search algorithm to get approximate solu-
tions, and Sarawagi and Cohen (2004) introduced
a Viterbi style algorithm for exact inference. Since
the exact inference algorithm is efficient enough,
we use this algorithm in our segmenter at both
training and testing time.
For our character-based segmenter, the local
feature map ?(c, y[1:i]) is defined on c, yi?1
and yi, and Eq. 4 is defined as y? =
arg maxy?Y |c| ?>
?|c|
i=1 ?(?, yi?1, yi). In our
character-based segmenter, we also use a Viterbi
algorithm for decoding.
3.2 Learning
We adopt Passive-Aggressive (PA) framework
(Crammer et al, 2006), a family of margin based
online learning algorithms, for the parameter es-
timation. It is fast and easy to implement. Alg.
1 illustrates the learning procedure. The param-
eter vector w is initialized to (0, ..., 0). A PA
learner processes all the instances (t is from 1
to n) in each iteration (I). If current hypothe-
sis (w) fails to predict xt, the learner update w
through calculating the loss lt and the difference
between ?(xt,y?t ) and ?(xt,yt) (line 5-7). There
are three variants in the update step. We here only
present the PA-II rule1, which performs best in our
experiments.
The PA algorithm utilizes a paradigm of cost-
sensitive learning to resolve structured prediction.
A cost function ? is necessary to calculate the loss
lt (line 5). For every pair of labels (y?,y), users
should define a cost ?(y?,y) associated with pre-
dicting y? when the correct label is y. ? should be
defined differently for different purposes. There
are two natural costs for segmentation: (1) sum
of the number of wrong and missed word predic-
tions and (2) sum of the number of wrongly clas-
sified characters. We tried both cost functions for
both models. We find that the first one is suitable
for word-based segmenter and the second one is
suitable for character-based segmenter. We do not
report segmentation performance with ?weaker?
cost in later sections. C (in line 6) is the slack vari-
able. In our experiments, the segmentation per-
formance is not sensitive to C . In the following
experiments, we set C = 1.
3.3 Features
3.3.1 Word-based Segmenter
For the convenience of illustration, we de-
note a candidate word token wi with a context
cj?1[wi?1cj ...ck][wick+1...cl]cl+1.
The character features includes,
Boundary character unigram: cj , ck, ck+1, cl
and cl+1; Boundary character bigram: ckck+1 and
clcl+1.
Inside character unigram: cs (k + 1 < s < l);
Inside character bigram: cscs+1 (k + 1 < s < l).
Length of current word.
Whether ck+1 and ck+1 are identical.
Combination Features: ck+1 and cl,
The word token features includes,
Word Unigram: previous word wi?1 and cur-
rent word wi; Word Bigram: wi?1wi.
1See the original paper for more details.
1214
The identity of wi, if it is a Single character
word.
Combination Features: wi?1 and length of wi,
wi and length of wi?1. ck+1 and length of wi, cl
and length of wi.
3.3.2 Character-based Segmenter
We use the exact same feature templates dis-
cribed in (Sun et al, 2009b). The features are di-
vided into two types: character features and word
type features. Note that the word type features
are indicator functions that fire when the local
character sequence matches a word unigram or
bigram. Dictionaries containing word unigrams
and bigrams was collected from the training data.
Limited to the document length, we do not give
the discription for the features. We suggest read-
ers to refer to the original paper for details.
4 Empirical Analysis
We present a series of experiments that relate seg-
mentation performance to a set of properties of in-
put words. We argue that the results can be corre-
lated to specific theoretical aspects of each model.
4.1 Experimental Setting
We used the data provided by the second SIGHAN
Bakeoff (Emerson, 2005) to test the two segmen-
tation models. The data contains four corpora
from different sources: Academia Sinica Corpus
(AS), City University of Hong Kong (CU), Mi-
crosoft Research Asia (MSR), and Peking Univer-
sity (PKU). There is no fixed standard for Chinese
word segmentation. The four data sets above are
annotated with different standards. To catch gen-
eral properties, we do experiments on all the four
data sets. Three metrics were used for evaluation:
precision (P), recall (R) and balanced F-score (F)
defined by 2PR/(P+R).
4.2 Baseline Performance
Tab. 1 shows the performance of our two seg-
menters. Numbers of iterations are respectively
set to 15 and 20 for our word-based segmenter and
character-based segmenter. The word-based seg-
menter performs slightly worse than the character-
based segmenter. This is different from the exper-
iments reported in (Zhang and Clark, 2007). We
Model P(%) R(%) F
AS Character 94.8 94.7 94.7
Word 93.5 94.8 94.2
CU Character 95.5 94.6 95.0
Word 94.4 94.7 94.6
MSR Character 96.1 96.5 96.3
Word 96.0 96.3 96.1
PKU Character 94.6 94.9 94.8
Word 94.7 94.3 94.5
Table 1: Baseline performance.
think the main reason is that we use a different
learning architecture.
4.3 Word Frequency Factors
 60
 65
 70
 75
 80
 85
 90
 95
 100
OOV
1 2 3-5 6-10
11-100
101-1000
1001-
R
ec
al
l (%
)
word occurances in training data
AS data set
character-based
word-based
 70
 75
 80
 85
 90
 95
 100
OOV
1 2 3-5 6-10
11-100
101-1000
1001-
R
ec
al
l (%
)
word occurances in training data
CU data set
character-based
word-based
 60
 65
 70
 75
 80
 85
 90
 95
 100
OOV
1 2 3-5 6-10
11-100
101-1000
1001-
R
ec
al
l (%
)
word occurances in training data
MSR data set
character-based
word-based
 60
 65
 70
 75
 80
 85
 90
 95
 100
OOV
1 2 3-5 6-10
11-100
101-1000
1001-
R
ec
al
l (%
)
word occurances in training data
PKU data set
character-based
word-based
Figure 1: Segmentation recall relative to gold
word frequency.
Our theoretical analysis also suggests that
character-based has stronger word induction abil-
ity because it focuses more on word internal struc-
tures and thereby expresses more nonlinearity. To
test the word induction ability, we present the re-
call relative to word frequency. If a word appears
in a training data many times, the learner usually
works in a ?memorizing? way. On the contrary,
infrequent words should be correctly recognized
in a somehow ?inductive? way. Fig. 1 shows
the recall change relative to word frequency in
each training data. Note that, the words with fre-
quency 0 are out-of-vocabulary (OOV) words. We
can clearly see that character-based model outper-
forms word-based model for infrequent word, es-
pecially OOV words, recognition. The ?memoriz-
1215
 76
 78
 80
 82
 84
 86
 88
 90
 92
 94
 96
 98
 1  2  3  4
Pr
ec
is
io
n 
(%
)
word length
AS data set
character-based
word-based
 84
 86
 88
 90
 92
 94
 96
 98
 1  2  3  4
Pr
ec
is
io
n 
(%
)
word length
CU data set
character-based
word-based
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 1  2  3  4
Pr
ec
is
io
n 
(%
)
word length
MSR data set
character-based
word-based
 78
 80
 82
 84
 86
 88
 90
 92
 94
 96
 98
 1  2  3  4
Pr
ec
is
io
n 
(%
)
word length
PKU data set
character-based
word-based
 76
 78
 80
 82
 84
 86
 88
 90
 92
 94
 96
 98
 1  2  3  4
R
ec
al
l (%
)
word length
AS data set
character-based
word-based
 78
 80
 82
 84
 86
 88
 90
 92
 94
 96
 98
 1  2  3  4
R
ec
al
l (%
)
word length
CU data set
character-based
word-based
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 1  2  3  4
R
ec
al
l (%
)
word length
MSR data set
character-based
word-based
 84
 86
 88
 90
 92
 94
 96
 98
 1  2  3  4
R
ec
al
l (%
)
word length
PKU data set
character-based
word-based
Figure 2: Segmentation precision/recall relative to gold word length in training data.
ing? ability of the twomodels is similar; on the AS
and CU data sets, the word-based model performs
slightly better. Neither model is robust enough
to reliably segment unfamiliar words. The recall
of OOV words is much lower than in-vocabulary
words.
4.4 Length Factors
Length AS CU MSR PKU
1 61254 19116 48092 45911
2 52268 18186 49472 49861
3 6990 2682 4652 5132
4 1417 759 2711 2059
5(+) 690 193 1946 656
Table 2: Word length statistics on test sets.
Tab. 2 shows the statistics of word counts
relative to word length on each test data sets.
There are much less words with length more than
4. Analysis on long words may not be statis-
tical significant, so we only present length fac-
tors on small words (length is less than 5). Fig.
2 shows the precision/recall of both segmenta-
tion models relative sentence length. We can see
that word-based model tends to predict more sin-
gle character words, but making more mistakes.
Since about 50% word tokens are single-character
words, this is one main source of error for word-
segmenter. This can be explained by theoretical
properties of dynamic token prediction discussed
in Sec. 2.3.3. The score of a word boundary
assignment in a word-based segmenter is defined
like ?>
?|w|
i=1 ?(c, w[1:i]). The upper bound of this
score varies with the length |w|. If a segmen-
tation result is with more fragments, i.e. |w| is
larger, the upper bound of its score is higher. As
a result, in many cases, a word-based segmenter
prefers shorter words, which may cause errors.
4.5 Feature Factors
We would like to measure the effect of features
empirically. In particular, we do not use dy-
namic word token features in our word-based seg-
menter, and word type features in our character-
based segmenter as comparison with ?standard?
segmenters. The difference in performance can be
seen as the contribution of word features. There
are obvious drops in both cases. Though it is
not a fair comparison, word token features seem
more important, since the numerical decrease in
the word-based experiment is larger.
word-based character-based
? + ? +
AS 93.1 94.2 94.1 94.7
CU 92.6 94.6 94.2 95.0
MSR 95.7 96.1 95.8 96.3
PKU 93.3 94.5 94.4 94.8
Table 3: F-score of two segmenters, with (?) and
without (+) word token/type features.
4.6 Discussion
The experiments highlight the fundamental dif-
ference between word-based and character-based
models, which enlighten us to design new mod-
els. The above analysis indicates that the theoret-
ical differences cause different error distribution.
1216
The two approaches are either based on a particu-
lar view of segmentation. Our analysis points out
several drawbacks of each one. It may be help-
ful for both models to overcome their shortcom-
ings. For example, one weakness of word-based
model is its word induction ability which is par-
tially caused by its neglect of internal structure of
words. A word-based model may be improved by
solving this problem.
5 System Combination
The error analysis also suggests that there is still
space for improvement, just by combining the two
existing models. Here, we introduce a classifier
ensemble method for system combination.
5.1 Upper Bound of System Combination
To get an upper bound of the improvement that
can be obtained by combining the strengths of
each model, we have performed an oracle exper-
iment. We think the optimal combination system
should choose the right prediction when the two
segmenters do not agree with each other. There
is a gold segmenter that generates gold-standard
segmentation results. In the oracle experiment, we
let the three segmenters, i.e. baseline segmenters
and the gold segmenter, vote. The three seg-
menters output three segmentation results, which
are further transformed into IOB2 representa-
tion (Ramshaw and Marcus, 1995). Namely, each
character has three B or I labels. We assign each
character an oracle label which is chosn by at least
two segmenters. When the baseline segmenters
are agree with each other, the gold segmenter can-
not change the segmentation whether it is right
or wrong. In the situation that the two baseline
segmenters disagree, the vote given by the gold
segmenter will decide the right prediction. This
kind of optimal performance is presented in Tab.
4. Compared these results with Tab. 1, we see a
significant increase in accuracy for the four data
sets. The upper bound of error reduction with sys-
tem combination is over 30%.
5.2 Our Model
Bootstrap aggregating (Bagging) is a machine
learning ensemble meta-algorithm to improve
classification and regression models in terms of
P(%) R(%) F ER (%)
AS 96.6 96.9 96.7 37.7
CU 97.4 97.1 97.3 46.0
MSR 97.5 97.7 97.6 35.1
PKU 96.8 96.2 96.5 32.7
Table 4: Upper bound for combination. The error
reduction (ER) rate is a comparison between the
F-score produced by the oracle combination sys-
tem and the character-based system (see Tab. 1).
stability and classification accuracy (Breiman,
1996). It also reduces variance and helps to avoid
overfitting. Given a training set D of size n, Bag-
ging generates m new training sets Di of size
n? ? n, by sampling examples from D uniformly.
The m models are fitted using the above m boot-
strap samples and combined by voting (for classi-
fication) or averaging the output (for regression).
We propose a Bagging model to combine mul-
tiple segmentation systems. In the training phase,
given a training set D of size n, our model gener-
ates m new training sets Di of size 63.2% ? n by
sampling examples from D without replacement.
Namely no example will be repeated in each Di.
Each Di is separately used to train a word-based
segmenter and a character-based segmenter. Us-
ing this strategy, we can get 2m weak segmenters.
Note that the sampling strategy is different from
the standard one. Our experiment shows that there
is no significant difference between the two sam-
pling strategies in terms of accuracy. However,
the non-placement strategy is more efficient. In
the segmentation phase, the 2m models outputs
2m segmentation results, which are further trans-
formed into IOB2 representation. In other words,
each character has 2m B or I labels. The final seg-
mentation is the voting result of these 2m labels.
Note that since 2m is an even number, there may
be equal number of B and I labels. In this case,
our system prefer B to reduce error propagation.
5.3 Results
Fig. 4 shows the influence of m in the bagging
algorithm. Because each new data set Di in bag-
ging algorithm is generated by a random proce-
dure, the performance of all bagging experiments
are not the same. To give a more stable evaluation,
we repeat 5 experiments for each m and show the
1217
 93.5
 94
 94.5
 95
 95.5
 96
 96.5
AS CU MSR PKU
Pr
ec
is
io
n 
(%
)
character-based
word-based
bagging
 94
 94.5
 95
 95.5
 96
 96.5
 97
 97.5
AS CU MSR PKU
R
ec
al
l (%
)
character-based
word-based
bagging
 94
 94.5
 95
 95.5
 96
 96.5
 97
AS CU MSR PKU
F-
m
ea
su
re
character-based
word-based
bagging
Figure 3: Precision/Recall/F-score of different models.
averaged F-score. We can see that the bagging
model taking two segmentation models as basic
systems consistently outperform the baseline sys-
tems and the bagging model taking either model
in isolation as basic systems. An interesting phe-
nomenon is that the bagging method can also im-
prove word-based models. In contrast, there is no
significant change in character-based models.
 93
 93.5
 94
 94.5
 95
 95.5
 1  2  3  4  5  6  7  8  9  10 11 12 13
F-
m
ea
su
re
Number of sampling data sets m
AS data set
baseline (C)
baseline (W)
character-bagging
word-bagging
bagging
 93.5
 94
 94.5
 95
 95.5
 96
 1  2  3  4  5  6  7  8  9  10 11 12 13
F-
m
ea
su
re
Number of sampling data sets m
CU data set
baseline (C)
baseline (W)
character-bagging
word-bagging
bagging
 93.5
 94
 94.5
 95
 95.5
 96
 96.5
 97
 1  2  3  4  5  6  7  8  9  10 11 12 13
F-
m
ea
su
re
Number of sampling data sets m
MSR data set
baseline (C)
baseline (W)
character-bagging
word-bagging
bagging
 93.4
 93.6
 93.8
 94
 94.2
 94.4
 94.6
 94.8
 95
 95.2
 1  2  3  4  5  6  7  8  9  10 11 12 13
F-
m
ea
su
re
Number of sampling data sets m
PKU data set
baseline (C)
baseline (W)
character-bagging
word-bagging
bagging
Figure 4: F-score of bagging models with differ-
ent numbers of sampling data sets. Character-
bagging means that the bagging system built
on the single character-based segmenter. Word-
bagging is named in the same way.
Fig. 3 shows the precision, recall, F-score of
the two baseline systems and our final system for
which we generate m = 15 new data sets for
bagging. We can see significant improvements
on the four datasets in terms of the balanced F-
score. The improvement of precision and recall
are not consistent. The improvement of AS and
CU datasets is from the recall improvement; the
improvement of PKU datasets is from the preci-
sion improvement. We think the different perfor-
mance is mainly because the four datasets are an-
notated by using different standards.
AS CU MSR PKU
(Zhang et al, 2006) 95.1 95.1 97.1 95.1
(Zhang and Clark, 2007) 94.6 95.1 97.2 94.5
(Sun et al, 2009b) N/A 94.6 97.3 95.2
This paper 95.2 95.6 96.9 95.2
Table 5: Segmentation performance presented in
previous work and of our combination model.
Tab. 5 summarizes the performance of our final
system and other systems reported in a majority of
previous work. The left most column indicates the
reference of previous systems that represent state-
of-the-art results. The comparison of the accuracy
between our integrating system and the state-of-
the-art segmentation systems in the literature in-
dicates that our combination system is competi-
tive with the best systems, obtaining the highest
reported F-scores on three data sets.
6 Conclusion
We have presented a thorough study of the dif-
ference between word-based and character-based
segmentation approaches for Chinese. The the-
oretical and empirical analysis provides insights
leading to better models. The strengths and weak-
nesses of the two methods are not exactly the
same. To exploit their complementary strengths,
we propose a Bagging model for system combi-
nation. Experiments show that the combination
strategy is helpful.
Acknowledgments
The work is supported by the project TAKE
(Technologies for Advanced Knowledge Extrac-
tion), funded under contract 01IW08003 by the
German Federal Ministry of Education and Re-
search. The author is also funded by German Aca-
demic Exchange Service (DAAD).
1218
References
Galen Andrew. 2006. A hybrid markov/semi-
markov conditional random field for sequence
segmentation. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 465?472. Association
for Computational Linguistics, Sydney, Aus-
tralia.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for mandarin Chinese sentences.
In Proceedings of the 14th conference on Com-
putational linguistics, pages 101?107. Associ-
ation for Computational Linguistics, Morris-
town, NJ, USA.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. JOUR-
NAL OF MACHINE LEARNING RESEARCH,
7:551?585.
Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In In Pro-
ceedings of the Second SIGHAN Workshop on
Chinese Language Processing, pages 123?133.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic adaptation of annotation standards:
Chinese word segmentation and pos tagging ?
a case study. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 522?530. Association for Computational
Linguistics, Suntec, Singapore.
Percy Liang, Hal Daume?, III, and Dan Klein.
2008. Structure compilation: trading structure
for features. In ICML ?08: Proceedings of
the 25th international conference on Machine
learning, pages 592?599. ACM, New York,
NY, USA.
L. A. Ramshaw and M. P. Marcus. 1995. Text
chunking using transformation-based learning.
In Proceedings of the 3rd ACL/SIGDAT Work-
shop on Very Large Corpora, Cambridge, Mas-
sachusetts, USA, pages 82?94.
Sunita Sarawagi and William W. Cohen. 2004.
Semi-markov conditional random fields for in-
formation extraction. In In Advances in Neu-
ral Information Processing Systems 17, pages
1185?1192.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin
Wang. 2009a. Chinese semantic role label-
ing with shallow parsing. In Proceedings of
the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1475?
1483. Association for Computational Linguis-
tics, Singapore.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2009b. A discriminative latent variable Chinese
segmenter with hybrid word/character informa-
tion. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 56?64. Asso-
ciation for Computational Linguistics, Boulder,
Colorado.
Huihsin Tseng. 2005. A conditional random field
word segmenter. In In Fourth SIGHAN Work-
shop on Chinese Language Processing.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. In International Journal
of Computational Linguistics and Chinese Lan-
guage Processing.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging by con-
ditional random fields for Chinese word seg-
mentation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 193?
196. Association for Computational Linguis-
tics, New York City, USA.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algo-
rithm. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Lin-
guistics, pages 840?847. Association for Com-
putational Linguistics, Prague, Czech Republic.
1219
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970?979,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Enhancing Chinese Word Segmentation Using Unlabeled Data
Weiwei Sun?? and Jia Xu?
?Department of Computational Linguistics, Saarland University
?German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
wsun@coli.uni-saarland.de, Jia.Xu@dfki.de
Abstract
This paper investigates improving supervised
word segmentation accuracy with unlabeled
data. Both large-scale in-domain data and
small-scale document text are considered. We
present a unified solution to include features
derived from unlabeled data to a discrimina-
tive learning model. For the large-scale data,
we derive string statistics from Gigaword to
assist a character-based segmenter. In addi-
tion, we introduce the idea about transductive,
document-level segmentation, which is de-
signed to improve the system recall for out-of-
vocabulary (OOV) words which appear more
than once inside a document. Novel features1
result in relative error reductions of 13.8% and
15.4% in terms of F-score and the recall of
OOV words respectively.
1 Introduction
Chinese sentences are written in continuous se-
quence of characters without explicit delimiters such
as space characters. To find the basic language units,
i.e. words, segmentation is a necessary initial step
for Chinese language processing. Previous research
shows that word segmentation models trained on la-
beled data are reasonably accurate. In this paper,
we investigate improving supervised word segmen-
tation with unlabeled data.
We distinguish three types of unlabeled data,
namely large-scale in-domain data, out-of-domain
data and small-scale document text. Both large-scale
1You can download our derived features at
http://www.coli.uni-saarland.de/?wsun/
semi-cws-feats-emnlp11.tgz.
in-domain and out-of-domain data is popular for en-
hancing NLP tasks. Learning from these two types
of unlabeled data normally involves semi-supervised
learning. The difference between them is that out-
of-domain data is usually used for domain adapta-
tion. For a number of NLP tasks, there are relatively
large amounts of labeled training data. In this sit-
uation, supervised learning can provide competitive
results, and it is difficult to improve them any further
by using extra unlabeled data. Chinese word seg-
mentation is one of this kind of tasks, since several
large-scale manually annotated corpora are publicly
available. In this work, we first exploit unlabeled in-
domain data to improve strong supervised models.
We leave domain adaptation for our future work.
We introduce the third type of unlabeled data with
a transductive learning, document-level view. Many
applications of word segmentation involve process-
ing a whole document, such as information retrieval.
In this situation, the text of the current document
can provide additional useful information to seg-
ment a sentence. Take the word ????/elastane?
for example2. As a translated terminology word, it
lacks compositionality. Moreover, this word appears
rarely in general texts. As a result, if it does not ap-
pear in the training data, it is very hard for statis-
tical models to recognize this word. Nevertheless,
when we deal with an article discussing an elastane
company, this word may appear more than once in
this article, and the document information can help
recognize this word. This idea is closely related to
transductive learning in the sense that the segmen-
tation model knows something about the problem it
2This example is from an article indexed as chtb 0041 in the
Penn Chinese Treebank corpus.
970
is going to resolve. In this work, we are also con-
cerned with enhancing word segmentation with the
document information.
We present a unified ?feature engineering? ap-
proach for learning segmentation models from both
labeled and unlabeled data. Our method is a simple
two-stage process. First, we use unannotated corpus
to extract string and document information, and then
we use these information to construct new statistics-
based and document-based feature mapping for a
discriminative word segmenter. We are relying on
the ability of discriminative learning method to iden-
tify and explore informative features, which play
central role to boost the segmentation performance.
This simple solution has been shown effective for
named entity recognition (Miller et al, 2004) and
dependency parsing (Koo et al, 2008). In their im-
plementations, word clusters derived from unlabeled
data are imported as features to discriminative learn-
ing approaches.
To demonstrate the effectiveness of our approach,
we conduct experiments on the Penn Chinese Tree-
bank (CTB) data. CTB is a collection of docu-
ments which are separately annotated. This anno-
tation style allows us to evaluate our transductive
segmentation method. Our experiments show that
both statistics-based and document-based features
are effective in the word segmentation application.
In general, the use of unlabeled data can be moti-
vated by two concerns: First, given a fixed amount
of labeled data, we might wish to leverage unla-
beled data to improve the performance of a super-
vised model. Second, given a fixed target perfor-
mance level, we might wish to use unlabeled data
to reduce the amount of annotated data necessary
to reach this target. We show that our approach
yields improvements for fixed data sets, even when
large-scale labeled data is available. The new fea-
tures result in relative error reductions of 13.8% and
15.4% in terms of the balanced F-score and the re-
call of out-of-vocabulary (OOV) words respectively.
By conducting experiments on data sets of varying
sizes, we demonstrate that for fixed levels of perfor-
mance, the new features derived from unlabeled data
can significantly reduce the need of labeled data.
The remaining part of the paper is organized as
follows. Section 2 describes the details of our sys-
tem, especially the design of the derived features.
B Current character is the start of a word con-
sisting of more than one character.
E Current character is the end of a word con-
sisting of more than one character.
I Current character is a middle of a word con-
sisting of more than two characters.
S Current character is a word consisting of
only one character.
Table 1: The start/end representation.
Section 3 presents experimental results and empir-
ical analysis. Section 4 reviews the related work.
Section 5 concludes the paper.
2 Method
2.1 Discriminative Character-based Word
Segmentation
The Character-based approach is a dominant word
segmentation solution for Chinese text process-
ing. This approach treats word segmentation as a
sequence tagging problem, assigning labels to the
characters indicating whether a character locates at
the beginning of, inside or at the end of a word. This
character-by-character method was first proposed
by (Xue, 2003), and a number of discriminative
sequential learning algorithms have been exploited,
including structured perceptron (Jiang et al, 2009),
the Passive-Aggressive algorithm (Sun, 2010),
conditional random fields (CRFs) (Tseng et al,
2005), and latent variable CRFs (Sun et al, 2009).
In this work, we use the Start/End representation to
express the position information of every character.
Table 2.1 shows the meaning of each character
label. For example, the target label representation
of the book title ???????????/The Se-
cret Journal of Premier Zhao Ziyang? is as follows.
? ? ? ? ? ? ? ? ? ?
B I E B E S B E B E
Key to our approach is to allow informative fea-
tures derived from unlabeled data to assist the seg-
menter. In our experiments, we employed three
different feature sets: a baseline feature set which
draws upon ?normal? information from labeled
training data, a statistics-based feature set that uses
statistical information derived from a large-scale in-
domain corpus, and a document-based feature set
971
that uses information encoded in the surrounding
text.
2.2 Baseline Features
In this work, to train a good traditional supervised
segmenter, our baseline feature templates includes
the ones described in (Sun et al, 2009; Sun, 2010).
These features are divided into two types: char-
acter features and word type features. Note that
the word type features are indicator functions that
fire when the local character sequence matches a
word uni-gram or bi-gram. Dictionary containing
word uni-grams and bi-grams is collected from the
training data. To conveniently illustrate, we de-
note a candidate character token ci with a context
...ci?1cici+1.... We use c[s:e] to express a string that
starts at the position s and ends at the position e.
For example, c[i:i+1] expresses a character bi-gram
cici+1. The character features are listed below.
? Character uni-grams: cs (i? 3 < s < i+ 3)
? Character bi-grams: cscs+1 (i?3 < s < i+3)
? Whether cs and cs+1 are identical, for i ? 2 <
s < i+ 2.
? Whether cs and cs+2 are identical, for i ? 4 <
s < i+ 2.
The word type features are listed as follows.
? The identity of the string c[s:i] (i? 6 < s < i),
if it matches a word from the list of uni-gram
words;
? The identity of the string c[i:e] (i < e < i + 6),
if it matches a word; multiple features could be
generated.
? The identity of the bi-gram c[s:i?1]c[i:e] (i?6 <
s, e < i+6), if it matches a word bi-gram from
the list of uni-gram words.
? The identity of the bi-gram c[s:i]c[i+1:e] (i?6 <
s, e < i + 6), if it matches a word bi-gram;
multiple features could be generated.
Idiom In linguistics, idioms are usually presumed
to be figures of speech contradicting the principle of
compositionality. As a result, it is very hard to rec-
ognize out-of-vocabulary idioms for word segmen-
tation. Nonetheless, the lexicon of idioms can be
taken as a close set, which helps resolve the problem
well. In our previous work (Sun, 2011), we collect
12992 idioms from several free online Chinese dic-
tionaries. This linguistic resource is publicly avail-
able3. In this paper, we use this idiom dictionary to
derive the following feature.
? Does ci locate at the beginning of, inside or
at the end of an idiom? If the string c[s:i]
(s < i) matches an item from the idiom lexi-
con, the feature template receives a string value
?E-IDIOM?. Similarly, we can define when this
feature ought to be set to ?B-IDIOM? or ?I-
IDIOM?. Note that all idioms are larger than
one character, so there is no ?S-IDIOM? fea-
ture here.
2.3 Statistics-based Features
In order to distill information from unlabeled data,
we borrow ideas from some previous research on
unsupervised word segmentation. The statistical in-
formation acquired from a relatively large amount
of unlabeled data are designed as features correlated
with the position where a character locates in a word
token. These features are based on three widely used
criteria.
2.3.1 Mutual Information
Empirical mutual information is widely used in
NLP. Informally, mutual information compares the
probability of observing x and y together with the
probabilities of observing x and y independently. If
there is a genuine association between x and y, the
I(x, y) = log p(x,y)p(x)p(y) should be greater than 0.Some previous work claimed that the larger
the mutual information between two consecutive
strings, the higher the possibility of the two strings
being combined together. We adopt this idea in our
character-based segmentation model. The empiri-
cal mutual information between two character bi-
grams is computed by counting how often they ap-
pear in the large-scale unlabeled corpus. Given a
3http://www.coli.uni-saarland.de/?wsun/
idiom.txt.
972
Chinese character string c[i?2:i+1], the mutual infor-
mation between substrings c[i?2:i?1] and c[i:i+1] is
computed as:
MI(c[i?2:i?1], c[i:i+1]) = log
p(c[i?2:i+1])
p(c[i?2:i?1])p(c[i:i+1])
For each character ci, we incorporate the MI of the
character bi-grams into our model. They include,
? MI(c[i?2:i?1], c[i:i+1]),
? MI(c[i?1:i], c[i+1:i+2]).
2.3.2 Accessor Variety
When a string appears under different linguistic
environments, it may carry a meaning. This prin-
ciple is introduced as the accessor variety criterion
for identifying meaningful Chinese words in (Feng
et al, 2004). This criterion evaluates how indepen-
dently a string is used, and thus how likely it is that
the string can be a word. Given a string s, which
consists of l (l ? 2) characters, we define the left
accessor variety of Llav(s) as the number of distinct
characters that precede s in a corpus. Similarly, the
right accessor variety Rlav(s) is defined as the num-
ber of distinct characters that succeed s.
We first extract all strings whose length are be-
tween 2 and 4 from the unlabeled data, and calculate
their accessor variety values. For each character ci,
we then incorporate the following information into
our model,
? Accessor variety of strings with length 4:
L4av(c[i:i+3]), L4av(c[i+1:i+4]), R4av(c[i?3:i]),
R4av(c[i?4:i?1]);
? Accessor variety of strings with length 3:
L3av(c[i:i+2]), L3av(c[i+1:i+3]), R3av(c[i?2:i]),
R3av(c[i?3:i?1]);
? Accessor variety of strings with length 2:
L2av(c[i:i+1]), L2av(c[i+1:i+2]), R2av(c[i?1:i]),
R2av(c[i?2:i?1]).
2.3.3 Punctuation as Anchor Words
Punctuation marks are symbols that indicate the
structure and organization of written language, as
well as intonation and pauses to be observed when
reading aloud. Punctuation marks can be taken as
perfect word delimiters, and can be used as anchor
words to harvest lexical knowledge. The preced-
ing and succeeding strings of punctuations carry ad-
ditional wordbreak information, since punctuations
should be segmented as a word. Note that such in-
formation is biased because not all words can appear
before or after punctuations. For example, punctua-
tions can not be followed by particles, such as ???,
??? and ??? which are indicators of aspects. Nev-
ertheless, our experiments will show this kind of in-
formation is still useful for word segmentation.
When a string appears many times preceding or
succeeding punctuations, there tends to be word-
breaks succeeding or preceding that string. To uti-
lize the wordbreak information provided by punctu-
ations, we extract all strings with length l(2 ? l ?
4) which precede or succeed punctuations in the un-
labeled data. We define the left punctuation variety
of Llpv(s) as the number of times a punctuation pre-
cedes s in a corpus. Similarly, the right punctua-
tion variety Rlpv(s) is defined as the number of how
many times a punctuation succeeds s. These two
variables evaluate how likely a string can be sepa-
rated at its start or end positions.
We first gather all strings surrounding punctua-
tions in the unlabeled data, and calculate their punc-
tuation variety values. The length of each string is
also restricted between 2 and 4. For each charac-
ter ci, we import the following information into our
model,
? Punctuation variety of strings with length 4:
L4pv(c[i:i+3]), R4pv(c[i?3:i]);
? Punctuation variety of strings with length 3:
L3pv(c[i:i+2]), R3pv(c[i?2:i]);
? Punctuation variety of strings with length 2:
L2pv(c[i:i+1]), R2pv(c[i?1:i]).
Punctuations can be viewed as mark-up?s of Chi-
nese text. Our motivation to use the punctuation in-
formation to assist a word segmenter is similiar to
(Spitkovsky et al, 2010) in a way to explore ?artifi-
cial? word (or phrase) break symbols. In their work,
four common HTML tags are successfully used as
raw phrase bracketings to improve unsupervised de-
pendency parsing.
973
2.3.4 Binary or Numeric Features
The derived information introduced above is all
expressed as real values. The natural way to in-
corporate these statistics into a discriminative learn-
ing model is to directly use them as numeric fea-
tures. However, our experiments show that this sim-
ple choice does not work well. The reason is that
these statistics actually behave non-linearly to pre-
dict character labels. For each type of statistics, one
weight alone cannot capture the relation between its
value and the possibility that a string forms a word.
Instead, we represent these statistics as discrete fea-
tures.
For the mutual information, this is done by round-
ing down decimal number. The integer part of each
MI value is used as a string feature. For the ac-
cessor variety and punctuation variety information,
since their values are integer, we can directly use
them as string features. The accessor variety and
punctuation variety could be very large, so we set
thresholds to cut off large values to deal with the
data sparse problem. Specially, if an accessor va-
riety value is greater than 50, it is incorporated as
a feature ?> 50?; if the value is greater than 30
but not greater than 50, it is incorporated as a fea-
ture ?30 ? 50?; else the value is individually in-
corporated as a string feature. For example, if the
left accessory variety of a character bi-gram c[i:i+1]
is 29, the binary feature ?L2av(c[i:i+1])=29? will be
set to 1, while other related binary features such as
?L2av(c[i:i+1]) = 15? or ?L2av(c[i:i+1]) > 50? will
be set to 0. Similarly, we can discretize the punc-
tuation variety features. However, we only set one
threshold, 30, for this value. These thresholds can
be tuned by using held-out data.
2.4 Document-based Features
It is meaningless to derive statistics of a document
and use it for word segmentation, since most doc-
uments are relatively short, and values are statisti-
cally unreliable. Our experiments confirm this idea.
Instead, we propose the following binary features
which are based on the string count in the given doc-
ument that is simply the number of times a given
string appears in that document. For each character
ci, our document-based features include,
? Whether the string count of c[s:i] is equal to that
of c[s:i+1] (i ? 3 ? s ? i). Multiple features
are generated for different string length.
? Whether the string count of c[i:e] is equal to that
of c[i?1:e] (i ? e ? i + 3). Multiple features
are generated for different string length.
The intuition is as follows. The string counts of
c[s:i] and c[s:i+1] being equal means that when c[s:i]
appears, it appears inside c[s:i+1]. In this case, c[s:i]
is not independently used in this document, and this
feature suggests the segmenter not assign a ?S? or
?E? label to the character ci. Similarly, the string
counts of c[i:e] and c[i?1:e] being equal means c[i:e]
is not independently used in this document, and this
feature suggests segmenter not assign a ?S? or ?B?
label to ci. We do not directly use the string counts
to prevent a bias towards longer documents.
3 Experiments
3.1 Setting
The SIGHAN Bakeoffs provide several large-scale
labeled data for the research on Chinese word seg-
mentation. Although these data sets are labeled on
continuous run texts, they do not contain the docu-
ment boundary information. CTB is a segmented,
part-of-speech tagged, and fully bracketed corpus
in the constituency formalism. It is also an popu-
lar data set to evaluate word segmentation methods,
such as (Jiang et al, 2009; Sun, 2011). CTB is a
collection of documents which are separately anno-
tated. This annotation style allows us to calculate
the so-called document-based features and to further
evaluate our approach. In this paper, we use CTB 6.0
as our main corpus and define the training, develop-
ment and test sets according to the Chinese sub-task
of the CoNLL 2009 shared task4. Table 2 shows the
statistics of our experimental settings.
Data set # of sent. # of words # of char.
Training 22277 609060 1004266
Devel. 1763 49646 83710
Test 2557 73152 121008
Table 2: Training, development and test data on CTB 6.0
4We would like to thank Prof. Nianwen Xue for the help
with the division of the data
974
Chinese Gigaword is a comprehensive archive
of newswire text data that has been acquired over
several years by the Linguistic Data Consortium
(LDC). The large-scale unlabeled data we use in
our experiments comes from the Chinese Gigaword
(LDC2005T14). We choose the Mandarin news text,
i.e. Xinhua newswire. This data covers all news
published by Xinhua News Agency (the largest news
agency in China) from 1991 to 2004, which contains
over 473 million characters.
F-score is used as the accuracy measure. Define
precision p as the percentage of words in the decoder
output that are segmented correctly, and recall r as
the percentage of gold standard output words that are
correctly segmented by the decoder. The (balanced)
F-score is 2pr/(p + r). We also report the recall of
OOV words. Note that, all idioms in our extra idiom
lexicon are added into the in-vocabulary word list.
CRFsuite (Okazaki, 2007) is an implementation
of Conditional Random Fields (CRFs) (Lafferty
et al, 2001) for labeling sequential data. It is a
speed-oriented implementation, which is written in
pure C. In our experiments, we use this toolkit to
learn global linear models for segmentation. We use
the stochastic gradient descent algorithm to resolve
the optimization problem, and set default values for
other learning parameters.
3.2 Main Results
Table 3 summarizes the segmentation results on the
development data with different configurations, rep-
resenting a few choices between baseline, statistics-
based and document-based feature sets. In this table,
the symbol ?+? means features of current configura-
tion contains both the baseline features and new fea-
tures for semi-supervised or transductive learning.
From this table, we can clearly see the impact of fea-
tures derived from the large-scale unlabeled data and
the current document. Comparison between the per-
formance of the baseline and ?+MI? shows that the
widely used mutual information is not helpful. Both
good segmentation techniques and valuable labeled
corpora have been developed, and pure supervised
systems can provide strong performance. It is not
a trial to design new features to enhance supervised
models.
There are significant increases when accessor va-
riety features and punctuation variety features are
Devel. P R F?=1 Roov
Baseline 95.41 95.52 95.46 77.68
+MI 95.50 95.48 95.49 77.98
+AV(2) 95.85 96.04 95.94 79.31
+AV(2,3) 95.95 96.19 96.07 80.61
+AV(2,3,4) 96.14 95.99 96.07 81.83
+PU(2) 95.86 96.07 95.97 79.70
+PU(2,3) 95.98 96.25 96.11 80.42
+PU(2,3,4) 96.00 96.19 96.10 80.53
+MI+AV(2,3,4)+PU(2,3,4)
96.17 96.22 96.19 80.42
+DOC 95.69 95.64 95.66 79.89
+MI+AV(2,3,4)+PU(2,3,4)+DOC
96.21 96.23 96.22 81.75
Table 3: Segmentation performance with different feature
sets on the development data. Abbreviations: MI=mutual
information; AV=accessor variety; PU=punctuation va-
riety; DOC=document features. The numbers in each
bracket pair are the lengths of strings. For example,
PU(2,3) means punctuation variety features of character
bi-grams and tri-grams are added.
separately added. Extending the length of neigh-
boring string helps a little from 2 to 3. Al-
though the OOV recall increases when the length
is extended from 3 to 4, there is no improve-
ment of the overall balanced F-score. The
line ?+MI+AV(2,3,4)+PU(2,3,4)? shows the perfor-
mance when all statistics-based features are added.
The combination of the ?AV? and ?PU? features
gives further helps. This system can be seen as a
pure semi-supervised system. The line ?+DOC? is
the result when document-based features are added.
In spite of its simplicity, the document-based fea-
tures can help the task. However, when we combine
statistics-based features with document-based fea-
tures, we cannot get further improvement in terms
of F-score.
Table 4 shows the segmentation perfor-
mance on the test data set. The final re-
sults of our system are achieved with the
?+MI+AV(2,3,4)+PU(2,3,4)+DOC? feature config-
uration. The new features result in relative error
reductions of 13.8% and 15.4% in terms of the
balanced F-score and the recall of OOV words
respectively.
975
 87 88
 89 90
 91 92
 93 94
 95 96
 97
 100  200  300  400  500  600  700  800  900  1000
F
-
s
c
o
r
e
Training data size (thousands of characters)
Baseline features+Statistics-based features+Document-based featuresAll features
 68
 70
 72
 74
 76
 78
 80
 82
 84
 100  200  300  400  500  600  700  800  900  1000
O
O
V
 
R
e
c
a
l
l
 
(
%
)
Training data size (thousands of characters)
Baseline features+Statistics-based features+Document-based featuresAll features
Figure 1: The learning curves of different models.
-15-10
-5 0
 5 10
 15 20
 25
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?B?
-20-15
-10-5
 0 5
 10 15
 20
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?I?
-30-25
-20-15
-10-5
 0 5
 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?E?
-15-10
-5 0
 5 10
 15
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?S?
-14-12
-10-8
-6-4
-2 0
 2 4
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?B?
-10-5
 0 5
 10 15
 20 25
 30
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?I?
-5 0
 5 10
 15 20
 25
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?E?
-25-20
-15-10
-5 0
 5 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?S?
Figure 2: Scatter plot of feature score against feature value. The left side shows is L2pv(c[i:i+1] feature while the right
side is the R2pv(c[i:i+1] feature.
Test P R F?=1 Roov
Baseline 95.21 94.90 95.06 75.52
Final 95.86 95.62 95.74 79.28
Table 4: Segmentation performance on the test data.
3.3 Learning Curves
We performed additional experiments to evaluate the
effect of the derived features as the amount of train-
ing data is varied. Figure 1 displays the F-score
and the OOV recall of systems with different feature
sets when trained on smaller portions of the labeled
data. Note that there is no change in the configura-
tion of the unlabeled data. We can clearly see that
the derived features obtain consistent gains regard-
less of the size of the training set. The improvement
is more significant when little labeled data is ap-
plied. Both statistics-based features and document-
based features can help improve the overall perfor-
mance. Especially, they can help to recognize more
unknown words, which is important for many appli-
cations. The F-score of semi-supervised models, i.e.
models trained with statistics-based features, does
not achieve further improvement when document-
based features are added. Nonetheless, the OOV re-
call obtains slightly improvements.
It is interesting to consider the amount by which
derived features reduce the need for supervised data,
given a desired level of accuracy. The change of
the F-score in Figure 1 suggests that derived fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
model with extra features trained on 500k characters
976
is slightly higher than the performance of the model
with only baseline features trained on the whole la-
beled data.
3.4 Feature Analysis
We discussed the choice of using binary or numeric
features in Section 2.3.4. In our experiment, when
the accessor variety and punctuation variety infor-
mation are integrated as numeric features, they do
not contribute. To show the non-linear way that
these features contribute to the prediction problem,
we present the scatter plots of the score of each
feature (i.e. the weight multiply the feature value)
against the value of the feature. Figure 2 shows
the relation between the score and the value of
the punctuation variety features. For example, the
weight of the binary feature ?L2pu(c[i:i+1]) = 26
combined with the label ?B? learned by the final
model is 0.815141, so the score of this combina-
tion is 0.815141 ? 26 = 21.193666 and a point
(26, 21.193666) is drawn. These plots indicate the
punctuation variety features contribute to the final
model in a very complicated way. It is impossible
to use one weight to capture it. The accessor va-
riety features affect the model in the same way, so
we do not give detailed discussions. We only show
the same scatter plot of the L2av(c[i:i+1]) feature tem-
plate in Figure 3.
-20-15
-10-5
 0 5
 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?B?
-10-8
-6-4
-2 0
 2 4
 6 8
 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?I?
-10-8
-6-4
-2 0
 2 4
 6 8
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?E?
-5
 0
 5
 10
 15
 20
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?S?
Figure 3: Scatter plot of feature score against feature
value for L2av(c[i:i+1]).
4 Related Work
Xu et al (2008) presented a Bayesian semi-
supervised approach to derive task-oriented word
segmentation for machine translation (MT). This
method learns new word types and word distribu-
tions on unlabeled data by considering segmentation
as a hidden variable in MT. Different from their con-
cern, our focus is general word segmentation.
The ?feature-engineering? semi-supervised ap-
proach has been successfully applied to named en-
tity recognition (Miller et al, 2004) and depen-
dency parsing (Koo et al, 2008). These two papers
demonstrated the effectiveness of using word clus-
ters as features in discriminative learning. More-
over, Turian et al (2010) compared different word
clustering algorithms and evaluated their effect on
both named entity recognition and text chunking.
As mentioned earlier, the feature design is in-
spired by some previous research on word segmen-
tation. The accessor variety criterion is proposed to
extract word types, i.e. the list of possible words,
in (Feng et al, 2004). Different from their work,
our method resolves the segmentation problem of
running texts, in which this criterion is used to de-
fine features correlated with the character position
labels. Li and Sun (2009) observed that punctuations
are perfect delimiters which provide useful informa-
tion for segmentation. Their method can be viewed
as a self-training procedure, in which extra punctu-
ation information is incorporated to filter out auto-
matically predicted samples. We use the punctua-
tion information in a different way. In our method,
the counts of the preceding and succeeding strings
of punctuations are incorporated directly as features
into a supervised model.
In machine learning, transductive learning is a
learning framework that typically makes use of un-
labeled data. The goal of transductive learning is
to only infer labels for the unlabeled data points in
the test set rather than to learn a general classifica-
tion function that can be applied to any future data
sets. This means that the test data is known as a
priori knowledge and can be used to construct bet-
ter hypotheses. Although the idea to explore the
document-level information in our work is similar
to transductive learning, we do not use state-of-the-
art transductive learning algorithms which involve
learning when they meet the test data. For real-world
applications, our approach is efficient by avoiding
re-training.
977
5 Conclusion and Future Work
In this paper, we have presented a simple yet effec-
tive approach to explore unlabeled data for Chinese
word segmentation. We are concerned with large-
scale in-domain data and the document text. Ex-
periments show that our approach achieves substan-
tial improvement over a competitive baseline. Es-
pecially, the informative features derived from un-
labeled data lead to significant improvement of the
recall of unknown words. Our immediate concern
for future work is to exploit the out-of-domain data
to improve the robustness of current word segmen-
tation systems. The idea would be to extract do-
main information from unlabeled data and define
them as features in our unified approach. The word-
based approach is an alternative for word segmenta-
tion. This kind of segmenters sequentially predicts
whether the local sequence of characters make up a
word. A natural avenue for future work is the exten-
sion of our method to the word-based approach. The
word segmentation task is similar to constituency
parsing, in the sense of finding boundaries of lan-
guage units. Another interesting question is whether
our method can be adapted to resolve constituency
parsing.
Acknowledgments
The work is supported by the project TAKE (Tech-
nologies for Advanced Knowledge Extraction),
funded under contract 01IW08003 by the German
Federal Ministry of Education and Research. The
author is also funded by German Academic Ex-
change Service (DAAD).
References
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Comput. Linguist., 30:75?
93.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic adaptation of annotation standards:
Chinese word segmentation and pos tagging ? a
case study. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 522?
530. Association for Computational Linguistics,
Suntec, Singapore.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Proceedings of ACL-08: HLT, pages 595?
603. Association for Computational Linguistics,
Columbus, Ohio.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ?01: Proceedings of
the Eighteenth International Conference on Ma-
chine Learning, pages 282?289. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
Zhongguo Li and Maosong Sun. 2009. Punctuation
as implicit annotations for Chinese word segmen-
tation. Comput. Linguist., 35:505?512.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 337?342. As-
sociation for Computational Linguistics, Boston,
Massachusetts, USA.
Naoaki Okazaki. 2007. Crfsuite: a fast implementa-
tion of conditional random fields (crfs).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan
Alshawi. 2010. Profiting from mark-up: Hyper-
text annotations for guided parsing. In Proceed-
ings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1278?
1287. Association for Computational Linguistics,
Uppsala, Sweden.
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Comparison
and combination. In Coling 2010: Posters, pages
1211?1219. Coling 2010 Organizing Committee,
Beijing, China.
Weiwei Sun. 2011. A stacked sub-word model
for joint Chinese word segmentation and part-of-
speech tagging. In Proceedings of the ACL 2011
Conference. Association for Computational Lin-
guistics, Portland, Oregon, United States.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii. 2009. A
978
discriminative latent variable Chinese segmenter
with hybrid word/character information. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 56?64. Association for Com-
putational Linguistics, Boulder, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In In
Fourth SIGHAN Workshop on Chinese Language
Processing.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple and
general method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
384?394. Association for Computational Linguis-
tics, Uppsala, Sweden.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised Chi-
nese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics
(Coling 2008), pages 1017?1024. Coling 2008
Organizing Committee, Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. In International Journal
of Computational Linguistics and Chinese Lan-
guage Processing.
979
Proceedings of the ACL 2010 Conference Short Papers, pages 103?108,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling
Weiwei Sun
Department of Computational Linguistics, Saarland University
German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
wsun@coli.uni-saarland.de
Abstract
One deficiency of current shallow pars-
ing based Semantic Role Labeling (SRL)
methods is that syntactic chunks are too
small to effectively group words. To par-
tially resolve this problem, we propose
semantics-driven shallow parsing, which
takes into account both syntactic struc-
tures and predicate-argument structures.
We also introduce several new ?path? fea-
tures to improve shallow parsing based
SRL method. Experiments indicate that
our new method obtains a significant im-
provement over the best reported Chinese
SRL result.
1 Introduction
In the last few years, there has been an increas-
ing interest in Semantic Role Labeling (SRL) on
several languages, which consists of recognizing
arguments involved by predicates of a given sen-
tence and labeling their semantic types. Both
full parsing based and shallow parsing based SRL
methods have been discussed for English and Chi-
nese. In Chinese SRL, shallow parsing based
methods that cast SRL as the classification of
syntactic chunks into semantic labels has gained
promising results. The performance reported in
(Sun et al, 2009) outperforms the best published
performance of full parsing based SRL systems.
Previously proposed shallow parsing takes into
account only syntactic information and basic
chunks are usually too small to group words into
argument candidates. This causes one main defi-
ciency of Chinese SRL. To partially resolve this
problem, we propose a new shallow parsing. The
new chunk definition takes into account both syn-
tactic structure and predicate-argument structures
of a given sentence. Because of the semantic in-
formation it contains, we call it semantics-driven
shallow parsing. The key idea is to make basic
chunks as large as possible but not overlap with ar-
guments. Additionally, we introduce several new
?path? features to express more structural infor-
mation, which is important for SRL.
We present encouraging SRL results on Chinese
PropBank (CPB) data. With semantics-driven
shallow parsing, our SRL system achieves 76.10
F-measure, with gold segmentation and POS tag-
ging. The performance further achieves 76.46
with the help of new ?path? features. These re-
sults obtain significant improvements over the best
reported SRL performance (74.12) in the literature
(Sun et al, 2009).
2 Related Work
CPB is a project to add predicate-argument rela-
tions to the syntactic trees of the Chinese Tree-
Bank (CTB). Similar to English PropBank, the ar-
guments of a predicate are labeled with a contigu-
ous sequence of integers, in the form of AN (N is
a natural number); the adjuncts are annotated as
such with the label AM followed by a secondary
tag that represents the semantic classification of
the adjunct. The assignment of argument labels
is illustrated in Figure 1, where the predicate is the
verb ???/provide? For example, the noun phrase
?????/the insurance company? is labeled as
A0, meaning that it is the proto-Agent of ????.
Sun et al (2009) explore the Chinese SRL prob-
lem on the basis of shallow syntactic information
at the level of phrase chunks. They present a se-
mantic chunking method to resolve SRL on basis
of shallow parsing. Their method casts SRL as
the classification of syntactic chunks with IOB2
representation for semantic roles (i.e. semantic
103
WORD: ?? ?? ? ? ?? ?? ?? ?? ??
insurance company already for Sanxia Project provide insurance service
POS: [NN NN] [AD] [P] [NR] [NN] [VV] [NN NN]
SYN CH: [NP] [ADVP] [PP NP NP ] [VP] [NP]
SEM CH: B-A0 B-AM-ADV B-A2 I-A2 I-A2 B-V B-A1
The insurance company has provided insurance services for the Sanxia Project.
Figure 1: An example from Chinese PropBank.
chunks). Two labeling strategies are presented: 1)
directly tagging semantic chunks in one-stage, and
2) identifying argument boundaries as a chunking
task and labeling their semantic types as a clas-
sification task. On the basis of syntactic chunks,
they define semantic chunks which do not overlap
nor embed using IOB2 representation. Syntactic
chunks outside a chunk receive the tag O (Out-
side). For syntactic chunks forming a chunk of
type A*, the first chunk receives the B-A* tag (Be-
gin), and the remaining ones receive the tag I-A*
(Inside). Then a SRL system can work directly
by using sequence tagging technique. Shallow
chunk definition presented in (Chen et al, 2006)
is used in their experiments. The definition of syn-
tactic and semantic chunks is illustrated Figure 1.
For example, ?????/the insurance company?,
consisting of two nouns, is a noun phrase; in the
syntactic chunking stage, its two components ??
?? and ???? should be labeled as B-NP and
I-NP. Because this phrase is the Agent of the pred-
icate ???/provide?, it takes a semantic chunk
label B-A0. In the semantic chunking stage, this
phrase should be labeled as B-A0.
Their experiments on CPB indicate that accord-
ing to current state-of-the-art of Chinese parsing,
SRL systems on basis of full parsing do not per-
form better than systems based on shallow parsing.
They report the best SRL performance with gold
segmentation and POS tagging as inputs. This is
very different from English SRL. In English SRL,
previous work shows that full parsing, both con-
stituency parsing and dependency parsing, is nec-
essary.
Ding and Chang (2009) discuss semantic
chunking methods without any parsing informa-
tion. Different from (Sun et al, 2009), their
method formulates SRL as the classification of
words with semantic chunks. Comparison of ex-
perimental results in their work shows that parsing
is necessary for Chinese SRL, and the semantic
chunking methods on the basis of shallow parsing
outperform the ones without any parsing.
Joint learning of syntactic and semantic struc-
tures is another hot topic in dependency parsing
research. Some models have been well evalu-
ated in CoNLL 2008 and 2009 shared tasks (Sur-
deanu et al, 2008; Hajic? et al, 2009). The
CoNLL 2008/2009 shared tasks propose a unified
dependency-based formalism to model both syn-
tactic dependencies and semantic roles for multi-
ple languages. Several joint parsing models are
presented in the shared tasks. Our focus is differ-
ent from the shared tasks. In this paper, we hope
to find better syntactic representation for semantic
role labeling.
3 Semantics-Driven Shallow Parsing
3.1 Motivation
There are two main jobs of semantic chunking: 1)
grouping words as argument candidate and 2) clas-
sifying semantic types of possible arguments. Pre-
viously proposed shallow parsing only considers
syntactic information and basic chunks are usu-
ally too small to effectively group words. This
causes one main deficiency of semantic chunking.
E.g. the argument ??????/for the Sanxia
Project? consists of three chunks, each of which
only consists of one word. To rightly recognize
this A2, Semantic chunker should rightly predict
three chunk labels. Small chunks also make the
important ?path? feature sparse, since there are
more chunks between a target chunk and the pred-
icate in focus. In this section, we introduce a new
chunk definition to improve shallow parsing based
SRL, which takes both syntactic and predicate-
argument structures into account. The key idea
is to make syntactic chunks as large as possible
for semantic chunking. The formal definition is as
follows.
3.2 Chunk Bracketing
Given a sentence s = w1, ..., wn, let c[i : j]
denote a constituent that is made up of words
between wi and wj (including wi and wj); let
pv = {c[i : j]|c[i : j] is an argument of v}
104
WORD POS TARGET PROPOSITION CHUNK 1 CHUNK 2
China ?? NR - (A0* * * * B-NP B-NP?S
tax ?? NN - * * * * I-NP I-NP?S
department ?? NN - *) * * * I-NP I-NP?S
stipulate ?? VV ?? (V*) * * * O O
: ? PU - * * * * O O
owing ?? VV ?? (A1* (V*) * (A0* O O
tax payment ?? NN - * (A1*) * * B-NP B-NP?VP
company ?? NN - * (A0*) * * B-NP B-NP?NP
Function Word ? DEG - * * * * O O
leaders ??? NN - * * * *) B-NP B-NP?NP
not ? AD - * * * (AM-ADV*) B-ADVP B-ADVP?VP
can ? VV ? * * (V*) * O O
leave the country ?? VV ?? *) * * (V*) B-VP B-VP?VP
Figure 2: An example for definition of semantics-driven chunks with IOB2 representation.
denote one predicate-argument structure where v
is the predicate in focus. Given a syntactic tree
Ts = {c[i : j]|c[i : j] is a constituent of s}, and
its all argument structures Ps = {pv| v is a verbal
predicate in s}, there is one and only one chunk
set C = {c[i : j]} s.t.
1. ?c[i : j] ? C, c[i : j] ? Ts;
2. ?c[i : j] ? C, ?c[iv : jv] ? ?Ps, j < iv or
i > jv or iv ? i ? j ? jv;
3. ?c[i : j] ? C, the parent of c[i : j] does not
satisfy the condition 2.
4. ?C? satisfies above conditions, C? ? C.
The first condition guarantees that every chunk
is a constituent. The second condition means that
chunks do not overlap with arguments, and further
guarantees that semantic chunking can recover all
arguments with the last condition. The third condi-
tion makes new chunks as big as possible. The last
one makes sure that C contains all sub-components
of all arguments. Figure 2 is an example to illus-
trate our new chunk definition. For example, ??
?/Chinese ??/tax ??/department? is a con-
stituent of current sentence, and is also an argu-
ment of ???/stipulate?. If we take it as a chunk,
it does not conflict with any other arguments, so
it is a reasonable syntactic chunk. For the phrase
???/owing ??/tax payment?, though it does
not overlap with the first, third and fourth proposi-
tions, it is bigger than the argument ???? (con-
flicting with condition 2) while labeling the pred-
icate ????, so it has to be separated into two
chunks. Note that the third condition also guar-
antees the constituents in C does not overlap with
each other since each one is as large as possible.
So we can still formulate our new shallow parsing
as an ?IOB? sequence labeling problem.
3.3 Chunk Type
We introduce two types of chunks. The first is
simply the phrase type, such as NP, PP, of cur-
rent chunk. The column CHUNK 1 illustrates
this kind of chunk type definition. The second is
more complicated. Inspired by (Klein and Man-
ning, 2003), we split one phrase type into several
subsymbols, which contain category information
of current constituent?s parent. For example, an
NP immediately dominated by a S, will be sub-
stituted by NP?S. This strategy severely increases
the number of chunk types and make it hard to
train chunking models. To shrink this number, we
linguistically use a cluster of CTB phrasal types,
which was introduced in (Sun and Sui, 2009). The
column CHUNK 2 illustrates this definition. E.g.,
NP?S implicitly represents Subject while NP?VP
represents Object.
3.4 New Path Features
The Path feature is defined as a chain of base
phrases between the token and the predicate. At
both ends, the chain is terminated with the POS
tags of the predicate and the headword of the to-
ken. For example, the path feature of ????
?? in Figure 1 is ???-ADVP-PP-NP-NP-VV?.
Among all features, the ?path? feature contains
more structural information, which is very impor-
tant for SRL. To better capture structural infor-
mation, we introduce several new ?path? features.
They include:
? NP|PP|VP path: only syntactic chunks
that takes tag NP, PP or VP are kept.
105
When labeling the predicate ???/leave the
country? in Figure 2, this feature of ??
?????/Chinese tax departments? is
NP+NP+NP+NP+VP.
? V|? path: a sequential container of POS tags
of verbal words and ???; This feature of ??
?????? is NP+VV+VV+?+VV+VP.
? O2POS path: if a word occupies a chunk
label O, use its POS in the path fea-
ture. This feature of ???????? is
NP+VV+PU+VV+NP+NP+DEG+ADVP+
VV+VP.
4 Experiments and Analysis
4.1 Experimental Setting
Experiments in previous work are mainly based
on CPB 1.0 and CTB 5.0. We use CoNLL-2005
shared task software to process CPB and CTB. To
facilitate comparison with previous work, we use
the same data setting with (Xue, 2008). Nearly
all previous research on Chinese SRL evalua-
tion use this setting, also including (Ding and
Chang, 2008, 2009; Sun et al, 2009; Sun, 2010).
The data is divided into three parts: files from
chtb 081 to chtb 899 are used as training set; files
from chtb 041 to chtb 080 as development set;
files from chtb 001 to chtb 040, and chtb 900 to
chtb 931 as test set. Both syntactic chunkers and
semantic chunkers are trained and evaluated by us-
ing the same data set. By using CPB and CTB, we
can extract gold standard semantics-driven shal-
low chunks according to our definition. We use
this kind of gold chunks automatically generated
from training data to train syntactic chunkers.
For both syntactic and semantic chunking, we
used conditional random field model. Crfsgd1, is
used for experiments. Crfsgd provides a feature
template that defines a set of strong word and POS
features to do syntactic chunking. We use this
feature template to resolve shallow parsing. For
semantic chunking, we implement a similar one-
stage shallow parsing based SRL system described
in (Sun et al, 2009). There are two differences be-
tween our system and Sun et al?s system. First,
our system uses Start/End method to represent se-
mantic chunks (Kudo and Matsumoto, 2001). Sec-
ond, word formation features are not used.
Test P(%) R(%) F?=1
(Chen et al, 2006) 93.51 92.81 93.16
Overall (C1) 91.66 89.13 90.38
Bracketing (C1) 92.31 89.72 91.00
Overall (C2) 88.77 86.71 87.73
Bracketing (C2) 92.71 90.55 91.62
Table 1: Shallow parsing performance.
4.2 Syntactic Chunking Performance
Table 1 shows the performance of shallow syntac-
tic parsing. Line Chen et al, 2006 is the chunk-
ing performance evaluated on syntactic chunk def-
inition proposed in (Chen et al, 2006). The sec-
ond and third blocks present the chunking perfor-
mance with new semantics-driven shallow pars-
ing. The second block shows the overall perfor-
mance when the first kind of chunks type is used,
while the last block shows the performance when
the more complex chunk type definition is used.
For the semantic-driven parsing experiments, we
add the path from current word to the first verb be-
fore or after as two new features. Line Bracketing
evaluates the word grouping ability of these two
kinds of chunks. In other words, detailed phrase
types are not considered. Because the two new
chunk definitions use the same chunk boundaries,
the fourth and sixth lines are comparable. There
is a clear decrease between the traditional shallow
parsing (Chen et al, 2006) and ours. We think one
main reason is that syntactic chunks in our new
definition are larger than the traditional ones. An
interesting phenomenon is that though the second
kind of chunk type definition increases the com-
plexity of the parsing job, it achieves better brack-
eting performance.
4.3 SRL Performance
Table 2 summarizes the SRL performance. Line
Sun et al, 2009 is the SRL performance reported
in (Sun et al, 2009). To the author?s knowledge,
this is the best published SRL result in the liter-
ature. Line SRL (Chen et al, 2006) is the SRL
performance of our system. These two systems
are both evaluated by using syntactic chunking de-
fined in (Chen et al, 2006). From the first block
we can see that our semantic chunking system
reaches the state-of-the-art. The second and third
blocks in Table 2 present the performance with
1http://leon.bottou.org/projects/sgd
106
new shallow parsing. Line SRL (C1) and SRL (C2)
show the overall performances with the first and
second chunk definition. The lines following are
the SRL performance when new ?path? features
are added. We can see that new ?path? features
are useful for semantic chunking.
Test P(%) R(%) F?=1
(Sun et al, 2009) 79.25 69.61 74.12
SRL [(Chen et al, 2006)] 80.87 68.74 74.31
SRL [C1] 80.23 71.00 75.33
+ NP|PP|VP path 80.25 71.19 75.45
+ V|? path 80.78 71.67 75.96
+ O2POS path 80.44 71.59 75.76
+ All new path 80.73 72.08 76.16
SRL [C2] 80.87 71.86 76.10
+ All new path 81.03 72.38 76.46
Table 2: SRL performance on the test data. Items
in the first column SRL [(Chen et al, 2006)], SRL
[C1] and SRL [C2] respetively denote the SRL
systems based on shallow parsing defined in (Chen
et al, 2006) and Section 3.
5 Conclusion
In this paper we propose a new syntactic shal-
low parsing for Chinese SRL. The new chunk
definition contains both syntactic structure and
predicate-argument structure information. To im-
prove SRL, we also introduce several new ?path?
features. Experimental results show that our new
chunk definition is more suitable for Chinese SRL.
It is still an open question what kinds of syntactic
information is most important for Chinese SRL.
We suggest that our attempt at semantics-driven
shallow parsing is a possible way to better exploit
this problem.
Acknowledgments
The author is funded both by German Academic
Exchange Service (DAAD) and German Research
Center for Artificial Intelligence (DFKI).
The author would like to thank the anonymous
reviewers for their helpful comments.
References
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 97?104. As-
sociation for Computational Linguistics, Syd-
ney, Australia.
Weiwei Ding and Baobao Chang. 2008. Improv-
ing Chinese semantic role classification with hi-
erarchical feature selection strategy. In Pro-
ceedings of the EMNLP 2008, pages 324?
333. Association for Computational Linguis-
tics, Honolulu, Hawaii.
Weiwei Ding and Baobao Chang. 2009. Fast se-
mantic role labeling for Chinese based on se-
mantic chunking. In ICCPOL ?09: Proceed-
ings of the 22nd International Conference on
Computer Processing of Oriental Languages.
Language Technology for the Knowledge-
based Economy, pages 79?90. Springer-Verlag,
Berlin, Heidelberg.
Jan Hajic?, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Anto`nia
Mart??, Llu??s Ma`rquez, Adam Meyers, Joakim
Nivre, Sebastian Pado?, Jan S?te?pa?nek, Pavel
Stran?a?k, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning (CoNLL-2009), June 4-5. Boulder,
Colorado, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, pages 423?430. As-
sociation for Computational Linguistics, Sap-
poro, Japan.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL ?01:
Second meeting of the North American Chapter
of the Association for Computational Linguis-
tics on Language technologies 2001, pages 1?
8. Association for Computational Linguistics,
Morristown, NJ, USA.
Weiwei Sun. 2010. Improving Chinese semantic
role labeling with rich features. In Proceedings
of the ACL 2010.
Weiwei Sun and Zhifang Sui. 2009. Chinese func-
tion tag labeling. In Proceedings of the 23rd
Pacific Asia Conference on Language, Informa-
tion and Computation. Hong Kong.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin
Wang. 2009. Chinese semantic role labeling
107
with shallow parsing. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1475?1483.
Association for Computational Linguistics, Sin-
gapore.
Mihai Surdeanu, Richard Johansson, Adam Mey-
ers, Llu??s Ma`rquez, and Joakim Nivre. 2008.
The conll 2008 shared task on joint parsing of
syntactic and semantic dependencies. In CoNLL
2008: Proceedings of the Twelfth Conference
on Computational Natural Language Learning,
pages 159?177. Coling 2008 Organizing Com-
mittee, Manchester, England.
Nianwen Xue. 2008. Labeling Chinese predi-
cates with semantic roles. Comput. Linguist.,
34(2):225?255.
108
Proceedings of the ACL 2010 Conference Short Papers, pages 168?172,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving Chinese Semantic Role Labeling with Rich Syntactic Features
Weiwei Sun?
Department of Computational Linguistics, Saarland University
German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
wsun@coli.uni-saarland.de
Abstract
Developing features has been shown cru-
cial to advancing the state-of-the-art in Se-
mantic Role Labeling (SRL). To improve
Chinese SRL, we propose a set of ad-
ditional features, some of which are de-
signed to better capture structural infor-
mation. Our system achieves 93.49 F-
measure, a significant improvement over
the best reported performance 92.0. We
are further concerned with the effect
of parsing in Chinese SRL. We empiri-
cally analyze the two-fold effect, grouping
words into constituents and providing syn-
tactic information. We also give some pre-
liminary linguistic explanations.
1 Introduction
Previous work on Chinese Semantic Role La-
beling (SRL) mainly focused on how to imple-
ment SRL methods which are successful on En-
glish. Similar to English, parsing is a standard
pre-processing for Chinese SRL. Many features
are extracted to represent constituents in the input
parses (Sun and Jurafsky, 2004; Xue, 2008; Ding
and Chang, 2008). By using these features, se-
mantic classifiers are trained to predict whether a
constituent fills a semantic role. Developing fea-
tures that capture the right kind of information en-
coded in the input parses has been shown crucial
to advancing the state-of-the-art. Though there
has been some work on feature design in Chinese
SRL, information encoded in the syntactic trees is
not fully exploited and requires more research ef-
fort. In this paper, we propose a set of additional
?The work was partially completed while this author was
at Peking University.
features, some of which are designed to better cap-
ture structural information of sub-trees in a given
parse. With help of these new features, our sys-
tem achieves 93.49 F-measure with hand-crafted
parses. Comparison with the best reported results,
92.0 (Xue, 2008), shows that these features yield a
significant improvement of the state-of-the-art.
We further analyze the effect of syntactic pars-
ing in Chinese SRL. The main effect of parsing
in SRL is two-fold. First, grouping words into
constituents, parsing helps to find argument candi-
dates. Second, parsers provide semantic classifiers
plenty of syntactic information, not to only recog-
nize arguments from all candidate constituents but
also to classify their detailed semantic types. We
empirically analyze each effect in turn. We also
give some preliminary linguistic explanations for
the phenomena.
2 Chinese SRL
The Chinese PropBank (CPB) is a semantic anno-
tation for the syntactic trees of the Chinese Tree-
Bank (CTB). The arguments of a predicate are la-
beled with a contiguous sequence of integers, in
the form of AN (N is a natural number); the ad-
juncts are annotated as such with the label AM
followed by a secondary tag that represents the se-
mantic classification of the adjunct. The assign-
ment of semantic roles is illustrated in Figure 1,
where the predicate is the verb ???/investigate?.
E.g., the NP ?????/the cause of the accident?
is labeled as A1, meaning that it is the Patient.
In previous research, SRL methods that are suc-
cessful on English are adopted to resolve Chinese
SRL (Sun and Jurafsky, 2004; Xue, 2008; Ding
and Chang, 2008, 2009; Sun et al, 2009; Sun,
2010). Xue (2008) produced complete and sys-
tematic research on full parsing based methods.
168
IP
bbbbbb
bbbbbb
bbbbbb
bbbbbb
bbbbbb
bbb
A0 VP
dddd
dddd
dddd
dddd
dddd
dd
iii
iii
iii
iii
NP AM-TMP AM-MNR VP
ZZZZ
ZZZZ
ZZZZ
ZZZZ
ZZZZ
ZZ
NN ADVP ADVP Rel A1
??
police
AD AD VV NP
iii
iii
iii
iii
??
now
??
thoroughly
??
investigate
NN NN
??
accident
??
cause
Figure 1: An example sentence: The police are
thoroughly investigating the cause of the accident.
Their method divided SRL into three sub-tasks: 1)
pruning with a heuristic rule, 2) Argument Identi-
fication (AI) to recognize arguments, and 3) Se-
mantic Role Classification (SRC) to predict se-
mantic types. The main two sub-tasks, AI and
SRC, are formulated as two classification prob-
lems. Ding and Chang (2008) divided SRC into
two sub-tasks in sequence: Each argument should
first be determined whether it is a core argument or
an adjunct, and then be classified into fine-grained
categories. However, delicately designed features
are more important and our experiments suggest
that by using rich features, a better SRC solver
can be directly trained without using hierarchical
architecture. There are also some attempts at re-
laxing the necessity of using full syntactic parses,
and semantic chunking methods have been intro-
duced by (Sun et al, 2009; Sun, 2010; Ding and
Chang, 2009).
2.1 Our System
We implement a three-stage (i.e. pruning, AI and
SRC) SRL system. In the pruning step, our sys-
tem keeps all constituents (except punctuations)
that c-command1 current predicate in focus as ar-
gument candidates. In the AI step, a lot of syntac-
tic features are extracted to distinguish argument
and non-argument. In other words, a binary classi-
fier is trained to classify each argument candidate
as either an argument or not. Finally, a multi-class
classifier is trained to label each argument recog-
nized in the former stage with a specific semantic
role label. In both AI and SRC, the main job is to
select strong syntactic features.
1See (Sun et al, 2008) for detailed definition.
3 Features
A majority of features used in our system are a
combination of features described in (Xue, 2008;
Ding and Chang, 2008) as well as the word for-
mation and coarse frame features introduced in
(Sun et al, 2009), the c-command thread fea-
tures proposed in (Sun et al, 2008). We give
a brief description of features used in previous
work, but explain new features in details. For
more information, readers can refer to relevant
papers and our source codes2 that are well com-
mented. To conveniently illustrate, we denote
a candidate constituent ck with a fixed context
wi?1[ckwi...wh...wj ]wj+1, where wh is the head
word of ck, and denote predicate in focus with
a context wv?2w
v
?1w
vwv+1w
v
+2, where w
v is the
predicate in focus.
3.1 Baseline Features
The following features are introduced in previous
Chinese SRL systems. We use them as baseline.
Word content of wv, wh, wi, wj and wi+wj ;
POS tag of wv, wh. subcategorization frame, verb
class of wv; position, phrase type ck, path from ck
to wv (from (Xue, 2008; Ding and Chang, 2008))
First character, last character and word length
of wv, first character+length, last character+word
length, first character+position, last charac-
ter+position, coarse frame, frame+wv, frame+left
character, frame+verb class, frame+ck (from (Sun
et al, 2009)).
Head word POS, head word of PP phrases, cat-
egory of ck?s lift and right siblings, CFG rewrite
rule that expands ck and ck?s parent (from (Ding
and Chang, 2008)).
3.2 New Word Features
We introduce some new features which can be
extracted without syntactic structure. We denote
them as word features. They include:
Word content of wv?1, w
v
+1, wi?1 and wj+1;
POS tag of wv?1, w
v
+1, w
v
?2, w
v
+2, wi?1, wi, wj ,
wj+1, wi+2 and wj?2.
Length of ck: how many words are there in ck.
Word before ?LC?: If the POS of wj is ?LC?
(localizer), we use wj?1 and its POS tag as two
new features.
NT: Does ck contain a word with POS ?NT?
(temporal noun)?
2Available at http://code.google.com/p/
csrler/.
169
Combination features: wi?s POS+wj?s POS,
wv+Position
3.3 New Syntactic Features
Taking complex syntax trees as inputs, the clas-
sifiers should characterize their structural proper-
ties. We put forward a number of new features to
encode the structural information.
Category of ck?s parent; head word and POS of
head word of parent, left sibling and right sibling
of ck.
Lexicalized Rewrite rules: Conjuction of
rewrite rule and head word of its corresponding
RHS. These features of candidate (lrw-c) and its
parent (lrw-p) are used. For example, this lrw-
c feature of the NP ?????? in Figure 1 is
NP ? NN +NN (??).
Partial Path: Path from the ck or wv to the low-
est common ancestor of ck and wv. One path fea-
ture, hence, is divided into left path and right path.
Clustered Path: We use the manually created
clusters (see (Sun and Sui, 2009)) of categories of
all nodes in the path (cpath) and right path.
C-commander thread between ck and wv (cct):
(proposed by (Sun et al, 2008)). For example, this
feature of the NP ???? in Figure 1 is NP +
ADV P +ADV P + V V .
Head Trace: The sequential container of the
head down upon the phrase (from (Sun and Sui,
2009)). We design two kinds of traces (htr-p, htr-
w): one uses POS of the head word; the other uses
the head word word itself. E.g., the head word of
???? is ???? therefore these feature of this
NP are NP?NN and NP???.
Combination features: verb class+ck, wh+wv,
wh+Position, wh+wv+Position, path+wv,
wh+right path, wv+left path, frame+wv+wh,
and wv+cct.
4 Experiments and Analysis
4.1 Experimental Setting
To facilitate comparison with previous work, we
use CPB 1.0 and CTB 5.0, the same data set-
ting with (Xue, 2008). The data is divided into
three parts: files from 081 to 899 are used as
training set; files from 041 to 080 as develop-
ment set; files from 001 to 040, and 900 to 931
as test set. Nearly all previous research on con-
stituency based SRL evaluation use this setting,
also including (Ding and Chang, 2008, 2009; Sun
et al, 2009; Sun, 2010). All parsing and SRL ex-
periments use this data setting. To resolve clas-
sification problems, we use a linear SVM classi-
fier SVMlin3, along with One-Vs-All approach for
multi-class classification. To evaluate SRL with
automatic parsing, we use a state-of-the-art parser,
Bikel parser4 (Bikel, 2004). We use gold segmen-
tation and POS as input to the Bikel parser and
use it parsing results as input to our SRL system.
The overall LP/LR/F performance of Bikel parser
is 79.98%/82.95%/81.43.
4.2 Overall Performance
Table 1 summarizes precision, recall and F-
measure of AI, SRC and the whole task (AI+SRC)
of our system respectively. The forth line is
the best published SRC performance reported in
(Ding and Chang, 2008), and the sixth line is the
best SRL performance reported in (Xue, 2008).
Other lines show the performance of our system.
These results indicate a significant improvement
over previous systems due to the new features.
Test P(%) R(%) F/A
AI 98.56 97.91 98.24
SRC - - - - 95.04
(Ding and Chang, 2008) - - - - 94.68
AI + SRC 93.80 93.18 93.49
(Xue, 2008) 93.0 91.0 92.0
Table 1: SRL performance on the test data with
gold standard parses.
4.3 Two-fold Effect of Parsing in SRL
The effect of parsing in SRL is two-fold. On the
one hand, SRL systems should group words as ar-
gument candidates, which are also constituents in
a given sentence. Full parsing provides bound-
ary information of all constituents. As arguments
should c-command the predicate, a full parser can
further prune a majority of useless constituents. In
other words, parsing can effectively supply SRL
with argument candidates. Unfortunately, it is
very hard to rightly produce full parses for Chi-
nese text. On the other hand, given a constituent,
SRL systems should identify whether it is an argu-
ment and further predict detailed semantic types if
3http://people.cs.uchicago.edu/
?vikass/svmlin.html
4http://www.cis.upenn.edu/?dbikel/
software.html
170
Task Parser Bracket Feat P(%) R(%) F/A
AI - - Gold W 82.44 86.78 84.55
CTB Gold W+S 98.69 98.11 98.40
Bikel Bikel W+S 77.54 71.62 74.46
SRC - - Gold W - - - - 93.93
CTB Gold W+S - - - - 95.80
Bikel Gold W+S - - - - 92.62
Table 2: Classification perfromance on develop-
ment data. In the Feat column, W means word
features; W+S means word and syntactic feautres.
it is an argument. For the two classification prob-
lems, parsing can provide complex syntactic infor-
mation such as path features.
4.3.1 The Effect of Parsing in AI
In AI, full parsing is very important for both
grouping words and classification. Table 2 sum-
marizes relative experimental results. Line 2 is the
AI performance when gold candidate boundaries
and word features are used; Line 3 is the perfor-
mance with additional syntactic features. Line 4
shows the performance by using automatic parses
generated by Bikel parser. We can see that: 1)
word features only cannot train good classifiers to
identify arguments; 2) it is very easy to recognize
arguments with good enough syntactic parses; 3)
there is a severe performance decline when auto-
matic parses are used. The third observation is a
similar conclusion in English SRL. However this
problem in Chinese is much more serious due to
the state-of-the-art of Chinese parsing.
Information theoretic criteria are popular cri-
teria in variable selection (Guyon and Elisse-
eff, 2003). This paper uses empirical mutual
information between each variable and the tar-
get, I(X,Y ) =
?
x?X,y?Y p(x, y) log
p(x,y)
p(x)p(y) , to
roughly rank the importance of features. Table 3
shows the ten most useful features in AI. We can
see that the most important features all based on
full parsing information. Nine of these top 10 use-
ful features are our new features.
Rank Feature Rank Feature
1 wv cct 2 ? wh+wv+Position
3 htr-w 4 htr-p
5 path 6 ? wh+wv
7 cpath 8 cct
9 path+wv 10 lrw-p
Table 3: Top 10 useful features for AI. ? means
word features.
4.3.2 The Effect of Parsing in SRC
The second block in Table 2 summarizes the SRC
performance with gold argument boundaries. Line
5 is the accuracy when word features are used;
Line 6 is the accuracy when additional syntactic
features are added; The last row is the accuracy
when syntactic features used are extracted from
automatic parses (Bikel+Gold). We can see that
different from AI, word features only can train
reasonable good semantic classifiers. The com-
parison between Line 5 and 7 suggests that with
parsing errors, automatic parsed syntactic features
cause noise to the semantic role classifiers.
4.4 Why Word Features Are Effective for
SRC?
Rank Feature Rank Feature
1 ?frame+wh+wv 2 ?wh+wv+position
3 ?wh+wv 4 wv+cct
5 lrw-p 6 ?wi+wj
7 lrw-c 8 ?wh+Postion
9 ?frame+wv 10 htr-p
Table 4: Top 10 useful features for SRC.
Table 4 shows the ten most useful features in
SRC. We can see that two of these ten features
are word features (denoted by ?). Namely, word
features play a more important role in SRC than
in AI. Though the other eight features are based
on full parsing, four of them (denoted by ?) use
the head word which can be well approximated
by word features, according to some language spe-
cific properties. The head rules described in (Sun
and Jurafsky, 2004) are very popular in Chinese
parsing research, such as in (Duan et al, 2007;
Zhang and Clark, 2008). From these head rules,
we can see that head words of most phrases in
Chinese are located at the first or the last position.
We implement these rules on Chinese Tree Bank
and find that 84.12% 5 nodes realize their heads as
either their first or last word. Head position sug-
gests that boundary words are good approximation
of head word features. If head words have good
approximation word features, then it is not strange
that the four features denoted by ? can be effec-
tively represented by word features. Similar with
feature effect in AI, most of most useful features
in SRC are our new features.
5This statistics excludes all empty categories in CTB.
171
5 Conclusion
This paper proposes an additional set of features
to improve Chinese SRL. These new features yield
a significant improvement over the best published
performance. We further analyze the effect of
parsing in Chinese SRL, and linguistically explain
some phenomena. We found that (1) full syntactic
information playes an essential role only in AI and
that (2) due to the head word position distribution,
SRC is easy to resolve in Chinese SRL.
Acknowledgments
The author is funded both by German Academic
Exchange Service (DAAD) and German Research
Center for Artificial Intelligence (DFKI).
The author would like to thank the anonymous
reviewers for their helpful comments.
References
Daniel M. Bikel. 2004. A distributional analysis
of a lexicalized statistical parsing model. In
Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 182?189. Associa-
tion for Computational Linguistics, Barcelona,
Spain.
Weiwei Ding and Baobao Chang. 2008. Improv-
ing Chinese semantic role classification with hi-
erarchical feature selection strategy. In Pro-
ceedings of the EMNLP 2008, pages 324?
333. Association for Computational Linguis-
tics, Honolulu, Hawaii.
Weiwei Ding and Baobao Chang. 2009. Fast se-
mantic role labeling for Chinese based on se-
mantic chunking. In ICCPOL ?09: Proceed-
ings of the 22nd International Conference on
Computer Processing of Oriental Languages.
Language Technology for the Knowledge-
based Economy, pages 79?90. Springer-Verlag,
Berlin, Heidelberg.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007.
Probabilistic models for action-based Chinese
dependency parsing. In ECML ?07: Pro-
ceedings of the 18th European conference on
Machine Learning, pages 559?566. Springer-
Verlag, Berlin, Heidelberg.
Isabelle Guyon and Andre? Elisseeff. 2003. An
introduction to variable and feature selec-
tion. Journal of Machine Learning Research,
3:1157?1182.
Honglin Sun and Daniel Jurafsky. 2004. Shallow
semantc parsing of Chinese. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings.
Weiwei Sun. 2010. Semantics-driven shallow
parsing for Chinese semantic role labeling. In
Proceedings of the ACL 2010.
Weiwei Sun and Zhifang Sui. 2009. Chinese func-
tion tag labeling. In Proceedings of the 23rd
Pacific Asia Conference on Language, Informa-
tion and Computation. Hong Kong.
Weiwei Sun, Zhifang Sui, and Haifeng Wang.
2008. Prediction of maximal projection for se-
mantic role labeling. In Proceedings of the
22nd International Conference on Computa-
tional Linguistics.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin
Wang. 2009. Chinese semantic role labeling
with shallow parsing. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1475?1483.
Association for Computational Linguistics, Sin-
gapore.
Nianwen Xue. 2008. Labeling Chinese predi-
cates with semantic roles. Comput. Linguist.,
34(2):225?255.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-
based and transition-based dependency parsing.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 562?571. Association for Computa-
tional Linguistics, Honolulu, Hawaii.
172
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1385?1394,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Stacked Sub-Word Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
Weiwei Sun
Department of Computational Linguistics, Saarland University
German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
wsun@coli.uni-saarland.de
Abstract
The large combined search space of joint word
segmentation and Part-of-Speech (POS) tag-
ging makes efficient decoding very hard. As a
result, effective high order features represent-
ing rich contexts are inconvenient to use. In
this work, we propose a novel stacked sub-
word model for this task, concerning both ef-
ficiency and effectiveness. Our solution is
a two step process. First, one word-based
segmenter, one character-based segmenter and
one local character classifier are trained to pro-
duce coarse segmentation and POS informa-
tion. Second, the outputs of the three pre-
dictors are merged into sub-word sequences,
which are further bracketed and labeled with
POS tags by a fine-grained sub-word tag-
ger. The coarse-to-fine search scheme is effi-
cient, while in the sub-word tagging step rich
contextual features can be approximately de-
rived. Evaluation on the Penn Chinese Tree-
bank shows that our model yields improve-
ments over the best system reported in the lit-
erature.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are necessary initial steps for more advanced
Chinese language processing tasks, such as pars-
ing and semantic role labeling. Joint approaches
that resolve the two tasks simultaneously have re-
ceived much attention in recent research. Previous
work has shown that joint solutions led to accu-
racy improvements over pipelined systems by avoid-
ing segmentation error propagation and exploiting
POS information to help segmentation. A challenge
for joint approaches is the large combined search
space, which makes efficient decoding and struc-
tured learning of parameters very hard. Moreover,
the representation ability of models is limited since
using rich contextual word features makes the search
intractable. To overcome such efficiency and effec-
tiveness limitations, the approximate inference and
reranking techniques have been explored in previous
work (Zhang and Clark, 2010; Jiang et al, 2008b).
In this paper, we present an effective and effi-
cient solution for joint Chinese word segmentation
and POS tagging. Our work is motivated by several
characteristics of this problem. First of all, a major-
ity of words are easy to identify in the segmentation
problem. For example, a simple maximum match-
ing segmenter can achieve an f-score of about 90.
We will show that it is possible to improve the ef-
ficiency and accuracy by using different strategies
for different words. Second, segmenters designed
with different views have complementary strength.
We argue that the agreements and disagreements of
different solvers can be used to construct an inter-
mediate sub-word structure for joint segmentation
and tagging. Since the sub-words are large enough
in practice, the decoding for POS tagging over sub-
words is efficient. Finally, the Chinese language is
characterized by the lack of morphology that often
provides important clues for POS tagging, and the
POS tags contain much syntactic information, which
need context information within a large window for
disambiguation. For example, Huang et al (2007)
showed the effectiveness of utilizing syntactic infor-
mation to rerank POS tagging results. As a result,
the capability to represent rich contextual features
is crucial to a POS tagger. In this work, we use
a representation-efficiency tradeoff through stacked
learning, a way of approximating rich non-local fea-
1385
tures.
This paper describes a novel stacked sub-word
model. Given multiple word segmentations of one
sentence, we formally define a sub-word structure
that maximizes the agreement of non-word-break
positions. Based on the sub-word structure, joint
word segmentation and POS tagging is addressed as
a two step process. In the first step, one word-based
segmenter, one character-based segmenter and one
local character classifier are used to produce coarse
segmentation and POS information. The results of
the three predictors are then merged into sub-word
sequences, which are further bracketed and labeled
with POS tags by a fine-grained sub-word tagger. If
a string is consistently segmented as a word by the
three segmenters, it will be a correct word prediction
with a very high probability. In the sub-word tag-
ging phase, the fine-grained tagger mainly considers
its POS tag prediction problem. For the words that
are not consistently predicted, the fine-grained tag-
ger will also consider their bracketing problem. The
coarse-to-fine scheme significantly improves the ef-
ficiency of decoding. Furthermore, in the sub-word
tagging step, word features in a large window can be
approximately derived from the coarse segmentation
and tagging results. To train a good sub-word tagger,
we use the stacked learning technique, which can ef-
fectively correct the training/test mismatch problem.
We conduct our experiments on the Penn Chinese
Treebank and compare our system with the state-
of-the-art systems. We present encouraging results.
Our system achieves an f-score of 98.17 for the word
segmentation task and an f-score of 94.02 for the
whole task, resulting in relative error reductions of
14.1% and 5.5% respectively over the best system
reported in the literature.
The remaining part of the paper is organized as
follows. Section 2 gives a brief introduction to the
problem and reviews the relevant previous research.
Section 3 describes the details of our method. Sec-
tion 4 presents experimental results and empirical
analyses. Section 5 concludes the paper.
2 Background
2.1 Problem Definition
Given a sequence of characters c = (c1, ..., c#c),
the task of word segmentation and POS tagging is
to predict a sequence of word and POS tag pairs
y = (?w1, p1?, ?w#y, p#y?), where wi is a word, pi
is its POS tag, and a ?#? symbol denotes the number
of elements in each variable. In order to avoid error
propagation and make use of POS information for
word segmentation, the two tasks should resolved
jointly. Previous research has shown that the inte-
grated methods outperformed pipelined systems (Ng
and Low, 2004; Jiang et al, 2008a; Zhang and Clark,
2008).
2.2 Character-Based and Word-Based
Methods
Two kinds of approaches are popular for joint word
segmentation and POS tagging. The first is the
?character-based? approach, where basic process-
ing units are characters which compose words. In
this kind of approach, the task is formulated as
the classification of characters into POS tags with
boundary information. Both the IOB2 representa-
tion (Ramshaw and Marcus, 1995) and the Start/End
representation (Kudo and Matsumoto, 2001) are
popular. For example, the label B-NN indicates that
a character is located at the begging of a noun. Using
this method, POS information is allowed to inter-
act with segmentation. Note that word segmentation
can also be formulated as a sequential classification
problem to predict whether a character is located at
the beginning of, inside or at the end of a word. This
character-by-character method for segmentation was
first proposed in (Xue, 2003), and was then further
used in POS tagging in (Ng and Low, 2004). One
main disadvantage of this model is the difficulty in
incorporating the whole word information.
The second kind of solution is the ?word-based?
method, where the basic predicting units are words
themselves. This kind of solver sequentially decides
whether the local sequence of characters makes up
a word as well as its possible POS tag. In partic-
ular, a word-based solver reads the input sentence
from left to right, predicts whether the current piece
of continuous characters is a word token and which
class it belongs to. Solvers may use previously pre-
dicted words and their POS information as clues to
find a new word. After one word is found and classi-
fied, solvers move on and search for the next possi-
ble word. This word-by-word method for segmenta-
tion was first proposed in (Zhang and Clark, 2007),
1386
and was then further used in POS tagging in (Zhang
and Clark, 2008).
In our previous work(Sun, 2010), we presented
a theoretical and empirical comparative analysis of
character-based and word-based methods for Chi-
nese word segmentation. We showed that the two
methods produced different distributions of segmen-
tation errors in a way that could be explained by
theoretical properties of the two models. A system
combination method that leverages the complemen-
tary strength of word-based and character-based seg-
mentation models was also successfully explored in
their work. Different from our previous focus, the
diversity of different models designed with different
views is utilized to construct sub-word structures in
this work. We will discuss the details in the next
section.
2.3 Stacked Learning
Stacked generalization is a meta-learning algorithm
that was first proposed in (Wolpert, 1992) and
(Breiman, 1996). The idea is to include two ?levels?
of predictors. The first level includes one or more
predictors g1, ...gK : Rd ? R; each receives input
x ? Rd and outputs a prediction gk(x). The second
level consists of a single function h : Rd+K ? R
that takes as input ?x, g1(x), ..., gK(x)? and outputs
a final prediction y? = h(x, g1(x), ..., gK(x)).
Training is done as follows. The training data S =
{(xt,yt) : t ? [1, T ]} is split into L equal-sized dis-
joint subsets S1, ..., SL. Then functions g1, ...,gL
(where gl = ?gl1, ..., g
l
K?) are seperately trained on
S ? Sl, and are used to construct the augmented
dataset S? = {(?xt, y?1t , ..., y?
K
t ?,yt) : y?
k
t = g
l
k(xt)
and xt ? Sl}. Finally, each gk is trained on the origi-
nal dataset and the second level predictor h is trained
on S?. The intent of the cross-validation scheme is
that ykt is similar to the prediction produced by a
predictor which is learned on a sample that does not
include xt.
Stacked learning has been applied as a system en-
semble method in several NLP tasks, such as named
entity recognition (Wu et al, 2003) and dependency
parsing (Nivre and McDonald, 2008). This frame-
work is also explored as a solution for learning non-
local features in (Torres Martins et al, 2008). In
the machine learning research, stacked learning has
been applied to structured prediction (Cohen and
Carvalho, 2005). In this work, stacked learning is
used to acquire extended training data for sub-word
tagging.
3 Method
3.1 Architecture
In our stacked sub-word model, joint word segmen-
tation and POS tagging is decomposed into two
steps: (1) coarse-grained word segmentation and
tagging, and (2) fine-grained sub-word tagging. The
workflow is shown in Figure 1. In the first phase, one
word-based segmenter (SegW) and one character-
based segmenter (SegC) are trained to produce word
boundaries. Additionally, a local character-based
joint segmentation and tagging solver (SegTagL) is
used to provide word boundaries as well as inaccu-
rate POS information. Here, the word local means
the labels of nearby characters are not used as fea-
tures. In other words, the local character classi-
fier assumes that the tags of characters are indepen-
dent of each other. In the second phase, our system
first combines the three segmentation and tagging
results to get sub-words which maximize the agree-
ment about word boundaries. Finally, a fine-grained
sub-word tagger (SubTag) is applied to bracket sub-
words into words and also to obtain their POS tags.
Raw sentences
Character-based
segmenter SegC
Local character
classifier
SegTagL
Word-based
Segmenter SegW
Segmented
sentences
Segmented
sentences
Segmented
sentences
Merging
Sub-word
sequences
Sub-word tag-
ger SubTag
Figure 1: Workflow of the stacked sub-word model.
In our model, segmentation and POS tagging in-
teract with each other in two processes. First, al-
though SegTagL is locally trained, it resolves the
1387
two sub-tasks simultaneously. Therefore, in the sub-
word generating stage, segmentation and POS tag-
ging help each other. Second, in the sub-word tag-
ging stage, the bracketing and the classification of
sub-words are jointly resolved as one sequence la-
beling problem.
Our experiments on the Penn Chinese Treebank
will show that the word-based and character-based
segmenters and the local tagger on their own pro-
duce high quality word boundaries. As a result, the
oracle performance to recover words from a sub-
word sequence is very high. The quality of the fi-
nal tagger relies on the quality of the sub-word tag-
ger. If a high performance sub-word tagger can be
constructed, the whole task can be well resolved.
The statistics will also empirically show that sub-
words are significantly larger than characters and
only slightly smaller than words. As a result, the
search space of the sub-word tagging is significantly
shrunken, and exact Viterbi decoding without ap-
proximately pruning can be efficiently processed.
This property makes nearly all popular sequence la-
beling algorithms applicable.
Zhang et al (2006) described a sub-word based
tagging model to resolve word segmentation. To
get the pieces which are larger than characters but
smaller than words, they combine a character-based
segmenter and a dictionary matching segmenter.
Our contributions include (1) providing a formal
definition of our sub-word structure that is based on
multiple segmentations and (2) proposing a stacking
method to acquire sub-words.
3.2 The Coarse-grained Solvers
We systematically described the implementation of
two state-of-the-art Chinese word segmenters in
word-based and character-based architectures, re-
spectively (Sun, 2010). Our word-based segmenter
is based on a discriminative joint model with a
first order semi-Markov structure, and the other seg-
menter is based on a first order Markov model. Ex-
act Viterbi-style search algorithms are used for de-
coding. Limited to the document length, we do not
give the description of the features. We refer readers
to read the above paper for details. For parameter
estimation, our work adopt the Passive-Aggressive
(PA) framework (Crammer et al, 2006), a family
of margin based online learning algorithms. In this
work, we introduce two simple but important refine-
ments: (1) to shuffle the sample orders in each itera-
tion and (2) to average the parameters in each itera-
tion as the final parameters.
Idiom In linguistics, idioms are usually presumed
to be figures of speech contradicting the principle
of compositionality. As a result, it is very hard to
recognize out-of-vocabulary idioms for word seg-
mentation. However, the lexicon of idioms can be
taken as a close set, which helps resolve the problem
well. We collect 12992 idioms1 from several on-
line Chinese dictionaries. For both word-based and
character-based segmentation, we first match every
string of a given sentence with idioms. Every sen-
tence is then splitted into smaller pieces which are
seperated by idioms. Statistical segmentation mod-
els are then performed on these smaller character se-
quences.
We use a local classifier to predict the POS
tag with positional information for each character.
Each character can be assigned one of two possi-
ble boundary tags: ?B? for a character that begins a
word and ?I? for a character that occurs in the mid-
dle of a word. We denote a candidate character to-
ken ci with a fixed window ci?2ci?1cici+1ci+2. The
following features are used:
? character uni-grams: ck (i? 2 ? k ? i+ 2)
? character bi-grams: ckck+1 (i? 2 ? k ? i+1)
To resolve the classification problem, we use the lin-
ear SVM classifier LIBLINEAR2.
3.3 Merging Multiple Segmentation Results
into Sub-Word Sequences
A majority of words are easy to identify in the seg-
mentation problem. We favor the idea treating dif-
ferent words using different strategies. In this work
we try to identify simple and difficult words first and
to integrate them into a sub-word level. Inspired by
previous work, we constructed this sub-word struc-
ture by using multiple solvers designed from differ-
ent views. If a piece of continuous characters is con-
sistently segmented by multiple segmenters, it will
1This resource is publicly available at http://www.
coli.uni-saarland.de/?wsun/idioms.txt.
2Available at http://www.csie.ntu.edu.tw/
?cjlin/liblinear/.
1388
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Answer: [P] [JJ] [ NN ] [ CD ] [M] [VV] [ JJ ] [ NN ]
SegW: [] [] [ ] [ ] [ ] [ ] [ ] [ ]
SegC: [] [] [ ] [ ] [] [ ] [ ]
SegTagL: [P] [JJ] [ NN ] [ CD ] [NT] [CD] [NT] [VV] [ VV ] [ NN ]
Sub-words: [P] [JJ] [ NN ] [ B-CD ] [I-CD] [NT] [CD] [NT] [VV] [ VV ] [ NN ]
Figure 2: An example phrase: ???????????????? (Being in front with a total score of 355.35
points).
not be separated in the sub-word tagging step. The
intuition is that strings which are consistently seg-
mented by the different segmenters tend to be cor-
rect predictions. In our experiment on the Penn Chi-
nese Treebank (Xue et al, 2005), the accuracy is
98.59% on the development data which is defined
in the next section. The key point for the interme-
diate sub-word structures is to maximize the agree-
ment of the three coarse-grained systems. In other
words, the goal is to make merged sub-words as
large as possible but not overlap with any predicted
word produced by the three coarse-grained solvers.
In particular, if the position between two continu-
ous characters is predicted as a word boundary by
any segmenter, this position is taken as a separation
position of the sub-word sequence. This strategy
makes sure that it is still possible to re-segment the
strings of which the boundaries are disagreed with
by the coarse-grained segmenters in the fine-grained
tagging stage.
The formal definition is as follows. Given a se-
quence of characters c = (c1, ..., c#c), let c[i : j]
denote a string that is made up of characters between
ci and cj (including ci and cj), then a partition of
the sentence can be written as c[0 : e1], c[e1 + 1 :
e2], ..., c[em : #c]. Let sk = {c[i : j]} denote the
set of all segments of a partition. Given multiple
partitions of a character sequence S = {sk}, there
is one and only one merged partition sS = {c[i : j]}
s.t.
1. ?c[i : j] ? sS ,?sk ? S, ?c[s : e] ? sk, s ?
i ? j ? e.
2. ?C? satisfies the above condition, |C?| > |C|.
The first condition makes sure that all segments in
the merged partition can be only embedded in but do
not overlap with any segment of any partition from
S. The second condition promises that segments of
the merged partition achieve maximum length.
Figure 2 is an example to illustrate the proce-
dure of our method. The lines SegW, SegC and
SegTagL are the predictions of the three coarse-
grained solvers. For the three words at the begin-
ning and the two words at the end, the three predic-
tors agree with each other. And these five words are
kept as sub-words. For the character sequence ??
????????, the predictions are very differ-
ent. Because there are no word break predictions
among the first three characters ?????, it is as
a whole taken as one sub-word. For the other five
characters, either the left position or the right po-
sition is segmented as a word break by some pre-
dictor, so the merging processor seperates them and
takes each one as a single sub-word. The last line
shows the merged sub-word sequence. The coarse-
grained POS tags with positional information are de-
rived from the labels provided by SegTagL.
3.4 The Fine-grained Sub-Word Tagger
Bracketing sub-words into words is formulated as
a IOB-style sequential classification problem. Each
sub-word may be assigned with one POS tag as well
as two possible boundary tags: ?B? for the begin-
ning position and ?I? for the middle position. A
tagger is trained to classify sub-word by using the
features derived from its contexts.
The sub-word level allows our system to utilize
features in a large context, which is very important
for POS tagging of the morphologically poor lan-
guage. Features are formed making use of sub-word
contents, their IOB-style inaccurate POS tags. In
the following description, ?C? refers to the content
of the sub-word, while ?T? refers to the IOB-style
POS tags. For convenience, we denote a sub-word
with its context ...si?2si?1sisi+1si+2..., where si is
1389
C(si?1)=????; T(si?1)=?NN?
C(si)=?????; T(si)=?B-CD?
C(si+1)=???; T(si+1)=?I-CD?
C(si?1)C(si)=??? ????
T(si?1)T(si)=?NN B-CD?
C(si)C(si+1)=???? ??
T(si)T(si+1)=?B-CD I-CD?
C(si?1)C(si+1)=??? ??
T(si?1)T(si+1)=?B-NN I-CD?
Prefix(1)=???; Prefix(2)=????; Prefix(3)=?????
Suffix(1)=???; Suffix(2)=????; Suffix(3)=?????
Table 1: An example of features used in the sub-word
tagging.
the current token. We denote lC , lT as the sizes of
the window.
? Uni-gram features: C(sk) (?lC ? k ? lC),
T(sk) (?lT ? k ? lT )
? Bi-gram features: C(sk)C(sk+1) (?lC ? k ?
lC ? 1), T(sk)T(sk+1) (?lT ? k ? lT ? 1)
? C(si?1)C(si+1) (if lC ? 1), T(si?1)T(si+1) (if
lT ? 1)
? T(si?2)T(si+1) (if lT ? 2)
? In order to better handle unknown words, we
also extract morphological features: character
n-gram prefixes and suffixes for n up to 3.
These features have been shown useful in pre-
vious research (Huang et al, 2007).
Take the sub-word ????? in Figure 2 for ex-
ample, when lC and lT are both set to 1, all features
used are listed in Table 1.
In the following experiments, we will vary win-
dow sizes lC and lT to find out the contribution of
context information for the disambiguation. A first
order Max-Margin Markov Networks model is used
to resolve the sequence tagging problem. We use the
SVM-HMM3 implementation for the experiments in
this work. We use the basic linear model without
applying any kernel function.
3Available at http://www.cs.cornell.edu/
People/tj/svm_light/svm_hmm.html.
Algorithm 1: The stacked learning procedure
for the sub-word tagger.
input : Data S = {(ct,yt), t = 1, 2, ..., n}
Split S into L partitions {S1, ...SL}
for l = 1, ..., L do
Train SegWl, SegCl and SegTagLl using
S ? Sl.
Predict Sl using SegW
l, SegCl and
SegTagLl.
Merge the predictions to get sub-words
training sample S?l .
end
Train the sub-word tagger SubTag using S?.
3.5 Stacked Learning for the Sub-Word Tagger
The three coarse-grained solvers SegW, SegC and
SegTagL are directly trained on the original train-
ing data. When these three predictors are used to
produce the training data, the performance is per-
fect. However, this does not hold when these mod-
els are applied to the test data. If we directly apply
SegW, SegC and SegTagL to extend the training data
to generate sub-word samples, the extended training
data for the sub-word tagger will be very different
from the data in the run time, resulting in poor per-
formance.
One way to correct the training/test mismatch is
to use the stacking method, where a K-fold cross-
validation on the original data is performed to con-
struct the training data for sub-word tagging. Algo-
rithm 1 illustrates the learning procedure. First, the
training data S = {(ct,yt)} is split into L equal-
sized disjoint subsets S1, ..., SL. For each subset Sl,
the complementary set S ? Sl is used to train three
coarse solvers SegWl, SegCl and SegTagLl, which
process the Sl and provide inaccurate predictions.
Then the inaccurate predictions are merged into sub-
word sequences and Sl is extended to S?l . Finally,
the sub-word tagger is trained on the whole extended
data set S?.
4 Experiments
4.1 Setting
Previous studies on joint Chinese word segmenta-
tion and POS tagging have used the Penn Chinese
Treebank (CTB) in experiments. We follow this set-
1390
ting in this paper. We use CTB 5.0 as our main
corpus and define the training, development and test
sets according to (Jiang et al, 2008a; Jiang et al,
2008b; Kruengkrai et al, 2009; Zhang and Clark,
2010). Table 2 shows the statistics of our experi-
mental settings.
Data set CTB files # of sent. # of words
Training 1-270 18,089 493,939
400-931
1001-1151
Devel. 301-325 350 6821
Test 271-300 348 8008
Table 2: Training, development and test data on CTB 5.0
Three metrics are used for evaluation: precision
(P), recall (R) and balanced f-score (F) defined by
2PR/(P+R). Precision is the relative amount of cor-
rect words in the system output. Recall is the rela-
tive amount of correct words compared to the gold
standard annotations. For segmentation, a token is
considered to be correct if its boundaries match the
boundaries of a word in the gold standard. For the
whole task, both the boundaries and the POS tag
have to be correctly identified.
4.2 Performance of the Coarse-grained Solvers
Table 3 shows the performance on the development
data set of the three coarse-grained solvers. In this
paper, we use 20 iterations to train SegW and SegC
for all experiments. Even only locally trained, the
character classifier SegTagL still significantly out-
performs the two state-of-the-art segmenters SegW
and SegC. This good performance indicates that the
POS information is very important for word segmen-
tation.
Devel. Task P(%) R(%) F
SegW Seg 94.55 94.84 94.69
SegC Seg 95.10 94.38 94.73
SegTagL Seg 95.67 95.98 95.83
Seg&Tag 87.54 91.29 89.38
Table 3: Performance of the coarse-grained solvers on the
development data.
4.3 Statistics of Sub-Words
Since the base predictors to generate coarse infor-
mation are two word segmenters and a local charac-
ter classifier, the coarse decoding is efficient. If the
length of sub-words is too short, i.e. the decoding
path for sub-word sequences are too long, the decod-
ing of the fine-grained stage is still hard. Although
we cannot give a theoretical average length of sub-
words, we can still show the empirical one. The av-
erage length of sub-words on the development set is
1.64, while the average length of words is 1.69. The
number of all IOB-style POS tags is 59 (when using
5-fold cross-validation to generate stacked training
samples). The number of all POS tags is 35. Empir-
ically, the decoding over sub-words is 1.691.64?(
59
35)
n+1
times as slow as the decoding over words, where n
is the order of the markov model. When a first order
markov model is used, this number is 2.93. These
statistics empirically suggest that the decoding over
sub-word sequence can be efficient.
On the other hand, the sub-word sequences are
not perfect in the sense that they do not promise
to recover all words because of the errors made in
the first step. Similarly, we can only show the em-
pirical upper bound of the sub-word tagging. The
oracle performance of the final POS tagging on the
development data set is shown in Table 4. The up-
per bound indicates that the coarse search procedure
does not lose too much.
Task P(%) R(%) F
Seg&Tag 99.50% 99.09% 99.29
Table 4: Upper bound of the sub-word tagging on the
development data.
One main disadvantage of character-based ap-
proach is the difficulty to incorporate word features.
Since the sub-words are on average close to words,
sub-word features are good approximations of word
features.
4.4 Rich Contextual Features Are Useful
Table 5 shows the effect that features within differ-
ent window size has on the sub-word tagging task.
In this table, the symbol ?C? means sub-word con-
tent features while the symbol ?T? means IOB-style
POS tag features. The number indicates the length
1391
Devel. P(%) R(%) F
C:?0 T:?0 92.52 92.83 92.67
C:?1 T:?0 92.63 93.27 92.95
C:?1 T:?1 92.62 93.05 92.83
C:?2 T:?0 93.17 93.86 93.51
C:?2 T:?1 93.27 93.64 93.45
C:?2 T:?2 93.08 93.61 93.34
C:?3 T:?0 93.12 93.86 93.49
C:?3 T:?1 93.34 93.96 93.65
C:?3 T:?2 93.34 93.96 93.65
Table 5: Performance of the stacked sub-word model
(K = 5) with features in different window sizes.
of the window. For example, ?C:?1? means that the
tagger uses one preceding sub-word and one suc-
ceeding sub-word as features. From this table, we
can clearly see the impact of features derived from
neighboring sub-words. There is a significant in-
crease between ?C:?2? and ?C:?1? models. This
confirms our motivation that longer history and fu-
ture features are crucial to the Chinese POS tagging
problem. It is the main advantage of our model that
making rich contextual features applicable. In all
previous solutions, only features within a short his-
tory can be used due to the efficiency limitation.
The performance is further slightly improved
when the window size is increased to 3. Using the
labeled bracketing f-score, the evaluation shows that
the ?C:?3 T:?1? model performs the same as the
?C:?3 T:?2? model. However, the sub-word clas-
sification accuracy of the ?C:?3 T:?1? model is
higher, so in the following experiments and the fi-
nal results reported on the test data set, we choose
this setting.
This table also suggests that the IOB-style POS
information of sub-words does not contribute. We
think there are two main reasons: (1) The POS infor-
mation provided by the local classifier is inaccurate;
(2) The structured learning of the sub-word tagger
can use real predicted sub-word labels during its de-
coding time, since this learning algorithm does in-
ference during the training time. It is still an open
question whether more accurate POS information in
rich contexts can help this task. If the answer is YES,
how can we efficiently incorporate these features?
4.5 Stacked Learning Is Useful
Table 6 compares the performance of ?C:?3 T:?1?
models trained with no stacking as well as differ-
ent folds of cross-validation. We can see that al-
though it is still possible to improve the segmenta-
tion and POS tagging performance compared to the
local character classifier, the whole task just benefits
only a little from the sub-word tagging procedure if
the stacking technique is not applied. The stacking
technique can significantly improve the system per-
formance, both for segmentation and POS tagging.
This experiment confirms the theoretical motivation
of using stacked learning: simulating the test-time
setting when a sub-word tagger is applied to a new
instance. There is not much difference between the
5-fold and the 10-fold cross-validation.
Devel. Task P(%) R(%) F
No stacking Seg 95.75 96.48 96.12
Seg&Tag 91.42 92.13 91.77
K = 5 Seg 96.42 97.04 96.73
Seg&Tag 93.34 93.96 93.65
K = 10 Seg 96.67 97.11 96.89
Seg&Tag 93.50 94.06 93.78
Table 6: Performance on the development data. No stack-
ing and different folds of cross-validation are separately
applied.
4.6 Final Results
Table 7 summarizes the performance of our final
system on the test data and other systems reported
in a majority of previous work. The final results
of our system are achieved by using 10-fold cross-
validation ?C:?3 T:?1? models. The left most col-
umn indicates the reference of previous systems that
represent state-of-the-art results. The comparison of
the accuracy between our stacked sub-word system
and the state-of-the-art systems in the literature in-
dicates that our method is competitive with the best
systems. Our system obtains the highest f-score per-
formance on both segmentation and the whole task,
resulting in error reductions of 14.1% and 5.5% re-
spectively.
1392
Test Seg Seg&Tag
(Jiang et al, 2008a) 97.85 93.41
(Jiang et al, 2008b) 97.74 93.37
(Kruengkrai et al, 2009) 97.87 93.67
(Zhang and Clark, 2010) 97.78 93.67
Our system 98.17 94.02
Table 7: F-score performance on the test data.
5 Conclusion and Future Work
This paper has described a stacked sub-word model
for joint Chinese word segmentation and POS tag-
ging. We defined a sub-word structure which maxi-
mizes the agreement of multiple segmentations pro-
vided by different segmenters. We showed that this
sub-word structure could explore the complemen-
tary strength of different systems designed with dif-
ferent views. Moreover, the POS tagging could be
efficiently and effectively resolved over sub-word
sequences. To train a good sub-word tagger, we in-
troduced a stacked learning procedure. Experiments
showed that our approach was superior to the exist-
ing approaches reported in the literature.
Machine learning and statistical approaches en-
counter difficulties when the input/output data have
a structured and relational form. Research in em-
pirical Natural Language Processing has been tack-
ling these complexities since the early work in the
field. Recent work in machine learning has pro-
vided several paradigms to globally represent and
process such data: linear models for structured pre-
diction, graphical models, constrained conditional
models, and reranking, among others. A general
expressivity-efficiency trade off is observed. Al-
though the stacked sub-word model is an ad hoc so-
lution for a particular problem, namely joint word
segmentation and POS tagging, the idea to em-
ploy system ensemble and stacked learning in gen-
eral provides an alternative for structured problems.
Multiple ?cheap? coarse systems are used to provide
diverse outputs, which may be inaccurate. These
outputs are further merged into an intermediate rep-
resentation, which allows an extractive system to use
rich contexts to predict the final results. A natu-
ral avenue for future work is the extension of our
method to other NLP tasks.
Acknowledgments
The work is supported by the project TAKE (Tech-
nologies for Advanced Knowledge Extraction),
funded under contract 01IW08003 by the German
Federal Ministry of Education and Research. The
author is also funded by German Academic Ex-
change Service (DAAD).
The author would would like to thank Dr. Jia
Xu for her helpful discussion, and Regine Bader for
proofreading this paper.
References
Leo Breiman. 1996. Stacked regressions. Mach. Learn.,
24:49?64, July.
William W. Cohen and Vitor R. Carvalho. 2005. Stacked
sequential learning. In Proceedings of the 19th in-
ternational joint conference on Artificial intelligence,
pages 671?676, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JOURNAL OF MACHINE
LEARNING RESEARCH, 7:551?585.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and discrim-
inative reranking. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1093?1102,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT, pages 897?904, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 385?392, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
1393
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL ?01: Second
meeting of the North American Chapter of the Associa-
tion for Computational Linguistics on Language tech-
nologies 2001, pages 1?8, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 277?
284, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd ACL/SIGDAT Workshop on Very Large Cor-
pora, Cambridge, Massachusetts, USA, pages 82?94.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Coling 2010: Posters, pages 1211?1219,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking dependency
parsers. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 157?166, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
David H. Wolpert. 1992. Original contribution: Stacked
generalization. Neural Netw., 5:241?259, February.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. A
stacked, voted, stacked model for named entity recog-
nition. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of the Seventh Conference on Nat-
ural Language Learning at HLT-NAACL 2003, pages
200?203.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics and Chinese Language Process-
ing.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 840?847, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings of ACL-08: HLT, pages 888?896, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 843?852, Cambridge, MA,
October. Association for Computational Linguistics.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for Chinese word segmentation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Companion Volume: Short Papers, pages
193?196, New York City, USA, June. Association for
Computational Linguistics.
1394
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232?241,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Reducing Approximation and Estimation Errors for Chinese Lexical
Processing with Heterogeneous Annotations
Weiwei Sun? and Xiaojun Wan? ?
??Institute of Computer Science and Technology, Peking University
?Saarbru?cken Graduate School of Computer Science
?Department of Computational Linguistics, Saarland University
?Language Technology Lab, DFKI GmbH
{ws,wanxiaojun}@pku.edu.cn
Abstract
We address the issue of consuming heteroge-
neous annotation data for Chinese word seg-
mentation and part-of-speech tagging. We em-
pirically analyze the diversity between two
representative corpora, i.e. Penn Chinese
Treebank (CTB) and PKU?s People?s Daily
(PPD), on manually mapped data, and show
that their linguistic annotations are systemat-
ically different and highly compatible. The
analysis is further exploited to improve pro-
cessing accuracy by (1) integrating systems
that are respectively trained on heterogeneous
annotations to reduce the approximation error,
and (2) re-training models with high quality
automatically converted data to reduce the es-
timation error. Evaluation on the CTB and
PPD data shows that our novel model achieves
a relative error reduction of 11% over the best
reported result in the literature.
1 Introduction
A majority of data-driven NLP systems rely on
large-scale, manually annotated corpora that are im-
portant to train statistical models but very expensive
to build. Nowadays, for many tasks, multiple het-
erogeneous annotated corpora have been built and
publicly available. For example, the Penn Treebank
is popular to train PCFG-based parsers, while the
Redwoods Treebank is well known for HPSG re-
search; the Propbank is favored to build general se-
mantic role labeling systems, while the FrameNet is
attractive for predicate-specific labeling. The anno-
?This work is mainly finished when the first author was
in Saarland University and DFKI. Both authors are the corre-
sponding authors.
tation schemes in different projects are usually dif-
ferent, since the underlying linguistic theories vary
and have different ways to explain the same lan-
guage phenomena. Though statistical NLP systems
usually are not bound to specific annotation stan-
dards, almost all of them assume homogeneous an-
notation in the training corpus. The co-existence of
heterogeneous annotation data therefore presents a
new challenge to the consumers of such resources.
There are two essential characteristics of hetero-
geneous annotations that can be utilized to reduce
two main types of errors in statistical NLP, i.e. the
approximation error that is due to the intrinsic sub-
optimality of a model and the estimation error that is
due to having only finite training data. First, hetero-
geneous annotations are (similar but) different as a
result of different annotation schemata. Systems re-
spectively trained on heterogeneous annotation data
can produce different but relevant linguistic analy-
sis. This suggests that complementary features from
heterogeneous analysis can be derived for disam-
biguation, and therefore the approximation error can
be reduced. Second, heterogeneous annotations are
(different but) similar because their linguistic analy-
sis is highly correlated. This implies that appropriate
conversions between heterogeneous corpora could
be reasonably accurate, and therefore the estimation
error can be reduced by reason of the increase of re-
liable training data.
This paper explores heterogeneous annotations
to reduce both approximation and estimation errors
for Chinese word segmentation and part-of-speech
(POS) tagging, which are fundamental steps for
more advanced Chinese language processing tasks.
We empirically analyze the diversity between two
representative popular heterogeneous corpora, i.e.
232
Penn Chinese Treebank (CTB) and PKU?s People?s
Daily (PPD). To that end, we manually label 200
sentences from CTB with PPD-style annotations.1
Our analysis confirms the aforementioned two prop-
erties of heterogeneous annotations. Inspired by
the sub-word tagging method introduced in (Sun,
2011), we propose a structure-based stacking model
to fully utilize heterogeneous word structures to re-
duce the approximation error. In particular, joint
word segmentation and POS tagging is addressed
as a two step process. First, character-based tag-
gers are respectively trained on heterogeneous an-
notations to produce multiple analysis. The outputs
of these taggers are then merged into sub-word se-
quences, which are further re-segmented and tagged
by a sub-word tagger. The sub-word tagger is de-
signed to refine the tagging result with the help of
heterogeneous annotations. To reduce the estima-
tion error, we employ a learning-based approach to
convert complementary heterogeneous data to in-
crease labeled training data for the target task. Both
the character-based tagger and the sub-word tagger
can be refined by re-training with automatically con-
verted data.
We conduct experiments on the CTB and PPD
data, and compare our system with state-of-the-
art systems. Our structure-based stacking model
achieves an f-score of 94.36, which is superior to
a feature-based stacking model introduced in (Jiang
et al, 2009). The converted data can also enhance
the baseline model. A simple character-based model
can be improved from 93.41 to 94.11. Since the
two treatments are concerned with reducing differ-
ent types of errors and thus not fully overlapping, the
combination of them gives a further improvement.
Our final system achieves an f-score of 94.68, which
yields a relative error reduction of 11% over the best
published result (94.02).
2 Joint Chinese Word Segmentation and
POS Tagging
Different from English and other Western languages,
Chinese is written without explicit word delimiters
such as space characters. To find and classify the
1The first 200 sentences of the development data for experi-
ments are selected. This data set is submitted as a supplemental
material for research purposes.
basic language units, i.e. words, word segmentation
and POS tagging are important initial steps for Chi-
nese language processing. Supervised learning with
specifically defined training data has become a dom-
inant paradigm. Joint approaches that resolve the
two tasks simultaneously have received much atten-
tion in recent research. Previous work has shown
that joint solutions led to accuracy improvements
over pipelined systems by avoiding segmentation er-
ror propagation and exploiting POS information to
help segmentation (Ng and Low, 2004; Jiang et al,
2008a; Zhang and Clark, 2008; Sun, 2011).
Two kinds of approaches are popular for joint
word segmentation and POS tagging. The first is the
?character-based? approach, where basic processing
units are characters which compose words (Jiang et
al., 2008a). In this kind of approach, the task is for-
mulated as the classification of characters into POS
tags with boundary information. For example, the
label B-NN indicates that a character is located at the
begging of a noun. Using this method, POS infor-
mation is allowed to interact with segmentation. The
second kind of solution is the ?word-based? method,
also known as semi-Markov tagging (Zhang and
Clark, 2008; Zhang and Clark, 2010), where the ba-
sic predicting units are words themselves. This kind
of solver sequentially decides whether the local se-
quence of characters makes up a word as well as its
possible POS tag. Solvers may use previously pre-
dicted words and their POS information as clues to
process a new word.
In addition, we proposed an effective and efficient
stacked sub-word tagging model, which combines
strengths of both character-based and word-based
approaches (Sun, 2011). First, different character-
based and word-based models are trained to produce
multiple segmentation and tagging results. Sec-
ond, the outputs of these coarse-grained models are
merged into sub-word sequences, which are fur-
ther bracketed and labeled with POS tags by a fine-
grained sub-word tagger. Their solution can be
viewed as utilizing stacked learning to integrate het-
erogeneous models.
Supervised segmentation and tagging can be im-
proved by exploiting rich linguistic resources. Jiang
et al (2009) presented a preliminary study for an-
notation ensemble, which motivates our research as
well as similar investigations for other NLP tasks,
233
e.g. parsing (Niu et al, 2009; Sun et al, 2010). In
their solution, heterogeneous data is used to train an
auxiliary segmentation and tagging system to pro-
duce informative features for target prediction. Our
previous work (Sun and Xu, 2011) and Wang et al
(2011) explored unlabeled data to enhance strong
supervised segmenters and taggers. Both of their
work fall into the category of feature induction based
semi-supervised learning. In brief, their methods
harvest useful string knowledge from unlabeled or
automatically analyzed data, and apply the knowl-
edge to design new features for discriminative learn-
ing.
3 About Heterogeneous Annotations
For Chinese word segmentation and POS tag-
ging, supervised learning has become a dominant
paradigm. Much of the progress is due to the devel-
opment of both corpora and machine learning tech-
niques. Although several institutions to date have
released their segmented and POS tagged data, ac-
quiring sufficient quantities of high quality training
examples is still a major bottleneck. The annotation
schemes of existing lexical resources are different,
since the underlying linguistic theories vary. Despite
the existence of multiple resources, such data cannot
be simply put together for training systems, because
almost all of statistical NLP systems assume homo-
geneous annotation. Therefore, it is not only inter-
esting but also important to study how to fully utilize
heterogeneous resources to improve Chinese lexical
processing.
There are two main types of errors in statistical
NLP: (1) the approximation error that is due to the
intrinsic suboptimality of a model and (2) the esti-
mation error that is due to having only finite train-
ing data. Take Chinese word segmentation for ex-
ample. Our previous analysis (Sun, 2010) shows
that one main intrinsic disadvantage of character-
based model is the difficulty in incorporating the
whole word information, while one main disadvan-
tage of word-based model is the weak ability to ex-
press word formation. In both models, the signifi-
cant decrease of the prediction accuracy of out-of-
vocabulary (OOV) words indicates the impact of the
estimation error. The two essential characteristics
about systematic diversity of heterogeneous annota-
tions can be utilized to reduce both approximation
and estimation errors.
3.1 Analysis of the CTB and PPD Standards
This paper focuses on two representative popular
corpora for Chinese lexical processing: (1) the Penn
Chinese Treebank (CTB) and (2) the PKU?s Peo-
ple?s Daily data (PPD). To analyze the diversity be-
tween their annotation standards, we pick up 200
sentences from CTB and manually label them ac-
cording to the PPD standard. Specially, we employ a
PPD-style segmentation and tagging system to auto-
matically label these 200 sentences. A linguistic ex-
pert who deeply understands the PPD standard then
manually checks the automatic analysis and corrects
its errors.
These 200 sentences are segmented as 3886 and
3882 words respectively according to the CTB and
PPD standards. The average lengths of word tokens
are almost the same. However, the word bound-
aries or the definitions of words are different. 3561
word tokens are consistently segmented by both
standards. In other words, 91.7% CTB word tokens
share the same word boundaries with 91.6% PPD
word tokens. Among these 3561 words, there are
552 punctuations that are simply consistently seg-
mented. If punctuations are filtered out to avoid
overestimation of consistency, 90.4% CTB words
have same boundaries with 90.3% PPD words. The
boundaries of words that are differently segmented
are compatible. Among all annotations, only one
cross-bracketing occurs. The statistics indicates that
the two heterogenous segmented corpora are sys-
tematically different, and confirms the aforemen-
tioned two properties of heterogeneous annotations.
Table 1 is the mapping between CTB-style tags
and PPD-style tags. For the definition and illus-
tration of these tags, please refers to the annotation
guidelines2. The statistics after colons are how many
times this POS tag pair appears among the 3561
words that are consistently segmented. From this ta-
ble, we can see that (1) there is no one-to-one map-
ping between their heterogeneous word classifica-
tion but (2) the mapping between heterogeneous tags
is not very uncertain. This simple analysis indicates
2Available at http://www.cis.upenn.edu/
?chinese/posguide.3rd.ch.pdf and http://www.
icl.pku.edu.cn/icl_groups/corpus/spec.htm.
234
that the two POS tagged corpora also hold the two
properties of heterogeneous annotations. The dif-
ferences between the POS annotation standards are
systematic. The annotations in CTB are treebank-
driven, and thus consider more functional (dynamic)
information of basic lexical categories. The annota-
tions in PPD are lexicon-driven, and thus focus on
more static properties of words. Limited to the doc-
ument length, we only illustrate the annotation of
verbs and nouns for better understanding of the dif-
ferences.
? The CTB tag VV indicates common verbs that
are mainly labeled as verbs (v) too according
to the PPD standard. However, these words can
be also tagged as nominal categories (a, vn, n).
The main reason is that there are a large num-
ber of Chinese adjectives and nouns that can be
realized as predicates without linking verbs.
? The tag NN indicates common nouns in CTB.
Some of them are labeled as verbal categories
(vn, v). The main reason is that a majority of
Chinese verbs could be realized as subjects and
objects without form changes.
4 Structure-based Stacking
4.1 Reducing the Approximation Error via
Stacking
Each annotation data set alne can yield a predictor
that can be taken as a mechanism to produce struc-
tured texts. With different training data, we can con-
struct multiple heterogeneous systems. These sys-
tems produce similar linguistic analysis which holds
the same high level linguistic principles but differ in
details. A very simple idea to take advantage of het-
erogeneous structures is to design a predictor which
can predict a more accurate target structure based
on the input, the less accurate target structure and
complementary structures. This idea is very close
to stacked learning (Wolpert, 1992), which is well
developed for ensemble learning, and successfully
applied to some NLP tasks, e.g. dependency parsing
(Nivre and McDonald, 2008; Torres Martins et al,
2008).
Formally speaking, our idea is to include two
?levels? of processing. The first level includes one
AS? u:44; CD? m:134;
DEC? u:83; DEV? u:7;
DEG? u:123; ETC? u:9;
LB? p:1; NT? t:98;
OD? m:41; PU? w:552;
SP? u:1; VC? v:32;
VE? v:13; BA? p:2; d:1;
CS? c:3; d:1; DT? r:15; b:1;
MSP? c:2; u:1; PN? r:53; n:2;
CC? c:73; p:5; v:2; M? q:101; n:11; v:1;
LC? f:51; Ng:3; v:1; u:1; P? p:133; v:4; c:2; Vg:1;
VA ? a:57; i:4; z:2; ad:1;
b:1;
NR ? ns:170; nr:65; j:23;
nt:21; nz:7; n:2; s:1;
VV ? v:382; i:5; a:3; Vg:2;
vn:2; n:2; p:2; w:1;
JJ ? a:43; b:13; n:3; vn:3;
d:2; j:2; f:2; t:2; z:1;
AD? d:149; c:11; ad:6; z:4;
a:3; v:2; n:1; r:1; m:1; f:1;
t:1;
NN ? n:738; vn:135; v:26;
j:19; Ng:5; an:5; a:3; r:3; s:3;
Ag:2; nt:2; f:2; q:2; i:1; t:1;
nz:1; b:1;
Table 1: Mapping between CTB and PPD POS Tags.
or more base predictors f1, ..., fK that are indepen-
dently built on different training data. The second
level processing consists of an inference function h
that takes as input ?x, f1(x), ..., fK(x)?3 and out-
puts a final prediction h(x, f1(x), ..., fK(x)). The
only difference between model ensemble and anno-
tation ensemble is that the output spaces of model
ensemble are the same while the output spaces of an-
notation ensemble are different. This framework is
general and flexible, in the sense that it assumes al-
most nothing about the individual systems and take
them as black boxes.
4.2 A Character-based Tagger
With IOB2 representation (Ramshaw and Marcus,
1995), the problem of joint segmentation and tag-
ging can be regarded as a character classification
task. Previous work shows that the character-based
approach is an effective method for Chinese lexical
processing. Both of our feature- and structure-based
stacking models employ base character-based tag-
gers to generate multiple segmentation and tagging
results. Our base tagger use a discriminative sequen-
tial classifier to predict the POS tag with positional
information for each character. Each character can
be assigned one of two possible boundary tags: ?B?
for a character that begins a word and ?I? for a char-
acter that occurs in the middle of a word. We denote
3x is a given Chinese sentence.
235
a candidate character token ci with a fixed window
ci?2ci?1cici+1ci+2. The following features are used
for classification:
? Character unigrams: ck (i? l ? k ? i+ l)
? Character bigrams: ckck+1 (i? l ? k < i+ l)
4.3 Feature-based Stacking
Jiang et al (2009) introduced a feature-based stack-
ing solution for annotation ensemble. In their so-
lution, an auxiliary tagger CTagppd is trained on a
complementary corpus, i.e. PPD, to assist the tar-
get CTB-style tagging. To refine the character-based
tagger CTagctb, PPD-style character labels are di-
rectly incorporated as new features. The stacking
model relies on the ability of discriminative learning
method to explore informative features, which play
central role to boost the tagging performance. To
compare their feature-based stacking model and our
structure-based model, we implement a similar sys-
tem CTagppd?ctb. Apart from character uni/bigram
features, the PPD-style character labels are used to
derive the following features to enhance our CTB-
style tagger:
? Character label unigrams: cppdk (i?l
ppd ? k ?
i+ lppd)
? Character label bigrams: cppdk c
ppd
k+1 (i? l
ppd ?
k < i+ lppd)
In the above descriptions, l and lppd are the win-
dow sizes of features, which can be tuned on devel-
opment data.
4.4 Structure-based Stacking
We propose a novel structured-based stacking model
for the task, in which heterogeneous word struc-
tures are used not only to generate features but also
to derive a sub-word structure. Our work is in-
spired by the stacked sub-word tagging model in-
troduced in (Sun, 2011). Their work is motivated
by the diversity of heterogeneous models, while
our work is motivated by the diversity of heteroge-
neous annotations. The workflow of our new sys-
tem is shown in Figure 1. In the first phase, one
character-based CTB-style tagger (CTagctb) and
one character-based PPD-style tagger (CTagppd)
are respectively trained to produce heterogenous
Raw sentences
CTB-style character
tagger CTagctb
PPD-style character
tagger CTagppd
Segmented and
tagged sentences
Segmented and
tagged sentences
Merging
Sub-word
sequences
CTB-style
sub-word tag-
ger STagctb
Figure 1: Sub-word tagging based on heterogeneous tag-
gers.
word boundaries. In the second phase, this system
first combines the two segmentation and tagging re-
sults to get sub-words which maximize the agree-
ment about word boundaries. Finally, a fine-grained
sub-word tagger (STagctb) is applied to bracket sub-
words into words and also to label their POS tags.
We can also apply a PPD-style sub-word tagger. To
compare with previous work, we specially concen-
trate on the PPD-to-CTB adaptation.
Following (Sun, 2011), the intermediate sub-word
structures is defined to maximize the agreement of
CTagctb and CTagppd. In other words, the goal is
to make merged sub-words as large as possible but
not overlap with any predicted word produced by
the two taggers. If the position between two con-
tinuous characters is predicted as a word boundary
by any segmenter, this position is taken as a separa-
tion position of the sub-word sequence. This strat-
egy makes sure that it is still possible to correctly
re-segment the strings of which the boundaries are
disagreed with by the heterogeneous segmenters in
the sub-word tagging stage.
To train the sub-word tagger STagctb, features
are formed making use of both CTB-style and PPD-
style POS tags provided by the character-based tag-
gers. In the following description, ?C? refers to the
content of a sub-word; ?Tctb? and ?Tppd? refers to
the positional POS tags generated from CTagctb and
CTagppd; lC , lctbT and l
ppd
T are the window sizes.
For convenience, we denote a sub-word with its con-
236
text ...si?1sisi+1..., where si is the current token.
The following features are applied:
? Unigram features: C(sk) (i ? lC ? k ? +lC),
Tctb(sk) (i ? lctbT ? k ? i + l
ctb
T ), Tppd(sk)
(i? lppdT ? k ? i+ l
ppd
T )
? Bigram features: C(sk)C(sk+1) (i ? lC ? k <
i + lC), Tctb(sk)Tctb(sk+1) (i ? lctbT ? k <
i+ lctbT ), Tppd(sk)Tppd(sk+1) (i? l
ppd
T ? k <
i+ lppdT )
? C(si?1)C(si+1) (if lC ? 1),
Tctb(si?1)Tctb(si+1) (if lctbT ? 1),
Tppd(si?1)Tppd(si+1) (if l
ppd
T ? 1)
? Word formation features: character n-gram
prefixes and suffixes for n up to 3.
Cross-validation CTagctb and CTagppd are di-
rectly trained on the original training data, i.e. the
CTB and PPD data. Cross-validation technique has
been proved necessary to generate the training data
for sub-word tagging, since it deals with the train-
ing/test mismatch problem (Sun, 2011). To con-
struct training data for the new heterogeneous sub-
word tagger, a 10-fold cross-validation on the origi-
nal CTB data is performed too.
5 Data-driven Annotation Conversion
It is possible to acquire high quality labeled data
for a specific annotation standard by exploring ex-
isting heterogeneous corpora, since the annotations
are normally highly compatible. Moreover, the ex-
ploitation of additional (pseudo) labeled data aims to
reduce the estimation error and enhances a NLP sys-
tem in a different way from stacking. We therefore
expect the improvements are not much overlapping
and the combination of them can give a further im-
provement.
The stacking models can be viewed as annota-
tion converters: They take as input complementary
structures and produce as output target structures.
In other words, the stacking models actually learn
statistical models to transform the lexical represen-
tations. We can acquire informative extra samples
by processing the PPD data with our stacking mod-
els. Though the converted annotations are imperfect,
they are still helpful to reduce the estimation error.
Character-based Conversion The feature-based
stacking model CTagppd?ctb maps the input char-
acter sequence c and its PPD-style character label
sequence to the corresponding CTB-style character
label sequence. This model by itself can be taken as
a corpus conversion model to transform a PPD-style
analysis to a CTB-style analysis. By processing the
auxiliary corpus Dppd with CTagppd?ctb, we ac-
quire a new labeled data set D?ctb = D
CTagppd?ctb
ppd?ctb .
We can re-train the CTagctb model with both origi-
nal and converted data Dctb ?D?ctb.
Sub-word-based Conversion Similarly, the
structure-based stacking model can be also taken
as a corpus conversion model. By processing the
auxiliary corpus Dppd with STagctb, we acquire
a new labeled data set D??ctb = D
STagctb
ppd?ctb. We can
re-train the STagctb model with Dctb ? D??ctb. If
we use the gold PPD-style labels of D??ctb to extract
sub-words, the new model will overfit to the gold
PPD-style labels, which are unavailable at test time.
To avoid this training/test mismatch problem, we
also employ a 10-fold cross validation procedure to
add noise.
It is not a new topic to convert corpus from one
formalism to another. A well known work is trans-
forming Penn Treebank into resources for various
deep linguistic processing, including LTAG (Xia,
1999), CCG (Hockenmaier and Steedman, 2007),
HPSG (Miyao et al, 2004) and LFG (Cahill et al,
2002). Such work for corpus conversion mainly
leverages rich sets of hand-crafted rules to convert
corpora. The construction of linguistic rules is usu-
ally time-consuming and the rules are not full cover-
age. Compared to rule-based conversion, our statis-
tical converters are much easier to built and empiri-
cally perform well.
6 Experiments
6.1 Setting
Previous studies on joint Chinese word segmenta-
tion and POS tagging have used the CTB in experi-
ments. We follow this setting in this paper. We use
CTB 5.0 as our main corpus and define the train-
ing, development and test sets according to (Jiang
et al, 2008a; Jiang et al, 2008b; Kruengkrai et al,
2009; Zhang and Clark, 2010; Sun, 2011). Jiang et
237
al. (2009) present a preliminary study for the annota-
tion adaptation topic, and conduct experiments with
the extra PPD data4. In other words, the CTB-sytle
annotation is the target analysis while the PPD-style
annotation is the complementary/auxiliary analysis.
Our experiments for annotation ensemble follows
their setting to lead to a fair comparison of our sys-
tem and theirs. A CRF learning toolkit, wapiti5
(Lavergne et al, 2010), is used to resolve sequence
labeling problems. Among several parameter esti-
mation methods provided by wapiti, our auxiliary
experiments indicate that the ?rprop-? method works
best. Three metrics are used for evaluation: preci-
sion (P), recall (R) and balanced f-score (F) defined
by 2PR/(P+R). Precision is the relative amount of
correct words in the system output. Recall is the rel-
ative amount of correct words compared to the gold
standard annotations. A token is considered to be
correct if its boundaries match the boundaries of a
word in the gold standard and their POS tags are
identical.
6.2 Results of Stacking
Table 2 summarizes the segmentation and tagging
performance of the baseline and different stacking
models. The baseline of the character-based joint
solver (CTagctb) is competitive, and achieves an
f-score of 92.93. By using the character labels
from a heterogeneous solver (CTagppd), which is
trained on the PPD data set, the performance of this
character-based system (CTagppd?ctb) is improved
to 93.67. This result confirms the importance of a
heterogeneous structure. Our structure-based stack-
ing solution is effective and outperforms the feature-
based stacking. By better exploiting the heteroge-
neous word boundary structures, our sub-word tag-
ging model achieves an f-score of 94.03 (lctbT and
lppdT are tuned on the development data and both set
to 1).
The contribution of the auxiliary tagger is two-
fold. On one hand, the heterogeneous solver pro-
vides structural information, which is the basis to
construct the sub-word sequence. On the other
hand, this tagger provides additional POS informa-
tion, which is helpful for disambiguation. To eval-
4http://icl.pku.edu.cn/icl_res/
5http://wapiti.limsi.fr/
Devel. P R F
CTagctb 93.28% 92.58% 92.93
CTagppd?ctb 93.89% 93.46% 93.67
STagctb 94.07% 93.99% 94.03
Table 2: Performance of different stacking models on the
development data.
uate these two contributions, we do another experi-
ment by just using the heterogeneous word boundary
structures without the POS information. The f-score
of this type of sub-word tagging is 93.73. This re-
sult indicates that both the word boundary and POS
information are helpful.
6.3 Learning Curves
We do additional experiments to evaluate the effect
of heterogeneous features as the amount of PPD data
is varied. Table 3 summarizes the f-score change.
The feature-based model works well only when a
considerable amount of heterogeneous data is avail-
able. When a small set is added, the performance is
even lower than the baseline (92.93). The structure-
based stacking model is more robust and obtains
consistent gains regardless of the size of the com-
plementary data.
PPD? CTB
#CTB #PPD CTag STag
18104 7381 92.21 93.26
18104 14545 93.22 93.82
18104 21745 93.58 93.96
18104 28767 93.55 93.87
18104 35996 93.67 94.03
9052 9052 92.10 92.40
Table 3: F-scores relative to sizes of training data. Sizes
(shown in column #CTB and #PPD) are numbers of sen-
tences in each training corpus.
6.4 Results of Annotation Conversion
The stacking models can be viewed as data-driven
annotation converting models. However they are not
trained on ?real? labeled samples. Although the tar-
get representation (CTB-style analysis in our case)
is gold standard, the input representation (PPD-style
analysis in our case) is labeled by a automatic tag-
ger CTagppd. To make clear whether these stacking
238
models trained with noisy inputs can tolerate per-
fect inputs, we evaluate the two stacking models on
our manually converted data. The accuracies pre-
sented in Table 4 indicate that though the conver-
sion models are learned by applying noisy data, they
can refine target tagging with gold auxiliary tagging.
Another interesting thing is that the gold PPD-style
analysis does not help the sub-word tagging model
as much as the character tagging model.
Auto PPD Gold PPD
CTagppd?ctb 93.69 95.19
STagctb 94.14 94.70
Table 4: F-scores with gold PPD-style tagging on the
manually converted data.
6.5 Results of Re-training
Table 5 shows accuracies of re-trained models. Note
that a sub-word tagger is built on character taggers,
so when we re-train a sub-word system, we should
consider whether or not re-training base character
taggers. The error rates decrease as automatically
converted data is added to the training pool, espe-
cially for the character-based tagger CTagctb. When
the base CTB-style tagging is improved, the final
tagging is improved in the end. The re-training does
not help the sub-word tagging much; the improve-
ment is very modest.
CTagctb STagctb P(%) R(%) F
Dctb ?D?ctb - - 94.46 94.06 94.26
Dctb ?D?ctb Dctb 94.61 94.43 94.52
Dctb Dctb ?D??ctb 94.05 94.08 94.06
Dctb ?D?ctb Dctb ?D
??
ctb 94.71 94.53 94.62
Table 5: Performance of re-trained models on the devel-
opment data.
6.6 Comparison to the State-of-the-Art
Table 6 summarizes the tagging performance of
different systems. The baseline of the character-
based tagger is competitive, and achieve an f-score
of 93.41. By better using the heterogeneous word
boundary structures, our sub-word tagging model
achieves an f-score of 94.36. Both character and
sub-word tagging model can be enhanced with auto-
matically converted corpus. With the pseudo labeled
data, the performance goes up to 94.11 and 94.68.
These results are also better than the best published
result on the same data set that is reported in (Jiang
et al, 2009).
Test P R F
(Sun, 2011) - - - - 94.02
(Jiang et al, 2009) - - - - 94.02
(Wang et al, 2011) - - - - 94.186
Character model 93.31% 93.51% 93.41
+Re-training 93.93% 94.29% 94.11
Sub-word model 94.10% 94.62% 94.36
+Re-training 94.42% 94.93% 94.68
Table 6: Performance of different systems on the test
data.
7 Conclusion
Our theoretical and empirical analysis of two rep-
resentative popular corpora highlights two essential
characteristics of heterogeneous annotations which
are explored to reduce approximation and estima-
tion errors for Chinese word segmentation and POS
tagging. We employ stacking models to incorporate
features derived from heterogeneous analysis and
apply them to convert heterogeneous labeled data for
re-training. The appropriate application of hetero-
geneous annotations leads to a significant improve-
ment (a relative error reduction of 11%) over the best
performance for this task. Although our discussion
is for a specific task, the key idea to leverage het-
erogeneous annotations to reduce the approximation
error with stacking models and the estimation error
with automatically converted corpora is very general
and applicable to other NLP tasks.
Acknowledgement
This work is mainly finished when the first author
was in Saarland University and DFKI. At that time,
this author was funded by DFKI and German Aca-
demic Exchange Service (DAAD). While working
in Peking University, both author are supported by
NSFC (61170166) and National High-Tech R&D
Program (2012AA011101).
6This result is achieved with much unlabeled data, which is
different from our setting.
239
References
Aoife Cahill, Mairead Mccarthy, Josef Van Genabith, and
Andy Way. 2002. Automatic annotation of the penn
treebank with lfg f-structure information. In Proceed-
ings of the LREC Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping Anno-
tated Language Data, Las Palmas, Canary Islands,
pages 8?15.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
A corpus of ccg derivations and dependency structures
extracted from the penn treebank. Computational Lin-
guistics, 33(3):355?396.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT, pages 897?904, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 385?392, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522?530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. pages 504?
513, July.
Yusuke Miyao, Takashi Ninomiya, and Jun ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a head-driven phrase structure grammar from
the penn treebank. In IJCNLP, pages 684?693.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 277?
284, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46?54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarowsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word
segmentation using unlabeled data. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 970?979, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Weiwei Sun, Rui Wang, and Yi Zhang. 2010. Dis-
criminative parse reranking for Chinese with homoge-
neous and heterogeneous annotations. In Proceedings
of Joint Conference on Chinese Language Processing
(CIPS-SIGHAN), Beijing, China, August.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1211?1219, Beijing, China, August. Coling
2010 Organizing Committee.
Weiwei Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech tag-
ging. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1385?1394, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking dependency
parsers. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 157?166, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving chinese word segmentation and
pos tagging with semi-supervised methods using large
240
auto-analyzed data. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 309?317, Chiang Mai, Thailand, Novem-
ber. Asian Federation of Natural Language Processing.
David H. Wolpert. 1992. Original contribution: Stacked
generalization. Neural Netw., 5:241?259, February.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of Natural Lan-
guage Processing Pacific Rim Symposium, pages 398?
403.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings of ACL-08: HLT, pages 888?896, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 843?852, Cambridge, MA,
October. Association for Computational Linguistics.
241
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 242?252,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Capturing Paradigmatic and Syntagmatic Lexical Relations:
Towards Accurate Chinese Part-of-Speech Tagging
Weiwei Sun?? and Hans Uszkoreit?
?Institute of Computer Science and Technology, Peking University
?Saarbru?cken Graduate School of Computer Science
??Department of Computational Linguistics, Saarland University
??Language Technology Lab, DFKI GmbH
ws@pku.edu.cn, uszkoreit@dfki.de
Abstract
From the perspective of structural linguistics,
we explore paradigmatic and syntagmatic lex-
ical relations for Chinese POS tagging, an im-
portant and challenging task for Chinese lan-
guage processing. Paradigmatic lexical rela-
tions are explicitly captured by word cluster-
ing on large-scale unlabeled data and are used
to design new features to enhance a discrim-
inative tagger. Syntagmatic lexical relations
are implicitly captured by constituent pars-
ing and are utilized via system combination.
Experiments on the Penn Chinese Treebank
demonstrate the importance of both paradig-
matic and syntagmatic relations. Our linguis-
tically motivated approaches yield a relative
error reduction of 18% in total over a state-
of-the-art baseline.
1 Introduction
In grammar, a part-of-speech (POS) is a linguis-
tic category of words, which is generally defined
by the syntactic or morphological behavior of the
word in question. Automatically assigning POS tags
to words plays an important role in parsing, word
sense disambiguation, as well as many other NLP
applications. Many successful tagging algorithms
developed for English have been applied to many
other languages as well. In some cases, the meth-
ods work well without large modifications, such
as for German. But a number of augmentations
and changes become necessary when dealing with
highly inflected or agglutinative languages, as well
as analytic languages, of which Chinese is the focus
?This work is mainly finished when this author (correspond-
ing author) was in Saarland University and DFKI.
of this paper. The Chinese language is characterized
by the lack of formal devices such as morphological
tense and number that often provide important clues
for syntactic processing tasks. While state-of-the-
art tagging systems have achieved accuracies above
97% on English, Chinese POS tagging has proven to
be more challenging and obtained accuracies about
93-94% (Tseng et al, 2005b; Huang et al, 2007,
2009; Li et al, 2011).
It is generally accepted that Chinese POS tag-
ging often requires more sophisticated language pro-
cessing techniques that are capable of drawing in-
ferences from more subtle linguistic knowledge.
From a linguistic point of view, meaning arises from
the differences between linguistic units, including
words, phrases and so on, and these differences are
of two kinds: paradigmatic (concerning substitu-
tion) and syntagmatic (concerning positioning). The
distinction is a key one in structuralist semiotic anal-
ysis. Both paradigmatic and syntagmatic lexical re-
lations have a great impact on POS tagging, because
the value of a word is determined by the two rela-
tions. Our error analysis of a state-of-the-art Chinese
POS tagger shows that the lack of both paradigmatic
and syntagmatic lexical knowledge accounts for a
large part of tagging errors.
This paper is concerned with capturing paradig-
matic and syntagmatic lexical relations to advance
the state-of-the-art of Chinese POS tagging. First,
we employ unsupervised word clustering to explore
paradigmatic relations that are encoded in large-
scale unlabeled data. The word clusters are then ex-
plicitly utilized to design new features for POS tag-
ging. Second, we study the possible impact of syn-
tagmatic relations on POS tagging by comparatively
analyzing a (syntax-free) sequential tagging model
242
and a (syntax-based) chart parsing model. Inspired
by the analysis, we employ a full parser to implicitly
capture syntagmatic relations and propose a Boot-
strap Aggregating (Bagging) model to combine the
complementary strengths of a sequential tagger and
a parser.
We conduct experiments on the Penn Chinese
Treebank and Chinese Gigaword. We implement
a discriminative sequential classification model for
POS tagging which achieves the state-of-the-art ac-
curacy. Experiments show that this model are sig-
nificantly improved by word cluster features in ac-
curacy across a wide range of conditions. This con-
firms the importance of the paradigmatic relations.
We then present a comparative study of our tagger
and the Berkeley parser, and show that the combi-
nation of the two models can significantly improve
tagging accuracy. This demonstrates the importance
of the syntagmatic relations. Cluster-based features
and the Bagging model result in a relative error re-
duction of 18% in terms of the word classification
accuracy.
2 State-of-the-Art
2.1 Previous Work
Many algorithms have been applied to computation-
ally assigning POS labels to English words, includ-
ing hand-written rules, generative HMM tagging
and discriminative sequence labeling. Such meth-
ods have been applied to many other languages as
well. In some cases, the methods work well without
large modifications, such as German POS tagging.
But a number of augmentations and changes became
necessary when dealing with Chinese that has little,
if any, inflectional morphology. While state-of-the-
art tagging systems have achieved accuracies above
97% on English, Chinese POS tagging has proven
to be more challenging and obtains accuracies about
93-94% (Tseng et al, 2005b; Huang et al, 2007,
2009; Li et al, 2011).
Both discriminative and generative models have
been explored for Chinese POS tagging (Tseng
et al, 2005b; Huang et al, 2007, 2009). Tseng
et al (2005a) introduced a maximum entropy based
model, which includes morphological features for
unknown word recognition. Huang et al (2007) and
Huang et al (2009) mainly focused on the gener-
ative HMM models. To enhance a HMM model,
Huang et al (2007) proposed a re-ranking proce-
dure to include extra morphological and syntactic
features, while Huang et al (2009) proposed a la-
tent variable inducing model. Their evaluations on
the Chinese Treebank show that Chinese POS tag-
ging obtains an accuracy of about 93-94%.
2.2 Our Discriminative Sequential Model
According to the ACL Wiki, all state-of-the-art En-
glish POS taggers are based on discriminative se-
quence labeling models, including structure percep-
tron (Collins, 2002; Shen et al, 2007), maximum
entropy (Toutanova et al, 2003) and SVM (Gimnez
and Mrquez, 2004). A discriminative learner is easy
to be extended with arbitrary features and therefore
suitable to recognize more new words. Moreover, a
majority of the POS tags are locally dependent on
each other, so the Markov assumption can well cap-
tures the syntactic relations among words. Discrim-
inative learning is also an appropriate solution for
Chinese POS tagging, due to its flexibility to include
knowledge from multiple linguistic sources.
To deeply analyze the POS tagging problem for
Chinese, we implement a discriminative sequential
model. A first order linear-chain CRF model
is used to resolve the sequential classification
problem. We choose the CRF learning toolkit
wapiti1 (Lavergne et al, 2010) to train models.
In our experiments, we employ a feature set
which draws upon information sources such as
word forms and characters that constitute words.
To conveniently illustrate, we denote a word in
focus with a fixed window w?2w?1ww+1w+2,
where w is the current token. Our features includes:
Word unigrams: w?2, w?1, w, w+1, w+2;
Word bigrams: w?2 w?1, w?1 w, w w+1, w+1 w+2;
In order to better handle unknown words, we extract
morphological features: character n-gram prefixes and
suffixes for n up to 3.
2.3 Evaluation
2.3.1 Setting
Penn Chinese Treebank (CTB) (Xue et al, 2005)
is a popular data set to evaluate a number of Chinese
NLP tasks, including word segmentation (Sun and
1http://wapiti.limsi.fr/
243
Xu, 2011), POS tagging (Huang et al, 2007, 2009),
constituency parsing (Zhang and Clark, 2009; Wang
et al, 2006) and dependency parsing (Zhang and
Clark, 2008; Huang and Sagae, 2010; Li et al,
2011). In this paper, we use CTB 6.0 as the labeled
data for the study. The corpus was collected during
different time periods from different sources with a
diversity of topics. In order to obtain a representa-
tive split of data sets, we define the training, devel-
opment and test sets following two settings. To com-
pare our tagger with the state-of-the-art, we conduct
an experiment using the data setting of (Huang et al,
2009). For detailed analysis and evaluation, we con-
duct further experiments following the setting of the
CoNLL 2009 shared task. The setting is provided by
the principal organizer of the CTB project, and con-
siders many annotation details. This setting is more
robust for evaluating Chinese language processing
algorithms.
2.3.2 Overall Performance
Table 1 summarizes the per token classification
accuracy (Acc.) of our tagger and results reported in
(Huang et al, 2009). Huang et al (2009) introduced
a bigram HMM model with latent variables (Bigram
HMM-LA in the table) for Chinese tagging. Com-
pared to earlier work (Tseng et al, 2005a; Huang
et al, 2007), this model achieves the state-of-the-art
accuracy. Despite of simplicity, our discriminative
POS tagging model achieves a state-of-the-art per-
formance, even better.
System Acc.
Trigram HMM (Huang et al, 2009) 93.99%
Bigram HMM-LA (Huang et al, 2009) 94.53%
Our tagger 94.69%
Table 1: Tagging accuracies on the test data (setting 1).
2.4 Motivating Analysis
For the following experiments, we only report re-
sults on the development data of the CoNLL setting.
2.4.1 Correlating Tagging Accuracy with Word
Frequency
Table 2 summarizes the prediction accuracy on
the development data with respect to the word fre-
quency on the training data. To avoid overestimat-
ing the tagging accuracy, these statistics exclude all
punctuations. From this table, we can see that words
with low frequency, especially the out-of-vocabulary
(OOV) words, are hard to label. However, when a
word is very frequently used, its behavior is very
complicated and therefore hard to predict. A typi-
cal example of such words is the language-specific
function word ??.? This analysis suggests that a
main topic to enhance Chinese POS tagging is to
bridge the gap between the infrequent words and fre-
quent words.
Freq. Acc.
0 83.55%
1-5 89.31%
6-10 90.20%
11-100 94.88%
101-1000 96.26%
1001- 93.65%
Table 2: Tagging accuracies relative to word frequency.
2.4.2 Correlating Tagging Accuracy with Span
Length
A word projects its grammatical property to its
maximal projection and it syntactically governs all
words under the span of its maximal projection. The
words under the span of current token thus reflect
its syntactic behavior and good clues for POS tag-
ging. Table 3 shows the tagging accuracies relative
to the length of the spans. We can see that with the
increase of the number of words governed by the
token, the difficulty of its POS prediction increase.
This analysis suggests that syntagmatic lexical re-
lations plays a significant role in POS tagging, and
sometimes words located far from the current token
affect its tagging much.
Len. Acc.
1-2 93.79%
3-4 93.39%
5-6 92.19%
7- 94.18%
Table 3: Tagging accuracies relative to span length.
3 Capturing Paradigmatic Relations via
Word Clustering
To bridge the gap between high and low fre-
quency words, we employ word clustering to acquire
244
the knowledge about paradigmatic lexical relations
from large-scale texts. Our work is also inspired
by the successful application of word clustering to
named entity recognition (Miller et al, 2004) and
dependency parsing (Koo et al, 2008).
3.1 Word Clustering
Word clustering is a technique for partitioning sets
of words into subsets of syntactically or semanti-
cally similar words. It is a very useful technique
to capture paradigmatic or substitutional similarity
among words.
3.1.1 Clustering Algorithms
Various clustering techniques have been pro-
posed, some of which, for example, perform au-
tomatic word clustering optimizing a maximum-
likelihood criterion with iterative clustering algo-
rithms. In this paper, we focus on distributional
word clustering that is based on the assumption that
words that appear in similar contexts (especially
surrounding words) tend to have similar meanings.
They have been successfully applied to many NLP
problems, such as language modeling.
Brown Clustering Our first choice is the bottom-
up agglomerative word clustering algorithm of
(Brown et al, 1992) which derives a hierarchical
clustering of words from unlabeled data. This al-
gorithm generates a hard clustering ? each word be-
longs to exactly one cluster. The input to the algo-
rithm is sequences of words w1, ..., wn. Initially, the
algorithm starts with each word in its own cluster.
As long as there are at least two clusters left, the al-
gorithm merges the two clusters that maximizes the
quality of the resulting clustering. The quality is de-
fined based on a class-based bigram language model
as follows.
P (wi|w1, ...wi?1) ? p(C(wi)|C(wi?1))p(wi|C(wi))
where the function C maps a word w to its class
C(w). We use a publicly available package2 (Liang
et al, 2005) to train this model.
MKCLS Clustering We also do experiments by
using another popular clustering method based on
2http://cs.stanford.edu/?pliang/
software/brown-cluster-1.2.zip
the exchange algorithm (Kneser and Ney, 1993).
The objective function is maximizing the likelihood
?n
i=1 P (wi|w1, ..., wi?1) of the training data given
a partially class-based bigram model of the form
P (wi|w1, ...wi?1) ? p(C(wi)|wi?1)p(wi|C(wi))
We use the publicly available implementation MK-
CLS3 (Och, 1999) to train this model.
We choose to work with these two algorithms
considering their prior success in other NLP appli-
cations. However, we expect that our approach can
function with other clustering algorithms.
3.1.2 Data
Chinese Gigaword is a comprehensive archive
of newswire text data that has been acquired over
several years by the Linguistic Data Consortium
(LDC). The large-scale unlabeled data we use in
our experiments comes from the Chinese Gigaword
(LDC2005T14). We choose the Mandarin news text,
i.e. Xinhua newswire. This data covers all news
published by Xinhua News Agency (the largest news
agency in China) from 1991 to 2004, which contains
over 473 million characters.
3.1.3 Pre-processing: Word Segmentation
Different from English and other Western lan-
guages, Chinese is written without explicit word de-
limiters such as space characters. To find the basic
language units, i.e. words, segmentation is a neces-
sary pre-processing step for word clustering. Previ-
ous research shows that character-based segmenta-
tion models trained on labeled data are reasonably
accurate (Sun, 2010). Furthermore, as shown in
(Sun and Xu, 2011), appropriate string knowledge
acquired from large-scale unlabeled data can signif-
icantly enhance a supervised model, especially for
the prediction of out-of-vocabulary (OOV) words.
In this paper, we employ such supervised and semi-
supervised segmenters4 to process raw texts.
3.2 Improving Tagging with Cluster Features
Our discriminative sequential tagger is easy to be ex-
tended with arbitrary features and therefore suitable
to explore additional features derived from other
3http://code.google.com/p/giza-pp/
4http://www.coli.uni-saarland.de/?wsun/
ccws.tgz
245
sources. We propose to use of word clusters as sub-
stitutes for word forms to assist the POS tagger. We
are relying on the ability of the discriminative learn-
ing method to explore informative features, which
play a central role in boosting the tagging perfor-
mance. 5 clustering-based uni/bi-gram features are
added: w?1, w, w+1, w?1 w, w w+1.
3.3 Evaluation
Features Data Brown MKCLS
Baseline CoNLL 94.48%
+c100 +1991-1995(S) 94.77% 94.83%
+c500 +1991-1995(S) 94.84% 94.93%
+c1000 +1991-1995(S) - - 94.95%
+c100 +1991-1995(SS) 94.90% 94.97%
+c500 +1991-1995(SS) 94.94% 94.88%
+c1000 +1991-1995(SS) 94.89% 94.94%
+c100 +1991-2000(SS) 94.82% 94.93%
+c500 +1991-2000(SS) 94.92% 94.99%
+c1000 +1991-2000(SS) 94.90% 95.00%
+c100 +1991-2004(SS) - - 94.87%
+c500 +1991-2004(SS) - - 95.02%
+c1000 +1991-2004(SS) - - 94.97%
Table 4: Tagging accuracies with different features. S:
supervised segmentation; SS: semi-supervised segmenta-
tion.
Table 4 summarizes the tagging results on the de-
velopment data with different feature configurations.
In this table, the symbol ?+? in the Features col-
umn means current configuration contains both the
baseline features and new cluster-based features; the
number is the total number of the clusters; the sym-
bol ?+? in the Data column means which portion of
the Gigaword data is used to cluster words; the sym-
bol ?S? and ?SS? in parentheses denote (s)upervised
and (s)emi-(s)upervised word segmentation. For ex-
ample, ?+1991-2000(S)? means the data from 1991
to 2000 are processed by a supervised segmenter
and used for clustering. From this table, we can
clearly see the impact of word clustering features on
POS tagging. The new features lead to substantial
improvements over the strong supervised baseline.
Moreover, these increases are consistent regardless
of the clustering algorithms. Both clustering algo-
rithms contributes to the overall performance equiv-
alently. A natural strategy for extending current ex-
periments is to include both clustering results to-
gether, or to include more than one cluster granular-
ity. However, we find no further improvement. For
each clustering algorithm, there are not much dif-
ferences among different sizes of the total clustering
numbers. When a comparable amount of unlabeled
data (five years? data) is used, the further increase
of the unlabeled data for clustering does not lead to
much changes of the tagging performance.
3.4 Learning Curves
Size Baseline +Cluster
4.5K 90.10% 91.93%
9K 92.91% 93.94%
13.5K 93.88% 94.60%
18K 94.24% 94.77%
Table 5: Tagging accuracies relative to sizes of training
data. Size=#sentences in the training corpus.
We do additional experiments to evaluate the ef-
fect of the derived features as the amount of la-
beled training data is varied. We also use the
?+c500(MKCLS)+1991-2004(SS)? setting for these
experiments. Table 5 summarizes the accuracies of
the systems when trained on smaller portions of the
labeled data. We can see that the new features obtain
consistent gains regardless of the size of the training
set. The error is reduced significantly on all data
sets. In other words, the word cluster features can
significantly reduce the amount of labeled data re-
quired by the learning algorithm. The relative reduc-
tion is greatest when smaller amounts of the labeled
data are used, and the effect lessens as more labeled
data is added.
3.5 Analysis
Word clustering derives paradigmatic relational in-
formation from unlabeled data by grouping words
into different sets. As a result, the contribution of
word clustering to POS tagging is two-fold. On
the one hand, word clustering captures and abstracts
context information. This new linguistic knowledge
is thus helpful to better correlate a word in a cer-
tain context to its POS tag. On the other hand, the
clustering of the OOV words to some extent fights
the sparse data problem by correlating an OOV word
with in-vocabulary (IV) words through their classes.
To evaluate the two contributions of the word clus-
tering, we limit entries of the clustering lexicon to
only contain IV words, i.e. words appearing in
the training corpus. Using this constrained lexicon,
246
we train a new ?+c500(MKCLS)+1991-2004(SS)?
model and report its prediction power in Table 6.
The gap between the baseline and +IV clustering
models can be viewed as the contribution of the first
effect, while the gap between the +IV clustering and
+All clustering models can be viewed as the second
contribution. This result indicates that the improved
predictive power partially comes from the new in-
terpretation of a POS tag through a clustering, and
partially comes from its memory of OOV words that
appears in the unlabeled data.
Baseline +IV Clustering +All clustering
Acc. 94.48% 94.70%(?0.22) 95.02%(?0.32)
Table 6: Tagging accuracies with IV clustering.
Table 7 shows the recall of OOV words on the
development data set. Only the word types appear-
ing more than 10 times are reported. The recall of
all OOV words are improved, especially of proper
nouns (NR) and common verbs (VV). Another in-
teresting fact is that almost all of them are content
words. This table is also helpful to understand the
impact of the clustering information on the predic-
tion of OOV words.
4 Capturing Syntagmatic Relations via
Constituency Parsing
Syntactic analysis, especially the full and deep one,
reflects syntagmatic relations of words and phrases
of sentences. We present a series of empirical stud-
ies of the tagging results of our syntax-free sequen-
tial tagger and a syntax-based chart parser5, aiming
at illuminating more precisely the impact of infor-
mation about phrase-structures on POS tagging. The
analysis is helpful to understand the role of syntag-
matic lexical relations in POS prediction.
4.1 Comparing Tagging and PCFG-LA Parsing
The majority of the state-of-the-art constituent
parsers are based on generative PCFG learning, with
lexicalized (Collins, 2003; Charniak, 2000) or la-
tent annotation (PCFG-LA) (Matsuzaki et al, 2005;
Petrov et al, 2006; Petrov and Klein, 2007) refine-
ments. Compared to lexicalized parsers, the PCFG-
LA parsers leverages on an automatic procedure to
5Both the tagger and the parser are trained on the same por-
tion from CTB.
#Words Baseline +Clustering ?
AD 21 33.33% 42.86% <
CD 249 97.99% 98.39% <
JJ 86 3.49% 26.74% <
NN 1028 91.05% 91.34% <
NR 863 81.69% 88.76% <
NT 25 60.00% 68.00% <
VA 15 33.33% 53.33% <
VV 402 67.66% 72.39% <
Table 7: The tagging recall of OOV words.
learn refined grammars and are therefore more ro-
bust to parse non-English languages that are not well
studied. For Chinese, a PCFG-LA parser achieves
the state-of-the-art performance and defeat many
other types of parsers (Zhang and Clark, 2009). For
full parsing, the Berkeley parser6, an open source
implementation of the PCFG-LA model, is used for
experiments. Table 8 shows their overall and de-
tailed performance.
4.1.1 Content Words vs. Function Words
Table 8 gives a detailed comparison regarding dif-
ferent word types. For each type of word, we re-
port the accuracy of both solvers and compare the
difference. The majority of the words that are bet-
ter labeled by the tagger are content words, includ-
ing nouns(NN, NR, NT), numbers (CD, OD), pred-
icates (VA, VC, VE), adverbs (AD), nominal modi-
fiers (JJ), and so on. In contrast, most of the words
that are better predicted by the parser are function
words, including most particles (DEC, DEG, DER,
DEV, AS, MSP), prepositions (P, BA) and coordi-
nating conjunction (CC).
4.1.2 Open Classes vs. Close Classes
POS can be divided into two broad supercate-
gories: closed class types and open class types.
Open classes accept the addition of new morphemes
(words), through such processes as compounding,
derivation, inflection, coining, and borrowing. On
the other hand closed classes are those that have rel-
atively fixed membership. For example, nouns and
verbs are open classes because new nouns and verbs
are continually coined or borrowed from other lan-
guages, while DEC/DEG are two closed classes be-
cause only the function word ??? is assigned to
6http://code.google.com/p/
berkeleyparser/
247
Parser<Tagger Parser>Tagger
? AD 94.15<94.71 ? AS 98.54>98.44
? CD 94.66<97.52 ? BA 96.15>92.52
CS 91.12<92.12 ? CC 93.80>90.58
ETC 99.65<100.0 ? DEC 85.78>81.22
? JJ 81.35<84.65 ? DEG 88.94>85.96
LB 91.30<93.18 ? DER 80.95>77.42
LC 96.29<97.08 ? DEV 84.89>74.78
M 95.62<96.94 DT 98.28>98.05
? NN 93.56<94.95 ?MSP 91.30>90.14
? NR 89.84<95.07 ? P 96.26>94.56
? NT 96.70<97.26 VV 91.99>91.87
? OD 81.06<86.36
PN 98.10<98.15
SB 95.36<96.77
SP 61.70<68.89
? VA 81.27<84.25 Overall
? VC 95.91<97.67 Tagger: 94.48%
? VE 97.12<98.48 Parser: 93.69%
Table 8: Tagging accuracies of relative to word classes.
them. The discriminative model can conveniently
include many features, especially features related to
the word formation, which are important to predict
words of open classes. Table 9 summarizes the tag-
ging accuracies relative to IV and OOV words. On
the whole, the Berkeley parser processes IV words
slightly better than our tagger, but processes OOV
words significantly worse. The numbers in this ta-
ble clearly shows the main weakness of the Berkeley
parser is the the predictive power of the OOV words.
IV OOV
Tagger 95.22% 81.59%
Parser 95.38% 64.77%
Table 9: Tagging accuracies of the IV and OOV words.
4.1.3 Local Disambiguation vs. Global
Disambiguation
Closed class words are generally function words
that tend to occur frequently and often have struc-
turing uses in grammar. These words have little
lexical meaning or have ambiguous meaning, but
instead serve to express grammatical relationships
with other words within a sentence. They signal
the structural relationships that words have to one
another and are the glue that holds sentences to-
gether. Thus, they serve as important elements to the
structures of sentences. The disambiguation of these
words normally require more syntactic clues, which
is very hard and inappropriate for a sequential tagger
to capture. Based on global grammatical inference
of the whole sentence, the full parser is relatively
good at dealing with structure related ambiguities.
We conclude that discriminative sequential tag-
ging model can better capture local syntactic and
morphological information, while the full parser can
better capture global syntactic structural informa-
tion. The discriminative tagging model are limited
by the Markov assumption and inadequate to cor-
rectly label structure related words.
4.2 Enhancing POS Tagging via Bagging
The diversity analysis suggests that we may im-
prove parsing by simply combining the tagger and
the parser. Bootstrap aggregating (Bagging) is a ma-
chine learning ensemble meta-algorithm to improve
classification and regression models in terms of sta-
bility and classification accuracy (Breiman, 1996). It
also reduces variance and helps to avoid overfitting.
We introduce a Bagging model to integrate different
POS tagging models. In the training phase, given
a training set D of size n, our model generates m
new training sets Di of size 63.2%? n by sampling
examples from D without replacement. Namely no
example will be repeated in each Di. Each Di is
separately used to train a tagger and a parser. Us-
ing this strategy, we can get 2m weak solvers. In the
tagging phase, the 2m models outputs 2m tagging
results, each word is assigned one POS label. The
final tagging is the voting result of these 2m labels.
There may be equal number of different tags. In this
case, our system prefer the first label they met.
4.3 Evaluation
We evaluate our combination model on the same
data set used above. Figure 1 shows the influence
of m in the Bagging algorithm. Because each new
data set Di in bagging algorithm is generated by a
random procedure, the performance of all Bagging
experiments are not the same. To give a more sta-
ble evaluation, we repeat 5 experiments for each m
and show the averaged accuracy. We can see that
the Bagging model taking both sequential tagging
and chart parsing models as basic systems outper-
form the baseline systems and the Bagging model
taking either model in isolation as basic systems. An
248
 93
 93.5
 94
 94.5
 95
 95.5
 1  2  3  4  5  6  7  8  9  10
A
c
c
u
r
a
c
y
 
(
%
)
Number of sampling data sets m
TaggerParserTagger(WC)Tagger-BaggingParser-BaggingTagger+Parser-BaggingTagger(WC)-BaggingTagger(WC)+Parser-Bagging
Figure 1: Tagging accuracies of Bagging models.
Tagger-Bagging and Tagger(WC)-Bagging means that the
Bagging system built on the tagger with and without
word clusters. Parser-Bagging is named in the same way.
Tagger+Paser-Bagging and Tagger(WC)+Paser-Bagging
means that the Bagging systems are built on both tagger
and parser.
interesting phenomenon is that the Bagging method
can also improve the parsing model, but there is a
decrease while only combining taggers.
5 Combining Both
We have introduced two separate improvements for
Chinese POS tagging, which capture different types
of lexical relations. We therefore expect further im-
provement by combining both enhancements, since
their contributions to the task is different. We still
use a Bagging model to integrate the discriminative
tagger and the Berkeley parser. The only differ-
ence between current experiment and previous ex-
periment is that the sub-tagging models are trained
with help of word clustering features. Figure 1 also
shows the performance of the new Bagging model
on the development data set. We can see that the im-
provements that come from two ways, namely cap-
turing syntagmatic and paradigmatic relations, are
not much overlapping and the combination of them
gives more.
Table 10 shows the performance of different sys-
tems evaluated on the test data. The final result is
remarkable. The word clustering features and the
Bagging model result in a relative error reduction of
18% in terms of the classification accuracy. The sig-
nificant improvement of the POS tagging also help
successive language processing. Results in Table
Systems Acc.
Baseline 94.33%
Tagger(WC) 94.85%
Tagger+Parser(m = 15) 94.96%
Tagger(WC)+Parser(m = 15) 95.34%
Table 10: Tagging accuracies on the test data (CoNLL).
11 indicate that the parsing accuracy of the Berke-
ley parser can be simply improved by inputting the
Berkeley parser with the POS Bagging results. Al-
though the combination with a syntax-based tagger
is very effective, there are two weaknesses: (1) a
syntax-based model relies on linguistically rich syn-
tactic annotations that are not easy to acquire; (2)
a syntax-based model is computationally expensive
which causes efficiency difficulties.
Tagger LP LR F
Berkeley 82.71% 80.57% 81.63
Bagging(m = 15) 82.96% 81.44% 82.19
Table 11: Parsing accuracies on the test data. (CoNLL)
6 Conclusion
We hold a view of structuralist linguistics and study
the impact of paradigmatic and syntagmatic lexical
relations on Chinese POS tagging. First, we har-
vest word partition information from large-scale raw
texts to capture paradigmatic relations and use such
knowledge to enhance a supervised tagger via fea-
ture engineering. Second, we comparatively analyze
syntax-free and syntax-based models and employ a
Bagging model to integrate a sequential tagger and
a chart parser to capture syntagmatic relations that
have a great impact on non-local disambiguation.
Both enhancements significantly improve the state-
of-the-art of Chinese POS tagging. The final model
results in an error reduction of 18% over a state-of-
the-art baseline.
Acknowledgement
This work is mainly finished when the first author
was in Saarland University and DFKI. At that time,
this author was funded by DFKI and German Aca-
demic Exchange Service (DAAD). While working
in Peking University, the first author is supported
by NSFC (61170166) and National High-Tech R&D
Program (2012AA011101).
249
References
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467?
479. URL http://portal.acm.org/
citation.cfm?id=176313.176316.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first con-
ference on North American chapter of the Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing,
pages 1?8. Association for Computational Lin-
guistics. URL http://www.aclweb.org/
anthology/W02-1001.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Jes?s Gim?nez and Llu?s M?rquez. 2004.
Svmtool: A general pos tagger generator based
on support vector machines. In In Proceedings
of the 4th International Conference on Language
Resources and Evaluation, pages 43?46.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1077?1086. Association for Computational Lin-
guistics, Uppsala, Sweden. URL http://www.
aclweb.org/anthology/P10-1110.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and
self-training. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 213?216. As-
sociation for Computational Linguistics, Boulder,
Colorado. URL http://www.aclweb.org/
anthology/N/N09/N09-2054.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-
CoNLL), pages 1093?1102. Association for
Computational Linguistics, Prague, Czech Re-
public. URL http://www.aclweb.org/
anthology/D/D07/D07-1117.
Reinhard Kneser and Hermann Ney. 1993. Im-
proved clustering techniques for class-based sta-
tistical language modeling. In In Proceedings of
the European Conference on Speech Communica-
tion and Technology (Eurospeech).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency
parsing. In Proceedings of ACL-08: HLT,
pages 595?603. Association for Computa-
tional Linguistics, Columbus, Ohio. URL
http://www.aclweb.org/anthology/
P/P08/P08-1068.
Thomas Lavergne, Olivier Cappe?, and Franc?ois
Yvon. 2010. Practical very large scale CRFs.
pages 504?513. URL http://www.aclweb.
org/anthology/P10-1052.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting
Liu, Wenliang Chen, and Haizhou Li. 2011.
Joint models for Chinese pos tagging and depen-
dency parsing. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1180?1191. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1109.
Percy Liang, Michael Collins, and Percy Liang.
2005. Semi-supervised learning for natural lan-
guage. In Master?s thesis, MIT.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent an-
notations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ?05, pages 75?82. Associa-
tion for Computational Linguistics, Stroudsburg,
250
PA, USA. URL http://dx.doi.org/10.
3115/1219840.1219850.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 337?342. As-
sociation for Computational Linguistics, Boston,
Massachusetts, USA.
Franz Josef Och. 1999. An efficient method for
determining bilingual word classes. In Pro-
ceedings of the ninth conference on European
chapter of the Association for Computational
Linguistics, EACL ?99, pages 71?76. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/977035.977046.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433?440. Association for Computational Linguis-
tics, Sydney, Australia.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the
Main Conference, pages 404?411. Association
for Computational Linguistics, Rochester, New
York.
Libin Shen, Giorgio Satta, and Aravind Joshi.
2007. Guided learning for bidirectional sequence
classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computa-
tional Linguistics, pages 760?767. Association
for Computational Linguistics, Prague, Czech Re-
public. URL http://www.aclweb.org/
anthology/P07-1096.
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Compari-
son and combination. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1211?1219.
Coling 2010 Organizing Committee, Beijing,
China. URL http://www.aclweb.org/
anthology/C10-2139.
Weiwei Sun and Jia Xu. 2011. Enhancing
Chinese word segmentation using unlabeled
data. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 970?979. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1090.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages
173?180. Association for Computational Linguis-
tics, Stroudsburg, PA, USA. URL http://dx.
doi.org/10.3115/1073445.1073478.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning.
2005a. A conditional random field word seg-
menter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Huihsin Tseng, Daniel Jurafsky, and Christopher
Manning. 2005b. Morphological features help
pos tagging of unknown words across language
varieties. In The Fourth SIGHAN Workshop on
Chinese Language Processing.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for
Chinese. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pages 425?432. As-
sociation for Computational Linguistics, Sydney,
Australia. URL http://www.aclweb.org/
anthology/P06-1054.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn Chinese treebank: Phrase
structure annotation of a large corpus. Natural
Language Engineering, 11(2):207?238.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
251
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
562?571. Association for Computational Linguis-
tics, Honolulu, Hawaii. URL http://www.
aclweb.org/anthology/D08-1059.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a
global discriminative model. In Proceedings
of the 11th International Conference on Pars-
ing Technologies (IWPT?09), pages 162?171. As-
sociation for Computational Linguistics, Paris,
France. URL http://www.aclweb.org/
anthology/W09-3825.
252
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446?456,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Grammatical Relations in Chinese:
GB-Ground Extraction and Data-Driven Parsing
Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, Xiaojun Wan
?
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,duyantao,kouxin,wanxiaojun}@pku.edu.cn, dsy100@gmail.com
Abstract
This paper is concerned with building linguistic re-
sources and statistical parsers for deep grammatical
relation (GR) analysis of Chinese texts. A set of
linguistic rules is defined to explore implicit phrase
structural information and thus build high-quality
GR annotations that are represented as general di-
rected dependency graphs. The reliability of this
linguistically-motivated GR extraction procedure is
highlighted by manual evaluation. Based on the
converted corpus, we study transition-based, data-
driven models for GR parsing. We present a novel
transition system which suits GR graphs better than
existing systems. The key idea is to introduce a new
type of transition that reorders top k elements in the
memory module. Evaluation gauges how successful
GR parsing for Chinese can be by applying data-
driven models.
1 Introduction
Grammatical relations (GRs) represent functional
relationships between language units in a sen-
tence. They are exemplified in traditional gram-
mars by the notions of subject, direct/indirect
object, etc. GRs have assumed an important
role in linguistic theorizing, within a variety of
approaches ranging from generative grammar to
functional theories. For example, several com-
putational grammar formalisms, such as Lexi-
cal Function Grammar (LFG; Bresnan and Ka-
plan, 1982; Dalrymple, 2001) and Head-driven
Phrase Structure Grammar (HPSG; Pollard and
Sag, 1994) encode grammatical functions directly.
In particular, GRs can be viewed as the depen-
dency backbone of an LFG analysis that provide
general linguistic insights, and have great potential
advantages for NLP applications, (Kaplan et al,
2004; Briscoe and Carroll, 2006; Clark and Cur-
ran, 2007a; Miyao et al, 2007).
?
Email correspondence.
In this paper, we address the question of an-
alyzing Chinese sentences with deep GRs. To
acquire high-quality GR corpus, we propose a
linguistically-motivated algorithm to translate a
Government and Binding (GB; Chomsky, 1981;
Carnie, 2007) grounded phrase structure treebank,
i.e. Chinese Treebank (CTB; Xue et al, 2005)
to a deep dependency bank where GRs are ex-
plicitly represented. Different from popular shal-
low dependency parsing that focus on tree-shaped
structures, our GR annotations are represented as
general directed graphs that express not only lo-
cal but also various long-distance dependencies,
such as coordinations, control/raising construc-
tions, topicalization, relative clauses and many
other complicated linguistic phenomena that goes
beyond shallow syntax (see Fig. 1 for example.).
Manual evaluation highlights the reliability of our
linguistically-motivated GR extraction algorithm:
The overall dependency-based precision and recall
are 99.17 and 98.87. The automatically-converted
corpus would be of use for a wide variety of NLP
tasks.
Recent years have seen the introduction of a
number of treebank-guided statistical parsers ca-
pable of generating considerably accurate parses
for Chinese. With the high-quality GR resource
at hand, we study data-driven GR parsing. Previ-
ous work on dependency parsing mainly focused
on structures that can be represented in terms of
directed trees. We notice two exceptions. Sagae
and Tsujii (2008) and Titov et al (2009) individ-
ually studied two transition systems that can gen-
erate more general graphs rather than trees. In-
spired by their work, we study transition-based
models for building deep dependency structures.
The existence of a large number of crossing arcs in
GR graphs makes left-to-right, incremental graph
spanning computationally hard. Applied to our
data, the two existing systems cover only 51.0%
and 76.5% GR graphs respectively. To better suit
446
?? ?? ? ?? ?? ? ?? ?? ?? ? ??? ??
Pudong recently issue practice involve economic field regulatory document
root root
comp
temp
temp
subj
subj
prt
prt
obj
obj
comp
subj*ldd
obj
nmod
relative
nmod
Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field.
The symbol ?*ldd? indicates long-distance dependencies; ?subj*ldd? between the word ???/involve?
and the word ???/documents? represents a long-range subject-predicate relation. The arguments and
adjuncts of the coordinated verbs, namely ???/issue? and ???/practice,? are separately yet distribu-
tively linked the two heads.
our problem, we extend Titov et al?s work and
study what we call K-permutation transition sys-
tem. The key idea is to introduce a new type of
transition that reorders top k (2 ? k ? K) el-
ements in the memory module of a stack-based
transition system. With the increase of K, the ex-
pressiveness of the corresponding system strictly
increases. We propose an oracle deriving method
which is guaranteed to find a sound transition se-
quence if one exits. Moreover, we introduce an
effective approximation of that oracle, which de-
creases decoding ambiguity but practically covers
almost exactly the same graphs for our data.
Based on the stronger transition system, we
build a GR parser with a discriminative model for
disambiguation and a beam decoder for inference.
We conduct experiments on CTB 6.0 to profile this
parser. With the increase of the K, the parser is
able to utilize more GR graphs for training and
the numeric performance is improved. Evaluation
gauges how successful GR parsing for Chinese
can be by applying data-driven models. Detailed
analysis reveal some important factors that may
possibly boost the performance. To our knowl-
edge, this work provides the first result of exten-
sive experiments of parsing Chinese with GRs.
We release our GR processing kit and gold-
standard annotations for research purposes. These
resources can be downloaded at http://www.
icst.pku.edu.cn/lcwm/omg.
2 GB-grounded GR Extraction
In this section, we discuss the construction of the
GR annotations. Basically, the annotations are au-
tomatically converted from a GB-grounded phrase-
structure treebank, namely CTB. Conceptually,
this conversion is similar to the conversions from
CTB structures to representations in deep gram-
mar formalisms (Tse and Curran, 2010; Yu et al,
2010; Guo et al, 2007; Xia, 2001). However, our
work is grounded in GB, which is the linguistic ba-
sis of the construction of CTB. We argue that this
theoretical choice makes the conversion process
more compatible with the original annotations and
therefore more accurate. We use directed graphs to
explicitly encode bi-lexical dependencies involved
in coordination, raising/control constructions, ex-
traction, topicalization, and many other compli-
cated phenomena. Fig. 1 shows an example of
such a GR graph and its original CTB annotation.
2.1 Linguistic Basis
GRs are encoded in different ways in different lan-
guages. In some languages, e.g. Turkish, gram-
matical function is encoded by means of morpho-
logical marking, while in highly configurational
languages, e.g. Chinese, the grammatical function
of a phrase is heavily determined by its constituent
structure position. Dominant Chomskyan theo-
ries, including GB, have defined GRs as configu-
rations at phrase structures. Following this princi-
ple, CTB groups words into constituents through
the use of a limited set of fundamental grammat-
ical functions. Transformational grammar utilizes
empty categories (ECs) to represent long-distance
dependencies. In CTB, traces are provided by
relating displaced linguistic material to where it
should be interpreted semantically. By exploiting
configurational information, traces and functional
tag annotations, GR information can be hopefully
447
IP
VP
?=?
VP
?=?
NP
?=(? OBJ)
NP
?=?
NN
?=?
??
NN
??(? NMOD)
???
CP
??(? REL)
DEC
?=?
?
IP
?=(? COMP)
VP
?=?
NP
?=(? OBJ)
NN
?=?
??
NP
??(? NMOD)
NN
?=?
??
VV
?=?
??
NP
?=(? SBJ)
-NONE-
*T*
AS
?=(? PRT)
?
VCD
?=?
VV
???
??
VV
???
??
LCP
??(? TMP)
LC
?=?
?
NP
?=(? COMP)
NT
?=?
??
NP
?=(? SBJ)
NR
??
Figure 2: The original CTB annotation augmented with LFG-like f-structure annotations of the running
example.
derived from CTB trees with high accuracy.
2.2 The Extraction Algorithm
Our treebank conversion algorithm borrows
key insights from Lexical Functional Grammar
(LFG; Bresnan and Kaplan, 1982; Dalrymple,
2001). LFG posits two levels of representation:
c(onstituent)-structure and f(unctional)-structure
minimally. C-structure is represented by phrase-
structure trees, and captures surface syntactic con-
figurations such as word order, while f-structure
encodes grammatical functions. It is easy to ex-
tract a dependency backbone which approximates
basic predicate-argument-adjunct structures from
f-structures. The construction of the widely used
PARC DepBank (King et al, 2003) is a good ex-
ample.
LFG relates c-structure and f-structure through
f-structure annotations, which compositionally
map every constituent to a corresponding f-
structure. Borrowing this key idea, we translate
CTB trees to dependency graphs by first augment-
ing each constituency with f-structure annotations,
then propagating the head words of the head or
conjunct daughter(s) upwards to their parents, and
finally creating a dependency graph. The follow-
ing presents details step-by-step.
Tapping implicit information. Xue (2007) in-
troduced a systematic study to tap the implicit
functional information of CTB. This gives us a
very good start to extract GRs. We slightly modify
their method to enrich a CTB tree with f-structure
annotations: Each node in a resulting tree is anno-
tated with one and only one corresponding equa-
tion. See Fig. 2 for example. Comparing the orig-
inal annotation and enriched one, we can see that
the functionality of this step is to explicitly repre-
sent and regulate grammatical functions.
Beyond CTB annotations: tracing more. Nat-
ural languages do not always interpret linguistic
material locally. In order to obtain accurate and
complete GR, predicate-argument, or logical form
representations, a hallmark of deep grammars is
that they usually involve a non-local dependency
resolution mechanism. CTB trees utilize ECs and
coindexed materials to represent long-distance de-
pendencies. An EC is a nominal element that does
not have any phonological content and is therefore
unpronounced. Two kinds of anaphoric ECs, i.e.
big PRO and trace, are annotated in CTB. Theo-
retically speaking, only trace is generated as the
result of movement and therefore annotated with
antecedents in CTB. We carefully check the anno-
tation and find that a considerable amount of an-
tecedents are not labeled, and hence a lot of impor-
448
VP{??,??}
VP{??,??}
NP{??}
NP{??}CP{?}
DEC{?}IP{??}
VP{??}NP{??*ldd}
AS{?}VP{??,??}
LCP{?}
Figure 3: An example of lexicalized tree after
head word upward passing. Only partial result is
shown. The long-distance dependency between
???/involve? and ???/document? is created
through copying the dependent to a coindexed
anaphoric EC position.
tant non-local information is missing. In addition,
since the big PRO is also anaphoric, it is possible
to find coindexed components sometimes. Such
non-local information is also very valuable.
Beyond CTB annotations, we introduce a num-
ber of phrase-structure patterns to extract more
non-local dependencies. The method heavily
leverages linguistic rules to exploit structural in-
formation. We take into account both theoreti-
cal assumptions and analyzing practices to enrich
coindexation information according to phrase-
structure patterns. In particular, we try to link
an anaphoric EC e with its c-commonders if no
non-empty antecedent has already been coindexed
with e. Because the CTB is influenced deeply by
the X-bar syntax, which regulates constituent anal-
ysis much, the number of our linguistic rules is
quite modest. For the development of conversion
rules, we used the first 9 files of CTB, which con-
tains about 100 sentences. Readers can refer to
the well-documented Perl script for details. See
Fig. 2 for example. The noun phrase ????
??/regulatory documents? is related to the trace
?*T*.? This coindexation is not labeled by the
original annotation.
Passing head words and linking ECs. Based
on an enriched tree, our algorithm propagates the
head word of the head daughter upwards to their
parents, linking coindexed units, and finally creat-
ing a GR graph. The partial result after head word
passing of the running example is shown in Fig. 3.
There are two differences of the head word passing
between our GR extraction and a ?normal? depen-
dency tree extraction. First, the GR extraction pro-
cedure may pass multiple head words to its parent,
especially in a coordination construction. Second,
Precision Recall F-score
Unlabeled 99.48 99.17 99.32
Labeled 99.17 98.87 99.02
Table 1: Manual evaluation of 209 sentences.
long-distance dependencies are created by linking
ECs and their coindexed phrases.
2.3 Manual Evaluation
To have a precise understanding of whether our ex-
traction algorithm works well, we have selected 20
files that contains 209 sentences in total for man-
ual evaluation. Linguistic experts carefully exam-
ine the corresponding GR graphs derived by our
extraction algorithm and correct all errors. In other
words, a gold standard GR annotation set is cre-
ated. The measure for comparing two dependency
graphs is precision/recall of GR tokens which are
defined as ?w
h
, w
d
, l? tuples, wherew
h
is the head,
w
d
is the dependent and l is the relation. Labeled
precision/recall (LP/LR) is the ratio of tuples cor-
rectly identified by the automatic generator, while
unlabeled precision/recall (UP/UR) is the ratio re-
gardless of l. F-score is a harmonic mean of pre-
cision and recall. These measures correspond to
attachment scores (LAS/UAS) in dependency tree
parsing. To evaluate our GR parsing models that
will be introduced later, we also report these met-
rics.
The overall performance is summarized in Tab.
1. We can see that the automatical GR extraction
achieves relatively high performance. There are
two sources of errors in treebank conversion: (1)
inadequate conversion rules and (2) wrong or in-
consistent original annotations. During the cre-
ation of the gold standard corpus, we find that
the former is mainly caused by complicated un-
bounded dependencies and the lack of internal
structure for some kinds of phrases. Such prob-
lems are very hard to solve through rules only, if
not possible, since original annotations do not pro-
vide sufficient information. The latter problem is
more scattered and unpredictable.
2.4 Statistics
Allowing non-projective dependencies generally
makes parsing either by graph-based or transition-
based dependency parsing harder. Substantial re-
search effort has been devoted in recent years to
the design of elegant solutions for this problem.
There are much more crossing arcs in the GR
449
graphs than syntactic dependency trees. In the
training data (defined in Section 4.1), there are
558132 arcs and 86534 crossing pairs, About half
of the sentences have crossing arcs (10930 out of
22277). The wide existence of crossing arcs poses
an essential challenge for GR parsing, namely, to
find methods for handling crossing arcs without a
significant loss in accuracy and efficiency.
3 Transition-based GR Parsing
The availability of large-scale treebanks has con-
tributed to the blossoming of statistical approaches
to build accurate shallow constituency and depen-
dency parsers. With high-quality GR resources at
hand, it is possible to study statistical approaches
to automatically parse GR graphs. In this section,
we investigate the feasibility of applying a data-
driven, grammar-free approach to build GRs di-
rectly. In particular, transition-based dependency
parsing method is studied.
3.1 Data-Driven Dependency Parsing
Data-driven, grammar-free dependency parsing
has received an increasing amount of attention in
the past decade. Such approaches, e.g. transition-
based (Yamada and Matsumoto, 2003; Nivre,
2008) and graph-based (McDonald, 2006; Tor-
res Martins et al, 2009) models have attracted
the most attention of dependency parsing in re-
cent years. Transition-based parsers utilize tran-
sition systems to derive dependency trees together
with treebank-induced statistical models for pre-
dicting transitions. This approach was pioneered
by (Yamada and Matsumoto, 2003) and (Nivre
et al, 2004). Most research concentrated on sur-
face syntactic structures, and the majority of ex-
isting approaches are limited to producing only
trees. We notice two exceptions. Sagae and Tsu-
jii (2008) and Titov et al (2009) individually in-
troduced two transition systems that can generate
specific graphs rather than trees. Inspired by their
work, we study transition-based approach to build
GR graphs.
3.2 Transition Systems
Following (Nivre, 2008), we define a transition
system for dependency parsing as a quadruple S =
(C, T, c
s
, C
t
), where
1. C is a set of configurations, each of which
contains a buffer ? of (remaining) words and
a set A of dependency arcs,
Transitions
SHIFT (?, j|?,A)? (?|j, ?,A)
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
POP (?|i, ?, A)? (?, ?,A)
ROTATE
k
(?|i
k
| . . . |i
2
|i
1
, ?, A)? (?|i
1
|i
k
| . . . |i
2
, ?, A)
Table 2: K-permutation System.
2. T is a set of transitions, each of which is a
(partial) function t : C 7? C,
3. c
s
is an initialization function, mapping a
sentence x to a configuration, with ? =
[1, . . . , n],
4. C
t
? C is a set of terminal configurations.
Given a sentence x = w
1
, . . . , w
n
and a graph
G = (V,A) on it, if there is a sequence of tran-
sitions t
1
, . . . , t
m
and a sequence of configura-
tions c
0
, . . . , c
m
such that c
0
= c
s
(x), t
i
(c
i?1
) =
c
i
(i = 1, . . . ,m), c
m
? C
t
, and A
c
m
= A, we say
the sequence of transitions is an oracle sequence.
And we define
?
A
c
i
= A ? A
c
i
for the arcs to be
built in c
i
. In a typical transition-based parsing
process, the input words are put into a queue and
partially built structures are organized by a stack.
A set of SHIFT/REDUCE actions are performed se-
quentially to consume words from the queue and
update the partial parsing results.
3.3 Online Reordering
Among existing systems, Sagae and Tsujii?s is de-
signed for projective graphs (denoted by G
1
in
Definition 1), and Titov et al?s handles only a
specific subset of non-projective graphs as well
as projective graphs (G
2
). Applied to our data,
only 51.0% and 76.5% of the extracted graphs are
parsable with their systems. Obviously, it is nec-
essary to investigate new transition systems for the
parsing task in our study. To deal with crossing
arcs, Titov et al (2009) and Nivre (2009) designed
a SWAP transition that switches the position of the
two topmost nodes on the stack. Inspired by their
work, we extend this approach to parse more gen-
eral graphs. The basic idea is to provide our new
system with an ability to reorder more nodes dur-
ing decoding in an online fashion, which we refer
to as online reordering.
3.4 K-Permutation System
We define a K-permutation transition system
S
K
= (C, T, c
s
, C
t
), where a configuration c =
450
(?, ?,A) ? C contains a stack ? of nodes be-
sides ? and A. We set the initial configuration
for a sentence x = w
1
, . . . , w
n
to be c
s
(x) =
([], [1, . . . , n], {}), and take C
t
to be the set of all
configurations of the form c
t
= (?, [], A) (for any
arc set A). The set of transitions T contains five
types of actions, as shown in Tab. 2:
1. SHIFT removes the front element from ? and
pushes it onto ?.
2. LEFT-ARC
l
/RIGHT-ARC
l
updates a configu-
ration by adding (i, l, j)/(j, l, i) to A where i
is the top of ?, and j is the front of ?.
3. POP deletes the top element of ?.
4. ROTATE
k
updates a configuration with stack
?|i
k
| . . . |i
2
|i
1
by rotating the top k nodes
in stack left by one index, obtaining
?|i
1
|i
k
| . . . |i
2
, with constraint 2 ? k ? K.
We refer to this system as K-permutation because
by rotating the top k (2 ? k ? K) nodes in the
stack, we can obtain all the permutations of the
top K nodes. Note that S
2
is identical to Titov
et al?s; S
?
is complete with respect to the class of
all directed graphs without self-loop, since we can
arbitrarily permute the nodes in the stack. The K-
permutation system exhibits a nice property: The
sets of corresponding graphs are strictly mono-
tonic with respect to the ? operation.
Definition 1. If a graphG can be parsed with tran-
sition system S
K
, we say G is a K-perm graph.
We use G
K
to denote the set of all k-perm graphs.
Specially, G
0
= ?, G
1
is the set of all projective
graphs, and G
?
=
?
?
k=0
G
k
.
Theorem 1. G
i
( G
i+1
,?i ? 0.
Proof. It is obvious that G
i
? G
i+1
and G
0
( G
1
.
Fig. 4 gives an example which is in G
i+1
but not
in G
i
for all i > 0, indicating G
i
6= G
i+1
.
Theorem 2. G
?
is the set of all graphs without
self-loop.
Proof. It follows immediately from the fact that
G ? G
|V |
, ?G = ?V,E?.
The transition systems introduced in (Sagae and
Tsujii, 2008) and (Titov et al, 2009) can be viewed
as S
1
1
and S
2
.
1
Though Sagae and Tsujii (2008) introduced additional
constraints to exclude cyclic path, the fundamental transition
mechanism of their system is the same to S
1
.
w
1
? ? ? w
i
w
i+1
? ? ? w
2i
w
2i+1
w
2i+2
Figure 4: A graph which is in G
i+1
, but not in G
i
.
3.5 Normal Form Oracle
The K-permutation transition system may allow
multiple oracle transition sequences on one graph,
but trying to sum all the possible oracles is usu-
ally computational expensive. Here we give a con-
struction procedure which is guaranteed to find an
oracle sequence if one exits. We refer it as normal
form oracle (NFO).
Let L(j) be the ordered list of nodes connected
to j in
?
A
c
i?1
for j ? ?
c
i?1
, and let L
K
(?
c
i?1
) =
[L(j
1
), . . . , L(j
max{l,K})
]. If ?
c
i?1
is empty, then
we set t
i
to SHIFT; if there is no arc linked to
j
1
in
?
A
c
i?1
, then we set t
i
to POP; if there exits
a ?
?
A
c
i?1
linking j
1
and b, then we set t
i
to LEFT-
ARC or RIGHT-ARC correspondingly. When there
are only SHIFT and ROTATE left, we first apply
a sequence of ROTATE?s to make L
K
(?) com-
plete ordered by lexicographical order, then apply
a SHIFT. Let c
i
= t
i
(c
i?1
), we continue to com-
pute t
i+1
, until ?
c
i
is empty.
Theorem 3. If a graph is parsable with the transi-
tion system S
K
then the construction procedure is
guaranteed to find an oracle transition sequence.
Proof. During the construction, all the arcs are
built by LEFT-ARC or RIGHT-ARC, which links
the top of the stack and the front of the buffer.
Therefore, we prefer L(?) to be as orderly as pos-
sible, to make the words to be linked sooner on the
top of the stack. the construction procedure above
does best within the power of the system S
K
.
3.6 An Approximation for NFO
In the construction of NFO transitions, we ex-
haustively use the ROTATE?s to make L(?) com-
plete ordered. We also observed that the tran-
sition LEFT-ARC, RIGHT-ARC and SHIFT only
change the relative order between the first element
of L(?) and the rest elements. Therefore we ex-
plored an approximate procedure to determine the
ROTATE?s, based on the observation. We call it ap-
proximate NFO (ANFO). Using notation defined
in Section 3.5, the approximate procedure goes as
follows. When it comes to the determination of
451
w1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
Figure 5: A graph that can be parsed
with S
3
with a transition sequence
SSSSR
3
SR
3
APAPR
2
R
3
SR
3
SR
3
APAPAPAPAP,
where S stands for SHIFT, R for ROTATE, A for
LEFT-ARC, and P for POP. But the approximate
procedure fails to find the oracle, since R
2
R
3
in
bold in the sequence are not to be applied.
the ROTATE sequence, let k be the largest m such
that 0 ? m ? min{K, l} and L(j
m
) strictly pre-
cedes L(j
1
) by the lexicographical order (here we
assume L(j
0
) strictly precedes any L(j), j ? ?).
If k > 0, we set t
i
to ROTATE
k
; else we set t
i
to
SHIFT. The approximation assumes L(?) is com-
pletely ordered except the first element, and insert
the first element to its proper place each time.
Definition 2. We define
?
G
K
as the graphs the ora-
cle of which can be extracted by S
K
with the ap-
proximation procedure.
It can be inferred similarly that Theorem 1 and
Theorem 2 also hold for
?
G?s. However, the
?
G
K
is
not equal to G
K
in non-trivial cases.
Theorem 4.
?
G
i
( G
i
,?i ? 3.
Proof. It is trivial that
?
G
i
? G
i
. An example graph
that is in G
3
but not in
?
G
3
is shown in Figure 5,
examples for arbitrary i > 3 can be constructed
similarly.
The above theorem indicates the inadequacy of
the ANFO deriving procedure. Nevertheless, em-
pirical evaluation (Section 4.2) shows that the cov-
erage of AFO and ANFO deriving procedures are
almost identical when applying to linguistic data.
3.7 Statistical Parsing
When we parse a sentence w
1
w
2
? ? ?w
n
, we start
with the initial configuration c
0
= c
s
(x), and
choose next transition t
i
= C(c
i?1
) iteratively ac-
cording to a discriminative classifier trained on or-
acle sequences. To build a parser, we use a struc-
tured classifier to approximate the oracle, and ap-
ply the Passive-Aggressive (PA) algorithm (Cram-
mer et al, 2006) for parameter estimation. The
PA algorithm is similar to the Perceptron algo-
rithm, the difference from which is the update of
weight vector. We also use parameter averaging
and early update to achieve better training. Devel-
oping features has been shown crucial to advanc-
ing the state-of-the-art in dependency tree parsing
(Koo and Collins, 2010; Zhang and Nivre, 2011).
To build accurate deep dependency parsers, we
utilize a large set of features for disambiguation.
See the notes included in the supplymentary ma-
terial for details. To improve the performance, we
also apply the technique of beam search, which
keep a beam of transition sequences with highest
scores when parsing.
4 Experiments
4.1 Experimental setup
CTB is a segmented, part-of-speech (POS) tagged,
and fully bracketed corpus in the constituency for-
malism, and very popular to evaluate fundamen-
tal NLP tasks, including word segmentation (Sun
and Xu, 2011), POS tagging (Sun and Uszkoreit,
2012), and syntactic parsing (Zhang and Clark,
2009; Sun and Wan, 2013). We use CTB 6.0 and
define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. We use
gold-standard word segmentation and POS tag-
ing results as inputs. All transition-based parsing
models are trained with beam 16 and iteration 30.
Overall precision/recall/f-score with respect to de-
pendency tokens is reported. To evaluate the abil-
ity to recover non-local dependencies, the recall of
such dependencies are reported too.
4.2 Coverage and Accuracy
There is a dual effect of the increase of the param-
eter k to our transition-based dependency parser.
On one hand, the higher k is, the more expres-
sivity the corresponding transition system has. A
system with higher k covers more structures and
allows to use more data for training. On the other
hand, higher k brings more ambiguities to the cor-
responding parser, and the parsing performance
may thus suffer. Note that the ambiguity exists not
only in each step for transition decision, but also
in selecting the training oracle.
The left-most columns of Tab. 3 shows the cov-
erage of K-permutation transition system with re-
spect to different K and different oracle deriving
algorithms. Readers may be surprised that the
coverage of NFO and ANFO deriving procedures
is the same. Actually, all the covered graphs by
the two oracle deriving procedures are exactly the
452
System NFO ANFO UP UR UF LP LR LF UR
L
LR
L
UR
NL
LR
NL
S
2
76.5 76.5 85.88 81.00 83.37 83.98 79.21 81.53 81.93 80.34 58.88 52.17
S
3
89.0 89.0 86.02 81.72 83.82 84.07 79.86 81.91 82.61 80.94 60.46 54.28
S
4
95.6 95.6 86.28 82.06 84.12 84.35 80.22 82.23 82.92 81.29 61.48 54.77
S
5
98.4 98.4 86.44 82.21 84.27 84.51 80.37 82.39 83.15 81.51 59.80 53.30
Table 3: Coverage and accuracy of the GR parser on the development data.
same, except for S
3
. Only 1 from 22277 sen-
tences can find a NFO but not an ANFO. This
number demonstrates the effectiveness of ANFO.
In the following experiments, we use the ANFO?s
to train our parser.
Applied to our data, S
2
, i.e. the exact system in-
troduced by Titov et al (2009), only covers 76.5%
GR graphs. This is very different from the re-
sult obtained on the CoNLL shared task data for
English semantic role labeling (SRL). According
to (Titov et al, 2009), 99% semantic-role-labelled
graphs can be generated by S
2
. We think there are
two main reasons accounting for the differences,
and highlight the importance of the expressiveness
of transition systems to solve deep dependency
parsing problems. First, the SRL task only focuses
on finding arguments and adjuncts of verbal (and
nominal) predicates, while dependencies headed
by other words are not contained in its graph rep-
resentation. On contrast, a deep dependency struc-
ture, like GR graph, approximates deep syntactic
or semantic information of a sentence as a whole,
and therefore is more dense. As a result, permuta-
tion system with a very low k is incapable to han-
dle more cases. Another reason is about the Chi-
nese language. Some language-specific properties
result in complex crossing arcs. For example, se-
rial verb constructions are widely used in Chinese
to describe several separate events without con-
junctions. The verbal heads in such constructions
share subjects and adjuncts, both of which are be-
fore the heads. The distributive dependencies be-
tween verbal heads and subjects/adjuncts usually
produce crossing arcs (see Fig. 6). To test our as-
sumption, we evaluate the coverage of S
2
over the
functor-argument dependency graphs provided by
the English and Chinese CCGBank (Hockenmaier
and Steedman, 2007; Tse and Curran, 2010). The
result is 96.9% vs. 89.0%, which confirms our
linguistic intuition under another grammar formal-
ism.
Tab. 3 summarizes the performance of the
transition-based parser with different configura-
tions to reveal how well data-driven parsing can
subject adjunct verb
1
verb
2
Figure 6: A simplified example to illustrate cross-
ing arcs in serial verbal constructions.
be performed in realistic situations. We can see
that with the increase of K, the overall parsing ac-
curacy incrementally goes up. The high complex-
ity of Chinese deep dependency structures demon-
strates the importance of the expressiveness of a
transition system, while the improved numeric ac-
curacies practically certify the benefits. The two
points merit further exploration to more expressive
transition systems for deep dependency parsing, at
least for Chinese. The labeled evaluation scores
on the final test data are presented in Tab. 4.
Test UP UR UF LR
L
LR
NL
S
5
83.93 79.82 81.82 80.94 54.38
Table 4: Performance on the test data.
4.3 Precision vs. Recall
A noteworthy thing about the overall performance
is that the precision is promising but the recall is
too low behind. This difference is consistent with
the result obtained by a shift-reduce CCG parser
(Zhang and Clark, 2011). The functor-argument
dependencies generated by that parser also has a
relatively high precision but considerably low re-
call. There are two similarities between our parser
and theirs: 1) both parsers produce dependency
graphs rather trees; 2) both parser employ a beam
decoder that does not guarantee global optimality.
To build NLP application, e.g. information extrac-
tion, systems upon GR parsing, such property mer-
its attention. A good trade-off between the preci-
sion and the recall may have a great impact on final
results.
453
4.4 Local vs. Non-local
Although the micro accuracy of all dependencies
are considerably good, the ability of current state-
of-the-art statistical parsers to find difficult non-
local materials is far from satisfactory, even for
English (Rimell et al, 2009; Bender et al, 2011).
We report the accuracy in terms of local and non-
local dependencies respectively to show the diffi-
culty of the recovery of non-local dependencies.
The last four columns of Tab. 3 demonstrates the
labeled/unlabeled recall of local (UR
L
/LR
L
) and
non-local dependencies (UR
NL
/LR
NL
). We can
clearly see that non-local dependency recovery is
extremely difficult for Chinese parsing.
4.5 Deep vs. Deep
CCG and HPSG parsers also favor the dependency-
based metrics for evaluation (Clark and Curran,
2007b; Miyao and Tsujii, 2008). Previous work
on Chinese CCG and HPSG parsing unanimously
agrees that obtaining the deep analysis of Chinese
is more challenging (Yu et al, 2011; Tse and Cur-
ran, 2012). The successful C&C and Enju parsers
provide very inaccurate results for Chinese texts.
Though the numbers profiling the qualities of deep
dependency structures under different formalisms
are not directly comparable, all empirical eval-
uation indicates that the state-of-the-art of deep
linguistic processing for Chinese lag behind very
much.
5 Related Work
Wide-coverage in-depth and accurate linguistic
processing is desirable for many practical NLP ap-
plications, such as machine translation (Wu et al,
2010) and information extraction (Miyao et al,
2008). Parsing in deep formalisms, e.g. CCG,
HPSG, LFG and TAG, provides valuable, richer
linguistic information, and researchers thus draw
more and more attention to it. Very recently, study
on deep linguistic processing for Chinese has been
initialized. Our work is one of them.
To quickly construct deep annotations, corpus-
driven grammar engineering has been studied.
Phrase structure trees in CTB have been semi-
automatically converted to deep derivations in the
CCG (Tse and Curran, 2010), LFG (Guo et al,
2007), TAG (Xia, 2001) and HPSG (Yu et al,
2010) formalisms. Our GR extraction work is sim-
ilar, but grounded in GB, which is more consistent
with the construction of the original annotations.
Based on converted fine-grained linguistic an-
notations, successful English deep parsers, such as
C&C (Clark and Curran, 2007b) and Enju (Miyao
and Tsujii, 2008), have been evaluated (Yu et al,
2011; Tse and Curran, 2012). We also borrow
many ideas from recent advances in deep syntac-
tic or semantic parsing for English. In particular,
Sagae and Tsujii (2008)?s and Titov et al (2009)?s
studies on transition-based deep dependency pars-
ing motivated our work very much. However, sim-
ple adoption of their systems does not resolve Chi-
nese GR parsing well because the GR graphs are
much more complicated. Our investigation on the
K-permutation transition system advances the ca-
pacity of existing methods.
6 Conclusion
Recent years witnessed rapid progress made on
deep linguistic processing for English, and ini-
tial attempts for Chinese. Our work stands in
between traditional dependency tree parsing and
deep linguistic processing. We introduced a sys-
tem for automatically extracting grammatical rela-
tions of Chinese sentences from GB phrase struc-
ture trees. The present work remedies the re-
source gap by facilitating the accurate extraction
of GR annotations from GB trees. Manual evalua-
tion demonstrate the effectiveness of our method.
With the availability of high-quality GR resources,
transition-based methods for GR parsing was stud-
ied. A new formal system, namely K-permutation
system, is well theoretically discussed and prac-
tically implemented as the core module of a deep
dependency parser. Empirical evaluation and anal-
ysis were presented to give better understanding of
the Chinese GR parsing problem. Detailed anal-
ysis reveals some important directions for future
investigation.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
References
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 397?408.
Association for Computational Linguistics, Edinburgh,
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1037.
454
J. Bresnan and R. M. Kaplan. 1982. Introduction: Grammars
as mental representations of language. In J. Bresnan, edi-
tor, The Mental Representation of Grammatical Relations,
pages xvii?lii. MIT Press, Cambridge, MA.
Ted Briscoe and John Carroll. 2006. Evaluating the ac-
curacy of an unlexicalized statistical parser on the parc
depbank. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 41?48. Associ-
ation for Computational Linguistics, Sydney, Australia.
URL http://www.aclweb.org/anthology/P/
P06/P06-2006.
Andrew Carnie. 2007. Syntax: A Generative Introduction.
Blackwell Publishing, Blackwell Publishing 350 Main
Street, Malden, MA 02148-5020, USA, second edition.
Noam Chomsky. 1981. Lectures on Government and Binding.
Foris Publications, Dordecht.
Stephen Clark and James Curran. 2007a. Formalism-
independent parser evaluation with ccg and depbank. In
Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 248?255. Associa-
tion for Computational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/anthology/
P07-1032.
Stephen Clark and James R. Curran. 2007b. Wide-coverage
efficient statistical parsing with CCG and log-linear mod-
els. Comput. Linguist., 33(4):493?552. URL http://
dx.doi.org/10.1162/coli.2007.33.4.493.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JOURNAL OF MACHINE
LEARNING RESEARCH, 7:551?585.
M. Dalrymple. 2001. Lexical-Functional Grammar, vol-
ume 34 of Syntax and Semantics. Academic Press, New
York.
Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2007.
Treebank-based acquisition of lfg resources for Chinese.
In Proceedings of the LFG07 Conference. CSLI Publica-
tions, California, USA.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
corpus of CCG derivations and dependency structures ex-
tracted from the penn treebank. Computational Linguis-
tics, 33(3):355?396.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic pars-
ing. In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages 97?
104. Association for Computational Linguistics, Boston,
Massachusetts, USA.
Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700
dependency bank. In In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1?8.
Terry Koo and Michael Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 1?11. Association for Computational Linguistics,
Uppsala, Sweden. URL http://www.aclweb.org/
anthology/P10-1001.
Ryan McDonald. 2006. Discriminative learning and span-
ning tree algorithms for dependency parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented evalu-
ation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54. Associ-
ation for Computational Linguistics, Columbus, Ohio.
URL http://www.aclweb.org/anthology/P/
P08/P08-1006.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007. To-
wards framework-independent evaluation of deep linguis-
tic parsers. In Ann Copestake, editor, Proceedings of
the GEAF 2007 Workshop, CSLI Studies in Computa-
tional Linguistics Online, page 21 pages. CSLI Publica-
tions. URL http://www.cs.cmu.edu/
?
sagae/
docs/geaf07miyaoetal.pdf.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Comput. Lin-
guist., 34(1):35?80. URL http://dx.doi.org/10.
1162/coli.2008.34.1.35.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Comput. Linguist., 34:513?
553. URL http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 351?
359. Association for Computational Linguistics, Sun-
tec, Singapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1040.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-
based dependency parsing. In Hwee Tou Ng and Ellen
Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Con-
ference on Computational Natural Language Learning
(CoNLL-2004), pages 49?56. Association for Computa-
tional Linguistics, Boston, Massachusetts, USA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The University of Chicago Press,
Chicago.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Un-
bounded dependency recovery for parser evaluation. In
Proceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 813?821.
Association for Computational Linguistics, Singapore.
URL http://www.aclweb.org/anthology/D/
D09/D09-1085.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguistics,
pages 753?760. Coling 2008 Organizing Committee,
Manchester, UK. URL http://www.aclweb.org/
anthology/C08-1095.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing paradig-
matic and syntagmatic lexical relations: Towards accu-
rate Chinese part-of-speech tagging. In Proceedings of the
50th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics.
Weiwei Sun and Xiaojun Wan. 2013. Data-driven, pcfg-based
and pseudo-pcfg-based models for Chinese dependency
parsing. Transactions of the Association for Computa-
tional Linguistics (TACL).
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?979.
Association for Computational Linguistics, Edinburgh,
455
Scotland, UK. URL http://www.aclweb.org/
anthology/D11-1090.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of the 21st international jont con-
ference on Artifical intelligence, pages 1562?1567. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, USA.
URL http://dl.acm.org/citation.cfm?id=
1661445.1661696.
Andre Torres Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350. Asso-
ciation for Computational Linguistics, Suntec, Singapore.
URL http://www.aclweb.org/anthology/P/
P09/P09-1039.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the penn Chi-
nese treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling 2010),
pages 1083?1091. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-1122.
Daniel Tse and James R. Curran. 2012. The challenges
of parsing Chinese with combinatory categorial gram-
mar. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
pages 295?304. Association for Computational Linguis-
tics, Montr?eal, Canada. URL http://www.aclweb.
org/anthology/N12-1030.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages
325?334. Association for Computational Linguistics, Up-
psala, Sweden. URL http://www.aclweb.org/
anthology/P10-1034.
Fei Xia. 2001. Automatic grammar generation from two dif-
ferent perspectives. Ph.D. thesis, University of Pennsylva-
nia.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The penn Chinese treebank: Phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11:207?238. URL http://portal.acm.org/
citation.cfm?id=1064781.1064785.
Nianwen Xue. 2007. Tapping the implicit information for the
PS to DS conversion of the Chinese treebank. In Proceed-
ings of the Sixth International Workshop on Treebanks and
Linguistics Theories.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
The 8th International Workshop of Parsing Technologies
(IWPT2003), pages 195?206.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang,
and Junichi Tsujii. 2011. Analysis of the difficul-
ties in Chinese deep parsing. In Proceedings of the
12th International Conference on Parsing Technologies,
pages 48?57. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.org/
anthology/W11-2907.
Kun Yu, Miyao Yusuke, Xiangli Wang, Takuya Matsuzaki,
and Junichi Tsujii. 2010. Semi-automatically devel-
oping Chinese hpsg grammar from the penn Chinese
treebank for deep parsing. In Coling 2010: Posters,
pages 1417?1425. Coling 2010 Organizing Committee,
Beijing, China. URL http://www.aclweb.org/
anthology/C10-2162.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global discrim-
inative model. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT?09),
pages 162?171. Association for Computational Linguis-
tics, Paris, France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 683?692.
Association for Computational Linguistics, Portland,
Oregon, USA. URL http://www.aclweb.org/
anthology/P11-1069.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies, pages 188?193. Association for Computational Lin-
guistics, Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
456
Data-driven, PCFG-based and Pseudo-PCFG-based Models for Chinese
Dependency Parsing
Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,wanxiaojun}@pku.edu.cn
Abstract
We present a comparative study of transition-,
graph- and PCFG-based models aimed at il-
luminating more precisely the likely contri-
bution of CFGs in improving Chinese depen-
dency parsing accuracy, especially by com-
bining heterogeneous models. Inspired by
the impact of a constituency grammar on de-
pendency parsing, we propose several strate-
gies to acquire pseudo CFGs only from de-
pendency annotations. Compared to linguistic
grammars learned from rich phrase-structure
treebanks, well designed pseudo grammars
achieve similar parsing accuracy and have
equivalent contributions to parser ensemble.
Moreover, pseudo grammars increase the di-
versity of base models; therefore, together
with all other models, further improve sys-
tem combination. Based on automatic POS
tagging, our final model achieves a UAS of
87.23%, resulting in a significant improve-
ment of the state of the art.
1 Introduction
Popular approaches to dependency parsing can
be divided into two classes: grammar-free and
grammar-based. Data-driven, grammar-free ap-
proaches make essential use of machine learning
from linguistic annotations in order to parse new
sentences. Such approaches, e.g. transition-based
(Nivre, 2008) and graph-based (McDonald, 2006;
Torres Martins et al, 2009) have attracted the most
attention in recent years. In contrast, grammar-
based approaches rely on linguistic grammars (in
either dependency or constituency formalisms) to
shape the search space for possible syntactic anal-
ysis. In particular, CFG-based dependency parsing
exploits a mapping between dependency and con-
stituency representations and reuses parsing algo-
rithms developed for CFG to produce dependency
structures. In previous work, data-driven, discrim-
inative approaches have been widely discussed for
Chinese dependency parsing. On the other hand,
various PCFG-based constituent parsing methods
have been applied to obtain phrase-structures as
well. With rich linguistic rules, phrase-structures of
Chinese sentences can be well transformed to their
corresponding dependency structures (Xue, 2007).
Therefore, PCFG parsers with such conversion rules
can be taken as another type of dependency parser.
We call them PCFG-based parsers, in this paper.
Explicitly defining linguistic rules to express
precisely generic grammatical regularities, a con-
stituency grammar can be applied to arrange sen-
tences into a hierarchy of nested phrases, which de-
termines constructions between larger phrases and
their smaller component phrases. This type of infor-
mation is different from, but highly related to, the
information captured by a dependency representa-
tion. A constituency grammar, thus, has great possi-
ble contributions to dependency parsing. In order
to pave the way for new and better methods, we
study the impact of CFGs on Chinese dependency
parsing. A series of empirical analysis of state-of-
the-art graph-, transition-and PCFG-based parsers is
presented to illuminate more precisely the properties
of heterogeneous models. We show that CFGs have
a great impact on dependency parsing and PCFG-
based models have complementary predictive pow-
ers to data-driven models.
System ensemble is an effective and important
technique to build more accurate parsers based on
multiple, diverse, weaker models. Exploiting differ-
301
Transactions of the Association for Computational Linguistics, 1 (2013) 301?314. Action Editor: Jason Eisner.
Submitted 6/2012; Revised 10/2012; Published 7/2013. c?2013 Association for Computational Linguistics.
ent data-driven models, e.g. transition- and graph-
based models, has received the most attention in
dependency parser ensemble (Nivre and McDon-
ald, 2008; Torres Martins et al, 2008; Sagae and
Lavie, 2006). Only a few works investigate inte-
grating data-driven and PCFG-based models (Mc-
Donald, 2006). We argue that grammars can signif-
icantly increase the diversity of base models, which
plays a central role in parser ensemble, and therefore
lead to better and more promising hybrid systems.
We introduce a general classifier enhancing tech-
nique, i.e. bootstrap aggregating (Bagging), to im-
prove dependency parsing accuracy. This technique
can be applied to enhance a single-view parser, or
to combine multiple heterogeneous parsers. Exper-
iments on the CoNLL 09 shared task data demon-
strate its effectiveness: (1) Bagging can improve in-
dividual single-view parsers, especially the PCFG-
based one; (2) Bagging is more effective than pre-
viously introduced ensemble methods to combine
multi-view parsers; (3) Integrating data-driven and
PCFG-based models is more useful than combining
different data-driven models.
Although PCFG-based models have a big con-
tribution to data-driven dependency parsing, they
have a serious limitation: There are no corre-
sponding constituency annotations for some depen-
dency treebanks, e.g. Chinese Dependency Tree-
bank (LDC2012T05). To overcome this limita-
tion, we propose several strategies to acquire pseudo
grammars only from dependency annotations. In
particular, dependency trees are converted to pseudo
constituency trees and PCFGs can be extracted from
such trees. Another motivation of this study is to in-
crease the diversity of candidate models for parser
ensemble. Experiments show that pseudo-PCFG-
based models are very competitive: (1) Pseudo
grammars achieve similar or even better parsing re-
sults than linguistic grammars learned from rich
constituency annotations; (2) Compared to linguistic
grammars, well designed, single-view pseudo gram-
mars have an equivalent contribution to parser en-
semble; (3) Combining different pseudo grammars
even work better for ensemble than linguistic gram-
mars; (4) Pseudo-PCFG-based models increase the
diversity of base models, and therefore lead to fur-
ther improvements for ensemble.
Based on automatic POS tagging, our final model
achieves a UAS of 87.23% on the CoNLL data and
84.65% on CTB5, which yield relative error reduc-
tions of 18-24% over the best published results in
the literature.
2 Background and related work
2.1 Data-driven dependency parsing
The mainstream work on recent dependency pars-
ing focuses on data-driven approaches that automat-
ically learn to produce dependency graphs for sen-
tences solely from a hand-crafted dependency tree-
bank. The advantage of such models is that they
are easily ported to any language in which labeled
linguistic resources exist. Practically all statisti-
cal models that have been proposed in recent years
can be mainly described as either graph-based or
transition-based (McDonald and Nivre, 2007). Both
models have been adopted to learn Chinese depen-
dency structures (Zhang and Clark, 2011; Zhang
and Nivre, 2011; Huang and Sagae, 2010; Hatori
et al, 2011; Li et al, 2011, 2012). According to
published results, graph-based and transition-based
parsers achieve similar accuracy.
In the graph-based framework, informative evalu-
ation results have been presented in (Li et al, 2011).
First, second and third order projective parsing mod-
els are well evaluated. In the transition-based frame-
work, two advanced techniques have been stud-
ied. First, developing features has been shown
crucial to advancing parsing accuracy and a very
rich feature set is carefully evaluated by Zhang and
Nivre (2011). Second, beyond deterministic greedy
search, principled dynamic programming strategies
can be employed to explore more possible hypothe-
ses (Huang and Sagae, 2010). Both techniques have
been examined and shown helpful for Chinese de-
pendency parsing. Furthermore, Hatori et al (2011)
combined both and obtained a state-of-the-art super-
vised parsing result.
2.2 PCFG-based dependency parsing
PCFG-based dependency parsing approaches are
based on the finding that projective dependency trees
can be transformed from constituency trees by ap-
plying rich linguistic rules. In such approaches, de-
pendency parsing can be resolved by a two-step pro-
cess: constituent parsing and rule-based extraction
302
of dependencies from phrase structures. The ad-
vantage of constituency-grammar-based approach is
that all the well-studied parsing methods for such
grammars can be used for dependency parsing as
well. Two language-specific properties essentially
make PCFG-based approaches easy to be applied
to Chinese dependency parsing: (1) Chinese gram-
maticians favor using projective structures;1 (2) Chi-
nese phrase-structure annotations normally contain
richer information and thus are reliable for tree con-
version.
2.2.1 Constituency parsing
Compared to many other languages, statistical
constituent parsing for Chinese has reached early
success, due to the fact that the language has rela-
tively fixed word order and extremely poor inflec-
tional morphology. Both facts allow PCFG-based
statistical modeling to perform well. For the con-
stituent parsing, the majority of the state-of-the-
art parsers are based on generative PCFG learn-
ing. For example, the well-known and success-
ful Collins and Charniak&Johnson parsers (Collins,
2003; Charniak, 2000; Charniak and Johnson, 2005)
implement generative lexicalized statistical models.
Apart from lexicalized PCFG parsing, unlex-
icalized parsing with latent variable grammars
(PCFGLA) can also produce comparable accuracy
(Matsuzaki et al, 2005; Petrov et al, 2006). Latent
variable grammars model an observed treebank of
coarse parse trees with a model over more refined,
but unobserved, derivation trees that represent much
more complex syntactic processes. Rather than
attempting to manually specify fine-grained cate-
gories, previous work shows that automatically in-
ducing the sub-categories from data can work quite
well. A PCFGLA parser leverages on an automatic
procedure to learn refined grammars and are there-
fore more robust to parse non-English languages that
are not well studied. For Chinese, such a parser
achieves the state-of-the-art performance and de-
feats many other types of parsers, including Collins
as well as Charniak parser (Che et al, 2012) and
1For example, as two popular dependency treebanks, the
CoNLL 2009 data and the Chinese Dependency Treebank both
excluede non-projective annotations. It is worth noting that the
former one is converted from a constituency treebank while the
latter one is directly annotated by lingusitics.
discriminative transition-based models (Zhang and
Clark, 2009).
2.2.2 CS to DS conversion
In the absence of dependency and constituency
structures for a particular treebank, treebank-guided
parser developers normally apply rich linguistic
rules to convert one representation formalism to an-
other to get necessary data to train parsers. Xue
(2007) examines the linguistic adequacy of depen-
dency structure annotation automatically converted
from phrase structure treebanks with rule-based ap-
proaches. A structural approach is introduced for
the constituency structure (CS) to dependency struc-
ture (DS) conversion for the Chinese Treebank data,
which is the basis of the CoNLL 2009 shared task
data. By applying this conversion procedure on the
outputs of an automatic phrase structure parser, we
can build a PCFG-based dependency parser.
2.3 Parser ensemble
NLP systems built on particular single views nor-
mally capture different properties of an original
problem, and therefore differ in predictive powers.
As a result, NLP systems can take advantage of com-
plementary strengths of multiple views. Combining
the outputs of several systems has been shown in the
past to improve parsing performance significantly,
including integrating phrase-structure parsers (Hen-
derson and Brill, 1999), dependency parsers (Nivre
and McDonald, 2008), or both (McDonald, 2006).
Several ensemble models have been proposed for
the parsing of syntactic constituents and dependen-
cies, including learning-based stacking (Nivre and
McDonald, 2008; Torres Martins et al, 2008) and
learning-free post-inference (Henderson and Brill,
1999; Sagae and Lavie, 2006). Surdeanu and Man-
ning (2010) present a systematic analysis of these
ensemble methods and find several non-obvious
facts:
? the diversity of base parsers is more important
than complex models for learning, and
? simplest scoring model for voting and repars-
ing performs essentially as well as other more
complex models.
303
3 A comparative analysis of heterogeneous
parsers
The information encoded in a dependency repre-
sentation is different from the information captured
in a constituency representation. While the depen-
dency structure represents head-dependent relations
between words, the constituency structure repre-
sents the grouping of words into phrases, classified
by structural categories. These differences concern
what is explicitly encoded in the respective represen-
tations, and affects data-driven and PCFG-based de-
pendency parsing models substantially. In this sec-
tion, we give a comparative analysis of transition-,
graph- and PCFG-based models aimed at illuminat-
ing more precisely the likely contribution of CFGs
in dependency parsing.
3.1 Experimental setup
Penn Chinese TreeBank (CTB) is a segmented,
POS tagged, and fully bracketed corpus in the con-
stituency formalism, and very popular to evaluate
fundamental NLP tasks, including word segmenta-
tion, POS tagging, constituent parsing as well as de-
pendency parsing. We use CTB 6 as our main corpus
and define the training, development and test sets ac-
cording to the CoNLL 2009 shared task. To evaluate
and analyze dependency parsers, we directly use the
CoNLL data. CTB?s syntactic annotations also in-
cludes functional information and empty categories.
Modern parsers, e.g. Collins and Berkeley parsers,
ignore these types of linguistic knowledge. To train
a constituent parser, we perform a heuristic proce-
dure on the treebank data to delete function tags and
empty categories as well as its associated redundant
ancestors. Many papers reported parsing results of
an older version CTB (namely CTB 5). To compare
with systems introduced in these papers, we evaluate
our final ensemble model on CTB5 in Section 5.4.
For dependency parsing, we choose a second
order graph-based parser2 (Bohnet, 2010) and a
transition-based parser (Hatori et al, 2011), for
experiments. For constituent parsing, we choose
Berkeley parser,3 a well known implementation of
the unlexicalized PCFGLA model and Bikel parser,4
2code.google.com/p/mate-tools/
3code.google.com/p/berkeleyparser/
4cis.upenn.edu/?dbikel/software.html
a well known implementation of Collins? lexical-
ized model, for experiments. In data-driven pars-
ing, features consisting of POS tags are very effec-
tive, so typically POS tagging is performed as a pre-
processing. We use the baseline sequential tagger
described in (Sun and Uszkoreit, 2012) to provide
such lexical information to the graph-based parser.
Note that the transition-based parser performs a joint
inference to acquire POS and dependency informa-
tion simultaneously, so there is no need to offer extra
tagging results to it.
3.2 Overall performance
Table 1 (Column 2-6) summarizes the overall accu-
racy of different parsers. Two transition-based pars-
ing results are presented: The first one employ a
simple feature set (Zhang and Clark, 2008) and a
small beam (16); the second one employ rich fea-
tures (Zhang and Nivre, 2011) and a larger beam
(32). Two graph-based parsing results are reported;
the difference between them is whether integrate re-
lation labels into the parsing procedure. Roughly
speaking, currently state-of-the-art data-driven mod-
els achieves slightly better precision than unlexical-
ized PCFG-based models with regard to unlabeled
dependency prediction.
There is a big gap between lexicalized and unlexi-
calized parsing. The same phenomenon has been ob-
served by (Che et al, 2012) and (Zhuang and Zong,
2010). In addition to dependency parsing, Zhuang
and Zong (2010) found that Berkeley parser pro-
duce much more accurate syntactic analyses to assist
a Chinese semantic role labeler than Bikel parser.
Charniak and Stanford parsers are two other well-
known and frequently used tools that can provide
lexicalized parsing results. According to (Che et al,
2012), they perform even worse than Bikel parser,
at least for Stanford dependencies. Due to the poor
parsing performance, we only concentrate on the un-
lexicalized model in the remainder of this paper.
The performance of labeled dependency predic-
tion of the unlexicalized PCFG-based parser is much
lower. We can learn that the CS to DS conversion is
not robust to assign functional categories to depen-
dencies and simple linguistic rules are not capable
to do fine-grained classification. Previous research
on English indicates that the main difficulty in de-
pendency parsing is the prediction of dependency
304
Devel. UAS LAS Compl. Fsib Fgrd
Tran[b=16,Z08] 82.80 N/A 29.00 66.55 79.74
Tran[b=32,Z11] 83.80 N/A 31.61 68.58 80.87
Graph[-lab] 83.66 N/A 29.28 67.96 80.82
Graph[+lab] 84.24 80.55 30.99 69.11 81.38
Unlex 82.86 67.44 27.98 69.07 81.22
Lex 70.38 58.10 - - - - - -
Bagging(15)
Tran[b=16,Z08] 83.25 N/A 28.66 67.17 78.89
Tran[b=32,Z11] 84.25 N/A 31.21 69.14 81.49
Graph[-lab] 83.81 N/A 29.68 68.00 80.62
Graph[+lab] 84.50 N/A 31.44 69.48 81.10
Unlex 84.92 N/A 32.35 71.08 83.66
Bagging(8)
Unlex 84.35 N/A 31.16 70.49 83.57
Table 1: Accuracy of different parsers. The first block
presents baseline parsers; the last two blocks present
Bagging-enhanced parsers, where m is respectively set to
15 and 8. Z08 and Z11 distinguish different feature sets;
b=16 and b=32 are beam sizes. +/-lab means whether to
incorporate relation labels to a model.
structures, and an extra statistical classifier can be
employed to label automatically recognized depen-
dencies with a high accuracy. Although this issue
is not well studied for Chinese dependency parsing,
previous research on function tag labeling (Sun and
Sui, 2009) and semantic role labeling (Sun, 2010a)
gives us some clues. Their research shows that both
functional and predicate-argument structural infor-
mation is relatively easy to predict if high-quality
syntactic parses are available. We mainly focus on
the UAS metric in the following experiments.
3.3 Constraints
A grammar-based model utilizes an explicitly de-
fined formal grammar to shape the search space for
possible syntactic hypotheses. Parameters of a sta-
tistical grammar-based model are related to a gram-
mar rule, and as a result specific language construc-
tions are constrained by each other. For example,
parameters are assigned to rewrite rules for a CFG-
based model. Since the PCFG-based model lever-
ages rewrite rules to locally constrain several possi-
ble dependents for one head word, it does relatively
better for locally connected dependencies. The tra-
ditional evaluation metrics, i.e. UAS and LAS, only
consider bi-lexical (first order) dependencies, which
are smallest pieces of a dependency structure. Be-
sides bi-lexical dependencies, we report the predic-
tion accuracy of grandparent and sibling dependen-
cies, i.e. second order dependencies. The metrics
are defined as follows.
? For every word d whose parent is not the root,
we consider the word triple ?d, p, g? among d
and its parent p and grandparent g. A word
triple ?d, p, g? from a predicted tree is consid-
ered as correct if it also apprears in the corre-
sponding gold tree. Based on this definition,
precison, recall and f-score of grandparent de-
pendency can be defined in a normal sense. All
punctuations are excluded for calculation.
? For every word h that governs at least two chil-
dren (d1, ..., dn), we consider every word triple
?h, di, di+1?, among h and its sibling depen-
dents di as well as di+1 (0 ? i < n). Similar
to the grandparent dependencies, we can define
evaluation metrics for sibling dependencies.
From Table 1, we can see that the grammar-based
model parses relatively better for slightly larger frag-
ments. For example, the UAS of the graph-based
model is significantly higher than the grammar-
based one, but their sibling and grandparent scores
are similar. In the next section, we will introduce
a general parser enhancement technique and present
more discussions based on enhanced parsing results
(Column 7-14).
3.4 Endocentric and exocentric constructions
<-NN<- <-NR<- <-NT<- <-PN<- <-VA <- <-VC<- <-VE<- <-VV<-
Unlex 2 7.61 19.3 17.2 5 14.0 9 39.72 45.51 49.83 41.44
G raph[+lab] 2 4.82 17.45 12 .2 12 .1 38.12 49.9 51.18 42 .14
Tran[b=32 ,Z0 8] 2 5.2 5 17.82 15.16 13.48 41.32 47.7 49.83 42 .34
10
2 0
30
40
50
Er
ro
r 
ra
te
 
Figure 1: Nominal vs. verbal constructions.
Arguments in exocentric constructions help com-
plete the meaning of a predicate and are taken to be
obligatory and selected by their heads; adjuncts in
305
endocentric constructions are structurally dispens-
able parts that provide auxiliary information and
taken to be optional and not selected by their heads.
An important annotation policy of the CTB is ?one
grammatical relation per bracket?, which means
each constituent falls into one of the three primitive
grammatical relations: (1) head-complementation,
(2) head-adjunction and (3) coordination. Addi-
tionally, the argument is attached at a level that is
?closer? to the head than the adjuncts. Due to the
linguistic properties of different dependents and the
annotation strategies, a grammar-based model can
capture more syntactic preference properties of ar-
guments via hard constraints, i.e. grammar rules,
and are therefore more suitable to analyze exocen-
tric constructions.
Figure 1 is the error rate of unlabeled dependen-
cies considering different construction. A construc-
tion ?? X ?? is considered as correctly predicted
if and only if all dependent words and head word of
X are completely correctly found. The error rate
in terms of this metric seems rather high because
the units we consider are normally much larger than
word pairs. From this figure, we can clearly see that
the data-driven parser does better for the prediction
of nominal constructions (NN/NR/NT/PN5), which
relate more on optional adjuncts or modifiers; the
grammar-based parser performs better for the pre-
diction of verbal constructions (VC/VE/VV), which
relate more on obligatory arguments. The evalua-
tion of the nominal and verbal constructions roughly
confirms the strength of grammar-based model to
predict verbal constructions.
4 Bagging parsers
The comparative analysis highlights the fundamen-
tal diversity between data-driven and PCFG-based
models. In order to exploit the diversity gain, we ad-
dress the issue of parser combination. We employ
a general ensemble learning technique, i.e. Bag-
ging, to enhance a single-view parser and to com-
bine multi-view parsers.
5For the definition and illustration of these tags, please refer
to the annotation guidelines (http://www.cis.upenn.
edu/?chinese/posguide.3rd.ch.pdf).
4.1 Applying Bagging to dependency parsing
Bagging is a machine learning ensemble meta-
algorithm to improve classification and regression
models in terms of stability and classification accu-
racy (Breiman, 1996). It also reduces variance and
helps to avoid overfitting. Given a training set D of
size n, Bagging generates m new training sets Di
of size n? ? n, by sampling examples from D. m
models are separately learned on the m new train-
ing sets and combined by voting (for classification)
or averaging the output (for regression). Hender-
son and Brill (2000) successfully applied Bagging
to enhance a constituent parser. Moreover, Bagging
has been applied to combine multiple solutions for
Chinese lexical processing (Sun, 2010b; Sun and
Uszkoreit, 2012). In this paper, we apply Bagging
to dependency parsing. Since training even one sin-
gle parser takes hours (if not days), experiments on
Bagging is time-consuming. To save time, we con-
duct data-driven parsing experiments based on sim-
ple configuration. More specifically, the beam size
of the transition-based parser is set to 16, and the
simple feature set is utilized; dependency relations
are not incorporated for the graph-based parser.
Bootrapping step. In the training phase, given a
training set D of size n, our model generates m new
training sets Di of size ?n by sampling uniformly
without replacement. Each Di can be used to train
a single-view parser or multiple parsers according
to different views. Using this strategy, we can get m
weak parsers or km parsers if multiple views are im-
plemented. In the parsing phase, for each sentence,
the (k)m models output (k)m candidate analyses
that are combined in a post-inference procedure.
Aggregating step. Different from classification
problems, simple voting scheme is not suitable for
parsing, which is a typical structured prediction
problem. To aggregate outputs of (k)m sub-models,
a structured inference procedure is needed. Sagae
and Lavie (2006) present a framework for combin-
ing the output of several different parsers to produce
results that are superior to each of the individual
parsers. We implement their method to aggregate
models. Once we have obtained multiple depen-
dency trees respectively from base parsers, we can
build a graph where each word in the sentence is a
306
node. We then create weighted directed edges be-
tween the nodes corresponding to words for which
dependencies are obtained from each of the initial
structures. The weights are the word-by-word voting
results of sub-models. Based on this graph, the sen-
tence can be reparsed by a graph-based algorithm.
Taking Chinese as a projective language, we use Eis-
ner?s algorithm (Eisner, 1996) to combine multiple
dependency parses. Surdeanu and Manning (2010)
indicates that reparsing performs essentially as well
as other simpler or more complex models.
4.2 Parameter tuning
We evaluate our combination model on the same
data set used in the last section. The two hyper-
parameters (? and m) of our Bagging model are
tuned on the development (validation) set. On one
hand, with the increase of the size of sub-samples,
i.e. ?, the performance of sub-models is improved.
However, since the sub-models overlap more, the di-
versity of base models for ensemble will decrease
and the final prediction accuracy may go down. To
evaluate the effect of ?, we separately sample 50%,
60%, 70% and 80% sentences from the original
training data 5 times, train 5 sub-models for each
parser, and combine them together. The beam size
of the transition-based parser is set to 16. Table 2
shows the influence of the choice of ??s. For all fol-
lowing experiments, we set ? = 0.7.
? 50% 60% 70% 80%
Tran+Graph[-lab]+Unlex 83.50 85.96 86.15 85.60
Table 2: UAS of Bagging(5) models with different ?.
The second parameter for Bagging is the number
of sub-models to be used for combination. Figure 2
summarizes the Bagging performance when differ-
ent models are employed and different number (i.e.
m) of subsamples are used. From this figure, we can
learn the influence of the number of sub-models.
4.3 Bagging single-view parsers
4.3.1 Results
Table 1 indicates that Bagging can improve in-
dividual single-view parsers, especially Berkeley
parser. If we take Bagging as a general parser en-
hancement technique and still consider a Bagging-
enhanced parser as a single view, we conclude
81.5
82 .5
83.5
84.5
85.5
86.5
3 4 5 6 7 8 9
G raph[-lab]
Tran
Unlex
G raph[-lab]+Unlex
Tran+Unlex
G raph[-lab]+Tran
Figure 2: Averaged UAS of different Bagging models
with different numbers of sampling data sets.
that Bagging-enhanced PCFG-based method works
best among state-of-the-art approaches. For the
transition-based parser, though the score over single
words goes up, the score over sentences goes down.
The main reason is that the reparsing algorithm is a
graph-based one, which performs worse with regard
to the prediction of a whole sentence. The improve-
ment for the graph-based parser is very modest.
We train a Bagging(8)-enhanced Berkeley parser,
which achieves equivalent overall UAS to data-
driven parsers, and compare their parsing abilities
of second order dependencies. Now we can more
clearly see that the Bagging-enhanced PCFG-based
model performs better in the prediction of second
order dependencies.
4.3.2 Related experiments on sequence models
Bagging has been applied to enhance discrimina-
tive sequence models for Chinese word segmenta-
tion (Sun, 2010b) and POS tagging (Sun and Uszko-
reit, 2012). For word segmentation, experiments
on discriminative Markov and semi-Markov tagging
models are reported. Their experiments showed that
Bagging can consistently enhance a semi-Markov
model but not the Markov one. Experiments on POS
tagging indicated that BaggingMarkov models hurts
tagging performance. It seems that the relationships
among basic processing units affect Bagging.
PCFGLA parsers are built upon generative mod-
els with latent annotations. The use of automati-
cally induced latent variables may also affect Bag-
ging. Generative sequence models with latent anno-
307
tations can also achieve good performance for Chi-
nese POS tagging. Huang et al (2009) described
and evaluated a bi-gram HMM tagger that utilizes
latent annotations. Different from negative results of
Bagging discriminative models, our auxiliary exper-
iment shows that Bagging Huang et al?s tagger can
help Chinese POS tagging. In other words, Bagging
substantially improves both HMMLA and PCFGLA
models, at least for Chinese POS tagging and con-
stituency parsing. It seems that Bagging favors the
use of latent variables.
4.4 Bagging multi-view parsers
4.4.1 Results
Figure 2 clearly shows that the Bagging model
taking both data-driven and PCFG-based models as
basic systems outperform the Bagging model taking
either model in isolation as basic systems. The com-
bination of a PCFG-based model and a data-driven
model (either graph-based or transition-based) is
more effective than the combination of two data-
driven models, which has received the most atten-
tion in dependency parser ensemble. Table 3 is
the performance of reparsing on the development
data. From this table, we can see by utilizing more
parsers, Bagging can enhance reparsing. According
to Surdeanu and Manning (2010)?s findings, repars-
ing performs as well as other combination mod-
els. Our auxiliary experiments confirm this finding:
Learning-based stacking cannot achieve better per-
formance. Limited to the document length, we do
not give descriptions of these experiments.
Devel. UAS
Reparsing(Tran[b=16,Z08]+Graph[-lab]+Unlex) 85.82
+Bagging(15) 86.37
bagging(reparse(g, t, c)) 86.09
reparse(bagging(g, t, c)) 85.86
Table 3: UAS of reparsing and Bagging.
4.4.2 Analysis
In our proposed model, Bagging has a two-fold
effect: One is as a system combination technique
and the other as a general parser enhancing tech-
nique. Two additional experiments are performed
to evaluate these two effects. To illustrate the differ-
ences between these two experiments, respectively
denote graph-based, transition-based and PCFG-
based parsers as g, t and c; denote the reparsing
procedure as reparse and the Bagging procedure as
bagging. The two experiments are as follows.
? Bagging a hybrid parser. In this experiment,
for each sub-sample Di, we first train three
parsers: gi, ti and ci. Then we combine these
three parsers by reparsing and construct a hy-
brid parser reparse(gi, ti, ci). Finally, all hy-
brid parsers are collected to build the final
parser: bagging(reparse(g, t, c)).
? Combining Bagging-enhanced parsers. In
this experiment, for each model, we first train
three Bagging-enhanced parsers: bagging(g),
bagging(t) and bagging(c). Then these
three Bagging-enhanced parsers are com-
bined by reparsing to build the final parser:
reparse(bagging(g, t, c)).
Evaluation results are presented in Table 3.
5 Pseudo-grammar-based models
Although the combination of data-driven and
grammar-based models is very effective, it has a
serious limitation: It is only applicable when con-
stituency annotations are available to learning a
grammar. However, many treebanks, e.g. Chinese
Dependency Treebank (LDC2012T05), do not have
such linguistically rich structures. Our experiments
also suggest that a constituency grammar can sig-
nificantly increase the diversity of base models for
parser ensemble, which plays a major role in boost-
ing prediction accuracy.
In order to reduce the need for phrase-structure
annotations, and to increase the diversity of candi-
date parsers, we study learning pseudo grammars
for dependency parsing. The key idea is very sim-
ple: By converting a dependency structure to a
constituency one, we can reuse the PCFGLA ap-
proach to learn pseudo grammars for dependency
parsing. Figure 3 is an example. The first tree is
an original dependency parse, while the second tree
is the corresponding CTB annotation. The next two
trees are two automatically converted pseudo con-
stituency trees. By applying DS to CS rules, we
can acquire pseudo constituency treebanks and then
learn pseudo grammars from them.
308
(1) Dependency tree (2) Linguistic constituency tree
(3) Flat constituency tree (4) Binarized constituency tree
Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure.
The basic idea of our method is to use parsing
models in one formalism for parsing in another for-
malism. In previous work, PCFGs are used to solve
parsing problems in many other formalisms, includ-
ing dependency (Collins et al, 1999), CCG (Fowler
and Penn, 2010), LFG (Cahill et al, 2004) and HPSG
(Zhang and Krieger, 2011) parsing.
5.1 Strategies for DS to CS conversion
The conversion from DS to CS is a non-trivial prob-
lem. One main issue in the conversion is the indeter-
minancy in the choice of a phrasal category given a
dependency relation, the level and position of attach-
ment of a dependent in the constituency structure, as
dependency relations typically do not encode such
information. To convert a DS to a CS, especially
for dependency parsing, we should consider (1) how
to transform between the topological structures, (2)
how to induce a syntactic category, and (3) how to
easily recover dependency trees from pseudo con-
stituency trees. From these three aspects, we present
the following strategies.
5.1.1 Topological structure
The topological structures represent the boundary
information of constituents in a given sentence. De-
pendency structures do not directly represent such
boundary information. Nevertheless, a complete
subtree in a projective dependency tree should be
considered as a constituent. We can construct a very
flat constituent tree, of which nodes are associated
with complete subtrees of a dependency parse. The
third tree in Figure 3 is an example of such conver-
sion.
Right-to-left binarization According to the study
in (Sun, 2010a), head words of most phrases in
Chinese are located at the first or the last position.
That means for binarizing most phrases, we only
need sequentially combine the right or left parts to-
gether with their head phrases. Main exceptions are
clauses, of which the head predicate locates inside,
since Chinese is an SVO language. To deal with
these exceptions, we split each phrase whose head
child is inside itself into three parts: left child(ren),
head and right child(ren). We first sequentially com-
bine the head and its right child(ren) that are usu-
ally objects as intermediate phrases, then sequen-
tially combine the left child(ren) until reach the orig-
inal parent node. For example, the first rewrite rule
in follows should be transferred into the second and
third types of rules.
1. Xp ? X1, ..., Xi, ..., Xm
309
2. X?p ? Xi, Xi+1; Xp?? ? Xp?, Xi+2; ...
3. X?p ? Xi?1, Xp?...?; X??p ? Xi?2, X?p; ...
This right-to-left binarization strategy is consistent
with most Chinese treebank annotation schemes.
The fourth tree in Figure 3 is an example of bina-
rized pseudo tree.
5.1.2 Phrasal category
Projection principle is introduced by Chomsky
to link together the levels of syntactic description.
It connects syntactic structures with lexical entries:
Lexical structure must be represented categorically
at every syntactic level, and representations at each
level of syntax are projected from the lexicon in that
they observe the subcategorisation properties of lex-
ical items. According to this principle, it is reason-
able to use the lexical category (POS) of the head
word as the phrasal category of a phrase.
5.1.3 Auxiliary symbol
We can use auxiliary symbols to denote the head
phrase position in a CFG rule. In other words, some
categories may be splitted into subcategories accord-
ing to if they are head phrases of their parent nodes
or which children are their head phrases. Auxiliary
symbols could be either assigned to one of the right
hand side or the left hand side. The first choice is to
conveniently use a H symbol to indicate that current
phrase is the head of its parent node. The second
choice is to practically use an L or R symbol to indi-
cate the head of current node is its left or right child,
in a binarized tree. The following table gives an ex-
ample of different rules with auxiliary symbols.
With head symbol With left/right symbol
Xl ? Xl#H, Xr Xl#L? Xl, Xr
Xr ? Xl, Xr#H Xr#R? Xl, Xr
5.2 Three conversions
Taking into account the above strategies, we propose
three concrete DS to CS conversions:
Flat conversion with H auxiliary symbol (FlatH).
Just as shown as the third tree in Figure 3, we can
learn a grammar from very flat constituency trees
where the auxiliary symbol H is used for extracting
dependencies.
Right-to-left binarizing with H auxiliary symbol
(BinH). Different from the flat conversion, we bi-
narize a tree according to the right-to-left principle.
Auxiliary symbol H is chosen.
Right-to-left binarizing with LR auxiliary sym-
bol (BinLR). Different from the second type of
conversion, we use auxiliary L/R symbols to denote
head phrases. See the fourth tree in Figure 3 for in-
stance.
Practically, every constituency parse that is pro-
duced by parsers trained with binarized trees exactly
maps to one dependency tree. However, the parser
trained with flat trees may produce very bad con-
stituency results. Sometimes, one parent node may
have zero child that is assigned with H or more than
one children that are are assigned H. In the first case,
we select the right most child as the head of such
parent, while in the second case, we select the right
most one from the children that are assigned H.
5.3 Evaluation
5.3.1 Equivalent parsing accuracy
Devel. Base Bagging(15)
CTB 83.49% 84.92%
FlatH 80.15% 83.53%
BinH 81.80% 84.64%
BinLR 82.46% 84.90%
Table 4: UAS of pseudo-grammar-based models.
Table 4 summarizes the performance of differ-
ent pseudo-grammar-based models. Compared to
the linguistic grammar learned from CTB, we can
see that pseudo grammars are very competitive.
Not that, the FlatH/BinH/BinLR trees are derived
from the CoNLL data, rather than the original CTB.
Among different DS to CS conversion strategies, the
BinLR conversion works best. More interestingly,
when we enhance the PCFGLA method by using
Bagging, the BinLR model performs as well as the
real-grammar-based model.
5.3.2 Better contribution to ensemble
The experiments above indicate that we can eas-
ily build good grammar-based dependency parser
without any constituency annotations. The fol-
lowing experiments on parser combination show
that compared to the linguistic grammar, binH and
310
Devel. UAS
Tran+Graph+CTB 86.37%
Tran+Graph+FlatH 86.14%
Tran+Graph+BinH 86.29%
Tran+Graph+BinLR 86.28%
Tran+Graph+flat+BinH+BinLR 87.03%
Tran+Graph+CTB+FlatH 86.96%
Tran+Graph+CTB+BinH 87.10%
Tran+Graph+CTB+BinLR 87.15%
Tran+Graph+CTB+BinH+BinLR 87.38%
Tran+Graph+CTB+FlatH+BinH+BinLR 87.35%
Table 5: UAS of different Bagging(15) models.
binLR grammars have equivalent contributions to
parser ensemble. Table 5 presents the ensem-
ble performance on the development data. By
Bagging, the data-driven models together with ei-
ther real grammar-based or pseudo-grammar-based
model reach a similar UAS.
5.3.3 Increased parser diversity
Since pseudo grammars are very different from
real grammars that are induced from large-scale lin-
guistic annotations. Pseudo-grammar-based parsing
models behave very differently with grammar-based
models. In other words, they increase the diver-
sity of model candidates for parser ensemble. As
a result, pseudo-grammar-based models lead to fur-
ther improvements for parser combination. Table 5
shows that the combination of data-driven, PCFG-
based and binarized pseudo-grammar-based models
is significantly better than the combination of data-
driven and PCFG-based models.
5.4 Comparison to the state-of-the-art
Table 6 summarizes the parsing performance on the
test data set, as well as the best published result re-
ported in Li et al (2012). To fairly compare the per-
formance of our parser and other systems which are
built without linguistic constituency trees, we only
use pseudo-PCFGs in this experiment. Based on
automatic POS tagging, our final model achieves a
UAS of 87.23%, which yields a relative error reduc-
tion of 24% over the best published result. Table
6 also presents the results evaluated on the CTB5
data that is more widely used for previous research.
Li et al (2011) and Hatori et al (2011) respec-
tively evaluated their graph-based and transition-
based parsers; Zhang and Clark (2011) evaluated
CoNLL-test UAS
(Li et al, 2012) 83.23%
Graph+Tran+FlatH+BinH+BinLR 87.23%
CTB5-test UAS
(Li et al, 2011) 80.79%
(Hatori et al, 2011) 81.33%
(Zhang and Clark, 2011) 81.21%
Graph+Tran+FlatH+BinH+BinLR 84.65%
Table 6: UAS of different models on the test data.
a hybrid data-driven parser. Our model is signifi-
cantly better than these systems: It achieves a UAS
of 84.65%, which obtains an error reduction of 18%
over the best system in the literature.
6 Conclusion and Future Work
There have been several attempts to develop high
accuracy parsers in both constituency and depen-
dency formalisms for Chinese, and many successful
parsing algorithms designed for English have been
applied. However, the state-of-the-art still falls far
short when compared to English. This paper stud-
ies data-driven and PCFG-based models for Chinese
dependency parsing. We present a comparative anal-
ysis of transition-, graph-, and PCFG-based parsers,
which highlights the systematic differences between
data-driven and PCFG-based models. Our analysis
may benefit parser ensemble, parser co-training, ac-
tive learning for treebank construction, and so on.
In order to exploit the diversity gain, we address
the issue of parser combination. To overcome the
limitation of the lack of constituency treebanks, we
study pseudo-grammar-based models. Experimental
results show that combining various data-driven and
PCFG-based models significantly advance the state-
of-the-art, and by converting parse trees, we can still
take advantages of the constituency representation
even without constituency annotations.
Acknowledgement
We would like to thank thank all anonymous review-
ers whose valuable comments led to signilicant re-
visions. The first author would like to thank Prof.
Hans Uszkoreit for discussion and feedback of an
early version of this work.
The work was supported by NSFC (61170166),
Beijing Nova Program (2008B03) and National
High-Tech R&D Program (2012AA011101).
311
References
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages
89?97. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1011.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef Van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically
acquired wide-coverage pcfg-based lfg approx-
imations. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 319?
326. Barcelona, Spain. URL http://www.
aclweb.org/anthology/P04-1041.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first con-
ference on North American chapter of the Associ-
ation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 173?180. Associa-
tion for Computational Linguistics, Ann Arbor,
Michigan.
Wanxiang Che, Valentin Spitkovsky, and Ting
Liu. 2012. A comparison of chinese parsers
for stanford dependencies. In Proceedings
of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume
2: Short Papers), pages 11?16. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-2003.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 505?512. Association for Com-
putational Linguistics, College Park, Maryland,
USA. URL http://www.aclweb.org/
anthology/P99-1065.
Jason M. Eisner. 1996. Three new probabilis-
tic models for dependency parsing: an ex-
ploration. In Proceedings of the 16th con-
ference on Computational linguistics - Vol-
ume 1, COLING ?96, pages 340?345. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/992628.992688.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cat-
egorial grammar. In Proceedings of the 48th
Annual Meeting of the Association for Com-
putational Linguistics, pages 335?344. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URL http://www.aclweb.org/
anthology/P10-1035.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Asian Federation of Natural Language Process-
ing, Chiang Mai, Thailand. URL http://www.
aclweb.org/anthology/I11-1136.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: Combin-
ing parsers. In In Proceedings of the Fourth Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 187?194.
John C. Henderson and Eric Brill. 2000. Bag-
ging and boosting a treebank parser. In Pro-
ceedings of the 1st North American chapter of
the Association for Computational Linguistics
conference, NAACL 2000, pages 34?41. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA. URL http://dl.acm.org/
citation.cfm?id=974305.974310.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
312
1077?1086. Association for Computational Lin-
guistics, Uppsala, Sweden. URL http://www.
aclweb.org/anthology/P10-1110.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and
self-training. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 213?216. As-
sociation for Computational Linguistics, Boulder,
Colorado. URL http://www.aclweb.org/
anthology/N/N09/N09-2054.
Zhenghua Li, Ting Liu, and Wanxiang Che.
2012. Exploiting multiple treebanks for pars-
ing with quasi-synchronous grammars. In Pro-
ceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 675?684. Associa-
tion for Computational Linguistics, Jeju Island,
Korea. URL http://www.aclweb.org/
anthology/P12-1071.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting
Liu, Wenliang Chen, and Haizhou Li. 2011.
Joint models for Chinese pos tagging and depen-
dency parsing. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1180?1191. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1109.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent an-
notations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ?05, pages 75?82. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/1219840.1219850.
RyanMcDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency pars-
ing. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA. AAI3225503.
Ryan McDonald and Joakim Nivre. 2007. Char-
acterizing the errors of data-driven dependency
parsing models. In Proceedings of the 2007
Joint Conference on Empirical Methods in
Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-
CoNLL), pages 122?131. Association for Com-
putational Linguistics, Prague, Czech Repub-
lic. URL http://www.aclweb.org/
anthology/D/D07/D07-1013.
Joakim Nivre. 2008. Algorithms for de-
terministic incremental dependency pars-
ing. Comput. Linguist., 34:513?553. URL
http://dx.doi.org/10.1162/coli.
07-056-R1-07-027.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based
dependency parsers. In Proceedings of ACL-08:
HLT, pages 950?958. Association for Compu-
tational Linguistics, Columbus, Ohio. URL
http://www.aclweb.org/anthology/
P/P08/P08-1108.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433?440. Association for Computational Linguis-
tics, Sydney, Australia.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the
Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers,
NAACL-Short ?06, pages 129?132. Association
for Computational Linguistics, Stroudsburg, PA,
USA. URL http://portal.acm.org/
citation.cfm?id=1614049.1614082.
Weiwei Sun. 2010a. Improving Chinese se-
mantic role labeling with rich syntactic fea-
tures. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 168?172. Associ-
ation for Computational Linguistics, Uppsala,
Sweden. URL http://www.aclweb.org/
anthology/P10-2031.
Weiwei Sun. 2010b. Word-based and character-
based word segmentation models: Compari-
son and combination. In Proceedings of the
313
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1211?1219.
Coling 2010 Organizing Committee, Beijing,
China. URL http://www.aclweb.org/
anthology/C10-2139.
Weiwei Sun and Zhifang Sui. 2009. Chinese func-
tion tag labeling. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation. Hong Kong.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations:
Towards accurate Chinese part-of-speech tagging.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics, pages 649?652. Association
for Computational Linguistics, Los Angeles, Cal-
ifornia. URL http://www.aclweb.org/
anthology/N10-1091.
Andre Torres Martins, Noah Smith, and Eric Xing.
2009. Concise integer linear programming for-
mulations for dependency parsing. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language Process-
ing of the AFNLP, pages 342?350. Associa-
tion for Computational Linguistics, Suntec, Sin-
gapore. URL http://www.aclweb.org/
anthology/P/P09/P09-1039.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 157?166. Associ-
ation for Computational Linguistics, Honolulu,
Hawaii. URL http://www.aclweb.org/
anthology/D08-1017.
Nianwen Xue. 2007. Tapping the implicit infor-
mation for the PS to DS conversion of the Chi-
nese treebank. In Proceedings of the Sixth Inter-
national Workshop on Treebanks and Linguistics
Theories.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-
scale corpus-driven pcfg approximation of an
hpsg. In Proceedings of the 12th International
Conference on Parsing Technologies, pages 198?
208. Association for Computational Linguistics,
Dublin, Ireland. URL http://www.aclweb.
org/anthology/W11-2923.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
562?571. Association for Computational Linguis-
tics, Honolulu, Hawaii. URL http://www.
aclweb.org/anthology/D08-1059.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a
global discriminative model. In Proceedings
of the 11th International Conference on Pars-
ing Technologies (IWPT?09), pages 162?171. As-
sociation for Computational Linguistics, Paris,
France. URL http://www.aclweb.org/
anthology/W09-3825.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Comput. Linguist., 37(1):105?
151. URL http://dx.doi.org/10.1162/
coli_a_00037.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 188?
193. Association for Computational Linguistics,
Portland, Oregon, USA. URL http://www.
aclweb.org/anthology/P11-2033.
Tao Zhuang and Chengqing Zong. 2010. A min-
imum error weighting combination strategy for
Chinese semantic role labeling. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 1362?
1370. Coling 2010 Organizing Committee, Bei-
jing, China. URL http://www.aclweb.
org/anthology/C10-1153.
314
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459?464,
Dublin, Ireland, August 23-24, 2014.
Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph
Parsing
Yantao Du, Fan Zhang, Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{duyantao,ws,wanxiaojun}@pku.edu.cn, zhangf717@gmail.com
Abstract
Using the SemEval-2014 Task 8 data, we
profile the syntactic tree parsing tech-
niques for semantic graph parsing. In par-
ticular, we implement different transition-
based and graph-based models, as well as
a parser ensembler, and evaluate their ef-
fectiveness for semantic dependency pars-
ing. Evaluation gauges how successful
data-driven dependency graph parsing can
be by applying existing techniques.
1 Introduction
Bi-lexical dependency representation is quite pow-
erful and popular to encode syntactic or semantic
information, and parsing techniques under the de-
pendency formalism have been well studied and
advanced in the last decade. The major focus is
limited to tree structures, which fortunately corre-
spond to many computationally good properties.
On the other hand, some leading linguistic theo-
ries argue that more general graphs are needed to
encode a wide variety of deep syntactic and se-
mantic phenomena, e.g. topicalization, relative
clauses, etc. However, algorithms for statistical
graph spanning have not been well explored be-
fore, and therefore it is not very clear how good
data-driven parsing techniques developed for tree
parsing can be for graph generating.
Following several well-established syntactic
theories, SemEval-2014 task 8 (Oepen et al.,
2014) proposes using graphs to represent seman-
tics. Considering that semantic dependency pars-
ing is a quite new topic and there is little previ-
ous work, we think it worth appropriately profil-
ing successful tree parsing techniques for graph
parsing. To this end, we build a hybrid system
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
that combines several important data-driven pars-
ing techniques and evaluate their impact with the
given data. In particular, we implement different
transition-based and graph-based models, as well
as a parser ensembler.
Our experiments highlight the following facts:
? Graph-based models are more effective than
transition-based models.
? Parser ensemble is very useful to boost the
parsing accuracy.
2 Architecture
We explore two kinds of basic models: One is
transition-based, and the other is tree approxima-
tion. Transition-based models are widely used for
dependency tree parsing, and they can be adapted
to graph parsing (Sagae and Tsujii, 2008; Titov
et al., 2009). Here we implement 5 transition-
based models for dependency graph parsing, each
of which is based on different transition system.
The motivation of developing tree approxima-
tion models is to apply existing graph-based tree
parsers to generate graphs. At the training time,
we convert the dependency graphs from the train-
ing data into dependency trees, and train second-
order arc-factored models
1
. At the test phase, we
parse sentences using this tree parser, and convert
the output trees back into semantic graphs. We
think tree approximation can appropriately evalu-
ate the possible effectiveness of graph-based mod-
els for graph spanning.
Finally, we integrate the outputs of different
models with a simple voter to boost the perfor-
mance. The motivation of using system combi-
nation and the choice of voting is mainly due to
the experiments presented by (Surdeanu and Man-
ning, 2010). When we obtain all the outputs of
1
The mate parser (code.google.com/p/
mate-tools/) is used.
459
these models, we combine them into a final result,
which is better than any of them. For combination,
we explore various systems for this task, since em-
pirically we know that variety leads to better per-
formance.
3 Transition-Based Models
Transition-based models are usually used for de-
pendency tree parsing. For this task, we exploit it
for dependency graph parsing.
A transition system S contains a set C of con-
figurations and a set T of transitions. A configu-
ration c ? C generally contains a stack ? of nodes,
a buffer ? of nodes, and a set A of arcs. The ele-
ments in A is in the form (x, l, y), which denotes
a arc from x to y labeled l. A transition t ? T can
be applied to a configuration and turn it into a new
one by adding new arcs or manipulating elements
of the stack or the buffer. A statistical transition-
based parser leverages a classifier to approximate
an oracle that is able to generate target graphs by
transforming the initial configuration c
s
(x) into a
terminal configuration c
t
? C
t
.
An oracle of a given graph on sentence x is a
sequence of transitions which transform the initial
configuration to the terminal configuration the arc
set A
c
t
of which is the set of the arcs of the graph.
3.1 Our Transition Systems
We implemented 5 different transition systems for
graph parsing. Here we describe two of them
in detail, one is the Titov system proposed in
(Titov et al., 2009), and the other is our Naive
system. The configurations of the two systems
each contain a stack ?, a buffer ?, and a set A of
arcs, denoted by ??, ?,A?. The initial configura-
tion of a sentence x = w
1
w
2
? ? ?w
n
is c
s
(x) =
?[0], [1, 2, ? ? ? , n], {}?, and the terminal configu-
ration set C
t
is the set of all configurations with
empty buffer. These two transition systems are
shown in 1.
The transitions of the Titov system are:
? LEFT-ARC
l
adds an arc from the front of the
buffer to the top of the stack, labeled l, into
A.
? RIGHT-ARC
l
adds an arc from the top of the
stack to the front of the buffer, labeled l, into
A.
? SHIFT removes the front of the buffer and
push it onto the stack;
? POP pops the top of the stack.
? SWAP swaps the top two elements of the
stack.
This system uses a transition SWAP to change the
node order in the stack, thus allowing some cross-
ing arcs to be built.
The transitions of the Naive system are similar
to the Titov system?s, except that we can directly
manipulate all the nodes in the stack instead of just
the top two. In this case, the transition SWAP is not
needed.
The Titov system can cover a great proportion,
though not all, of graphs in this task. For more
discussion, see (Titov et al., 2009). The Naive
system, by comparison, covers all graphs. That
is to say, with this system, we can find an oracle
for any dependency graph on a sentence x. Other
transition systems we build are also designed for
dependency graph parsing, and they can cover de-
pendency graphs without self loop as well.
3.2 Statistical Disambiguation
First of all, we derive oracle transition sequences
for every sentence, and train Passive-Aggressive
models (Crammer et al., 2006) to predict next tran-
sition given a configuration. When it comes to
parsing, we start with the initial configuration, pre-
dicting next transition and updating the configura-
tion with the transition iteratively. And finally we
will get a terminal configuration, we then stop and
output the arcs of the graph contained in the final
configuration.
We extracted rich feature for we utilize a set
of rich features for disambiguation, referencing to
Zhang and Nivre (2011). We examine the several
tops of the stack and the one or more fronts of the
buffer, and combine the lemmas and POS tags of
them in many ways as the features. Additionally,
we also derive features from partial parses such as
heads and dependents of these nodes.
3.3 Sentence Reversal
Reversing the order the words of a given sentence
is a simple way to yield heterogeneous parsing
models, thus improving parsing accuracy of the
model ensemble (Sagae, 2007). In our experi-
ments, one transition system produces two mod-
els, one trained on the normal corpus, and the other
on the corpus of reversed sentences. Therefore we
can get 10 parse of a sentence based on 5 transition
systems.
460
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP (?|i, ?, A)? (?, ?,A)
SWAP (?|i|j, ?,A)? (?|j|i, ?, A)
Titov System
LEFT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(j, l, i
k
)})
RIGHT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(i
k
, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP
k
(?|i
k
|i
k?1
| . . . |i
2
|i
1
, ?, A)? (?|i
k?1
| . . . |i
2
|i
1
, ?, A)
Naive System
Figure 1: Two of our transition systems.
4 Tree Approximation Models
Parsing based on graph spanning is quite challeng-
ing since computational properties of the seman-
tic graphs given by the shared task are less ex-
plored and thus still unknown. On the other hand,
finding the best higher-order spanning for general
graph is NP complete, and therefore it is not easy,
if not impossible, to implement arc-factored mod-
els with exact inference. In our work, we use a
practical idea to indirectly profile the graph-based
parsing techniques for dependency graph parsing.
Inspired by the PCFG approximation idea (Fowler
and Penn, 2010; Zhang and Krieger, 2011) for
deep parsing, we study tree approximation ap-
proaches for graph spanning.
This tree approximation technique can be ap-
plied to both transition-based and graph-based
parsers. However, since transition systems that
can directly handle build graphs have been devel-
oped, we only use this technique to evaluate the
possible effectiveness of graph-based models for
semantic parsing.
4.1 Graph-to-Tree Transformation
In particular, we develop different methods to con-
vert a semantic graph into a tree, and use edge
labels to encode dependency relations as well as
structural information which helps to transform a
converted tree back to its original graph. By the
graph-to-tree transformation, we can train a tree
parser with a graph-annotated corpus, and utilize
the corresponding tree-to-graph transformation to
generate target graphs from the outputs of the tree
parser. Given that the tree-to-graph transformation
is quite trivial, we only describe the graph-to-tree
transformation approach.
We use graph traversal algorithms to convert a
directed graph to a directed tree. The transforma-
tion implies that we may lose, add or modify some
dependency relations in order to make the graph a
tree.
4.2 Auxiliary Labels
In the transformed trees, we use auxiliary labels to
carry out information of the original graphs. To
encode multiple edges to one, we keep the origi-
nal label on the directed edge but may add other
edges? information. On the other hand, through-
out most transformations, some edges must be re-
versed to make a tree, so we need a symbol to in-
dicate a edge on the tree is reversed during trans-
formation. The auxiliary labels are listed below:
? Label with following ?R: The symbol ?R
means this directed edge is reversed from the
original directed graph.
? Separator: Semicolon separates two encoded
original edges.
? [N ] followed by label: The symbol [N ] (N
is an integer) represents the head of the edge.
The dependent is the current one, but the head
is the dependent?s N -th ancestor where 1st
ancestor is its father and 2nd ancestor is its
father?s father.
See Figure 2 for example.
4.3 Traversal Strategies
Given directed graph (V,E), the task is to traverse
all edges on the graph and decide how to change
the labels or not contain the edge on the output.
We use 3 strategies for traversal. Here we use
x ?
g
y to denote the edge on graph, and x ?
t
y
the edge on tree.
461
Mrs Ward was relieved
noun ARG1 verb ARG1 verb ARG2
adj ARG1
root
Mrs Ward was relieved
noun ARG1?R verb ARG1 verb ARG2
root
Mrs Ward was relieved
noun ARG1?R verb ARG2
adj ARG1;[2]verb ARG1
root
Figure 2: One dependency graph and two possible
dependency trees after converting.
Depth-first-search We try graph traversal by
depth-first-search starting from the root on the di-
rected graph ignoring the direction of edges. Dur-
ing the traversal, we add edges to the directed tree
with (perhaps new) labels. We traverse the graph
recursively. Suppose the depth-first-search is run-
ning at the node x and the nodes set A which have
been searched. And suppose we find node y is
linked to x on the graph (x ?
g
y or y ?
g
x).
If y /? A, we add the directed edge x ?
t
y to the
tree immediately. In the case of y ?
g
x, we add
?R to the edge label. If y ? A, then y must be one
of the ancestors of x. In this case, we add this in-
formation to the label of the existing edge z ?
t
x.
Since the distance between two nodes x and y is
sufficient to indicate the node y, we use the dis-
tance to represent the head or dependent of this
directed edge and add the label and the distance to
the label of z ?
t
x. It is clear that the auxiliary
label [N ] can be used for multiple edge encoding.
Under this strategy, all edges can be encoded on
the tree.
Breadth-first-search An alternative traversal
strategy is based on breadth-first-search starting
from the root. This search ignores the direction
of edge too. We regard the search tree as the de-
pendency tree. During the breadth-first-search, if
(x, l, y) exists but node y has been searched, we
just ignore the edge. Under this strategy, we may
lose some edges.
Iterative expanding This strategy is based on
depth-first-search but slightly different. The strat-
egy only searches through the forward edges on
the directed graph at first. When there is no for-
ward edge to expend, a traversed node linked to
some nodes that are not traversed must be the de-
pendent of them. Then we choose an edge and add
it (reversed) to the tree and continue to expand the
tree. Also, we ignore the edges that does not sat-
isfy the tree constraint. We call this strategy iter-
ative expanding. When we need to expand output
tree, we need to design a strategy to decide which
edge to be add. The measure to decide which node
should be expanded first is its possible location on
the tree and the number of nodes it can search dur-
ing depth-first-search. Intuitively, we want the re-
versed edges to be as few as possible. For this
purpose, this strategy is practical but not necessar-
ily the best. Like the Breadth-first-search strategy,
this strategy may also cause edge loss.
4.4 Forest-to-Tree
After a primary searching process, if there is still
edge x ?
g
y that has not been searched yet, we
start a new search procedure from x or y. Even-
tually, we obtain a forest rather than a tree. To
combine disconnected trees in this forest to the fi-
nal dependency tree, we use edges with label None
to link them. Let the node setW be the set of roots
of the trees in the forest, which are not connected
to original graph root. The mission is to assign a
node v /? W for each w ? W . If we assign v
i
for
w
i
, we add the edge v
i
? w
i
labeled by None to
the final dependency tree. We try 3 strategies in
this step:
? For each w ? W we look for the first node
v /?W on the left of w.
? For each w ? W we look for the first node
v /?W on the right of w.
? By defining the distance between two nodes
as how many words are there between the two
words, we can select the nearest node. If the
distances of more than one node are equal,
we choose v randomly.
We also tried to link all of the nodes in W di-
rectly to the root, but it does not work well.
5 Model Ensemble
We have 19 heterogeneous basic models (10
transition-based models, 9 tree approximation
models), and use a simple voter to combine their
outputs.
462
Algorithm DM PAS PCEDT
DFS 0 0 0
BFS 0.0117 0.0320 0.0328
FEF 0.0127 0.0380 0.0328
Table 1: Edge loss of transformation algorithms.
For each pair of words of a sentence, we count
the number of the models that give positive pre-
dictions. If the number is greater than a threshold,
we put this arc to the final graph, and label the arc
with the most common label of what the models
give.
Furthermore, we find that the performance of
the tree approximation models is better than the
transition based models, and therefore we take
weights of individual models too. Instead of just
counting, we sum the weights of the models that
give positive predictions. The tree approximation
models are assigned higher weights.
6 Experiments
There are 3 subtasks in the task, namely DM, PAS,
and PCEDT. For subtask DM, we finally obtained
19 models, just as stated in previous sections.
For subtask PAS and PCEDT, only 17 models are
trained due to the tight schedule.
The tree approximation algorithms may cause
some edge loss, and the statistics are shown in Ta-
ble 1. We can see that DFS does not cause edge
loss, but edge losses of other two algorithm are
not negligible. This may result in a lower recall
and higher precision, but we can tune the final re-
sults during model ensemble. Edge loss in subtask
DM is less than those in subtask PAS and PCEDT.
We present the performance of several repre-
sentative models in Table 2. We can see that the
tree approximation models performs better than
the transition-based models, which highlights the
effective of arc-factored models for semantic de-
pendency parsing. For model ensemble, besides
the accuracy of each single model, it is also im-
portant that the models to be ensembled are very
different. As shown in Table 2, the evaluation be-
tween some of our models indicates that our mod-
els do vary a lot.
Following the suggestion of the task organizers,
we use section 20 of the train data as the devel-
opment set. With the help of development set,
we tune the parameters of the models and ensem-
Models DM PAS PCEDT
Titov 0.8468 0.8754 0.6978
Titov
r
0.8535 0.8928 0.7063
Naive 0.8481 - -
DFS
n
0.8692 0.9034 0.7370
DFS
l
0.8692 0.9015 0.7246
BFS
n
0.8686 0.8818 0.7247
Titov vs. Titov
r
0.8607 0.8831 0.7613
Titov vs. Naive 0.9245 - -
Titov vs. DFS
n
0.8590 0.8865 0.7650
DFS
n
vs. DFS
l
0.9273 0.9579 0.8688
DFS
n
vs. BFS
n
0.9226 0.9169 0.8367
Table 2: Evaluation between some of our models.
Labeled f-score on test set is shown. Titov
r
stands
for reversed Titov, DFS
n
for DFS+nearest, DFS
l
for DFS+left, and BFS
n
for BFS+nearest. The up-
per part gives the performance, and the lower part
gives the agreement between systems.
Format LP LR LF LM
DM 0.9027 0.8854 0.8940 0.2982
PAS 0.9344 0.9069 0.9204 0.3872
PCEDT 0.7875 0.7396 0.7628 0.1120
Table 3: Final results of the ensembled model.
bling. We set the weight of each transition-based
model 1, and tree approximation model 2 in run
1, 3 in run 2. The threshold is set to a half of the
total weight. The final results given by the orga-
nizers are shown in Table 3. Compared to Table 2
demonstrates the effectiveness of parser ensemble.
7 Conclusion
Data-driven dependency parsing techniques have
been greatly advanced during the parst decade.
Two dominant approaches, i.e. transition-based
and graph-based methods, have been well stud-
ied. In addition, parser ensemble has been shown
very effective to take advantages to combine the
strengthes of heterogeneous base parsers. In this
work, we propose different models to profile the
three techniques for semantic dependency pars-
ing. The experimental results suggest several di-
rections for future study.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
463
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JOURNAL OF MA-
CHINE LEARNING RESEARCH, 7:551?585.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cate-
gorial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, Uppsala, Sweden, July.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Haji?c, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation, Dublin, Ireland.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 753?760, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Kenji Sagae. 2007. Dependency parsing and domain
adaptation with lr models and parser ensembles. In
In Proceedings of the Eleventh Conference on Com-
putational Natural Language Learning.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Los Angeles, California, June.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisa-
tion for synchronous parsing of semantic and syn-
tactic dependencies. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1562?1567, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-scale
corpus-driven PCFG approximation of an hpsg. In
Proceedings of the 12th International Conference on
Parsing Technologies, Dublin, Ireland, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Portland, Oregon, USA, June.
464
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247
Manchester, August 2008
The Integration of Dependency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov Models
Weiwei Sun and Hongzhan Li and Zhifang Sui
Institute of Computational Linguistics
Peking University
{weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn
Abstract
This paper describes a system to solve
the joint learning of syntactic and seman-
tic dependencies. An directed graphical
model is put forward to integrate depen-
dency relation classification and semantic
role labeling. We present a bilayer di-
rected graph to express probabilistic re-
lationships between syntactic and seman-
tic relations. Maximum Entropy Markov
Models are implemented to estimate con-
ditional probability distribution and to do
inference. The submitted model yields
76.28% macro-average F1 performance,
for the joint task, 85.75% syntactic depen-
dencies LAS and 66.61% semantic depen-
dencies F1.
1 Introduction
Dependency parsing and semantic role labeling are
becoming important components in many kinds of
NLP applications. Given a sentence, the task of de-
pendency parsing is to identify the syntactic head
of each word in the sentence and classify the rela-
tion between the dependent and its head; the task
of semantic role labeling consists of analyzing the
propositions expressed by some target predicates.
The integration of syntactic and semantic parsing
interests many researchers and some approaches
has been proposed (Yi and Palmer, 2005; Ge and
Mooney, 2005). CoNLL 2008 shared task pro-
poses the merging of both syntactic dependencies
and semantic dependencies under a unique unified
representation (Surdeanu et al, 2008). We explore
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the integration problem and evaluate our approach
using data provided on CoNLL 2008.
This paper explores the integration of depen-
dency relation classification and semantic role la-
beling, using a directed graphical model that is also
known as Bayesian Networks. The directed graph
of our system can be seen as one chain of obser-
vations with two label layers: the observations are
argument candidates; one layer?s label set is syn-
tactic dependency relations; the other?s is semantic
dependency relations. To estimate the probability
distribution of each arc and do inference, we im-
plement a Maximum Entropy Markov Model (Mc-
Callum et al, 2000). Specially, a logistic regres-
sion model is used to get the conditional probabil-
ity of each arc; dynamic programming algorithm
is applied to solve the ?argmax? problem.
2 System Description
Our DP-SRL system consists of 5 stages:
1. dependency parsing;
2. predicate prediction;
3. syntactic dependency relation classification
and semantic dependency relation identifica-
tion;
4. semantic dependency relation classification;
5. semantic dependency relation inference.
2.1 Dependency Parsing
In dependency parsing stage, MSTParser
1
(Mc-
Donald et al, 2005), a dependency parser that
searches for maximum spanning trees over di-
rected graphs, is used. we use MSTParser?s default
1
http://www.seas.upenn.edu/ strctlrn/MSTParser/MSTParser.html
243
Lemma and its POS tag
Number of children
Sequential POS tags of children
Lemma and POS of Neighboring words
Lemma and POS of parent
Is the word in word list of NomBank
Is the word in word list of PropBank
Is POS of the word is VB* or NN*
Table 1: Features used to predict target predicates
parameters to train a parsing model. In the third
stage of our system, dependency relations between
argument candidates and target predicates are up-
dated, if there are dependency between the candi-
dates and the predicates.
2.2 Predicate Prediction
Different from CoNLL-2005 shared task, the tar-
get predicates are not given as input. Our system
formulates the predicate predication problem as a
two-class classification problem using maximum
entropy classifier MaxEnt
2
(Berger et al, 1996).
Table 1 lists features used. We use a empirical
threshold to filter words: if the ?being target? prob-
ability of a word is greater than 0.075, it is seen as
a target predicate. This strategy achieves a 79.96%
precision and a 98.62% recall.
2.3 Syntactic Dependency Relation
Classification and Semantic Dependency
Relation Identification
We integrate dependency parsing and semantic
role labeling to some extent in this stage. Some de-
pendency parsing systems prefer two-stage archi-
tecture: unlabeled parsing and dependency clas-
sification (Nivre et al, 2007). Previous semantic
role labeling approaches also prefer two-stage ar-
chitecture: argument identification and argument
classification. Our system does syntactic relations
classification and semantic relations identification
at the same time. Specially, using a pruning al-
gorithm, we collect a set of argument candidates;
then we classify dependency relations between ar-
gument candidates and the predicates and predict
whether a candidate is an argument. A directed
graphical model is used to represent the relations
between syntactic and semantic relations.
2
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.h
tml
Lemma, POS tag voice of predicates
POS pattern of predicate?s children
Is the predicate from NomBank or PropBank
Predicate class. This information is extracted
form frame file of each predicate.
Position: whether the candidate is before or
after the predicate
Lemma and POS tag of the candidate
Lemma and POS of Neighboring words of the
candidate
Lemma and POS of sibling words of the
candidate
Length of the constituent headed by the
candidate
Lemma and POS of the left and right most
words of the constituent of the candidate
Punctuation before and after the candidate
POS path: the chain of POS from candidate to
predicate
Single Character POS path: each POS in a path
is clustered to a category defined by its
first character
POS Pattern (string of POS tags) of all
candidates
Single Character POS Pattern of all candidates
Table 2: Features used for semantic role labeling
2.4 Semantic Dependency Relation
Classification
This stage assigns the final argument labels to the
argument candidates supplied from the previous
stage. A multi-class classifier is trained to classify
the types of the arguments supplied by the previous
stage. Table 2 lists the features used. It is clear that
the general type of features used here is strongly
based on previous work on the SRL task (Gildea
and Jurafsky, 2002; Pradhan et al, 2005; Xue and
Palmer, 2004). Different from CoNLL-2005, the
sense of predicates should be labeled as a part of
the task. Our system assigns 01 to all predicates.
This is a harsh tactic since it do not take the lin-
guistic meaning of the argument-structure into ac-
count.
2.5 Semantic Dependency Relation Inference
The purpose of inference stage is to incorporate
some prior linguistic and structural knowledge,
such as ?each predicate takes at most one argument
of each type.? We use the inference process intro-
244
duced by (Punyakanok et al, 2004; Koomen et al,
2005). The process is modeled as an integer Lin-
ear Programming Problem (ILP). It takes the pre-
dicted probability over each type of the arguments
as inputs, and takes the optimal solution that max-
imizes the linear sum of the probability subject to
linguistic constraints as outputs. The constraints
are a subset of constraints raised by Koomen et al
(2005) and encoded as following: 1) No overlap-
ping or embedding arguments; 2) No duplicate ar-
gument classes for A0-A5; 3) If there is an R-arg
argument, then there has to be an arg argument;
4) If there is a C-arg argument, there must be an
arg argument; moreover, the C-arg argument must
occur after arg; 5) Given the predicate, some argu-
ment types are illegal. The list of illegal argument
types is extracted from framefile.
The ILP process can improve SRL performance
on constituent-based parsing (Punyakanok et al,
2004). In our experiment, it also works on
dependency-based parsing.
3 Bilayer Maximum Entropy Markov
Models
3.1 Sequentialization
The sequentialization of a argument-structure is si-
miliar to the pruning algorithm raised by (Xue and
Palmer, 2004). Given a constituent-based parsing
tree, the recursive pruning process starts from a tar-
get predicate. It first collects the siblings of the
predicate; then it moves to the parent of the pred-
icate, and collects the siblings of the parent. In
addition, if a constituent is a prepositional phrase,
its children are also collected.
Our system uses a similar pruning algorithm to
filter out very unlikely argument candidates in a
dependency-based parsing tree. Given a depen-
dency parsing tree, the pruning process also starts
from a target predicate. It first collects the depen-
dents of the predicate; then it moves to the parent
of the predicate, and collects all the dependents
again. Note that, the predicate is also taken into
account. If the target predicate is a verb, the pro-
cess goes on recursively until it reaches the root.
The process of a noun target ends when it sees a
PMOD, NMOD, SBJ or OBJ dependency relation.
If a preposition is returned as a candidate, its child
is also collected. When the predicate is a verb, the
set of constituents headed by survivors of our prun-
ing algorithm is a superset of the set of survivors of
the previous pruning algorithm on the correspond-
Figure 1: Directed graphical Model of The system
ing constituent-based parsing tree. This pruning
algorithm will recall 99.08% arguments of verbs,
and the candidates are 3.75 times of the real argu-
ments. If the stop relation such as PMOD of a noun
is not taken into account, the recall is 97.67% and
the candidates is 6.28 times of arguments. If the
harsh stop condition is implemented, the recall is
just 80.29%. Since the SRL performance of nouns
is very low, the harsh pruning algorithm works bet-
ter than the original one.
After pruning, our system sequentializes all ar-
gument candidates of the target predicate accord-
ing to their linear order in the given sentence.
3.2 Graphical Model
Figure 1 is the directed graph of our system.
There is a chain of candidates x = (x
0
=
BOS, x
1
, ..., x
n
) in the graph which are observa-
tions. There are two tag layers in the graph: the up
layer is information of semantic dependency rela-
tions; the down layer is information of syntactic
dependency relations.
Given x, denote the corresponding syntactic de-
pendency relations d = (d
0
= BOS, d
1
, ..., d
n
)
and the corresponding semantic dependency rela-
tions s = (s
0
= BOS, s
1
, ..., s
n
). Our system
labels the syntactic and semantic relations accord-
ing to the conditional probability in argmax fla-
vor. Formally, labels the system assigned make
the score p(d, s|x) reaches its maximum. We de-
compose the probability p(d, s|x) according to the
directed graph modeled as following:
p(d, s|x) = p(s
1
|s
0
, d
1
;x)p(d
1
|s
0
, d
0
;x) ? ? ?
p(s
i+1
|s
i
, d
i+1
;x)p(d
i+1
|s
i
, d
i
;x) ? ? ?
p(s
n
|s
n?1
, d
n
;x)p(d
n
|s
n?1
, d
n?1
;x)
=
n
?
i=1
p(s
i
|s
i?1
, d
i
;x)p(d
i
|s
i?1
, d
i?1
;x)
245
Lemma, POS tag voice of predicates
POS pattern of predicate?s children
Lemma and POS tag of the candidate
Lemma and POS of Neighboring words of the
candidate
Lemma and POS of sibling words of the
candidate
Length of the constituent headed by the
candidate
Lemma and POS of the left and right most
words of the constituent of the candidate
Conjunction of lemma of candidates and
predicates; Conjunction of POS of candidates
and predicates
POS Pattern of all candidates
Table 3: Features used to predict syntactic depen-
dency parsing
3.3 Probability Estimation
The system defines the conditional probability
p(s
i
|s
i?1
, d
i
;x) and p(d
i
|s
i?1
, d
i?1
;x) by using
the maximum entropy (Berger et al, 1996) frame-
work Denote the tag set of syntactic dependency
relations D and the tag set of semantic dependency
relations S. Formally, given a feature map ?
s
and
a weight vector w
s
,
p
w
s
(s
i
|s
i?1
, d
i
;x) =
exp{w
s
? ?
s
(x, s
i
, s
i?1
, d
i
)}
Z
x,s
i?1
,d
i
;w
s
where,
Z
x,s
i?1
,d
i
;w
s
=
?
s?S
exp{w
s
? ?
s
(x, s, s
i?1
, d
i
)}
Similarly, given a feature map ?
d
and
a weight vector w
d
, (p
w
d
(d
i
) is short for
p
w
d
(d
i
|s
i?1
, d
i?1
;x)
p
w
d
(d
i
) =
exp{w
d
? ?
d
(x, d
i
, s
i?1
, d
i?1
)}
Z
x,s
i?1
,d
i?1
;w
d
where,
Z
x,s
i?1
,d
i?1
;w
d
=
?
d?D
exp{w
d
? ?
d
(x, d, s
i?1
, d
i?1
)}
For different characteristic properties between
syntactic parsing and semantic parsing, different
feature maps are taken into account. Table 2
lists the features used to predict semantic depen-
dency relations, whereas table 3 lists the features
used to predict the syntactic dependency relations.
The features used for syntactic dependency rela-
tion classification are strongly based on previous
works (McDonald et al, 2006; Nakagawa, 2007).
We just integrate syntactic dependency Rela-
tion classification and semantic dependency rela-
tion here. If one combines identification and clas-
sification of semantic roles as one multi-class clas-
sification, the tag set of the second layer can be
substituted by the tag set of semantic roles plus a
NULL (?not an argument?) label.
3.4 Inference
The ?argmax problem? in structured prediction is
not tractable in the general case. However, the bi-
layer graphical model presented in form sections
admits efficient search using dynamic program-
ming solution. Searching for the highest probabil-
ity of a graph depends on the factorization chosen.
According to the form of the global score
p(d, s|x) =
n
?
i=1
p(s
i
|s
i?1
, d
i
;x)p(d
i
|s
i?1
, d
i?1
;x)
, we define forward probabilities ?
t
(s, d) to be the
probability of semantic relation being s and syn-
tactic relation being d at time t given observation
sequence up to time t. The recursive dynamic pro-
gramming step is
?
t+1
(d, s) = arg max
d?D,s?S
?
d
?
?D,s
?
?S
?
t
(d
?
, s
?
) ?
p(s
i
|s
i?1
, d
i
;x)p(d
i
|s
i?1
, d
i?1
;x)
Finally, to compute the globally most proba-
ble assignment (
?
d,
?
s) = argmax
d,s
p(d, s|x), a
Viterbi recursion works well.
4 Results
We trained our system using positive examples
extracted from all training data of CoNLL 2008
shared task. Table 4 shows the overall syntactic
parsing results obtained on the WSJ test set (Sec-
tion 23) and the Brown test set (Section ck/01-03).
Table 5 shows the overall semantic parsing results
obtained on the WSJ test set (Section 23) and the
Brown test set (Section ck/01-03).
246
Test Set UAS LAS Label Accuracy
WSJ 89.25% 86.37% 91.25%
Brown 86.12% 80.75% 87.14%
Table 4: Overall syntactic parsing results
Task Precision Recall F
?=1
WSJ ID 73.76% 85.24% 79.08
ID&CL 63.07% 72.88% 67.62
Brown ID 70.77% 80.50% 75.32
ID&CL 54.74% 62.26% 58.26
Table 5: Overall semantic parsing results
Test WSJ Precision(%) Recall(%) F
?=1
SRL of Verbs
All 73.53 73.28 73.41
Core-Arg 78.83 76.93 77.87
AM-* 62.51 64.83 63.65
SRL of Nouns
All 62.06 45.49 52.50
Core-Arg 61.47 46.56 52.98
AM-* 66.19 39.93 49.81
Table 6: Semantic role labeling results on verbs
and nouns. Core-Arg means numbered argument.
Table 6 shows the detailed semantic parsing re-
sults obtained on the WSJ test set (Section 23)
of verbs and nouns respectively. The comparison
suggests that SRL on NomBank is much harder
than PropBank.
Acknowlegements
The work is supported by the National Natural
Science Foundation of China under Grants No.
60503071, 863 the National High Technology Re-
search and Development Program of China un-
der Grants No.2006AA01Z144, and the Project of
Toshiba (China) Co., Ltd. R&D Center.
References
Berger, Adam, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A Maximum Entropy Approach to
Natural Language Processing. Computional Lin-
guistics, 22(1):39?71.
Ge, Ruifang and Raymond J. Mooney. 2005. A Statis-
tical Semantic Parser that Integrates Syntax and Se-
mantics. In Proceedings of the Conference of Com-
putational Natural Language Learning.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computional Linguis-
tics, 28(3):245?288.
Koomen, Peter, Vasina Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of Conference on Natural Language Learn-
ing.
McCallum, Andrew, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for Information Extraction and Segmentation.
In Proceedings of International Conference on Ma-
chine Learning.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual Dependency Analysis
with a Two-Stage Discriminative Parser. In Proceed-
ings of Conference on Natural Language Learning.
Nakawa, Tetsuji. 2007. Multilingual Dependency
Parsing using Global Features. In Proceedings of
Conference on Natural Language Learning.
Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. The CoNLL 2007 Shared Task on Depen-
dency Parsing. 2007. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, 915?
932,
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. In Proceedings of Conference
on Association for Computational Linguistics.
Punyakanok, Vasin , Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
the 20th International Conference on Computational
Linguistics.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Nivre, Joakim. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings
of the 12th Conference on Computational Natural
Language Learning (CoNLL-2008).
Xue, Nianwen and Martha Palmer. 2004. Calibrat-
ing Features for Semantic Role Labeling. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.
Yi, Szu-ting and Martha Palmer. 2005. The Integra-
tion of Syntactic Parsing and Semantic Role Label-
ing. In Proceedings of the Conference of Computa-
tional Natural Language Learning.
247
Discriminative Parse Reranking for Chinese with Homogeneous and
Heterogeneous Annotations
Weiwei Sun?? and Rui Wang? and Yi Zhang??
?Department of Computational Linguistics, Saarland University
?German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
{wsun,rwang,yzhang}@coli.uni-saarland.de
Abstract
Discriminative parse reranking has been
shown to be an effective technique to im-
prove the generative parsing models. In
this paper, we present a series of exper-
iments on parsing the Tsinghua Chinese
Treebank with hierarchically split-merge
grammars and reranked with a perceptron-
based discriminative model. In addition to
the homogeneous annotation on TCT, we
also incorporate the PCTB-based parsing
result as heterogeneous annotation into
the reranking feature model. The rerank-
ing model achieved 1.12% absolute im-
provement on F1 over the Berkeley parser
on a development set. The head labels in
Task 2.1 are annotated with a sequence
labeling model. The system achieved
80.32 (B+C+H F1) in CIPS-SIGHAN-
2010 Task 2.1 (Open Track) and 76.11
(Overall F1) in Task 2.2 (Open Track)1.
1 Introduction
The data-driven approach to syntactic analysis of
natural language has undergone revolutionary de-
velopment in the last 15 years, ever since the
first few large scale syntactically annotated cor-
pora, i.e. treebanks, became publicly available in
the mid-90s of the last century. One and a half
decades later, treebanks remain to be an expensive
type of language resources and only available for
1This result is achieved with a bug-fixed version of the
system and does not correspond to the numbers in the origi-
nal evaluation report.
a small number of languages. The main issue that
hinders large treebank development projects is
the difficulties in creating a complete and consis-
tent annotation guideline which then constitutes
the very basis for sustainable parallel annotation
and quality assurance. While traditional linguistic
studies typically focus on either isolated language
phenomena or limited interaction among a small
groups of phenomena, the annotation scheme in
treebanking project requires full coverage of lan-
guage use in the source media, and proper treat-
ment with an uniformed annotation format. Such
high demand from the practical application of lin-
guistic theory has given rise to a countless num-
ber of attempts and variations in the formaliza-
tion frameworks. While the harsh natural selec-
tion set the bar high and many attempts failed to
even reach the actual annotation phase, a hand-
ful highly competent grammar frameworks have
given birth to several large scale treebanks.
The co-existence of multiple treebanks with
heterogeneous annotation presents a new chal-
lenge to the consumers of such resources. The im-
mediately relevant task is the automated syntactic
analysis, or parsing. While many state-of-the-art
statistical parsing systems are not bound to spe-
cific treebank annotation (assuming the formal-
ism is predetermined independently), almost all
of them assume homogeneous annotation in the
training corpus. Therefore, such treebanks can not
be simply put together when training the parser.
One approach would be to convert them into an
uniformed representation, although such conver-
sion is usually difficult and by its nature error-
prune. The differences in annotations constitute
different generative stories: i.e., when the pars-
ing models are viewed as mechanisms to produce
structured sentences, each treebank model will as-
sociate its own structure with the surface string in-
dependently. On the other hand, if the discrimina-
tive view is adopted, it is possible to use annota-
tions in different treebanks as indication of good-
ness of the tree in the original annotation.
In this paper, we present a series of experi-
ments to improve the Chinese parsing accuracy
on the Tsinghua Chinese Treebank. First, we use
coarse-to-fine parsing with hierarchically split-
merge generative grammars to obtain a list of can-
didate trees in TCT annotation. A discriminative
parse selection model is then used to rerank the
list of candidates. The reranking model is trained
with both homogeneous (TCT) and heterogeneous
(PCTB) data. A sequence labeling system is used
to annotate the heads in Task 2-1.
The remaining part of the paper is organized as
follows. Section 2 reviews the relevant previous
study on generative split-merge parsing and dis-
criminative reranking models. Section 3 describes
the work flow of our system participated in the
CIPS-SIGHAN-2010 bake-off Task 2. Section 4
describes the detailed settings for the evaluation
and the empirical results. Section 5 concludes the
paper.
2 Background
Statistical constituent-based parsing is popular-
ized through the decade-long competition on pars-
ing the Wall Street Journal sections of the English
Penn Treebank. While the evaluation setup has
for long seen its limitation (a frustratingly low
of 2% overall improvement throughout a decade
of research), the value of newly proposed pars-
ing methods along the way has clearly much more
profound merits than the seemly trivial increase in
evaluation figures. In this section we review two
effective techniques in constituent-based statisti-
cal parsing, and their potential benefits in parsing
Chinese.
Comparing with many other languages, statisti-
cal parsing for Chinese has reached early success,
due to the fact that the language has relatively
fixed word order and extremely poor inflectional
morphology. Both facts allow the PCFG-based
statistical modeling to perform well. On the other
hand, the much higher ambiguity between basic
word categories like nouns and verbs makes Chi-
nese parsing interestingly different from the situ-
ation of English.
The type of treebank annotations also affects
the performance of the parsing models. Tak-
ing the Penn Chinese Treebank (PCTB; Xue
et al (2005)) and Tsinghua Chinese Treebank
(TCT; Zhou (2004)) as examples, PCTB is anno-
tated with a much more detailed set of phrase cat-
egories, while TCT uses a more fine-grained POS
tagset. The asymmetry in the annotation informa-
tion is partially due to the difference of linguis-
tic treatment. But more importantly, it shows that
both treebanks have the potential of being refined
with more detailed classification, on either phrasal
or word categories. One data-driven approach to
derive more fine-grained annotation is the hierar-
chically split-merge parsing (Petrov et al, 2006;
Petrov and Klein, 2007), which induces subcat-
egories from coarse-grained annotations through
an expectation maximization procedure. In com-
bination with the coarse-to-fine parsing strategy,
efficient inference can be done with a cascade
of grammars of different granularity. Such pars-
ing models have reached (close to) state-of-the-art
performance for many languages including Chi-
nese and English.
Another effective technique to improve parsing
results is discriminative reranking (Charniak and
Johnson, 2005; Collins and Koo, 2005). While
the generative models compose candidate parse
trees, a discriminative reranker reorders the list
of candidates in favor of those trees which max-
imizes the properties of being a good analysis.
Such extra model refines the original scores as-
signed by the generative model by focusing its de-
cisions on the fine details among already ?good?
candidates. Due to this nature, the set of features
in the reranker focus on those global (and poten-
tially long distance) properties which are difficult
to model with the generative model. Also, since
it is not necessary for the reranker to generate the
candidate trees, one can easily integrate additional
external information to help adjust the ranking of
the analysis. In the following section, we will de-
Berkeley 
Parser
...
Parse 
Reranker
TCT
Head
Classifier
...
H
H
H
A B
C D
C
D B
A
C
Task 2.1
Task 2.2
Open
e.g. ?? ??? ? ??
PCTB
Parser
Figure 1: Workflow of the System
scribe the reranking model we developed for the
CIPS-SIGHAN-2010 parsing tasks. We will also
show how the heterogeneous parsing results can
be integrated through the reranker to further im-
prove the performance of the system.
3 System Description
In this section, we will present our approach
in detail. The whole system consists of three
main components, the Berkeley Parser, the Parse
Reranker, and the Head Classifier. The workflow
is shown in Figure 1. Firstly, we use the Berke-
ley Parser trained on the TCT to parse the in-
put sentence and obtain a list of possible parses;
then, all the parses2 will be re-ranked by the Parse
Reranker; and finally, the Head Classifer will an-
notate the head information for each constituent
2In practice, we only take the top n parses. We have dif-
ferent n values in the experiment settings, and n is up to 50.
Algorithm 1: The Perptron learning proce-
dure.
input : Data {(xt, yt), t = 1, 2, ...,m}
Initialize: w? (0, ..., 0)1
for i = 1, 2, ..., I do2
for t =SHUFFLE (1, ...,m) do3
y?t =4
arg maxy?GENbestn (xt) w
>?(xt, y)
if y?t 6= yt then5
w? w+(?(xt, yt)??(xt, y?t ))6
end7
end8
wi ? w9
end10
return aw = 1I
?I
i=1 wi11
on the best parse tree. For parse reranking, we
can extract features either from TCT-style parses
or together with the PCTB-style parse of the same
sentence. For example, we can check whether
the boundary predictions given by the TCT parser
are agreed by the PCTB parser. Since the PCTB
parser is trained on a different treebank from TCT,
our reranking model can be seen as a method to
use a heterogenous resource. The best parse tree
given by the Parse Reranker will be the result for
Task 2.2; and the final output of the system will
be the result for Task 2.1. Since we have already
mentioned the Berkeley Parser in the related work,
we will focus on the other two modules in the rest
of this section.
3.1 Parse Reranker
We follow Collins and Koo (2005)?s discrimina-
tive reranking model to score possible parse trees
of each sentence given by the Berkeley Parser.
Previous research on English shows that struc-
tured perceptron (Collins, 2002) is one of the
strongest machine learning algorithms for parse
reranking (Collins and Duffy, 2002; Gao et al,
2007). In our system, we use the averaged per-
ceptron algorithm to do parameter estimation. Al-
gorithm 1 illustrates the learning procedure. The
parameter vector w is initialized to (0, ..., 0). The
learner processes all the instances (t is from 1 to
n) in each iteration (i). If current hypothesis (w)
fails to predict xt, the learner update w through
calculating the difference between ?(xt, y?t ) and
?(xt, yt). At the end of each iteration, the learner
save the current model as w + i, and finally all
these models will be added up to get aw.
3.2 Features
We use an example to show the features we extract
in Figure 2.
vp
v
?
eat
np
v
?
buy
uJDE
?
n
??
apple
Figure 2: An Example
Rules The context-free rule itself:
np? v + uJDE + np.
Grandparent Rules Same as the Rules, but
also including the nonterminal above the rule:
vp(np? v + uJDE + np)
Bigrams Pairs of nonterminals from the left to
right of the the rule. The example rule would con-
tribute the bigrams np(STOP, v), np(v,uJDE),
np(uJDE,np) and np(np, STOP).
Grandparent Bigrams Same as Bigrams, but
also including the nonterminal above the bigrams.
For instance, vp(np(STOP, v))
Lexical Bigrams Same as Bigrams, but with
the lexical heads of the two nonterminals also in-
cluded. For instance, np(STOP,?).
Trigrams All trigrams within the rule. The
example rule would contribute the trigrams
np(STOP, STOP, v), np(STOP, v,uJDE),
np(v,uJDE,np), np(uJDE,np,STOP) and
np(np,STOP,STOP).
Combination of Boundary Words and
Rules The first word and the rule (i.e.
?+(np? v + uJDE + np)), the last word
and the rule one word before and the rule, one
word after and the rule, the first word, the last
word and the rule, and the first word?s POS, last
word?s POS and the rule.
Combination of Boundary Words and Phrasal
Category : Same as combination of boundary
words and rules, but substitute the rule with the
category of current phrases.
Two level Rules Same as Rules, but also
including the entire rule above the rule:
vp? v + (np? v + uJDE + np)
Original Rank : The logarithm of the original
rank of n-best candidates.
Affixation features In order to better handle
unknown words, we also extract morphologi-
cal features: character n-gram prefixes and suf-
fixes for n up to 3. For example, for word/tag
pair ????/n, we add the following fea-
tures: (prefix1,?,n), (prefix2,??,n), (prefix3,?
??,n), (suffix1,?,n), (suffix2,??,n), (suf-
fix3,???,n).
Apart from training the reranking model using
the same dataset (i.e. the TCT), we can also use
another treebank (e.g. the PCTB). Although they
have quite different annotations as well as the data
source, it would still be interesting to see whether
a heterogenous resource is helpful with the parse
reranking.
Consist Category If a phrase is also analyzed
as one phrase by the PCTB parser, both the TCT
and PCTB categories are used as two individual
features. The combination of the two categories
are also used.
Inconsist Category If a phrase is not analyzed
as one phrase by the PCTB parser, the TCT cate-
gory is used as a feature.
Number of Consist and Inconsist phrases The
two number are used as two individual featuers.
We also use the ratio of the number of consist
phrases and inconsist phrase (we add 0.1 to each
number for smoothing), the ratio of the number
of consist/inconsist phrases and the length of the
current sentence.
POS Tags For each word, the combination of
TCT and PCTB POS tags (with or without word
content) are used.
3.3 Head Classifier
Following (Song and Kit, 2009), we apply a se-
quence tagging method to find head constituents.
We suggest readers to refer to the original paper
for details of the method. However, since the fea-
ture set is different, we give the discription of
them in this paper. To predict whether current
phrase is a head phrase of its parent, we use the
same example above (Figure 2) for convenience.
If we consider np as our current phrase, the fol-
lowing features are extracted,
Rules The generative rule, vp? v + (np).
Category of the Current Phrase and its Parent
np, vp, and (np, vp).
Bigrams and Trigrams (v, np), (np,STOP),
(STOP, v,np), and (np,STOP,STOP).
Parent Bigrams and Trigrams vp(v, np),
vp(np,STOP), vp(STOP, v, np),
vp(np,STOP,STOP).
Lexical Unigram The first word ?, the last
word ??, and together with the parent, (vp,?)
and (vp,??)
4 Evaluation
4.1 Datasets
The dataset used in the CIPS-ParsEval-2010 eval-
uation is converted from the Tsinghua Chinese
Treebank (TCT). There are two subtasks: (1)
event description sub-sentence analysis and (2)
complete sentence parsing. On the assumption
that the boundaries and relations between these
event description units are determined separately,
the first task aims to identify the local fine-grained
syntactic structures. The goal of the second task
is to evaluate the performance of the automatic
parsers on complete sentences in real texts. The
training dataset is a mixture of several genres, in-
cluding newspaper texts, encyclopedic texts and
novel texts.
The annotation in the dataset is different to
the other frequently used Chinese treebank (i.e.
PCTB) Whereas TCT annotation strongly reflects
early descriptive linguistics, PCTB draws primar-
ily on Government-Binding (GB) theory from
1980s. PCTB annotation differs from TCT anno-
tation from many perspectives:
? TCT and PCTB have different segmentation
standards.
? TCT is somehow branching-rich annota-
tion, while PCTB annotation is category-
rich. Specifically the topological tree struc-
tures is more detailed in TCT, and there
are not many flat structures. However con-
stituents are detailed classified, namely the
number of phrasal categories is small. On the
contrary, though flat structures are very com-
mon in PCTB, the categorization of phrases
is fine-grained. In addition, PCTB contains
functional information. Function tags ap-
pended to constituent labels are used to in-
dicate additional syntactic or semantic infor-
mation.
? TCT contains head indices, making head
identification of each constituent an impor-
tant goal of task 1.
? Following the GB theory, PCTB assume
there are movements, so there are empty cat-
egory annotation. Because of different theo-
retical foundations, there are different expla-
nations for a series of linguistic phenomena
such as the usage of function word ???.
In the reranking experiments, we also use a
parser trained on PCTB to provide more syntac-
tic clues.
4.2 Setting
In order to gain a representative set of training
data, we use cross-validation scheme described in
(Collins, 2000). The dataset is a mixture of three
genres. We equally split every genre data into 10
subsets, and collect three subset of different gen-
res as one fold of the whole data. In this way, we
can divide the whole data into 10 balanced sub-
sets. For each fold data, a complement parser is
trained using all other data to produce multiple hy-
potheses for each sentence. This cross-validation
n 1 2 5 10 20 30 40 50
F1 79.97 81.62 83.51 84.63 85.59 86.07 86.38 86.60
Table 1: Upper bound of f-score as a function of number n of n-best parses.
scheme can prevent the initial model from being
unrealistically ?good? on the training sentences.
We use the first 9 folds as training data and the last
fold as development data for the following exper-
iments. For the final submission of the evaluation
task, we re-train a reranking model using all 10
folds data. All reranking models are trained with
30 iterations.
For parsing experiments, we use the Berkeley
parser3. All parsers are trained with 5 iterations
of split, merge, smooth. To produce PCTB-style
analysis, we train the Berkeley parse with PCTB
5.0 data that contains 18804 sentences and 508764
words. For the evaluation of development experi-
ments, we used the EVALB tool4 for evaluation,
and used labeled recall (LR), labeled precision
(LP) and F1 score (which is the harmonic mean
of LR and LP) to measure accuracy.
For the head classification, we use SVMhmm5,
an implementation of structural SVMs for se-
quence tagging. The main setting of learning pa-
rameter is C that trades off margin size and train-
ing error. In our experiments, the head classifica-
tion is not sensitive to this parameter and we set
it to 1 for all experiments reported. For the kernel
function setting, we use the simplest linear kernel.
4.3 Results
4.3.1 Upper Bound of Reranking
The upper bound of n-best parse reranking is
shown in Table 1. From the 1-best result we see
that the base accuracy of the parser is 79.97. 2-
best and 10-best show promising oracle-rate im-
provements. After that things start to slow down,
and we achieve an oracle rate of 86.60 at 50-best.
4.3.2 Reranking Using Homogeneous Data
Table 2 summarizes the performance of the ba-
sic reranking model. It is evaluated on short sen-
3http://code.google.com/p/
berkeleyparser/
4http://nlp.cs.nyu.edu/evalb/
5http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
tences (less than 40 words) from the development
data of the task 2. When 40 reranking candidates
are used, the model gives a 0.76% absolute im-
provement over the basic Berkeley parser.
POS(%) LP(%) LR(%) F1
Baseline 93.59 85.60 85.36 85.48
n = 2 93.66 85.84 85.54 85.69
n = 5 93.62 86.04 85.73 85.88
n = 10 93.66 86.22 85.85 86.04
n = 20 93.70 86.19 85.87 86.03
n = 30 93.70 86.32 86.00 86.16
n = 40 93.76 86.40 86.09 86.24
n = 50 93.73 86.10 85.81 85.96
Table 2: Reranking performance with different
number of parse candidates on the sentences that
contain no more than 40 words in the development
data.
4.3.3 Reranking Using Heterogeneous Data
Table 3 summarizes the reranking performance
using PCTB data. It is also evaluated on short sen-
tences of the task 2. When 30 reranking candi-
dates are used, the model gives a 1.12% absolute
improvement over the Berkeley parser. Compar-
ison of Table 2 and 3 shows an improvement by
using heterogeneous data.
POS(%) LP(%) LR(%) F1
n = 2 93.70 85.98 85.67 85.82
n = 5 93.75 86.52 86.19 86.35
n = 10 93.77 86.64 86.29 86.47
n = 20 93.79 86.71 86.34 86.53
n = 30 93.80 86.72 86.48 86.60
n = 40 93.80 86.54 86.22 86.38
n = 50 93.89 86.73 86.41 86.57
Table 3: Reranking performance with different
number of parse candidates on the sentences that
contain no more than 40 words in the development
data.
Task 1 ?B+C?-P ?B+C?-R ?B+C?-F1 ?B+C+H?-P ?B+C+H?-R ?B+C+H?-F1 POS
Old data 82.37 83.05 82.71 79.99 80.65 80.32 81.87
Table 4: Final results of task 1.
Task 2 dj-P dj-R dj-F1 fj-P fj-R fj-F1 Avg. POS
Old data 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23
New data 79.60 79.13 79.36 70.01 75.94 72.85 76.11 89.05
Table 5: Final results of task 2.
4.3.4 Head Classification
The head classification performance is evalu-
ated using gold-standard syntactic trees. For each
constituent in a gold parse tree, a structured clas-
sifier is trained to predict whether it is a head con-
stituent of its parent. Table 6 shows the overall
performance of head classification. We can see
that the head classification can achieve a high per-
formance.
P(%) R(%) F?=1
98.59% 98.20% 98.39
Table 6: Head classification performance with
gold trees on the development data.
4.3.5 Final Result
Table 4 and 5 summarize the final results. Here
we use the reranking model with heterogeneous
data. The second line of Table 5 shows the offi-
cal final results. In this submission, we trained a
model using an old version of training data. Note
that, the standard of POS tags of the ?old? version
is different from the latest version which is also
used as test data. For example, the name of some
tags are changed. The third line of Table 46 shows
the results predicted by the newest data7. This re-
sult is comparable to other systems.
5 Conclusion
In this paper, we described our participation of
the CIPS-SIGHAN-2010 parsing task. The gen-
6There are two sentences that are not parsed by the Berke-
ley parser. We use a simple strategy to solve this problem:
We first roughly segment the sentence according to punctu-
ation; Then the parsed sub-sentences are merged as a single
zj.
7We would like to thank the organizer to re-test our new
submission.
erative coarse-to-fine parsing model is integrated
with a discriminative parse reranking model, as
well as a head classifier based on sequence la-
beling. We use the perceptron algorithm to train
the reranking models and experiment with both
homogenous and heterogenous data. The results
show improvements over the baseline in both
cases.
Acknowledgments
The first author is supported by the German Aca-
demic Exchange Service (DAAD). The second
author is supported by the PIRE scholarship pro-
gram; the third author thanks DFKI and the Clus-
ter of Excellence on Multimodal Computing and
Interaction for their support of the work.
References
Charniak, E. and M Johnson. 2005. oarse-to-fine n-
best parsing and maxent discriminative reranking.
In Proceedings of ACL, pages 173?180.
Collins, Michael and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In
Proceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Collins, Michael and Terry Koo. 2005. Discriminative
reranking for natural language parsing. In Compu-
tational Linguistics, volume 31(1), pages 25?69.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Computational Lin-
guistics, pages 175?182. Morgan Kaufmann.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, pages
1?8, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Gao, Jianfeng, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Petrov, S. and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL-2007, Rochester, NY, USA, April.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Song, Yan and Chunyu Kit. 2009. Pcfg parsing with
crf tagging for head recognition. In Proceedings of
the CIPS-ParsEval-2009.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Zhou, Qiang. 2004. Annotation scheme for chinese
treebank (in chinese). Journal of Chinese Informa-
tion Processing, 18(4):1?8.
