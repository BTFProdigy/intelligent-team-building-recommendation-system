Proceedings of NAACL HLT 2007, pages 508?515,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Statistical Phrase-based Post-editing
Michel Simard Cyril Goutte
Interactive Language Technologies
National Research Council of Canada
Gatineau, Canada, K1A 0R6
FirstName.LastName@nrc.gc.ca
Pierre Isabelle
Abstract
We propose to use a statistical phrase-
based machine translation system in a
post-editing task: the system takes as in-
put raw machine translation output (from
a commercial rule-based MT system), and
produces post-edited target-language text.
We report on experiments that were per-
formed on data collected in precisely such
a setting: pairs of raw MT output and
their manually post-edited versions. In our
evaluation, the output of our automatic
post-editing (APE) system is not only bet-
ter quality than the rule-based MT (both
in terms of the BLEU and TER metrics),
it is also better than the output of a state-
of-the-art phrase-based MT system used
in standalone translation mode. These re-
sults indicate that automatic post-editing
constitutes a simple and efficient way of
combining rule-based and statistical MT
technologies.
1 Introduction
The quality of machine translation (MT) is gener-
ally considered insufficient for use in the field with-
out a significant amount of human correction. In the
translation world, the term post-editing is often used
to refer to the process of manually correcting MT
output. While the conventional wisdom is that post-
editing MT is usually not cost-efficient compared to
full human translation, there appear to be situations
where it is appropriate and even profitable. Unfortu-
nately, there are few reports in the literature about
such experiences (but see Allen (2004) for exam-
ples).
One of the characteristics of the post-editing task,
as opposed to the revision of human translation for
example, is its partly repetitive nature. Most MT
systems invariably produce the same output when
confronted with the same input; in particular, this
means that they tend to make the same mistakes over
and over again, which the post-editors must correct
repeatedly. Batch corrections are sometimes pos-
sible when multiple occurrences of the same mis-
take appear in the same document, but when it is
repeated over several documents, or equivalently,
when the output of the same machine translation
system is handled by multiple post-editors, then the
opportunities for factoring corrections become much
more complex. MT users typically try to reduce
the post-editing load by customizing their MT sys-
tems. However, in Rule-based Machine Translation
(RBMT), which still constitutes the bulk of the cur-
rent commercial offering, customization is usually
restricted to the development of ?user dictionaries?.
Not only is this time-consuming and expensive, it
can only fix a subset of the MT system?s problems.
The advent of Statistical Machine Translation,
and most recently phrase-based approaches (PBMT,
see Marcu and Wong (2002), Koehn et al (2003))
into the commercial arena seems to hold the promise
of a solution to this problem: because the MT sys-
tem learns directly from existing translations, it can
be automatically customized to new domains and
tasks. However, the success of this operation cru-
508
cially depends on the amount of training data avail-
able. Moreover, the current state of the technology
is still insufficient for consistently producing human
readable translations.
This state of affairs has prompted some to ex-
amine the possibility of automating the post-editing
process itself, at least as far as ?repetitive errors? are
concerned. Allen and Hogan (2000) sketch the out-
line of such an automated post-editing (APE) sys-
tem, which would automatically learn post-editing
rules from a tri-parallel corpus of source, raw MT
and post-edited text. Elming (2006) suggests using
tranformation-based learning to automatically ac-
quire error-correcting rules from such data; however,
the proposed method only applies to lexical choice
errors. Knight and Chander (1994) also argue in fa-
vor of using a separate APE module, which is then
portable across multiple MT systems and language
pairs, and suggest that the post-editing task could be
performed using statistical machine translation tech-
niques. To the best of our knowledge, however, this
idea has never been implemented.
In this paper, we explore the idea of using a
PBMT system as an automated post-editor. The un-
derlying intuition is simple: if we collect a paral-
lel corpus of raw machine-translation output, along
with its human-post-edited counterpart, we can train
the system to translate from the former into the lat-
ter. In section 2, we present the case study that mo-
tivates our work and the associated data. In section
3, we describe the phrase-based post-editing model
that we use for improving the output of the auto-
matic translation system. In section 4, we illus-
trate this on a dataset of moderate size containing
job ads and their translation. With less than 500k
words of training material, the phrase-based MT
system already outperforms the rule-based MT base-
line. However, a phrase-based post-editing model
trained on the output of that baseline outperforms
both by a fairly consistent margin. The resulting
BLEU score increases by up to 50% (relative) and
the TER is cut by one third.
2 Background
2.1 Context
The Canadian government?s department of Human
Resources and Social Development (HRSDC) main-
tains a web site called Job Bank,1 where poten-
tial employers can post ads for open positions in
Canada. Over one million ads are posted on Job
Bank every year, totalling more than 180 million
words. By virtue of Canada?s Official Language Act,
HRSDC is under legal obligation to post all ads in
both French and English. In practice, this means
that ads submitted in English must be translated into
French, and vice-versa.
To address this task, the department has put to-
gether a complex setup, involving text databases,
translation memories, machine translation and hu-
man post-editing. Employers submit ads to the Job
Bank website by means of HTML forms containing
?free text? data fields. Some employers do period-
ical postings of identical ads; the department there-
fore maintains a database of previously posted ads,
along with their translations, and new ads are sys-
tematically checked against this database. The trans-
lation of one third of all ads posted on the Job Bank
is actually recuperated this way. Also, employers
will often post ads which, while not entirely identi-
cal, still contain identical sentences. The department
therefore also maintains a translation memory of in-
dividual sentence pairs from previously posted ads;
another third of all text is typically found verbatim
in this way.
The remaining text is submitted to machine trans-
lation, and the output is post-edited by human ex-
perts. Overall, only a third of all submitted text re-
quires human intervention. This is nevertheless very
labour-intensive, as the department tries to ensure
that ads are posted at most 24 hours after submis-
sion. The Job Bank currently employs as many as
20 post-editors working full-time, most of whom are
junior translators.
2.2 The Data
HRSDC kindly provided us with a sample of data
from the Job Bank. This corpus consists in a collec-
tion of parallel ?blocks? of textual data. Each block
contains three parts: the source language text, as
submitted by the employer, its machine-translation,
produced by a commercial rule-based MT system,
and its final post-edited version, as posted on the
website.
1http://www.jobbank.gc.ca
509
The entire corpus contains less than one million
words in each language. This corresponds to the
data processed in less than a week by the Job Bank.
Basic statistics are given in Table 1 (see Section 4.1).
Most blocks contain only one sentence, but some
blocks may contain many sentences. The longest
block contains 401 tokens over several sentences.
Overall, blocks are quite short: the median number
of tokens per source block is only 9 for French-to-
English and 7 for English-to-French. As a conse-
quence, no effort was made to segment the blocks
further for processing.
We evaluated the quality of the Machine Transla-
tion contained in the corpus using the Translation
Edit Rate (TER, cf. Snover et al (2006)). The
TER counts the number of edit operations, including
phrasal shifts, needed to change a hypothesis trans-
lation into an adequate and fluent sentence, and nor-
malised by the length of the final sentence. Note
that this closely corresponds to the post-editing op-
eration performed on the Job Bank application. This
motivates the choice of TER as the main metric in
our case, although we also report BLEU scores in
our experiments. Note that the emphasis of our work
is on reducing the post-edition effort, which is well
estimated by TER. It is not directly on quality so the
question of which metric better estimates translation
quality is not so relevant here.
The global TER (over all blocks) are 58.77%
for French-to-English and 53.33% for English-to-
French. This means that more than half the words
have to be post-edited in some way (delete / substi-
tute / insert / shift). This apparently harsh result is
somewhat mitigated by two factors.
First, the distribution of the block-based TER2
shows a large disparity in performance, cf. Figure 1.
About 12% of blocks have a TER higher than 100%:
this is because the TER normalises on the length of
the references, and if the raw MT output is longer
than its post-edited counterpart, then the number of
edit operations may be larger than that length.3 At
the other end of the spectrum, it is also clear that
many blocks have low TER. In fact more than 10%
2Contrary to BLEU or NIST, the TER naturally decomposes
into block-based scores.
3A side effect of the normalisation is that larger TER are
measured on small sentences, e.g. 3 errors for 2 reference
words.
Histogram of TER for rule?based MT
TER for rule?based MT
Fr
eq
ue
nc
y
0 50 100 150
0
10
00
20
00
30
00
40
00
Figure 1: Distribution of TER on 39005 blocks from
the French-English corpus (thresholded at 150%).
have a TER of 0. The global score therefore hides a
large range of performance.
The second factor is that the TER measures the
distance to an adequate and uent result. A high
TER does not mean that the raw MT output is not
understandable. However, many edit operations may
be needed to make it fluent.
3 Phrase-based Post-editing
Translation post-editing can be viewed as a simple
transformation process, which takes as input raw
target-language text coming from a MT system, and
produces as output target-language text in which ?er-
rors? have been corrected. While the automation
of this process can be envisaged in many differ-
ent ways, the task is not conceptually very differ-
ent from the translation task itself. Therefore, there
doesn?t seem to be any good reason why a machine
translation system could not handle the post-editing
task. In particular, given such data as described in
Section 2.2, the idea of using a statistical MT system
for post-editing is appealing. Portage is precisely
such a system, which we describe here.
Portage is a phrase-based, statistical machine
translation system, developed at the National Re-
search Council of Canada (NRC) (Sadat et al,
510
2005). A version of the Portage system is made
available by the NRC to Canadian universities for
research and education purposes. Like other SMT
systems, it learns to translate from existing parallel
corpora.
The system translates text in three main phases:
preprocessing of raw data into tokens; decoding to
produce one or more translation hypotheses; and
error-driven rescoring to choose the best final hy-
pothesis. For languages such as French and English,
the first of these phases (tokenization) is mostly a
straightforward process; we do not describe it any
further here.
Decoding is the central phase in SMT, involv-
ing a search for the hypotheses t that have high-
est probabilities of being translations of the cur-
rent source sentence s according to a model for
P (t|s). Portage implements a dynamic program-
ming beam search decoding algorithm similar to that
of Koehn (2004), in which translation hypotheses
are constructed by combining in various ways the
target-language part of phrase pairs whose source-
language part matches the input. These phrase pairs
come from large phrase tables constructed by col-
lecting matching pairs of contiguous text segments
from word-aligned bilingual corpora.
Portage?s model for P (t|s) is a log-linear com-
bination of four main components: one or more n-
gram target-language models, one or more phrase
translation models, a distortion (word-reordering)
model, and a sentence-length feature. The phrase-
based translation model is similar to that of Koehn,
with the exception that phrase probability estimates
P (s?|t?) are smoothed using the Good-Turing tech-
nique (Foster et al, 2006). The distortion model is
also very similar to Koehn?s, with the exception of a
final cost to account for sentence endings.
Feature function weights in the loglinear model
are set using Och?s minium error rate algorithm
(Och, 2003). This is essentially an iterative two-step
process: for a given set of source sentences, generate
n-best translation hypotheses, that are representative
of the entire decoding search space; then, apply a
variant of Powell?s algorithm to find weights that op-
timize the BLEU score over these hypotheses, com-
pared to reference translations. This process is re-
peated until the set of translations stabilizes, i.e. no
new translations are produced at the decoding step.
To improve raw output from decoding, Portage re-
lies on a rescoring strategy: given a list of n-best
translations from the decoder, the system reorders
this list, this time using a more elaborate loglinear
model, incorporating more feature functions, in ad-
dition to those of the decoding model: these typ-
ically include IBM-1 and IBM-2 model probabili-
ties (Brown et al, 1993) and an IBM-1-based fea-
ture function designed to detect whether any word
in one language appears to have been left without
satisfactory translation in the other language; all of
these feature functions can be used in both language
directions, i.e. source-to-target and target-to-source.
In the experiments reported in the next section,
the Portage system is used both as a translation and
as an APE system. While we can think of a number
of modifications to such a system to better adapt it
to the post-editing task (some of which are discussed
later on), we have done no such modifications to the
system. In fact, whether the system is used for trans-
lation or post-editing, we have used exactly the same
translation model configuration and training proce-
dure.
4 Evaluation
4.1 Data and experimental setting
The corpus described in section 2.2 is available for
two language pairs: English-to-French and French-
to-English.4 In each direction, each block is avail-
able in three versions (or slices): the original text
(or source), the output of the commercial rule-based
MT system (or baseline) and the final, post-edited
version (or reference).
In each direction (French-to-English and English-
to-French), we held out two subsets of approxi-
mately 1000 randomly picked blocks. The valida-
tion set is used for testing the impact of various high-
level choices such as pre-processing, or for obtain-
ing preliminary results based on which we setup new
experiments. The test set is used only once, in order
to obtain the final experimental results reported here.
The rest of the data constitutes the training set,
which is split in two. We sampled a subset of
1000 blocks as train-2, which is used for optimiz-
4Note that, in a post-editing context, translation direction is
crucially important. It is not possible to use the same corpus in
both directions.
511
English-to-French French-to-English
Corpus words: words:
blocks source baseline reference blocks source baseline reference
train-1 28577 310k 382k 410k 36005 485k 501k 456k
train-2 1000 11k 14k 14k 1000 13k 14k 12k
validation 881 10k 13k 13k 966 13k 14k 12k
test 899 10k 12k 13k 953 13k 13k 12k
Table 1: Data and split used in our experiments, (in thousand words). ?baseline? is the output of the com-
mercial rule-based MT system and ?reference? is the final, post-edited text.
ing the log-linear model parameters used for decod-
ing and rescoring. The rest is the train-1 set, used
for estimating IBM translation models, constructing
phrasetables and estimating a target language model.
The composition of the various sets is detailed in
Table 1. All data was tokenized and lowercased;
all evaluations were performed independent of case.
Note that the validation and test sets were originally
made out of 1000 blocks sampled randomly from
the data. These sets turned out to contain blocks
identical to blocks from the training sets. Consider-
ing that these would normally have been handled by
the translation memory component (see the HRSDC
workflow description in Section 2.1), we removed
those blocks for which the source part was already
found in the training set (in either train-1 or train-2),
hence their smaller sizes.
In order to check the sensitivity of experimental
results to the choice of the train-2 set, we did a
run of preliminary experiments using different sub-
sets of 1000 blocks. The experimental results were
nearly identical and highly consistent, showing that
the choice of a particular train-2 subset has no in-
fluence on our conclusions. In the experiments re-
ported below, we therefore use a single identical
train-2 set.
We initially performed two sets of experiments
on this data. The first was intended to compare the
performance of the Portage PBMT system as an al-
ternative to the commercial rule-based MT system
on this type of data. In these experiments, English-
to-French and French-to-English translation systems
were trained on the source and reference (manually
post-edited target language) slices of the training set.
In addition to the target language model estimated
on the train-1 data, we used an external contribution,
Language TER BLEU
English-to-French
Baseline 53.5 32.9
Portage translation 53.7 36.0
Baseline + Portage APE 47.3 41.6
French-to-English
Baseline 59.3 31.2
Portage translation 43.9 41.0
Baseline + Portage APE 41.0 44.9
Table 2: Experimental Results: For TER, lower (er-
ror) is better, while for BLEU, higher (score) is bet-
ter. Best results are in bold.
a trigram target language model trained on a fairly
large quantity of data from the Canadian Hansard.
The goal of the second set of experiments was to
assess the potential of the Portage technology in au-
tomatic post-editing mode. Again, we built systems
for both language directions, but this time using the
existing rule-based MT output as source and the ref-
erence as target. Apart from the use of different
source data, the training procedure and system con-
figurations of the translation and post-editing sys-
tems were in all points identical.
4.2 Experimental results
The results of both experiments are presented in Ta-
ble 2. Results are reported both in terms of the TER
and BLEU metrics; Baseline refers to the commer-
cial rule-based MT output.
The first observation from these results is that,
while the performance of Portage in translation
mode is approximately equivalent to that of the base-
line system when translating into French, its perfor-
mance is much better than the baseline when trans-
lating into English. Two factors possibly contribute
512
to this result: first, the fact that the baseline system
itself performs better when translating into French;
second, and possibly more importantly, the fact that
we had access to less training data for English-to-
French translation.
The second observation is that when Portage is
used in automatic post-editing mode, on top of the
baseline MT system, it achieves better quality than
either of the two translation systems used on its own.
This appears to be true regardless of the translation
direction or metric. This is an extremely interesting
result, especially in light of how little data was actu-
ally available to train the post-editing system.
One aspect of statistical MT systems is that, con-
trary to rule-based systems, their performance (usu-
ally) increases as more training data is available. In
order to quantify this effect in our setting, we have
computed learning curves by training the Portage
translation and Portage APE systems on subsets of
the training data of increasing sizes. We start with
as little as 1000 blocks, which corresponds to around
10-15k words.
Figure 2 (next page) compares the learning rates
of the two competing approaches (Portage transla-
tion vs. Portage APE). Both approaches display very
steady learning rates (note the logarithmic scale for
training data size). These graphs strongly suggest
that both systems would continue to improve given
more training data. The most impressive aspect is
how little data is necessary to improve upon the
baseline, especially when translating into English:
as little as 8000 blocks (around 100k words) for di-
rect translation and 2000 blocks (around 25k words)
for automatic post-editing. This suggests that such
a post-editing setup might be worth implementing
even for specialized domains with very small vol-
umes of data.
4.3 Extensions
Given the encouraging results of the Portage APE
approach in the above experiments, we were curi-
ous to see whether a Portage+Portage combination
might be as successful: after all, if Portage was good
at correcting some other system?s output, could it
not manage to correct the output of another Portage
translator?
We tested this in two settings. First, we actu-
ally use the output of the Portage translation sys-
Language TER BLEU
English-to-French
Portage Job Bank 53.7 36.0
+ Portage APE 53.7 36.2
Portage Hansard 76.9 13.0
+ Portage APE 64.6 26.2
French-to-English
Portage Job Bank 43.9 41.0
+ Portage APE 43.9 41.4
Portage Hansard 80.1 14.0
+ Portage APE 57.7 28.6
Table 3: Portage translation - Portage APE system
combination experimental results.
tem obtained above, i.e. trained on the same data.
In our second experiment, we use the output of
a Portage translator trained on different domain
data (the Canadian Hansard), but with much larger
amounts of training material (over 85 million words
per language). In both sets of experiments, the
Portage APE system was trained as previously, but
using Portage translations of the Job Bank data as
input text.
The results of both experiments are presented in
Table 3. The first observation in these results is that
there is nothing to be gained from post-editing when
both the translation and APE systems are trained on
the same data sets (Portage Job Bank + Portage APE
experiments). In other words, the translation system
is apparently already making the best possible use of
the training data, and additional layers do not help
(but nor do they hurt, interestingly).
However, when the translation system has been
trained using distinct data (Portage Hansard +
Portage APE experiments), post-editing makes a
large difference, comparable to that observed with
the rule-based MT output provided with the Job
Bank data. In this case, however, the Portage trans-
lation system behaves very poorly in spite of the im-
portant size of the training set for this system, much
worse in fact than the ?baseline? system. This high-
lights the fact that both the Job Bank and Hansard
data are very much domain-specific, and that access
to appropriate training material is crucial for phrase-
based translation technology.
In this context, combining two phrase-based sys-
513
1000 2000 5000 10000 20000
40
45
50
55
60
TER learning curves
Training set size
TE
R
to English
to French
Post?edition
Translation
1000 2000 5000 10000 20000
0.
30
0.
35
0.
40
0.
45
BLEU learning curves
Training set size
BL
EU
to English
to French
Post?edition
Translation
Figure 2: TER and BLEU scores of the phrase-based post-editing models as the amount of training data
increases (log scale). The horizontal lines correspond to the performance of the baseline system (rule-based
translation).
tems as done here can be seen as a way of adapting
an existing MT system to a new text domain; the
APE system then acts as an ?adapter?, so to speak.
Note however that, in our experiments, this setup
doesn?t perform as well as a single Portage transla-
tion system, trained directly and exclusively on the
Job Bank data.
Such an adaptation strategy should be contrasted
with one in which the translation models of the
old and new domains are ?merged? to create a new
translation system. As mentioned earlier, Portage
allows using multiple phrase translation tables and
language models concurrently. For example, in the
current context, we can extract phrase tables and lan-
guage models from the Job Bank data, as when train-
ing the ?Portage Job Bank? translation system, and
then build a Portage translation model using both the
Hansard and Job Bank model components. Loglin-
ear model parameters are then optimized on the Job
Bank data, so as to find the model weights that best
fit the new domain.
In a straightforward implementation of this idea,
we obtained performances almost identical to those
of the Portage translation system trained solely on
Job Bank data. Upon closer examination of the
model parameters, we observed that Hansard model
components (language model, phrase tables, IBM
translation models) were systematically attributed
negligeable weights. Again, the amount of training
material for the new domain may be critical in chos-
ing between alternative adaptation mechanisms.
5 Conclusions and Future Work
We have proposed using a phrase-based MT sys-
tem to automatically post-edit the output of an-
other MT system, and have tested this idea with
the Portage MT system on the Job Bank data set, a
corpus of manually post-edited French-English ma-
chine translations. In our experiments, not only does
phrase-based APE significantly improve the quality
of the output translations, this approach outperforms
a standalone phrase-based translation system.
While these results are very encouraging, the
learning curves of Figure 2 suggest that the output
quality of the PBMT systems increases faster than
that of the APE systems as more data is used for
training. So while the combination strategy clearly
performs better with limited amounts of training
data, there is reason to believe that, given sufficient
training data, it would eventually be outperformed
514
by a direct phrase-based translation strategy. Of
course, this remains to be verified empirically, some-
thing which will obviously require more data than is
currently available to us. But this sort of behavior
is expectable: while both types of system improve
as more training data is used, inevitably some de-
tails of the source text will be lost by the front-end
MT system, which the APE system will never be
able to retrieve.5 Ultimately, the APE system will
be weighted down by the inherent limitations of the
front-end MT system.
One way around this problem would be to modify
the APE system so that it not only uses the base-
line MT output, but also the source-language input.
In the Portage system, this could be achieved, for
example, by introducing feature functions into the
log-linear model that relate target-language phrases
with the source-language text. This is one research
avenue that we are currently exploring.
Alternatively, we could combine these two in-
puts differently within Portage: for example, use
the source-language text as the primary input, and
use the raw MT output as a secondary source. In
this perspective, if we have multiple MT systems
available, nothing precludes using all of them as
providers of secondary inputs. In such a setting, the
phrase-based system becomes a sort of combination
MT system. We intend to explore such alternatives
in the near future as well.
Acknowledgements
The work reported here was part of a collaboration
between the National Research Council of Canada
and the department of Human Resources and Social
Development Canada. Special thanks go to Souad
Benayyoub, Jean-Fre?de?ric Hu?bsch and the rest of
the Job Bank team at HRSDC for preparing data that
was essential to this project.
References
Jeffrey Allen and Christofer Hogan. 2000. Towardthe development of a post-editing module for Ma-chine Translation raw output: a new productivity tool
for processing controlled language. In Third Inter-
5As a trivial example, imagine an MT system that ?deletes?
out-of-vocabulary words.
national Controlled Language Applications Workshop(CLAW2000), Washington, USA.
Jeffrey Allen. 2004. Case study: Implementing MT forthe translation of pre-sales marketing and post-salessoftware deployment documentation. In Proceedingsof AMTA-2004, pages 1?6, Washington, USA.
Peter F Brown, Stephen A Della Pietra, Vincent J DellaPietra, and Robert L Mercer. 1993. The Mathematicsof Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Jakob Elming. 2006. Transformation-based correctionsof rule-based MT. In Proceedings of the EAMT 11thAnnual Conference, Oslo, Norway.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical MachineTranslation. In Proceedings of EMNLP 2006, pages53?61, Sydney, Australia.
Kevin Knight and Ishwar Chander. 1994. Automated
Postediting of Documents. In Proceedings of NationalConference on Artificial Intelligence, pages 779?784,Seattle, USA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-ings of HLT-NAACL 2003, pages 127?133, Edmonton,Canada.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-tion Models. In Proceedings of AMTA 2004, pages115?124, Washington, USA.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-chine Translation. In Proceedings of EMNLP 2002,Philadelphia, USA.
Franz Josef Och. 2003. Minimum error rate trainingin Statistical Machine Translation. In Proceedings ofACL-2003, pages 160?167, Sapporo, Japan.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.2005. PORTAGE: A Phrase-Based Machine Trans-lation System. In Proceedings of the ACL Workshopon Building and Using Parallel Texts, pages 129?132,
Ann Arbor, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-notation. In Proceedings of AMTA-2006, Cambridge,USA.
515
Proceedings of the Second Workshop on Statistical Machine Translation, pages 203?206,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rule-based Translation With Statistical Phrase-based Post-editing
Michel Simard, Nicola Ueffing, Pierre Isabelle and Roland Kuhn
Interactive Language Technologies Group
National Research Council of Canada
Gatineau, Canada, K1A 0R6
firstname.lastname@nrc-cnrc.gc.ca
Abstract
This article describes a machine translation
system based on an automatic post-editing
strategy: initially translate the input text into
the target-language using a rule-based MT
system, then automatically post-edit the out-
put using a statistical phrase-based system.
An implementation of this approach based
on the SYSTRAN and PORTAGE MT sys-
tems was used in the shared task of the Sec-
ond Workshop on Statistical Machine Trans-
lation. Experimental results on the test data
of the previous campaign are presented.
1 Introduction
Simard et al (2007) have recently shown how a sta-
tistical phrase-based machine translation system can
be used as an automatic post-editing (APE) layer,
on top of a rule-based machine translation system.
The motivation for their work is the repetitive nature
of the errors typically made by rule-based systems.
Given appropriate training material, a statistical MT
system can be trained to correct these systematic er-
rors, therefore reducing the post-editing effort. The
statistical system views the output of the rule-based
system as the source language, and reference hu-
man translations as the target language. Because the
training material for the APE layer will typically be
domain-specific, this process can be viewed as a way
of automatically adapting a rule-based system to a
specific application domain.
This approach has been shown experimentally
to produce large improvements in performance not
only over the baseline rule-based system that it cor-
rects, but also over a similar statistical phrase-based
MT system used in standalone mode, i.e. translating
the ?real? source text directly: Simard et al report a
reduction in post-editing effort of up to a third when
compared to the input rule-based translation, and as
much as 5 BLEU points improvement over the direct
SMT approach.
These impressive results, however, were obtained
in a very specific and somewhat unusual context:
the training and test corpora were extracted from
a collection of manually post-edited machine trans-
lations. The two corpora (one English-to-French,
one French-to-English) each contained three paral-
lel ?views? of the same data: 1) the source language
text, 2) a machine translation of that text into the
target language, as produced by a commercial rule-
based MT system, and 3) the final target-language
version of the text, produced by manually post-
editing the machine translation. Furthermore, the
corpus was very small, at least by SMT standards:
500K words of source-language data in the French-
to-English direction, 350K words in the English-to-
French. Because of this, the authors were left with
two important questions: 1) how would the results
scale up to much larger quantities of training data?
and 2) are the results related to the dependent nature
of the translations, i.e. is the automatic post-editing
approach still effective when the machine and hu-
man translations are produced independently of one
another?
With these two questions in mind, we partici-
pated in the shared task of the Second Workshop
on Statistical Machine Translation with an auto-
matic post-editing strategy: initially translate the in-
put text into the target-language using a rule-based
system, namely SYSTRAN, and automatically post-
edit the output using a statistical phrase-based sys-
tem, namely PORTAGE. We describe our system in
more detail in Section 2, and present some experi-
mental results in Section 3.
203
2 System description
Our system is composed of two main components:
a rule-based MT system, which handles the initial
translation into the target language, and a statistical
phrase-based post-editing system, which performs
domain-specific corrections and adaptations to the
output. We describe each component separately be-
low.
2.1 Rule-based Translation
The initial source-to-target language translation is
performed using the SYSTRAN machine translation
system, version 6. A detailed overview of SYS-
TRAN systems can be found in Dugast et al (2007).
For this shared task, we used the French-to-English
and English-to-French configurations of the system.
Although it is possible to provide the system with
specialized lexica, we did not rely on this feature,
and used the system in its basic ?out-of-the-box?
configuration.
2.2 Statistical Phrase-based Post-Editing
The output of the rule-based MT system described
above is fed into a post-editing layer that performs
domain-specific corrections and adaptation. This
operation is conceptually not very different from a
?target-to-target? translation; for this task, we used
the PORTAGE system, a state-of-the-art statistical
phrase-based machine translation system developed
at the National Research Council of Canada (NRC).1
A general description of PORTAGE can be found in
(Sadat et al, 2005).
For our participation in this shared task, we de-
cided to configure and train the PORTAGE system
for post-editing in a manner as much as possible
similar to the corresponding translation system, the
details of which can be found in (Ueffing et al,
2007). The main features of this configuration are:
? The use of two distinct phrase tables, contain-
ing phrase pairs extracted from the Europarl
and the News Commentary training corpora re-
spectively.
? Multiple phrase-probability feature functions
in the log-linear models, including a joint prob-
1A version of PORTAGE is made available by the NRC to
Canadian universities for research and education purposes.
ability estimate, a standard frequency-based
conditional probability estimate, and variants
thereof based on different smoothing methods
(Foster et al, 2006).
? A 4-gram language model trained on the com-
bined Europarl and News Commentary target-
language corpora.
? A 3-gram adapted language model: this is
trained on a mini-corpus of test-relevant target-
language sentences, extracted from the training
material using standard information retrieval
techniques.
? A 5-gram truecasing model, trained on the
combined Europarl and News Commentary
target-language corpora.
2.3 Training data
Ideally, the training material for the post-editing
layer of our system should consist in a corpus of
text in two parallel versions: on the one hand, raw
machine translation output, and on the other hand,
manually post-edited versions of these translations.
This is the type of data that was used in the initial
study of Simard et al (2007).
Unfortunately, this sort of training data is seldom
available. Instead, we propose using training ma-
terial derived directly from standard, source-target
parallel corpora. The idea is to translate the source
portion of the parallel corpus into the target lan-
guage, using the rule-based MT component. The
post-editing component can then be trained using
this translation as ?source? training material, and the
existing target portion of the parallel corpus as ?tar-
get? training material. Note how this sort of data
is subtly different from the data used by Simard et
al.: there, the ?target? text was dependent on the
?source?, in the sense that it was produced by manu-
ally post-editing the machine translation; here, the
two can be said to be independent, in the sense
that both ?source? and ?target? were produced inde-
pendently by man and machine (but from the same
?real? source, of course). It was one of the initial
motivations of the current work to verify to what ex-
tent the performance of the APE approach is affected
by using two different translations (human and ma-
204
en ? fr fr ? en
Europarl (>32M words/language)
SYSTRAN 23.06 20.11
PORTAGE 31.01 30.90
SYSTRAN+PORTAGE 31.11 30.61
News Commentary (1M words/language)
SYSTRAN 24.41 18.09
PORTAGE 25.98 25.17
SYSTRAN+PORTAGE 28.80 26.79
Table 1: System performances on WMT-06 test. All
figures are single-reference BLEU scores, computed
on truecased, detokenized translations.
chine) instead of two versions of the same transla-
tion (raw MT versus post-edited MT).
We concentrated our efforts on the English-
French language pair. For each translation direc-
tion, we prepared two systems: one for the Eu-
roparl domain, and one for the News Commentary
domain. The two systems have almost identical
configurations (phrase tables, log-linear model fea-
tures, etc.); the only differences between the two
are the adapted language model, which is computed
based on the specific text to be translated and the
parameters of the log-linear models, which are opti-
mized using domain-specific development sets. For
the Europarl domain system, we used the dev2006
and devtest2006 data sets, while for the News Com-
mentary, we used the nc-dev2007. Typically, the
optimization procedure will give higher weights to
Europarl-trained phrase tables for the Europarl do-
main systems, and inversely for the News Commen-
tary domain systems.
3 Experimental Results
We computed BLEU scores for all four systems on
the 2006 test data (test2006 for the Europarl do-
main and nc-devtest2007 for the News Commen-
tary). The results are presented in Table 1. As points
of comparison, we also give the scores obtained by
the SYSTRAN systems on their own (i.e. without a
post-editing layer), and by the PORTAGE MT sys-
tems on their own (i.e. translating directly source
into target).
The first observation is that, as was the case
in the Simard et al study, post-editing (SYS-
TRAN+PORTAGE lines) very significantly in-
creases the BLEU scores of the rule-based system
(SYSTRAN lines). This increase is more spectacu-
lar in the Europarl domain and when translating into
English, but it is visible for all four systems.
For the News Commentary domain, the APE
strategy (SYSTRAN+PORTAGE lines) clearly out-
performs the direct SMT strategy (PORTAGE lines):
translating into English, the gain exceeds 1.5 BLEU
points, while for French, it is close to 3 BLEU
points. In contrast, for the Europarl domain, both ap-
proaches display similar performances. Let us recall
that the News Commentary corpus contains less than
50K sentence pairs, totalling a little over one mil-
lion words in each language. With close to 1.3 mil-
lion sentence pairs, the Europarl corpus is almost 30
times larger. Our results therefore appear to confirm
one of the conjectures of the Simard et al study:
that APE is better suited for domains with limited
quantities of available training data. To better un-
derstand this behavior, we trained series of APE and
SMT systems on the Europarl data, using increas-
ing amounts of training data. The resulting learning
curves are presented in Figure 1.2
As observed in the Simard et al study, while both
the SMT and APE systems improve quite steadily
with more data (note the logarithmic scale), SMT
appears to improve more rapidly than APE. How-
ever, there doesn?t seem to be a clear ?crossover?
point, as initially conjectured by Simard et al In-
stead, SMT eventually catches up with APE (any-
where between 100K and 1M sentence pairs), be-
yond which point both approaches appear to be more
or less equivalent. Again, one impressive feature
of the APE strategy is how little data is actually re-
quired to improve upon the rule-based system upon
which it is built: around 5000 sentence pairs for
English-to-French, and 2000 for French-to-English.
4 Conclusions
We have presented a combination MT system based
on a post-editing strategy, in which a statistical
phrase-based system corrects the output of a rule-
based translation system. Experiments confirm the
2The systems used for this experiment are simplified ver-
sions of those described in Section 2, using only one phrase
table, a trigram language model and no rescoring; furthermore,
they were optimized and tested on short sentences only.
205
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 1  10  100  1000
BL
EU
 s
co
re
Training sentences (x 1000)
English to French
SYSTRAN
PORTAGE
SYSTRAN + PORTAGE
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 1  10  100  1000
BL
EU
 s
co
re
Training sentences (x 1000)
French to English
SYSTRAN
PORTAGE
SYSTRAN + PORTAGE
Figure 1: BLEU scores on Europarl data under increasing amounts of training data for PORTAGE SMT
alone and SYSTRAN MT with PORTAGE APE.
conclusions of earlier studies: not only can phrase-
based post-editing significantly improve the out-
put of a rule-based MT system (in terms of BLEU
score), but when training data is scarce, it also out-
performs a direct phrase-based MT strategy. Fur-
thermore, our results indicate that the training data
for the post-editing component does not need to be
manually post-edited translations, it can be gener-
ated from standard parallel corpora. Finally, our ex-
periments show that while post-editing is most effec-
tive when little training data is available, it remains
competitive with phrase-based translation even with
much larger amounts of data.
This work opens the door to a number of lines of
investigation. For example, it was mentioned earlier
that phrase-based APE could be seen as a form of au-
tomatic domain-adaptation for rule-based methods.
One thing we would like to verify is how this ap-
proach compares to the standard ?lexical customiza-
tion? method proposed by most rule-based MT ven-
dors. Also, in the experiments reported here, we
have used identical configurations for the APE and
direct SMT systems. However, it might be possible
to modify the phrase-based system so as to better
adapt it to the APE task. For example, it could be
useful for the APE layer to ?look? at the real source-
language text, in addition to the MT output it is post-
editing. Finally, we have so far considered the front-
end rule-based system as a ?black box?. But in the
end, the real question is: Which part of the rule-
based processing is really making things easier for
the phrase-based post-editing layer? Answering this
question will likely require diving into the internals
of the rule-based component. These are all direc-
tions that we are currently pursuing.
Acknowledgements
This work was done as part of a collaboration with
SYSTRAN S.A. Many thanks go to Jean Senellart,
Jens Stephan, Dimitris Sabatakakis and all those
people behind the scene at SYSTRAN.
References
L. Dugast, J. Senellart, and P. Koehn. 2007. StatisticalPost-Edition on SYSTRAN Rule-Based TranslationSystem. In Proceedings of the Second Workshop OnStatistical Machine Translation, Prague, Czech Re-
public.
G. Foster, R. Kuhn, and H. Johnson. 2006. PhrasetableSmoothing for Statistical Machine Translation. InProceedings of EMNLP 2006, pages 53?61, Sydney,
Australia.
F. Sadat, H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin, and A. Tikuisis. 2005. PORTAGE: APhrase-Based Machine Translation System. In Pro-ceedings of the ACL Workshop on Building and UsingParallel Texts, pages 129?132, Ann Arbor, USA.
M. Simard, C. Goutte, and P. Isabelle. 2007. Sta-
tistical Phrase-Based Post-Editing. In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 508?515, Rochester, USA.
N. Ueffing, M. Simard, S. Larkin, and H. Johnson. 2007.NRC?s PORTAGE system for WMT 2007. In Pro-ceedings of the Second Workshop On Statistical Ma-chine Translation, Prague, Czech Republic.
206
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 631?642, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Enlarging Paraphrase Collections through Generalization and Instantiation
Atsushi Fujita
Future University Hakodate
116-2 Kameda-nakano-cho,
Hakodate, Hokkaido, 041-8655, Japan
fujita@fun.ac.jp
Pierre Isabelle Roland Kuhn
National Research Council Canada
283 Alexandre-Tache? Boulevard,
Gatineau, QC, J8X 3X7, Canada
{Pierre.Isabelle, Roland.Kuhn}@nrc.ca
Abstract
This paper presents a paraphrase acquisition
method that uncovers and exploits generali-
ties underlying paraphrases: paraphrase pat-
terns are first induced and then used to col-
lect novel instances. Unlike existing methods,
ours uses both bilingual parallel and monolin-
gual corpora. While the former are regarded as
a source of high-quality seed paraphrases, the
latter are searched for paraphrases that match
patterns learned from the seed paraphrases.
We show how one can use monolingual cor-
pora, which are far more numerous and larger
than bilingual corpora, to obtain paraphrases
that rival in quality those derived directly from
bilingual corpora. In our experiments, the
number of paraphrase pairs obtained in this
way from monolingual corpora was a large
multiple of the number of seed paraphrases.
Human evaluation through a paraphrase sub-
stitution test demonstrated that the newly ac-
quired paraphrase pairs are of reasonable qual-
ity. Remaining noise can be further reduced
by filtering seed paraphrases.
1 Introduction
Paraphrases are semantically equivalent expressions
in the same language. Because ?equivalence? is the
most fundamental semantic relationship, techniques
for generating and recognizing paraphrases play an
important role in a wide range of natural language
processing tasks (Madnani and Dorr, 2010).
In the last decade, automatic acquisition of knowl-
edge about paraphrases from corpora has been draw-
ing the attention of many researchers. Typically, the
acquired knowledge is simply represented as pairs of
semantically equivalent sub-sentential expressions
as in (1).
(1) a. look like ? resemble
b. control system ? controller
The challenge in acquiring paraphrases is to ensure
good coverage of the targeted classes of paraphrases
along with a low proportion of incorrect pairs. How-
ever, no matter what type of resource has been used,
it has proven difficult to acquire paraphrase pairs
with both high recall and high precision.
Among various types of corpora, monolingual
corpora can be considered the best source for high-
coverage paraphrase acquisition, because there is
far more monolingual than bilingual text avail-
able. Most methods that exploit monolingual cor-
pora rely on the Distributional Hypothesis (Harris,
1968): expressions that appear in similar contexts
are expected to have similar meaning. However,
if one uses purely distributional criteria, it is dif-
ficult to distinguish real paraphrases from pairs of
expressions that are related in other ways, such as
antonyms and cousin words.
In contrast, since the work in (Bannard and
Callison-Burch, 2005), bilingual parallel corpora
have been acknowledged as a good source of high-
quality paraphrases: paraphrases are obtained by
putting together expressions that receive the same
translation in the other language (pivot language).
Because translation expresses a specific meaning
more directly than context in the aforementioned ap-
proach, pairs of expressions acquired in this manner
tend to be correct paraphrases. However, the cov-
erage problem remains: there is much less bilingual
parallel than monolingual text available.
Our objective in this paper is to obtain para-
phrases that have high quality (like those extracted
from bilingual parallel corpora via pivoting) but can
be generated in large quantity (like those extracted
631
from monolingual corpora via contextual similar-
ity). To achieve this, we propose a method that ex-
ploits general patterns underlying paraphrases and
uses both bilingual parallel and monolingual sources
of information. Given a relatively high-quality set of
paraphrases obtained from a bilingual parallel cor-
pus, a set of paraphrase patterns is first induced.
Then, appropriate instances of such patterns, i.e.,
potential paraphrases, are harvested from a mono-
lingual corpus.
After reviewing existing methods in Section 2,
our method is presented in Section 3. Section 4
describes our experiments in acquiring paraphrases
and presents statistics summarizing the coverage of
our method. Section 5 describes a human evaluation
of the quality of the acquired paraphrases. Finally,
Section 6 concludes this paper.
2 Literature on Paraphrase Acquisition
This section summarizes existing corpus-based
methods for paraphrase acquisition, following the
classification in (Hashimoto et al2011): similarity-
based and alignment-based methods.
2.1 Similarity-based Methods
Techniques that use monolingual (non-parallel) cor-
pora mostly rely on the Distributional Hypothesis
(Harris, 1968). Because a large quantity of mono-
lingual data is available for many languages, a large
number of paraphrase candidates can be acquired
(Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bha-
gat and Ravichandran, 2008, etc.). The recipes pro-
posed so far are based on three main ingredients, i.e.,
features used for representing context of target ex-
pression (contextual features), criteria for weighting
and filtering features, and aggregation functions.
A drawback of relying only on contextual simi-
larity is that it tends to give high scores to semanti-
cally related but non-equivalent expressions, such as
antonyms and cousin words. To enhance the preci-
sion of the results, filtering mechanisms need to be
introduced (Marton et al2011).
2.2 Alignment-based Methods
Pairs of expressions that get translated to the same
expression in a different language can be regarded as
paraphrases. On the basis of this hypothesis, Barzi-
lay and McKeown (2001) and Pang et al2003)
created monolingual parallel corpora from multiple
human translations of the same source. Then, they
extracted corresponding parts of such parallel sen-
tences as sub-sentential paraphrases.
Leveraging recent advances in statistical ma-
chine translation (SMT), Bannard and Callison-
Burch (2005) proposed a method for acquiring sub-
sentential paraphrases from bilingual parallel cor-
pora. As in SMT, a translation table is first built on
the basis of alignments between expressions, such as
words, phrases, and subtrees, across a parallel sen-
tence pair. Then, pairs of expressions (e1, e2) in the
same language that are aligned with the same ex-
pressions in the other language (pivot language) are
extracted as paraphrases. The likelihood of e2 being
a paraphrase of e1 is given by
p(e2|e1) =
?
f?Tr(e1,e2)
p(e2|f)p(f |e1), (1)
where Tr(e1, e2) stands for the set of shared trans-
lations of e1 and e2. Each factor p(e|f) and p(f |e)
is estimated from the number of times e and f are
aligned and the number of occurrences of each ex-
pression in each language. Kok and Brockett (2010)
showed how one can discover paraphrases that do
not share any translation in one language by travers-
ing a graph created from multiple translation tables,
each corresponding to a bilingual parallel corpus.
This approach, however, suffers from a cover-
age problem, because both monolingual parallel and
bilingual parallel corpora tend to be significantly
smaller than monolingual non-parallel corpora. The
acquired pairs of expressions include some non-
paraphrases as well. Many of these come from er-
roneous alignments, which are particularly frequent
when the given corpus is small.
Monolingual comparable corpora have also been
exploited as sources of paraphrases using alignment-
based methods. For instance, multiple news arti-
cles covering the same event (Shinyama et al2002;
Barzilay and Lee, 2003; Dolan et al2004; Wubben
et al2009) have been used. Such corpora have
also been created manually through crowdsourcing
(Chen and Dolan, 2011). However, the availabil-
ity of monolingual comparable corpora is very lim-
ited for most languages; thus, approaches relying
on these corpora have typically produced only very
632
small collections of paraphrases. Hashimoto et al
(2011) found a way around this limitation by collect-
ing sentences that constitute explicit definitions of
particular words or phrases from monolingual non-
parallel Web documents, pairing sentences that de-
fine the same noun phrase, and then finding corre-
sponding phrases in each sentence pair. One limita-
tion of this approach is that it requires a considerable
amount of labeled data for both the corpus construc-
tion and the paraphrase extraction steps.
2.3 Summary
Existing methods have investigated one of the fol-
lowing four types of corpora as their principal re-
source1: monolingual non-parallel corpora, mono-
lingual parallel corpora, monolingual comparable
corpora, and bilingual parallel corpora. No matter
what type of resource has been used, however, it
has proven difficult to acquire paraphrases with both
high recall and precision, with the possible excep-
tion of the method in (Hashimoto et al2011) which
requires large amounts of labeled data.
3 Proposed Method
While most existing methods deal with expressions
only at the surface level, ours exploits generalities
underlying paraphrases to achieve better coverage
while retaining high precision. Furthermore, unlike
existing methods, ours uses both bilingual parallel
and monolingual non-parallel corpora as sources for
acquiring paraphrases.
The process is illustrated in Figure 1. First, a
set of high-quality seed paraphrases, PSeed , is ac-
quired from bilingual parallel corpora by using an
alignment-based method. Then, our method collects
further paraphrases through the following two steps.
Generalization (Step 2): Paraphrase patterns are
learned from the seed paraphrases, PSeed .
Instantiation (Step 3): A novel set of paraphrase
pairs, PHvst , is finally harvested from mono-
lingual non-parallel corpora using the learned
patterns; each newly acquired paraphrase pair
is assessed by contextual similarity.
1Chan et al2011) used monolingual corpora only for re-
ranking paraphrases obtained from bilingual parallel corpora.
To the best of our knowledge, bilingual comparable corpora
have never been used as sources for acquiring paraphrases.
Monolingual Non-parallel Corpus
Step 1. Seed Paraphrase Acquisition
Step 2. Paraphrase Pattern Induction
Step 3. Paraphrase Instance Acquisition
?health issue? ? ?health problem? ?look like? ? ?resemble? ?regional issue? ? ?regional problem? 
?health issue? ? ?probl?me de sant?? ?health problem? ? ?probl?me de sant?? ?look like? ? ?ressemble? ?regional issue? ? ?probl?me r?gional? ?regional problem? ? ?probl?me r?gional? ?resemble? ? ?ressemble? 
?X issue? ? ?X problem?;         {food, regional, ...}
?backlog issue? ? ?backlog problem? ?communal issue? ? ?communal problem? ?phishing issue? ? ?phishing problem? ?spatial issue? ? ?spatial problem?
Translation Table
PSeed: Seed Paraphrases
Paraphrase Patterns
PHvst: Novel Paraphrases
Bilingual Parallel Corpus
Figure 1: Process of paraphrase acquisition.
The set PSeed acquired early in the process can be
pooled with the set PHvst harvested in the last stage
of the process.
3.1 Step 1. Seed Paraphrase Acquisition
The goal of the first step is to obtain a set of high-
quality paraphrase pairs, PSeed .
For this purpose, alignment-based methods with
bilingual or monolingual parallel corpora are prefer-
able to similarity-based methods applied to non-
parallel corpora. Among various options, in this pa-
per, we start from the standard technique proposed
by Bannard and Callison-Burch (2005) with bilin-
gual parallel corpora (see also Section 2.2). In par-
ticular, we assume the phrase-based SMT frame-
work (Koehn et al2003). Then, we purify the re-
sults with several filtering methods.
The phrase pair extraction process of phrase-
based SMT systems aims at high recall for increased
robustness of the translation process. As a result,
a naive application of the paraphrase acquisition
method produces pairs of expressions that are not
exact paraphrases. For instance, the algorithm ex-
plained in Koehn (2009, p.134) extracts both ?dass?
and ?, dass? as counterparts of ?that? from the sen-
tence pair. To reduce that kind of noise, we apply
some filtering techniques to the candidate translation
pairs. First, statistically unreliable translation pairs
(Johnson et al2007) are filtered out. Then, we also
filter out phrases made up entirely of stop words (in-
cluding punctuation marks), both in the language of
interest and in the pivot language.
Let PRaw be the initial set of paraphrase pairs ex-
tracted from the sanitized translation table. We first
633
lp: control apparatus
rp: control devicep(rp|lp)
.172
rp: control system
.032
rp: the control device
.015
rp: control device of the.005
rp: controlling device.004
rp: control system of
.003
rp: a control system for an
.001
rp: a controlling device
.001
Figure 2: RHS-filtering for ?control apparatus?.
rp: control device
lp: controller p(lp|rp)
.153
lp: control apparatus
.135
lp: the control apparatus
.010
lp: control apparatus of .008
lp: controlling unit .004
lp: control equipment
.002
lp: controller for a
.001
lp: to the control apparatus
.001
Figure 3: LHS-filtering for ?control device?.
discard pairs whose difference comprises only stop
words, such as ?the schools? ? ?schools and?. We
also remove pairs containing only singular-plural
differences, such as ?family unit? ? ?family units?.
Depending on the language of interest, other types of
morphological variants, such as those shown in (2),
may also be ignored.
(2) a. ?europe?enne? ? ?europe?en?
(Gender in French)
b. ?guten Lo?sungen? ? ?gute Lo?sungen?
(Case in German)
We further filter out less reliable pairs, such as
those shown with dotted lines in Figures 2 and 3.
This is carried out by comparing the right-hand side
(RHS) phrases of each left-hand side (LHS) phrase,
and vice versa2. Given a set of paraphrase pairs,
RHS phrases corresponding to the same LHS phrase
lp are compared. A RHS phrase rp is not licensed iff
lp has another RHS phrase rp? (?= rp) which satis-
fies the following two conditions (see also Figure 2).
? rp? is a word sub-sequence of rp
? rp? is a more likely paraphrase than rp,
i.e., p(rp ?|lp) > p(rp|lp)
LHS phrases for each RHS phrase rp are also com-
pared in a similar manner, i.e., a LHS phrase lp is
not qualified as a legitimate source of rp iff rp has
another LHS phrase lp? (?= lp) which satisfies the
following conditions (see also Figure 3).
? lp? is a word sub-sequence of lp
? lp? is a more likely source than lp,
i.e., p(lp ?|rp) > p(lp|rp)
The two directions of filtering are separately applied
and the intersection of their results is retained.
2cf. Denkowski and Lavie (2011); they only compared each
RHS phrase to its corresponding LHS phrase.
Candidate pairs are finally filtered on the basis
of their reliability score. Traditionally, a threshold
(thp) on the conditional probability given by Eq. (1)
is used (Du et al2010; Max, 2010; Denkowski
and Lavie, 2011, etc.). Furthermore, we also re-
quire that LHS and RHS phrases exceed a thresh-
old (ths ) on their contextual similarity in a mono-
lingual corpus. This paper neither proposes a spe-
cific recipe nor makes a comprehensive comparison
of existing recipes for computing contextual simi-
larity, although one particular recipe is used in our
experiments (see Section 4.1).
3.2 Step 2. Paraphrase Pattern Induction
From a set of seed paraphrases, PSeed , paraphrase
patterns are induced. For instance, from paraphrases
in (3), we induce paraphrase patterns in (4).
(3) a. ?restraint system? ? ?restraint apparatus?
b. ?movement against racism?
? ?anti-racism movement?
c. ?middle eastern countries?
? ?countries in the middle east?
(4) a. ?X system? ? ?X apparatus?
b. ?X against Y ? ? ?anti-Y X?
c. ?X eastern Y ? ? ?Y in the X east?
Word pairs of LHS and RHS phrases will be re-
placed with variable slots iff they are fully identi-
cal or singular-plural variants. Note that stop words
are retained. While a deeper level of lexical cor-
respondences, such as ?eastern? and ?east? in (3c)
and ?system? and ?apparatus? in (3a), could be cap-
tured, this would require the use of rich language
resources, thereby making the method less portable
to resource-poor languages.
634
Note that our aim is to automatically capture gen-
eral paraphrase patterns of the kind that have some-
times been manually described (Jacquemin, 1999;
Fujita et al2007). This is different from ap-
proaches that attach variable slots to paraphrases for
calculating their similarity (Lin and Pantel, 2001;
Szpektor and Dagan, 2008) or for constraining
the context in which they are regarded legitimate
(Callison-Burch, 2008; Zhao et al2009).
3.3 Step 3. Paraphrase Instance Acquisition
Given a set of paraphrase patterns, such as those
shown in (4), a set of novel instances, i.e., novel
paraphrases, PHvst , will now be harvested from
monolingual non-parallel corpora. In other words,
a set of appropriate slot-fillers will be extracted.
First, expressions that match both elements of
the pattern, except stop words, are collected from
a given monolingual corpus. Pattern matching alone
may generate inappropriate pairs, so we then assess
the legitimacy of each collected slot-filler.
Let LHS (w) and RHS (w) be the expressions
generated by instantiating the k variable slots in
LHS and RHS phrases of the pattern with a k-tuple
of slot-fillers w (= w1, . . . , wk), respectively. We
estimate how likelyRHS (w) is to be a paraphrase of
LHS (w) based on the contextual similarity between
them using a monolingual corpus; a pair of phrases
is discarded if they are used in substantially dissim-
ilar contexts. We use the same recipe and threshold
value for ths with Step 1 in our experiments.
Contextual similarity of antonyms and cousin
words can also be high, as they are often used in sim-
ilar contexts. However, this is not a problem in our
framework, because semantic equivalence between
LHS (w) and RHS (w) is almost entirely guaran-
teed as a result of the way the corresponding patterns
were learned from a bilingual parallel corpus.
3.4 Characteristics
In terms of coverage, PHvst is expected to be greatly
larger than PSeed , although it will not cover to-
tally different pairs of paraphrases, such as those
shown in (1). On the other hand, the quality of
PHvst depends on that of PSeed . Unlike in the pure
similarity-based method, PHvst is constrained by the
paraphrase patterns derived from the set of high-
quality paraphrases, PSeed , and will therefore gen-
erally exclude the kind of semantically similar but
non-equivalent pairs that contextual similarity alone
tends to extract alongside real paraphrases.
As mentioned in Section 3.1, other types of meth-
ods can be used for obtaining high-quality seed
paraphrases, PSeed . For instance, the supervised
method proposed by Hashimoto et al2011) uses
the existence of shared words as a feature to deter-
mine whether the given pair of expressions are para-
phrases, and thereby extracts many pairs sharing the
same words. Thus, their output has a high potential
to be used as an alternative seed for our method.
Another advantage of our method is that it does
not require any labeled data, unlike the super-
vised methods proposed by Zhao et al2009) and
Hashimoto et al2011).
4 Quantitative Impact
4.1 Experimental Settings
Two different sets of corpora were used as data
sources; in both settings, we acquired English para-
phrases.
Europarl: The English-French version of the Eu-
roparl Parallel Corpus3 consisting of 1.8M sen-
tence pairs (51M words in English and 56M
words in French) was used as a bilingual par-
allel corpus, while its English side and the En-
glish side of the 109 French-English corpus4
consisting of 23.8M sentences (649M words)
were used as monolingual data.
Patent: The Japanese-English Patent Translation
data (Fujii et al2010) consisting of 3.2M sen-
tence pairs (122M morphemes in Japanese and
106Mwords in English) was used as a bilingual
parallel corpus, while its English side and the
30.0M sentences (626M words) from the 2007
chapter of NTCIR unaligned patent documents
were used as monolingual data.
To study the behavior of our method for different
amounts of bilingual parallel data, we carried out
learning curve experiments.
We used our in-house tokenizer for segmentation
of English and French sentences and MeCab5 for
Japanese sentences.
3http://statmt.org/europarl/, release 6
4http://statmt.org/wmt10/training-giga-fren.tar
5http://mecab.sourceforge.net/, version 0.98
635
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# of words in the English side of bilingual corpus
PRawPRaw (thp=0.01)PSeed (thp=?, ths=?)PSeed (thp=0.01, ths=?)
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# of words in the English side of bilingual corpus
PRawPRaw (thp=0.01)PSeed (thp=?, ths=?)PSeed (thp=0.01, ths=?)
Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent).
Stop word lists for sanitizing translation pairs and
paraphrase pairs were manually compiled: we enu-
merated 442 English words, 193 French words, and
149 Japanese morphemes, respectively.
From a bilingual parallel corpus, a translation ta-
ble was created by our in-house phrase-based SMT
system, PORTAGE (Sadat et al2005). Phrase
alignments of each sentence pair were identified by
the heuristic ?grow-diag-final?6 with a maximum
phrase length 8. The resulting translation pairs were
then filtered with the significance pruning technique
of (Johnson et al2007), using ? + ? as threshold.
As contextual features for computing similarity
of each paraphrase pair, all of the 1- to 4-grams of
words adjacent to each occurrence of a phrase were
counted. This is a compromise between less expen-
sive but noisier approaches, such as bag-of-words,
and more accurate but more expensive approaches
that incorporate syntactic features (Lin and Pantel,
2001; Shinyama et al2002; Pang et al2003;
Szpektor and Dagan, 2008). Contextual similarity is
finally measured by taking cosine between two fea-
ture vectors.
4.2 Statistics on Acquired Paraphrases
Seed Paraphrases (PSeed )
Figure 4 shows the number of paraphrase pairs
PSeed obtained from the bilingual parallel corpora.
The general trend is simply that the larger the cor-
pus is, the more paraphrases are acquired.
Given the initial set of paraphrases, PRaw (???),
our filtering techniques (?
2
?) discarded a large por-
tion (63-75% in Europarl and 43-64% in Patent) of
them. Pairs with zero similarity were also filtered
out, i.e., ths = ?. This suggests that many incorrect
6http://statmt.org/moses/?n=FactoredTraining.AlignWords
and/or relatively useless pairs, such as those shown
in Figures 2 and 3, had originally been acquired.
Lines with ??? show the results based on a
widely-used threshold value on the conditional prob-
ability in Eq. (1), i.e., thp = 0.01 (Du et al2010;
Max, 2010; Denkowski and Lavie, 2011, etc.). The
percentage of paraphrase pairs thereby discarded
varied greatly depending on the corpus size (17-78%
in Europarl and 31-82% in Patent), suggesting that
the threshold value should be determined depending
on the given corpus. In the following experiment,
however, we conform to the convention thp = 0.01
(???) to ensure the quality of PSeed that we will be
using for inducing paraphrase patterns, even though
this results in discarding some less frequent but cor-
rect paraphrase pairs, such as ?control apparatus?
? ?controlling device? in Figure 2.
Paraphrase Patterns
Figures 5 and 6 show the number of paraphrase
patterns that our method induced and their cover-
age against PSeed , respectively. Due to their rather
rigid form, the patterns covered no more than 15%
of PSeed in Europarl. In contrast, a higher propor-
tion of PSeed in Patent was generalized into patterns.
We speculate it is because the patent domain con-
tains many expressions, including technical terms,
that have similar variations of constructions.
The acquired patterns were mostly one-variable
patterns: 88-93% and 80-91% of total patterns for
different variants of the Europarl and Patent set-
tings, respectively. Given that there are far more
one-variable patterns than other types, and that one-
variable patterns are the simplest type, we hence-
forth focus on them. More complex patterns, includ-
ing two-variable patterns (7-11% and 8-17% in each
setting), will be investigated in our future work.
636
102
103
104
105
106
106 107 108
# o
f pa
rap
hra
se 
pat
tern
s
# of words in the English side of bilingual corpus
All (Patent)1-var (Patent)All (Europarl)1-var (Europarl)
Figure 5: # of paraphrase patterns.
 0
 10
 20
 30
 40
106 107 108
Co
ver
age
 [%]
# of words in the English side of bilingual corpus
All (Patent)1-var (Patent)All (Europarl)1-var (Europarl)
Figure 6: Coverage of the paraphrase patterns.
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# o
f un
iqu
e L
HS
 ph
ras
es
# of words in the English side of bilingual corpus
Pair in PHvstLHS in PHvstPair in PSeedLHS in PSeed
18.1M
1.22M
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# o
f un
iqu
e L
HS
 ph
ras
es
# of words in the English side of bilingual corpus
Pair in PHvstLHS in PHvstPair in PSeedLHS in PSeed
28.7M
1.41M
Figure 7: # of paraphrase pairs and unique LHS phrases in PSeed and PHvst (left: Europarl, right: Patent).
Novel Paraphrases (PHvst )
Using the paraphrase patterns, novel paraphrase
pairs, PHvst , were harvested from the monolingual
non-parallel corpora. In this experiment, we only
retained one-variable patterns and regarded only sin-
gle words as slot-fillers for them. Nevertheless, we
managed to acquire a large number of paraphrase
pairs as depicted in Figure 7, where pairs having
zero similarity were excluded. For instance, when
the full size of bilingual parallel corpus in Patent was
used, we acquired 1.41M pairs of seed paraphrases,
PSeed , and 28.7M pairs of novel paraphrases, PHvst .
In other words, our method expanded PSeed by about
21 times. The number of unique LHS phrases that
PHvst covers was also significantly larger than that
of PSeed .
Figure 8 highlights the remarkably large ratio of
PHvst to PSeed in terms of the number of paraphrase
pairs and the number of unique LHS phrases. The
smaller the bilingual corpus is, the higher the ratio
is, except when there is only a very small amount of
Europarl data. This demonstrates that our method is
quite powerful, given a minimum amount of data.
Another striking difference between PSeed and
PHvst is the average number of RHS phrases per
unique LHS phrase, i.e., their relative yield. As
displayed in Figure 9, the yield for PHvst increased
rapidly with the scaling up of the bilingual cor-
pus, while that of PSeed only grew slowly. The
alignment-based method with bilingual corpora can-
not produce very many RHS phrases per unique
LHS phrase due to its reliance on conditional prob-
ability and the surface level processing. In con-
trast, our method does not limit the number of RHS
phrases: each RHS phrase is separately assessed by
its similarity to the corresponding LHS phrase. One
limitation of our method is that it cannot achieve
high yield for PHvst whenever only a small num-
ber of paraphrase patterns can be extracted from the
bilingual corpus (see also Figure 5).
Both the ratio of PHvst to PSeed and the relative
yield could probably be increased by scaling up the
monolingual corpus. For instance, in the patent do-
main, monolingual documents 10 times larger than
the one used in the above experiments are avail-
able at the NTCIR project7. It would be interesting
to compare the relative gains brought by in-domain
versus general-purpose corpora.
7http://ntcir.nii.ac.jp/PatentMT-2/
637
 0
 20
 40
 60
 80
106 107 108
Ra
tio 
of P
Hv
st t
o P
Se
ed
# of words in the English side of bilingual corpus
LHS (Patent)Pair (Patent)LHS (Europarl)Pair (Europarl)
Figure 8: Ratio of PHvst to PSeed .
 1
 2
 3
 4
 5
106 107 108
Av
g. #
 of 
RH
S p
hra
ses
# of words in the English side of bilingual corpus
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)
Figure 9: Average # of RHS phrases per LHS phrase.
103
104
105
106
107
108
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
# o
f pa
rap
hra
se 
pai
rs
Probability threshold thp
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl) 103
104
105
106
107
108
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
# o
f pa
rap
hra
se 
pai
rs
Similarity threshold ths
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)
Figure 10: # of acquired paraphrase pairs against threshold values.
(left: probability-based (0.01 ? thp ? 0.9, ths = ?), right: similarity-based (? ? ths ? 0.9, thp = 0.01))
Finally, we investigated how the number of para-
phrase pairs varies depending on the values for the
two thresholds, i.e., thp on the conditional probabil-
ity and ths on the contextual similarity, respectively.
Figure 10 shows the results when the full sizes of
bilingual corpora are used. When the pairs were fil-
tered only with thp , the number of paraphrase pairs
in PHvst decreased more slowly than that of PSeed
according to the increase of the threshold value. This
is a benefit from our generalization and instantiation
method. The same paraphrase pattern is often in-
duced from more than one paraphrase pair in PSeed .
Thus, as long as at least one of them has a proba-
bility higher than the given threshold value, corre-
sponding novel paraphrases can be harvested.
On the other hand, as a results of assessing each
individual paraphrase pair by the contextual similar-
ity, many pairs in PHvst , which are supposed to be
incorrect instances of their corresponding pattern,
are filtered out by a larger threshold value for ths .
In contrast, many pairs in PSeed have a relatively
high similarity, e.g., 40% of all pairs have similarity
higher than 0.4. This indicates the quality of PSeed
is highly guaranteed by the shared translations.
5 Human Evaluation of Quality
To confirm that the quality of PHvst is sufficiently
high, we carried out a substitution test.
First, by substituting sub-sentential paraphrases
to existing sentences in a given test corpus, pairs
of slightly different sentences were automatically
generated. For instance, by applying ?looks like?
? ?resembles? to (5), (6) was generated.
(5) The roof looks like a prehistoric lizard?s spine.
(6) The roof resembles a prehistoric lizard?s spine.
Human evaluators were then asked to score each
pair of an original sentence and a paraphrased sen-
tence with the following two 5-point scale grades
proposed by Callison-Burch (2008):
Grammaticality: whether the paraphrased sen-
tence is grammatical (1: horrible, 5: perfect)
Meaning: whether the meaning of the original sen-
tence is properly retained by the paraphrased
sentence (1: totally different, 5: equivalent)
To make results more consistent and reduce the
human labor, evaluators were asked to rate at the
same time several paraphrases for the same source
phrase. For instance, given a source sentence (5), the
638
evaluators might be given the following sentences in
addition to a paraphrased sentence (6).
(7) The roof seems like a prehistoric lizard?s spine.
(8) The roofwould look like a prehistoric lizard?s spine.
In this experiment, we showed five paraphrases
per source phrase, assuming that evaluators would
get confused if too large a number of paraphrase
candidates were presented at the same time.
5.1 Data for Evaluation
As in previous work (Callison-Burch, 2008; Chan
et al2011), we evaluated paraphrases acquired
from the Europarl corpus on news sentences. Para-
phrase examples were automatically generated from
the English part ofWMT 2008-2011 ?newstest? data
(10,050 unique sentences) by applying the union of
PSeed and PHvst of the Europarl setting (19.3M para-
phrases for 5.95M phrases).
On the other hand, paraphrases acquired from
patent documents are much more difficult to eval-
uate due to the following reasons. First, they may
be too domain-specific to be of any use in general
areas such as news sentences. However, conduct-
ing an in-domain evaluation would be difficult with-
out enrolling domain experts. We expect that para-
phrases from a domain can be used safely in that
domain. Nevertheless, deciding under what circum-
stances they can be used safely in another domain is
an interesting research question.
To reduce the human labor for the evaluation, sen-
tences were restricted to those with moderate length:
10-30 words, which are expected to provide suf-
ficient but succinct context. To propose multiple
paraphrase candidates at the same time, we also re-
stricted phrases to be paraphrased (LHS phrases) to
those having at least five paraphrases including ones
from PHvst . This resulted in 60,421 paraphrases for
988 phrase tokens (353 unique phrases).
Finally, we randomly sampled 80 unique phrase
tokens and five unique paraphrases for each phrase
token (400 examples in total), and asked six people
having a high level of English proficiency to evalu-
ate them. Inter-evaluator agreement was calculated
from five different pairs of evaluators, each judging
the same 10 examples. The remaining 350 exam-
ples were divided into six chunks of slightly unequal
length, with each chunk being judged by one of the
six evaluators.
5-point Binary
n G M G M Both
PSeed 55 4.60 4.35 0.85 0.93 0.78
PHvst 295 4.22 3.35 0.74 0.67 0.55
Total 350 4.28 3.50 0.76 0.71 0.58
Table 1: Avg. score and precision of binary classification.
5.2 Results
Table 1 shows the average of the original 5-point
scale scores and the percentage of examples that
are judged correct based on a binary judgment
(Callison-Burch, 2008): an example is considered to
be correct iff the grammaticality score is 4 or above
and/or the meaning score is 3 or above. Paraphrases
based on PSeed achieved a quite high performance
in both grammaticality (?G?) and meaning (?M?) in
part because of the effectiveness of our filtering tech-
niques. The performance of paraphrases drawn from
PHvst was reasonably high and similar to the scores
0.68 for grammaticality, 0.61 for meaning, and 0.55
for both, of the best model reported in (Callison-
Burch, 2008), although it was inferior to PSeed .
Despite the fact that all of our evaluators had a
high-level command of English, the agreement was
not very high. This was true even when the col-
lected scores were mapped into binary classes. In
this case, the ? values (Cohen, 1960) for each crite-
rion were 0.45 and 0.45, respectively, which indicate
the agreement was ?fair?. To obtain a better ? value,
the criteria for grading will need to be improved.
However, we think that was not too low either8.
The most promising way for improving the qual-
ity of PHvst is to ensure that paraphrase patterns
cover only legitimate paraphrases. We investigated
this by filtering the manually scored paraphrase ex-
amples with two thresholds for cleaning seed para-
phrases PSeed : thp on the conditional probability es-
timated using the bilingual parallel corpus and ths
on the contextual similarity in the monolingual non-
parallel corpus. Figure 11 shows the average score
of the examples whose corresponding paraphrase is
obtainable with the given threshold values. Note that
the points in the figure with higher threshold values
are less reliable than the others, because filtering re-
duces the number of the manually scored examples
8Note that Callison-Burch (2008) might possibly underesti-
mate the chance agreement and overestimate the ? values, be-
cause the distribution of human scores would not be uniform.
639
 3
 3.5
 4
 4.5
 5
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
g. s
cor
e
Probability threshold thp
Grammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)
 3
 3.5
 4
 4.5
 5
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
g. s
cor
e
Similarity threshold ths
Grammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)
Figure 11: Average score of paraphrase examples against threshold values.
(left: probability-based (0.01 ? thp ? 0.9, ths = ?), right: similarity-based (? ? ths ? 0.9, thp = 0.01))
The points with higher threshold values are less reliable than the others,
because filtering reduces the number of the manually scored examples used to calculate scores.
used to calculate scores. Nevertheless, it indicates
that better filtering of PSeed with higher threshold
values is likely to produce a better-quality set of
paraphrases PHvst . For instance, an inappropriate
paraphrase pattern (9a) was excluded with thp = 0.1
or ths = 0.1, while correct ones (9b) and (9c) re-
mained even when a large threshold value is used.
(9) a. ?X years? ? ?turn X?
b. ?X supplied? ? ?X provided?
c. ?main X? ? ?most significant X?
Kendall?s correlation coefficient ?B (Kendall,
1938) between the contextual similarity and each of
the human scores were 0.24 for grammaticality and
0.21 for meaning, respectively. Although they are ri-
valing the best results reported in (Chan et al2011),
i.e., 0.24 and 0.21, similarity metrics should be fur-
ther investigated to realize a more accurate filtering.
6 Conclusion
In this paper, we exploited general patterns under-
lying paraphrases to acquire automatically a large
number of high-quality paraphrase pairs using both
bilingual parallel and monolingual non-parallel cor-
pora. Experiments using two sets of corpora demon-
strated that our method is able to leverage informa-
tion in a relatively small bilingual parallel corpus
to exploit large amounts of information in a rela-
tively large monolingual non-parallel corpus. Hu-
man evaluation through a paraphrase substitution
test revealed that the acquired paraphrases are gen-
erally of reasonable quality. Our original objective
was to extract from monolingual corpora a large
quantity of paraphrases whose quality is as high as
that of paraphrases from bilingual parallel corpora.
We have met the quantity part of the objective, and
have come close to meeting the quality part.
There are three main directions for our future
work. First, we intend to carry out in-depth anal-
yses of the proposed method. For instance, while
we showed that the performance of phrase substi-
tution could be improved by removing noisy seed
paraphrases, this also strongly affected the quan-
tity. We will therefore investigate similarity metrics
in our future work. Other interesting questions re-
lated to the work presented here are, as mentioned in
Section 4.2, exploitation of patterns with more than
one variable, learning curve experiments with dif-
ferent amounts of monolingual data, and compari-
son of in-domain and general-purpose monolingual
corpora. Second, we have an interest in exploiting
sophisticated paraphrase patterns; for instance, by
inducing patterns hierarchically (recursively) and in-
corporating lexical resources such as those exempli-
fied in (4). Finally, the developed paraphrase col-
lection will be attested through applications, such
as sentence compression (Cohn and Lapata, 2008;
Ganitkevitch et al2011) and machine translation
(Callison-Burch et al2006; Marton et al2009).
Acknowledgments
We are deeply grateful to our colleagues at National
Research Council Canada, especially George Foster,
Eric Joanis, and Samuel Larkin, for their technical
support. The first author is currently a JSPS (the
Japan Society for the Promotion of Science) Post-
doctoral Fellow for Research Abroad.
640
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 597?604.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Hu-
man Language Technology Conference and the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 16?23.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 161?170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 17?24.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
196?205.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van-
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
Proceedings of the Workshop on Geometrical Models
of Natual Language Semantics (GEMS), pages 33?42.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 190?200.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING), pages 137?144.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalua-
tion of machine translation systems. In Proceedings of
the 6th Workshop on Statistical Machine Translation
(WMT), pages 85?91.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th International Conference
on Computational Linguistics (COLING), pages 350?
356.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 420?429.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the patent
translation task at the NTCIR-8 workshop. In Pro-
ceedings of NTCIR-8 Workshop Meeting, pages 371?
376.
Atsushi Fujita, Shuhei Kato, Naoki Kato, and Satoshi
Sato. 2007. A compositional approach toward dy-
namic phrasal thesaurus. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing (WTEP), pages 151?158.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel cor-
pora for text-to-text generation. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1168?1179.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. John Wiley & Sons.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun?ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
Web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1087?1097.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 341?348.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the 2007 Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975.
Maurice Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30(1-2):81?93.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Con-
ference and the North American Chapter of the Asso-
641
ciation for Computational Linguistics (HLT-NAACL),
pages 48?54.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT), pages 145?
153.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 381?390.
Yuval Marton, Ahmed El Kholy, and Nizar Habash.
2011. Filtering antonymous, trend-contrasting, and
polarity-dissimilar distributional paraphrases for im-
proving statistical machine translation. In Proceedings
of the 6th Workshop on Statistical Machine Translation
(WMT), pages 237?249.
Aure?lien Max. 2010. Example-based paraphrasing for
improved phrase-based statistical machine translation.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 656?666.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the Web.
In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 119?130.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of the 2003 Human Language Technol-
ogy Conference and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 102?109.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.
2005. PORTAGE: A phrase-based machine transla-
tion system. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 129?132.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of the 2002
Human Language Technology Conference (HLT).
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING). 849-856.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation, pages 122?125.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(4):503?526.
642
