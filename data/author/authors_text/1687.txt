Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 94?99,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Morphological annotation of the Lithuanian corpus 
Vidas Daudaravi?ius 
Centre of Computational 
linguistics 
Vytautas Magnus University 
Donelai?io 58, Kaunas, 
 Lithuania 
Erika Rimkut? 
Centre of Computational 
linguistics 
Vytautas Magnus University 
Donelai?io 58, Kaunas, 
Lithuania 
Andrius Utka 
Centre of Computational 
linguistics 
Vytautas Magnus University
Donelai?io 58, Kaunas, 
Lithuania 
 
Abstract 
As the development of information 
technologies makes progress, large 
morphologically annotated corpora become 
a necessity, as they are necessary for 
moving onto higher levels of language 
computerisation (e. g. automatic syntactic 
and semantic analysis, information 
extraction, machine translation). Research 
of morphological disambiguation and 
morphological annotation of the 100 
million word Lithuanian corpus are 
presented in the article. Statistical methods 
have enabled to develop the automatic tool 
of morphological annotation for 
Lithuanian, with the disambiguation 
precision of 94%. Statistical data about the 
distribution of parts of speech, most 
frequent wordforms, and lemmas, in the 
annotated Corpus of The Contemporary 
Lithuanian Language is also presented. 
1 Introduction 
The goal of this paper is to present the experience 
and results of compiling a large Lithuanian 
morphologically annotated corpus by using an 
available Lithuanian morphological analyser and 
dealing with the disambiguation problem. 
The Corpus of the Contemporary Lithuanian 
Language is a database of electronic texts, which is 
widely used in Lithuania. It well represents the 
present Lithuanian language and its different 
varieties (more about that in http://donelaitis.vdu.lt/). 
vidas@donelaitis.vdu.lt e.rimkute@hmf.vdu.lt a.utka@hmf.vdu.lt 
<word="Nenuostabu" lemma="nenuostabus" type="bdvr 
neig nelygin.l ne?vard? bevrd.gim"> 
<sep=","> 
<word="kad" lemma="kad" type="jngt"> 
<word="muziejus" lemma="muziejus" type="dktv 
vyr.gim vnsk V"> 
<word="susilaukia" lemma="susilaukti(-ia,-?)" 
type="vksm teig sngr tiesiog.nuos esam.l vnsk IIIasm"> 
<word="daugelio" lemma="daugelis" type="dktv 
vyr.gim vnsk K"> 
<word="sve?i?" lemma="sve?ias" type="dktv vyr.gim 
dgsk K"> 
<word="ne tik" lemma="ne tik" type="jngt"> 
<word="i?" lemma="i?" type="prln"> 
<word="?ikagos" lemma="?ikaga" type="tikr dktv 
mot.gim vnsk K"> 
<word="ir" lemma="ir" type="jngt"> 
<word="apylinki?" lemma="apylink?" type="dktv 
mot.gim dgsk K"> 
<sep=","> 
<word="bet ir" lemma="bet ir" type="jngt"> 
<word="tolimiausi?" lemma="tolimas" type="bdvr teig 
auk??.l ne?vard? vyr.gim dgsk K"> 
<word="Amerikos" lemma="Amerika" type="tikr dktv 
mot.gim vnsk K"> 
<word="kampeli?" lemma="kampelis" type="dktv 
vyr.gim dgsk K"> 
<word="bei" lemma="bei" type="jngt"> 
<word="kit?" lemma="kitas" type="?vrd mot.gim dgsk 
K"> 
<word="?ali?" lemma="?alis" type="dktv mot.gim dgsk 
K"> 
<sep="."> 
 
Figure 1: Extract from the morphologically 
annotated corpus (The following morpho-
logically annotated sentence is presented: "It is 
no surprise that the museum is visited by 
guests not only from Chicago region, but also 
from distant American places and other 
countries."). 
94
 Morphological annotation of the corpus will 
further increase capabilities of the corpus enabling 
extraction of unambiguous lexical and morpho-
logical information. The annotated corpus will 
soon be accessible for search on the internet. At the 
moment this corpus is fully accessible only at the 
Centre of Computational Linguistics of the 
Vytautas Magnus University. The tools for 
annotating Lithuanian texts are available for 
research purposes by request. 
The Lithuanian morphological analyser 
Lemuoklis (Zinkevi?ius, 2000) produces results of 
morphological analysis of Lithuanian wordforms, 
but leaves unsolved the problem of morphological 
ambiguity. Considering successful application of 
statistical methods in solving the morphological 
ambiguity for other languages, statistical methods 
have also been chosen for Lithuanian. Research of 
morphological disambiguation and results of 
morphological annotation of the 100 million word 
Lithuanian corpus are presented in the article.  
2 Morphological analysis of Lithuanian 
Morphologically ambiguous wordforms are words 
or wordforms that have two or more possible 
lemma interpretations or morphological annota-
tions, e. g. for the wordform kov? (en. fights, pl. 
Gen.) the morphological analyser Lemuoklis 
identifies two lemmas kovas (en. rook [bird] or 
March [month]) and kova (en. fight), while the 
wordform naktis (en. night) can be in Singular 
Nominative or in Plural Accusative case (more 
information on ambiguity for Lithuanian see 
Rimkut?, 2006). 
Approximately a half of all wordforms in the 
Lithuanian annotated corpus are morphologically 
ambiguous (Rimkut?, 2006), which is comparable 
to other inflected languages, e.g. for the Czech 
language it is 46% (Haji?, 2004:173). 
For developing the automatic disambiguation 
system a morphologically annotated training 
corpus is necessary. Manual creation of 1 M word 
Lithuanian annotated corpus is a very time consu-
ming task, which has taken 5 man-years to 
complete. Firstly, the annotation format needs to 
be developed and mastered (see Figure 1), then it is 
necessary to assign a word to an appropriate part of 
speech, and often it is very difficult to find a 
correct grammatical reading for a word. It also 
takes a lot of time reviewing and trying to put all 
annotated texts into one uniform standard. 
3 Automatic morphological annotation of 
the Lithuanian corpus 
Statistical morphological disambiguation using 
small manually annotated training corpora looks as 
quite a simple task, when frequencies of 
grammatical features are generated during the 
training phase and the most likely sequence of 
morphological features is found in a new text by 
the help of various probability methods. Drawing 
on the experience of morphological annotation 
systems for other free word order languages 
(D?bowski, 2004; Haji? et al, 2001; Palanisamy et 
al., 2006 etc.), it is obvious that the corpus-based 
method is most suitable for the developing such 
systems for Lithuanian.  
The Czech experience (Hladk?, 2000) was very 
expedient for developing automatic morphological 
annotation tool for Lithuanian, especially because 
Czech similarly to Lithuanian is a free word order 
language. Czech research applies statistical Hidden 
Markov Models and formal rule-based methods for 
Czech and English languages. It is important to 
note that these methods are language independent 
and can be applied to Lithuanian. The only 
language dependent factor is a small morpho-
logically annotated corpus for training. In various 
experiments the selection of Czech morphological 
features was regularized and optimised, which 
helped to achieve close to English language 
precision of 96%. However this precision is 
achieved with a limited number of Czech morpho-
logical features. The precision of 94 % is achieved 
when all features of Czech language are selected 
(Hladk?, 2000). 
4 Statistical morphological 
disambiguation 
Morphologically analysed words are the input of 
the automatic morphological annotation system, 
while the best sequence of morphological features 
is its output. Annotation of a new text involves 
establishing the most likely sequence of morpho-
logical features by the help of Hidden Markov 
models. Not all combinations of trigrams and 
bigrams can be found even in the biggest corpora. 
Therefore, the linear smoothing of the missing 
cases is used, as the probability of the most likely 
95
 sequence cannot be equal to zero (see more on 
HMM in Jurafsky (2000:305-307)).  
The following HMM model is used by Czech 
scientists: 
nttt
t
iiiiii
n
t
it
iiiiT
tttTtttp
twp
ttptptwp
,...,,),,|(
)|(
)|()()|(max
2121
1211
~
3
~
~~
1
~
=?
??
?????
??
?
=
 
 
We expanded the model by including the 
lemma. This procedure is important to Lithuanian, 
where different lemmas often have identical 
wordforms and morphological features. Therefore 
the probability of a lemma is also included: 
nttt
tt
iiiiii
n
t
ititii
iiiT
tttTtttp
lwptwpttp
tplwptwp
,...,,),,|(
)|()|()|(
)()|()|(max
2121
12
111
~
3
~~~
~
1
~
1
~
=?
????
?????
??
?
=
 
 
where 
 
titt
twitwit Wtwptwp /1)1()|()|(
~ ??+?= ??  
is the smoothed probability of a wordform and tag 
pair. 
 
titt
twitwit Llwplwp /1)1()|()|( 11
~ ??+?= ??  
is the smoothed probability of a wordform and 
lemma pair. 
 
Tii Ctptp tt /1)1()()( 0101
~ ??+?= ??   
is the smoothed probability of a tag. 
 
Ti
iiii
Ctp
ttpttp
t
tttt
/1)1()(
)|()|(
121112
11
~
11
???+?+
+?= ??
???
?
  
is the smoothed probability of a bigram tag . 
 
T
iii
iiiiii
C
tpttp
tttptttp
ttt
tttttt
/1)1(
)()|(
),|(),|(
232221
2322
21
~
1
2121
????+
+?+?+
+?=
?
????
???
??
?
  
is the smoothed probability of a trigram tag . 
 
)(
)|(
)|(
t
t
t
i
it
it tCount
twCount
twp =   
is the probability of a wordform containing a 
particular tag in the training corpus. 
 
||
)(
)(
train
i
i T
tCount
tp t
t
=   
is the probability of a tag in the training corpus. 
 
)(
),(
)|(
1
1
1
?
?
? =
t
tt
tt
i
ii
ii tCount
ttCount
ttp   
is the probability of a bigram tag in the training 
corpus. 
 
),(
),,(
),|(
21
21
21
??
??
?? =
tt
ttt
ttt
ii
iii
iii ttCount
tttCount
tttp   
is the probability of a trigram tag in the training 
corpus. 
 
tit
W is a number of wordforms with the feature  
ti
t
tit
L is a number of lemmas with the feature  
ti
t
TC  is a number of tags in  training set. trainT
A function Count(x) corresponds to the 
frequency of a tag or a bigram.  
Smoothing lambdas 1w? , w? , 01? , 11? , 12? , 
21? , 22? , 23? < 1 are used to combine the 
probabilities of lower order. The smoothing is very 
important when unknown events occur in the 
training corpus.  
We used such lambda values: 
1w? =0.85, 
w? =0.85, 
01? =0.99, 
11? =0.74, 12? =0.25, 
21? =0.743, 22? =0.203, 23? =0.053 
96
 If a trigram tag is not found in the training 
corpus then the probability of a trigram is not 
assigned to zero, but rather the probability of a 
bigram is included with some weight. In case no 
trigram tag, bigram tag and unigram tag is found 
then the probability of a trigram assumes a very 
small number which is equal to 1 divided by the 
size of the tagset. The highest score is assigned to a 
trigram, lower ? to bigram, and lowest ? unigram.  
The disambiguation tool has been developed at the 
Centre of Computational Linguistics of the 
Vytautas Magnus University using C++ tools. All 
results reported in this paper are based on approach 
using an accuracy criterion (number of correctly 
disambiguated results divided by number of input 
words). We do not use any morphological pre-
processing. A precision of 94% has been achieved 
for establishing tags, which is comparable to 
results achieved for other languages, when the 1 
million word training corpus is used. A precision 
of 99% is achieved for establishing lemmas. For 
the precision test a special 50 thousand word 
corpus has been used, which is not included in the 
training corpus. 
The following statistics has been derived from 
the 1 M word training corpus1:  
 
Different lemmas   41,408 
Different pairs of wordforms 
and tags  
 130,511 
Different pairs of wordforms 
and lemmas  
 121,634 
Unigram tags   
TC  1,449 
Bigram tags   76,312 
Trigram tags   544,922 
Training corpus size | |  trainT  1,009,516 
Table 1: Corpus statistics 
 
The number of lemmas in the training corpus is 
sufficient to gather frequencies in order to solve 
ambiguous lemmas. Unknown lemmas are not 
ambiguous in the training corpus, as they are rare 
and have unique meanings. 
The size of the tagset is 1449. Lithuanian is a 
relatively free word order language, and therefore 
it is difficult to get reliable bigram and trigram 
statistics. We decided to gather distant bigram and 
                                                 
1 See more about manually tagged Lithuanian Corpus 
and Lithuanian language tagset in Zinkevi?ius et al 
2005. 
trigram frequencies using a gap of 1. As a bigram 
we consider two subsequent tags (<A> <B>) or 
two tags with a gap of 1 in between (<A> <gap> 
<B>). Similarly, a trigram is a sequence of three 
subsequent tags (<A> <B> <C>) or a sequence of 
three tags with a gap of 1 between the first and 
second tag (<A> <gap> <B> <C>) or between the 
second and third tag (<A> <B> <gap> <C>). 
Distant n-grams help to reduce the number of 
unknown bigrams and trigrams in the training 
corpus. 
5 Statistical data for the morphologically 
annotated corpus of Lithuanian 
Most important statistical data for the 
morphologically annotated Lithuanian corpus: 
? Corpus size ? 111,745,938 running words; 
? Number of wordforms ? 1,830,278; 
? Number of unrecognized wordforms ? 
824,387 (5,6 % of all tokens); 
? Number of recognized wordforms ? 
1,005,891. 
225,319 different lemmas have been recognized 
in the Corpus of Contemporary Lithuanian. 
Distribution of parts of speech in the whole 100 
M word corpus does not differ significantly from 
the distribution in the training corpus (see Figure 
2). The biggest difference is in the number of 
unknown words. There are no unknown words in 
the training corpus, because it has been semi-
automatically annotated and disambiguated. The 
number of unknown words in the 100 M word 
corpus is influenced by morphological analyzer, 
i.e., not all words are successfully recognized.  
A big part of unknown words are proper nouns. 
Presently the dictionary of the morphological 
analyser contains 5255 high frequency proper noun 
lemmas (e.g. Lietuva (en. Lithuania)), which 
account for 3.2% of the vocabulary in the large 
annotated corpus. In the training corpus proper 
nouns account for 4.3% of the vocabulary, and we 
expect the similar proportion in the large annotated 
corpus. The average frequency of a proper noun 
lemma is 4.6 in the training corpus. Thus we could 
estimate the size of the dictionary of proper nouns 
at about 250,000 lemmas. 
 
97
 6 The remaining problems 
The achieved precision of 94% for morphological 
annotation leaves some room for improvement. It 
is still difficult to solve homographic problems, 
where some wordforms of different words are 
identical. For example, wrong lemmas are 
frequently chosen for the wordforms tonas (en. 
tone) and tona (en. ton), kovas (en. rook [bird]) and 
kova (en. fight), Bir?ai (Lithuanian town) and bir?a 
(en. stock-market). 
Syncretism of grammatical cases is not always 
solved correctly. Most often the incorrect analysis 
is given for words of feminine gender, when 
singular Genitive and plural Nominative cases are 
confused (e. g. mokyklos (en. school)). 
Some cases are problematic even for a human 
linguist, when it is not clear which part of speech 
(noun or verb) is used in such collocations: kovos 
d?l teis?s likti pirmajame e?elone (lit. fight/ fights 
for the right to stay in the first league); kovos su 
narkotikais (lit. fight/ fights against drugs); kovos 
su okupantais (lit. fight/ fights against occupants). 
Even if the part of speech of the word kovos is 
chosen as a noun, then the ambiguity case still 
remains. The broader context is needed to solve 
such problems. 
Interjections are not very often used in 
Lithuanian, nevertheless the morphological abbre-
viation a is confused with the interjection a. 
Abbreviations that are identical to Roman 
numerals are often annotated incorrectly: the most 
problems are caused by the abbreviation V. 
Sometimes wrong lemma is chosen. The words 
with fixed forms such as ir (en. and), tik (en. only) 
cause many problems as they can be interjections, 
particles, or adverbs. The lemma of the wordform 
vienas (en. one, alone, single) is not always chosen 
correctly, as this word can be a pronoun, an 
adjective, a numeral, or even a proper noun. It is 
hoped that some of these problems will disappear 
after improving the program of morphological 
analysis. 
7 Conclusions 
The method of Hidden Markov models for 
morphological annotation has allowed achieving 
the precision of 94%, which is comparable to the 
precision achieved for other languages, when 1 M 
word training corpus is used. The precision of 99% 
is achieved for establishing lemmas of Lithuanian 
words. The precision measure estimates only the 
process of disambiguation, while unrecognised 
words are not included in the precision test. 
The amount of unrecognised wordforms makes 
up 5,6% of all tokens (more that 800,000 different 
wordforms). In order to analyse the missing 
wordforms around 100-150 thousand lemmas need 
to be added to the lexicon of morphological 
 
0
5
10
15
20
25
30
35
40
N
ou
n
V
er
b
A
dj
.
N
um
.
P
ro
n.
A
dv
.
P
ar
t.
C
on
j.
In
te
rj.
O
no
m
.
P
re
p.
A
cr
on
.
A
bb
r.
U
nk
no
w
n
O
th
er
1 M word corpus
100 M word corpus
P
er
ce
nt
s
POS
 
Figure 2: Distribution of parts of speech in 1 M and 100 M word corpora. 
 
98
 analyser, i.e. the amount is similar to the present 
size of the lexicon. 
One million word morphologically annotated 
corpus is enough for the analysis of morphological 
phenomena in Lithuanian, as distribution of parts 
of speech in the 100 million word corpus does not 
differ significantly 
8 Acknowledgements 
This work is a part of the project ?Preservation 
of the Lithuanian Language under Conditions of 
Globalization: annotated corpus of the Lithuanian 
language (ALKA)?, which was financed by the 
Lithuanian State Science and Study Foundation. 
References: 
Arulmozhi Palanisamy and Sobha Lalitha Devi. 2006. 
HMM based POS Tagger for a Relatively Free Word 
Order Language. Research in Computing Science 18, 
pp. 37-48  
Barbora Vidov?-Hladk?. 2000. Czech language tagging. 
Ph.D. thesis, ?FAL MFF UK, Prague. 
Daniel Jurafsky, James H. Martin. 2000. Speech and 
Language Processing, Prentice-Hall, Upper Saddle 
River, NJ. 
Erika Rimkut?. 2006. Morfologinio daugiareik?mi?-
kumo ribojimas kompiuteriniame tekstyne 
(Morphological Disambiguation of the Corpus of 
Lithuanian Language). Doctoral dissertation, 
Vytautas Magnus University, Kaunas. 
Jan Haji?. 2004. Disambiguation of rich inflection. 
Computational morphology of Czech. Karolinum 
Charles University, Prague. 
Jan Haji?, Pavel Krbec, Pavel Kv?to?, Karel Oliva, 
Vladim?r Petkevi?. 2001. Serial Combination of 
Rules and Statistics: A Case Study in Czech Tagging. 
In Proceedings of the 39Annual Meeting of the ACL 
(ACL-EACL 2001). Universit? de Sciences Sociales, 
Toulouse, France. 
?ukasz D?bowski. 2004. Trigram morphosyntactic 
tagger for Polish. In Proceedings of the International 
IIS:IIPWM'04 Conference, pp. 409-413, Zakopane. 
Vytautas Zinkevi?ius. 2000. Lemuoklis ? morfologinei 
analizei (A tool for morphological analysis - 
Lemuoklis). Darbai ir Dienos, 24, pp. 246?273. 
Vytautas Magnus University, Kaunas. 
Vytautas Zinkevi?ius, Vidas Daudaravi?ius, and Erika 
Rimkut?. 2005. The Morphologically annotated 
Lithuanian Corpus. In Proceedings of The Second 
Baltic Conference on Human Language 
Technologies, pp. 365?370. Tallinn. 
 
Appendix 1. Lithuanian morphological 
categories and appropriate tags 
 
Grammatical 
Category 
Equivalent in 
English 
Tag 
 
Abbreviation dr. sntrmp 
Acronym NATO akronim 
Adjective good bdvr 
Adverb perfectly prvks 
Onomatopoetic 
interjection 
cock-a-doodle-do i?tk 
Conjunction and jngt 
Half participle when speaking psdlv 
Infinitive to be bndr 
Second Infinitive at a run b?dn 
Interjection yahoo jstk 
Noun a book dktv 
Number one sktv 
Roman Number I rom skai? 
Proper Noun London tikr dktv 
Proper Noun2 Don tikr dktv2 
Participle walking dlv 
Gerund on the walk home padlv 
Preposition on prln 
Pronoun he ?vrd 
Verb do vksm 
Idiom AA rest eternal idAA 
Connective idiom et cetera idJngt 
P.S. P.S. idPS 
Prepositional 
idiom 
inter alia idPrln 
Pronominal idiom nevertheless id?vrd 
Particle also dll 
 
 
99
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 53?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Characteristics and Analysis of Finnish and Swedish  Clinical Intensive Care Nursing Narratives   Helen Allvinf, Elin Carlssonf, Hercules Dalianisf, Riitta Danielsson-Ojalaa, Vidas Daudaravi?iusb, Martin Hasself, Dimitrios Kokkinakisc, Helj? Lund-gren-Lainea, Gunnar Nilssonf, ?ystein Nytr?d, Sanna Salanter?a, Maria Skeppstedtf, Hanna Suominene, Sumithra Velupillaif aDepartment of Nursing Science, University of Turku, VSSHP, Turku, Finland, bVytautas Magnus University, Lithuania, cDepartment of Swedish, University of Gothenburg, Sweden, dIDI, The Norwegian University of Science and Technology, Norway, eNICTA Canberra Research Laboratory and Australian National University, Australia fDepartment of Computer and Systems Sciences/Stockholm University Forum 100 SE-164 40 Kista, Sweden http://www.dsv.su.se/hexanord 
Abstract 
We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothe-sis is that there are similarities that are impor-tant and interesting from a language technology point of view. This may have im-plications when building tools to support pro-ducing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nurs-ing narratives. Our findings are that ICU nurs-ing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to devel-oping language technology tools. 
1 Introduction The purpose of this study1 is to do content and lexical analysis of nursing narratives written in an                                                            1 Our research on the Stockholm EPR Corpus (Dalianis et al, 2009) has been approved by Etikpr?vningsn?mnden i Stock-holm, (the Regional Vetting Board), reference number 
Intensive Care Unit (ICU). The ultimate goal of our research is to define linguistic similarities and language-specific aspects that differentiate clinical narratives in Finnish and Swedish in order to lay groundwork for developing internationally appli-cable language technology solutions and create a framework for characterising and comparing clini-cal narratives. Free text is handy for information entry but a challenge for information extraction, care handover and other uses of gathered informa-tion. Language technology can alleviate some of these problems in retrospective analysis by offer-ing a more semantically informed interpretation and abstraction. However, the most promising po-tential of language technology is to interactively improve, interpret and code during text entry so that the resulting structured, coded, free text can be validated immediately. The critical bottleneck to-day is namely information handover and reuse, and extensive text is simply not used nor is useful. In-teractively validated, semantically processed text could be more usable and support abstraction, visualization and query tools for the benefit of cli-nicians, patients, researchers and quality adminis-trators.                                                                                               2009/1742-31/5. Our research on the Finnish Corpus (Salan-ter? et al, 2009) has been approved by the Ethical committee of the Hospital District of South West Finland, reference number 2/2009, ?66. 
53
In this paper, we analyze Finnish and Swedish ICU nursing narratives from both qualitative and quantitative perspectives. Our data includes textual nursing documentation of adult patients with a protracted inpatient period. We have chosen ICUs because of their international similarity in decision making (Lauri & Salanter? 2002) and nursing documentation because it covers the entire inpa-tient period. 2 Background 
2.1 Clinical text Clinical text covers the text documents produced for clinical work by clinicians and occurs in clini-cal information systems. It is written by clinicians, that is, professionals (physicians, nurses, therapists and other specialist) responsible for patient care. Its primary purpose is to serve patient care as a summary or hand-over note. However, clinical text is also written for legal requirements, care continu-ity and purposes of reimbursement, management and research. Clinical text covers every care phase and, depending on the purpose, documents differ. Documents that describe the patient?s state, current health problems and socio-medical history are very different from those describing a care plan, its ac-tualization and evaluation of care outcomes. Again, these differ from diagnostic notes, lab results, ra-diography readings, pathology reports and dis-charge documents that plan further care at discharge.  Finally, clinical text may have been entered in ?real time?, in retrospect, or as a sum-mary, by the bedside or elsewhere. The enterer can be a clinician, secretary who transcribes a dictate, speech recognition software or another system that generates or synthesizes text, (McDonald 1997, Thoroddsen et al, 2009.).  2.2 Legal requirements for clinical documen-tation in different countries In several countries clinical documentation is based on law. In Finland, the Ministry of Social Affairs and Health (Statutes of Finland, 298/2009) defines that to ensure good care, all necessary and wide-ranging information has to be registered in patient records. In Sweden, the National Board of Health and Welfare has a similar approach (Pa-tientdatalagen 2008:355). Clinical text should be 
explicit and intelligible, and only generally well-known, accepted concepts and abbreviations are allowed to be used. It should detail adequately the patient?s conditions, care and recovery.  2.3 Special features of ICU and nursing An ICU is an essential component of most large hospitals with high quality care.  ICUs provide care for critically ill patients and focus on condi-tions that are life-threatening and require compre-hensive care and constant monitoring (Webster's 2010). This task is fairly similar universally. It is based on optional, international guidelines focus-ing on triage, admission, discharge and education. This international similarity was evident when nurses? decision making was studied in Canada, Finland, Northern Ireland, Switzerland, Norway and the USA (Lauri & Salanter? 2002); the study showed that decision making of ICU nurses was the most uniform in different countries when com-pared with nurses working in public health care, psychiatric care, and short and long term care. Clinical text written by nurses, that is, nursing narratives, both in Finland and in Sweden is based on the care process which stands for gathering in-formation from the patient, setting goals for care, implementing nursing interventions, and evaluat-ing the results of given care. In Finland, the na-tional standardized documentation model has been implemented with the Finnish care classification (assessment, interventions and outcomes of care) (Tanttu & Ikonen 2007). The Swedish VIPS model provides a structure for the documentation process with key words that reflect the nursing process (Ehrenberg et al, 1996).  ICU nursing narratives can be lengthy, espe-cially when the patient stay in the ICU is pro-longed. As much as 60 A4 pages equivalents of written text may be gathered during one period of care. However, clinicians have somewhat different opinion on how to organize the information they write. For example, headings are often inconsistent and text under headings can cover a lot of other issues than those directly concerning the given heading. (Suominen et al, 2009.)  2.4 Related studies Since most of the available clinical documents are in free-text form, a number of stylistically oriented efforts to characterize the data from various angles 
54
have taken place. This may include various topics, from viewing detailed information about specific items (e.g. readability, Kim et al, 2007) to identi-fying patterns and structures in order to provide better technology to automatically process the sublanguage (Pakhomov et al, 2006). The majority of such efforts investigate different aspects of lin-guistic features at a monolingual level, for in-stance, Hahn & Wermter (2004); Tomanek et al, (2007); Chung (2009); Harkema et al, (2009); while for a thorough review of various related is-sues see Meystre et al, (2008). In the Nordic con-text, Josefsson (1999) discusses Swedish clinical language and shows examples on how verb con-structions in a clinical setting differ from a non clinical setting.  One claim is that the physician unmarks the verb forms for agentivity when writ-ing about the patient and what actions she takes, for example, Patienten hallucinerar [The patient hallucinates] instead of the normal form  Patienten f?r hallucinationer [The patient experiences hallu-cinations].   Helles? (2005) describes nurses' general use of the language function in the nursing discharge notes. She finds that the text in the nursing dis-charge notes is information-dense and character-ized by technical terms, and that the use of standardised templates helped nurses improve the completeness, structure and content of the informa-tion. Comparisons at a monolingual level between written clinical text and lay text has been carried out by Dalianis et al, (2009). A contrastive com-putational linguistics study was carried out be-tween the Stockholm EPR Corpus (SEPR) and a general language corpus, both written Swedish text. The findings showed that SEPR contained longer words and that the vocabulary was highly domain-specific. Other work is described in Ownby, (2005). Comparing clinical text at a crosslingual level has, to our knowledge, only been done by Borin et al, (2007).  3 Analysis of Finnish and Swedish ICU nursing narratives  The analyzed nursing narratives origin from one ICU in a university-affiliated hospital both in Fin-land and Sweden. Our inclusion criterion was an ICU inpatient period of at least 5 days and patient's age of at least 16 years. The Finnish data includes nursing narratives from 514 patient records (496 
unique patients, 18 rebounds, a patient record is defined as each inpatient period of at least 5 days per patient) between January 2005 and August 2006. The Swedish data includes nursing narra-tives from 379 patient records (333 unique pa-tients, 46 rebounds) between January 2006 and May 2008. Since we did not have complete admis-sion and discharge documents from both countries, our analysis is performed on daily nursing narra-tives. These documents are written by ICU nurses during the actual inpatient period from the patient admission to the discharge.    3.1 Qualitative analysis A manual content analysis was performed by four health care professionals (i.e., three native Finnish speakers knowing Swedish, one Swedish native speaker) and one native Swedish speaking lan-guage consultant. Three average-sized patient re-cords each from Finland and Sweden were chosen for our analysis (average size 2,389 words for Fin-land and 5,169 words for Sweden). In the analysis, we considered special features (Table 1) of daily notes both from the structural and content related points of view.  The style and context of both Finnish and Swed-ish text is very similar. For health care profession-als, and especially with an ICU background, all the texts are intelligible and the meaning of a writer becomes evident from the context even in the pres-ence of numerous linguistic and grammatical mis-takes; almost all the sentences are lacking both grammatical subjects and objects. It is evident that in both countries, the narratives are written from a professional to a professional in order to support information transfer, remind about important facts, and supplement numerical data.  A feature common for all the six records is that they rarely contain any subjects or objects when nurses are writing about patients. However, in the Swedish nursing narratives the word patient is used as a subject or object much more often than in the Finnish narratives. The abbreviation pat. is mostly used for this reference and she/he is never used for this purpose. In the whole data, pat. is 40 percent more common than she/he, which is the most common personal pronoun. It seems that the word patient or pat. is used more when the profes-sionals are writing about relatives. In general, pro-nouns are used infrequently in the narratives, and  
55
  Table 1. Special structural and contextual features of Finnish and Swedish daily ICU nursing narratives. The original examples are added in ().  I very rarely. If the reader is not a health care pro-fessional, a risk for confusing the subject (i.e., the patient or nurse) arises. However, the context makes it almost always clear who is referred to.  Approximately half of the narratives do not contain any verb. The most common tense is perfect, but 
without the auxiliary has. When the meaning does not contain a subject it becomes ?unnatural? to use has. Instead, the supine form is used, for example slept, lain, and eaten. Both present and past parti- ciples without be-verb are common, for example, Breathing: Ventilator parameters unchanged. 
Special features of Finnish narratives   Special features of Swedish narratives   Structure Examples Structure Examples 
Headings are used in 2 out of 3 patient records. Headings are typi-cally used as subjects or subjects are partially used.  
Diuresis: occasionally profuse. (Diureesi: ajoittain runsasta.)  Pupils move under eyelids but does not open eyes. (Pupillit liikkuvat luomien alla, mutta ei avaa silmi??n.)  
Headings are used in all daily narra-tives. In Swedish daily narratives, the structure of headings seems to be obligatory. The headings are used typically as subjects.  
Circulation: Stable with ino-trop. (Cirkulation: Stabil med ino-tropi.)  Reacts only for pain stimula-tion during the suction of intu-bation tube. (Reagerar enbart vid sm?rt-stimuli vid sugning i tuben.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Consciousness remained un-changed. (Tajunta pysynyt ennallaan.)  Blood pressure low. (Verenpaine matala.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Breathing: Ventilator parame-ters unchanged. (Andning: Ventilator paramet-rarna of?r?ndrade.) 
Complete sentences are rarely used.  
No spontaneous movements, rigidifies. (Ei spontaania liikett?, j?ykiste-lee.)  Husband and daughter have been staying a long time beside the patient. (Mies ja tyt?r olleet pitk??n potilaan vierell?.) 
Complete sentences are rarely used.  
Light sedation, looks up now and then. (L?tt sederad, tittar upp ibl-and.)  She took the wedding ring and the watch home. (Hon tog med sig vigselring och klocka hem.) 
Misspellings are found but the content or meaning is still clear.  Hemodynamic ? hemodynamic (Hemodynamiikka ? henody-namiikka) Misspellings are found but the content or meaning is still clear.  
The motther is informed. (Mammman ?r informered.)  Magnesium is addded. (Magnesium har tilllsatts.) Content Examples Content  Examples 
The word patient as a subject or object is infrequently mentioned. If this word is mentioned it is not abbreviated.  
Oxidates well or ventilates well. (Happeutuu hyvin tai ventiloituu hyvin.)  
The word patient is used more often than in Finnish narratives as a subject or object. It is also replaced with abbreviations of Pat or Pt.  
Patient got a percutanous tracheostomy today. (Patienten har f?tt en perkutan trakeostomi idag.)  Very worried about patient?s condition. (Mycket oroliga ?ver patien-tens tillst?nd.)  Pt. wakes up for talking and appears to be adequate. (Pt. vakner p? tilltal och up-plevs som adekvat.) 
Signs are typically used: e.g., >, <, -->, +, -.  
The height for the drain raised from 10 --> 20 mmHg. (Dreneerausrajaa nostettu 10  --> 20 mmHg.)  Got medicine --> good response. (Sai l??kett? -->hyv? vaste.) 
Many different abbreviations are used. The origin of entire word is Swedish, English, Latin, professional or ICU typical.  
em. [eftermiddag, afternoon], HR [heart rate], VF [Ven-tricula/Fibrillation, Ventricular Fibrillation]  
56
The use of headings is frequent and good ? most of the time the content matches the headings (Tables 1 and 3). In addition, headings are used similarly in the Swedish and Finnish documents. Most of the time the headings are considered as subjects of the sentence, for example, Consciousness: Unchanged. Liquor brighter than yesterday. However, in the use of headings there are two interesting findings: If the headings are to be cho-sen freely, as in the Finnish narratives, nurses tend to use their own headings and hence many syno-nyms or closely related concepts are used; for ex-ample, hemodynamics versus blood pressure and pulse or breathing versus oxidation. If the headings are obligatory, as in the Swedish narratives, nurses tend to write their observations under the heading which is somehow closest to the subject; for exam-ple, body temperature under circulation or level of sedation under sleep. For both languages the use of different abbrevia-tions is very common. Almost every daily nursing narrative included several abbreviations. Most of the abbreviations are typical for an ICU domain: CVP [central venous pressure], PEEP [Positive End-Expiratory Pressure], EN [Enteral Nutrition], TPN [Total Parenteral Nutrition], pO2 [partial pressure of oxygen], pCO2 [partial pressure of carbon dioxide], MV [Minute Ventilation] and MAP [Mean Arterial Pressure]. From a language technology point of view this means that ICU nurs-ing narratives contain language-independent vo-cabulary. However, nurses in both countries also use many language dependent abbreviations. 
3.2 Quantitative analysis The Finnish data set (n=514) was quantitatively analyzed using the morphological analyser FinT-WOL and the disambiguator FinCG, (Lingsoft 2010), and the Swedish data set (n=379) using the GTA, Granska Text Analyzer (Knutsson et al, 2003). Both data sets are rich in terms of amount of text and vocabulary (Table 2). It is also clear that the amount of text written per day and patient varies a lot in both data sets. More complex words were spelled in numerous ways. For example, the pharmacological substance Noradrenalin had ap-proximately 350 and 60 different spellings in the Finnish and Swedish data sets, respectively. This problem is part of a more general issue of refer-
ence resolution e.g. when mapping different lexical terms referring to the same concept. In our quantitative analysis, we have included punctuation characters. In the Swedish data there was a large amount of html-tags and other format-ting characters, which has a high impact on the total number of tokens (see Table 2). Moreover, as Finnish is highly inflective, FinCG produces alter-native lemmas, hence it is possible to reduce the sparseness of the data by processing the output by choosing only one alternative lemma (see total number of types in Table 2).  To further illustrate the richness of ICU nursing language, the number of unique bigrams (e.g., ?is not?, ?oxidate well? and ?night time? (note: a mis-spelled compound) are the most common ones for Finnish) and trigrams (e.g., ?oxygeneated and ven-tilated?, ?and ventilate well? are among the most common ones for Finnish) were 368,166 (275,205 after FinCG) and 745,407 (356,307 after FinCG) for Finnish patient records. For the Swedish data, the number of unique bigrams was 469,455 (344,127 after GTA) and 1,064,944 (905,539 after GTA). Examples of common Swedish ICU bi-grams and trigrams include ?circulation stabile?, ?during night?, ?in connection with?, and ?with good effect?. Of the content of Finnish nursing narratives, 11% are verbs, 7% nouns and less than 1% pronouns. For Swedish nursing narratives, the respective percentages are 11%, 27% and 2%. One reason for the high numbers for nouns in the Swed-ish data might be due to the large amount of (obligatory) headings relative to the Finnish data (see Table 3).  To support fluent information flow, language technology is needed to strengthen referential con-gruence. Much of this richness of vocabulary is explained by abbreviations and personal differ-ences in professional jargon. In particular, abbre-viations were common. Based on the analysis of the most common words, abbreviations were rela-tively established in Swedish data. For the Finnish data, abbreviations were less standard but RR, SR, CVP, h, ad, ml, ok, vas. [vasen, left] and oik. [oikea, right] were extremely common. Thus, ref-erential congruence can be strengthened by spell-ing out the most common abbreviations automatically. Adding topical content headings is another way to support information flow. Topical content head-ings were mandatory for Swedish data, but no de-
57
fault headings for Finnish existed. However, the headings for Finnish were established in terms of content. In Table 3, we see that the headings for both languages cover similar topics, which indi-cates that the clinical information need is similar for professionals in both countries (and languages). Thus, we recommend forming a standardized set of headings from which the user can voluntarily se-lect the ones to be used. This does not exclude add-ing other headings. Another alternative is to develop language technology for topic segmenta-tion and labeling. We have promising results from this approach (see, e.g., Suominen 2009).  Temporal expressions (e.g., time, evening, night) were often used in both data sets. This poses the question of tense analysis of verbs being unneces-sary and the time-related words being enough to imply the needed temporal information. It is also interesting to note that the negations inte [not, Swe], ingen [none, Swe], ej [not, Swe] and ei [no/not, Fin] are all among the most common types, which is an important property to take into account in information extraction applications. Furthermore, words regarding the oral cavity, such as breathing and mucus, as well as relations, such as daughter, son, wife, and husband are very com-mon in both data sets.  Inspired by the tf?idf-measure from information retrieval, we also analyzed the most common words in terms of a) the number of patients in whose documents the word was used and b) the number of daily nursing narratives in which the word was used. Here, we found, in both data sets, that those words that were used for all patients as well as all daily narratives, were very similar in both data sets, and were related to the most com-mon headings, temporal expressions, negations and monitoring (e.g., increase, continue, begin).  The amount of Protected Health Information (PHI) in form of person names was equal in both of the data sets: 1.5 person names per thousand tokens. This is notable, since this has implications when it comes to integrity issues and reuse of data for research purposes. FinCG did not recognize 36% of the content of Finnish nursing narratives. However, words marked as unrecognized by FinCG also included punctuation marks. In our previous study (see Suominen 2009 and references therein), we tai-lored FinCG by extending approximately 35,000 clinical terms. The extension not only substantially 
improved the applicability of FinCG to the health domain but also initiated piloting of our language technology components in an authentic healthcare environment in the fall 2008. This lead to the re-lease of commercial language technology for Fin-nish health records (Lingsoft 2010).   Data Finnish  Swedish  Total number of patients  514  379  Total number of  tokens,  types (unique tokens) and types after processing 
 1,227,909 63,328 38,649 
 1,959,271 - 41,883 Number of tokens per patient: Minimum Maximum Average Standard deviation 
 540 14,118 2,389 1,635   
 92 36,830 5,169 5,271 Total number of  daily documents and shifts  5,915 17,103  4,700 ? Number of tokens per daily document: Minimum Maximum Average Standard deviation 
  0 915 208 87 
  5 9,389 417 239  Table 2. Comparison of Finnish and Swedish ICU data sets: total amount of text per patient. A daily document, i.e. nursing narrative, contains all text written about a given patient during a calendar day.  Finnish n ? Swedish  n = Hemodynamics  7,800  Respiratory  11,301  Consciousness  6,900  Circulation 10,630  Relatives  5,700  Elimination  10,041  Diuresis  5,400  Nutrition  8,258  Breathing  4,500  Communication  5,880  Oxygenation  3,600  Event Time  5,681  Other  3,200  Pain  4,732  Excretion  590  Psychosocial  4,682  Hemodialysis  370  Sleep  4,438  Pulse  160  Skin  4,402  Skin  160  Activity  3,794   Table 3. Comparison of Finnish and Swedish ICU data sets: the most common headings. For the Finnish data, where default headings were not given, we approxi-mated the amount of heading by using an automated heuristics followed by manual combination of headings with the same meaning. 
58
For Swedish, GTA handles unknown words dif-ferently than FinCG. However, by comparing the ICU words with a Swedish general language cor-pus (PAROLE, Gellerstam et al (2000)), we found that 69% of the types are not included in PAROLE, which indicates a need for tailoring GTA (or simi-lar tools for Swedish) with domain-specific ICU terms.  4 Conclusions The purpose of this study was to do content and lexical analysis of nursing narratives written in an ICU. Our findings are that, even though the Fin-nish and Swedish languages are not linguistically closely related, the way of writing clinical nursing ICU narratives in both countries is very similar. Moreover, the written context made sentences clear for content experts, even though the texts were full of specialized jargon, misspellings, ab-breviations, and missing subjects and objects. However, these characteristics make clinical text challenging for language technology. For example coreference resolution as in the case of noradrena-lin. We have also shown that the content characteris-tics of Finnish and Swedish ICU nursing narratives are very similar. This implies that developing tools for documentation support in ICUs is not country or language dependant in that respect. Developing such tools may improve possibilities for informa-tion extraction and text mining, enabling the possi-bilities to reuse the vast amounts of important practice-based information and evidence captured in clinical narratives. The framework we have in-troduced here could easily be employed in other studies of clinical texts. 6 Future work In the future, we will use the results of this study in developing language technology for Finnish, Swe-dish and other Nordic ICU narratives. We will study how to identify abbreviations, misspellings and normalize and correct them, by using various distance measures and concept management tech-niques. We will also study how to automatically identify important parts of text and highlight them. Furthermore, we are interested in studying text provenance and pragmatics in this particular set-ting. In addition, we will evaluate the influence of 
these technology components in clinical practice. We will also address similarities and differences in clinical text written by various professional groups or at other hospital wards and health care units. Finally, we are eager to seek possibilities to incor-porate laymen's information needs and their inter-action with health care providers into our study. Acknowledgments We would like to thank Nordforsk and the Nordic Council of Ministers for the funding of our research network HEXAnord ? HEalth teXt Analysis network in the Nordic and Baltic countries and NICTA, funded by the Australian Government as represented by the De-partment of Broadband, Communications and the Digi-tal Economy and the Australian Research Council through the ICT Centre of Excellence program. We would also like to thank the Department of Information Technology and TUCS, University of Turku, Finland. References Lars Borin, Natalia Grabar, Catalina Hallett, Davis Hardcastle, Maria Toporowska Gronostaj, Dimitrios Kokkinakis, Sandra Williams and Alistair Willis. 2007. Empowering the patient with language tech-nology. SemanticMining NoE 507505: Deliverable D27.2. <http://gup.ub.gu.se/gup/record/index.xsql?pubid=53590>  Grace Yuet-Chee Chung. 2009. Towards identifying intervention arms in randomized controlled trials: ex-tracting coordinating constructions.  Journal of Bio-medical Informatics. 42(5):790?800  Hercules Dalianis, Martin Hassel and Sumithra Velupil-lai. 2009. The Stockholm EPR Corpus - Characteris-tics and Some Initial Findings. Proceedings of ISHIMR 2009, Evaluation and implementation of e-health and health information initiatives: interna-tional perspectives. 14th International Symposium for Health Information Management Research, Kal-mar, Sweden, 14-16 October, 2009, pp 243-249, pdf. Awarded best paper.  Anna Ehrenberg, Margareta Ehnfors and Ingrid Thorell-Ekstrand. 1996. Nursing documentation in patient re-cords: experience of the use of the VIPS model. Journal of Advanced Nursing 24, 853?867.  Martin Gellerstam, Yvonne Cederholm, and Torgny Rasmark. The bank of Swedish. In: Proceedings of LREC 2000 -- The 2nd International Conference on Language Resources and Evaluation, pages 329?333, Athens, Greece.  Udo Hahn and Joachim Wermter. 2004. High-performance tagging on medical texts. Proceedings of the 20th international conference on Computa-tional Linguistics. Geneva, Switzerland.  
59
Henk Harkema, Dowling JN, Thornblade T, Chapman WW. 2009. ConText: an algorithm for determining negation, experiencer, and temporal status from clin-ical reports. Journal of Biomedical Informatics 2009;42(5):839?51.  Ragnhild Helles?. 2005. Information handling in the nursing discharge notes, Journal of Clinical Nursing, Volume 15 Issue 1, 11 - 21. Blackwell publishing  Gunl?g Josefsson. 1999. F? feber eller tempa? N?gra tankar om agentivitet i medicinskt fackspr?k, Alla tiders spr?k: en v?nskrift till Gertrud Pettersson. Pages 127. Institutionen f?r nordiska spr?k. Lund. (In Swedish)  Hyeoneui Kim, Sergey Goryachev, Craciela Rosemblat, Allen Browne, Alla Keselman and Qing Zeng-Treitler. 2007. Beyond surface characteristics: a new health text-specific readability measurement. AMIA Annual Symp. 11:418-22.  Ola Knutsson, Johnny Bigert, and Vigg Kann. 2003. A robust shallow parser for Swedish. In Proceedings 14th Nordic Conf. on Comp. Ling. NODALIDA. Sirkka Lauri and Sanna Salanter?;. 2002. Developing an instrument to measure and describe clinical decision making in different nursing fields. Journal of Profes-sional Nursing. Mar-Apr;18(2), 93-100.  Lingsoft. 2010, Lingsoft Oy, http://www.lingsoft.fi/  Clement J. McDonald. 1997. The Barriers to Electronic Medical Record Systems and How to Overcome Them. JAMIA. 1997;4:213?221.   St?phane M. Meystre, Guergana K. Savova, Karin C. Kipper-Schuler and John E. Hurdle. 2008. Extracting Information from Textual Documents in the Elec-tronic Health Record: a Review of Recent Research. Yearbook Med Inform. 2008:128-44.  Raymond L. Ownby 2005. Influence of Vocabulary and Sentence Complexity and Passive Voice on the Readability of Consumer-Oriented Mental Health In-formation on the Internet. AMIA Annual Symposium Proceedings. 2005: 585?588. Serguei V. S. Pakhomov, Anni Coden and Christopher G. Chute. 2006. Developing a corpus of clinical notes manually annotated for part-of-speech. International Journal of Medical Informatics. 2006 Jun;75(6):418-29. Epub 2005 Sep 19. Patientdatalagen (2008:355) Svensk f?rfattnings-samling, Socialdepartementet, 2008, Stockholm. (In Swedish)  Hanna Suominen. 2009. Machine Learning and Clinical Text: Supporting Health Information Flow. TUCS Dissertations No 125, Turku Centre for Computer Science, 2009, Turku, Finland.  Hanna Suominen, Helj? Lundgr?n-Laine, Sanna Salan-ter?, Helena Karsten, and Tapio Salakoski. 2009. In-formation flow in intensive care narratives. In Chen J, Chen C, Ely J, Hakkani-Tr D, He J, Hsu H.-H, Liao L, Liu C, Pop  M, Ranganathan S, Reddy C.K, 
Ruan J, Song Y, Tseng V.S, Ungar L, Wu D, Wu Z, Xu K, Yu H, Zelikovsky A, editors. Proceedings IEEE International Conference on Bioinformatics and Biomedicine Workshops, BIBM 2009, pages 325?330. Institute of Electrical and Electronics Engi-neers, Los Alamitos, California, USA.  Kaarina Tanttu and Helena Ikonen. 2007. Nationally standardized electronic nursing documentation in Finland by the year 2007. Stud Health Technol In-form.122:540-1.  Asta Thoroddsen, Kaija Saranto, Anna Ehrenberg, Wal-ter Sermeus. 2009. Models, standards and structures of nursing documentation in European countries. Stud Health Technol Inform.146:327-31.  Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007. A Reappraisal of sentence and token splitting for life sciences documents. Stud Health Technol In-form. 129 (Pt 1):524-8. Webster?s 2010. Webster?s New World Medical Dic-tionary. http://www.medterms.com/script/main/hp.asp,last visited February 2, 2010. 
60
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 98?102,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Using collocation segmentation to augment the phrase table
Carlos A. Henr?quez Q.
?
, Marta R. Costa-juss?
?
, Vidas Daudaravicius?
Rafael E. Banchs
?
, Jos? B. Mari?o
?
?
TALP Research Center, Universitat Polit?cnica de Catalunya, Barcelona, Spain
{carlos.henriquez,jose.marino}@upc.edu
?
Barcelona Media Innovation Center, Barcelona, Spain
{marta.ruiz,rafael.banchs}@barcelonamedia.org
?Faculty of Informatics, Vytautas Magnus University, Kaunas, Lithuania
vidas@donelaitis.vdu.lt
Abstract
This paper describes the 2010 phrase-based
statistical machine translation system de-
veloped at the TALP Research Center of
the UPC
1
in cooperation with BMIC
2
and
VMU
3
. In phrase-based SMT, the phrase
table is the main tool in translation. It is
created extracting phrases from an aligned
parallel corpus and then computing trans-
lation model scores with them. Performing
a collocation segmentation over the source
and target corpus before the alignment
causes that different and larger phrases
are extracted from the same original doc-
uments. We performed this segmentation
and used the union of this phrase set with
the phrase set extracted from the non-
segmented corpus to compute the phrase
table. We present the configurations con-
sidered and also report results obtained
with internal and official test sets.
1 Introduction
The TALP Research Center of the UPC
1
in coop-
eration with BMIC
2
and VMU
3
participated in the
Spanish-to-English WMT task. Our primary sub-
mission was a phrase-based SMT system enhanced
with POS tags and our contrastive submission was
an augmented phrase-based system using colloca-
tion segmentation (Costa-juss? et al, 2010), which
mainly is a way of introducing new phrases in the
translation table. This paper presents the descrip-
tion of both systems together with the results that
we obtained in the evaluation task and is organized
as follows: first, Section 2 and 3 present a brief de-
scription of a phrase-based SMT, followed by a gen-
eral explanation of collocation segmentation. Sec-
tion 4 presents the experimental framework, corpus
used and a description of the different systems built
for the translation task; the section ends showing
the results we obtained over the official test set. Fi-
nally, section 5 presents the conclusions obtained
from the experiments.
1
Universitat Polit?cnica de Catalunya
2
Barcelona Media Innovation Center
3
Vytautas Magnus University
2 Phrase-based SMT
This approach to SMT performs the translation
splitting the source sentence in segments and as-
signing to each segment a bilingual phrase from
a phrase-table. Bilingual phrases are translation
units that contain source words and target words,
e.g. < unidad de traduccio?n | translation unit >,
and have different scores associated to them. These
bilingual phrases are then sorted in order to max-
imize a linear combination of feature functions.
Such strategy is known as the log-linear model
(Och and Ney, 2003) and it is formally defined as:
e? = arg max
e
[
M?
m=1
?mhm (e, f)
]
(1)
where hm are different feature functions with
weights ?m. The two main feature functions
are the translation model (TM) and the target
language model (LM). Additional models include
POS target language models, lexical weights, word
penalty and reordering models among others.
3 Collocation segmentation
Collocation segmentation is the process of de-
tecting boundaries between collocation segments
within a text (Daudaravicius and Marcinkeviciene,
2004). A collocation segment is a piece of text be-
tween boundaries. The boundaries are established
in two steps using two different measures: the Dice
score and a Average Minimum Law (AML).
The Dice score is used to measure the associa-
tion strength between two words. It has been used
before in the collocation compiler XTract (Smadja,
1993) and in the lexicon extraction system Cham-
pollion (Smadja et al, 1996). It is defined as fol-
lows:
Dice (x; y) =
2f (x, y)
f (x) + f (y)
(2)
where f (x, y) is the frequency of co-occurrence of
x and y, and f (x) and f (y) the frequencies of
occurrence of x and y anywhere in the text. It gives
high scores when x and y occur in conjunction.
The first step then establishes a boundary between
98
two adjacent words when the Dice score is lower
than a threshold t = exp (?8). Such a threshold
was established following the results obtained in
(Costa-juss? et al, 2010), where an integration of
this technique and a SMT system was performed
over the Bible corpus.
The second step of the procedure uses the AML.
It defines a boundary between words xi?1 and xi
when:
Dice (xi?2;xi?1) +Dice (xi;xi+1)
2
> Dice (xi?1;xi)
(3)
That is, the boundary is set when the Dice value
between words xi and xi?1 is lower than the aver-
age of preceding and following values.
4 Experimental Framework
All systems were built using Moses (Koehn et al,
2007), a state-of-the-art software for phrase-based
SMT. For preprocessing Spanish, we used Freeling
(Atserias et al, 2006), an open source library of
natural language analyzers. For English, we used
TnT (Brants, 2000) and Moses' tokenizer. The
language models were built using SRILM (Stolcke,
2002).
4.1 Corpus
This year, the translation task provided four dif-
ferent sources to collect corpora for the Spanish-
English pair. Bilingual corpora included version 5
of the Europarl Corpus (Koehn, 2005), the News
Commentary corpus and the United Nations cor-
pus. Additional English corpora was available from
the News corpus. The organizers also allowed the
use of the English Gigaword Third and Fourth Edi-
tion, released by the LDC. As for development
and internal test, the test sets from 2008 and 2009
translation tasks were available.
For our experiments, we selected as training data
the union of the Europarl and the News Commen-
tary. Development was performed with a section
of the 2008 test set and the 2009 test set was se-
lected as internal test. We deleted all empty lines,
removed pairs that were longer than 40 words, ei-
ther in Spanish or English; and also removed pairs
whose ratio between number of words were bigger
than 3.
As a preprocess, all corpora were lower-cased
and tokenized. The Spanish corpus was tokenized
and POS tags were extracted using Freeling, which
split clitics from verbs and also separated words
like del into de el. In order to build a POS tar-
get language model, we also obtained POS tags
from the English corpus using the TnT tagger.
Statistics of the selected corpus can be seen in Ta-
ble 1.
Corpora Spanish English
Training sent 1, 180, 623 1, 180, 623
Running words 26, 454, 280 25, 291, 370
Vocabulary 118, 073 89, 248
Development sent 1, 729 1, 729
Running words 37, 092 34, 774
Vocabulary 7, 025 6, 199
Internal test sent 2, 525 2, 525
Running words 69, 565 65, 595
Vocabulary 10, 539 8, 907
Official test sent 2, 489 -
Running words 66, 714 -
Vocabulary 10, 725 -
Table 1: Statistics for the training, development
and test sets.
Internal test Official test
Adjectives 137 72
Common nouns 369 188
Proper nouns 408 2, 106
Verbs 213 128
Others 119 168
Total 1246 2662
Table 2: Unknown words found in internal and
official test sets
It is important to notice that neither the United
Nations nor the Gigaword corpus were used for
bilingual training. Nevertheless, the English part
from the United Nations and the monolingual
News corpus were used to build the language model
of our systems.
4.1.1 Unknown words
We analyzed the content from the internal and of-
ficial test and realized that they both contained
many words that were not seen in the training data.
Table 2 shows the number of unknown words found
in both sets, classified according to their POS.
In average, we may expect an unknown word
every two sentences in the internal test and more
than one per sentence in the official test set. It can
also be seen that most of those unknown words are
proper nouns, representing 32% and 79% of the
unknown sets, respectively. Common nouns were
the second most frequent type of unknown words,
followed by verbs and adjectives.
4.2 Systems
We submitted two different systems for the trans-
lation task. First a baseline using the training data
mentioned before; and then an augmented system,
where the baseline-extracted phrase list was ex-
tended with additional phrases coming from a seg-
mented version of the training corpus.
We also considered an additional system built
99
with two different decoding path, a standard path
from words to words and POS and an alternative
path from stems to words and POS in the target
side. At the end, we did not submit this system
to the translation task because it did not provide
better results than the previous two in our internal
test.
The set of feature functions used include: source-
to-target and target-to-source relative frequen-
cies, source-to-target and target-to-source lexical
weights, word and phrase penalties, a target lan-
guage model, a POS target language model, and a
lexicalized reordering model (Tillman, 2004).
4.2.1 Considering stems as an alternate
decoding path.
Using Moses' framework for factored translation
models we defined a system with two decoding
paths: one decoding path using words and the
other decoding path using stems in the source lan-
guage and words in the target language. Both de-
coding paths only had a single translation step.
The possibility of using multiple alternative decod-
ing path was developed by Birch et. al. (2007).
This system tried to solve the problem with the
unknown words. Because Spanish is morphologi-
cally richer than English, this alternative decoding
path allowed the decoder translate words that were
not seen in the training data and shared the same
root with other known words.
4.2.2 Expanding the phrase table using
collocation segmentation.
In order to build the augmented phrase table with
the technique mentioned in section 3, we seg-
mented each language of the bilingual corpus in-
dependently and then, using the collocation seg-
ments as words, we aligned the corpus and ex-
tracted the phrases from it. Once the phrases were
extracted, the segments of each phrase were split
again in words to have standard phrases. Finally,
we use the union of this phrases and the phrases
extracted from the baseline system to compute the
final phrase table. A diagram of the whole proce-
dure can be seen in figure 1.
The objective of this integration is to add new
phrases in the translation table and to enhance
the relative frequency of the phrases that were ex-
tracted from both methods.
4.2.3 Language model interpolation.
Because SMT systems are trained with a bilingual
corpus, they ended highly tied to the domain the
corpus belong to. Therefore, when the documents
we want to translate belong to a different domain,
additional domain adaptation techniques are rec-
ommended to build the system. Those techniques
usually employ additional corpora that correspond
to the domain we want to translate from.
internal test
baseline 24.25
baseline+stem 23.45
augmented 23.9
Table 3: Internal test results.
test testcased?detok
baseline 26.1 25.1
augmented 26.1 25.1
Table 4: Results from translation task
The test set for this translation task comes from
the news domain, but most of our bilingual cor-
pora belonged to a political domain, the Europarl.
Therefore we use the additional monolingual cor-
pus to adapt the language model to the news do-
main.
The strategy used followed the experiment per-
formed last year in (R. Fonollosa et al, 2009).
We used SRILM during the whole process. All
language models were order five and used modi-
fied Kneser-Ney discount and interpolation. First,
we build three different language models accord-
ing to their domain: Europarl, United Nations and
news; then, we obtained the perplexity of each lan-
guage model over the News Commentary develop-
ment corpus; next, we used compute-best-mix to
obtain weights for each language model that di-
minish the global perplexity. Finally, the models
were combined using those weights.
In our experiments all systems used the resulting
language model, therefore the difference obtained
in our results were cause only by the translation
model.
4.3 Results
We present results from the three systems devel-
oped this year. First, the baseline, which included
all the features mentioned in section 4.2; then, the
system with an alternative decoding path, called
baseline+stem; and finally the augmented system,
which integrated collocation segmentation to the
baseline. Internal test results can be seen in table
3. Automatic scores provided by the WMT 2010
organizers for the official test can be found in ta-
ble 4. All BLEU scores are case-insensitive and
tokenized except for the official test set which also
contains case-sensitive and non-tokenized score.
We obtained a BLEU score of 26.1 and 25.1 for
our case-insensitive and sensitive outputs, respec-
tively. The highest score was obtained by Uni-
versity of Cambridge, with 30.5 and 29.1 BLEU
points.
100
Figure 1: Example of the expansion of the phrase table using collocation segmentation. New phrases
added by the collocation-based system are marked with a ??.
4.3.1 Comparing systems
Once we obtained the translation outputs from the
baseline and the augmented system, we performed
a manual comparison of them. Even though we
did not find any significant advantages of the aug-
mented system over the baseline, the collocation
segmentation strategy chose a better morphologi-
cal structures in some cases as can be seen in Table
5 (only sentence sub-segments are shown):
5 Conclusion
We presented two different submissions for the
Spanish-English language pair. The language
model for both system was built interpolating two
big out-of-domain language models and one smaller
in-domain language model. The first system was a
baseline with POS target language model; and the
second one an augmented system, that integrates
the baseline with collocation segmentation. Re-
sults over the official test set showed no difference
in BLEU between these two, even though internal
results showed that the baseline obtained a better
score.
We also considered adding an additional decod-
ing path from stems to words in the baseline but
internal tests showed that it did not improve trans-
lation quality either. The high number of unknown
words found in Spanish suggested us that consider-
ing in parallel the simple form of stems could help
us achieve better results. Nevertheless, a deeper
study of the unknown set showed us that most
of those words were proper nouns, which do not
have inflection and therefore cannot benefited from
stems.
Finally, despite that internal test did not showed
an improvement with the augmented system, we
submitted it as a secondary run looking for the
effect these phrases could have over human evalu-
ation.
Acknowledgment
The research leading to these results has received
funding from the European Community's Seventh
Framework Programme (FP7/2007-2013) under
grant agreement number 247762, from the Span-
ish Ministry of Science and Innovation through the
Buceador project (TEC2009-14094-C04-01) and
the Juan de la Cierva fellowship program. The
authors also wants to thank the Barcelona Media
Innovation Centre for its support and permission
to publish this research.
References
Jordi Atserias, Bernardino Casas, Elisabet
Comelles, Meritxell Gonz?lez, Llu?s Padr?, and
Muntsa Padr?. 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP
101
Original: sabiendo que est? recibiendo el premio
Baseline: knowing that it receive the prize
Augmented: knowing that he is receiving the prize
Original: muchos de mis amigos prefieren no separarla.
Baseline: many of my friends prefer not to separate them.
Augmented: many of my friends prefer not to separate it.
Original: Los estadounidenses contar?n con un tel?fono m?vil
Baseline: The Americans have a mobile phone
Augmented: The Americans will have a mobile phone
Original: es plenamente consciente del camino m?s largo que debe emprender
Baseline: is fully aware of the longest journey must undertake
Augmented: is fully aware of the longest journey that need to be taken
Table 5: Comparison between baseline and augmented outputs
library. In Proceedings of the fifth interna-
tional conference on Language Resources and
Evaluation (LREC 2006), ELRA, Genoa, Italy,
May.
Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2007. Ccg supertags in factored statis-
tical machine translation. In StatMT '07: Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 916, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Thorsten Brants. 2000. TnT  a statistical part-
of-speech tagger. In Proceedings of the Sixth
Applied Natural Language Processing (ANLP-
2000), Seattle, WA.
Marta R. Costa-juss?, Vidas Daudaravicius, and
Rafael E. Banchs. 2010. Integration of statisti-
cal collocation segmentations in a phrase-based
statistical machine translation system. In 14th
Annual Conference of the European Association
for Machine Translation.
Vidas Daudaravicius and Ruta Marcinkeviciene.
2004. Gravity counts for the boundaries of col-
locations. International Journal of Corpus Lin-
guistics, 9:321348(28).
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL '07: Proceedings of
the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions, pages
177180, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models. Computational Linguistics, 29:19
51.
Jos? A. R. Fonollosa, Maxim Khalilov, Marta R.
Costa-juss?, Jos? B. Mari?o, Carlos A. Hen-
r?quez Q., Adolfo Hern?ndez H., and Rafael E.
Banchs. 2009. The TALP-UPC phrase-based
translation system for EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 8589, Athens,
Greece, March. Association for Computational
Linguistics.
Frank A. Smadja, Kathleen McKeown, and
Vasileios Hatzivassiloglou. 1996. Translating
collocations for bilingual lexicons: A statistical
approach. Computational Linguistics, 22(1):1
38.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Comput. Linguist., 19(1):143177.
Andreas Stolcke. 2002. SRILM  an extensible
language modeling toolkit. pages 901904.
Christoph Tillman. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In
HLT-NAACL.
102
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 225?232,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
VTEX Determiner and Preposition Correction System
for the HOO 2012 Shared Task
Vidas Daudaravic?ius
VTEX
Akademijos 4
LT-08412 Vilnius, Lithuania
vidas.daudaravicius@vtex.lt
Abstract
This paper describes the system has been
developed for the HOO 2012 Shared Task.
The task was to correct determiner and
preposition errors. I explore the possibil-
ity of learning error correcting rules from
the given manually annotated data using
features such as word length and word
endings only. Furthermore, I employ er-
ror correction ranking based on the ratio
of the sentence probabilities using original
and corrected language models. Our sys-
tem has been ranked for the ninth posi-
tion out of thirteen teams. The best result
was achieved in correcting missing prepo-
sitions, which was ranked for the sixth po-
sition.
1 Introduction
The correct usage of determiners and preposi-
tions is one of the toughest problems in English
language use for non-native speakers, especially
those living in a non-English speaking environ-
ment. The issues have been explored extensively
in the literature (see Leacock et al (2010)). It
was interesting to find that this error correction
topic was chosen for the HOO 2012 Shared Task.
This paper describes the experimental sys-
tem developed by VTEX team for this task ?
to correct determiner and preposition errors in
CLC FCE Dataset. It explores the possibility
of learning error correcting rules from the given
manually annotated data using features such as
word length and word endings only. Further-
more, it employs error correction ranking based
on the ratio of sentence probabilities using orig-
inal and corrected language models.
2 The data
The training data consisted of 1000 files drawn
from the publicly available FCE dataset and
converted into HOO data format (see Dale et
al. (2012)). I used the HOO 2012 training and
test data only. The training data had 8432 man-
ually annotated corrections of the following six
error types:
MD ? Missing Determiner;
MT ? Missing Preposition;
UD ? Unwanted Determiner;
UT ? Unwanted Preposition;
RD ? Replacement Determiner;
RT ? Replacement Preposition.
The total size of the training data was 374680
words. The test data consisted of 100 previ-
ously unseen files without error correction an-
notations. For more details about the training
and test data, see (Dale et al, 2012).
I have not used any other dictionaries, cor-
pora or language processing tools (like taggers
or parsers). Thus, the system is language in-
dependent and based on supervised learning of
manually annotated corrections.
3 Word length and word ending
The training corpus was small and insufficient
to get complete and reliable features and statis-
tics of error corrections based on the corrected
words. Therefore I needed to find features
which describe the contexts of error corrections
225
in a more generalized way. After some experi-
mentation, I chose word length and the word last
n characters. Words in the dataset were trans-
formed into tokens using these functions. I have
tested three word transformation combinations:
word ? keeps the whole word (e.g. make 7?
make);
2end ? takes the length of a word and adds the
last two characters (make 7? 4 ke );
1end ? takes the length of a word and adds the
last character (make 7? 4 e).
I have also used lists of reserved words that
were used to preserve the primary form of a
word:
corrections ? words that were corrected
to/from in HOO 2012 Gold Edits data;
mod ? functional words such as: have, has, can,
not, make, made, be, was, were, am, are,
and, or ;
pronouns ? pronouns that were not used as
corrections: we, he, she, they, yours, ours,
them.
For instance, using 2end transformation, the
incorrect sentence I feel that festival could be
even better next year was transformed into I 4el
that 8al 5ld be 4en 6er next 4ar, and the cor-
rected sentence into I 4el that the 8al 5ld be 4en
6er next 4ar.
In Section 5, I show that the word length
and ending retain a lot of information about the
word.
Each participating group in HOO 2012 Shared
Task was allowed to submit up to ten runs.
I have submitted nine runs that differ in word
length and word ending only. The different runs
are:
0 ? 1end: all words except reserved correction
words were encoded as word length+the last
character;
1 ? 2end: all words except reserved correction
words were encoded as word length + two
last characters;
2 ? word: no transformations;
3 ? 1end+mod: all words except reserved correc-
tion and mod words were encoded as word
length + the last character;
4 ? 2end+mod: all words except reserved correc-
tion and mod words were encoded as word
length + two last characters;
5 ? 1end+pron: all words except reserved cor-
rection and pronoun words were encoded as
word length + the last character;
6 ? 2end+pron: all words except reserved cor-
rection and pronoun words were encoded as
word length + two last characters;
7 ? 1end+mod+pron: all words except reserved
correction, pronoun and mod words were en-
coded as word length + the last character;
8 ? 2end+mod+pron: all words except reserved
correction, pronoun and mod words were en-
coded as word length + two last characters.
4 Error correction
Error correction consists of the rules that the
system is able to learn, and the actions that
the system is able to apply.
4.1 Error correction rules
Using error correction annotations from Gold
Edits of the training corpus we have built the
error correction rules. The error correction rule
is the error correction and the context of this
correction. From the training corpus I gather
contextual correction rules. The context are to-
kens on the left- or right-hand side of the error
correction. The best choice would be to take at
least two tokens on the left-hand side and two to-
kens on the right-hand side and to express error
correction rule as a 5-gram with the error correc-
tion in the middle. For instance, in the training
data, the error correction of for to about of type
RT is found within the the left-hand side con-
text i asked and the right-hand side context the
discounts.
The main problem of learning of correction
rules was the small size of the training corpus.
226
Bigger corpora could help in learning more cor-
rection rules. But it is hard to get bigger corpora
because it is very expensive to prepare them.
Two or three word context on each side of a
corrected fragment can produce good but rarely
applicable correction rules. Therefore, I have
implemented a smoothing technique for generat-
ing new rules that do not appear in the training
data.
I use trigrams to generate smoothed 5-gram
error correction rules. Three types of trigrams
were used for the smoothing:
centered ? one token on the left-hand side of
the correction, then the correction and one
token on the right-hand side of the correc-
tion (see line 1 in Table 1);
left ? two tokens on the left-hand side of the
correction and the correction (see lines 2
and 3 in Table 1);
right ? two tokens on the right-hand side of the
correction and the correction (see lines 4?13
in Table 1).
There are 8432 corrections in the training
data. Figure 1 shows the number of distinct
trigram rules for the different runs described in
Section 3. Most of the trigram rules appear
once. For instance,
L2 L1 type original correction R1 R2
asked RT for about the
i asked RT for about
to asked RT for about
RT for about the camp
RT for about the discounts
RT for about the experience
RT for about the first
RT for about the new
RT for about the news
RT for about the play
RT for about the prise
RT for about the terrible
RT for about the very
Table 1: Trigram error correction rules.
? the most frequent (38 occurrences) left-
context trigram rule without word encoding
is stay in /a/MD ;
? the most frequent (44 occurrences) right-
context trigram rule is on/in/RT july be-
cause; and
? the most frequent (38 occurrences)
centered-context trigram rule is travel
on/in/RT july.
We could expect similar generalization power
for left, right or centered contexts, but in Fig. 1
we can see that the number of distinct right-
hand side contexts is lower by 5% compare to
Figure 1: The number of context trigrams of error corrections for the different runs.
227
the number of distinct centered contexts. Sur-
prisingly, the number of trigram rules does not
degrade significantly whether the encoding 1end
is used or not.
The new smoothed 5-gram rules are exten-
sions of the centered trigram rules. The exten-
sion on the left hand-side is the union of centered
trigrams and the left trigrams when the error
correction and L1 match. And the extension on
the right hand-side is the union of the centered
trigrams and the right trigrams when error cor-
rection and R1 match. For instance, the error
correction of for to about of type RT within the
the left-hand side context i asked and the right-
hand side context the discounts is extended as
follows:
? take centered trigram (see line 1 in Table 1);
? take left trigrams, where correction and L1
match (see lines 2 and 3 in Table 1);
? take right trigrams, where correction and
R1 match (see lines 4?13 in Table 1);
? after that I have the following smoothed
rule: L2 = [I, to ], L1 = asked, C =
for/about/MT, R1 = the, R2 = [camp, dis-
counts, experience, first, news, play, prise,
terrible, very ].
This technique allows the generation of error
correction rules that do not appear in the train-
ing data, e.g. in the latter example I generate 18
smoothed 5-gram rules that do not appear in the
training data. The new smoothed 5-gram error
correction rule is boolean operation and the rule
does not contain any probabilistic information.
4.2 Error correction actions
The error correction system applies error correc-
tion rules using the following actions:
do not change ? word is kept as is;
insert ? missing word is inserted;
delete ? unnecessary word is deleted;
replace ? word is replaced by another one.
Each action is tested at each word but only
one at a time. In case the context allows to
apply several actions at one place then these ac-
tions are treated as alternatives. Alternative ac-
tions are not combined and no selection between
Doc
ID
Run Rules
applied
OC
ratio
sentence correction
2025
2
the//MD/
that/this/RD/
0.451 is the 8 th july till the end of that month , what do you think ?
that/this/RD/ 0.559 is the 8 th july till end of that month , what do you think ?
the//MD/ 0.633 is the 8 th july till the end of this month , what do you think ?
? 0.785 is the 8 th july till end of this month , what do you think ?
7
the//MD/
that/this/RD/
0.345 3s the 8 2h 4y till the 3d of that 5h , what 2o you 5k ?
that/this/RD/ 0.441 3s the 8 2h 4y till 3d of that 5h , what 2o you 5k ?
the//MD/ 0.533 3s the 8 2h 4y till the 3d of this 5h , what 2o you 5k ?
? 0.683 3s the 8 2h 4y till 3d of this 5h , what 2o you 5k ?
2043
2 /for/UT/ 0.976 i am writing in response to your last letter , to answer and askyou for some questions .
? 1.035 i am writing in response to your last letter , to answer and ask
you for some questions .
7
/for/UT/ 0.966 i am 7g in 8e to your 4t 6r , to 6r and 3k you for some 9s .
a//MD/
/for/UT/
1.022 i am 7g in a 8e to your 4t 6r , to 6r and 3k you for some 9s .
? 1.025 i am 7g in 8e to your 4t 6r , to 6r and 3k you for some 9s .
a//MD/ 1.085 i am 7g in a 8e to your 4t 6r , to 6r and 3k you for some 9s .
Table 2: Examples of ranking, selection and application of actions for sentence correction.
228
them is made at this step. The example of cor-
rection alternatives is shown in Table 2. Besides,
the probability of the action can be taken into
account but I do not do this and all actions are
considered equally possible.
5 Language model
I use language trigram modeling to estimate the
probability of a sentence. The probability of a
sequence of words is estimated as the product of
probabilities of trigrams:
p(x) =
?
i
p?(xi |xi?2, xi?1).
To avoid zero probability I have used Kneser?
Ney trigram smoothing (Kneser and Ney, 1995)
technique as follows:
p?(xi |xi?2, xi?1)
=
max[(freq(xi?2, xi?1, xi)? c3), 0]
max[freq(xi?2, xi?1), 1]
+
c3 ? |xi?2, xi?1, ?|
max[freq(xi?2, xi?1), 1]
?
max[(freq(xi?2, xi?1)? c2), 0]
max[freq(xi?2), 1]
+
c2 ? |xi?2, ?, ?|
max[freq(xi?2), 1]
?
max[(freq(xi?2)? c1), 0]
N
+
c1 ? T
N
,
where c3 = 0.8, c2 = 0.6, c1 = 0.4, T = | ? |, and
N is the corpus size.
I have built two language models: one for the
original language and one for the corrected lan-
guage. The original language model (O) was
built using the corpus without corrections. The
corrected language model (C ) was built using
the corpus with error corrections applied. The
different runs yield different number of token tri-
grams. But the number does not degrade signif-
icantly as we might expect when words are en-
coded with the 1end transformation (see Fig. 2).
Thus, the 1end transformation retains a lot of
information, although, the number of trigrams
of the original language model is always a little
bit higher than the number of trigrams of the
corrected language model.
6 The probability ratio of the
original and corrected language
models
The probability of a sentence depends on the
length of the sentence. The longer the sen-
tence the lower the probability. Error correc-
tion actions can change the length of a sentence.
Thus, it is hard to implement the error correc-
tion system which should rank different length
sentences. Therefore, I have used the ratio of the
probabilities of the sentence using the original
language model (O) and the corrected language
Figure 2: The number of trigrams of the original and corrected language models for different runs.
229
Figure 3: The histogram of OC ratio in the test data.
model (C ):
OC ratio =
p?(O)
p?(C)
,
where p?(O) is the probability of a sentence us-
ing the original language model and p?(C) is the
probability of the same sentence using the cor-
rected language model.
The lower the value of this ratio, the higher
the chance that the sentence is correct, i.e.
closer to corrected language rather than to orig-
inal language. In Fig. 3, I show the histogram of
the highest OC ratios of the corrected test sen-
tences. This histogram shows that most of the
ratios are close to 1, i.e. the probabilities of the
sentence are almost equal using both language
models. The histogram does not depend on the
type of word encoding. In Table 2, I show ex-
amples of corrections and the OC ratios for each
set of corrections. The error correction system
takes corrections which are applied for the sen-
tence with the lowest OC ratio (see Table 2).
7 The results and conclusions
The results for different runs of the error correc-
tion system are shown in the Table 3. The best
determiner and preposition correction F-score
results are achieved with Run 5, which is using
1end + pron encoding: all words except reserved
correction and pronoun words were encoded as
word length + the last character. This result was
ranked for ninth position out of 14 teams.
Nevertheless, the results for different types of
corrections are quite different. The error cor-
rection system was capable of performing UT,
MT and MD type error corrections but hopeless
for UD, RD and RT type error corrections. The
best results are for:
MT ? missing preposition error correction, no
encoding is used;
MD - missing determiner error correction, 2end
encoding is used;
UT - unwanted preposition error correction,
any type of encoding except no encoding.
Surprisingly, we had to use whole words for
missing preposition error correction, but never
for unwanted preposition error correction. Our
system was ranked at the seventh position for
UT error correction using F-score.
The result for MT error correction shows that
smoothed 5-gram rule generation was useful and
the whole word should be used. But encoding
with word length should never be used. Our
system is ranked at the sixth position for MT
error correction.
The result for MD error correction shows that
the system degrades when encodings with fewer
characters are used.
230
Run All MT MDP R F P R F P R F
0 8.15 4.19 5.54 4.65 3.50 4.00 7.84 9.60 8.63
1 24.5 2.87 5.13 12.5 3.51 5.48 34.8 6.40 10.8
2 35.5 2.43 4.54 25.0 3.51 6.15 46.7 5.60 10.0
3 8.41 3.75 5.19 5.56 3.51 4.30 8.27 8.80 8.53
4 25.0 2.87 5.15 13.3 3.51 5.56 34.8 6.40 10.8
5 8.76 4.19 5.67 5.00 3.51 4.12 8.57 9.60 9.06
6 24.5 2.87 5.14 12.5 3.51 5.48 34.8 6.40 10.8
7 9.04 3.75 5.30 5.71 3.51 4.35 9.17 8.80 8.98
8 25.0 2.87 5.15 13.3 3.51 5.56 34.8 6.40 10.8
Run UT UD RT RDP R F P R F P R F P R F
0 100 4.65 8.89 4.76 1.89 2.70 1.67 1.47 2.70 0 0 0
1 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 0
2 100 2.33 4.55 0 0 0 20.0 0.74 1.42 0 0 0
3 100 4.65 8.89 0 0 0 16.7 1.47 2.70 0 0 0
4 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 0
5 100 4.65 8.89 4.76 1.89 2.70 16.7 1.47 2.70 0 0 0
6 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 0
7 100 4.65 8.89 0 0 0 16.7 1.47 2.70 0 0 0
8 100 4.65 8.89 0 0 0 16.7 0.74 1.41 0 0 0
Table 3: Scores for correction of different runs.
The main conclusion is that there are no com-
mon features for all error corrections and the
different systems for different error types should
be implemented.
References
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Pro-
ceedings of the Seventh Workshop on Innovative
Use of NLP for Building Educational Applications,
Montreal, Canada, June.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
volume I, pages 181?184, Detroit, Michigan, May.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel Tetreault. 2010. Automated Gram-
matical Error Detection for Language Learners.
Morgan and Claypool Publishers.
231
MD
Team Run Precision Recall F-Score
CU 0 83.33 8.0 14.6
KU 2 1.98 20.0 3.6
LE 0 54.43 34.4 42.16
NA 1 29.09 38.4 33.1
NU 0 51.02 40.0 44.84
TC 3 6.21 7.2 6.67
TH 3 9.54 26.4 14.01
UI 0 51.92 43.2 47.16
UT 6 36.7 32.0 34.19
VA 0 6.4 6.4 6.4
VT 1 34.78 6.4 10.81
MT
Team Run Precision Recall F-Score
CU 1 5.68 8.77 6.9
KU 1 0.51 19.3 1.0
LE 0 50.0 5.26 9.52
NA 3 11.43 7.02 8.7
NU 0 38.46 17.54 24.1
TC 3 4.65 3.51 4.0
UI 5 42.86 15.79 23.08
VA 1 1.71 7.02 2.75
VT 2 25.0 3.51 6.15
UT
Team Run Precision Recall F-Score
CU 1 4.83 39.53 8.61
JU 1 2.91 6.98 4.11
KU 5 60.0 13.95 22.64
LE 1 32.14 20.93 25.35
NA 3 40.91 20.93 27.69
NU 0 40.0 13.95 20.69
TC 9 4.69 30.23 8.13
TH 1 10.32 30.23 15.38
VA 0 12.9 18.6 15.24
VT 0 100.0 4.65 8.89
UD
Team Run Precision Recall F-Score
CU 3 17.86 18.87 18.35
JU 1 4.84 5.66 5.22
KU 8 26.92 13.21 17.72
LE 0 22.67 32.08 26.56
NA 5 40.0 11.32 17.65
NU 0 33.33 9.43 14.71
TC 9 5.11 16.98 7.86
TH 1 38.89 13.21 19.72
UI 2 23.38 33.96 27.69
VA 0 7.06 11.32 8.7
VT 0 4.76 1.89 2.7
Table 4: Scores for correction.
232
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 66?75,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Applying Collocation Segmentation to the ACL Anthology Reference Corpus
Vidas Daudaravic?ius
Vytautas Magnus University / Vileikos 8, Lithuania
v.daudaravicius@if.vdu.lt
Abstract
Collocation is a well-known linguistic phe-
nomenon which has a long history of research
and use. In this study I employ collocation
segmentation to extract terms from the large
and complex ACL Anthology Reference Cor-
pus, and also briefly research and describe
the history of the ACL. The results of the
study show that until 1986, the most signifi-
cant terms were related to formal/rule based
methods. Starting in 1987, terms related to
statistical methods became more important.
For instance, language model, similarity mea-
sure, text classification. In 1990, the terms
Penn Treebank, Mutual Information , statis-
tical parsing, bilingual corpus, and depen-
dency tree became the most important, show-
ing that newly released language resources ap-
peared together with many new research areas
in computational linguistics. Although Penn
Treebank was a significant term only tem-
porarily in the early nineties, the corpus is still
used by researchers today. The most recent
significant terms are Bleu score and semantic
role labeling. While machine translation as a
term is significant throughout the ACL ARC
corpus, it is not significant for any particu-
lar time period. This shows that some terms
can be significant globally while remaining in-
significant at a local level.
1 Introduction
Collocation is a well-known linguistic phenomenon
which has a long history of research and use. The
importance of the collocation paradigm shift is
raised in the most recent study on collocations (Sere-
tan, 2011). Collocations are a key issue for tasks like
natural language parsing and generation, as well as
real-life applications such as machine translation, in-
formation extraction and retrieval. Collocation phe-
nomena are simple, but hard to employ in real tasks.
In this study I introduce collocation segmentation as
a language processing method, maintaining simplic-
ity and clarity of use as per the n-gram approach. In
the beginning, I study the usage of the terms collo-
cation and segmentation in the ACL Anthology Ref-
erence Corpus (ARC), as well as other related terms
such as word, multi-word, and n-gram. To evaluate
the ability of collocation segmentation to handle dif-
ferent aspects of collocations, I extract the most sig-
nificant collocation segments in the ACL ARC. In
addition, based on a ranking like that of TF -IDF ,
I extract terms that are related to different phenom-
ena of natural language analysis and processing. The
distribution of these terms in ACL ARC helps to un-
derstand the main breakpoints of different research
areas across the years. On the other hand, there was
no goal to make a thorough study of the methods
used by the ACL ARC, as such a task is complex
and prohibitively extensive.
2 ACL Anthology Reference Corpus
This study uses the ACL ARC version 20090501.
The first step was to clean and preprocess the corpus.
First of all, files that were unsuitable for the analysis
were removed. These were texts containing charac-
ters with no clear word boundaries, i.e., each charac-
ter was separated from the next by whitespace. This
problem is related to the extraction of text from .pdf
66
format files and is hard to solve. Each file in the
ACL ARC represents a single printed page. The file
name encodes the document ID and page number,
e.g., the file name C04-1001 0007.txt is made up of
four parts: C is the publication type, (20)04 is the
year, 1001 is the document ID, and 0007 is the page
number. The next step was to compile files of the
same paper into a single document. Also, headers
and footers that appear on each document page were
removed, though they were not always easily rec-
ognized and, therefore, some of them remained. A
few simple rules were then applied to remove line
breaks, thus keeping each paragraph on a single line.
Finally, documents that were smaller than 1 kB were
also removed. The final corpus comprised 8,581
files with a total of 51,881,537 tokens.
3 Terms in the ACL ARC related to
collocations
The list of terms related to the term collocation
could be prohibitively lengthy and could include
many aspects of what it is and how it is used. For
simplicitys sake, a short list of related terms, includ-
ing word, collocation, multiword, token, unigram,
bigram, trigram, collocation extraction and segmen-
tation, was compiled. Table 2 shows when these
terms were introduced in the ACL ARC: some terms
were introduced early on, others more recently. The
term collocation was introduced nearly 50 years ago
and has been in use ever since. This is not unex-
pected, as collocation phenomena were already be-
ing studied by the ancient Greeks (Seretan, 2011).
Table 2 presents the first use of terms, showing that
the terms segmentation, collocation and multiword
are related to a similar concept of gathering consec-
utive words together into one unit.
Term Count Documents Introduced in
word 218813 7725 1965
segmentation 11458 1413 1965
collocation 6046 786 1965
multiword 1944 650 1969
token 3841 760 1973
trigram 3841 760 1973/87
bigram 5812 995 1988
unigram 2223 507 1989
collocation extraction 214 57 1992
Table 1: Term usage in ACL ARC
While the term collocation has been used for
many years, the first attempt to define what a col-
location is could be related to the time period when
statistics first began to be used in linguistics heavily.
Until that time, collocation was used mostly in the
sense of an expression produced by a particular syn-
tactic rule. The first definition of collocation in ACL
ARC is found in (Cumming, 1986).
(Cumming, 1986): By ?collocation? I mean lex-
ical restrictions (restrictions which are not pre-
dictable from the syntactic or semantic properties of
the items) on the modifiers of an item; for example,
you can say answer the door but not answer the
window. The phenomenon which I?ve called col-
location is of particular interest in the context of a
paper on the lexicon in text generation because this
particular type of idiom is something which a gener-
ator needs to know about, while a parser may not.
It is not the purpose of this paper to provide a def-
inition of the term collocation, because at the mo-
ment there is no definition that everybody would
agree upon. The introduction of unigrams, bigrams
and trigrams in the eighties had a big influence on
the use of collocations in practice. N -grams, as
a substitute to collocations, started being used in-
tensively and in many applications. On the other
hand, n-grams are lacking in generalization capabil-
ities and recent research tends to combine n-grams,
syntax and semantics (Pecina, 2005) .
The following sections introduce collocation seg-
mentation and apply it to extracting the most signif-
icant collocation segments to study the main break-
points of different research areas in the ACL ARC.
4 Collocation Segmentation
The ACL ARC contains many different segmenta-
tion types: discourse segmentation (Levow, 2004),
topic segmentation (Arguello and Rose, 2006), text
segmentation (Li and Yamanishi, 2000), Chinese
text segmentation (Feng et al, 2004), word segmen-
tation (Andrew, 2006). Segmentation is performed
by detecting boundaries, which may also be of sev-
eral different types: syllable boundaries (Mu?ller,
2006), sentence boundaries (Liu et al, 2004), clause
boundaries (Sang and Dejean, 2001), phrase bound-
aries (Bachenko and Fitzpatrick, 1990), prosodic
boundaries (Collier et al, 1993), morpheme bound-
67
Term Source and Citation
word (Culik, 1965) : 3. Translation ?word by word? .
?Of the same simplicity and uniqueness is the decomposition of the sentence S in its
single words w1 , w2 , ..., wk separated by interspaces, so that it is possible to write
s = (w1 w2 ... wk ) like at the text.?
A word is the result of a sentence decomposition.
segmentation (Sakai, 1965): The statement ?x is transformed to y? is a generalization of the original
fact, and this generalization is not always true. The text should be checked before a
transformational rule is applied to it. Some separate steps for this purpose will save
the machine time. (1) A text to be parsed must consist of segments specified by the
rule. The correct segmentation can be done by finding the tree structure of the text.
Therefore, the concatenation rules must be prepared so as to account for the structure
of any acceptable string.
Collocation (Tosh, 1965): We shall include features such as lexical collocation (agent-action
agreement) and transformations of semantic equivalence in a systematic description
of a higher order which presupposes a morpho-syntactic description for each lan-
guage [8, pp. 66-71]. The following analogy might be drawn: just as strings of
alphabetic and other characters are taken as a body of data to be parsed and classified
by a phrase structure grammar, we may regard the string of rule numbers generated
from a phrase structure analysis as a string of symbols to be parsed and classified in a
still higher order grammar [11; 13, pp. 67-83], for which there is as yet no universally
accepted nomenclature.
multi-word (Yang, 1969): When title indices and catalogs, subject indices and catalogs, business
telephone directories, scientific and technical dictionaries, lexicons and idiom-and-
phrase dictionaries, and other descriptive multi-word information are desired, the
first character of each non-trivial word may be selected in the original word sequence
to form a keyword. For example, the rather lengthy title of this paper may have a
keyword as SADSIRS. Several known information systems are named exactly in this
manner such as SIR (Raphael?s Semantic Information Retrieval), SADSAM (Lind-
say?s Sentence Appraiser and Diagrammer and Semantic Analyzing Machine), BIRS
(Vinsonhaler?s Basic Indexing and Retrieval System), and CGC (Klein and Simmons?
Computational Grammar Coder).
token (Beebe, 1973): The type/token ratio is calculated by dividing the number of discrete
entries by the total number of syntagms in the row.
trigram (Knowles, 1973): sort of phoneme triples (trigrams), giving list of clusters and third-
order information-theoretic values.
(D?Orta et al, 1987): Such a model it called trigram language model. It is based
on a very simple idea and, for this reason, its statistics can be built very easily only
counting all the sequences of three consecutive words present in the corpus. On the
other hand, its predictive power is very high.
bigram (van Berkelt and Smedt, 1988): Bigrams are in general too short to contain any
useful identifying information while tetragrams and larger n-gram are already close
to average word length.
(Church and Gale, 1989): Our goal is to develop a methodology for extending an
n-gram model to an (n+l)-gram model. We regard the model for unigrams as com-
pletely fixed before beginning to study bigrams.
unigram the same as bigram for (Church and Gale, 1989)
collocation
extraction
(McKeown et al, 1992): Added syntactic parser to Xtract, a collocation extraction
system, to further filter collocations produced, eliminating those that are not consis-
tently used in the same syntactic relation.
Table 2: Terms introductions in ACL ARC.
aries (Monson et al, 2004), paragraph boundaries
(Filippova and Strube, 2006), word boundaries (Ryt-
ting, 2004), constituent boundaries (Kinyon, 2001),
topic boundaries (Tur et al, 2001).
Collocation segmentation is a new type of seg-
mentation whose goal is to detect fixed word se-
quences and to segment a text into word sequences
called collocation segments. I use the definition of
a sequence in the notion of one or more. Thus, a
collocation segment is a sequence of one or more
consecutive words that collocates and have colloca-
bility relations. A collocation segment can be of any
68
Figure 1: The collocation segmentation of the sentence a collocation is a recurrent and conventional fixed expression
of words that holds syntactic and semantic relations . (Xue et al, 2006).
length (even a single word) and the length is not de-
fined in advance. This definition differs from other
collocation definitions that are usually based on n-
gram lists (Tjong-Kim-Sang and S., 2000; Choueka,
1988; Smadja, 1993). Collocation segmentation is
related to collocation extraction using syntactic rules
(Lin, 1998). The syntax-based approach allows the
extraction of collocations that are easier to describe,
and the process of collocation extraction is well-
controlled. On the other hand, the syntax-based ap-
proach is not easily applied to languages with fewer
resources. Collocation segmentation is based on a
discrete signal of associativity values between two
consecutive words, and boundaries that are used to
chunk a sequence of words.
The main differences of collocation segmentation
from other methods are: (1) collocation segmenta-
tion does not analyze nested collocations it takes
the longest one possible in a given context, while the
n-gram list-based approach cannot detect if a collo-
cation is nested in another one, e.g., machine trans-
lation system; (2) collocation segmentation is able to
process long collocations quickly with the complex-
ity of a bigram list size, while the n-gram list-based
approach is usually limited to 3-word collocations
and has high processing complexity.
There are many word associativity measures,
such as Mutual Information (MI), T-score, Log-
Likelihood, etc. A detailed overview of associativ-
ity measures can be found in (Pecina, 2010), and
any of these measures can be applied to colloca-
tion segmentation. MI and Dice scores are almost
similar in the sense of distribution of values (Dau-
daravicius and Marcinkeviciene, 2004), but the Dice
score is always in the range between 0 and 1, while
the range of the MI score depends on the corpus
size. Thus, the Dice score is preferable. This score
is used, for instance, in the collocation compiler
XTract (Smadja, 1993) and in the lexicon extraction
system Champollion (Smadja et al, 1996). Dice is
defined as follows:
D(xi?1;xi) =
2 ? f(xi?1;xi)
f(xi?1) + f(xi)
where f(xi?1;xi) is the number of co-occurrence
of xi?1 and xi, and f(xi?1) and f(xi) are the num-
bers of occurrence of xi?1 and xi in the training cor-
pus. If xi?1 and xi tend to occur in conjunction,
their Dice score will be high. The Dice score is
sensitive to low-frequency word pairs. If two con-
secutive words are used only once and appear to-
gether, there is a good chance that these two words
are highly related and form some new concept, e.g.,
a proper name. A text is seen as a changing curve of
Dice values between two adjacent words (see Figure
1). This curve of associativity values is used to de-
tect the boundaries of collocation segments, which
can be done using a threshold or by following cer-
tain rules, as described in the following sections.
69
length unique segments segment count word count corpus coverage
1 289,277 31,427,570 31,427,570 60.58%
2 222,252 8,594,745 17,189,490 33.13%
3 72,699 994,393 2,983,179 5.75%
4 12,669 66,552 266,208 0.51%
5 1075 2,839 14,195 0.03%
6 57 141 846 0.00%
7 3 7 49 0.00%
Total 598,032 41,086,247 51,881,537 100%
Table 3: The distribution of collocation segments
2 word segments CTFIDF 3 word segments CTFIDF
machine translation 10777 in terms of 4099
speech recognition 10524 total number of 3926
training data 10401 th international conference 3649
language model 10188 is used to 3614
named entity 9006 one or more 3449
error rate 8280 a set of 3439
test set 8083 note that the 3346
maximum entropy 7570 it is not 3320
sense disambiguation 7546 is that the 3287
training set 7515 associated with the 3211
noun phrase 7509 large number of 3189
our system 7352 there is a 3189
question answering 7346 support vector machines 3111
information retrieval 7338 are used to 3109
the user 7198 extracted from the 3054
word segmentation 7194 with the same 3030
machine learning 7128 so that the 3008
parse tree 6987 for a given 2915
knowledge base 6792 it is a 2909
information extraction 6675 fact that the 2876
4 word segments CTFIDF 5 word segments CTFIDF
if there is a 1690 will not be able to 255
human language technology conference 1174 only if there is a 212
is defined as the 1064 would not be able to 207
is used as the 836 may not be able to 169
human language technology workshop 681 a list of all the 94
could be used to 654 will also be able to 43
has not yet been 514 lexical information from a large 30
may be used to 508 should not be able to 23
so that it can 480 so that it can also 23
our results show that 476 so that it would not 23
would you like to 469 was used for this task 23
as well as an 420 indicate that a sentence is 17
these results show that 388 a list of words or 16
might be able to 379 because it can also be 16
it can also be 346 before or after the predicate 16
have not yet been 327 but it can also be 16
not be able to 323 has not yet been performed 16
are shown in table 320 if the system has a 16
is that it can 311 is defined as an object 16
if there is an 305 is given by an expression 16
Table 4: Top 20 segments for the segment length of two to five words.
70
4.1 Setting segment boundaries with a
Threshold
A boundary can be set between two adjacent words
in a text when the Dice value is lower than a cer-
tain threshold. We use a dynamic threshold which
defines the range between the minimum and the av-
erage associativity values of a sentence. Zero equals
the minimum associativity value and 100 equals the
average value of the sentence. Thus, the threshold
value is expressed as a percentage between the min-
imum and the average associativity values. If the
threshold is set to 0, then no threshold filtering is
used and no collocation segment boundaries are set
using the threshold. The main purpose of using a
threshold is to keep only strongly connected tokens.
On the other hand, it is possible to set the thresh-
old to the maximum value of associativity values.
This would make no words combine into more than
single word segments, i.e., collocation segmentation
would be equal to simple tokenization. In general,
the threshold makes it possible to move from only
single-word segments to whole-sentence segments
by changing the threshold from the minimum to the
maximum value of the sentence. There is no reason
to use the maximum value threshold, but this helps
to understand how the threshold can be used. (Dau-
daravicius and Marcinkeviciene, 2004) uses a global
constant threshold which produces very long collo-
cation segments that are like the clichs used in le-
gal documents and hardly related to collocations. A
dynamic threshold allows the problem of very long
segments to be reduced. In this study I used a thresh-
old level of 50 percent. An example of threshold is
shown in Figure 1. In the example, if the threshold
is 50 percent then segmentation is as follows: a |
collocation | is a | recurrent | and | conventional |
fixed | expression | of words that | holds | syntactic
| and | semantic relations | . To reduce the problem
of long segments even more, the Average Minimum
Law can also be used, as described in the following
section.
4.2 Setting segment boundaries with Average
Minimum Law
(Daudaravicius, 2010) introduces the Average Min-
imum Law (AML) for setting collocation segmen-
tation boundaries. AML is a simple rule which is
applied to three adjacent associativity values and is
expressed as follows:
boundary(xi?2, xi?1) =
=
?
?
?
True
D(xi?3;xi?2) + D(xi?1;xi)
2
< D(xi?2;xi?1)
False otherwise
The boundary between two adjacent words in the
text is set where the Dice value is lower than the av-
erage of the preceding and following Dice values.
In order to apply AML to the first two or last two
words, I use sequence beginning and sequence end-
ing as tokens and calculate the associativity between
the beginning of the sequence and the first word,
and the last word and the end of the sequence as
shown in Figure 1. AML can be used together with
Threshold or alone. The recent study of (Daudar-
avicius, 2012) shows that AML is able to produce
segmentation that gives the best text categorization
results, while the threshold degrades them. On the
other hand, AML can produce collocation segments
where the associativity values between two adjacent
words are very low (see Figure 1). Thus, for lexicon
extraction tasks, it is a good idea to use AML and a
threshold together.
5 Collocation segments from the ACL
ARC
Before the collocation segmentation, the ACL ARC
was preprocessed with lowercasing and tokeniza-
tion. No stop-word lists, taggers or parsers were
used, and all punctuation was kept. Collocation seg-
mentation is done on a separate line basis, i.e., for
each text line, which is usually a paragraph, the av-
erage and the minimum combinability values are de-
termined and the threshold is set at 50 percent, mid-
way between the average and the minimum. The Av-
erage Minimum Law is applied in tandem. The tool
CoSegment for collocation segmentation is available
at (http://textmining.lt/).
Table 3 presents the distribution of segments by
length, i.e., by the number of words. The length
of collocation segments varies from 1 to 7 words.
In the ACL ARC there are 345,455 distinct tokens.
After segmentation, the size of the segment list was
598,032 segments, almost double the length of the
single word list. The length of the bigram list is
71
4,484,358, which is more than 10 times the size of
the word list and 7 times that of the collocation seg-
ment list. About 40 percent of the corpus comprises
collocation segments of two or more words, showing
the amount of fixed language present therein. The
longest collocation segment is described in section
2 . 2 , which contains seven words (when punctu-
ation is included as words). This shows that collo-
cation segmentation with a threshold of 50 percent
and AML diverges to one-, two- or three-word seg-
ments. Despite that, the list size of collocation seg-
ments is much shorter than the list size of bigrams,
and shorter still than that of trigrams.
After segmentation, it was of interest to find the
most significant segments used in the ACL ARC.
For this purpose I used a modified TF-IDF which
is defined as follows:
CTFIDF (x) = TF (x)?ln
(
N ?D(x) + 1
D(x) + 1
)
where TF (x) is the raw frequency of segment x in
the corpus, N is the total number of documents in
the corpus, and D(x) is the number of documents
in which the segment x occurs. Table 4 presents the
top 20 collocation segments for two-, three-, four-
and five-word segments of items that contain alpha-
betic characters only. The term machine transla-
tion is the most significant in CTFIDF terms. This
short list contains many of the main methods and
datasets used in daily computational linguistics re-
search, such as: error rate, test set, maximum en-
tropy, training set, parse tree, unknown words, word
alignment, Penn Treebank, language models, mutual
information, translation model, etc. These terms
show that computational linguistics has its own ter-
minology, methods and tools to research many top-
ics.
Finally, 76 terms of two or more words in length
with the highest CTFIDF values were selected. The
goal was to try to find how significant terms were
used yearly in the ACL ARC. The main part of the
ACL ARC was compiled using papers published af-
ter 1995. Therefore, for each selected term, the av-
erage CTFIDF value of each document for each year
was calculated. This approach allows term usage
throughout the history of the ACL to be analysed,
and reduces the influence of the unbalanced amount
of published papers. Only those terms whose aver-
age CTFIDF in any year was higher than 20 were
kept. For instance, the term machine translation had
to be removed, as it was not significant throughout
all the years. Each term was ranked by the year
in which its average CTFIDF value peaked. The
ranked terms are shown in Table 5. For instance,
the peak of the CTFIDF average of the term sta-
tistical parsing occurred in 1990, of the term lan-
guage model in 1987, and of the term bleu score
in 2006. The results (see Table 5) show the main
research trends and time periods of the ACL com-
munity. Most of the terms with CTFIDF peaks
prior to 1986 are related to formal/rule-based meth-
ods. Beginning in 1987, terms related to statistical
methods become more important. For instance, lan-
guage model, similarity measure, and text classifi-
cation. The year 1990 stands out as a kind of break-
through. In this year, the terms Penn Treebank, Mu-
tual Information, statistical parsing, bilingual cor-
pus, and dependency tree became the most impor-
tant terms, showing that newly released language re-
sources were supporting many new research areas
in computational linguistics. Despite the fact that
Penn Treebank was only significant temporarily, the
corpus is still used by researchers today. The most
recent important terms are Bleu score and semantic
role labeling.
This study shows that collocation segmentation
can help in term extraction from large and complex
corpora, which helps to speed up research and sim-
plify the study of ACL history.
6 Conclusions
This study has shown that collocation segmentation
can help in term extraction from large and complex
corpora, which helps to speed up research and sim-
plify the study of ACL history. The results show that
the most significant terms prior to 1986 are related
to formal/rule based research methods. Beginning in
1987, terms related to statistical methods (e.g., lan-
guage model, similarity measure, text classification)
become more important. In 1990, a major turning
point appears, when the terms Penn Treebank, Mu-
tual Information, statistical parsing, bilingual cor-
pus, and dependency tree become the most impor-
tant, showing that research into new areas of compu-
72
tational linguistics is supported by the publication of
new language resources. The Penn Treebank, which
was only significant temporarily, it still used today.
The most recent terms are Bleu score and semantic
role labeling. While machine translation as a term
is significant throughout the ACL ARC, it is not sig-
nificant in any particular time period. This shows
that some terms can be significant globally, but in-
significant at a local level.
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 465?
472, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Jaime Arguello and Carolyn Rose. 2006. Topic-
segmentation of dialogue. In Proceedings of the An-
alyzing Conversations in Text and Speech, pages 42?
49, New York City, New York, June. Association for
Computational Linguistics.
J. Bachenko and E. Fitzpatrick. 1990. A computational
grammar of discourse-neutral prosodic phrasing in en-
glish. Computational Linguistics, 16:155?170.
Ralph D. Beebe. 1973. The frequency distribution of
english syntagms. In Proceedings of the International
Conference on Computational Linguistics, COLING.
Y. Choueka. 1988. Looking for needles in a haystack, or
locating interesting collocational expressions in large
textual databases. In Proceedings of the RIAO Confer-
ence on User-Oriented Content-Based Text and Image
Handling, pages 21?24, Cambridge, MA.
Kenneth W. Church and William A. Gale. 1989. En-
hanced good-turing and cat.cal: Two new methods for
estimating probabilities of english bigrams (abbrevi-
ated version). In Speech and Natural Language: Pro-
ceedings of a Workshop Held at Cape Cod.
Rene? Collier, Jan Roelof de Pijper, and Angelien San-
derman. 1993. Perceived prosodic boundaries and
their phonetic correlates. In Proceedings of the work-
shop on Human Language Technology, HLT ?93,
pages 341?345, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karel Culik. 1965. Machine translation and connected-
ness between phrases. In International Conference on
Computational Linguistics, COLING.
Susanna Cumming. 1986. The lexicon in text gener-
ation. In Strategic Computing - Natural Language
Workshop: Proceedings of a Workshop Held at Ma-
rina del Rey.
V. Daudaravicius and R Marcinkeviciene. 2004. Grav-
ity counts for the boundaries of collocations. Interna-
tional Journal of Corpus Linguistics, 9(2):321?348.
Vidas Daudaravicius. 2010. The influence of colloca-
tion segmentation and top 10 items to keyword assign-
ment performance. In Alexander F. Gelbukh, editor,
CICLing, volume 6008 of Lecture Notes in Computer
Science, pages 648?660. Springer.
Vidas Daudaravicius. 2012. Automatic multilingual an-
notation of eu legislation with eurovoc descriptors. In
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?2012).
Paolo D?Orta, Marco Ferretti, Alessandro Martelli, and
Stefano Scarci. 1987. An automatic speech recogni-
tion system for the italian language. In Third Confer-
ence of the European Chapter of the Association for
Computational Linguistics.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for chinese
word extraction. Computational Linguistics, 30:75?
93.
Katja Filippova and Michael Strube. 2006. Using lin-
guistically motivated features for paragraph boundary
identification. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 267?274, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Alexandra Kinyon. 2001. A language independent
shallow-parser compiler. In Proceedings of 39th An-
nual Meeting of the Association for Computational
Linguistics, pages 330?337, Toulouse, France, July.
Association for Computational Linguistics.
F. Knowles. 1973. The quantitative syntagmatic anal-
ysis of the russian and polish phonological systems.
In Computational And Mathematical Linguistics: Pro-
ceedings of the International Conference on Computa-
tional Linguistics, COLING.
Gina-Anne Levow. 2004. Prosodic cues to discourse
segment boundaries in human-computer dialogue. In
Michael Strube and Candy Sidner, editors, Proceed-
ings of the 5th SIGdial Workshop on Discourse and
Dialogue, pages 93?96, Cambridge, Massachusetts,
USA, April 30 - May 1. Association for Computational
Linguistics.
Hang Li and Kenji Yamanishi. 2000. Topic analysis
using a finite mixture model. In 2000 Joint SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 35?
44, Hong Kong, China, October. Association for Com-
putational Linguistics.
D. Lin. 1998. Extracting collocations from text cor-
pora. In First Workshop on Computational Terminol-
ogy, Montreal.
73
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2004. Comparing and combining generative
and posterior probability models: Some advances in
sentence boundary detection in speech. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 64?71, Barcelona, Spain, July. Association for
Computational Linguistics.
Kathleen McKeown, Diane Litman, and Rebecca Passon-
neau. 1992. Extracting constraints on word usage
from large text corpora. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Harriman.
Christian Monson, Alon Lavie, Jaime Carbonell, and
Lori Levin. 2004. Unsupervised induction of natural
language morphology inflection classes. In Proceed-
ings of the Seventh Meeting of the ACL Special Inter-
est Group in Computational Phonology, pages 52?61,
Barcelona, Spain, July. Association for Computational
Linguistics.
Karin Mu?ller. 2006. Improving syllabification models
with phonotactic knowledge. In Proceedings of the
Eighth Meeting of the ACL Special Interest Group on
Computational Phonology and Morphology at HLT-
NAACL 2006, pages 11?20, New York City, USA,
June. Association for Computational Linguistics.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL Student Research Workshop, pages 13?18, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):137?158.
C. Anton Rytting. 2004. Segment predictability as a cue
in word segmentation: Application to modern greek.
In Proceedings of the Seventh Meeting of the ACL
Special Interest Group in Computational Phonology,
pages 78?85, Barcelona, Spain, July. Association for
Computational Linguistics.
Itiroo Sakai. 1965. Some mathematical aspects on
syntactic discription. In International Conference on
Computational Linguistics, COLING.
Erik F. Tjong Kim Sang and Herve Dejean. 2001. Intro-
duction to the conll-2001 shared task: clause identifi-
cation. In Proceedings of the ACL 2001 Workshop on
Computational Natural Language Learning, Toulouse,
France, July. Association for Computational Linguis-
tics.
Violeta Seretan. 2011. Syntax-Based Collocation Ex-
traction, volume 44 of Text, Speech and Language
Technology. Springer.
Frank Smadja, Vasileios Hatzivassiloglou, and Kath-
leen R. McKeown. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22:1?38.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19:143?177.
E. Tjong-Kim-Sang and Buchholz S. 2000. Introduction
to the conll-2000 shared task: Chunking. In Proc. of
CoNLL-2000 and LLL-2000, pages 127?132, Lisbon,
Portugal.
L. W. Tosh. 1965. Data preparation for syntactic trans-
lation. In International Conference on Computational
Linguistics, COLING.
Gokhan Tur, Andreas Stolcke, Dilek Hakkani-Tur, and
Elizabeth Shriberg. 2001. Integrating prosodic and
lexical cues for automatic topic segmentation. Com-
putational Linguistics, 27:31?57.
Brigitte van Berkelt and Koenraad De Smedt. 1988. Tri-
phone analysis: A combined method for the correction
of orthographical and typographical errors. In Pro-
ceedings of the Second Conference on Applied Natu-
ral Language Processing, pages 77?83, Austin, Texas,
USA, February. Association for Computational Lin-
guistics.
Nianwen Xue, Jinying Chen, and Martha Palmer. 2006.
Aligning features with sense distinction dimensions.
In Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, pages 921?928, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Shou-Chuan Yang. 1969. A search algorithm and data
structure for an efficient information system. In In-
ternational Conference on Computational Linguistics,
COLING.
74
65
67
69
73
75
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
pa
rsi
ng
alg
or
ith
m
25
2
9
5
13
13
7
10
10
lex
ica
le
ntr
y
36
21
4
14
9
13
11
so
ur
ce
lan
gu
ag
e
11
21
4
15
9
12
wo
rd
se
ns
es
10
31
22
12
6
10
7
7
19
17
13
6
9
9
tar
ge
tla
ng
ua
ge
11
15
24
2
18
5
11
10
13
10
17
br
ow
nc
or
pu
s
4
36
16
6
30
21
18
21
20
6
14
9
29
log
ica
lf
or
m
8
21
11
17
13
2
6
9
18
12
19
15
16
16
17
14
8
13
8
11
11
10
10
se
ma
nti
cr
ep
res
en
tat
ion
9
4
3
21
9
11
mu
lti
-w
or
d
22
21
9
ref
ere
nc
er
es
olu
tio
n
4
7
8
41
9
30
17
13
16
18
9
9
23
14
12
lan
gu
ag
em
od
el
9
34
11
19
14
13
12
18
7
13
11
12
10
9
tex
tg
en
era
tio
n
24
17
9
25
25
13
9
29
19
12
7
sp
ok
en
lan
gu
ag
e
6
37
23
20
19
21
13
14
sp
ee
ch
rec
og
nit
ion
12
11
33
19
21
19
16
16
sim
ila
rit
ym
ea
su
re
13
13
33
17
15
10
tex
tc
las
sifi
ca
tio
n
55
23
17
11
16
sta
tis
tic
al
pa
rsi
ng
30
tre
ea
djo
ini
ng
gr
am
ma
rs
3
14
22
19
29
19
15
13
12
11
mu
tua
lin
fo
rm
ati
on
12
17
27
12
15
9
11
15
pe
nn
tre
eb
an
k
26
6
bil
ing
ua
lc
or
pu
s
22
11
19
9
11
9
16
17
12
11
de
pe
nd
en
cy
tre
e
10
8
9
11
8
9
7
10
21
15
12
11
13
16
11
16
15
11
po
st
ag
gin
g
23
14
10
11
10
10
10
11
13
9
sp
on
tan
eo
us
sp
ee
ch
16
8
34
22
20
42
16
17
18
9
13
10
tex
tc
ate
go
riz
ati
on
21
25
16
9
15
14
12
10
fea
tur
es
ele
cti
on
20
11
51
10
7
10
11
9
12
14
15
10
10
tra
ns
lat
ion
mo
de
l
28
27
49
17
11
15
18
15
14
10
sp
ell
ing
co
rre
cti
on
10
14
9
7
17
17
16
19
42
9
27
26
15
10
17
10
13
19
16
ed
itd
ist
an
ce
37
21
13
16
10
14
11
12
15
12
tar
ge
tw
or
d
42
13
9
12
9
16
14
20
13
11
sp
ee
ch
sy
nth
es
is
3
8
11
9
8
14
18
16
11
29
9
11
11
10
10
se
arc
he
ng
ine
27
31
16
12
20
11
9
10
ma
xim
um
en
tro
py
22
12
22
25
27
12
10
10
9
12
10
lex
ica
lr
ule
s
12
6
2
10
5
18
9
11
36
18
8
24
46
21
18
11
11
an
no
tat
ion
sc
he
me
15
14
31
21
14
15
11
18
10
co
ref
ere
nc
er
es
olu
tio
n
11
21
34
37
10
16
27
20
22
16
27
tex
ts
um
ma
riz
ati
on
15
36
17
14
13
13
13
12
na
ive
ba
ye
s
32
23
43
14
12
13
23
20
12
17
tri
gr
am
mo
de
l
20
20
16
13
13
14
38
10
13
11
13
10
na
me
de
nti
ty
20
12
14
11
14
31
12
10
19
16
10
11
an
ap
ho
ra
res
olu
tio
n
5
10
9
15
17
10
29
13
21
11
12
12
11
wo
rd
se
gm
en
tat
ion
20
19
26
16
16
13
10
11
20
31
26
24
30
wo
rd
ali
gn
me
nt
10
13
19
20
11
24
24
22
17
se
ma
nti
cr
ole
lab
eli
ng
25
25
28
ble
us
co
re
12
18
16
14
21
65
67
69
73
75
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
Ta
bl
e
5:
T
he
li
st
of
se
le
ct
ed
te
rm
s
an
d
th
e
ye
ar
ly
im
po
rt
an
ce
in
te
rm
s
of
C
T
F
ID
F.
75
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 89?95,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
VTEX System Description for the NLI 2013 Shared Task
Vidas Daudaravic?ius
VTEX
Akademijos 2a
Vilnius, Lithuania
vidas.daudaravicius@vtex.lt
Abstract
This paper describes the system developed
for the NLI 2013 Shared Task, requiring
to identify a writer?s native language by
some text written in English. I explore the
given manually annotated data using word
features such as the length, endings and
character trigrams. Furthermore, I em-
ploy k-NN classification. Modified TFIDF
is used to generate a stop-word list auto-
matically. The distance between two docu-
ments is calculated combining n-grams of
word lengths and endings, and character
trigrams.
1 Introduction
Native Language Identification (NLI) is the task
of identifying the first spoken language (L1) of
a person based on the person?s written text
in another language. As a natural language
processing (NLP) task, it is properly catego-
rized as text classification, and standard ap-
proaches like support vector machines (SVM)
are successufully applied to it. Koppel et al
(2005) trained SVM models with a set of stylis-
tic features, including Part of Speech (POS) and
character n-grams (sequences), function words,
and spelling error types, achieving 80% accu-
racy in a 5-language task. Tsur and Rappoport
(2007) focused on character n-grams. Wong and
Dras (2011) showed that syntactic patterns, de-
rived by a parser, are more effective than other
stylistic features. The Cambridge Learner Cor-
pus has been used recently by Kochmar (2011),
who concluded that character n-grams are the
most promising features. Brooke and Hirst
(2012) investigated function words, character n-
grams, POS n-grams, POS/function n-grams,
CFG productions, dependencies, word n-grams.
A notable problem in the recent NLI research
is a clear interaction between native languages
and topics in the corpora. The solution in the
mentioned work was to avoid lexical features
that might carry topical information.
2 Data
The NLI 2013 Shared Task uses the TOEFL11
corpus (Blanchard et al, 2013) which was de-
signed specifically for the task of native language
identification. The corpus contains 12 100 En-
glish essays from the TOEFL (Test of English
as a Foreign Language) that were collected
through ETS (Educational Testing Service) op-
erational test delivery system. TOEFL11 con-
tains eleven native languages: Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Ko-
rean, Spanish, Telugu, and Turkish. The sam-
pling of essays ensures approximately equal rep-
resentation of native languages across eight top-
ics, labeled as prompts. The corpus contains
more than 1000 essays for each L1 language.
Each essay is labelled with an English language
proficiency level ? high, medium, or low ? given
by human assessment specialists. The essays are
usually 300 to 400 words long. The corpus is
split into training, development and test data
(9900, 1100 and 1100, respectively). The corpus
contains plain text files and the index for these
89
File name Prompt Native
language
Language
proficiency
1000025.txt P2 CHI high
100021.txt P1 ARA low
1000235.txt P8 TEL medium
1000276.txt P4 TEL high
1000392.txt P3 JPN medium
1000599.txt P6 CHI medium
1000617.txt P4 GER high
1000719.txt P1 HIN high
100082.txt P2 TUR medium
Table 1: The sample of the training data index.
files. Sample of this index is shown in Table 1.
3 Nend transformation
The training and the development corpora con-
tain a lot of spelling errors and no POS tagging
is provided. For instance, a sentence from the
training corpus ?Acachely I write abawet may
communitie and who the people support youg
people?. Therefore I needed to find features
which encode the information about native lan-
guage of a writer in a more generalized way.
Also, my primary interest was to build a sys-
tem which does not utilize any language pro-
cessing tool, such as part of speech or syntactic
trees, and topic-related information, such as full
words. The reason for that is to have the possi-
bility to apply the same techniques for the texts
written in other languages than English in the
future. Thus, I choose to use the word length as
the number of characters together with the last
n characters of that word. Words in the essays
were transformed into tokens using five kinds of
transformations:
0end ? takes the pure length of a word (for ex-
ample, make 7? 4 );
1end ? adds to the length of a word the last
character (make 7? 4e);
2end ? adds to the length of a word the last
two characters (make 7? 4ke);
3end ? adds to the length of a word the last
three characters (make 7? 4ake);
4end ? adds to the length of a word the last
four characters (make 7? 4make).
For instance, the sentence ?Difference makes
a lot of opportunities .? is translated to:
0end: 10 5 1 3 2 13 1
1end: 10e 5s 1a 3t 2f 13s 1.
2end: 10ce 5es 1a 3ot 2of 13es 1.
3end: 10nce 5kes 1a 3lot 2of 13ies 1.
4end: 10ence 5akes 1a 3lot 2of 13ties 1.
4 N-gram features
The VTEX NLI 2013 system is based on n-
gram features. There are no strict rules for
how long n-grams should be. Frequently used n-
grams are unigrams, bigrams and trigrams as in
Brooke and Hirst (2012; Wong and Dras (2011).
The training NLI 2013 corpus is large enough
to build higher-order n-grams of nend tokens.
I use unigrams, bigrams, trigrams, quad-grams
and five-grams based on nend tokens. Some ex-
amples of these n-grams are shown below:
0end
1-gram: 3
2-gram: 1 3
3-gram: 1 10 6
4-gram: 1 5 3 3
5-gram: 1 3 3 3 7
3end
1-gram: 7ess
2-gram: 2to 7ess
3-gram: 4est 2to 7ess
4-gram: 3but 3not 3for 7ess
5-gram: 3try 5eir 4est 2to 7ess
Beside n-grams of nends, the character n-
grams are of interest also. Kochmar (2011)
noted that character n-grams provide promiss-
ing features for NLI task. Therefore, I tried to
use character trigrams also. For instance, from
the sentence ?Difference makes a lot of opportu-
nities .? the following trigrams were generated:
Dif iff ffe fer ere ren enc nce ce e m
ma mak ake kes es s a a a l lo lot
ot t o of of f o op opp ppo por ort
rtu tun uni nit iti tie ies es s .
Whitespace is included in character trigrams
and denotes the beginning or the end of a word.
90
5 CTFIDF for weigthing features
The most widely used technique for weight-
ing items in a list is Term-Frequency?Inverse-
Document-Frequency, known as TF?IDF. Dau-
daravicius (2012) shows that the small change of
TF?IDF allows to the generation of stop-word
lists automatically. For the NLI 2013 Shared
Task I use Conditional TF?IDF :
CTFIDF(x) = TF(x) ? ln
Dmax ? d(x) + 1
4 ? d(x) + 1
,
where TF(x) is the frequency of the item x in
the training corpus, d(x) is the number of doc-
uments in the training corpus where the item
x appears, known as document frequency, Dmax
is the maximum of document frequency of any
item in the training corpus.
The idea of my Conditional TF?IDF is as fol-
lows: if a term occures in less than Dmax/4 doc-
uments then this term is considered a normal
term, and the term is considered as stop-word if
it occures in more than Dmax/4 documents. The
range of TF-IDF is between 0 and positive infin-
ity. The range of CTFIDF is from minus infinity
to zero for items that are considered stop-words.
And the range of CTFIDF is from zero to infin-
ity for the rest of the items.
For instance, the Dmax for the different n-
gram length and different Nend transformations
is presented in Table 2. The example list of 4end
ungrams with positive and negative CTFIDFs
are shown in Tables 4 and 3, respectively.
It is important to note that I count Dmax and
d(x) for each training language separately; i.e.,
when I measure the distance between a docu-
ment and the document in the training data,
The number of n-grams
1 2 3 4 5
0end 900 899 834 444 168
1end 900 759 358 320 148
2end 899 581 354 319 148
3end 899 572 320 303 148
4end 899 572 320 303 148
Table 2: The maximum of the document frequency
in the training corpus.
I use Dmax and d(x) of the language which the
training document denotes.
token ctfidf token ctfidf token ctfidf
5earn 0.00 4Most 1.16 10ents 2.51
7ally 0.04 7lity 1.20 4your 2.59
10sion 0.10 2Of 1.22 7arly 2.59
7ieve 0.10 6ance 1.22 6eple 2.64
5hing 0.12 6mous 1.22 7tory 2.71
10ence 0.12 5hier 1.24 8tics 2.94
9tion 0.15 3Now 1.25 9gers 3.00
2us 0.22 5eing 1.27 4cool 3.07
6rson 0.23 12tion 1.30 3Let 3.13
7hout 0.29 2He 1.30 4rule 3.29
3may 0.30 4ways 1.41 5imes 3.52
3say 0.31 6hers 1.43 3job 3.53
3see 0.34 5reat 1.45 13ties 3.60
3try 0.35 9rent 1.53 8cial 3.68
3did 0.36 3him 1.55 5eals 3.81
2? 0.42 5ower 1.61 6lent 3.81
2? 0.44 12ties 1.65 4lose 3.95
2he 0.46 3You 1.68 8naly 4.13
4hard 0.52 11lity 1.74 6skes 4.34
7pany 0.58 4cost 1.76 7cted 4.34
5akes 0.60 5ince 1.78 7test 4.34
4kind 0.68 6ills 1.82 6alth 4.36
7blem 0.70 5isks 1.82 5eall 4.60
5ever 0.71 5oney 1.89 9dent 4.73
4been 0.74 6rget 2.07 7cess 4.75
4same 0.81 5ired 2.10 7kers 5.36
8king 0.86 9nies 2.11 9ters 5.46
6king 0.93 4ever 2.15 2D. 5.52
5ften 0.96 6ates 2.15 5neof 5.52
6urse 0.97 3his 2.22 8idnt 5.52
7ling 0.97 10ered 2.24 8klin 5.52
4Even 0.98 4love 2.24 9velt 5.52
8ible 0.99 6ited 2.24 10sful 6.62
4used 1.02 9ties 2.27 4four 7.62
10tely 1.07 4earn 2.30 3oil 8.05
4best 1.09 6llow 2.30 9cans 8.26
7ught 1.10 9ated 2.37 4jobs 8.96
4easy 1.12 3got 2.42 3FDR 11.04
4Then 1.12 8ngly 1.13
Table 3: The list of 4end unigrams with positive CT-
FIDFs of one document from the training corpus.
91
token ctfidf token ctfidf token ctfidf
1. -224.19 3but -3.48 3lot -0.92
1, -127.63 5bout -2.58 2we -0.88
2to -69.62 3get -2.57 5hich -0.85
2of -56.92 7mple -2.54 9ment -0.84
3the -45.09 2by -2.39 3who -0.84
3and -27.25 4from -2.26 3The -0.81
2is -24.79 4they -2.18 4them -0.79
1a -23.19 3can -2.12 3one -0.77
6ople -22.78 4will -2.11 4only -0.75
3not -22.31 3all -1.83 4much -0.70
3are -18.11 2If -1.72 4what -0.68
3for -15.82 2at -1.63 4also -0.64
4that -14.39 2In -1.50 4want -0.57
2do -13.16 6ings -1.38 6cond -0.56
2it -12.50 5irst -1.35 9tant -0.43
4have -11.53 3For -1.33 3how -0.35
4with -9.39 5gree -1.33 3new -0.31
1I -8.72 3you -1.31 6ould -0.31
7ause -7.73 2so -1.30 4need -0.20
2in -6.40 4time -1.15 5oing -0.15
5heir -6.23 3was -1.08 4take -0.11
2be -5.44 7ever -0.98 2So -0.10
4many -5.40 5ther -0.95 6ally -0.09
2as -5.06 4make -0.93 3But -0.08
5here -3.92 5hink -3.64
Table 4: The list of 4end unigrams with negative
CTFIDFs of the same document as in Fig. 3.
6 Distance between documents
Cosine distance is a widely used technique to
measure the distance between two feature vec-
tors. It is calculated as follows:
cos(X,Y ) =
?
i(XiYi)??
iX
2
i +
??
i Y
2
i
.
CTFIDF allows the splitting of feature vectors
into the list of ?informative? items and the list
of functional items. For the NLI 2013 Shared
task, I combine two cosine distances of negative
and positive CTFIDFs as follows:
cos?(X,Y ) =
2 cos(X ?, Y ?) + cos(X ??, Y ??)
3
,
where
X ? = filter?0 X, Y
? = filter?0 Y,
X ?? = abs(filter<0 X), Y
?? = abs(filter<0 Y ),
so X ? and Y ? contain features with positive CT-
FIDF, while X ?? and Y ?? contain features with
negative CTFIDF.
The cos? combines two cosine distances giving
the weight for cosine of positive CTFIDFs equal
to 2 and for the negative CTFIDFs equal to 1.
I have also tested combinations of 1 to 0, 0 to
1, 1 to 1, and 1 to 2. But these combinations
did not achieve better results. Therefore, for all
submitted system results I used the same com-
bination of 2 to 1.
I utilize 26 feature vectors and obtain 26 com-
bined cosine distances for each document: one
for character trigrams and other 25 for token
n-grams of diverse word transformations. Each
combined cosine distance has an assigned weight
to get the final distance between two documents.
The distance between two documents X and Y
is calculated as follows:
dist(X,Y ) =
?
iwi cos
?(Xi, Yi)
?
iwi
? [0, 1],
where wi is the weight of ith feature vector.
The most difficult task was to find the best
combination of these 26 weights. For the NLI
2013 Shared Task I have used the combinations
shown in Table 5. The n-gram weights in most
cases are diagonal with the highest value at the
0end unigram and the lowest at the 4end five-
gram. In the beggining I tested the opposite
combination, but this led to worse results. Also,
the influence of character trigrams on the results
was high. The first and second combinations in
Table 5 differ in the use of five-grams and 4end
transformations, while the leverage of charac-
ter trigrams were kept the same. The final of-
ficial results show that richer features improve
results. Also, I found that the higher leverage
is for character trigrams over n-grams the bet-
ter the results are. But, the results of character
trigrams only resulted in lower performance. It
is a long way to find the optimal combination of
the weights.
92
Token n-gram
1 2 3 4 5
1-closed
Character trigrams 64
0end 7 6 5 4 0
1end 6 5 4 3 0
2end 5 4 3 2 0
3end 4 3 2 1 0
4end 0 0 0 0 0
2-closed
Character trigrams 125
0end 9 8 7 6 5
1end 8 7 6 5 4
2end 7 6 5 4 3
3end 6 5 4 3 2
4end 5 4 3 2 1
3-closed
Character trigrams 25
0end 1 1 1 1 1
1end 1 1 1 1 1
2end 1 1 1 1 1
3end 1 1 1 1 1
4end 1 1 1 1 1
4-closed
Character trigrams 225
0end 17 15 13 11 9
1end 15 13 11 9 7
2end 13 11 9 7 5
3end 11 9 7 5 3
4end 9 7 5 3 1
5-closed
Character trigrams 550
0end 17 15 13 11 9
1end 15 13 11 9 7
2end 13 11 9 7 5
3end 11 9 7 5 3
4end 9 7 5 3 1
Table 5: Weights of the NLI 2013 different submis-
sions.
7 Assigning native language to a text
I used the k-NN technique to assign native lan-
guage to a text. I counted the distances between
the test document and all training documents,
and take some amount of closest documents for
each language. To reduce the influnce of out-
liers, I dropped off the n closest documents and
only then take some amount from the rest. At
first, I remove the 10 top documents from each
language, and then kept the 20 closest docu-
ments for each language. In total, I obtained 220
documents and ranked them by distance. Then,
I employed voting for the closest 20 documents.
A winner language is assigned to a document as
the native language. This technique was used for
VTEX-closed-(1, 2 and 3) system submitions.
For the VTEX-closed-(4 and 5) I used another
number for outliers and the top closest ones:
the 50 closest documents for each language were
dropped off, the remianing 25 for each language
were kept, and, finally, the closest 25 documents
are used for the voting of native language.
8 Results
My primary interest in participating in the NLI
2013 Shared Task was to investigate new fea-
tures that were not used earlier, and what the
value of each feature in the identification of a
writer?s native language is. The results of five
submitted systems are shown in Tables 6 and
7. The best submitted system had 31.9 percent
accuracy. This result was the worst of all par-
ticipating teams. At the time of writing this re-
port, I tested new combinations of outliers and
tops, ?stop-words? and significant items, nend
n-grams and character trigram weights. New
settings improved my best submitted system ac-
curacy from 31.9 to 63.9 percent. This result
was achieved with the following settings. I took
the last 50 percent of closest documents for each
language. I set to use only stop-words and to
exclude significant items, i.e., items with only
negative CTFIDF. Finaly, I set n-gram weights
accordingly: 84 for character trigrams, and
for nend 1,1,1,1,1, 1,3,3,3,1, 1,3,5,3,1, 1,3,3,3,1,
1,1,1,1,1. This result shows that 2end and 3end
transformation trigrams have the highest impact
on the results. Nevertheless, all tested transfor-
mations help to improve the results. In con-
clusion, I investigated the influence of features,
such as character trigrams and Nend n-grams,
to the identification of writer?s native language
and found them very informative.
93
Results for VTEX-closed-1
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 30 5 2 5 5 11 12 6 10 13 1 26.3% 30.0% 28.0%
CHI 4 20 2 5 5 6 21 20 5 9 3 24.1% 20.0% 21.9%
FRE 6 8 9 13 3 14 14 9 8 10 6 28.1% 9.0% 13.6%
GER 6 4 5 30 7 13 4 1 7 20 3 35.3% 30.0% 32.4%
HIN 15 5 0 7 17 5 6 5 3 31 6 23.0% 17.0% 19.5%
ITA 7 2 4 3 4 47 9 3 4 15 2 34.8% 47.0% 40.0%
JPN 4 5 1 4 5 7 44 12 4 14 0 25.3% 44.0% 32.1%
KOR 2 8 1 3 2 9 35 27 3 9 1 26.0% 27.0% 26.5%
SPA 13 10 4 3 5 15 13 8 12 13 4 19.0% 12.0% 14.7%
TEL 13 8 0 1 13 4 2 1 4 52 2 26.3% 52.0% 34.9%
TUR 14 8 4 11 8 4 14 12 3 12 10 26.3% 10.0% 14.5%
Accuracy = 27.1%
Results for VTEX-closed-2
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 31 5 1 3 5 11 13 6 8 15 2 26.5% 31.0% 28.6%
CHI 6 23 1 4 6 5 21 15 6 10 3 27.7% 23.0% 25.1%
FRE 5 8 7 12 7 15 12 10 6 10 8 25.9% 7.0% 11.0%
GER 7 4 4 28 9 12 6 1 6 20 3 35.0% 28.0% 31.1%
HIN 13 5 2 6 17 4 6 5 4 30 8 20.2% 17.0% 18.5%
ITA 7 2 4 3 4 47 9 3 4 15 2 35.1% 47.0% 40.2%
JPN 4 7 0 5 6 7 36 16 3 15 1 22.0% 36.0% 27.3%
KOR 3 7 1 3 2 9 34 26 4 9 2 25.7% 26.0% 25.9%
SPA 15 7 3 5 6 17 10 7 10 15 5 16.4% 10.0% 12.4%
TEL 13 6 1 0 15 2 2 1 6 52 2 25.5% 52.0% 34.2%
TUR 13 9 3 11 7 5 15 11 4 13 9 20.0% 9.0% 12.4%
Accuracy = 26.0%
Results for VTEX-closed-3
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 27 6 1 5 6 11 11 7 11 13 2 25.2% 27.0% 26.1%
CHI 6 22 2 6 8 2 21 14 5 12 2 27.2% 22.0% 24.3%
FRE 6 8 6 12 8 14 15 7 5 10 9 17.1% 6.0% 8.9%
GER 7 4 6 24 9 13 1 2 7 22 5 27.3% 24.0% 25.5%
HIN 15 4 2 7 17 4 6 3 5 30 7 19.5% 17.0% 18.2%
ITA 7 0 6 3 4 45 8 5 4 16 2 34.1% 45.0% 38.8%
JPN 4 9 0 5 6 8 32 15 4 16 1 21.2% 32.0% 25.5%
KOR 2 6 1 5 2 9 31 26 4 12 2 27.7% 26.0% 26.8%
SPA 15 7 4 6 8 16 7 6 11 14 6 15.3% 11.0% 12.8%
TEL 10 6 2 0 13 5 2 1 10 50 1 23.9% 50.0% 32.4%
TUR 8 9 5 15 6 5 17 8 6 14 7 15.9% 7.0% 9.7%
Accuracy = 24.3%
Table 6: The results for closed-task VTEX systems.
94
Results for VTEX-closed-4
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 21 5 1 6 4 14 15 6 14 12 2 30.4% 21.0% 24.9%
CHI 2 22 2 5 5 5 24 18 7 7 3 26.2% 22.0% 23.9%
FRE 4 9 8 13 3 14 16 9 6 12 6 22.2% 8.0% 11.8%
GER 5 4 8 25 8 13 5 2 6 19 5 28.7% 25.0% 26.7%
HIN 7 7 1 7 15 5 7 7 4 31 9 22.1% 15.0% 17.9%
ITA 2 3 3 4 2 48 12 3 4 16 3 33.8% 48.0% 39.7%
JPN 1 5 1 5 4 8 42 17 4 13 0 21.8% 42.0% 28.7%
KOR 1 6 1 2 1 7 36 33 2 10 1 30.0% 33.0% 31.4%
SPA 9 11 5 6 4 18 14 5 10 14 4 15.9% 10.0% 12.3%
TEL 8 5 3 1 15 5 2 1 4 53 3 27.0% 53.0% 35.8%
TUR 9 7 3 13 7 5 20 9 2 9 16 30.8% 16.0% 21.1%
Accuracy = 26.6%
Results for VTEX-closed-5
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 40 7 0 2 2 14 10 4 7 11 3 33.9% 40.0% 36.7%
CHI 6 32 4 0 4 4 21 16 4 8 1 27.8% 32.0% 29.8%
FRE 5 13 13 9 2 15 14 8 6 12 3 28.9% 13.0% 17.9%
GER 10 5 8 22 2 13 7 3 8 16 6 45.8% 22.0% 29.7%
HIN 12 9 4 5 11 5 6 6 4 30 8 28.9% 11.0% 15.9%
ITA 3 5 6 2 1 54 7 4 5 11 2 36.5% 54.0% 43.5%
JPN 2 6 0 3 1 8 48 16 3 12 1 26.4% 48.0% 34.0%
KOR 1 12 1 0 2 6 29 39 2 7 1 35.1% 39.0% 37.0%
SPA 12 9 5 1 3 20 14 5 16 12 3 27.1% 16.0% 20.1%
TEL 14 6 0 0 8 5 2 0 3 59 3 31.4% 59.0% 41.0%
TUR 13 11 4 4 2 4 24 10 1 10 17 35.4% 17.0% 23.0%
Accuracy = 31.9%
Table 7: The results for closed-task VTEX systems.
References
Blanchard D., Tetreault J. and Cahill A. 2013. Sum-
mary Report on the First Shared Task on Na-
tive Language Identification. In Proceedings of
the Eighth Workshop on Building Educational Ap-
plications Using NLP, Association for Computa-
tional Linguistics, Atlanta, GA, USA
Brooke, J. and Hirst, G. 2012. Robust, Lexicalized
Native Language Identification. In Proceedings of
COLING 2012, Mumbai, India, 391?408.
Daudaravicius, V. 2012. Collocation segmentation
for text chunking. PhD thesis, Vytautas Magnus
University.
Kochmar, E. 2011. Identification of a Writer?s Na-
tive Language by Error Analysis. Master?s thesis,
University of Cambridge.
Koppel M., Schler J. and Zigdon, K. 2005. Deter-
mining an author?s native language by mining a
text for errors. In Proceedings of the 11th ACM
SIGKDD International Conference on Knowledge
Discovery in Data Mining (KDD ?05), 624-628.
Tsur, O. and Rappoport, A. 2007. Using classi-
fier features for studying the effect of native lan-
guage on the choice of written second language
words. In Proceedings of the Workshop on Cogni-
tive Aspects of Computational Language Acquisi-
tion (CACLA?07), 9-16.
Wong, S.J. and Dras, M. 2011. Exploiting parse
structures for native language identification. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, 1600-1610.
95
