Data-Driven Strategies for an Automated Dialogue System 
Hilda HARDY, Tomek 
STRZALKOWSKI, Min WU 
ILS Institute 
University at Albany, SUNY 
1400 Washington Ave., SS262 
Albany, NY  12222   USA 
hhardy|tomek|minwu@ 
cs.albany.edu  
Cristian URSU, Nick WEBB 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield  S1 4DP   UK 
c.ursu@sheffield.ac.uk, 
n.webb@dcs.shef.ac.uk 
Alan BIERMANN, R. Bryce 
INOUYE, Ashley MCKENZIE 
Department of Computer Science 
Duke University 
P.O. Box 90129, Levine Science 
Research Center, D101  
Durham, NC  27708   USA 
awb|rbi|armckenz@cs.duke.edu 
 
Abstract 
We present a prototype natural-language 
problem-solving application for a financial 
services call center, developed as part of the 
Amiti?s multilingual human-computer 
dialogue project. Our automated dialogue 
system, based on empirical evidence from real 
call-center conversations, features a data-
driven approach that allows for mixed 
system/customer initiative and spontaneous 
conversation. Preliminary evaluation results 
indicate efficient dialogues and high user 
satisfaction, with performance comparable to 
or better than that of current conversational 
travel information systems. 
1 Introduction 
Recently there has been a great deal of interest in 
improving natural-language human-computer 
conversation. Automatic speech recognition 
continues to improve, and dialogue management 
techniques have progressed beyond menu-driven 
prompts and restricted customer responses. Yet 
few researchers have made use of a large body of 
human-human telephone calls, on which to form 
the basis of a data-driven automated system.  
The Amiti?s project seeks to develop novel 
technologies for building empirically induced 
dialogue processors to support multilingual 
human-computer interaction, and to integrate these 
technologies into systems for accessing 
information and services (http://www.dcs.shef.ac. 
uk/nlp/amities). Sponsored jointly by the European 
Commission and the US Defense Advanced 
Research Projects Agency, the Amiti?s Consortium 
includes partners in both the EU and the US, as 
well as financial call centers in the UK and France. 
A large corpus of recorded, transcribed 
telephone conversations between real agents and 
customers gives us a unique opportunity to analyze 
and incorporate features of human-human 
dialogues into our automated system. (Generic 
names and numbers were substituted for all 
personal details in the transcriptions.) This corpus 
spans two different application areas: software 
support and (a much smaller size) customer 
banking. The banking corpus of several hundred 
calls has been collected first and it forms the basis 
of our initial multilingual triaging application, 
implemented for English, French and German 
(Hardy et al, 2003a); as well as our prototype 
automatic financial services system, presented in 
this paper, which completes a variety of tasks in 
English. The much larger software support corpus 
(10,000 calls in English and French) is still being 
collected and processed and will be used to 
develop the next Amiti?s prototype. 
We observe that for interactions with structured 
data ? whether these data consist of flight 
information, spare parts, or customer account 
information ? domain knowledge need not be built 
ahead of time. Rather, methods for handling the 
data can arise from the way the data are organized. 
Once we know the basic data structures, the 
transactions, and the protocol to be followed (e.g., 
establish caller?s identity before exchanging 
sensitive information); we need only build 
dialogue models for handling various 
conversational situations, in order to implement a 
dialogue system. For our corpus, we have used a 
modified DAMSL tag set (Allen and Core, 1997) 
to capture the functional layer of the dialogues, and 
a frame-based semantic scheme to record the 
semantic layer (Hardy et al, 2003b). The ?frames? 
or transactions in our domain are common 
customer-service tasks: VerifyId, ChangeAddress, 
InquireBalance, Lost/StolenCard and Make 
Payment. (In this context ?task? and ?transaction? 
are synonymous.) Each frame is associated with 
attributes or slots that must be filled with values in 
no particular order during the course of the 
dialogue; for example, account number, name, 
payment amount, etc. 
2 Related Work 
Relevant human-computer dialogue research 
efforts include the TRAINS project and the 
DARPA Communicator program. 
The classic TRAINS natural-language dialogue 
project (Allen et al, 1995) is a plan-based system 
which requires a detailed model of the domain and 
therefore cannot be used for a wide-ranging 
application such as financial services. 
The US DARPA Communicator program has 
been instrumental in bringing about practical 
implementations of spoken dialogue systems. 
Systems developed under this program include 
CMU?s script-based dialogue manager, in which 
the travel itinerary is a hierarchical composition of 
frames (Xu and Rudnicky, 2000). The AT&T 
mixed-initiative system uses a sequential decision 
process model, based on concepts of dialog state 
and dialog actions (Levin et al, 2000). MIT?s 
Mercury flight reservation system uses a dialogue 
control strategy based on a set of ordered rules as a 
mechanism to manage complex interactions 
(Seneff and Polifroni, 2000). CU?s dialogue 
manager is event-driven, using a set of hierarchical 
forms with prompts associated with fields in the 
forms. Decisions are based not on scripts but on 
current context (Ward and Pellom, 1999). 
Our data-driven strategy is similar in spirit to 
that of CU. We take a statistical approach, in 
which a large body of transcribed, annotated 
conversations forms the basis for task 
identification, dialogue act recognition, and form 
filling for task completion.  
3 System Architecture and Components 
The Amiti?s system uses the Galaxy 
Communicator Software Infrastructure (Seneff et 
al., 1998). Galaxy is a distributed, message-based, 
hub-and-spoke infrastructure, optimized for spoken 
dialogue systems. 
  
 
Figure 1. Amiti?s System Architecture 
 
Components in the Amiti?s system (Figure 1) 
include a telephony server, automatic speech 
recognizer, natural language understanding unit, 
dialogue manager, database interface server, 
response generator, and text-to-speech conversion. 
3.1 Audio Components 
Audio components for the Amiti?s system are 
provided by LIMSI. Because acoustic models have 
not yet been trained, the current demonstrator 
system uses a Nuance ASR engine and TTS 
Vocalizer.  
To enhance ASR performance, we integrated 
static GSL (Grammar Specification Language) 
grammar classes provided by Nuance for 
recognizing several high-frequency items: 
numbers, dates, money amounts, names and yes-no 
statements. 
Training data for the recognizer were collected 
both from our corpus of human-human dialogues 
and from dialogues gathered using a text-based 
version of the human-computer system. Using this 
version we collected around 100 dialogues and 
annotated important domain-specific information, 
as in this example: ?Hi my name is [fname ; 
David] [lname ; Oconnor] and my account number 
is [account ; 278 one nine five].? 
Next we replaced these annotated entities with 
grammar classes. We also utilized utterances from 
the Amiti?s banking corpus (Hardy et al, 2002) in 
which the customer specifies his/her desired task, 
as well as utterances which constitute common, 
domain-independent speech acts such as 
acceptances, rejections, and indications of non-
understanding. These were also used for training 
the task identifier and the dialogue act classifier 
(Section 3.3.2). The training corpus for the 
recognizer consists of 1744 utterances totaling 
around 10,000 words. 
Using tools supplied by Nuance for building 
recognition packages, we created two speech 
recognition components: a British model in the UK 
and an American model at two US sites. 
For the text to speech synthesizer we used 
Nuance?s Vocalizer 3.0, which supports multiple 
languages and accents. We integrated the 
Vocalizer and the ASR using Nuance?s speech and 
telephony API into a Galaxy-compliant server 
accessible over a telephone line. 
3.2 Natural Language Understanding 
The goal of the language understanding 
component is to take the word string output of the 
ASR module, and identify key semantic concepts 
relating to the target domain. This is a specialized 
kind of information extraction application, and as 
such, we have adapted existing IE technology to 
this task.  
Hub 
Speech 
Recognition 
Dialogue 
Manager Database 
Server 
Nat?l Language 
Understanding 
Telephony 
Server 
Response      
Generation 
Customer 
Database 
Text-to-speech
Conversion 
We have used a modified version of the ANNIE 
engine (A Nearly-New IE system; Cunningham et 
al., 2002; Maynard, 2003). ANNIE is distributed as 
the default built-in IE component of the GATE 
framework (Cunningham et al, 2002). GATE is a 
pure Java-based architecture developed over the 
past eight years in the University of Sheffield 
Natural Language Processing group. ANNIE has 
been used for many language processing 
applications, in a number of languages both 
European and non-European. This versatility 
makes it an attractive proposition for use in a 
multilingual speech processing project. 
ANNIE includes customizable components 
necessary to complete the IE task ? tokenizer, 
gazetteer, sentence splitter, part of speech tagger 
and a named entity recognizer based on a powerful 
engine named JAPE (Java Annotation Pattern 
Engine; Cunningham et al, 2000). 
Given an utterance from the user, the NLU unit 
produces both a list of tokens for detecting 
dialogue acts, an important research goal inside 
this project, and a frame with the possible named 
entities specified by our application. We are 
interested particularly in account numbers, credit 
card numbers, person names, dates, amounts of 
money, locations, addresses and telephone 
numbers.  
In order to recognize these, we have updated the 
gazetteer, which works by explicit look-up tables 
of potential candidates, and modified the rules of 
the transducer engine, which attempts to match 
new instances of named entities based on local 
grammatical context. There are some significant 
differences between the kind of prose text more 
typically associated with information extraction, 
and the kind of text we are expecting to encounter. 
Current models of IE rely heavily on punctuation 
as well as certain orthographic information, such as 
capitalized words indicating the presence of a 
name, company or location. We have access to 
neither of these in the output of the ASR engine, 
and so had to retune our processors to data which 
reflected that. 
In addition, we created new processing 
resources, such as those required to spot number 
units and translate them into textual representations 
of numerical values; for example, to take ?twenty 
thousand one hundred and fourteen pounds?, and 
produce ??20,114?. The ability to do this is of 
course vital for the performance of the system. 
If none of the main entities can be identified 
from the token string, we create a list of possible 
fallback entities, in the hope that partial matching 
would help narrow the search space. 
For instance, if a six-digit account number is not 
identified, then the incomplete number recognized 
in the utterance is used as a fallback entity and sent 
to the database server for partial matching. 
Our robust IE techniques have proved 
invaluable to the efficiency and spontaneity of our 
data-driven dialogue system. In a single utterance 
the user is free to supply several values for 
attributes, prompted or unprompted, allowing tasks 
to be completed with fewer dialogue turns. 
3.3 Dialogue Manager 
The dialogue manager identifies the goals of the 
conversation and performs interactions to achieve 
those goals. Several ?Frame Agents?, implemented 
within the dialogue manager, handle tasks such as 
verifying the customer?s identity, identifying the 
customer?s desired transaction, and executing those 
transactions. These range from a simple balance 
inquiry to the more complex change of address and 
debit-card payment. The structure of the dialogue 
manager is illustrated in Figure 2. 
Rather than depending on a script for the 
progression of the dialogue, the dialogue manager 
takes a data-driven approach, allowing the caller to 
take the initiative. Completing a task depends on 
identifying that task and filling values in frames, 
but this may be done in a variety of ways: one at a 
time, or several at once, and in any order. 
For example, if the customer identifies himself 
or herself before stating the transaction, or even if 
he or she provides several pieces of information in 
one utterance?transaction, name, account number, 
payment amount?the dialogue manager is flexible 
enough to move ahead after these variations. 
Prompts for attributes, if needed, are not restricted 
to one at a time, but they are usually combined in 
the way human agents request them; for example, 
city and county, expiration date and issue number, 
birthdate and telephone number. 
 
 
 
Figure 2. Amiti?s Dialogue Manager 
If the system fails to obtain the necessary values 
from the user, reprompts are used, but no more 
than once for any single attribute. For the customer 
verification task, different attributes may be 
 
 
 
 
 
 
 
 
 Response Decision 
Input:  
from NLU via 
Hub (token string, 
language id, 
named entities) 
Task infoExternal files, 
domain-specific
Dialogue Act 
Classifier 
Frame Agent 
Task ID 
Frame Agent 
Verify-Caller 
Frame Agent 
DB Server 
Customer 
Database
 
 
 
 
 
 
Task Execution 
Frame Agents via Hub 
Dialogue History 
requested. If the system fails even after reprompts, 
it will gracefully give up with an explanation such 
as, ?I?m sorry, we have not been able to obtain the 
information necessary to update your address in 
our records. Please hold while I transfer you to a 
customer service representative.? 
3.3.1 Task ID Frame Agent 
For task identification, the Amiti?s team has 
made use of the data collected in over 500 
conversations from a British call center, recorded, 
transcribed, and annotated. Adapting a vector-
based approach reported by Chu-Carroll and 
Carpenter (1999), the Task ID Frame Agent is 
domain-independent and automatically trained. 
Tasks are represented as vectors of terms, built 
from the utterances requesting them. Some 
examples of labeled utterances are: ?Erm I'd like to 
cancel the account cover premium that's on my, 
appeared on my statement? [CancelInsurance] and 
?Erm just to report a lost card please? 
[Lost/StolenCard].   
The training process proceeds as follows: 
1. Begin with corpus of transcribed, annotated 
calls. 
2. Document creation: For each transaction, collect 
raw text of callers? queries. Yield: one 
?document? for each transaction (about 14 of 
these in our corpus). 
3. Text processing: Remove stopwords, stem 
content words, weight terms by frequency. 
Yield: one ?document vector? for each task. 
4. Compare queries and documents: Create ?query 
vectors.? Obtain a cosine similarity score for 
each query/document pair. Yield: cosine 
scores/routing values for each query/document 
pair. 
5. Obtain coefficients for scoring: Use binary 
logistic regression. Yield: a set of coefficients 
for each task. 
Next, the Task ID Frame Agent is tested on 
unseen utterances or queries: 
1. Begin with one or more user queries. 
2. Text processing: Remove stopwords, stem 
content words, weight terms (constant weights). 
Yield: ?query vectors?. 
3. Compare each query with each document. 
Yield: cosine similarity scores. 
4. Compute confidence scores (use training 
coefficients). Yield: confidence scores, 
representing the system?s confidence that the 
queries indicate the user?s choice of a particular 
transaction. 
Tests performed over the entire corpus, 80% of 
which was used for training and 20% for testing, 
resulted in a classification accuracy rate of 85% 
(correct task is one of the system?s top 2 choices). 
The accuracy rate rises to 93% when we eliminate 
confusing or lengthy utterances, such as requests 
for information about payments, statements, and 
general questions about a customer?s account. 
These can be difficult even for human annotators 
to classify. 
3.3.2 Dialogue Act Classifier 
The purpose of the DA Classifier Frame Agent 
is to identify a caller?s utterance as one or more 
domain-independent dialogue acts. These include 
Accept, Reject, Non-understanding, Opening, 
Closing, Backchannel, and Expression. Clearly, it 
is useful for a dialogue system to be able to 
identify accurately the various ways a person may 
say ?yes?, ?no?, or ?what did you say?? As with 
the task identifier, we have trained the DA 
classifier on our corpus of transcribed, labeled 
human-human calls, and we have used vector-
based classification techniques. Two differences 
from the task identifier are 1) an utterance may 
have multiple correct classifications, and 2) a 
different stoplist is necessary. Here we can filter 
out the usual stops, including speech dysfluencies, 
proper names, number words, and words with 
digits; but we need to include words such as yeah, 
uh-huh, hi, ok, thanks, pardon and sorry.  
Some examples of DA classification results are 
shown in Figure 3. For sure, ok, the classifier 
returns the categories Backchannel, Expression and 
Accept. If the dialogue manager is looking for 
either Accept or Reject, it can ignore Backchannel 
and Expression in order to detect the correct 
classification. In the case of certainly not, the first 
word has a strong tendency toward Accept, though 
both together constitute a Reject act.  
 
Text: ?sure, okay? Text: ?certainly not?
Categories returned: Backchannel, 
Expression, Accept 
Categories returned:
Reject, Accept 
Expression
Closing
Accept
Back.
0
0.2
0.4
0.6
0.8
1
Top four cosine scores
Expression
Accept Closing
Back.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Reject
Reject-part
Accept Expression
0
0.1
0.2
0.3
0.4
0.5
0.6
Top four cosine scores
Reject
Accept Expression
Reject-part
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Figure 3. DA Classification examples 
 
Our classifier performs well if the utterance is 
short and falls into one of the selected categories 
(86% accuracy on the British data); and it has the 
advantages of automatic training, domain 
independence, and the ability to capture a great 
variety of expressions. However, it can be 
inaccurate when applied to longer utterances, and it 
is not yet equipped to handle domain-specific 
assertions, questions, or queries about a 
transaction. 
3.4 Database Manager 
Our system identifies users by matching 
information provided by the caller against a 
database of user information. It assumes that the 
speech recognizer will make errors when the caller 
attempts to identify himself. Therefore perfect 
matches with the database entries will be rare. 
Consequently, for each record in the database, we 
attach a measure of the probability that the record 
is the target record. Initially, these measures are 
estimates of the probability that this individual will 
call. When additional identifying information 
arrives, the system updates these probabilities 
using Bayes? rule. 
Thus, the system might begin with a uniform 
probability estimate across all database records. If 
the user identifies herself with a name recognized 
by the machine as ?Smith?, the system will 
appropriately increment the probabilities of all 
entries with the name ?Smith? and all entries that 
are known to be confused with ?Smith? in 
proportion to their observed rate of substitution. Of 
course, all records not observed to be so 
confusable would similarly have their probabilities 
decreased by Bayes? rule. When enough 
information has come in to raise the probability for 
some record above a threshold (in our system 0.99 
probability), the system assumes that the caller has 
been correctly identified. The designer may choose 
to include a verification dialog, but our decision 
was to minimize such interactions to shorten the 
calls.  
Our error-correcting database system receives 
tokens with an identification of what field each 
token should represent. The system processes the 
tokens serially. Each represents an observation 
made by the speech recognizer. To process a token, 
the system examines each record in the database 
and updates the probability that the record is the 
target record using Bayes? rule: 
 
 
  
where rec is the event where the record under 
consideration is the target record.  
As is common in Bayes? rule calculations, the 
denominator P(obs) is treated as a scaling factor, 
and is not calculated explicitly. All probabilities 
are renormalized at the end of the update of all of 
the records. P(rec) is the previous estimate of the 
probability that the record is the target record. 
P(obs|rec) is the probability that the recognizer 
returned the observation that it did given that the 
target record is the current record under 
examination. For some of the fields, such as the 
account number and telephone number, the user 
responses consist of digits. We collected data on 
the probability that the speech recognition system 
we are using mistook one digit for another and 
calculated the values for P(obs|rec) from the data. 
For fields involving place names and personal 
names, the probabilities were estimated.  
Once a record has been selected (by virtue of its 
probability being greater than the threshold) the 
system compares the individual fields of the record 
with values obtained by the speech recognizer. If 
the values differ greatly, as measured by their 
Levenshtein distance, the system returns the field 
name to the dialogue manager as a candidate for 
additional verification. If no record meets the 
threshold probability criterion, the system returns 
the most probable record to the dialogue manager, 
along with the fields which have the greatest 
Levenshtein distance between the recognized and 
actual values, as candidates for reprompting.  
Our database contains 100 entries for the system 
tests described in this paper. We describe the 
system in a more demanding environment with one 
million records in Inouye et al (2004). In that 
project, we required all information to be entered 
by spelling the items out so that the vocabulary 
was limited to the alphabet plus the ten digits. In 
the current project, with fewer names to deal with, 
we allowed the complete vocabulary of the 
domain: names, streets, counties, and so forth.  
3.5 Response Generator 
Our current English-only system preserves the 
language-independent features of our original tri-
lingual generator, storing all language- and 
domain-specific information in separate text files. 
It is a template-based system, easily modified and 
extended. The generator constructs utterances 
according to the dialogue manager?s specification 
of one or more speech acts (prompt, request, 
confirm, respond, inform, backchannel, accept, 
reject), repetition numbers, and optional lists of 
attributes, values, and/or the person?s name. As far 
as possible, we modeled utterances after the 
human-human dialogues. 
For a more natural-sounding system, we 
collected variations of the utterances, which the 
generator selects at random. Requests, for 
example, may take one of twelve possible forms: 
Request, part 1 of 2: 
Can you just confirm | Can I have | Can I take | 
What is | What?s | May I have 
)(
)()|()|(
obsP
recPrecobsPobsrecP ?=
Request, part 2 of 2: 
[list of attributes], [person name]? | [list of 
attributes], please? 
Offers to close or continue the dialogue are 
similarly varied: 
Closing offer, part 1 of 2: 
Is there anything else | Anything else | Is there 
anything else at all 
Closing offer, part 2 of 2: 
I can do for you today? | I can help you with 
today? | I can do for you? | I can help you with? | 
you need today? | you need? 
4 Preliminary Evaluation 
Ten native speakers of English, 6 female and 4 
male, were asked to participate in a preliminary in-
lab system evaluation (half in the UK and half in 
the US). The Amiti?s system developers were not 
among these volunteers. Each made 9 phone calls 
to the system from behind a closed door, according 
to scenarios designed to test various customer 
identities as well as single or multiple tasks. After 
each call, participants filled out a questionnaire to 
register their degree of satisfaction with aspects of 
the interaction. 
Overall call success was 70%, with 98% 
successful completions for the VerifyId and 96% 
for the CheckBalance subtasks (Figure 4). 
?Failures? were not system crashes but simulated 
transfers to a human agent. There were 5 user 
terminations. 
Average word error rates were 17% for calls that 
were successfully completed, and 22% for failed 
calls. Word error rate by user ranged from 11% to 
26%. 
 
0.70
0.98 0.96
0.88 0.90
0.57
0.85
0.00
0.20
0.40
0.60
0.80
1.00
1.20
Ca
ll S
uc
ce
ss
Ve
rify
Id
Ch
ec
kB
ala
nc
e
Lo
stC
ar
d
Ma
ke
Pa
ym
en
t
Ch
an
ge
Ad
dr
es
s
Fin
ish
Di
alo
gu
e
 
Figure 4. Task Completion Rates 
Call duration was found to reflect the 
complexity of each scenario, where complexity is 
defined as the number of ?concepts? needed to 
complete each task. The following items are 
judged to be concepts: task identification; values 
such as first name, last name, house number, street 
and phone number; and positive or negative 
responses such as whether a new card is desired. 
Figures 5 and 6 illustrate the relationship between 
length of call and task complexity. It should be 
noted that customer verification, a task performed 
in every dialogue, requires a minimum of 3 
personal details to be verified against a database 
record, but may require more in the case of 
recognition errors. 
The overall average number of turns per 
dialogue was 18.28. The user spoke an average of 
6.89 words per turn and the system 11.42. 
User satisfaction for each call was assessed by 
way of a questionnaire containing five statements. 
These covered the clarity of the instructions, ease 
of doing the task, how well the system understands 
the caller, how well the system works, and the 
caller?s enjoyment of the system. Participants rated 
each on a five-point Likert scale. Summed results 
showed an average score of 20.45 over all users 
(range 5?25; higher = stronger agreement). 
 
0
2
4
6
8
10
12
1 2 3 4 5 6 7 8 9
Scenario
Nu
m
be
r o
f c
on
ce
pt
s
 
85.0
135.8
119.3
88.3
106.7 114.0
48.4
109.3
127.1
0.0
20.0
40.0
60.0
80.0
100.0
120.0
140.0
160.0
1 2 3 4 5 6 7 8 9
Figures 5 and 6. Scenario Complexity (top) and 
Average Call Duration by Scenario (in seconds) 
Although user satisfaction was high, we were 
more interested in identifying the major problems 
for the callers. Users were often frustrated by 
recognition failures and/or unsuccessful attempts 
to capture values such as a new street address, 
county, or phone number. Sometimes the system 
failed to determine that the user was finished. 
Because the system is designed to be non-
restrictive in accepting users? input, 
misidentifications were common. One user spoke 
for many by saying, ?There were times when the 
system would ask me to repeat information I had 
not yet given to it.? We plan to revise our strategy 
so that we are a little more cautious in our 
reprompts. Occasionally, the system misidentified 
the user?s desired task and had difficulty backing 
off gracefully and starting the correct task. We are 
working on improving our recovery strategies for 
these cases. 
 The following transcription of an interaction 
with our system illustrates the efficiency made 
possible when the data supplied by the caller drives 
the conversation: 
AMITI?S:  Hello, this is Am-ih-tee-eh! What can 
I do for you? 
USER:  yes this is Stella Lang. My card number 
is three one six seven one four. Can you tell me the 
balance on my account please? 
AMITI?S:  Can you just confirm the city and 
county? 
USER:  uh yes it?s London, Middlesex 
AMITI?S:  I?m sorry, I didn?t understand what 
you said.  Can you repeat, please? 
USER:  London, Middlesex 
AMITI?S:  All right, your balance is 1753 
pounds and 20 pence. Anything else I can do for 
you? 
USER:  yes I'd like to make a payment of fifty 
three pounds with my debit card please 
AMITI?S:  Can I have the debit card number, 
Miss Lang? 
USER:  sure, it's two eight seven four six six four 
nine five eight nine five three eight nine one, and it 
expires October two thousand six 
AMITI?S:  What's the issue number, please? 
USER:  one 
AMITI?S:  Ok, Miss Lang, I'll send in the 
payment amount of 53 pounds. Did I get that right? 
USER:  yes, what's my new balance please? 
AMITI?S:  Ok, that's gone through. Your 
payment will be credited overnight and will reflect 
on your next statement. Your balance is 1700 
pounds and 20 pence. Anything else I can do for 
you today? 
USER:  no, that's it thanks 
AMITI?S:  Thanks very much for your call, 
goodbye. 
USER:  bye 
5 Discussion, Future Work 
The preliminary evaluation reported here 
indicates promise for an automated dialogue 
system such as ours, which incorporates robust 
techniques for information extraction, record 
matching, task identification, dialogue act 
classification, and an overall data-driven strategy. 
Task duration and number of turns per dialogue 
both appear to indicate greater efficiency and 
corresponding user satisfaction than many other 
similar systems. In the DARPA Communicator 
evaluation, for example, between 60 and 79 calls 
were made to each of 8 participating sites (Walker, 
et al, 2001, 2002). A sample scenario for a 
domestic round-trip flight contained 8 concepts 
(airline, departure city, state, date, etc.). The 
average duration for such a call was over 300 
seconds; whereas our overall average was 104 
seconds. ASR accuracy rates in 2001 were about 
60% and 75%, for airline itineraries not completed 
and completed; and task completion rates were 
56%. Our average number of user words per turn, 
6.89, is also higher than that reported for 
Communicator systems. This number seems to 
reflect lengthier responses to open prompts, 
responses to system requests for multiple 
attributes, and greater user initiative. 
We plan to port the system to a new domain: 
from telephone banking to information-technology 
support. As part of this effort we are again 
collecting data from real human-human calls. For 
advanced speech recognition, we hope to train our 
ASR on new acoustic data. We also plan to expand 
our dialogue act classification so that the system 
can recognize more types of acts, and to improve 
our classification reliability.  
6 Acknowledgements 
This paper is based on work supported in part by 
the European Commission under the 5th 
Framework IST/HLT Programme, and by the US 
Defense Advanced Research Projects Agency. 
References 
J. Allen and M. Core. 1997. Draft of DAMSL: 
Dialog Act Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/resour
ces/damsl/. 
J. Allen, L. K. Schubert, G. Ferguson, P. Heeman, 
Ch. L. Hwang, T. Kato, M. Light, N. G. Martin, 
B. W. Miller, M. Poesio, and D. R. Traum. 
1995. The TRAINS Project: A Case Study in 
Building a Conversational Planning Agent. 
Journal of Experimental and Theoretical AI, 7 
(1995), 7?48. 
Amiti?s, http://www.dcs.shef.ac.uk/nlp/amities.  
J. Chu-Carroll and B. Carpenter. 1999. Vector-
Based Natural Language Call Routing. 
Computational Linguistics, 25 (3): 361?388. 
H. Cunningham, D. Maynard, K. Bontcheva, V. 
Tablan. 2002. GATE: A Framework and 
Graphical Development Environment for Robust 
NLP Tools and Applications. Proceedings of the 
40th Anniversary Meeting of the Association for 
Computational Linguistics (ACL'02), 
Philadelphia, Pennsylvania. 
H. Cunningham and D. Maynard and V. Tablan. 
2000. JAPE: a Java Annotation Patterns Engine 
(Second Edition). Technical report CS--00--10, 
University of Sheffield, Department of 
Computer Science.  
DARPA, 
http://www.darpa.mil/iao/Communicator.htm. 
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. 
Rosset, T. Strzalkowski, C. Ursu and N. Webb. 
2002. Multi-Layer Dialogue Annotation for 
Automated Multilingual Customer Service. 
Proceedings of the ISLE Workshop on Dialogue 
Tagging for Multi-Modal Human Computer 
Interaction, Edinburgh, Scotland. 
H. Hardy, T. Strzalkowski and M. Wu. 2003a. 
Dialogue Management for an Automated 
Multilingual Call Center. Research Directions in 
Dialogue Processing, Proceedings of the HLT-
NAACL 2003 Workshop, Edmonton, Alberta, 
Canada. 
H. Hardy, K. Baker, H. Bonneau-Maynard, L. 
Devillers, S. Rosset and T. Strzalkowski. 2003b. 
Semantic and Dialogic Annotation for 
Automated Multilingual Customer Service. 
Eurospeech 2003, Geneva, Switzerland. 
R. B. Inouye, A. Biermann and A. Mckenzie. 
2004. Caller Identification from Spelled-Out 
Personal Data Using a Database for Error 
Correction. Duke University Internal Report. 
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, 
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. 
Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and 
M. Walker. 2000. The AT&T-DARPA 
Communicator Mixed-Initiative Spoken Dialog 
System. ICSLP 2000. 
D. Maynard. 2003. Multi-Source and Multilingual 
Information Extraction. Expert Update. 
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, 
and V. Zue. 1998. Galaxy-II: A Reference 
Architecture for Conversational System 
Development. ICSLP 98, Sydney, Australia. 
S. Seneff and J. Polifroni. 2000. Dialogue 
Management in the Mercury Flight Reservation 
System. Satellite Dialogue Workshop, ANLP-
NAACL, Seattle, Washington. 
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. 
Garofolo, L. Hirschman, A. Le, S. Lee, S. 
Narayanan, K. Papineni, B. Pellom, J. Polifroni, 
A. Potamianos, P. Prabhu, A. Rudnicky, G. 
Sanders, S. Seneff, D. Stallard and S. Whittaker. 
2001. DARPA Communicator Dialog Travel 
Planning Systems: The June 2000 Data 
Collection. Eurospeech 2001. 
M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J. 
Garofolo, H. Hastie, A. Le, B. Pellom, A. 
Potamianos, R. Passonneau, R. Prasad, S. 
Roukos, G. Sanders, S. Seneff and D. Stallard. 
2002. DARPA Communicator Evaluation: 
Progress from 2000 to 2001. ICSLP 2002. 
W. Ward and B. Pellom. 1999. The CU 
Communicator System. IEEE ASRU, pp. 341?
344. 
W. Xu and A. Rudnicky. 2000. Task-based Dialog 
Management Using an Agenda. ANLP/NAACL 
Workshop on Conversational Systems, pp. 42?
47. 
 
Multilingual Authoring: the NAMIC approach
R. Basili, M.T. Pazienza
F. Zanzotto
Dept. of Computer Science
University of Rome, Tor Vergata
Via di Tor Vergata,
00133 Roma
Italy
basili@info.uniroma2.it
pazienza@info.uniroma2.it
zanzotto@info.uniroma2.it
R. Catizone, A. Setzer
N. Webb, Y. Wilks
Department of Computer Science
University of Sheffield
Regent Court
211 Portobello Street,
Sheffield S1 4DP, UK
R.Catizone@dcs.shef.ac.uk
A.Setzer@dcs.shef.ac.uk
N.Webb@dcs.shef.ac.uk
Y.Wilks@dcs.shef.ac.uk
L. Padro?, G. Rigau
Dept. Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
Centre de Recerca TALP
Jordi Girona Salgado 1-3,
08034 Barcelona
Spain
padro@lsi.upc.es
g.rigau@lsi.upc.es
Abstract
With increasing amounts of elec-
tronic information available, and the
increase in the variety of languages
used to produce documents of the
same type, the problem of how to
manage similar documents in dif-
ferent languages arises. This pa-
per proposes an approach to process-
ing/structuring text so that Multi-
lingual Authoring (creating hyper-
text links) can be effectively car-
ried out. This work, funded by
the European Union, is applied to
the Multilingual Authoring of news
agency text. We have applied meth-
ods from Natural Language Process-
ing, especially Information Extrac-
tion technology, to both monolingual
and Multilingual Authoring.
1 Introduction
Modern Information Technologies are faced
with the problem of selecting, filtering and
managing growing amounts of multilingual
information to which access is usually criti-
cal. Traditional Information Retrieval (IR)
approaches are too general in their selection
of relevant documents where as traditional
Information Extraction (IE) (Gaizauskas and
Wilks, 1998; Pazienza, 1997) approaches are
too specific and inflexible. Automatic Au-
thoring is a good example of how these two
methods can be improved and used to cre-
ate a hypertextual organisation of (multilin-
gual) information. This kind of information
is ?added value? to the information embodied
in the text and is not in contrast with other
retrieval paradigms. Automatic Authoring
is the activity of processing news items in
streams, detecting and extracting relevant in-
formation from them and, accordingly, organ-
ising texts in a non-linear fashion.
While IE systems like the ones participat-
ing in the Message Understanding Conference
(MUC, 1998) are oriented towards specific
phenomena (e.g. joint ventures) in restricted
domains, the scope of Automatic Authoring
is wider. In Automatic Authoring, the hy-
pertextual structure has to provide naviga-
tion guidelines to the final user which can also
refuse the system suggestions.
In this paper an architecture for Automatic
Multilingual Authoring is presented based on
knowledge-intensive and large-scale Informa-
tion Extraction. The general architecture
is presented capitalising robust methods of
Information Extraction (Cunningham et al,
1999) and large-scale multilingual resources
(e.g. EuroWordNet). The system is de-
veloped within a European project in the
Human Language Technologies area, called
NAMIC (News Agencies Multilingual Infor-
mation Categorisation)1. It aims to extract
relevant facts from the news streams of large
European news agencies and newspaper pro-
ducers2, to provide hypertextual structures
within each (monolingual) stream and then
produce cross-lingual links between streams.
2 Authoring
2.1 Automatic Authoring
As Automatic Authoring is the task of au-
tomatically deriving a hypertextual structure
from a set of available news articles (in three
different languages English, Spanish and Ital-
ian in our case), the complexity of the overall
framework requires a suitable decomposition:
Text processing requires at least the de-
tection of morphosyntactic information char-
acterising the source texts: recognition, nor-
malisation, and assignment of roles is required
for the main participants for the different
events/facts described.
Event Matching is then the activity of
selecting the relevant facts of a news arti-
cle, in terms of their general type (e.g. sell-
ing or buying companies, winning a football
match), their participants and their related
roles (e.g. the company sold or the winning
football team).
Authoring is thus the activity of gener-
ating links between news articles according
to relationships established among facts de-
tected in the previous phase.
For instance, a company acquisition can be
referred to in one (or more) news items as:
? Intel, the world?s largest chipmaker,
bought a unit of Danish cable maker NKT
that designs high-speed computer chips ...
1See http://namic.itaca.it.
2EFE and ANSA, the major news agencies in Spain
and Italy respectively, and the Financial Times are all
members of the NAMIC consortium.
? The giant chip maker Intel said it ac-
quired the closely held ICP Vortex Com-
putersysteme, a German maker of sys-
tems ...
? Intel ha acquistato Xircom inc. per 748
milioni di dollari.
The hypothesis underlying Authoring is
that all the above news items deal with facts
in the same area of interest to a potential class
of readers. They should be thus linked and
links should suggest to the user that the un-
derlying motivation (used to decide whether
or not to follow an available link) is that they
all refer to Intel acquisitions.
Notice that a link generation process based
only upon words would fail in the above case
as the common word (that could play the role
of anchor in linking) is the proper noun Intel.
As no other information is available, the re-
sulting set of potential matches can be huge
and the connectivity too high.
In order to get the suitable links the equiv-
alence between the senses of bought and ac-
quired in the first two news items must be
known. Although such a relation can be
drawn by mechanisms like query expansion or
thesauri of synonyms (e.g. WordNet (Miller,
1990)), word polysemy and noise may re-
sult in an inherent proliferation of irrelevant
matches. Contextual information is critical
here. Notice that the senses of ?buy? and ?ac-
quire? are constrained by the role played by
Intel as ?agent ? and NKT or ICP Vortex be-
ing the sold companies. In fact, Intel buys
silicon represents an unwanted sense of the
verb and should be distinguished.
The relevant information concerning Intel
should be thus limited to:
? Intel buys a unit of NKT
? Intel acquires ICP Vortex.
These descriptions provide the core infor-
mation able to establish equivalence among
the underlying events. Whenever base event
descriptions are available the linking process
can be carried out via simpler equivalence in-
ferences. The Authoring problem is thus a
side effect of the overall language-processing
task.
According to the suggested decomposition
all the above steps are mandatory. First text
processing is responsible for morpho-syntactic
recognition. Morphological units and syntac-
tic relations are produced for each sentence at
this stage. However, syntactic relations (e.g.
among subjects and verbs) are not sufficient
for proper event characterisation. In the ex-
ample(s), the subject of the verb acquire is
a pronoun only anaphorically referring to In-
tel. Co-reference resolution is usually applied
to this kind of mismatch at the surface level.
This capability is under the responsibility of
the event matching phase. Moreover, in or-
der to keep track of events over syntactic rep-
resentations, references to a target ontology
are required. In such an ontology, equiva-
lence among facts (e.g. buying companies) is
represented. For instance, the relation among
buy and acquire can be encoded under a more
general notion of financial acquisition. On-
tologies also define the set of relevant facts of
the target domain. A financial acquisition is
a perfect example of what is needed in cor-
porate industrial news but is less important,
for example, in sports news, where hiring of
players seems a more relevant event class.
Conceptual differences among facts (de-
tected during event matching) motivate a se-
lective notion of hyperlinking. These links
can be thus generated during the automatic
authoring phase. They are ontologically jus-
tified as their conceptual representation is al-
ready available at this stage. Types as same
acquisition fact, same person, or company can
be used to distinguish links and make expla-
nations available to the user.
2.2 Multilingual Automatic
Authoring
?From a multilingual perspective, the prob-
lem is to establish links among news in dif-
ferent languages. Full-text approaches can
rely only on language independent phenom-
ena (e.g. proper nouns like Intel) that are
very limited in texts. Most of the above-
mentioned inferences require language neu-
tral information (i.e. conceptual and not lexi-
cal constraints). The inherent overgeneration
related to word polysemy affects the results
of translation-based approaches. Again prin-
cipled representations made available by IE
processes (i.e. templates) provide a viable
solution. The different event realisations (in
the different languages) can be handled dur-
ing the overall event matching. A lexical in-
terface to the ontology is able to factor the
language specific information. As syntactic
differences are handled during text process-
ing, the result is a common domain model for
IE plus independent lexical interfaces. The
unified representation of the set of facts ac-
tivates multilingual linking at a conceptual
level, thus making the Authoring a language
independent process. Some challenges of such
a framework are:
? the size of the ontological resources re-
quired in terms of taxonomic (i.e. IS A
relations) and conceptual information
(i.e. classes of events and implied
participant-event relations)
? the size of the lexical interfaces to the
ontology available for the different lan-
guages
? the amount of task dependent knowledge.
For example the definition of the set of
events useful for the target application is
underspecified.
In the following, we propose a complex ar-
chitecture where the above problems are
approached according to well-assessed tech-
niques presented elsewhere. Robust Informa-
tion Extraction is adopted (Humphreys et al,
1998) as an overall method for text process-
ing and event matching. Target events are
semiautomatically derived from domain texts
and represented in the IE engine ontology. Fi-
nally, multilinguality is realised by assuming a
large-scale multilingual lexical hierarchy as a
reference ontology for nominal concepts. The
resulting architecture for Multilingual Auto-
matic Authoring is presented in Section 3.4.
3 The NAMIC system
3.1 Large scale IE for Automatic
Authoring
Information Extraction is a very good ap-
proach to Automatic Authoring for a num-
ber of reasons. The key components of an IE
system are events and objects - the kind of
components that trigger hyperlinks in an Au-
thoring system. Coreference is a significant
part of Information Extraction and indeed a
necessary component in Authoring. Named
Entities - people, places, and organisations,
etc. - play an important part in Authoring
and again are firmly addressed in Information
Extraction systems.
The role of a world model as a method
for event matching and coreferencing
The world model is an ontological represen-
tation of events and objects for a particular
domain or set of domains. The world model
is made up of a set of event and object types,
with attributes. The event types characterise
a set of events in a particular domain and
are usually represented in a text by verbs.
Object Types on the other hand, are best
thought of as characterising a set of people,
places or things and are usually represented
in a text by nouns (both proper and com-
mon). When used as part of an Information
Extraction system, the instances of each type
are inserted/added to the world model. Once
the instances have been added, a procedure
is carried out to link those instances that re-
fer to the same thing - achieving coreference
resolution.
In NAMIC, the world model is created
using the XI cross-classification hierarchy
(Gaizauskas and Humphreys, 1996). The def-
inition of a XI cross-classification hierarchy is
referred to as an ontology, and this together
with an association of attributes with nodes
in the ontology forms the world model. Pro-
cessing a text acts to populate this initially
bare world model with the various instances
and relations mentioned in the text, convert-
ing it into a discourse model specific to the
particular text.
The attributes associated with nodes in
the ontology are simple attribute:value pairs
where the value may either be fixed, as in
the attribute animate:yes which is associ-
ated with the person node, or where the value
may be dependent on various conditions, the
evaluation of which makes reference to other
information in the model.
3.1.1 The Description of LaSIE
LaSIE is a Large-scale Information Ex-
traction system, developed for MUC (Mes-
sage Understanding Conference) competi-
tions, comprised of a variety of modules, see
(Humphreys et al, 1998; MUC, 1998). Al-
though we are not using the complete LaSIE
system in NAMIC, we are using 2 of the key
modules - the Named Entity Matcher and the
Discourse Processor. Below is a description of
each of these modules.
Named Entity Matcher The Named En-
tity Matcher finds named entities through
a secondary phase of parsing which uses a
named entity grammar and a set of gazetteer
lists. It takes as input parsed text from the
first phase of parsing and the named entity
grammar which contains rules for finding a
predefined set of named entities and a set of
gazetteer lists containing proper nouns. The
Name Entity Matcher returns the text with
the Named Entities marked. The Named En-
tities in NAMIC are PERSONS, ORGANI-
SATIONS, LOCATIONS, and DATES. The
Named Entity grammar contains rules for
coreferring abbreviations as well as different
ways of expressing the same named entity
such as Dr. Smith, John Smith and Mr.
Smith occurring in the same article.
Discourse Processor The Discourse Pro-
cessor module translates the semantic rep-
resentation produced by the parser into a
representation of instances, their ontolog-
ical classes and their attributes, in the
XI knowledge representation language (see
Gaizauskas(1996)). XI allows a straightfor-
ward definition of cross-classification hierar-
chies, the association of arbitrary attributes
with classes or instances, and a simple mech-
anism to inherit attributes from classes or in-
stances higher in the hierarchy.
The semantic representation produced by
the parser for a single sentence is processed
by adding its instances, together with their
attributes, to the discourse model which has
been constructed so far for the text.
Following the addition of the instances
mentioned in the current sentence, together
with any presuppositions that they inherit,
the coreference algorithm is applied to at-
tempt to resolve, or in fact merge, each of
the newly added instances with instances cur-
rently in the discourse model.
The merging of instances involves the re-
moval of the least specific instance (i.e. the
highest in the ontology) and the addition of
all its attributes to the other instance. This
results in a single instance with more than one
realisation attribute, which corresponds to a
single entity mentioned more than once in the
text, i.e. a coreference.
3.2 Ontological Modeling
As we have seen in section 3.1, some critical
issues of the NAMIC project rely on the per-
formance of the lexical and conceptual compo-
nents of all linguistic processors. As NAMIC
faces large-scale coverage of news in several
languages we decided to adopt EuroWordNet
(Vossen, 1998) as a common semantic formal-
ism to support:
? lexical semantic inferences (e.g. general-
isation, disambiguation)
? broad coverage (e.g. lexical and semanti-
cal) and
? a common interlingual platform for link-
ing events from different documents.
The NAMIC ontology consists of 40 prede-
fined object classes and 46 attribute types re-
lated to Name Entity objects and nearly 1000
objects relating to EuroWordNet base con-
cepts.
3.2.1 EuroWordNet as a Multilingual
Lexical Knowledge Base
Since the world model aims to describe the
language used in a given domain via events
and objects, the accuracy and breadth of the
model will impact how well the information
extraction works.
EuroWordNet (Vossen, 1998) is a multilin-
gual lexical knowledge base (LKB) with word-
nets for several European languages (Dutch,
Italian, Spanish, German, French, Czech and
Estonian). The wordnets are structured
in the same way as the American wordnet
for English developed at Princeton (Miller,
1990) containing synsets (sets of synonymous
words) with basic semantic relations between
them.
Each wordnet represents a unique
language-internal system of lexicalisa-
tions. In addition, the wordnets are linked
to an Inter-Lingual-Index (ILI), based on
the Princeton WordNet 1.5. WordNet 1.6 is
also connected to the ILI as another English
WordNet (Daude et al, 2000). Via this
index, the languages are interconnected so
that it is possible to go from the words in
one language to words in any other language
having similar meaning. The index also
gives access to a shared top-ontology and
a subset of 1024 Base Concepts (BC). The
Base Concepts provide a common seman-
tic framework for all the languages, while
language specific properties are maintained
in the individual wordnets. The LKB can
be used, among others, for monolingual and
cross-lingual information retrieval, which
has been demonstrated in other projects
(Gonzalo et al, 1998).
3.3 Multilingual Event description
The traditional limitations of a knowledge-
based information extraction system such as
LaSIE have been the need to hand-code in-
formation for the world model - specifically
relating to the event structure of the domain.
For the NAMIC project, we have decided
to semi-automate the process of adding new
?event descriptions? to the World Model. To
us, event descriptions can be categorised as a
set of regularly occurring verbs within our do-
main, complete with their subcategorisation
information.
These verbs can be extracted with simple
statistical techniques and are, for the moment
subjected to hand pruning. Once a list of
verbs has been extracted, subcategorisation
patterns can be generated automatically using
a Galois lattice (as described in (Basili et al,
2000b)). These frames can then be uploaded
into the event hierarchy of the discourse in-
terpreter world model.
The world model can have a structure
which is essentially language independent in
all but the lowest level - at which stage lexi-
calisations relating to each representative lan-
guage are required. Associated with these lex-
icalisations are language dependent scenario
rules which control the behaviour of instances
of these events with a Discourse Model. These
rules are expected to differ across languages in
the way they control coreference for languages
which are constrained to lesser or greater de-
gree.
The lattice generates patterns which refer
to synsets in the WordNet hierarchy. For
our purposes, we will use patterns referring to
Base Concepts in the EuroWordNet hierarchy
- which allows us to exploit the Inter-Lingual-
Index as described in the previous section.
These Base Concepts serve as a level of mul-
tilingual abstraction for the conceptual con-
straints of our events, and allow us to extend
the number of semantic classes from seven
(the MUC Named Entity classifications) to
1024 - the number of base concepts in EWN.
3.4 The NAMIC Architecture
The complexity of the overall NAMIC sys-
tem required the adoption of a distributed
computing paradigm in the design. The sys-
tem is a distributed object oriented system
where services (like text processing or Multi-
lingual Authoring) are provided by indepen-
dent components and asynchronous communi-
cation is allowed. Independent news streams
for the different languages (English, Spanish,
and Italian) are assumed. Language specific
processors (LPs) are thus responsible for text
processing and event matching in indepen-
dent text units in each stream. LPs com-
pile an objective representation (see Fig. 1)
for each source texts, including the detected
morphosyntactic information, categorisation
in news standards (IPTC classes) and descrip-
tion of the relevant events. Any later Au-
thoring activity is based on this canonical
representation of the news. In particular a
monolingual process is carried out within any
stream by the three monolingual Authoring
Engines (English AE, Spanish AE, and Ital-
ian AE). A second phase is foreseen to take
into account links across streams, i.e. multi-
lingual hyper-linking: a Multilingual Author-
ing Engine (M-AE) is here foreseen. Figure
1 represents the overall flow of information.
The Language Processors are composed of a
morphosyntactic (Eng, Ita and Spa MS) and
an event-matching component (EM). The lex-
ical interfaces (ELI, SLI and ItLI) to the uni-
fied Domain model are also used during event
matching.
The linguistic processors are in charge of
producing the objective representation of in-
coming news. This task is performed during
MS analysis by two main subprocessors:
? a modular and lexicalised shallow
morpho-syntactic parser (Basili et al,
2000c), providing name entity match-
ing and extracting dependency graphs
from source sentences. Ambiguity is
controlled by part-of-speech tagging and
domain verb-subcategorisation frames
that guide the dependency recognition
phase.
? a statistical linear text classifier based
upon some of the derived linguistic fea-
tures (Basili et al, 2000a) (lemmas, POS
tags and proper nouns)
The results are then input to the event
matcher that by means of the discourse in-
terpreter (Humphreys et al, 1998) derive the
objective representation. As discussed in sec-
tion 3.1, coreferencing is a side effect of the
discourse interpretation (Humphreys et al,
1998). It is based on the multilingual domain
model where relevant events are described and
nominal concepts represented.
The overall architecture is highly modular
and open to load balancing activity as well as
to adaptation and porting. The communica-
tion interfaces among the MS and EM com-
ponents as well as among the AEs and the M-
AE processors are specified via XML DTDs.
This allows for user-friendly uploading of a
back-end database with the detected material
as well as the easy design and management of
the front-end databases (available for tempo-
rary tasks, like event matching after MS). All
the servers are objects in a distributed archi-
tecture within a CORBA environment. The
current version includes the linguistic proces-
sors (MS and EM) for all the three languages.
The English and Italian linguistic processors
are fully object oriented modules based on
EnglishMS
SpanishMS
ItalianMS
EnglishAE
SpanishAE
ItalianAE
news ObjectiveRepresentation Monolingual Links
Multilingual Links
EnglishEM
SpanishEM
ItalianEM
DomainModel
ELI
SLI
ItLI
Multi-LingualAuthoringEngine
Language Processors
Figure 1: Namic Architecture
Java. They integrate libraries written in C,
C++, Prolog, and Perl for specific functional-
ities (e.g. parsing) running under a Windows
NT platform. The Spanish linguistic proces-
sor shares the discourse interpreter and the
text classifier with the other modules, while
the morpho syntactic component is currently
a Unix server based on Perl. The use of a dis-
tributed architecture under CORBA allowed
a flexible solution to its integration into the
overall architecture. The servers can be in-
stantiated in multiple copies throughout the
network if the amount of required computa-
tion exceeds the capability of a current con-
figuration. As the workload of a news stream
is not easily predictable, distribution and dy-
namic load balancing is the only realistic ap-
proach.
4 Discussion and Future Work
The above sections have provided the out-
line of a general NLP-based approach to auto-
matic authoring. The emphasis given to tra-
ditional capabilities of Information Extraction
depends on the relevance of news content in
the target Web service scenarios as well as
on their inherent multilinguality. The bet-
ter is the generalisation provided by the IE
component, the higher is the independence
from the text source language. As a result,
IE is here seen as a natural approach to cross-
lingual hypertextual authoring. Other works
in this area make extensive use of traditional
IR techniques (e.g. full text search) or rely
on already traced (i.e. manually coded) hy-
perlinks (e.g. (Chakrabarti et al, 1998; Klein-
berg, 1999)). The suggested NAMIC architec-
ture exploits linguistic capabilities for deriv-
ing entirely original (ex novo) resources, over
dynamic, previously unreleased, streams of in-
formation.
The result is a large-scale multilingual NLP
application capitalising existing methods and
resources within an advanced software engi-
neering process. The use of a distributed
Java/CORBA architecture makes the system
very attractive for its scalability and adaptiv-
ity. It results in a very complex (but realis-
tic) NLP architecture. Its organisation (lexi-
cal interfaces with respect to the multilingual
ontology) makes it very well suited for cus-
tomisation and porting to large domains. Al-
though the current version is a prototype, it
realises the complete set of core functionali-
ties, including the main IE steps and the dis-
tributed Java/CORBA layer.
It is worth noticing that a set of extensions
are made viable within the proposed architec-
ture. A first line is the extension of the avail-
able multilingual lexical knowledge. The Dis-
course Model can be used to better reflect on-
tological relationships within a particular do-
main. These relationships could be examined
to confirm known word sense usage as well
as to postulate/propose novel word sense us-
age. Using the mechanism for the addition of
events (as categorised by verbs) to the world
model, users can specify new events which can
be added to the IE system, to achieve User
Driven IE, and deliver a form of adaptive in-
formation extraction.
The instantiated domain models can be
thus used as a basis for ontological resource
expansion as a form of adaptive process.
For example, the stored instantiations of dis-
course models within a specific domain can be
compared: it may be thus possible to recog-
nise new sets of events or objects which are
not currently utilised within the system.
The evaluation strategy that is made possi-
ble within the NAMIC consortium will make
use of the current users (i.e. news agencies)
expertise. The agreed evaluation methods
will provide evidence about the viability of
the proposed large-scale IE-based approach to
authoring, as a valuable paradigm for infor-
mation access.
Acknowledgements
This research is funded by the European
Union, grant number IST-1999-12392. We
would also like to thank all of the partners
in the NAMIC consortium.
References
R. Basili, A. Moschitti, and M.T. Pazienza. 2000a.
Language sensitive text classification. In In
proceeding of 6th RIAO Conference (RIAO
2000), Content-Based Multimedia Information
Access, Coll ge de France, Paris, France.
R. Basili, M.T. Pazienza, and M. Vindigni. 2000b.
Corpus-driven learning of event recognition
rules. In Proc. of Machine Learning for Infor-
mation Extraction workshop, held jointly with
the ECAI2000, Berlin, Germany.
R. Basili, M.T. Pazienza, and F.M. Zanzotto.
2000c. Customizable modular lexicalized pars-
ing. In Proc. of the 6th International Workshop
on Parsing Technology, IWPT2000, Trento,
Italy.
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg,
P. Raghavan, and S. Rajagopalan. 1998. Auto-
matic resource compilation by analysing hyper-
link structure and associated text. In Proceed-
ings of the 7th International World Wide Web
Conference, Brisbane, Australia.
C. Cunningham, R. Gaizauskas, K. Humphreys,
and Y. Wilks. 1999. Experience with a lan-
guage engineering architecture: 3 years of gate.
In Proceedings of the AISB?99 Workshop on
Reference Architectures and Data Standards for
NLP, Edinburgh, UK.
J. Daude, L. Padro, and G. Rigau. 2000. Map-
ping wordnets using structural information.
In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics
ACL?00, Hong Kong, China.
R. Gaizauskas and K. Humphreys. 1996. Xi:
A simple prolog-based language for cross-
classification and inheritance. In Proceedings of
the 6th International Conference on Artificial
Intelligence: Methodologies, Systems, Applica-
tions (AIMSA96), pages 86?95.
R. Gaizauskas and Y. Wilks. 1998. Information
Extraction: Beyond Document Retrieval. Jour-
nal of Documentation, 54(1):70?105.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigar-
ran. 1998. Indexing with wordnet synsets
can improve text retrieval. In Proceedings of
the COLING/ACL?98 Workshop on Usage of
WordNet for NLP, Montreal, Canada.
K. Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, H. Cunningham, and
Y. Wilks. 1998. University of sheffield: De-
scription of the lasie-ii system as used for muc-7.
In Proceedings of the Seventh Message Under-
standing Conferences (MUC-7). Morgan Kauf-
man. Available at http://www.saic.com.
Jon M. Kleinberg. 1999. Authoritative sources
in a hyperlinked environment. Journal of the
ACM, 46(5):604?632.
G. Miller. 1990. Five papers on wordnet. Inter-
national Journal of Lexicography, 4(3).
1998. Proceedings of the Seventh Message Under-
standing Conference (MUC-7). Morgan Kauf-
man. Available at http://www.saic.com.
M.T. Pazienza, editor. 1997. Information Ex-
traction. A Multidisciplinary Approach to an
Emerging Information Technology. Number
1299 in LNAI. Springer-Verlag, Heidelberg,
Germany.
P. Vossen. 1998. EuroWordNet: A Multilin-
gual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht.
Knowledge-Based Multilingual Document Analysis
R. Basili
 
and R. Catizone  and L. Padro  and M.T. Pazienza  
G. Rigau  and A. Setzer  and N. Webb 
F. Zanzotto
 
 
Dept. of Computer Science, Systems and Production
University of Rome, Tor Vergata
Via di Tor Vergata
00133 Roma, Italy
basili, pazienza, zanzotto@info.uniroma2.it
 Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
R.Catizone, A.Setzer, N.Webb@dcs.shef.ac.uk
 Departament de Llenguatges i Sistemes Informatics
Universitat Politecnica de Catalunya
Centre de Recerca TALP
Jordi Girona Salgado 1-3
08034 Barcelona, Spain
l.padro, g.rigau@lsi.upc.es
Abstract
The growing availability of multilingual resources,
like EuroWordnet, has recently inspired the develop-
ment of large scale linguistic technologies, e.g. mul-
tilingual IE and Q&A, that were considered infeasi-
ble until a few years ago. In this paper a system
for categorisation and automatic authoring of news
streams in different languages is presented. In our
system, a knowledge-based approach to Information
Extraction is adopted as a support for hyperlinking.
Authoring across documents in different languages
is triggered by Named Entities and event recogni-
tion. The matching of events in texts is carried out
by discourse processing driven by a large scale world
model. This kind of multilingual analysis relies on a
lexical knowledge base of nouns(i.e. the EuroWord-
net Base Concepts) shared among English, Spanish
and Italian lexicons. The impact of the design choices
on the language independence and the possibilities it
opens for automatic learning of the event hierarchy
will be discussed.
1 Introduction
Modern information technologies are faced with the
problem of selecting, filtering, linking and manag-
ing growing amounts of multilingual information to
which access is usually critical. Our work is moti-
vated by the linking of multilingual information in a
wide range of domains. Although this problem ap-
pears to be directly related to the Information Re-
trieval task, we aimed to link articles, not in the broad
sense of clustering documents related to the same
topic, but rather more specifically linking particular
pieces of information together from different docu-
ments. Furthermore, we found that IE research, al-
though appropriate for our task, was not designed for
the scale/variety of different domains that we needed
to process. In general, creating the world model nec-
essary for the addition of a new domain to an IE sys-
tem is a time-consuming process. As such, we de-
signed an IE system that could be semi-automatically
and easily adapted to new domains - a process we will
refer to as large scale IE. The key to creating new
world models relied on incorporating large amounts
of domain knowledge. As a result we selected Eu-
roWordnet as our base knowledge source. EuroWord-
net has the advantages of 1) providing the foundation
for broad knowledge across many domains and 2) is
multilingual in nature. In this paper, we will explain
how our system works, how the knowledge base was
incorporated and a discussion of other applications
that could make use of the same technology.
2 The Application
In the 5th Framework NAMIC Project (News Agen-
cies Multilingual Information Categorisation), the de-
fined task of the system was to support the automatic
authoring of multilingual news agencies texts where
the chosen languages were English, Italian and Span-
ish. The goal was the Hypertextual linking of related
articles in one language as well as related articles in
the other project languages. One of the intermediate
goals of NAMIC was to categorise incoming news ar-
ticles, in one of the three target languages and use
Natural Language Technology to derive an ?objec-
tive representation? of the events and agents contained
within the news. This representation which is ini-
tially created once using representative news corpora
is stored in a repository and accessed in the authoring
process.
2.1 Automatic Authoring
Automatic Authoring is the task of automatically de-
riving a hypertextual structure from a set of available
news articles (in three different languages English,
Spanish and Italian in our case). This relies on the ac-
tivity of event matching. Event matching is the pro-
cess of selecting the relevant facts in a news article
in terms of their general type (e.g. selling or buying
companies, winning a football match), their partici-
pants and their related roles (e.g. the company sold or
the winning football team) Authoring is the activity
of generating links between news articles according
to relationships established among facts detected in
the previous phase.
For instance, a company acquisition can be referred
to in one (or more) news items as:
 Intel, the world?s largest chipmaker, bought a
unit of Danish cable maker NKT that designs
high-speed computer chips used in products that
direct traffic across the internet and corporate
networks.
 The giant chip maker Intel said it acquired the
closely held ICP Vortex Computersysteme, a
German maker of systems for storing data on
computer networks, to enhance its array of data-
storage products.
 Intel ha acquistato Xircom inc. per 748 milioni
di dollari.
 Le dichiarazioni della Microsoft, infatti, sono
state precedute da un certo fermento, dovuto
all?interesse verso Linux di grandi ditte quali
Corel, Compaq e non ultima Intel (che ha ac-
quistato quote della Red Hat) ...
The hypothesis underlying Authoring is that all the
above news items deal with facts in the same area of
interest to a potential class of readers. They should be
thus linked and links should suggest to the user that
the underlying motivation is that they all refer to Intel
acquisitions.
3 The NAMIC Architecture
The NAMIC system uses a modularised IE architec-
ture whose principal components, used to create the
IE repository, are morpho-syntactic analysis, cate-
gorisation and semantic analysis. During Morpho-
Syntactic analysis, a modular and lexicalised shal-
low morpho-syntactic parser (Basili et al, 2000b),
provides the extraction of dependency graphs from
source sentences. Ambiguity is controlled by part-
of-speech tagging and domain verb-subcategorisation
frames that guide the dependency recognition phase.
It is within the semantic analysis, which relies on the
output of this parser, that objects in the text, and their
relationships to key events are captured. This process
is explained in more detail in 4. In the next two sec-
tions, we will elaborate on the IE engine. For a full
description of the NAMIC Architecture see (Basili et
al., 2001).
3.1 LaSIE
In NAMIC, we have integrated a key part of the Infor-
mation Extraction system called LaSIE (Large-scale
Information Extraction system, (Humphreys et al,
1998)). Specifically, we have taken the Named Entity
Matcher and the Discourse Processor from the over-
all architecture of LaSIE. The roles of each of these
modules is outlined below.
3.1.1 Named Entity Matcher
The Named Entity (NE) Matcher finds named enti-
ties (persons, organisations, locations, and dates, in
our case) through a secondary phase of parsing which
uses a NE grammar and a set of gazetteer lists. It takes
as input parsed text from the first phase of parsing and
the NE grammar which contains rules for finding a
predefined set of named entities and a set of gazetteer
lists containing proper nouns. The NE Matcher re-
turns the text with the Named Entities marked. The
NE grammar contains rules for coreferring abbrevia-
tions as well as different ways of expressing the same
named entity such as Dr. Smith, John Smith and Mr.
Smith occurring in the same article.
3.1.2 Discourse Processor
The Discourse Processor module translates the se-
mantic representation produced by the parser into a
representation of instances, their ontological classes
and their attributes, in the XI knowledge representa-
tion language (Gaizauskas and Humphreys, 1996).
XI allows a straightforward definition of cross-
classification hierarchies, the association of arbitrary
attributes with classes or instances, and a simple
mechanism to inherit attributes from classes or in-
stances higher in the hierarchy.
The semantic representation produced by the
parser for a single sentence is processed by adding
its instances, together with their attributes, to the dis-
course model which has been constructed for a text.
Following the addition of the instances mentioned
in the current sentence, together with any presuppo-
sitions that they inherit, the coreference algorithm is
applied to attempt to resolve, or in fact merge, each
of the newly added instances with instances currently
in the discourse model.
The merging of instances involves the removal of
the least specific instance (i.e. the highest in the on-
tology) and the addition of all its attributes to the other
instance. This results in a single instance with more
than one realisation attribute, which corresponds to a
single entity mentioned more than once in the text,
i.e. a coreference.
The mechanism described here is an extremely
powerful tool for accomplishing the IE task, however,
in common with all knowledge-based approaches,
and as highlighted in the introduction to this paper,
the significant overhead in terms of development and
deployment is in the creation of the world model rep-
resentation.
4 Large-Scale World Model Acquisition
The traditional limitations of a knowledge-based in-
formation extraction system such as LaSIE have been
the need to hand-code information for the world
model - specifically relating to the event structure of
the domain. This is also valid for NAMIC. To aid the
development of the world model, a semi-automatic
boot-strapping process has been developed, which
creates the event type component of the world model.
To us, event descriptions can be categorised as a set
of regularly occurring verbs within our domain, com-
plete with their subcategorisation information.
4.1 Event Hierarchy
The domain verbs can be selected according to sta-
tistical techniques and are, for the moment, subjected
to hand pruning. Once a list of verbs has been ex-
tracted, subcategorisation patterns can be generated
automatically using a combination of weakly super-
vised example-driven machine learning algorithms.
There are mainly three induction steps. First, syn-
tactic properties are derived for each verb, express-
ing the major subcategorisation information under-
lying those verbal senses which are more important
in the domain. Then, in a second phase, verb usage
examples are used to induce the semantic properties
of nouns in argumental positions. This information
relates to selectional constraints, independently as-
signed to each verb subcategorisation pattern. Thus,
different verb senses are derived, able to describe the
main properties of the domain events (e.g. Compa-
nies acquire companies). In a third and final phase
event types are derived by grouping verbs accord-
ing to their syntactic-semantic similarities. Here,
shared properties are used to generalise from the lex-
ical level, and generate verbal groups expressing spe-
cific semantic (and thus conceptual) aspects. These
types are then fed into the event hierarchy as required
for their straightforward application within the target
IE scenario.
4.1.1 Acquisition of Subcategorisation Patterns
Each verb  is separately processed. First, each local
context (extracted from sentences in the source cor-
pus) is mapped into a feature vector describing:
 the verb  of each vector (i.e. the lexical head of
the source clause);
 the different grammatical relationships (e.g.
Subj and Obj for grammatical subject and ob-
jects respectively) as observed in the clause;
 the lexical items, usually nouns, occurring in
specific grammatical positions, e.g. the subject
Named Entity, in the clause.
Then, vectors are clustered according to the set of
shared grammatical (not lexical) properties: Only the
clauses showing the same relationships (e.g. all the
Subj- 
	 -Obj triples) enter in the same subset  .
Each cluster thus expresses a specific grammatical be-
haviour shared by several contexts (i.e. clauses) in the
corpus. The shared properties in  characterise the
cluster, as they are necessary and sufficient member-
ship conditions for the grouped contexts.
As one context can enter in more than one cluster
(as it can share all (or part) of its relations with the
others), the inclusion property establishes a natural
partial order among clusters. A cluster  is included
in another cluster  if its set of properties is larger
(i.e.  ) but it is shown only by a subset of the
contexts of the latter   . The larger the set of mem-
bership constraints is, the smaller the resulting cluster
is. In this way, clusters are naturally organised into
a lattice (called Galois lattice). Complete properties
express for each cluster candidate subcategorisation
patterns for the target verb  .
Finally, the lattice is traversed top-down and the
search stops at the more important clusters (i.e. those
showing a large set of members and characterised
by linguistically appealing properties): they are re-
tained and a lexicon of subcategorisation structures
(i.e. grammatical patterns describing different us-
ages of the same verb) is compiled for the target verb
 . For example, (buy, [Subj:X, Obj:Y]) can
be used to describe the transitive usage of the verb
	 . More details can be found in (Basili et al, 1997).
4.1.2 Corpus-driven Induction of Verb
Selectional Restrictions
The lattice can be further refined to express seman-
tic constraints over the syntactic patterns specified at
the previous stage. A technique proposed in (Basili
et al, 2000a) is adopted by deriving semantic con-
straints via synsets (i.e. synonymy sets) in the Word-
Net 1.6 base concepts (part of EuroWordNet). When
a given lattice node expresses a set of syntactic prop-
erties, then this suggests:
 a set of grammatical relations necessary to ex-
press a given verb meaning, Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 977?984
Manchester, August 2008
Investigating the Portability of Corpus-Derived Cue Phrases for Dialogue
Act Classification
Nick Webb and Ting Liu
ILS Institute
University at Albany, SUNY
Albany, NY, USA
{nwebb|tl7612}@albany.edu
Abstract
We present recent work in the area of
Cross-Domain Dialogue Act tagging. Our
experiments investigate the use of a sim-
ple dialogue act classifier based on purely
intra-utterance features - principally in-
volving word n-gram cue phrases. We ap-
ply automatically extracted cues from one
corpus to a new annotated data set, to de-
termine the portability and generality of
the cues we learn. We show that our auto-
matically acquired cues are general enough
to serve as a cross-domain classification
mechanism.
1 Introduction
A number of researchers (Hirschberg and Litman,
1993; Grosz and Sidner, 1986) speak of cue or key
phrases in utterances that can serve as useful indi-
cators of discourse structure. We have previously
investigated the use of such cue phrases to predict
dialogue acts or DAs (functional tags which rep-
resent the communicative intentions behind each
user utterance) (Webb et al, 2005a). We devel-
oped an approach, in common with the work of
Samuel et al (1999), where word n-grams that
might serve as cue phrases are automatically de-
tected in a corpus and we have previously reported
the results of experiments evaluating this approach
on the SWITCHBOARD corpus, where our results
rival the best reported over that data (Stolcke et al,
2000), although our method adopts a significantly
less complex algorithm.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
An interesting by-product of our approach is the
ranked list of cue phrases derived from the source
corpus. Visual inspection of these cues reveals
that, as one might expect, there is a high degree of
correlation between phrases such as ?can you? and
the DA <yes/no question>, ?where is? and ?who
is? with the DA <wh-question> and ?right? or
?ok? with DA <agree/accept>. These cues appear
to be of a general nature, unrelated to the source
domain or application. Therefore, despite being
automatically acquired from one domain specific
corpus, these cues should be equally applicable to
new corpora, from a different domain and it is this
hypothesis we test. This paper presents our work
on dialogue act classification using cues automat-
ically extracted from a corpus from one domain,
and applying these cues directly as a classifier over
a new corpus from a different domain.
The material is presented as follows: Previous
work with dialogue act modelling is outlined in
Section 2. An overview of the corpora used for
the experiments we report can be seen in Section
3. A brief overview of our classification method
is given in Section 4. Our experiments evaluating
the cue-based dialogue act classifier tagging new,
out-of-domain data are given in Section 5. Finally
we end with some discussion and an outline of in-
tended further work.
2 Related Work
Dialogue Acts (DAs) (Bunt, 1994), also known as
speech acts or dialogue moves, represent the func-
tional performance of a speaker?s utterance, such
as a greeting ?Hello there?, asking a question like
?How is your mother?? or making a request ?Can
you move your foot??.
There are two broad categories of computational
model used to interpret these acts. The first, in-
977
Corpus Availability
Utterance
count
Dialogue
count
Word
count
Distinct
words
Dialogue
type
SWITCHBOARD public 223606 1155 1431725 21715 Conversational
AMITI
?
ES GE restricted 30206 1000 228165 7841 Task-oriented
Figure 1: Summary data for the dialogue corpora
cluding the work of Cohen and Perrault (1979) re-
lies on processing belief logics, centring on the im-
pact each utterance has on the hearer - what the
hearer believes the speaker intended to commu-
nicate. These models can be very accurate, but
often are complex, and require significant world-
knowledge to create.
The second model type is cue-based, and cen-
tres on the notion of repeated, predictive cues -
subsections of language which are strong indica-
tors of specific DAs. In this second category, much
of the work is cast as a probabilistic classification
task, solved by training approaches on labelled ex-
amples of dialogue acts. As an example of these
probabilistic methods, Stolcke et al (2000) apply
a HMM method to the SWITCHBOARD corpus, one
that exploits both the order of words within ut-
terances and the order of dialogue acts over ut-
terances. They use a single split of the data for
their experiments, with 198k utterances for train-
ing and 4k utterances for testing, achieving a DA
tagging accuracy of 71.0% on word transcripts.
Another learning approach by Samuel et al (1998)
uses transformation-based learning over a number
of utterance features, including utterance length,
speaker turn and the dialogue act tags of adja-
cent utterances. They achieved an average score
of 75.12% tagging accuracy over the VERBMO-
BIL corpus. A significant aspect of this work is
the automatic identification of word sequences that
might serve as useful dialogue act cues (Samuel et
al., 1999). A number of statistical criteria are ap-
plied to identify potentially useful word n-grams
that are then supplied to the transformation-based
learning method as ?features?.
What has been less explored is the portabil-
ity or adaptability of these models to new cor-
pora and new domains. Prasad and Walker (2002)
look at applying models generated from a Human-
Computer corpus to a Human-Human corpus in the
same domain, that of travel planning, and score a
very low 36.72% accuracy using their model. The
work of Tur et al (2006) is closer to the work re-
ported here - they apply models derived from the
SWITCHBOARD corpus to the ICSI-MRDA corpus
(Shriberg et al, 2004) using boosting, applied to
a high level of representation (comprising only 5
DA categories, one of which they exclude), where
they achieve 57.37% tagging accuracy. This seems
to indicate that cross-domain application of mod-
els is possible, although the level of accuracy as
presently reported is low.
3 Experimental Corpora
Our work as described here applies to two corpora
- the DA-tagged portion of the SWITCHBOARD cor-
pus (Jurafsky et al, 1998), and the AMITI
?
ES GE
corpus (Hardy et al, 2002; Hardy et al, 2003), cre-
ated as part of the AMITI
?
ES European 5th Frame-
work program project (Hardy et al, 2005). A sum-
mary of the two corpora can be seen in Figure 1.
3.1 Switchboard
The annotated portion of the SWITCHBOARD cor-
pus comprises 1155 annotated conversations be-
tween two human participants, where the dia-
logues are of an unstructured, non-directed char-
acter. Participants do not know each other, and
are provided only with a set of topics which they
may wish to discuss. The SWITCHBOARD corpus
is annotated using an elaboration of the DAMSL
tag set. In 1998 the Discourse Resource Initia-
tive finalised a task-independent set of DAs, called
DAMSL (Dialogue Act Markup in Several Layers),
for use across different domains. DAMSL has been
used to mark-up several dialogue corpora, such as
TRAINS (Core and Allen, 1997), and the SWITCH-
BOARD corpus (Jurafsky et al, 1998).
The annotation over the SWITCHBOARD corpus
involves 50 major classes, together with a num-
ber of diacritic marks, which combine to generate
220 distinct labels. Jurafsky et al (1998) propose
a clustering of these 220 tags into 42 larger classes
and it is this clustered set that was used both in
our experiments and those of Stolcke et al (2000).
In measuring the agreement between annotators in
labelling this data, Jurafsky et al (1998) report
an average pair-wise kappa of .80 (Carletta et al,
978
<Turn Id="utt3" Speaker="A" DA-Type="Open-question"> what do you think was different ten
years ago from now?</Turn>
<Turn Id="utt4" Speaker="B" DA-Type="Statement-opinion"> Well I would say as far as social
changes go I think families were more together.</Turn>
<Turn Id="utt5" Speaker="B" DA-Type="Statement-opinion"> They did more things
together</Turn>
<Turn Id="utt6" Speaker="A" DA-Type="Acknowledge"> Uh-huh</Turn>
Figure 2: Excerpt of dialogue from the SWITCHBOARD corpus
1997). An excerpt of dialogue from the SWITCH-
BOARD corpus can be seen in Figure 2.
3.2 AMITI
?
ES
The AMITI
?
ES project (Hardy et al, 2005) collected
1000 English human-human dialogues from Euro-
pean GE call centres. These calls are of an in-
formation seeking or transactional type, in which
customers interact with their financial accounts
by phone to check balances, make payments and
report lost credit cards. The resulting data has
been sanitised, to replace identifying features such
as names, addresses and account numbers with
generic information (?John Doe?, ?1 The Street?)
and the corpus is annotated with DAs using XDML,
combining slight variant of the 42-class DAMSL
(Hardy et al, 2002) with domain specific seman-
tic information such as account numbers and credit
card details (Hardy et al, 2003).
The most frequent tag in the AMITI
?
ES corpus
is Influence-on-listener=?Information-request?,
which occurs 20% of the time. For this corpus, the
average pair-wise kappa score of .59 was signifi-
cantly lower than the SWITCHBOARD corpus. For
the major categories (questions, answers), average
pair-wise kappa scores were around .70. Again,
according to the work of Carletta et al (1997), a
minimum kappa score of 0.67 is required to draw
tentative conclusions. An excerpt of dialogue
from the AMITI
?
ES corpus can be seen in Figure 3.
4 DA Classification
In this section we briefly describe our approach to
DA classification, based solely on intra-utterance
features. A key aspect of the approach is the se-
lection of the word n-grams to use as cue phrases.
Samuel et al (1999) investigate a series of different
statistical criteria for use in automatically selecting
cue phrases, but we use a criterion of predictivity,
described below, which is one that Samuel et al
(1999) do not consider.
4.1 Cue Phrase Selection
For our experiments, the word n-grams used as po-
tential cue phrases during classification are com-
puted from the training data. All word n-grams of
length 1?4 within the data are considered as can-
didates. The phrases chosen as cue phrases are
selected principally using a criterion of predictiv-
ity, which is the extent to which the presence of
a certain n-gram in an utterance is predictive of it
having a certain dialogue act category. For an n-
gram n and dialogue act d, this corresponds to the
conditional probability: P (d | n), a value that can
be straightforwardly computed. For each n-gram,
we are interested in its maximal predictivity, i.e.
the highest predictivity value found for it with any
DA category. This set of n-grams is then reduced
by applying thresholds of predictivity and occur-
rence, i.e. eliminating any n-gram whose maxi-
mal predictivity is below some minimum require-
ment, or whose maximal number of occurrences
in any category falls below some threshold value.
This thresholding removes some low frequency,
high predictivity n-grams that skew classification
performance. The n-grams that remain are identi-
fied as our cue phrases. The threshold values that
are used in all experiments were arrived at empiri-
cally, using a validation set to automatically set the
threshold levels independently of the test data, as
described in Webb et al (2005b).
4.2 Using Cue Phrases in Classification
To classify an utterance, we identify all the word
n-grams it contains, and determine which of these
has the highest predictivity of some dialogue act
category (i.e. is performing as some cue). If mul-
tiple cue phrases share the same maximal predic-
tivity, but predict different categories, we select the
979
<Turn Id="2.1" Speaker="Operator" Info-level="Communication-mgt"
Conventional="Opening">good morning customer services sam speaking</Turn>
<Turn Id="3.1" Speaker="Customer" Info-level="Communication-mgt"
Conventional="Opening">erm good morning</Turn>
<Turn Id="3.2" Speaker="Customer" Info-level="Task"
Forward-function="Explanation">erm I was away for about two months and i came back
and my card i don?t know whether i have lost it or it is stolen</Turn>
<Turn Id="4.1" Speaker="Operator" Understanding="Backchannel"
Response-to="T3.2">right okay</Turn>
<Turn Id="4.2" Speaker="Operator" Info-level="Task"
Influence-on-listener="Info-request-explicit">can you confirm your name
for me please</Turn>
Figure 3: Excerpt of dialogue from the AMITI
?
ES GE corpus
DA for the phrase with the highest frequency. If the
combination of predicitivity and occurrence count
is insufficient to determine a single DA, then a ran-
dom choice is made amongst the remaining can-
didate DAs. If no cue phrases are present, then a
default tag is assigned, corresponding to the most
frequent tag within the training corpus.
Our best reported figures on the 202k utterance
SWITCHBOARD corpus are a cross-validated score
of 69.09%, with a single high score of 71.29%,
which compares very favourably with the (not
cross-validated) 71% reported in Stolcke et al
(2000) for the same corpus. We also presented in-
formation that shows that adding a sequence model
of DA progressions - an n-gram model of DAs -
results in no significant increase in performance
(Webb et al, 2005a). This is surprising consid-
ering that Stolcke et al (2000) report their best fig-
ures when combining a HMM model of the words
inside utterances with a tri-gram model of the Di-
alogue Act sequence, as in the work of Reithinger
and Klesen (1997). When Stolcke et al (2000) add
the sequence model to the HMM language model, it
adds around 20% points to the final accuracy score
over the SWITCHBOARD data.
However, our observation is confirmed by both
Serafin and Eugenio (2004) and Ries (March
1999). On the basis of this result, we hypothe-
sise that our cues are highly predictive of dialogue
structure, and that much dialogue processing may
take place at a very shallow level.
5 Cross-Domain Classification
The central purpose of this paper is to examine
the use of automatically extracted cues to tag data
other than the corpus from which they are de-
rived. The hypothesis we wish to test is that these
cues are sufficiently general to work as a classi-
fication device on a corpus from a different do-
main, even containing interactions of a different
conversational style. Specifically, SWITCHBOARD
is an open domain spoken human-human conver-
sational corpus and we have shown state-of-the-art
tagging performance over this data using our cue-
based model. We now wish to see how well these
same cues perform over the AMITI
?
ES GE corpus
of spoken task-based dialogues. The dialogues in
the AMITI
?
ES GE corpus are far more goal directed,
and contain domain specific cues not found in the
general conversational SWITCHBOARD corpus.
The ability to apply cues extracted from one cor-
pus to new data is an interesting challenge. It could
confirm work which indicates the prominence of
such word cues in language (Hirschberg and Lit-
man, 1993). A tag mechanism that can operate
across domains presents a range of benefits - for
example it can be used to annotate or partially an-
notate new data collections.
5.1 DA Mapping
Cross-corpus classification would be simplified if
both corpora were annotated with identical DA tax-
onomies. In actuality, the SWITCHBOARD corpus
and the AMITI
?
ES GE corpus are annotated with
variants of the DAMSL DA annotation scheme. In
the SWITCHBOARD corpus, the hierarchical nature
of the DAMSL schema has been flattened and clus-
tered, to produce 42 major classes. In the AMITI
?
ES
GE corpus, the dialogue level schema has been left
largely untouched from the DAMSL original. In or-
980
der to be able to compare automatic classification
performance across the two corpora, a mapping is
required between the 42-class schema of SWITCH-
BOARD and the DAMSL-like XDML schema of
the AMITI
?
ES GE corpus. In their work, Juraf-
sky et al (1998) include such a mapping between
SWITCHBOARD and DAMSL that covers approxi-
mately 80% of the labels in the SWITCHBOARD
corpus. We have adapted this slightly to cover
minor differences between the XDML used in the
AMITI
?
ES GE corpus and the original DAMSL, al-
though this leaves us with two issues that we need
to address.
First there are differences in granularity on both
sides. Importantly, in many instances we may
identify the most salient role of the utterance, but
miss modification information which may make
little interpretative difference. For example, mark-
up in the AMITI
?
ES GE corpus makes the distinc-
tion between <Forward-function=?Assert?> and
<Forward-function=?Reassert?>, whereas mark-
up in the SWITCHBOARD corpus ignores such a
distinction, and annotates both as type <Forward-
function=?Assert?> - although the SWITCH-
BOARD corpus captures the difference between as-
sertions that are opinions, and those that are not,
whereas the original DAMSL does not capture this
distinction. To address this mismatch we create
a set of super classes by relating the annotations
of SWITCHBOARD-DAMSL and the AMITI
?
ES GE-
XDML corpora at the most salient level, according
to the mapping contained in Jurafsky et al (1998).
Whilst the majority of tags have a one-to-one cor-
relation, there are elements of both the Forward-
Looking Function (see Figure 4) and Backward-
Looking Function (Figure 5) that require mapping
in both directions.
Secondly, there are a number of AMITI
?
ES GE
tags that we know a-priori we have little or no
chance to recognise. For example, the AMITI
?
ES
GE corpus is meticulously annotated to include that
certain utterances are perceived as answers to prior
utterances. Our approach to DA tagging is purely
intra-utterance, taking no account of the wider dis-
course structure, so will not recognise these dis-
tinctions. Although such a model of discourse
structure should be trivial, based for example on
an adjacency pair approach, this will be evaluated
further in future work.
5.2 Evaluation Criteria
These issues require that we create two evaluation
criteria for our subsequent experiments - strict and
lenient. With strict evaluation, we are required to
match all elements of the AMITI
?
ES GE corpus an-
notation - despite knowing in advance that this is
not possible for a range of utterances. We use our
strict evaluation criteria to establish a lower bound
of performance for our classifier. Our lenient ap-
proach is a back-off model, where we require that
we correctly identify the most critical part of the
multi-part annotation - those that are identified as
the most salient.
We?ll use the dialogue excerpt shown in Fig-
ure 3 as an example of how these two scor-
ing mechanisms work. The first utterance (2.1)
is marked as <Info-level=?Communication-mgt?
Conventional=?Opening?>. This has a one-to-
one correlation with the SWITCHBOARD-DAMSL
tag <conventional-opening>. In the case of this
example, and in all instances in the AMITI
?
ES
GE corpus, utterances are marked as <Info-
level=?Task?>, unless they are from a small
set of exceptions, including openings, closings
or backchannels, that are annotated as <Info-
level=?Communication-mgt?>. Once an utterance
is tagged as one of these exceptions, we know to
change the <Info-level> assignment accordingly.
There will be no difference between our strict
and lenient evaluation models for the interpreta-
tion of this utterance. The same is true for the sec-
ond (3.1) utterance annotation, which has a direct
correlation with SWITCHBOARD-DAMSL annota-
tions. However, the fourth utterance (4.1) includes
a <Response-to=?T3.2?> annotation that we will
not be able to identify using our intra-utterance
model. This utterance will be judged correct us-
ing the lenient model, and incorrect using the strict
metric.
The third utterance (3.2) is marked as
<Forward-function=?Explanation?>. Using
the Forward-function map shown in Figure
4, we see that this maps to the super class
<Forward-function=?Assert?>, that in turn maps
to the SWITCHBOARD-DAMSL tags <statement-
non-opinion> and <statement-opinion>. This
means that any utterance identified by the
presence of a cue phrase as either <statement-
non-opinion> or <statement-opinion> will in
fact be tagged as <Info-level=?Task? Forward-
function=?Assert?>. Whilst this annotation
981
Forward? function = ?Assert?
Forward? function = ?Reassert?
Forward? function = ?Explanation?
Forward? function = ?Rexplanation?
Forward? function = ?Expression?
?
?
?
?
?
?
?
Forward? function = ?Assert?
{
statement? non? opinion
statement? opinion
Figure 4: Partial Forward-Looking Function mapping table (XDML } SUPERCLASS { SWITCHBOARD-
DAMSL)
Inf ? on? list = ?Info? req ? explicit?
Inf ? on? list = ?Info? req ? implicit?
Inf ? on? list = ?Conf ? req ? implicit?
Inf ? on? list = ?Conf ? req ? explicit?
?
?
?
?
?
Influence? on? listener =
?Information? request?
?
?
?
?
?
?
?
?
?
?
?
yes? no? question
wh? questions
open? questions
or ? clause
declarative? question
tag ? question
Figure 5: Partial Backward-Looking Function mapping table (XDML } SUPERCLASS { SWITCHBOARD-
DAMSL)
captures the salient behaviour of the utterance,
it is not an exact match to the original AMITI
?
ES
GE corpus annotation and correspondingly when
scoring the lenient model will score this as correct,
whereas the exact model will not.
The same is true with the fifth utterance
(4.2), annotated in this case as <Influence-on-
listener=?Info-request-explicit?>. A classifier
trained over the SWITCHBOARD corpus would
identify this (through the mapping see in Fig-
ure 5) as <Influence-on-listener=?Information-
request?>, which would be scored as correct using
the lenient measure, and incorrect using the exact.
5.3 Classification Experiments
The results of our experiments are summarised
in Figure 6. First, to establish our baseline tag-
ging performance, we take the classification al-
gorithm outlined earlier in Section 4, and apply
it to the SWITCHBOARD corpus for both training
and testing, replicating the work reported in Webb
et al (2005a). In this case, 198,000 utterances
are used for training, and a separate 4,000 utter-
ances are used for testing. We achieve a cross-
validated score of 69.6%, where the most frequent
tag in SWITCHBOARD, <statement-non-opinion>,
occurs 36% of the time. This is a confirmation
of the work reported in Webb et al (2005a), and
demonstrates that this simple model works excep-
tionally well for this task.
For the first of the new experiments to test our
hypothesis, we substitute the AMITI
?
ES GE corpus
for the SWITCHBOARD corpus in both steps - train-
ing and testing - which will give us an upper bound
of performance of this particular classification al-
gorithm over this data. In this experiment, we used
10% of the corpus for testing - giving us a total of
27,000 utterances for training and 3,000 utterances
for testing. For all experiments where AMITI
?
ES GE
data is used as a test corpus, both strict and lenient
scoring will be used. Strict scoring sets a lower
bound for this exercise, and should be greater than
chance, which corresponds to the distribution of
the most frequent DA tag in each corpus. For strict
scoring, where we are required to match all the el-
ements of the AMITI
?
ES GE XDML tag, we score
65.9% accuracy in this experiment. For lenient,
where we must match only the most salient fea-
tures, we score 70.8% accuracy. Whilst there is
no direct comparison to other work on this cor-
pus, Hardy et al (2005) show partial results for DA
classification on this task, looking only at a few
major classes, and achieve a score of 86%. How-
ever, this includes only the 5 most frequent DA
categories, and considers utterances shorter than a
certain number of words.
Finally, we attempt cross-domain classification:
First, we train our classifier using SWITCHBOARD
data, and test using AMITI
?
ES GE data. We recorded
a strict evaluation score of 39.8% tagging accu-
racy. Using the lenient score, we achieve around
55.7% accuracy. This can be considered a very
good result, given the lower bound score of 20%
- that is the count of the most frequent tag.
982
Training
corpus
Training
utterances
Testing
corpus
Test
utterances
Common
tag (%)
Lenient
score
Strict
score
SWITCHBOARD 198,000 SWITCHBOARD 4,000 36% n/a 69%
AMITI
?
ES GE 27,000 AMITI
?
ES GE 3,000 20% 70.8% 65.9%
SWITCHBOARD 198,000 AMITI
?
ES GE 30,000 20% 55.7% 39.8%
AMITI
?
ES GE 27,000 SWITCHBOARD 198,000 36% 48.3% 40%
SWITCHBOARD 27,000 AMITI
?
ES GE 3,000 20% 53.2% 38%
Figure 6: Experimental Results
Then we apply the classification in reverse -
we train on AMITI
?
ES GE data, and test on the
SWITCHBOARD corpus, using all available data
in both cases. Using the strict evaluation met-
ric, we achieve a score of 40.0%, and a lenient
score of 48.3%. This compares to a baseline of
36%, so is not a drastic improvement over our
lower bound. Some inspection of the data in-
formed us that the AMITI
?
ES GE data did not include
many <backchannel> utterances, so subsequently
most of these instances in the SWITCHBOARD cor-
pus were missed by our classifier. By changing
the default tag to be <backchannel>, rather than
the most frequent tag for the training corpus, we
achieve a performance gain to 47.7% with strict
scoring, and 56.0% with the lenient metric.
For the last experiment, we also wanted to study
the effect of limiting the training data on cross-
domain classification, by reducing the SWITCH-
BOARD data to match that of the AMITI
?
ES GE train-
ing set - that is, to use only 27,000 utterances of
the SWITCHBOARD corpus as training data to ex-
tract cues, which are then applied both to itself (for
reference), and to the AMITI
?
ES GE corpus. On a
related note, part of the work conducted in Webb
et al (2005a) studied the impact of different size
training models when classifying SWITCHBOARD
data, using models of 4k, 50k and 202k utterances.
Whilst substantial improvement was seen when
moving from 4k utterances to 50k utterances, the
subsequent increase from 50k to 202k utterances
had a negligible impact on classification accuracy.
With the reduced SWITCHBOARD training set, we
score 53.2% with the lenient metric, and 38% with
strict, indicating that the reduction is size of the
training data has some effect on classification ac-
curacy.
6 Discussion, Future Work
We have shown that the cues extracted from the
SWITCHBOARD corpus can be used to success-
fully classify utterances in the AMITI
?
ES GE cor-
pus. We achieve almost 80% of the upper baseline
performance over the AMITI
?
ES GE corpus, when
judged using our lenient scoring mechanism - scor-
ing 55.7% using the cross-domain cues, compared
to the 70.8% when using in-domain cues. When
using the strict measure we still achieve around
60% of the upper bound performance, both results
being a substantial improvement over the baseline
measure of 20%, corresponding to the most fre-
quent tag in the AMITI
?
ES GE corpus. This is a sig-
nificant result, which confirms the idea that cues
can be sufficiently general across domains to be
used in classification.
However, whilst the experiment using SWITCH-
BOARD corpus derived cues to classify AMITI
?
ES
GE data works well, the same is not true in re-
verse. There are two possible explanations for this
result. It could be related to the size of data avail-
able for training, although our experiments in this
area seem to suggest otherwise and so we believe
that the composition of the training data is a more
crucial element. Although the DA distribution in
the SWITCHBOARD corpus is uneven, there is suf-
ficient data for the major classes to be effective on
new data that also contains these classes. Although
the AMITI
?
ES GE contains a lot of questions and
statements, there is very little of the other signif-
icant categories, such as <backchannels>, a key
DA in the SWITCHBOARD corpus and conversa-
tional speech in general. Correspondingly, the cues
derived from the AMITI
?
ES GE data perform well
on a selection of utterances in the SWITCHBOARD
corpus, but very poorly on others. We want to per-
form an in-depth error analysis to see if the errors
we obtain in classification accuracy are consistent.
We can also compare our list of automatically de-
rived cues phrases, particularly those that overlap
between the two corpora, to those reported in prior
literature. It might be interesting to see if more
complex models, derived using state-of-the art ma-
983
chine learning approaches, could demonstrate sim-
ilar portability - i.e is it the simplicity of our model
that allows for the observed robust portability?
Finally, we wish to combine SWITCHBOARD
and AMITI
?
ES corpora in the cue learning phase, to
see how this effects classification, and apply the
results to a range of other corpora, including the
ICSI-MRDA corpus (Shriberg et al, 2004).
References
Bunt, Harry. 1994. Context and dialogue control.
THINK, 3:19?31.
Carletta, J. C., A. Isard, S. Isard, J. Kowtko,
G. Doherty-Sneddon, and A. Anderson. 1997. The
Reliability of a Dialogue Structure Coding Scheme.
Computational Linguistics, 23:13?31.
Cohen, P. R. and C. R. Perrault. 1979. Elements of a
plan based theory of speech acts. Cognitive Science,
3.
Core, Mark G. and James Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, MIT, Cambridge, MA.
Grosz, Barbara and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. Compu-
tational Linguistics, 19(3).
Hardy, Hilda, Kirk Baker, Laurence Devillers, Lori
Lamel, Sophie Rosset, Tomek Strzalkowski, Cristian
Ursu, and Nick Webb. 2002. Multi-layered dialogue
annotation for automated multilingual customer ser-
vice. In Proceedings of the ISLE workshop on Dia-
logue Tagging for Multimodal Human Computer In-
teraction, Edinburgh.
Hardy, H., K. Baker, H. Bonneau-Maynard, L. Dev-
illers, S. Rosset, and T. Strzalkowski. 2003. Se-
mantic and dialogic annotation for automated mul-
tilingual customer service. In Eurospeech, Geneva,
Switzerland.
Hardy, H., A. Biermann, R. Bryce Inouye, A. McKen-
zie, T. Strzalkowski, C. Ursu, N. Webb, and M. Wu.
2005. The AMITIES System: Data-Driven Tech-
niques for Automated Dialogue. Speech Communi-
cation, 48:354?373.
Hirschberg, Julia and Diane Litman. 1993. Empiri-
cal Studies on the Disambiguation of Cue Phrases.
Computational Linguistics, 19(3):501?530.
Jurafsky, Daniel, Rebecca Bates, Noah Coccaro,
Rachel Martin, Marie Meteer, Klaus Ries, Eliza-
beth Shriberg, Andreas Stolcke, Paul Taylor, and
Carol Van Ess-Dykema. 1998. Switchboad dis-
course language modeling project final report. Re-
search Note 30, Center for Language and Speech
Processing, Johns Hopkins University, Baltimore.
Prasad, Rashmi and Marilyn Walker. 2002. Train-
ing a Dialogue Act Tagger for Humna-Human and
Human-Computer Travel Dialogues. In Proceedings
of the 3rd SIGdial workshop on Discourse and Dia-
logue, Philadelphia, Pennsylvania.
Reithinger, Norbert and Martin Klesen. 1997. Dia-
logue act classification using language models. In
Proceedings of EuroSpeech-97.
Ries, Klaus. March, 1999. Hmm and neural network
based speech act classification. In Proceddings of
the IEEE Conference on Acoustics, Speech and Sig-
nal Processing, volume 1, pages 497?500, Phoenix,
AZ.
Samuel, Ken, Sandra Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-
based learning. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Montreal.
Samuel, Ken, Sandra Carberry, and K. Vijay-Shanker.
1999. Automatically selecting useful phrases for di-
alogue act tagging. In Proceedings of the Fourth
Conference of the Pacific Association for Computa-
tional Linguistics, Waterloo, Ontario, Canada.
Serafin, Riccardo and Barbara Di Eugenio. 2004.
FLSA: Extending Latent Semantic Analysis with
features for dialogue act classification. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, Barcelona, Spain.
Shriberg, E., R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Special Interest Group on Dis-
course and Dialogue (SIGdial), Boston, USA.
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue act model-
ing for automatic tagging and recognition of conver-
sational speech. In Computational Linguistics 26(3),
339?373.
Tur, Gokhan, Umit Guz, and Dilek Hakkani-Tur. 2006.
Model Adaptation for Dialogue Act Tagging. In
IEEE Spoken Language Technology Workshop.
Webb, Nick, Mark Hepple, and Yorick Wilks.
2005a. Dialogue Act Classification Based on Intra-
Utterance Features. In Proceedings of the AAAI
Workshop on Spoken Language Understanding.
Webb, Nick, Mark Hepple, and Yorick Wilks. 2005b.
Empirical determination of thresholds for optimal di-
alogue act classification. In Proceedings of the Ninth
Workshop on the Semantics and Pragmatics of Dia-
logue.
984
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038?1046,
Beijing, August 2010
Modeling Socio-Cultural Phenomena in Discourse 
Tomek Strzalkowski1,2, George Aaron Broadwell1, Jennifer Stromer-Galley1, Samira Shaikh1, Sarah Taylor3 and Nick Webb1 1ILS Institute, University at Albany, SUNY 2IPI, Polish Academy of Sciences 3Lockheed Martin Corporation tomek@albany.edu 
 
Abstract In this paper, we describe a novel ap-proach to computational modeling and understanding of social and cul-tural phenomena in multi-party dia-logues. We developed a two-tier ap-proach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred.  1. Introduction We investigate the language dynamics in small group interactions across various set-tings. Our focus in this paper is on English online chat conversations; however, the mod-els we are developing are more universal and applicable to other conversational situations: informal face-to-face interactions, formal meetings, moderated discussions, as well as interactions conducted in languages other than English, e.g., Urdu and Mandarin.  Multi-party online conversations are particu-larly interesting because they become a per-vasive form of communication within virtual communities, ubiquitous across all age groups. In particular, a great amount of communica-tion online occurs in virtual chat-rooms, typi-cally conducted using a highly informal text dialect. At the same time, the reduced-cue environment of online interaction necessitates more explicit linguistic devices to convey social and cultural nuances than is typical in face-to-face or even voice conversations.  Our objective is to develop computational models of how certain social phenomena such as leadership, power, and conflict are signaled and reflected in language through the choice of lexical, syntactic, semantic and conversa-tional forms by discourse participants. In this 
paper we report the results of an initial phase of our work during which we constructed a prototype system called DSARMD-1 (De-tecting Social Actions and Roles in Multi-party Dialogue). Given a representative seg-ment of multiparty task-oriented dialogue, DSARMD-1 automatically classifies all dis-course participants by the degree to which they deploy selected social language uses, such as topic control, task control, involve-ment, and disagreement. These are the mid-level social phenomena, which are de-ployed by discourse participants in order to achieve or assert higher-level social con-structs, including leadership. In this work we adopted a two-tier empirical approach where social language uses are modeled through observable linguistic features that can be automatically extracted from dialogue. The high-level social constructs are then inferred from a combination of language uses attrib-uted to each discourse participant; for exam-ple, a high degree of influence and a high de-gree of involvement by the same person may indicate a leadership role. In this paper we limit our discussion to the first tier only: how to effectively model and classify social lan-guage uses in multi-party dialogue.  2. Related Research Issues related to linguistic manifestation of social phenomena have not been systemati-cally researched before in computational lin-guistics; indeed, most of the effort thus far was directed towards the communicative di-mension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dia-logue focus on information content and structural components (Blaylock, 2002; Car-berry & Lambert, 1999; Stolcke, et al, 2000) in dialogue; few take into account the effects that speech acts may have upon the social 
1038
roles of discourse participants. Also relevant is research on modeling sequences of dia-logue acts ? to predict the next one (Samuel et al 1998; Ji & Bilmes, 2006 inter alia) ? or to map them onto subsequences or ?dialogue games? (Carlson 1983; Levin et al, 1998), which are attempts to formalize participants? roles in conversation (e.g., Linell, 1990; Poe-sio &?Mikheev, 1998; Field et al, 2008). There is a body of literature in anthropology, linguistics, sociology, and communication on the relationship between language and power, as well as other social phenomena, e.g., con-flict, leadership; however, existing ap-proaches typically look at language use in situations where the social relationships are known, rather than using language predic-tively. For example, conversational analysis (Sacks et al, 1974) is concerned with the structure of interaction: turn-taking, when interruptions occur, how repairs are signaled, but not what they reveal about the speakers. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and Scollon, 2001; Agar, 1994) with few systematic studies attempting to ex-plore the reverse, i.e., what the linguistic phenomena tell us about social norms and behaviors.  3. Data & Annotation Our initial focus has been on on-line chat dialogues. While chat data is plentiful on-line, its adaptation for research purposes presents a number of challenges that include users? pri-vacy issues on the one hand, and their com-plete anonymity on the other. Furthermore, most data that may be obtained from public chat-rooms is of limited value for the type of modeling tasks we are interested in due to its high-level of noise, lack of focus, and rapidly shifting, chaotic nature, which makes any longitudinal studies virtually impossible. To derive complex models of conversational be-havior, we need the interaction to be reasona-bly focused on a task and/or social objectives within a group. Few data collections exist covering multiparty dialogue, and even fewer with on-line chat. Moreover, the few collections that exist were built primarily for the purpose of training dialogue act tagging and similar linguistic phenomena; few if any of these corpora are 
suitable for deriving pragmatic models of conversation, including socio-linguistic phe-nomena. Existing resources include a multi-person meeting corpus ICSI-MRDA and the AMI Meeting Corpus (Carletta, 2007), which contains 100 hours of meetings cap-tured using synchronized recording devices. Still, all of these resources look at spoken language rather than on-line chat. There is a parallel interest in the online chat environ-ment, although the development of useful re-sources has progressed less. Some corpora exist such as the NPS Internet chat corpus (Forsyth and Martell, 2007), which has been hand-anonymized and labeled with part-of-speech tags and dialogue act labels. The StrikeCom corpus (Twitchell et al, 2007) consists of 32 multi-person chat dialogues between players of a strategic game, where in 50% of the dialogues one participant has been asked to behave ?deceptively?. It is thus more typical that those interested in the study of Internet chat compile their own corpus on an as needed basis, e.g., Wu et al (2002), Khan et al (2002), Kim et al (2007).  Driven by the need to obtain a suitable dataset we designed a series of experiments in which recruited subjects were invited to participate in a series of on-line chat sessions in a spe-cially designed secure chat-room. The ex-periments were carefully designed around topics, tasks, and games for the participants to engage in so that appropriate types of behav-ior, e.g., disagreement, power play, persuasion, etc. may emerge spontaneously. These ex-periments and the resulting corpus have been described elsewhere (Shaikh et al, 2010b), and we refer the reader to this source. Ulti-mately a corpus of 50 hours of English chat dialogue was collected comprising more than 20,000 turns and 120,000 words. In addition we also assembled a corpus of 20 hours of Urdu chat.  A subset of English language dataset has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics (which are essentially the most persistent lo-cal topics). Although full details of these an-notations are impossible to explain within the scope of this article, we briefly describe them below. Annotated datasets were used to de-velop and train automatic modules that detect and classify social uses of language in dis-course. It is important to note that the annota-
1039
tion has been developed to support the objec-tives of our project and does not necessarily conform to other similar annotation systems used in the past.  ? Communicative links. In a multi-party dia-logue an utterance may be directed towards a specific participant, a subgroup of par-ticipants or to everyone.  ? Dialogue Acts. We developed a hierarchy of 15 dialogue acts for annotating the func-tional aspect of the utterance in discussion.  The tagset we adopted is based on DAMSL (Allen & Core, 1997) and SWBD (Jurafsky et al, 1997), but compressed to 15 tags tuned significantly towards dialogue prag-matics and away from more surface char-acteristics of utterances (Shaikh et al, 2010a).  ? Local topics. Local topics are defined as nouns or noun phrases introduced into dis-course that are subsequently mentioned again via repetition, synonym, or pronoun.  ? Topic reference polarity. Some topics, which we call meso-topics, persist through a number of turns in conversation. A selec-tion of meso-topics is closely associated with the task in which the discourse par-ticipants are engaged. Meso-topics can be distinguished from the local topics because the speakers often make polarized state-ments about them.  4. Socio-linguistic Phenomena We are interested in modeling the social phe-nomena of Leadership and Power in discourse. These high-level phenomena (or Social Roles, SR) will be detected and attributed to dis-course participants based on their deployment of selected Language Uses (LU) in multi-party dialogue. Language Uses are mid-level socio-linguistic devices that link linguistic components deployed in discourse (from lexical to pragmatic) to social con-structs obtaining for and between the partici-pants. The language uses that we are currently studying are Agenda Control, Disagreement, and Involvement (Broadwell et al, 2010). Our research so far is focused on the analysis of English-language synchronous chat, and we are looking for correlations between vari-ous metrics that can be used to detect LU in multiparty dialogue. We expect that some of these correlations may be culturally specific or language-specific, as we move into the 
analysis of Urdu and Mandarin discourse in the next phase of this project. 4.1 Agenda Control in Dialogue Agenda Control is defined as efforts by a member or members of the group to advance the group?s task or goal. This is a complex LU that we will model along two dimensions: (1) Topic Control and (2) Task Control. Topic Control refers to attempts by any discourse participants to impose the topic of conversa-tion. Task Control, on the other hand, is an effort by some members of the group to de-fine the group?s project or goal and/or steer the group towards that goal. We believe that both behaviors can be detected using scalar measures per participant based on certain linguistic features of their utterances. For example, one hypothesis is that topic control is indicated by the rate of local topic introductions (LTI) per participant (Givon, 1983). Local topics may be defined quite simply as noun phrases introduced into dis-course, which are subsequently mentioned again via repetition, synonym, pronoun, or other form of co-reference. Thus, one meas-ure of topic control is the number of local topics introduced by each participant as per-centage of all local topics in a discourse.  Using an LTI index we can construct asser-tions about topic control in a discourse. For example, suppose the following information is discovered about the speaker LE in a multi-party discussion dialogue-11 where 90 local topics are identified: 1. LE introduces 23/90 (25.6%) of local top-ics in this dialogue. 2. The mean rate of local topic introductions is this dialogue is 14.29%, and standard deviation is 8.01. 3. LE is in the top quintile of participants for introducing new local topics We can now claim the following, with a de-gree of confidence (to be determined): TopicControlLTI (LE, 5, dialogue-1) We read this as follows: speaker LE exerts the highest degree of topic control in dialogue-1. Of course, LTI is just one source of evidence and we developed other metrics to comple-ment it. We mention three of them here:                                                 1 Dialogue-1 refers to an actual dataset of 90-minute chat among 7 participants, covering approximately 700 turns. The task is to select a candidate for a job given a set of resumes. 
1040
? SMT Index. This is a measure of topic con-trol suggested in (Givon, 1983) and it is based on subsequent mentions of already introduced local topics. Speakers who in-troduce topics that are discussed at length by the group tend to control the topic of the discussion. The subsequent mentions of lo-cal topics (SMT) index calculates the per-centage of second and subsequent refer-ences to the local topics, by repetition, synonym, or pronoun, relative to the speakers who introduced them.  ? Cite Score. This index measures the extent to which other participants discuss topics introduced by that speaker. The difference between SMT and CiteScore is that the lat-ter reflect to what degree a speaker?s efforts to control the topic are assented to by other participants in a conversation. ? TL Index (TL). This index stipulates that more influential speakers take longer turns than those who are less influential. The TL index is defined as the average number of words per turn for each speaker. Turn length also reflects the extent to which other participants are willing to ?yield the floor? in conversation. Like LTI, all the above indices are mapped into a degree of topic control, based on quin-tiles in normal distribution (Table 1).   
 
LTI SMT CS TL AVG LE 5 5 5 5 5.00 JR 4 4 4 3 3.75 KI 4 3 3 1 2.75 KN 3 5 4 4 4.00 KA 2 2 2 4 2.50 CS 2 2 2 2 2.00 JY 1 1 1 2 1.25 Table 1: Topic Control distribution in dialogue-1. Each row represents a speaker in the group (LE, JR, etc.). Columns show indices used, with degrees per speaker on 5-point scale based on quintiles in normal distribu-tion, and the average value. Ideally, all the above indices (and others yet to be defined) should predict the same out-come, i.e., for each dialogue participant they should assign the same degree of topic control, relative to other speakers. This is not always the case, and where the indices divert in their predictions, our level of confidence in the generated claims decreases. We are currently 
working on how these different metrics cor-relate to each other and how they should be weighted to maximize accuracy of making Topic Control claims. Nonetheless, we can already output a Topic Control map (shown in Table 1) that captures a sense of internal so-cial dynamics within the group.  The other aspect of Agenda Control phe-nomenon is Task Control. It is defined as an effort to determine the group's goal and/or steer the group towards that goal. Unlike Topic Control, which is imposed by influenc-ing the subject of conversation, Task Control is gained by directing other participants to perform certain tasks or accept certain opin-ions. Consequently, Task Control is detected by observing the usage of certain dialogue acts, including Action-Directive, Agree-Accept, Disagree-Reject, and related categories. Here again, we define several in-dices that allow us to compute a degree of Task Control in dialogue for each participant: ? Directive Index (DI). The participant who directs others is attempting to control the course of the task that the group is per-forming. We count the number of directives, i.e., utterances classified as Ac-tion-Directive, made by each participant as a percentage of all directives in discourse. ? Directed Topic Shift Index (DTSI). When a participant who controls the task offers a directive on the task, then the topic of con-versation shifts. In order to detect this con-dition, we calculate the ratio of coincidence of directive dialogue acts by each partici-pant with topic shifts following them.  ? Process Management index (PMI). Another measure of Task Control is the proportion of turns each participant has that explicitly address the problem solving process. This includes utterances that involve coordinat-ing the activities of the participants, plan-ning the order of activities, etc. These fall into the category of Task (or Process) Management in most DA tagging systems.  ? Process Management Success Index (PMSI). This index measures the degree of success by each speaker at controlling the task. A credit is given to the speaker whose suggested curse of action is supported by other speakers for each response that sup-ports the suggestion. Conversely, a credit is taken away for each response that rejects or 
1041
qualifies the suggestion. PMSI is computed as distribution of task management credits among the participants over all dialogue utterances classified as Task/Process Man-agement. 2 As an example, let?s consider the following information computed for the PMI index over dialogue-1:  1. Dialogue-1 contains 246 utterances classi-fied as Task/Process Management rather than doing the task. 2. Speaker KI makes 65 of these utterances for a PMI of 26.4%. 3. Mean PMI for participants is 14.3%; 80th percentile is >21.2%. PMI for KI is in the top quintile for all participants. Based on this evidence we may claim (with yet to be determined confidence) that: TaskControlPMI(KI, 5, dialogue-1) This may be read as follows: speaker KI ex-erts the highest degree of Task Control in dialogue-1. We note that Task Control and Topic Control do not coincide in this dis-course, at least based on the PMI index. Other index values for Task Control may be com-puted and tabulated in a way similar to LTI in Table 1. We omit these here due to space limitations. 4.2 Disagreement in Dialogue Disagreement is another language use that correlates with speaker?s power and leader-ship. There are two ways in which disagree-ment is realized: expressive disagreement and topical disagreement (Stromer-Galley, 2007; Price, 2002). Both can be detected using sca-lar measures applied to subsets of participants, typically any two participants. In addition, we can also measure for each participant the rate with which he or she generates disagreement (with any and all other speakers). Expressive Disagreement is normally understood at the level of dialogue acts, i.e., when discourse participants make explicit utterances of dis-agreement, disapproval, or rejection in re-sponse to a prior speaker?s utterance. Here is an example (KI and KA are two speakers in a multiparty dialogue in which participants                                                 2 The exact structure of the credit function is still being deter-mined experimentally. For example, more credit may be given to first supporting response and less for subsequent responses; more credit may be given for unprompted suggestions than for those that were responding to questions from others. 
discuss candidates for a youth counselor job): KA: CARLA... women are always better with kids KI: That?s not true! KI: Men can be good with kids too While such exchanges are vivid examples of expressive disagreement, we are interested in more sustained phenomenon where two speakers repeatedly disagree, thus revealing a social relationship between them. Therefore, one measure of Expressive Disagreement that we consider is the number of Disagree-Reject dialogue acts between any two speakers as a percentage of all utterances exchanged be-tween these two speakers. This becomes a basis for the Disagree-Reject Index (DRX). In dialogue-1 we have: 1. Speakers KI and KA have 47 turns between them. Among these there are 8 turns classi-fied as Disagree-Reject, for the DRX of 15.7%. 2. The mean DRX for speakers who make any Disagree-Reject utterances is 9.5%. The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates. While Expressive Disagreement is based on the use of more overt linguistic devices, Topical Disagreement is defined as a differ-ence in referential valence in utterances (statements, opinions, questions, etc.) made on a topic. Referential valence of an utterance is determined by the type of statement made about the topic in question, which can be positive (+), negative (?), or neutral (0). A positive statement is one in favor of (express advocacy) or in support of (supporting infor-mation) the topic being discussed. A negative statement is one that is against or negative on 
1042
the topic being discussed. A neutral statement is one that does not indicate the speaker?s po-sition on the topic. Here is an example of op-posing polarity statements about the same topic in discourse: Sp-1: I like that he mentions ?Volunteerism and Leadership? Sp-2: but if they?re looking for someone who is experienced then I?d cross him off Detecting topical disagreement in discourse is more complicated because its strength may vary from one topic in a conversation to the next. A reasonable approach is thus to meas-ure the degree of disagreement between two speakers on one topic first, and then extrapo-late over the entire discourse. Accordingly, our measure of topical disagreement is valua-tion differential between any two speakers as expressed in their utterances about a topic. Here, the topic (or an ?issue?) is understood more narrowly than the local topic defined in the previous section (as used in Topic Control, for example), and may be assumed to cover only the most persistent local topics, i.e., top-ics with the largest number of references in dialogue, or what we call the meso-topics. For example, in a discussion of job applicants, each of the applicants becomes a meso-topic, and there may be additional meso-topics pre-sent, such as qualifications required, etc.  The resulting Topical Disagreement Metric (TDM) captures the degree to which any two speakers advocate the opposite sides of a meso-topic. TDM is computed as an average of P-valuation differential for one speaker (advocating for a meso-topic) and (?P)-valuation differential for the other speaker (advocating against the meso-topic).  Using TDM we can construct claims related to disagreement in a given multiparty dia-logue of sufficient duration (exactly what constitutes a sufficient duration is still being researched). Below is an example based on a 90-minute chat dialogue-1 about several job candidates for a youth counselor. The discus-sion involved 7 participants, including KI and KA. Topical disagreement is measured on 5 points scale (corresponding to quintiles in normal distribution): TpDisAgreeTDM(KI,KA,?Carla?,4,dialogue-1) This may be read as follows: speakers KI and KA topically disagree to degree 4 on topic [job candidate] ?Carla? in dialogue-1. In or-
der to calculate this we compute the value of TDM index between these two speakers. We find that KA makes 30% of all positive utter-ances made by anyone about Carla (40), while KI makes 45% of all negative utterances against Carla. This places these two speakers in the top quintiles in the ?for Carla? polarity distribution and ?against Carla? distribution, respectively. Taking into account any oppos-ing polarity statements made by KA against Carla and any statements made by KI for Carla, we calculate the level of topical dis-agreement between KA and KI to be 4 on the 1-5 scale. TDM allows us to compute topical disagree-ment between any two speakers in a discourse, which may also be represented in a 2-dimensional table revealing another inter-esting aspect of internal group dynamics.  4.3 Involvement in Dialogue The third type of social language use that we discuss in this paper is Involvement. In-volvement is defined as a degree of engage-ment or participation in the discussion of a group. It is an important element of leader-ship, although its importance is expected to differ between cultures; in Western cultures, high involvement and influence (topic control) often correlates with group leadership. In order to measure Involvement we designed several indices based on turn characteristics for each speaker. Four of the indices are briefly explained below:  ? The NP index (NPI) is a measure of gross informational content contributed by each speaker in discourse. NPI counts the ratio of third-person nouns and pronouns used by a speaker to the total number of nouns and pronouns in the discourse.  ? The Turn index (TI) is a measure of inter-actional frequency; it counts the ratio of turns per participant to the total number of turns in the discourse.  ? The Topic Chain Index (TCI) counts the degree to which participants discuss of the most persistent topics. In order to calculate TCI values, we define a topic chains for all local topics. We compute frequency of mentions of these longest topics for each participant.  ? The Allotopicality Index (ATP) counts the number of mentions of local topics that were introduced by other participants. An 
1043
ATP value is the proportion of a speaker's allotopical mentions, i.e., excluding ?self-citations?, to all allotopical mentions in a discourse.  As an example, we may consider the follow-ing situation in dialogue-1: 1. Dialogue-1 contains 796 third person nouns and pronouns, excluding mentions of participants? names. 2. Speaker JR uses 180 nouns and pronouns for an NPI of 22.6%.  3. The median NPI is 14.3%; JR are in the upper quintile of participants (> 19.9%). From the above evidence we can draw the following claim: InvolvementNPI(JR, 5, dialogue-1) This may be read as: speaker JR is the most involved participant in dialogue-1. As with other language uses, multiple indices for Involvement can be combined into a 2-dimensional map capturing the group in-ternal dynamics.  5. Implementation & Evaluation We developed a prototype automated DSARMD system that comprises a series of modules that create automated annotation of the source dialogue for all the language ele-ments discussed above, including communi-cative links, dialogue acts, local/meso topics, and polarity. Automatically annotated dia-logue is then used to generate language use degree claims. In order to evaluate accuracy of the automated process we conducted a pre-liminary evaluation comparing the LU claims generated from automatically annotated data to the claims generated from manually coded dialogues. Below we briefly describe the methodology and metrics used. Each language use is asserted per a partici-pant in a discourse (or per each pair of par-ticipants, e.g., for Disagreement) on a 5-point ?strength? scale. This can be represented as an ordered sequence LUX(d1, d2, ? dn), where LU is the language use being asserted, X is the index used, di is the degree of LU attrib-uted to speaker i. This assignment is therefore a 5-way classification of all discourse par-ticipants and its correctness is measured by dividing the number of correct assignments by the total number of elements to be classi-fied, which gives the micro-averaged preci-sion. The accuracy metric is computed with 
several variants as follows: 1. Strict mapping: each complete match is counted as 1; all mismatches are counted as 0. For example, the outputs LUX (5,4,3,2,1) and LUX (4,5,3,1,1) produce two exact matches (for the third and the last speaker) for a precision of 0.4. 2. Weighted mapping: since each degree value di in LUX(d1, d2, ? dn) represents a quintile in normal distribution, we consider the po-sition of the value within the quintile. If two mismatched values are less than ? quintile apart we assign a partial credit (currently 0.5). 3. Highest ? Rest: we measure accuracy with which the highest LU degree (but not nec-essarily the same degree) is assigned to the right speaker vs. any other score. This re-sults in binary classification of scores. The sequences in (1) produce 0.6 match score. 4. High ? Low: An alternative binary classifi-cation where scores 5 and 4 are considered High, while the remaining scores are con-sidered Low. Under this metric, the se-quences in (1) match with 100% precision. The process of automatic assignment of lan-guage uses derived from automatically proc-essed dialogues was evaluated against the control set of assignments based on hu-man-annotated data. In order to obtain a reli-able ?ground truth?, each test dialogue was annotated by at least three human coders (linguistics and communication graduate stu-dents, trained). Since human annotation was done at the linguistic component level, a strict inter-annotator agreement was not required; instead, we were interested whether in each case a comparable statistical distribution of the corresponding LU index was obtained. Annotations that produced index distributions dissimilar from the majority were eliminated. Automated dialogue processing involved the following modules: ? Local topics detection identifies first men-tions by tracking occurrences of noun phrases. Subsequent mentions are identi-fied using fairly simple pronoun resolution (based mostly on lexical features), with Wordnet used to identify synonyms, etc. ? Meso-topics are identified as longest-chain local topics. Their polarity is assessed at the utterance level by noting presence of positive or negative cue words and phrases. ? Dialogue acts are tagged based on presence 
1044
of certain cue phrases derived from a train-ing corpus (Webb et al, 2008).  ? Communicative links are mapped by com-puting inter-utterance similarity based on n-gram overlap. Preliminary evaluation results are shown in Tables 3-5 with average performance over 3 chat sessions (approx 4.5 hours) involving three groups of speakers and different tasks (job candidates, political issues). Topic Con-trol and Involvement tables show average accuracy per index. For example, the LTI in-dex, computed over automatically extracted local topics, produces Topic Control assign-ments with the average precision of 80% when compared to assignments derived from human-annotated data using the strict accu-racy metric. However, automated prediction of Involvement based on NPI index is far less reliable, although we can still pick the most involved speaker with 67% accuracy. We omit the indices based on turn length (TL) and turn count (TI) because their values are trivially computed. At this time we do not combine indices into a single LU prediction. Addi-tional experiments are needed to determine how much each of these indices contributes to LU prediction. Topic  Control LTI? SMT? CS?Strict? 0.80? 0.40? 0.40?Weighted? 0.90? 0.53? 0.53?Highest?Rest? 0.90? 0.67? 0.67?High?Low? 1.00? 0.84? 0.90?Table 3: Topic Control LU assignment performance averages of selected indices over a subset of data cov-ering three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, 7 per session). 
Involvement NPI? TCI? ATP?Strict? 0.31? 0.42? 0.39?Weighted? 0.46? 0.49? 0.42?Highest?Rest? 0.67? 0.77? 0.68?High?Low? 0.58? 0.74? 0.48?Table 4: Involvement LU assignment performance av-erages for selected indices over the same subset of data as in Table 3. Topical Disagreement performance is shown in Table 5. We calculated precision and recall of assigning a correct degree of disagreement 
to each pair of speakers who are members of a group. Precision and recall averages are then computed over all meso-topics identified in the test dataset, which consists of three separate 90-minute dialogues involving 7, 5 and 7 speakers, respectively. Our calculation includes the cases where different sets of meso-topics were identified by the system and by the human coder. A strict mapping of levels of disagreement between speakers is hard to compute accurately; however, finding the speakers who disagree the most, or the least, is significantly more robust. 
Topical Disagreement Prec.? Recall?Strict? 0.33? 0.32?Weighted? 0.54? 0.54?Highest?Rest? 0.89? 0.85?High?Low? 0.77? 0.73?Table 5: Topical Disagreement LU assignment per-formance averages over 13 meso-topics discussed in three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, and 7 per session). 6. Conclusion In this paper we presented a preliminary design for modeling certain types of social phenomena in multi-party on-line dialogues. Initial, limited-scale evaluation indicates that the model can be effectively automated. Much work lies ahead, including large scale evaluation, testing index stability and resilience to NL component level error. Current performance of the system is based on only preliminary versions of linguistic modules (topic extraction, polarity assignments, etc.) which perform at only 70-80% accuracy, so these need to be improved as well. Research on Urdu and Chinese dialogues is just starting. Acknowledgements This research was funded by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the U.S. Army Research Lab. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI or the U.S. Government. 
1045
References Agar, Michael. 1994. Language Shock, Under-standing the Culture of Conversation. Quill, William Morrow, New York. Allen, J. M. Core. 1997. Draft of DAMSL: Dialog Act Markup in Several Layers. www.cs. roch-ester.edu/research/cisd/resources/damsl/  Anderson, A., et al 1991. The HCRC Map Task Corpus. Language and Speech 34(4), 351--366. Austin, J. L. 1962. How to do Things with Words. Clarendon Press, Oxford. Bird, Steven, et al 2009. Natural Language Proc-essing with Python: Analyzing Text with the Natural Language Toolkit. O'Reilly Media.  Blaylock, Nate. 2002. Managing Communicative Intentions in Dialogue Using a Collaborative Problem-Solving Model. Technical Report 774, University of Rochester, CS Dept. Broadwell, G. A et al (2010). Social Phenomena and Language Use. ILS Technical report. Carberry, Sandra and Lynn Lambert. 1999. A Process Model for Recognizing Communicative Acts and Modeling Negotiation Dialogue. Computational Linguistics, 25(1), pp. 1-53. Carletta, J. (2007). Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation Journal 41(2): 181-190 Carlson, Lauri. 1983. Dialogue Games: An Ap-proach to Discourse Analysis. D. Reidel. Eric N. Forsyth and Craig H. Martell. 2007. Lexi-cal and Discourse Analysis of Online Chat Dia-log. First IEEE International Conference on Semantic Computing (ICSC 2007), pp. 19-26. Field, D., et al 2008. Automatic Induction of Dia-logue Structure from the Companions Dialogue Corpus, 4th Int. Workshop on Human-Computer Conversation, Bellagio. Givon, Talmy. 1983. Topic continuity in discourse: A quantitative cross-language study. Amster-dam: John Benjamins.  Ivanovic, Edward. 2005. Dialogue Act Tagging for Instant Messaging Chat Sessions. In Proceed-ings of the ACL Student Research Workshop. 79?84. Ann Arbor, Michigan. Ji, Gang Jeff Bilmes. 2006. Backoff Model Train-ing using Partially Observed Data: Application to Dialog Act Tagging. HLT-NAACL Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-asca. 1997. Switchboard SWBD-DAMSL Shal-low-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/ manual.august1.html Jurafsky, D., et al 1997. Automatic detection of discourse structure for speech recognition and understanding. IEEE Workshop on Speech Recognition and Understanding, Santa Barbara. Khan, Faisal M., et al 2002. Mining Chat-room Conversations for Social and Semantic Interac-
tions. Computer Science and Engineering, Le-high University. Kim, Jihie., et al 2007. An Intelligent Discus-sion-Bot for Guiding Student Interactions in Threaded Discussions. AAAI Spring Sympo-sium on Interaction Challenges for Intelligent Assistants Levin, L., et al (1998). A discourse coding scheme for conversational Spanish. Interna-tional Conference on Speech and Language Processing. Levin, L., et al (2003). Domain specific speech acts for spoken language translation. 4th SIG-dial Workshop on Discourse and Dialogue. Linell, Per. 1990. The power of dialogue dynamics. In Ivana Markov?a and Klaus Foppa, editors, The Dynamics of Dialogue. Harvester, 147?177. Poesio, Massimo and Andrei Mikheev. 1998. The predictive power of game structure in dialogue act recognition. International Conference on Speech and Language Processing (ICSLP-98). Price, V., Capella, J. N., & Nir, L. (2002). Does disagreement contribute to more deliberative opinion? Political Communication, 19, 95-112. Sacks, H. and Schegloff, E., Jefferson, G. 1974. A simplest systematic for the organization of turn-taking for conversation. In: Language 50(4), 696-735.  Samuel, K. et al 1998. Dialogue Act Tagging with Transformation-Based Learning. 36th Annual Meeting of the ACL. Scollon, Ron and Suzanne W. Scollon. 2001. Intercultural Communication, A Discourse Ap-proach. Blackwell Publishing, Second Edition. Searle, J. R. 1969. Speech Acts. Cambridge Uni-versity Press, London-New York. Shaikh, S. et al 2010. DSARMD Annotation Guidelines, V. 2.5. ILS Technical Report.  Shaikh S. et al 2010. MPC: A Multi-Party Chat Corpus for Modeling Social Phenomena in Discourse, Proc. LREC-2010, Malta. Stolcke, Andreas et al 2000. Dialogue Act Mod-eling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguis-tics, 26(3). Stromer-Galley, J. 2007. Measuring deliberation?s content: A coding scheme. Journal of Public Deliberation, 3(1).  Tianhao Wu, et al 2002. Posting Act Tagging Us-ing Transformation-Based Learning. Founda-tions of Data Mining and Discovery, IEEE In-ternational Conference on Data Mining Twitchell, Douglas P., Jay F. Nunamaker Jr., and Judee K. Burgoon. 2004. Using Speech Act Profiling for Deception Detection. Intelligence and Security Informatics, LNCS, Vol. 3073 Webb, N., T. Liu, M. Hepple and Y. Wilks. 2008. Cross-Domain Dialogue Act Tagging. 6th In-ternational Conference on Language Resources and Evaluation (LREC-2008), Marrakech. 
1046
Coling 2010: Poster Volume, pages 1310?1317,
Beijing, August 2010
Automatic Extraction of Cue Phrases for
Cross-Corpus Dialogue Act Classification
Nick Webb and Michael Ferguson
ILS Institute, SUNY Albany
nwebb@albany.edu, ferguson@cs.albany.edu
Abstract
In this paper, we present an investiga-
tion into the use of cue phrases as a ba-
sis for dialogue act classification. We de-
fine what we mean by cue phrases, and de-
scribe how we extract them from a manu-
ally labelled corpus of dialogue. We de-
scribe one method of evaluating the use-
fulness of such cue phrases, by applying
them directly as a classifier to unseen ut-
terances. Once we have extracted cue
phrases from one corpus, we determine
if these phrases are general in nature, by
applying them directly as a classification
mechanism to a different corpus to that
from which they were extracted. Finally,
we experiment with increasingly restric-
tive methods for selecting cue phrases,
and demonstrate that there are a small
number of core cue phrases that are use-
ful for dialogue act classification.
1 Motivation
In this paper we present a recent investigation into
the role of linguistic cues in dialogue act (DA)
classification. Dialogue acts (Bunt, 1994) are an-
notations over segments of dialogue that charac-
terise the function of those segments. Linguistic
cues, which can take many forms including lexi-
cal and syntactic structures, are features that can
serve as useful indicators of discourse structure
(Hirschberg and Litman, 1993; Grosz and Sidner,
1986). In prior work, several researchers have
shown that cue phrases can be a powerful fea-
ture for DA classification (Samuel et al, 1999;
Webb et al, 2005a). Webb and Liu (2008) have
previously shown that cue phrases automatically
extracted from one corpus can be used to clas-
sify utterances from a new corpus. We take this
approach and apply it to two established corpora
with manually encoded dialogue act annotations,
to investigate both the existence and the useful-
ness of cue phrases shared between the two cor-
pora.
2 Related Work
In parallel with the increased availability of man-
ually annotated dialogue corpora there has been
a proliferation of literature detailing dialogue act
labelling as a classification task. Prior work de-
scribes the selection of features from the corpus
(including word n-grams, cue phrases, syntactic
structures, dialogue history and prosodic cues)
which are then passed to some machine learn-
ing algorithm. Most studies have concentrated on
a single corpus, and optimised feature selection
and learning algorithm accordingly. In this work
we focus on two corpora, Switchboard and ICSI-
MRDA, and discuss prior classification efforts re-
lating to these two corpora.
2.1 Switchboard Corpus
The Switchboard corpus contains a large number
of approximately 5-minute conversations between
two people who are unknown to each other, who
were asked to converse about a range of every-
day topics with little or no constraint. The DA an-
notated portion of the Switchboard corpus (Juraf-
sky et al, 1997) consists of 1155 annotated con-
versations, containing some 225,000 utterances,
of which we use 200,000 utterances, the rest be-
ing held out for separate experiments. The dia-
logues are annotated with a non-hierarchical vari-
ant of the DAMSL annotation scheme (Core et al,
1999). The resulting Switchboard-DAMSL an-
notation was a set of more than 220 distinct la-
bels. To obtain enough data per class for statis-
tical modelling purposes, a clustered tag set was
devised, which distinguishes 42 mutually exclu-
1310
sive DA types. Classification over the Switch-
board corpus has been demonstrated using Deci-
sion Trees (Verbree et al, 2006), Memory-Based
Learning (Rotaru, 2002) and Hidden Markov
Models (HMM) (Stolcke et al, 2000). The work
of Stolcke et al (2000) is often cited as the best
performing, achieving a classification accuracy of
71% over the 42 labels, although there is no cross-
validation of these results. The approach of Stol-
cke et al (2000) combines HMM modelling of ut-
terances with a tri-gram model of DA sequences.
Webb et al (2005a) report a slightly lower cross-
validated score (of 69%) containing an individ-
ual classification high of 72%, using an intra-
utterance, cue-based classification model.
2.2 ICSI-MRDA Corpus
Like the Switchboard corpus, the ICSI Meeting
Room DA (MRDA) corpus (Shriberg et al, 2004)
was annotated using a variant of the DAMSL tag-
set, similar but not identical to the Switchboard-
DAMSL annotation. The differences (and a
translation between the two sets) can be seen in
Shriberg et al (2004). The underlying domain
of the dialogues in the ICSI-MRDA corpus was
that of multi-party meetings, with multiple partic-
ipants discussing an agenda of items in a struc-
tured meeting. This application required the in-
troduction of new tags specifically for this sce-
nario, such as a label introduced to indicate when
an utterance was used to take control of the meet-
ing. The ICSI-MRDA corpus comprises 75 nat-
urally occurring meetings, each around an hour
in length. The section of the corpus we use con-
sists of around 105,000 utterances. For each utter-
ance in the corpus, one general tag was assigned,
with zero or more additional specific tags. Ex-
cluding non-labelled cases, there are 11 general
tags and 39 specific tags resulting in 1,260 unique
dialogue acts used in the annotation. As with the
Switchboard corpus, processing steps were intro-
duced that compressed the number of unique DAs
to 55. In later work, the dimensionality was fur-
ther reduced, resulting in a subset of just 5 labels.
Over the ICSI-MRDA corpus, we also see DA
classification efforts using Decision Trees (Ver-
bree et al, 2006) and Memory-Based Learning
(Lendvai and Geertzen, 2007), in addition to
Graph Models (Ji and Bilmes, 2006) and Maxi-
mum Entropy (Ang et al, 2005). Comparatively
few approaches have been applied to the 55-label
annotated corpus, with most choosing to focus on
the 5-label clustering, presumably for the result-
ing increase in score. When Ji and Bilmes (2005)
apply a Graph Model to the 55 category corpus,
they achieve a classification accuracy of 66%.
However, when they apply the exact same method
to the 5-label corpus (Ji and Bilmes, 2006), clas-
sification accuracy is boosted to 81%. The best
reported classification score on the the 5-label ver-
sion of the corpus is reported by Verbree et al
(2006), who achieve 89% classification accuracy
by modelling the words of the utterance, the DA
history and some orthographic information (such
as the presence of question marks).
It remains very difficult to directly compare ap-
proaches, even when applied to the same corpus,
so cross-corpora comparisons must be carefully
considered. There are issues of the DA label set
used, the labels considered and those ignored, the
pre-processing of the corpus, the use of ortho-
graphic information, or prosody and so on. What
seems clear is that there are no obvious leading
contender for algorithm best suited to the DA clas-
sification task. Instead, we focus on the features
used for DA classification.
3 Automatic Cue Extraction
When examining prior approaches, we noticed
that they used a range of different features for the
DA classification task, including lexical, syntac-
tic, prosodic and dialogue context features. Most
classifiers used some lexical features (the words
in the utterances under consideration), frequently
employing some kind of Hidden Markov Mod-
elling to every utterance (Levin et al, 2003; Stol-
cke et al, 2000; Reithinger and Klesen, 1997), a
technique popular in speech processing. We were
inspired by the work of Samuel et al (1999), who
instead of modelling entire utterances, extract sig-
nificant cue phrases from the VerbMobil corpus
of dialogues. We use a method for cue extraction
unused by Samuel et al (1999).
What defines a good cue phrase? We are look-
ing for words or phrases in a corpus that regularly
co-occur with individual dialogue acts. We use
1311
the term predictivity to indicate how predictive a
phrase is of a particular DA. We want to select
phrases that are highly indicative, and so concern
ourselves with the highest predictivity of a par-
ticular cue phrase. We call this score the maxi-
mal predictivity. There are several other thresh-
olds that should also be apparent. First, below
some maximal predictivity score, we assume that
phrases will no longer be discriminative enough to
be useful for labelling DAs. Second, the number
of occurrences of each phrase in the corpus as a
whole is important. In their experiments, Samuel
et al (1999) constructed all n-grams of lengths
1 through 3 from the corpus, and then applied a
range of measures which pruned the n-gram list
until only candidate cue phrases remained. In or-
der to test the effectiveness of these automatically
acquired cue phrases, Samuel et al (1999) passed
them as features to a machine learning method, in
their case transformation-based learning.
More formally, we can describe our criteria,
predictivity, for selecting cue phrases from the set
of all possible cue phrases in the following way.
The predictivity of phrase c for DA d is the condi-
tional probability P (d|c), where:
P (d|c) = #(c&d)#(c)
We represent the set of all possible cue phrases
(all n-grams length 1?4 from the corpus) as C,
so given c ? C : c represents some possible cue
phrase. Similarly, D is the set of all dialogue act
labels, and d ? D : d represents some dialogue
act label. Therefore #(c) is the count of (pos-
sible) cue phrase c in corpus, and #(c&d) is the
count of occurrences of phrase c in utterances with
dialogue act d in the training data. The maximal
predictivity of a cue phrase c, written as mp(c), is
defined as:
mp(c) = max
d?D
P (d|c)
In their experiments, Samuel et al (1999) also
experimented with conditional probability, using
P (c|d), or the probability of some phrase occur-
ring given some Dialogue Act. For our exper-
iments, the word n-grams used as potential cue
phrases during are automatically extracted from
training data. All word n-grams of length 1?
4 within the data are considered as candidates.
The maximal predictivity of each cue phrase can
be computed directly from the corpus. We can
use this value as one threshold for pruning po-
tential cue phrases from our model. Removing
n-grams below some predictivity threshold will
improve the compactness of the model produced.
Another reasonable threshold would appear to be
the frequency count of each potential cue phrase.
Phrases which have a low frequency score are
likely to have very high predictivity scores, pos-
sibly skewing the model as a whole. For example,
any potential cue phrase which occurs only once
will de-facto have a 100% predictivity score. We
can use a minimal count value (t#) and minimal
predictivity thresholds (tmp) to prune the set C?
of ?useful? cue phrases derived from the training
data, as defined by:
C? = {c ? C |mp(c) ? tmp ?#(c) ? t#}
The n-grams that remain after this thresholding
process are those we identify as cue phrases. For
our initial experiments, we used a predictivity of
30% and a frequency of 2 as our thresholds for cue
extraction.
4 Cue-Based DA Classification
Having defined our mechanism to extract cue
phrases from a corpus, we need some way to
evaluate their effectiveness. Samuel et al (1999)
passed their cue phrases as a feature to a machine
learning method. We chose instead a method
where the cue phrases extracted from a corpus
could be used directly as a method of classifi-
cation. If our extracted cues are indeed reliable
predictors of dialogue acts, then a classifier that
uses these cues directly should perform reason-
ably well. If, on the other hand, this mechanism
did not work, it would not necessarily mean that
our cue phrases are not effective, only that we
need to pass them to a subsequent machine learn-
ing process as others had done. The benefit of our
direct classification approach is that it is very fast
to evaluate, and gives us immediate feedback as
to the possible effectiveness of our automatically
extracted cue phrases.
1312
The predictivity of a cue phrase can be ex-
ploited directly in a simple model of Dialogue
Act classification. We can extract potential cue
phrases as described in Section 3. The resulting
cue phrases selected using our measure of predic-
tivity are then used directly to classify unseen ut-
terances in the following manner. We identify all
the potential cue phrases a target utterance con-
tains, and determine which has the highest predic-
tivity of some dialogue act category, then assign
that category. Given the notation we define earlier,
we can obtain the DA predicted by a particular cue
(dp(c)) by:
dp(c) = argmax
d?D
P (d|c)
If multiple cue phrases share the same maxi-
mal predictivity, but predict different categories,
we select the DA category for the phrase which
has the higher number of occurrences (that is, the
n-gram with the highest frequency). If the combi-
nation of predictivity and occurrence count is in-
sufficient to determine a single DA, then a random
choice is made amongst the remaining candidate
DAs. If ng(u) defines the set of ngrams of length
1..4 in utterance u, and C?u is the set of n-grams in
the utterance u that are also in the threshold model
C? then C?u is defined as:
C?u = ng(u) ? C?
Given our thresholds, the mpu(u) (the utter-
ance maximal prediction, or mp value for the
highest scoring cue in utterance u) is defined as:
mpu(u) = max
c?C?u
mp(c)
The maximally predictive cues of an utterance
(mpcu(u)) are:
mpcu(u) = {c ? C?u |mp(c) = mpu(u)}
Then the maximal cue of utterance (mcu(u)),
i.e. one of its maximally predictive cues that has a
maximal count (from within that set), is:
mcu(u) = argmax
c?mpcu(u)
#(c)
Finally, for our classification model, dpu(u) ut-
terance DA prediction ? the DA predicted by
model for utterance u, is defined as:
dpu(u) = dp(mcu(u))
If no cue phrases are present in the utterance
under consideration, then a default tag is assigned.
To this basic model, we added three fur-
ther elaborations. The first used models sensi-
tive to utterance length. When examining the
ICSI-MRDA corpus, Ji and Bilmes (2006) found
that the mean length of <STATEMENT> utter-
ances was 8.60 words, <BACKCHANNEL> utter-
ances were 1.04 words, <PLACE-HOLDERS> ut-
terances were 1.31 words and <QUESTIONS> ut-
terances were 6.50 words. Taking this as a start
point, we grouped utterances into those of length
1 (i.e. short, or one word utterances), those with
lengths 2?4 (we call medium length utterances),
and those of length 5+ (the long length model, that
comprises everything else), and produced separate
cue-based models for each group.
Second, we introduced <start> and <finish>
tags to each utterance (independent of the calcula-
tion of utterance length), to capture position spe-
cific information for particular cues. For exam-
ple ?<start> okay? identifies the occurrence of
the word ?okay? as the first word in the utterance.
Finally, in the Switchboard annotation, there are
other markers dealing with various linguistic is-
sues, as outlined in Meteer (1995). A primary ex-
ample is the label <+>, which indicated the pres-
ence of overlapping speech. One approach to bet-
ter utilise this data is to ?reconnect? the divided
utterances, i.e. appending any utterance assigned
tag <+> to the last utterance by the same speaker.
We base the selection of these model elaborations
and the values for the parameters of frequency and
predictivity on prior research (cf. (Webb et al,
2005a; Webb et al, 2005b; Webb et al, 2005c)).
5 Cue-Based Classification Results
Ultimately, we want to compare classification
performance of a set of automatically extracted
cue phrases across the two corpora, Switchboard
and ICSI-MRDA. Both are annotated with sim-
ilar variants of the DAMSL annotation scheme,
1313
Condition Cue Source Cue Count Accuracy
(1) Switchboard training data 136,942 80.72%
(2) ICSI-MRDA training data 48,856 70.78%
(3) Intersection of Switchboard and ICSI-MRDA Training Data 25,053 72.34%
(4) As above, discard <STATEMENT> cue phrases 577 72.62%
(5) As above, retain only cue phrases containing <start> tags 242 72.52%
(6) As above, retain only cue phrases appearing in every training intersection 148 72.09%
Table 1: Switchboard Classification Results
but there are differences. For example, the
ICSI-MRDA corpus introduces several new la-
bels that do not exist in the Switchboard annota-
tion. Some labels in the Switchboard annotation
are clustered into a single corresponding label in
the ICSI-MRDA corpus, such as the two labels
from Switchboard, <STATEMENT-OPINION> and
<STATEMENT-NON-OPINION>, which are repre-
sented by a single label <STATEMENT> in the
ICSI-MRDA corpus. To facilitate cross-corpus
classification, we will cluster these labels as de-
scribed in Shriberg et al (2004). Of course, any
clustering of labels has an impact on classifier per-
formance, usually resulting in an increase. Webb
et al (2005c) indicate that clustering statement la-
bels in the Switchboard corpus should improve
performance by 8-10% percentage points.
5.1 Baseline Results
We need to establish baseline classification per-
formance for both corpora. Our baseline for this
classification task is to the most frequently oc-
curring label for all utterances. For a number
of dialogue corpora, the most frequently occur-
ring label is some sort of statement or asser-
tion, which is true for both the Switchboard and
ICSI-MRDA corpora, where <STATEMENT> is
the most frequent label. For the Switchboard
corpus, selecting this label results in 51.05% ac-
curacy. Remember that we are working with
a version of the Switchboard corpus where we
have clustered the original labels <STATEMENT-
OPINION> and <STATEMENT-NON-OPINION>
into a single label. In the original Switchboard
annotation, the most frequently occurring la-
bel is <STATEMENT-NON-OPINION>, which oc-
curs 36% of the time. Further analysis on the
Switchboard corpus by Webb et al (2005c) high-
lights that a significant number of <STATEMENT-
OPINION> utterances in Switchboard are mis-
labelled as <STATEMENT-NON-OPINION> by
human annotators. For the ICSI-MRDA corpus,
an accuracy of 31.77% is achieved by labelling
each utterance as <STATEMENT>.
Now we have established a simple baseline of
performance, we want to know how well our cue-
based classification method works applied to these
corpora, as an evaluation of how well our cue ex-
traction method works for each of these corpora.
We ran a 10-fold stratified cross-validation ex-
ercise (referred to as Condition (1) in Tables 1
and 2) using the cue-based extraction mecha-
nism described in Section 3, selecting cue phrases
from the training data (which averaged 180k ut-
terances for Switchboard, and 95k utterances for
ICSI-MRDA), resulting in an average of 135k cue
phrases from Switchboard and 50k cue phrases
from ICSI-MRDA. We then applied these cue-
based models to the held out test data as described
in Section 4, applying Switchboard extracted cue
phrases to Switchboard test data, and likewise
with the ICSI-MRDA data. This establishes the
best performance by our algorithm over these data
sets. For Switchboard, we achieve 80.72% accu-
racy, as predicted by the work reported in Webb et
al. (2005c). For ICSI-MRDA we obtain an accu-
racy of 58.14%. Remember, this model is applied
to the 55-label annotated ICSI-MRDA corpus.
Best reported classification accuracy for this cor-
pus is the 66% reported by Ji and Bilmes (2005),
using a graph-based model that models both ut-
terances and sequences of DA labels. For both
corpora, the cue-based model of classification out-
performs the baseline, using no dialogue context
whatsoever.
1314
Condition Cue Source Cue Count Accuracy
(1) ICSI-MRDA training data 48,856 58.14%
(2) Switchboard training data 136,942 47.07%
(3) Intersection of Switchboard and ICSI-MRDA Training Data 25,053 47.86%
(4) As above, discard <STATEMENT> cue phrases 577 48.05%
(5) As above, retain only cue phrases containing <start> tags 242 47.30%
(6) As above, retain only cue phrases appearing in every training intersection 148 46.34%
Table 2: ICSI-MRDA Classification Results
5.2 Cross-Corpus Results
The focus of our effort is not to maximise raw
performance over individual corpora, but to ex-
amine the effectiveness of our automatically ex-
tracted cue phrases, and one mechanism to do this
is to compare classification cross-corpora. If our
cue phrases are sufficiently general predictors of
DA labels across corpora, we believe that to be a
powerful claim for cue phrases as a DA classifica-
tion feature. Therefore, our next step was to take
the cue-phrases generated from each fold of the
Switchboard experiment, and apply them to the
held out test data from the corresponding fold of
the ICSI-MRDA experiment, and vice-versa. This
is a test to see how generally applicable are the cue
phrases extracted from each corpus.
When we take cues extracted from the Switch-
board corpus, and apply them to the held out por-
tion of the ICSI-MRDA corpus, we achieve an av-
erage classification accuracy (over our 10-folds)
of 47.07%. This score represents 81% of the ac-
curacy achieved by our prior result when ICSI-
MRDA test data is classified using ICSI-MRDA
training data. It also represents 71% of the best
published score on this corpus (Ji and Bilmes,
2005). When we classify held out Switchboard
test data with cue phrases extracted from the ICSI-
MRDA corpus, we achieve an average classifica-
tion accuracy of 70.78%, which corresponds to
88% of our best score on this corpus using Switch-
board training data. These results correspond to
Condition (2) in Tables 1 and 2.
These are very positive results for both di-
rections of classification, indicating that the cue
phrases we automatically extract from our corpora
are generally applicable as a feature for DA clas-
sification.
5.3 Cue Phrase Reduction
We have successfully shown that we can use cue
phrases extracted from one corpus to classify ut-
terances from a different corpus. We used an in-
clusive approach, using all cue phrases extracted
from the source corpus training data. Intuitively
however, we might expect to get comparable per-
formance by using only those cue phrases that ap-
pear in both corpora. For these intersection cue
phrases, we require a strict overlap. Once the cues
phrases are extracted from each individual train-
ing fold for each corpus, they are compared and
retained if and only if:
? the cue phrase itself is a direct match, includ-
ing any position specific label
? the DA the phrases predicts is a match
? the model number (as defined in Section 4) is
a match
For each fold of our cross-validation, we take
cues phrases extracted from the training data that
appear in both corpora, pruning out cue phrases
that only appear in one of the corpora. We then
retain only those cue phrases that meet these cri-
teria from both corpora for each specific fold, and
apply them to the held out test data from that fold
for each corpus.
Average classification performance for both
corpora rises very slightly in comparison to us-
ing all extracted cue phrases. These results can be
seen as Condition (3) in Tables 1 and 2. When ap-
plying the intersection cue phrases to the Switch-
board test data, we achieve an average classifica-
tion accuracy score of 72.34%. When we apply
the intersection cues to the ICSI-MRDA test data,
the average score is 47.86%. The average num-
ber of cue phrases that are used in this experiment
1315
(i.e that appear in all training folds for both cor-
pora, with matching model information) is around
25k. This represents 50% of the average num-
ber of cues extracted from the ICSI-MRDA cor-
pus, and only 19% of the average number of cue
phrases extracted from the Switchboard corpus.
We describe earlier that our default label as ap-
plied by our classifier when no cue phrase can be
found is the <STATEMENT> label, the most fre-
quent single label in both corpora. Given this,
we can safely remove cue phrases that predict
<STATEMENT> labels from our cue phrase set.
The absence of such cue phrases should have
no impact on our classification performance, but
should reduce our total number of cue phrases.
As can be seen in Condition (4) in Tables 1 and
2, this is indeed the case, with no statistical sig-
nificance between the results with and without
<STATEMENT> cue phrases. However, there is
a drop in the number of cue phrases. When we
remove all <STATEMENT> cue phrases from the
intersection of cue phrases, we are left with an av-
erage of 577 cue phrases.
Further analysis of classifier performance indi-
cates that a high percentage of actual labelling is
performed using a subset of even the cue phrases
extracted under Condition (4). We observed that
cue phrases that contain a <start> tag (as de-
scribed in Section 4) were used in the majority of
cases. Our final experiment was to extract, from
the 577 cue phrases, only those phrases that con-
tain the <start> tag. This reduced the average
number of cue phrases to 242. Classification per-
formance remains unaffected, scoring an average
of 72.52% for Switchboard and 47.30% for ICSI-
MRDA, as seen in Condition (5) in the results ta-
bles. We note that of those 242 phrases, 148 ap-
pear in the intersection of every training fold of
our 10-fold cross-validation. When we use only
those 148 cue phrases for classification, as seen
in Condition (6), average classification accuracy
remains the same; 72.09% for Switchboard, and
46.34% for ICSI-MRDA.
6 Conclusions
In this paper, we investigate a cue-based ap-
proach to DA classification, applied to two cor-
pora, Switchboard and ICSI-MRDA. We automat-
ically extracted cue phrases from both corpora,
and used them directly to classify unseen utter-
ances from the corresponding corpus, demonstrat-
ing that our automatically discovered cue phrases
are a sufficiently useful feature for this task.
We then explored the generality of our cue
phrases, by applying them directly as a classifier
to data from the alternate corpus. Whilst there
was some expected drop in performance, the clas-
sification accuracy for both experiments is good,
given such a small number of features and the sim-
ple design of the classifier. The result indicates
that cue phrases are a highly useful feature for
DA classification, and can be used to classify data
from new corpora, possibly as some part of some
quasi-automatic first annotation effort.
We experimented with reducing the set of cue
phrases, using increasingly restrictive measures
of retaining our automatically discovered cues.
We found that we did not have a drop in per-
formance compared to the cross-corpus classifica-
tion accuracy, even when the cue set is drastically
reduced (to 0.001% of the original Switchboard
cue phrases, and 0.003% of the ICSI-MRDA cue
phrases). This appears to be a strong indicator of
the discriminative power of some small number of
automatically discovered core cue phrases.
References
Ang, J., Y. Liu, and E. Shriberg. 2005. Automatic Di-
alog Act Segmentation and Classification in Multi-
party Meetings. In Proceedings of the International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 1061?1064, Philadelphia.
Bunt, H. 1994. Context and Dialogue Control.
THINK, 3:19?31.
Core, M., M. Ishizaki, J. Moore, and C. Nakatani.
1999. The Report of the Third Workshop of the Dis-
course Resource Initiative. Chiba University and
Kazusa Academia Hall.
Grosz, B. and C. Sidner. 1986. Attention, Intentions,
and the Structure of Discourse. Computational Lin-
guistics, 19(3).
Hirschberg, J. and D. Litman. 1993. Empirical Stud-
ies on the Disambiguation of Cue Phrases. Compu-
tational Linguistics, 19(3):501?530.
Ji, G. and J. Bilmes. 2005. Dialog Act Tagging Us-
ing Graphical Models. In Proceedings of the IEEE
1316
International Conference on Acoustics, Speech, and
Signal Processing, Philadelphia, PA.
Ji, G. and J. Bilmes. 2006. Backoff Model
Training using Partially Observed Data: Applica-
tion to Dialog Act Tagging. In Proceedings of
the Human Language Technology/ American chap-
ter of the Association for Computational Linguis-
tics(HLT/NAACL?06).
Jurafsky, D., R. Bates, N. Coccaro, R. Martin,
M. Meteer, K. Ries, E. Shriberg, A. Stolcke, P. Tay-
lor, and C. Van Ess-Dykema. 1997. Automatic De-
tection of Discourse Structure for Speech Recogni-
tion and Understanding. In Proceedings of the 1997
IEEE Workshop on Speech Recognition and Under-
standing, Santa Barbara.
Lendvai, P. and J. Geertzen. 2007. Token-Based
Chunking of Turn-Internal Dialogue Act Sequences.
In Proceedings of the 8th SIGDIAL Workshop on
Discourse and Dialogue, pages 174?181, Antwerp,
Belgium.
Levin, L., C. Langley, A. Lavie, D. Gates, and D. Wal-
lace. 2003. Domain Specific Speech Acts for Spo-
ken Language Translation. In Proceedings of 4th
SIGdial Workshop on Discourse and Dialogue.
Meteer, M. 1995. Dysfluency Annotation Stylebook
for the Switchboard Corpus. Working paper, Lin-
guistic Data Consortium.
Reithinger, N. and M. Klesen. 1997. Dialogue Act
Classification Using Language Models. In Proceed-
ings of EuroSpeech-97.
Rotaru, M. 2002. Dialog Act Tagging using Memory-
Based Learning. Term project, University of Pitts-
burgh.
Samuel, K., S. Carberry, and K. Vijay-Shanker. 1999.
Automatically Selecting Useful Phrases for Dia-
logue Act Tagging. In Proceedings of the Fourth
Conference of the Pacific Association for Computa-
tional Linguistics, Waterloo, Ontario, Canada.
Shriberg, E., R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI Meeting Recorder Dialog Act
(MRDA) Corpus. In Special Interest Group on Dis-
course and Dialogue (SIGdial), Boston, USA.
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue Act
Modeling for Automatic Tagging and Recognition
of Conversational Speech. In Computational Lin-
guistics 26(3), 339?373.
Verbree, D., R. Rienks, and D. Heylen. 2006. Dia-
logue Act Tagging using Smart Feature Selection;
Results on Multiple Corpora. Spoken Language
Technology Workshop, 2006. IEEE, pages 70?73.
Webb, N. and T. Liu. 2008. Investigating the Porta-
bility of Corpus-Derived Cue Phrases for Dialogue
Act Classification. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, United Kingdom.
Webb, N., M. Hepple, and Y. Wilks. 2005a. Dia-
logue Act Classification Based on Intra-Utterance
Features. In Proceedings of the AAAI Workshop
on Spoken Language Understanding, at the Twen-
tieth National Conference on Artificial Intelligence,
Pittsburgh, PA.
Webb, N., M. Hepple, and Y. Wilks. 2005b. Empir-
ical Determination of Thresholds for Optimal Di-
alogue Act Classification. In Proceedings of the
Ninth Workshop on the Semantics and Pragmatics
of Dialogue.
Webb, N., M. Hepple, and Y. Wilks. 2005c. Er-
ror Analysis of Dialogue Act Classification. In
Proceedings of the 8th International Conference on
Text, Speech and Dialogue, Carlsbad, Czech Repub-
lic.
1317
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43?48,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
VCA: An Experiment With A Multiparty Virtual Chat Agent 
Samira Shaikh1, Tomek Strzalkowski1, 2, Sarah Taylor3, Nick Webb1 1ILS Institute, University at Albany, State University of New York 2Institute of Computer Science, Polish Academy of Sciences 3Advancded Technology Office, Lockheed Martin IS&GS E-mail: ss578726@albany.edu, tomek@albany.edu   Abstract 
The purpose of this research was to advance the understanding of the behavior of small groups in online chat rooms. The research was conducted using Internet chat data collected through planned exercises with recruited par-ticipants. Analysis of the collected data led to construction of preliminary models of social behavior in online discourse. Some of these models, e.g., how to effectively change the topic of conversation, were subsequently im-plemented into an automated Virtual Chat Agent (VCA) prototype. VCA has been dem-onstrated to perform effectively and convinc-ingly in Internet conversation in multiparty chat environments.  1 Introduction Internet chat rooms provide a ready means of communication for people of most age groups these days. More often than not, these virtual chat rooms have multiple participants conversing on a wide variety of topics, using a highly infor-mal and free-form text dialect. An increasing use of virtual chat rooms by a variety of demograph-ics such as small children and impressionable youth leads to the risk of exploitation by deceit-ful individuals or organizations. Such risks might be reduced by presence of virtual chat agents that could keep conversations from progressing into certain topics by changing the topic of conversa-tion.  Our aim was to study the behavior of small groups of online chat participants and derive models of social phenomena that occur fre-quently in a virtual chat environment. We used the MPC chat corpus (Shaikh et al, 2010), which is 20 hours of multi-party chat data collected through a series of carefully designed online chat sessions. Chat data collected from public chat rooms, while easily available, presents signifi-cant concerns regarding its adaptability for our research use. Publicly available chat data is com-
pletely anonymous, has a high level of noise and lack of focus, in addition to engendering user privacy issues for its use in modeling tasks. The MPC corpus was used in (1) understanding how certain social behaviors are reflected in language and (2) building an automated chat agent that could effectively achieve certain (initially lim-ited) social objectives in the chat-room. A brief description of the MPC corpus and its relevant characteristics is given in Section 3 of this paper. One specific phenomenon of social behavior we wanted to model was an effective change of conversation topic, when a participant or a group of participants deliberately (if perhaps only tem-porarily) shift the discussion to a different, pos-sibly related topic. Both success and failure of these actions was of interest because the outcome depended upon the choice of utterance, the per-sons to whom it was addressed, their reaction, and the time when it was produced. Our analysis of the corpus for such phenomena led to the use of an annotation scheme that allows us to anno-tate for topic and focus change in conversation. We describe the annotation scheme used in Sec-tion 4.  We constructed an autonomous virtual chat agent (VCA) that could achieve initially limited social goals in a chat room with human partici-pants. We used a novel approach of exploiting the topic of conversation underway to search the web and find related topics that could be inserted in the conversation to change its flow. We tested the first prototype with the capability to opportu-nistically change to topic of conversation using a combination of linguistic, dialogic, and topic reference devices, which we observed effectively deployed by the most influential chat participants in the MPC corpus.  The VCA design, architec-ture and mode of operation are described in de-tail in Section 5 of this paper. 2 Related Work Automated dialogue agents such as the early ELIZA (Weizenbaum, 1966) and PARRY 
43
(Colby, 1974) could conduct a one-on-one ?con-versation? with a human using rules and pattern-matching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user, whether to perform a transaction (such as book-ing a flight or banking transactions) (Hardy et al, 2006) or for companionship (e.g., browsing of family photographs) (Wilks, 2010). Many of these systems were inspired by the challenge of the Turing Test or its more limited variants such as Loebner Prize.   Research in the field of developing a multi-user chat-room agent has been limited. This is some-what surprising because a multi-user setting makes the agent?s task of maintaining conversa-tion far less onerous than in one-on-one situa-tions. In a chat-room, with many users engaged in conversations, it is much easier for an agent to pass as just another user. Indeed, a skillfully de-signed agent may be able to influence an ongoing conversation. 3 MPC Chat Corpus The MPC chat corpus is a collection of 20 hours of chat sessions with multiple participants (on average 4), conversing for about 90 minutes in a secure online chat room. The topics of conversa-tion vary from free-flowing chat in the initial collection phase to allow participants to build comfortable a rapport with each other, to specific task-oriented dialogues in the latter phase; such as choosing the right candidate for a job inter-view from a list of given resumes. This corpus is suitable for our research purposes since the chat sessions were designed around enabling the so-cial phenomena we were interested in modeling. 4 Annotation Scheme We wished to annotate the data we collected to derive models from language use for social phe-nomena. These represent complex pragmatic concepts that are difficult to annotate directly, let alne detect automatically. Our approach was to build a multi-level annotation scheme.  In this paper we briefly outline our annotation scheme that consists of three layers: communica-                                                1 http://www.alicebot.org/aiml.html 2 http://www.daxtron.com/123start.htm?Cyn 
tive links, dialogue acts, and topic/focus changes. A more detailed description of the annotation scheme will be presented in a future publication.  4.1 Communicative Links Annotators are asked to mark each utterance in one of three categories ? utterance is addressed to a participant or a set of participants, it is in response to a specific prior utterance by another participant or it is a continuation of the partici-pant?s own prior utterance. By an utterance, we mean the set of words in a single turn by a par-ticipant. In multi-party chat, participants do not generally add addressing information in their utterances and it is often ambiguous to whom they are speaking. Communicative link annota-tion allows us to accurately map who is speaking to whom in the conversation, which is required for tracking social phenomena across partici-pants.  4.2 Dialogue Acts At this annotation level, we developed a hierar-chy of 20 dialogue acts, based loosely on DAMSL (Allen & Core, 1997) and SWBD-DAMSL (Jurafsky et al, 1997), but greatly re-duced and more tuned to dialogue pragmatics. For example, the utterance ?It is cold here today? may function as a Response-Answer when given in response to a question about the weather, and would act as an Assertion-Opinion if it is evalu-ated alone. The dialogue acts, thus augmented, become an important feature in modeling partici-pant behavior for our research purpose. A de-tailed description of the tags is beyond the scope of this paper. 4.3 Topic and Focus boundaries The flow of discussion in chat shifts quite rapidly from one topic to another. Furthermore, within each topic (e.g., music bands) the focus of conver-sation (e.g., dc for cutie) moves just as rapidly. We distinguish between topic and focus to accom-modate both broader thematic shifts and more narrow aspect changes of the topic being dis-cussed. For example, participants might discuss the topic of healthcare reform, by focusing on President Obama, and then switch the focus to some particulars of the reform, such as the ?public op-tion?. Similarly, topics may shift while the focus remains the same (e.g., moving on to Obama?s economic policies), although such changes are less common. Annotators typically marked the first mention of a substantive noun phrase as a topic or focus introduction. 
44
The effect of topic change is apparent when a subsequent utterance by another participant is about the same topic. This is a successful attempt at changing the topic. Shown in Figure 1 is an example of topic shift annotated in our data col-lection.  
 Figure 1. A topic change in dialogue, with three participants (AA, KA and KN)  We found this model of topic change fairly con-sistently exhibited, where the participants would ask an open question, in order to get other par-ticipants to respond to them, thereby changing the course of conversation. We collected all ut-terances marked topic shifts and focus shifts and created a set of templates from them.  These templates served as a model for the VCA to util-ize when creating a response.  Another model of behavior that we found as a consequence of topic change is topic sustain. This is an instance where the utterance is marked to be on the same topic as the one currently being discussed, for example, utterance 5 in Figure 1. These may be in the form of offering support or agreement with a previous utterance or asking a question about a new in-topic aspect. We gave our annotators a fair amount of lev-erage on how to label the topics and how to rec-ognize the focus. Our primary interest was in an accurate detection of topic/focus boundaries and shifts. Of the 14 sessions we selected from the MPC corpus, we selected 10 for annotation, with at least 3 annotators for each session. In Table 1 some of the overall statistics computed from this set are shown. We computed inter-annotator agreement on all three levels of our annotation, i.e. Communication Links, Dialogue Acts and 
Topic/Focus Shifts. Topic and Focus shifts had the highest inter-annotator agreement scores on different measures such as Krippendorf?s Alpha (Krippendorff, 1980) and Fliess? Kappa (Fliess, 1971). In Figure 2, we show inter-annotator agreement measures on Topic/Focus shift anno-tation for four of the annotated sessions. Krip-pendorff?s Alpha and Fleiss? Kappa measures show inter-annotator agreement on topic shift alone, and Conflated Krippendorff?s Alpha measures show the agreement when topic and focus are conflated as one category. With such high degree of agreement, we can reliably derive models of topic shift behavior from our anno-tated data.  Total Number of Sessions Annotated 10 Number of annotators per file 3 Total Utterances Annotated 4640 Average number of utterances per ses-sion ~520 Total topics identified per session 174 Total topic shifts identified per ses-sion 344 Table 1. Selected statistics from annotated data set  
 Figure 2. Inter-annotator agreement measures for Topic/Focus shifts 5 VCA Design A virtual chat agent is an automated program with the ability to respond to utterances in chat. Our VCA is distinctive in its ability to participate in multi-party chat and manage to steer the flow of conversation to a new topic. We exploit the dialogue mechanism underlying HITIQA (Small et al 2009) to drive the dialogue in VCA.  The topic as defined by the information con-tained in the participant?s utterance is used to mine outside data sources (e.g., a corpus, the web) in order to locate and learn additional in-formation about that topic. The objective is to identify some of the salient concepts that appear 
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
Krippendorff	 ?
's	 ?Alpha	 ?
Con?ated	 ?
Krippendorff	 ?
's	 ?Alpha	 ?
Fleiss	 ?'	 ?Kappa	 ?
AA 1: did anyone watch the morning talk shows today (MTP, for example)? KA 2: nope! AA 3: I missed them ? I was hoping someone else had. AA 4: My kids tell me the band you?re going to hear (dc for cutie) is great. (TOPIC: music bands, FOCUS: dc for cutie) KA 5: oh cool! Their lyrics are nice, I think. (TOPIC: music bands, FOCUS: dc for cutie) KA 6. what kind of music do you guys listen to? (TOPIC: music, FOCUS: none) KN 7: I don?t really have a favorite genre?.you on youtube right now? (TOPIC: music, FOCUS: youtube)  
45
associated with the topic, but are not directly mentioned in the utterance. Such associations may be postulated because additional concepts are repeatedly found near the concepts men-tioned in the utterance.  An illustrative example found in our annotated corpus is the utterance, ?Lars Ulrich might have a thing or two to say about technology.? Here, the topic of conversation prior to this utterance was ?tech-nology? and it was changed to ?music? after this utterance. Here, ?Lars Ulrich? is the bridge that connects the two concepts ?technology? and ?mu-sic? together. 5.1 VCA Architecture The VCA is composed of the following modules that interact as shown in Figure 3.   5.1.1 Chat Analyzer Every utterance in chat is first analyzed by the Chat Analyzer component. This process removes stop words, emoticons and punctuation, as well as any participant nicknames from the utterance. We postulate that the remaining content bearing words in the utterance represent the topic of that utterance. We call this analyzed utterance our chat ?query? which is sent in parallel to the Document Retrieval and NL Processing compo-nent.   5.1.2 Document Retrieval The document retrieval process retrieves docu-ments from either the web or a test document 
corpus. We use Google AJAX api for our web retrieval process and InQuery (Callan et al, 1992) retrieval engine for our offline mode of operation to retrieve documents from the test corpus. The test document corpus was collected by mining the web for all utterances in our data 
collection, creating a stable document set for ex-perimental purposes. Currently, the document corpus contains about 1Gb of text data.   5.1.3 Clustering We cluster the paragraphs in documents retrieved using clustering method in Hardy et al (Hardy et al, 2009) This process groups the paragraphs containing salient entities into sets of closely as-sociated concepts. From each cluster, we choose the most representative paragraph, usually called the ?seed? paragraph for further NL processing. Each seed paragraph and the chat query undergo the same further NL processing sequence.    5.1.4 Natural Language Processing We process each chat query by performing stemming, part-of-speech tagging and named-entity recognition on it. Each seed paragraph is also run through same three natural language processing tasks. We are using Stanford POS tagger for our part-of-speech tagging. For named entity recognition, we have the ability to choose between BBN?s IdentiFinder and AeroText? (Taylor, 2004).  5.1.5 Framing We build frames from the entities and attributes found in both the chat query and the paragraphs.. This work extends the concept of framing devel-oped for HITIQA (Small et al 2009) and COL-LANE (Strzalkowski, 2009). Framing provides an informative handle on text, which can be ex-
ploited to compare the underlying textual repre-sentations, as we explain in the next section.  5.1.6 Scoring and Frame Matching Using the information in the frames built in the previous step; we compare the chat query frame 
Figure 3. VCA Architecture 
46
built from the chat query, to the frames created from the paragraphs, called paragraph frames. We assign a score for each paragraph frame based on how many attributes and their corre-sponding values match; in the current version of VCA a very basic approach to counting how many attribute-value pairs match is taken. Of all the paragraph frames we select the highest scor-ing frames and select the attribute-value pairs that are not part of the chat query frame. For ex-ample, as shown in Figure 4a below, the chat utterance ?Aruba might be nice!? created the fol-lowing chat query frame.  
 a. Example chat query frame  
 b. Frame Matching, Scoring and Template  Selection  Figure 4. From frames to VCA responses  Correspondingly, we select all PLACE type en-tities from the highest-ranking paragraph frames. These are shown in Figure 4b as Aruba Entity list.  The entities ?NASCAR?, ?Women Seeking Men? and ?Mateo? are not of entity type ? PLACE, we assign them a score of 0. The score is the fre-quency of occurrence of that entity in the para-graph; in this example it is found to be 1. Assign-ing scores by frequency of occurrence ensures that the most commonly occurring concept around the one that is being discussed in the chat query utterance will be used to respond with.  5.1.7 Template Selection Once we have chosen the entity to respond with, we select a template from the set of templates for that entity. These are templates that are created based on the models created from topic change utterances annotated in our data set. For a select group of entities, which are quite frequently en-
countered in our data collection such as PLACE, PERSON, ORGANIZATION etc., we have a set of templates specific to that entity type. We also have several generic templates that may be used if the entity type does not match the ones that we have selected. For example, a PLACE specific template is ?Have you ever been to __?? and a PER-SON specific template is ?You heard about __??. Not all templates are formulated as questions. An-other example of a generic template is ?__rules!?.  6 Example of VCA Interaction Figure 5 represents an example of the VCA in action in a simulated environment; the VCA is the participant ?renee?. We can see how the con-versation changes from ?gun laws? to ?hunting? after renee?s utterance at 11:48 AM.  
  Figure 5. Topic change example 7 Evaluation  We ran two tests of this initial VCA prototype in a public chat-room. VCA was inserted into a public chat-room with multiple participants on two separate occasions. The general topic of dis-cussion during both instances was ?anime?. We have developed an evaluation protocol in order to test the effectiveness of the VCA prototype in a realistic setting. The initial metric of VCA ef-fectiveness is the rate of involvement measured in the number of utterances generated by the VCA during the test period. These utterances are subsequently judged for appropriateness using the metric developed for the Companions Project (Webb, 2010). The actual appropriateness anno-tation scheme can be quite involved, but for this simple test we reduced the coding to only binary assessment, so that the VCA utterances were an-notated as either appropriate or inappropriate, given the content of the utterance and the flow of dialogue thus far. Using this coarse grain evalua-tion on a live chat segment we noted that the VCA made 9 appropriate utterances and 7 inap-
[POS] NNP, Aruba JJ, nice [ENT] PLACE  
Aruba Entity List: VALUE = NASCAR and TYPE = ORGANIZATION and SCORE = 0 VALUE = Dallas and TYPE = PLACE and SCORE = 1 VALUE = Mateo and TYPE = PERSON and SCORE = 0  VCA: How about Dallas? 
47
propriate utterances, which gives the appropri-ateness score of 56%. While some of VCA utter-ances seem inappropriate (i.e., not related to the conversation topic), we noted also that other posters generally tolerated these inappropriate utterances that occurred early in the dialogue. Moreover, these early inappropriate utterances did generate appropriate responses from the hu-man users. This ?positive? dynamic changed gradually as the dialogue progressed, when the participants began to ignore VCA?s utterances.  While this coarse grained evaluation is useful, our plan is to conduct evaluation experiments by recruiting subjects for chat sessions and inserting the VCA in the discussion. We will measure the impact of the VCA in the chat session by having participants fill out post-session questionnaires, which can elicit their responses regarding (a) if they detect presence of a VCA at any time during the dialogue; (b) who was the VCA; (c) who changed the topic of conversation most often; and so on. Another metric of interest is the level of engagement of the VCA, which can be meas-ured by the number of direct responses to an ut-terance by the VCA. We are developing the evaluation process, and report on the results in a separate publication. References  Allen, J. M. Core. (1997). Draft of DAMSL: Dialog Act Markup in Several Layers. http://www.cs.rochester.edu/research/cisd/resources/damsl/  Callan, J. P., W. B. Croft, and S. M. Harding. 1992. The INQUERY Retrieval System, in Proceedings of the 3rd Inter- national Conference on Database and Expert Systems.  Colby, K.M, Hilf, F.D, and S. Weber. 1972. Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes. In: Ar-tificial Intelligence , Vol. 3, p. 199-221. Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bul-letin, 74(5):378{382. Hardy, Hilda, Nobuyuki Shimizu, Tomek Strzalk-owski, Ting Liu, Bowden Wise and Xinyang Zhang. 2002. Cross-document summarization by concept classification. In Proceedings of ACM SIGIR '02 Conference, pages 121-128, Tampere, Finland. Hardy, H., A Biermann, R. Bryce Inouye, A. McKenzie, T. Strzalkowski, C. Ursu, N. Webb and M. Wu. 2006. The AMITIES System: Data-Driven Techniques for Automated Dialogue. In 
Speech Communication 48 (3-4), pages 354-373.  Elsevier. Jurafsky, Dan, Elizabeth Shriberg, and Debra Biasca. (1997). Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/manual.august1.html Krippendorff, Klaus. 1980. Content Analysis, an In-troduction to its Methodology. Sage Publications, Thousand Oaks, CA. Samira S., Tomek Strzalkowski, Sarah Taylor and Jonathan Smith (2009) Comparing an Integrated QA system performance - A Preliminary Model. Proceedings of PACLING Conference, Sapporo, Japan. Shaikh, S., Strzalkowski, T.,  Broadwell, A., Stromer-Galley, J., Taylor, Sarah and Webb, N. 2010. Pro-ceedings of LREC Conference, Malta. Sharon Small and Tomek Strzalkowski. 2009. HITIQA: High-Quality Intelligence through Inter-active Question Answering. Journal of Natural Language Engineering, Vol. 15 (1), pp. 31?54. Cambridge.  Tomek Strzalkowski, Sarah Taylor, Samira Shaikh, Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song Chen. 2009. COLLANE: An experiment in com-puter-mediated tacit collaboration. In Aspects of Natural Language Processing (M. Marciniak and A. Mykowiecka, editors). Springer.  Taylor, Sarah M. 2004. "Information Extraction Tools: Deciphering Human Language." IT Profes-sional. Vol. 06, no. 6, pages: 28-34. Novem-ber/December, 2004. Online. http://ieeexplore.ieee.org/iel5/6294/30282/01390870.pdf?tp=&arnumber=1390870&isnumber=30282 Wallace, R. 2008. The Anatomy of A.L.I.C.E. In Parsing the Turing Test. (Robert Epstein, Gary Roberts and Grace Beber, editors). Springer. Webb, N., D. Benyon, P. Hansen and O. Mival. 2010. Evaluating Human-Machine Conversation for Ap-propriateness. In Proceedings of the 7th Interna-tional Conference on Language Resources and Evaluation (LREC2010), Valletta, Malta. Weizenbaum, Joseph. January 1966. "ELIZA ? A Computer Program For the Study of Natural Lan-guage Communication Between Man And Ma-chine", Communications of the ACM 9 (1): 36?45. Wilks, Y. 2010. Artificial Companions. In: Y.Wilks (ed.) Close Engagement with Companions: scien-tific, economic, psychological and philosophical perspectives. John Benjamins: Amsterdam. 
48
