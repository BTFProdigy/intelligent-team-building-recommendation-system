Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700?1709,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recurrent Continuous Translation Models
Nal Kalchbrenner Phil Blunsom
Department of Computer Science
University of Oxford
{nal.kalchbrenner,phil.blunsom}@cs.ox.ac.uk
Abstract
We introduce a class of probabilistic con-
tinuous translation models called Recur-
rent Continuous Translation Models that are
purely based on continuous representations
for words, phrases and sentences and do not
rely on alignments or phrasal translation units.
The models have a generation and a condi-
tioning aspect. The generation of the transla-
tion is modelled with a target Recurrent Lan-
guage Model, whereas the conditioning on the
source sentence is modelled with a Convolu-
tional Sentence Model. Through various ex-
periments, we show first that our models ob-
tain a perplexity with respect to gold transla-
tions that is > 43% lower than that of state-
of-the-art alignment-based translation models.
Secondly, we show that they are remarkably
sensitive to the word order, syntax, and mean-
ing of the source sentence despite lacking
alignments. Finally we show that they match a
state-of-the-art system when rescoring n-best
lists of translations.
1 Introduction
In most statistical approaches to machine transla-
tion the basic units of translation are phrases that are
composed of one or more words. A crucial com-
ponent of translation systems are models that esti-
mate translation probabilities for pairs of phrases,
one phrase being from the source language and the
other from the target language. Such models count
phrase pairs and their occurrences as distinct if the
surface forms of the phrases are distinct. Although
distinct phrase pairs often share significant similari-
ties, linguistic or otherwise, they do not share statis-
tical weight in the models? estimation of their trans-
lation probabilities. Besides ignoring the similar-
ity of phrase pairs, this leads to general sparsity is-
sues. The estimation is sparse or skewed for the
large number of rare or unseen phrase pairs, which
grows exponentially in the length of the phrases, and
the generalisation to other domains is often limited.
Continuous representations have shown promise
at tackling these issues. Continuous representations
for words are able to capture their morphological,
syntactic and semantic similarity (Collobert and We-
ston, 2008). They have been applied in continu-
ous language models demonstrating the ability to
overcome sparsity issues and to achieve state-of-the-
art performance (Bengio et al, 2003; Mikolov et
al., 2010). Word representations have also shown
a marked sensitivity to conditioning information
(Mikolov and Zweig, 2012). Continuous repre-
sentations for characters have been deployed in
character-level language models demonstrating no-
table language generation capabilities (Sutskever et
al., 2011). Continuous representations have also
been constructed for phrases and sentences. The rep-
resentations are able to carry similarity and task de-
pendent information, e.g. sentiment, paraphrase or
dialogue labels, significantly beyond the word level
and to accurately predict labels for a highly diverse
range of unseen phrases and sentences (Grefenstette
et al, 2011; Socher et al, 2011; Socher et al, 2012;
Hermann and Blunsom, 2013; Kalchbrenner and
Blunsom, 2013).
Phrase-based continuous translation models were
first proposed in (Schwenk et al, 2006) and re-
1700
cently further developed in (Schwenk, 2012; Le et
al., 2012). The models incorporate a principled way
of estimating translation probabilities that robustly
extends to rare and unseen phrases. They achieve
significant Bleu score improvements and yield se-
mantically more suggestive translations. Although
wide-reaching in their scope, these models are lim-
ited to fixed-size source and target phrases and sim-
plify the dependencies between the target words tak-
ing into account restricted target language modelling
information.
We describe a class of continuous translation
models called Recurrent Continuous Translation
Models (RCTM) that map without loss of generality
a sentence from the source language to a probabil-
ity distribution over the sentences in the target lan-
guage. We define two specific RCTM architectures.
Both models adopt a recurrent language model for
the generation of the target translation (Mikolov et
al., 2010). In contrast to other n-gram approaches,
the recurrent language model makes no Markov as-
sumptions about the dependencies of the words in
the target sentence.
The two RCTMs differ in the way they condi-
tion the target language model on the source sen-
tence. The first RCTM uses the convolutional sen-
tence model (Kalchbrenner and Blunsom, 2013) to
transform the source word representations into a rep-
resentation for the source sentence. The source sen-
tence representation in turn constraints the genera-
tion of each target word. The second RCTM intro-
duces an intermediate representation. It uses a trun-
cated variant of the convolutional sentence model to
first transform the source word representations into
representations for the target words; the latter then
constrain the generation of the target sentence. In
both cases, the convolutional layers are used to gen-
erate combined representations for the phrases in a
sentence from the representations of the words in the
sentence.
An advantage of RCTMs is the lack of latent
alignment segmentations and the sparsity associated
with them. Connections between source and target
words, phrases and sentences are learnt only implic-
itly as mappings between their continuous represen-
tations. As we see in Sect. 5, these mappings of-
ten carry remarkably precise morphological, syntac-
tic and semantic information. Another advantage is
that the probability of a translation under the models
is efficiently computable requiring a small number
of matrix-vector products that is linear in the length
of the source and the target sentence. Further, trans-
lations can be generated directly from the probabil-
ity distribution of the RCTM without any external
resources.
We evaluate the performance of the models in four
experiments. Since the translation probabilities of
the RCTMs are tractable, we can measure the per-
plexity of the models with respect to the reference
translations. The perplexity of the models is signifi-
cantly lower than that of IBM Model 1 and is> 43%
lower than the perplexity of a state-of-the-art variant
of the IBM Model 2 (Brown et al, 1993; Dyer et
al., 2013). The second and third experiments aim to
show the sensitivity of the output of the RCTM II
to the linguistic information in the source sentence.
The second experiment shows that under a random
permutation of the words in the source sentences,
the perplexity of the model with respect to the refer-
ence translations becomes significantly worse, sug-
gesting that the model is highly sensitive to word
position and order. The third experiment inspects
the translations generated by the RCTM II. The
generated translations demonstrate remarkable mor-
phological, syntactic and semantic agreement with
the source sentence. Finally, we test the RCTMs
on the task of rescoring n-best lists of translations.
The performance of the RCTM probabilities joined
with a single word penalty feature matches the per-
formance of the state-of-the-art translation system
cdec that makes use of twelve features including
five alignment-based translation models (Dyer et al,
2010).
We proceed as follows. We begin in Sect. 2 by
describing the general modelling framework under-
lying the RCTMs. In Sect. 3 we describe the RCTM
I and in Sect. 4 the RCTM II. Section 5 is dedicated
to the four experiments and we conclude in Sect. 6.1
2 Framework
We begin by describing the modelling framework
underlying RCTMs. An RCTM estimates the proba-
bility P (f|e) of a target sentence f = f1, ..., fm being
a translation of a source sentence e = e1, ..., ek. Let
1Code and models available at nal.co
1701
us denote by fi:j the substring of words fi, ..., fj . Us-
ing the following identity,
P (f|e) =
m?
i=1
P (fi|f1:i?1, e) (1)
an RCTM estimates P (f|e) by directly computing
for each target position i the conditional probability
P (fi|f1:i?1, e) of the target word fi occurring in the
translation at position i, given the preceding target
words f1:i?1 and the source sentence e. We see that
an RCTM is sensitive not just to the source sentence
e but also to the preceding words f1:i?1 in the target
sentence; by doing so it incorporates a model of the
target language itself.
To model the conditional probability P (f|e), an
RCTM comprises both a generative architecture for
the target sentence and an architecture for condition-
ing the latter on the source sentence. To fully cap-
ture Eq. 1, we model the generative architecture with
a recurrent language model (RLM) based on a re-
current neural network (Mikolov et al, 2010). The
prediction of the i-th word fi in a RLM depends on
all the preceding words f1:i?1 in the target sentence
ensuring that conditional independence assumptions
are not introduced in Eq. 1. Although the predic-
tion is most strongly influenced by words closely
preceding fi, long-range dependencies from across
the whole sentence can also be exhibited. The con-
ditioning architectures are model specific and are
treated in Sect. 3-4. Both the generative and con-
ditioning aspects of the models deploy continuous
representations for the constituents and are trained
as a single joint architecture. Given the modelling
framework underlying RCTMs, we now proceed to
describe in detail the recurrent language model un-
derlying the generative aspect.
2.1 Recurrent Language Model
A RLM models the probability P (f) that the se-
quence of words f occurs in a given language. Let
f = f1, ..., fm be a sequence of m words, e.g. a sen-
tence in the target language. Analogously to Eq. 1,
using the identity,
P (f) =
m?
i=1
P (fi|f1:i?1) (2)
the model explicitly computes without simpli-
fying assumptions the conditional distributions
R
I O
f
i
P(f   )
i+1
h
R
f
i-1
P(f )
i
f
i+1
P(f   )
i+2
OI
h h h
i-1 i i+1
Figure 1: A RLM (left) and its unravelling to depth 3
(right). The recurrent transformation is applied to the hid-
den layer hi?1 and the result is summed to the represen-
tation for the current word fi. After a non-linear transfor-
mation, a probability distribution over the next word fi+1
is predicted.
P (fi|f1:i?1). The architecture of a RLM comprises
a vocabulary V that contains the words fi of the
language as well as three transformations: an in-
put vocabulary transformation I ? Rq?|V |, a re-
current transformation R ? Rq?q and an output
vocabulary transformation O ? R|V |?q. For each
word fk ? V , we indicate by i(fk) its index in V
and by v(fk) ? R|V |?1 an all zero vector with only
v(fk)i(fk) = 1.
For a word fi, the result of I ? v(fi) ? Rq?1 is
the input continuous representation of fi. The pa-
rameter q governs the size of the word representa-
tion. The prediction proceeds by successively ap-
plying the recurrent transformation R to the word
representations and predicting the next word at each
step. In detail, the computation of each P (fi|f1:i?1)
proceeds recursively. For 1 < i < m,
h1 = ?(I ? v(f1)) (3a)
hi+1 = ?(R ? hi + I ? v(fi+1)) (3b)
oi+1 = O ? hi (3c)
and the conditional distribution is given by,
P (fi = v|f1:i?1) =
exp (oi,v)
?V
v=1 exp(oi,v)
(4)
In Eq. 3, ? is a nonlinear function such as tanh. Bias
values bh and bo are included in the computation. An
illustration of the RLM is given in Fig. 1.
The RLM is trained by backpropagation through
time (Mikolov et al, 2010). The error in the pre-
dicted distribution calculated at the output layer is
1702
backpropagated through the recurrent layers and cu-
mulatively added to the errors of the previous predic-
tions for a given number d of steps. The procedure is
equivalent to standard backpropagation over a RLM
that is unravelled to depth d as in Fig. 1.
RCTMs may be thought of as RLMs, in which
the predicted distributions for each word fi are con-
ditioned on the source sentence e. We next define
two conditioning architectures each giving rise to a
specific RCTM.
3 Recurrent Continuous Translation
Model I
The RCTM I uses a convolutional sentence model
(CSM) in the conditioning architecture. The CSM
creates a representation for a sentence that is pro-
gressively built up from representations of the n-
grams in the sentence. The CSM embodies a hierar-
chical structure. Although it does not make use of an
explicit parse tree, the operations that generate the
representations act locally on small n-grams in the
lower layers of the model and act increasingly more
globally on the whole sentence in the upper layers
of the model. The lack of the need for a parse tree
yields two central advantages over sentence models
that require it (Grefenstette et al, 2011; Socher et
al., 2012). First, it makes the model robustly appli-
cable to a large number of languages for which accu-
rate parsers are not available. Secondly, the transla-
tion probability distribution over the target sentences
does not depend on the chosen parse tree.
The RCTM I conditions the probability of each
target word fi on the continuous representation of the
source sentence e generated through the CSM. This
is accomplished by adding the sentence representa-
tion to each hidden layer hi in the target recurrent
language model. We next describe the procedure in
more detail, starting with the CSM itself.
3.1 Convolutional Sentence Model
The CSM models the continuous representation of
a sentence based on the continuous representations
of the words in the sentence. Let e = e1...ek be
a sentence in a language and let v(ei) ? Rq?1 be
the continuous representation of the word ei. Let
Ee ? Rq?k be the sentence matrix for e defined by,
Ee:,i = v(ei) (5)
(K    M)
*
:,1
M M M
:,1
:,2
:,3
the cat
sat
on the
mat
e
E 
e
K 
2
K 
3
L  
3
K 
i
   :,1
K 
i
   :,2
K 
i
   :,3
i
Figure 2: A CSM for a six word source sentence e and the
computed sentence representation e. K2,K3 are weight
matrices and L3 is a top weight matrix. To the right, an
instance of a one-dimensional convolution between some
weight matrix Ki and a generic matrix M that could for
instance correspond to Ee2. The color coding of weights
indicates weight sharing.
The main component of the architecture of the CSM
is a sequence of weight matrices (Ki)2?i?r that cor-
respond to the kernels or filters of the convolution
and can be thought of as learnt feature detectors.
From the sentence matrix Ee the CSM computes a
continuous vector representation e ? Rq?1 for the
sentence e by applying a sequence of convolutions
to Ee whose weights are given by the weight matri-
ces. The weight matrices and the sequence of con-
volutions are defined next.
We denote by (Ki)2?i?r a sequence of weight
matrices where each Ki ? Rq?i is a matrix of i
columns and r = d
?
2Ne, where N is the length of
the longest source sentence in the training set. Each
row of Ki is a vector of i weights that is treated as
the kernel or filter of a one-dimensional convolution.
Given for instance a matrix M ? Rq?j where the
number of columns j ? i, each row of Ki can be
convolved with the corresponding row in M, result-
ing in a matrix Ki ?M, where ? indicates the con-
volution operation and (Ki ?M) ? Rq?(j?i+1). For
i = 3, the value (Ki ?M):,a is computed by:
Ki:,1M:,a+K
i
:,2M:,a+1 +K
i
:,3M:,a+2 (6)
where  is component-wise vector product. Ap-
plying the convolution kernel Ki yields a matrix
(Ki?M) that has i?1 columns less than the original
matrix M.
Given a source sentence of length k, the CSM
convolves successively with the sentence matrix Ee
1703
the sequence of weight matrices (Ki)2?i?r, one af-
ter the other starting with K2 as follows:
Ee1 = E
e (7a)
Eei+1 = ?(K
i+1 ?Eei ) (7b)
After a few convolution operations, Eei is either a
vector in Rq?1, in which case we obtained the de-
sired representation, or the number of columns in
Eei is smaller than the number i + 1 of columns in
the next weight matrix Ki+1. In the latter case, we
equally obtain a vector in Rq?1 by simply apply-
ing a top weight matrix Lj that has the same num-
ber of columns as Eei . We thus obtain a sentence
representation e ? Rq?1 for the source sentence e.
Note that the convolution operations in Eq. 7b are
interleaved with non-linear functions ?. Note also
that, given the different levels at which the weight
matrices Ki and Li are applied, the top weight
matrix Lj comes from an additional sequence of
weight matrices (Li)2?i?r distinct from (Ki)2?i?r.
Fig. 2 depicts an instance of the CSM and of a one-
dimensional convolution.2
3.2 RCTM I
As defined in Sect. 2, the RCTM I models the condi-
tional probability P (f|e) of a sentence f = f1, ..., fm
in a target language F being the translation of a sen-
tence e = e1, ..., ek in a source language E. Accord-
ing to Eq. 1, the RCTM I explicitly computes the
conditional distributions P (fi|f1:i?1, e). The archi-
tecture of the RCTM I comprises a source vocabu-
lary V E and a target vocabulary V F, two sequences
of weight matrices (Ki)2?i?r and (Li)2?i?r that
are part of the constituent CSM, transformations
I ? Rq?|V
F|, R ? Rq?q and O ? R|V
F|?q that are
part of the constituent RLM and a sentence transfor-
mation S ? Rq?q. We write e = csm(e) for the
output of the CSM with e as the input sentence.
The computation of the RCTM I is a simple mod-
ification to the computation of the RLM described in
Eq. 3. It proceeds recursively as follows:
s = S ? csm(e) (8a)
h1 = ?(I ? v(f1) + s) (8b)
hi+1 = ?(R ? hi + I ? v(fi+1) + s) (8c)
oi+1 = O ? hi (8d)
2For a formal treatment of the construction, see (Kalchbren-
ner and Blunsom, 2013).
and the conditional distributions P (fi+1|f1:i, e) are
obtained from oi as in Eq. 4. ? is a nonlinear func-
tion and bias values are included throughout the
computation. Fig. 3 illustrates an RCTM I.
Two aspects of the RCTM I are to be remarked.
First, the length of the target sentence is predicted
by the target RLM itself that by its architecture has
a bias towards shorter sentences. Secondly, the rep-
resentation of the source sentence e constraints uni-
formly all the target words, contrary to the fact that
the target words depend more strongly on certain
parts of the source sentence and less on other parts.
The next model proposes an alternative formulation
of these aspects.
4 Recurrent Continuous Translation
Model II
The central idea behind the RCTM II is to first es-
timate the length m of the target sentence indepen-
dently of the main architecture. Given m and the
source sentence e, the model constructs a represen-
tation for the n-grams in e, where n is set to 4. Note
that each level of the CSM yields n-gram represen-
tations of e for a specific value of n. The 4-gram
representation of e is thus constructed by truncat-
ing the CSM at the level that corresponds to n = 4.
The procedure is then inverted. From the 4-gram
representation of the source sentence e, the model
builds a representation of a sentence that has the
predicted length m of the target. This is similarly
accomplished by truncating the inverted CSM for a
sentence of length m.
We next describe in detail the Convolutional n-
gram Model (CGM). Then we return to specify the
RCTM II.
4.1 Convolutional n-gram model
The CGM is obtained by truncating the CSM at the
level where n-grams are represented for the chosen
value of n. A column g of a matrix Eei obtained
according to Eq. 7 represents an n-gram from the
source sentence e. The value of n corresponds to
the number of word vectors from which the n-gram
representation g is constructed; equivalently, n is
the span of the weights in the CSM underneath g
(see Fig. 2-3). Note that any column in a matrix
Eei represents an n-gram with the same span value
n. We denote by gram(Eei ) the size of the n-grams
1704
RCTM IIRCTM I
P( f | e ) 
P( f | m, e )
e 
e 
e 
F 
T 
S 
S 
csm 
cgm 
icgm 
E 
F 
g 
g 
Figure 3: A graphical depiction of the two RCTMs. Arrows represent full matrix transformations while lines are
vector transformations corresponding to columns of weight matrices.
represented by Eei . For example, for a sufficiently
long sentence e, gram(Ee2) = 2, gram(E
e
3) = 4,
gram(Ee4) = 7. We denote by cgm(e, n) that matrix
Eei from the CSM that represents the n-grams of the
source sentence e.
The CGM can also be inverted to obtain a repre-
sentation for a sentence from the representation of
its n-grams. We denote by icgm the inverse CGM,
which depends on the size of the n-gram represen-
tation cgm(e, n) and on the target sentence length
m. The transformation icgm unfolds the n-gram
representation onto a representation of a target sen-
tence with m words. The architecture corresponds
to an inverted CGM or, equivalently, to an inverted
truncated CSM (Fig. 3). Given the transformations
cgm and icgm, we now detail the computation of the
RCTM II.
4.2 RCTM II
The RCTM II models the conditional probability
P (f|e) by factoring it as follows:
P (f|e) = P (f|m, e) ? P (m|e) (9a)
=
m?
i=1
P (fi+1|f1:i,m, e) ? P (m|e) (9b)
and computing the distributions P (fi+1|f1:i,m, e)
and P (m|e). The architecture of the RCTM II
comprises all the elements of the RCTM I together
with the following additional elements: a translation
transformation Tq?q and two sequences of weight
matrices (Ji)2?i?s and (Hi)2?i?s that are part of
the icgm3.
The computation of the RCTM II proceeds recur-
sively as follows:
Eg = cgm(e, 4) (10a)
Fg:,j = ?(T ?E
g
:,j) (10b)
F = icgm(Fg,m) (10c)
h1 = ?(I ? v(f1) + S ? F:,1) (10d)
hi+1 = ?(R ? hi + I ? v(fi+1) + S ? F:,i+1) (10e)
oi+1 = O ? hi (10f)
and the conditional distributions P (fi+1|f1:i, e) are
obtained from oi as in Eq. 4. Note how each re-
constructed vector F:,i is added successively to the
corresponding layer hi that predicts the target word
fi. The RCTM II is illustrated in Fig. 3.
3Just like r the value s is small and depends on the length
of the source and target sentences in the training set. See
Sect. 5.1.2.
1705
For the separate estimation of the length of the
translation, we estimate the conditional probability
P (m|e) by letting,
P (m|e) = P (m|k) = Poisson(?k) (11)
where k is the length of the source sentence e and
Poisson(?) is a Poisson distribution with mean ?.
This concludes the description of the RCTM II. We
now turn to the experiments.
5 Experiments
We report on four experiments. The first experiment
considers the perplexities of the models with respect
to reference translations. The second and third ex-
periments test the sensitivity of the RCTM II to the
linguistic aspects of the source sentences. The fi-
nal experiment tests the rescoring performance of
the two models.
5.1 Training
Before turning to the experiments, we describe the
data sets, hyper parameters and optimisation algo-
rithms used for the training of the RCTMs.
5.1.1 Data sets
The training set used for all the experiments com-
prises a bilingual corpus of 144953 pairs of sen-
tences less than 80 words in length from the news
commentary section of the Eighth Workshop on Ma-
chine Translation (WMT) 2013 training data. The
source language is English and the target language
is French. The English sentences contain about
4.1M words and the French ones about 4.5M words.
Words in both the English and French sentences
that occur twice or less are substituted with the
?unknown? token. The resulting vocabularies V E
and V F contain, respectively, 25403 English words
and 34831 French words.
For the experiments we use four different test sets
comprised of the Workshop on Machine Transla-
tion News Test (WMT-NT) sets for the years 2009,
2010, 2011 and 2012. They contain, respectively,
2525, 2489, 3003 and 3003 pairs of English-French
sentences. For the perplexity experiments unknown
words occurring in these data sets are replaced with
the ?unknown? token. The respective 2008 WMT-
NT set containing 2051 pairs of English-French sen-
tences is used as the validation set throughout.
5.1.2 Model hyperparameters
The parameter q that defines the size of the En-
glish vectors v(ei) for ei ? V E, the size of the hid-
den layer hi and the size of the French vectors v(fi)
for v(fi) ? V F is set to q = 256. This yields a
relatively small recurrent matrix and corresponding
models. To speed up training, we factorize the target
vocabulary V F into 256 classes following the proce-
dure in (Mikolov et al, 2011).
The RCTM II uses a convolutional n-gram model
CGM where n is set to 4. For the RCTM I, the num-
ber of weight matrices r for the CSM is 15, whereas
in the RCTM II the number r of weight matrices for
the CGM is 7 and the number s of weight matrices
for the inverse CGM is 9. If a test sentence is longer
than all training sentences and a larger weight matrix
is required by the model, the larger weight matrix is
easily factorized into two smaller weight matrices
whose weights have been trained. For instance, if a
weight matrix of 10 weights is required, but weight
matrices have been trained only up to weight 9, then
one can factorize the matrix of 10 weights with one
of 9 and one of 2. Across all test sets the proportion
of sentence pairs that require larger weight matrices
to be factorized into smaller ones is < 0.1%.
5.1.3 Objective and optimisation
The objective function is the average of the sum
of the cross-entropy errors of the predicted words
and the true words in the French sentences. The En-
glish sentences are taken as input in the prediction
of the French sentences, but they are not themselves
ever predicted. An l2 regularisation term is added to
the objective. The training of the model proceeds by
back-propagation through time. The cross-entropy
error calculated at the output layer at each step is
back-propagated through the recurrent structure for
a number d of steps; for all models we let d = 6.
The error accumulated at the hidden layers is then
further back-propagated through the transformation
S and the CSM/CGM to the input vectors v(ei) of
the English input sentence e. All weights, includ-
ing the English vectors, are randomly initialised and
inferred during training.
The objective is minimised using mini-batch
adaptive gradient descent (Adagrad) (Duchi et al,
2011). The training of an RCTM takes about 15
hours on 3 multicore CPUs. While our experiments
1706
WMT-NT 2009 2010 2011 2012
KN-5 218 213 222 225
RLM 178 169 178 181
IBM 1 207 200 188 197
FA-IBM 2 153 146 135 144
RCTM I 143 134 140 142
RCTM II 86 77 76 77
Table 1: Perplexity results on the WMT-NT sets.
are relatively small, we note that in principle our
models should scale similarly to RLMs which have
been applied to hundreds of millions of words.
5.2 Perplexity of gold translations
Since the computation of the probability of a trans-
lation under one of the RCTMs is efficient, we can
compute the perplexities of the RCTMs with respect
to the reference translations in the test sets. The per-
plexity measure is an indication of the quality that
a model assigns to a translation. We compare the
perplexities of the RCTMs with the perplexity of the
IBM Model 1 (Brown et al, 1993) and of the Fast-
Aligner (FA-IBM 2) model that is a state-of-the-art
variant of IBM Model 2 (Dyer et al, 2013). We add
as baselines the unconditional target RLM and a 5-
gram target language model with modified Kneser-
Nay smoothing (KN-5). The results are reported in
Tab. 1. The RCTM II obtains a perplexity that is
> 43% lower than that of the alignment based mod-
els and that is 40% lower than the perplexity of the
RCTM I. The low perplexity of the RCTMs suggests
that continuous representations and the transforma-
tions between them make up well for the lack of ex-
plicit alignments. Further, the difference in perplex-
ity between the RCTMs themselves demonstrates
the importance of the conditioning architecture and
suggests that the localised 4-gram conditioning in
the RCTM II is superior to the conditioning with the
whole source sentence of the RCTM I.
5.3 Sensitivity to source sentence structure
The second experiment aims at showing the sensi-
tivity of the RCTM II to the order and position of
words in the English source sentence. To this end,
we randomly permute in the training and testing sets
WMT-NT PERM 2009 2010 2011 2012
RCTM II 174 168 175 178
Table 2: Perplexity results of the RCTM II on the WMT-
NT sets where the words in the English source sentences
are randomly permuted.
the words in the English source sentence. The re-
sults on the permuted data are reported in Tab. 2. If
the RCTM II were roughly comparable to a bag-of-
words approach, there would be no difference under
the permutation of the words. By contrast, the dif-
ference of the results reported in Tab. 2 with those
reported in Tab. 1 is very significant, clearly indicat-
ing the sensitivity to word order and position of the
translation model.
5.3.1 Generating from the RCTM II
To show that the RCTM II is sensitive not only to
word order, but also to other syntactic and semantic
traits of the sentence, we generate and inspect can-
didate translations for various English source sen-
tences. The generation proceeds by sampling from
the probability distribution of the RCTM II itself and
does not depend on any other external resources.
Given an English source sentence e, we let m be
the length of the gold translation and we search the
distribution computed by the RCTM II over all sen-
tences of length m. The number of possible target
sentences of length m amounts to |V |m = 34831m
where V = V F is the French vocabulary; directly
considering all possible translations is intractable.
We proceed as follows: we sample with replace-
ment 2000 sentences from the distribution of the
RCTM II, each obtained by predicting one word at
a time. We start by predicting a distribution for the
first target word, restricting that distribution to the
top 5 most probable words and sampling the first
word of a candidate translation from the restricted
distribution of 5 words. We proceed similarly for
the remaining words. Each sampled sentence has a
well-defined probability assigned by the model and
can thus be ranked. Table 3 gives various English
source sentences and some candidate French trans-
lations generated by the RCTM II together with their
ranks.
The results in Tab. 3 show the remarkable syn-
tactic agreements of the candidate translations; the
1707
English source sentence French gold translation RCTM II candidate translation Rank
the patient is sick . le patient est malade . le patient est insuffisante . 1
le patient est mort . 4
la patient est insuffisante . 23
the patient is dead . le patient est mort . le patient est mort . 1
le patient est de?passe? . 4
the patient is ill . le patient est malade . le patient est mal . 3
the patients are sick . les patients sont malades . les patients sont confronte?s . 2
les patients sont corrompus . 5
the patients are dead . les patients sont morts . les patients sont morts . 1
the patients are ill . les patients sont malades . les patients sont confronte?s . 5
the patient was ill . le patient e?tait malade . le patient e?tait mal . 2
the patients are not dead . les patients ne sont pas morts . les patients ne sont pas morts . 1
the patients are not sick . les patients ne sont pas malades . les patients ne sont pas ?unknown? . 1
les patients ne sont pas mal . 6
the patients were saved . les patients ont e?te? sauve?s . les patients ont e?te? sauve?es . 6
Table 3: English source sentences, respective translations in French and candidate translations generated from the
RCTM II and ranked out of 2000 samples according to their decreasing probability. Note that end of sentence dots (.)
are generated as part of the translation.
WMT-NT 2009 2010 2011 2012
RCTM I + WP 19.7 21.1 22.5 21.5
RCTM II + WP 19.8 21.1 22.5 21.7
cdec (12 features) 19.9 21.2 22.6 21.8
Table 4: Bleu scores on the WMT-NT sets of each RCTM
linearly interpolated with a word penalty WP. The cdec
system includes WP as well as five translation models and
two language modelling features, among others.
large majority of the candidate translations are fully
well-formed French sentences. Further, subtle syn-
tactic features such as the singular or plural ending
of nouns and the present and past tense of verbs are
well correlated between the English source and the
French candidate targets. Finally, the meaning of
the English source is well transferred on the French
candidate targets; where a correlation is unlikely or
the target word is not in the French vocabulary, a se-
mantically related word or synonym is selected by
the model. All of these traits suggest that the RCTM
II is able to capture a significant amount of both
syntactic and semantic information from the English
source sentence and successfully transfer it onto the
French translation.
5.4 Rescoring and BLEU Evaluation
The fourth experiment tests the ability of the RCTM
I and the RCTM II to choose the best translation
among a large number of candidate translations pro-
duced by another system. We use the cdec sys-
tem to generate a list of 1000 best candidate trans-
lations for each English sentence in the four WMT-
NT sets. We compare the rescoring performance of
the RCTM I and the RCTM II with that of the cdec
itself. cdec employs 12 engineered features includ-
ing, among others, 5 translation models, 2 language
model features and a word penalty feature (WP). For
the RCTMs we simply interpolate the log probabil-
ity assigned by the models to the candidate transla-
tions with the word penalty feature WP, tuned on the
validation data. The results of the experiment are
reported in Tab. 4.
While there is little variance in the resulting Bleu
scores, the performance of the RCTMs shows that
their probabilities correlate with translation qual-
ity. Combining a monolingual RLM feature with
the RCTMs does not improve the scores, while re-
ducing cdec to just one core translation probability
and language model features drops its score by two
to five tenths. These results indicate that the RCTMs
have been able to learn both translation and language
modelling distributions.
1708
6 Conclusion
We have introduced Recurrent Continuous Transla-
tion Models that comprise a class of purely contin-
uous sentence-level translation models. We have
shown the translation capabilities of these models
and the low perplexities that they obtain with respect
to reference translations. We have shown the ability
of these models at capturing syntactic and semantic
information and at estimating during reranking the
quality of candidate translations.
The RCTMs offer great modelling flexibility due
to the sensitivity of the continuous representations to
conditioning information. The models also suggest
a wide range of potential advantages and extensions,
from being able to include discourse representations
beyond the single sentence and multilingual source
representations, to being able to model morpholog-
ically rich languages through character-level recur-
rences.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of ibm model 2. In Proc. of NAACL.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional
Semantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics. Forthcoming.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Convolutional Neural Networks for Discourse Com-
positionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In HLT-NAACL, pages 39?48.
Tomas Mikolov and Geoffrey Zweig. 2012. Context de-
pendent recurrent neural network language model. In
SLT, pages 234?239.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Takao
Kobayashi, Keikichi Hirose, and Satoshi Nakamura,
editors, INTERSPEECH, pages 1045?1048. ISCA.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model. In
ICASSP, pages 5528?5531. IEEE.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In ACL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein-
berger, editors, Advances in Neural Information Pro-
cessing Systems 24, pages 801?809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositional-
ity Through Recursive Matrix-Vector Spaces. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Ilya Sutskever, James Martens, and Geoffrey E. Hinton.
2011. Generating text with recurrent neural networks.
In Lise Getoor and Tobias Scheffer, editors, ICML,
pages 1017?1024. Omnipress.
1709
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655?665,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Convolutional Neural Network for Modelling Sentences
Nal Kalchbrenner Edward Grefenstette
{nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk
Department of Computer Science
University of Oxford
Phil Blunsom
Abstract
The ability to accurately represent sen-
tences is central to language understand-
ing. We describe a convolutional architec-
ture dubbed the Dynamic Convolutional
Neural Network (DCNN) that we adopt
for the semantic modelling of sentences.
The network uses Dynamic k-Max Pool-
ing, a global pooling operation over lin-
ear sequences. The network handles input
sentences of varying length and induces
a feature graph over the sentence that is
capable of explicitly capturing short and
long-range relations. The network does
not rely on a parse tree and is easily ap-
plicable to any language. We test the
DCNN in four experiments: small scale
binary and multi-class sentiment predic-
tion, six-way question classification and
Twitter sentiment prediction by distant su-
pervision. The network achieves excellent
performance in the first three tasks and a
greater than 25% error reduction in the last
task with respect to the strongest baseline.
1 Introduction
The aim of a sentence model is to analyse and
represent the semantic content of a sentence for
purposes of classification or generation. The sen-
tence modelling problem is at the core of many
tasks involving a degree of natural language com-
prehension. These tasks include sentiment analy-
sis, paraphrase detection, entailment recognition,
summarisation, discourse analysis, machine trans-
lation, grounded language learning and image re-
trieval. Since individual sentences are rarely ob-
served or not observed at all, one must represent
a sentence in terms of features that depend on the
words and short n-grams in the sentence that are
frequently observed. The core of a sentence model
involves a feature function that defines the process
 The  cat  sat  on  the  red  mat  The  cat  sat  on  the  red  mat
Figure 1: Subgraph of a feature graph induced
over an input sentence in a Dynamic Convolu-
tional Neural Network. The full induced graph
has multiple subgraphs of this kind with a distinct
set of edges; subgraphs may merge at different
layers. The left diagram emphasises the pooled
nodes. The width of the convolutional filters is 3
and 2 respectively. With dynamic pooling, a fil-
ter with small width at the higher layers can relate
phrases far apart in the input sentence.
by which the features of the sentence are extracted
from the features of the words or n-grams.
Various types of models of meaning have been
proposed. Composition based methods have been
applied to vector representations of word meaning
obtained from co-occurrence statistics to obtain
vectors for longer phrases. In some cases, com-
position is defined by algebraic operations over
word meaning vectors to produce sentence mean-
ing vectors (Erk and Pad?o, 2008; Mitchell and
Lapata, 2008; Mitchell and Lapata, 2010; Tur-
ney, 2012; Erk, 2012; Clarke, 2012). In other
cases, a composition function is learned and ei-
ther tied to particular syntactic relations (Guevara,
2010; Zanzotto et al, 2010) or to particular word
types (Baroni and Zamparelli, 2010; Coecke et
al., 2010; Grefenstette and Sadrzadeh, 2011; Kart-
saklis and Sadrzadeh, 2013; Grefenstette, 2013).
Another approach represents the meaning of sen-
tences by way of automatically extracted logical
forms (Zettlemoyer and Collins, 2005).
655
A central class of models are those based on
neural networks. These range from basic neu-
ral bag-of-words or bag-of-n-grams models to the
more structured recursive neural networks and
to time-delay neural networks based on convo-
lutional operations (Collobert and Weston, 2008;
Socher et al, 2011; Kalchbrenner and Blunsom,
2013b). Neural sentence models have a num-
ber of advantages. They can be trained to obtain
generic vectors for words and phrases by predict-
ing, for instance, the contexts in which the words
and phrases occur. Through supervised training,
neural sentence models can fine-tune these vec-
tors to information that is specific to a certain
task. Besides comprising powerful classifiers as
part of their architecture, neural sentence models
can be used to condition a neural language model
to generate sentences word by word (Schwenk,
2012; Mikolov and Zweig, 2012; Kalchbrenner
and Blunsom, 2013a).
We define a convolutional neural network archi-
tecture and apply it to the semantic modelling of
sentences. The network handles input sequences
of varying length. The layers in the network in-
terleave one-dimensional convolutional layers and
dynamic k-max pooling layers. Dynamic k-max
pooling is a generalisation of the max pooling op-
erator. The max pooling operator is a non-linear
subsampling function that returns the maximum
of a set of values (LeCun et al, 1998). The op-
erator is generalised in two respects. First, k-
max pooling over a linear sequence of values re-
turns the subsequence of k maximum values in the
sequence, instead of the single maximum value.
Secondly, the pooling parameter k can be dynam-
ically chosen by making k a function of other as-
pects of the network or the input.
The convolutional layers apply one-
dimensional filters across each row of features in
the sentence matrix. Convolving the same filter
with the n-gram at every position in the sentence
allows the features to be extracted independently
of their position in the sentence. A convolutional
layer followed by a dynamic pooling layer and
a non-linearity form a feature map. Like in the
convolutional networks for object recognition
(LeCun et al, 1998), we enrich the representation
in the first layer by computing multiple feature
maps with different filters applied to the input
sentence. Subsequent layers also have multiple
feature maps computed by convolving filters with
all the maps from the layer below. The weights at
these layers form an order-4 tensor. The resulting
architecture is dubbed a Dynamic Convolutional
Neural Network.
Multiple layers of convolutional and dynamic
pooling operations induce a structured feature
graph over the input sentence. Figure 1 illustrates
such a graph. Small filters at higher layers can cap-
ture syntactic or semantic relations between non-
continuous phrases that are far apart in the input
sentence. The feature graph induces a hierarchical
structure somewhat akin to that in a syntactic parse
tree. The structure is not tied to purely syntactic
relations and is internal to the neural network.
We experiment with the network in four set-
tings. The first two experiments involve predict-
ing the sentiment of movie reviews (Socher et
al., 2013b). The network outperforms other ap-
proaches in both the binary and the multi-class ex-
periments. The third experiment involves the cat-
egorisation of questions in six question types in
the TREC dataset (Li and Roth, 2002). The net-
work matches the accuracy of other state-of-the-
art methods that are based on large sets of en-
gineered features and hand-coded knowledge re-
sources. The fourth experiment involves predict-
ing the sentiment of Twitter posts using distant su-
pervision (Go et al, 2009). The network is trained
on 1.6 million tweets labelled automatically ac-
cording to the emoticon that occurs in them. On
the hand-labelled test set, the network achieves a
greater than 25% reduction in the prediction error
with respect to the strongest unigram and bigram
baseline reported in Go et al (2009).
The outline of the paper is as follows. Section 2
describes the background to the DCNN including
central concepts and related neural sentence mod-
els. Section 3 defines the relevant operators and
the layers of the network. Section 4 treats of the
induced feature graph and other properties of the
network. Section 5 discusses the experiments and
inspects the learnt feature detectors.
1
2 Background
The layers of the DCNN are formed by a convo-
lution operation followed by a pooling operation.
We begin with a review of related neural sentence
models. Then we describe the operation of one-
dimensional convolution and the classical Time-
Delay Neural Network (TDNN) (Hinton, 1989;
Waibel et al, 1990). By adding a max pooling
1
Code available at www.nal.co
656
layer to the network, the TDNN can be adopted as
a sentence model (Collobert and Weston, 2008).
2.1 Related Neural Sentence Models
Various neural sentence models have been de-
scribed. A general class of basic sentence models
is that of Neural Bag-of-Words (NBoW) models.
These generally consist of a projection layer that
maps words, sub-word units or n-grams to high
dimensional embeddings; the latter are then com-
bined component-wise with an operation such as
summation. The resulting combined vector is clas-
sified through one or more fully connected layers.
A model that adopts a more general structure
provided by an external parse tree is the Recursive
Neural Network (RecNN) (Pollack, 1990; K?uchler
and Goller, 1996; Socher et al, 2011; Hermann
and Blunsom, 2013). At every node in the tree the
contexts at the left and right children of the node
are combined by a classical layer. The weights of
the layer are shared across all nodes in the tree.
The layer computed at the top node gives a repre-
sentation for the sentence. The Recurrent Neural
Network (RNN) is a special case of the recursive
network where the structure that is followed is a
simple linear chain (Gers and Schmidhuber, 2001;
Mikolov et al, 2011). The RNN is primarily used
as a language model, but may also be viewed as a
sentence model with a linear structure. The layer
computed at the last word represents the sentence.
Finally, a further class of neural sentence mod-
els is based on the convolution operation and the
TDNN architecture (Collobert and Weston, 2008;
Kalchbrenner and Blunsom, 2013b). Certain con-
cepts used in these models are central to the
DCNN and we describe them next.
2.2 Convolution
The one-dimensional convolution is an operation
between a vector of weights m ? R
m
and a vector
of inputs viewed as a sequence s ? R
s
. The vector
m is the filter of the convolution. Concretely, we
think of s as the input sentence and s
i
? R is a sin-
gle feature value associated with the i-th word in
the sentence. The idea behind the one-dimensional
convolution is to take the dot product of the vector
m with each m-gram in the sentence s to obtain
another sequence c:
c
j
= m
?
s
j?m+1:j
(1)
Equation 1 gives rise to two types of convolution
depending on the range of the index j. The narrow
type of convolution requires that s ? m and yields
s1 s1ss ss
c1 c5c5
Figure 2: Narrow and wide types of convolution.
The filter m has size m = 5.
a sequence c ? R
s?m+1
with j ranging from m
to s. The wide type of convolution does not have
requirements on s or m and yields a sequence c ?
R
s+m?1
where the index j ranges from 1 to s +
m ? 1. Out-of-range input values s
i
where i < 1
or i > s are taken to be zero. The result of the
narrow convolution is a subsequence of the result
of the wide convolution. The two types of one-
dimensional convolution are illustrated in Fig. 2.
The trained weights in the filter m correspond
to a linguistic feature detector that learns to recog-
nise a specific class of n-grams. These n-grams
have size n ? m, where m is the width of the
filter. Applying the weights m in a wide convo-
lution has some advantages over applying them in
a narrow one. A wide convolution ensures that all
weights in the filter reach the entire sentence, in-
cluding the words at the margins. This is particu-
larly significant when m is set to a relatively large
value such as 8 or 10. In addition, a wide convo-
lution guarantees that the application of the filter
m to the input sentence s always produces a valid
non-empty result c, independently of the width m
and the sentence length s. We next describe the
classical convolutional layer of a TDNN.
2.3 Time-Delay Neural Networks
A TDNN convolves a sequence of inputs s with a
set of weights m. As in the TDNN for phoneme
recognition (Waibel et al, 1990), the sequence s
is viewed as having a time dimension and the con-
volution is applied over the time dimension. Each
s
j
is often not just a single value, but a vector of
d values so that s ? R
d?s
. Likewise, m is a ma-
trix of weights of size d?m. Each row of m is
convolved with the corresponding row of s and the
convolution is usually of the narrow type. Multi-
ple convolutional layers may be stacked by taking
the resulting sequence c as input to the next layer.
The Max-TDNN sentence model is based on the
architecture of a TDNN (Collobert and Weston,
2008). In the model, a convolutional layer of the
narrow type is applied to the sentence matrix s,
where each column corresponds to the feature vec-
657
tor w
i
? R
d
of a word in the sentence:
s =
?
?
w
1
. . . w
s
?
?
(2)
To address the problem of varying sentence
lengths, the Max-TDNN takes the maximum of
each row in the resulting matrix c yielding a vector
of d values:
c
max
=
?
?
?
max(c
1,:
)
.
.
.
max(c
d,:
)
?
?
?
(3)
The aim is to capture the most relevant feature, i.e.
the one with the highest value, for each of the d
rows of the resulting matrix c. The fixed-sized
vector c
max
is then used as input to a fully con-
nected layer for classification.
The Max-TDNN model has many desirable
properties. It is sensitive to the order of the words
in the sentence and it does not depend on external
language-specific features such as dependency or
constituency parse trees. It also gives largely uni-
form importance to the signal coming from each
of the words in the sentence, with the exception
of words at the margins that are considered fewer
times in the computation of the narrow convolu-
tion. But the model also has some limiting as-
pects. The range of the feature detectors is lim-
ited to the span m of the weights. Increasing m or
stacking multiple convolutional layers of the nar-
row type makes the range of the feature detectors
larger; at the same time it also exacerbates the ne-
glect of the margins of the sentence and increases
the minimum size s of the input sentence required
by the convolution. For this reason higher-order
and long-range feature detectors cannot be easily
incorporated into the model. The max pooling op-
eration has some disadvantages too. It cannot dis-
tinguish whether a relevant feature in one of the
rows occurs just one or multiple times and it for-
gets the order in which the features occur. More
generally, the pooling factor by which the signal
of the matrix is reduced at once corresponds to
s?m+1; even for moderate values of s the pool-
ing factor can be excessive. The aim of the next
section is to address these limitations while pre-
serving the advantages.
3 Convolutional Neural Networks with
Dynamic k-Max Pooling
We model sentences using a convolutional archi-
tecture that alternates wide convolutional layers
K-Max pooling
(k=3)
Fully connected 
layer
Folding
Wide
convolution
(m=2)
Dynamic
k-max pooling
 (k= f(s) =5)
 Projected
sentence 
matrix
(s=7)
Wide
convolution
(m=3)
 The cat sat on the red mat
Figure 3: A DCNN for the seven word input sen-
tence. Word embeddings have size d = 4. The
network has two convolutional layers with two
feature maps each. The widths of the filters at the
two layers are respectively 3 and 2. The (dynamic)
k-max pooling layers have values k of 5 and 3.
with dynamic pooling layers given by dynamic k-
max pooling. In the network the width of a feature
map at an intermediate layer varies depending on
the length of the input sentence; the resulting ar-
chitecture is the Dynamic Convolutional Neural
Network. Figure 3 represents a DCNN. We pro-
ceed to describe the network in detail.
3.1 Wide Convolution
Given an input sentence, to obtain the first layer of
the DCNN we take the embedding w
i
? R
d
for
each word in the sentence and construct the sen-
tence matrix s ? R
d?s
as in Eq. 2. The values
in the embeddings w
i
are parameters that are op-
timised during training. A convolutional layer in
the network is obtained by convolving a matrix of
weights m ? R
d?m
with the matrix of activations
at the layer below. For example, the second layer
is obtained by applying a convolution to the sen-
tence matrix s itself. Dimension d and filter width
m are hyper-parameters of the network. We let the
operations be wide one-dimensional convolutions
as described in Sect. 2.2. The resulting matrix c
has dimensions d? (s+m? 1).
658
3.2 k-Max Pooling
We next describe a pooling operation that is a gen-
eralisation of the max pooling over the time di-
mension used in the Max-TDNN sentence model
and different from the local max pooling opera-
tions applied in a convolutional network for object
recognition (LeCun et al, 1998). Given a value
k and a sequence p ? R
p
of length p ? k, k-
max pooling selects the subsequence p
k
max
of the
k highest values of p. The order of the values in
p
k
max
corresponds to their original order in p.
The k-max pooling operation makes it possible
to pool the k most active features in p that may be
a number of positions apart; it preserves the order
of the features, but is insensitive to their specific
positions. It can also discern more finely the num-
ber of times the feature is highly activated in p
and the progression by which the high activations
of the feature change across p. The k-max pooling
operator is applied in the network after the topmost
convolutional layer. This guarantees that the input
to the fully connected layers is independent of the
length of the input sentence. But, as we see next, at
intermediate convolutional layers the pooling pa-
rameter k is not fixed, but is dynamically selected
in order to allow for a smooth extraction of higher-
order and longer-range features.
3.3 Dynamic k-Max Pooling
A dynamic k-max pooling operation is a k-max
pooling operation where we let k be a function of
the length of the sentence and the depth of the net-
work. Although many functions are possible, we
simply model the pooling parameter as follows:
k
l
= max( k
top
, d
L? l
L
se ) (4)
where l is the number of the current convolutional
layer to which the pooling is applied and L is the
total number of convolutional layers in the net-
work; k
top
is the fixed pooling parameter for the
topmost convolutional layer (Sect. 3.2). For in-
stance, in a network with three convolutional lay-
ers and k
top
= 3, for an input sentence of length
s = 18, the pooling parameter at the first layer
is k
1
= 12 and the pooling parameter at the sec-
ond layer is k
2
= 6; the third layer has the fixed
pooling parameter k
3
= k
top
= 3. Equation 4
is a model of the number of values needed to de-
scribe the relevant parts of the progression of an
l-th order feature over a sentence of length s. For
an example in sentiment prediction, according to
the equation a first order feature such as a posi-
tive word occurs at most k
1
times in a sentence of
length s, whereas a second order feature such as a
negated phrase or clause occurs at most k
2
times.
3.4 Non-linear Feature Function
After (dynamic) k-max pooling is applied to the
result of a convolution, a bias b ? R
d
and a non-
linear function g are applied component-wise to
the pooled matrix. There is a single bias value for
each row of the pooled matrix.
If we temporarily ignore the pooling layer, we
may state how one computes each d-dimensional
column a in the matrix a resulting after the convo-
lutional and non-linear layers. Define M to be the
matrix of diagonals:
M = [diag(m
:,1
), . . . , diag(m
:,m
)] (5)
where m are the weights of the d filters of the wide
convolution. Then after the first pair of a convolu-
tional and a non-linear layer, each column a in the
matrix a is obtained as follows, for some index j:
a = g
?
?
?
M
?
?
?
w
j
.
.
.
w
j+m?1
?
?
?
+ b
?
?
?
(6)
Here a is a column of first order features. Sec-
ond order features are similarly obtained by ap-
plying Eq. 6 to a sequence of first order features
a
j
, ..., a
j+m
?
?1
with another weight matrix M
?
.
Barring pooling, Eq. 6 represents a core aspect
of the feature extraction function and has a rather
general form that we return to below. Together
with pooling, the feature function induces position
invariance and makes the range of higher-order
features variable.
3.5 Multiple Feature Maps
So far we have described how one applies a wide
convolution, a (dynamic) k-max pooling layer and
a non-linear function to the input sentence ma-
trix to obtain a first order feature map. The three
operations can be repeated to yield feature maps
of increasing order and a network of increasing
depth. We denote a feature map of the i-th order
by F
i
. As in convolutional networks for object
recognition, to increase the number of learnt fea-
ture detectors of a certain order, multiple feature
maps F
i
1
, . . . ,F
i
n
may be computed in parallel at
the same layer. Each feature map F
i
j
is computed
by convolving a distinct set of filters arranged in
a matrix m
i
j,k
with each feature map F
i?1
k
of the
lower order i? 1 and summing the results:
659
Fi
j
=
n
?
k=1
m
i
j,k
? F
i?1
k
(7)
where ? indicates the wide convolution. The
weights m
i
j,k
form an order-4 tensor. After the
wide convolution, first dynamic k-max pooling
and then the non-linear function are applied indi-
vidually to each map.
3.6 Folding
In the formulation of the network so far, feature
detectors applied to an individual row of the sen-
tence matrix s can have many orders and create
complex dependencies across the same rows in
multiple feature maps. Feature detectors in differ-
ent rows, however, are independent of each other
until the top fully connected layer. Full depen-
dence between different rows could be achieved
by making M in Eq. 5 a full matrix instead of
a sparse matrix of diagonals. Here we explore a
simpler method called folding that does not intro-
duce any additional parameters. After a convo-
lutional layer and before (dynamic) k-max pool-
ing, one just sums every two rows in a feature map
component-wise. For a map of d rows, folding re-
turns a map of d/2 rows, thus halving the size of
the representation. With a folding layer, a feature
detector of the i-th order depends now on two rows
of feature values in the lower maps of order i? 1.
This ends the description of the DCNN.
4 Properties of the Sentence Model
We describe some of the properties of the sentence
model based on the DCNN. We describe the no-
tion of the feature graph induced over a sentence
by the succession of convolutional and pooling
layers. We briefly relate the properties to those of
other neural sentence models.
4.1 Word and n-Gram Order
One of the basic properties is sensitivity to the or-
der of the words in the input sentence. For most
applications and in order to learn fine-grained fea-
ture detectors, it is beneficial for a model to be able
to discriminate whether a specific n-gram occurs
in the input. Likewise, it is beneficial for a model
to be able to tell the relative position of the most
relevant n-grams. The network is designed to cap-
ture these two aspects. The filters m of the wide
convolution in the first layer can learn to recognise
specific n-grams that have size less or equal to the
filter width m; as we see in the experiments, m in
the first layer is often set to a relatively large value
such as 10. The subsequence of n-grams extracted
by the generalised pooling operation induces in-
variance to absolute positions, but maintains their
order and relative positions.
As regards the other neural sentence models, the
class of NBoW models is by definition insensitive
to word order. A sentence model based on a recur-
rent neural network is sensitive to word order, but
it has a bias towards the latest words that it takes as
input (Mikolov et al, 2011). This gives the RNN
excellent performance at language modelling, but
it is suboptimal for remembering at once the n-
grams further back in the input sentence. Sim-
ilarly, a recursive neural network is sensitive to
word order but has a bias towards the topmost
nodes in the tree; shallower trees mitigate this ef-
fect to some extent (Socher et al, 2013a). As seen
in Sect. 2.3, the Max-TDNN is sensitive to word
order, but max pooling only picks out a single n-
gram feature in each row of the sentence matrix.
4.2 Induced Feature Graph
Some sentence models use internal or external
structure to compute the representation for the in-
put sentence. In a DCNN, the convolution and
pooling layers induce an internal feature graph
over the input. A node from a layer is connected
to a node from the next higher layer if the lower
node is involved in the convolution that computes
the value of the higher node. Nodes that are not
selected by the pooling operation at a layer are
dropped from the graph. After the last pooling
layer, the remaining nodes connect to a single top-
most root. The induced graph is a connected, di-
rected acyclic graph with weighted edges and a
root node; two equivalent representations of an
induced graph are given in Fig. 1. In a DCNN
without folding layers, each of the d rows of the
sentence matrix induces a subgraph that joins the
other subgraphs only at the root node. Each sub-
graph may have a different shape that reflects the
kind of relations that are detected in that subgraph.
The effect of folding layers is to join pairs of sub-
graphs at lower layers before the top root node.
Convolutional networks for object recognition
also induce a feature graph over the input image.
What makes the feature graph of a DCNN pecu-
liar is the global range of the pooling operations.
The (dynamic) k-max pooling operator can draw
together features that correspond to words that are
many positions apart in the sentence. Higher-order
features have highly variable ranges that can be ei-
660
ther short and focused or global and long as the
input sentence. Likewise, the edges of a subgraph
in the induced graph reflect these varying ranges.
The subgraphs can either be localised to one or
more parts of the sentence or spread more widely
across the sentence. This structure is internal to
the network and is defined by the forward propa-
gation of the input through the network.
Of the other sentence models, the NBoW is a
shallow model and the RNN has a linear chain
structure. The subgraphs induced in the Max-
TDNN model have a single fixed-range feature ob-
tained through max pooling. The recursive neural
network follows the structure of an external parse
tree. Features of variable range are computed at
each node of the tree combining one or more of
the children of the tree. Unlike in a DCNN, where
one learns a clear hierarchy of feature orders, in
a RecNN low order features like those of sin-
gle words can be directly combined with higher
order features computed from entire clauses. A
DCNN generalises many of the structural aspects
of a RecNN. The feature extraction function as
stated in Eq. 6 has a more general form than that
in a RecNN, where the value of m is generally 2.
Likewise, the induced graph structure in a DCNN
is more general than a parse tree in that it is not
limited to syntactically dictated phrases; the graph
structure can capture short or long-range seman-
tic relations between words that do not necessar-
ily correspond to the syntactic relations in a parse
tree. The DCNN has internal input-dependent
structure and does not rely on externally provided
parse trees, which makes the DCNN directly ap-
plicable to hard-to-parse sentences such as tweets
and to sentences from any language.
5 Experiments
We test the network on four different experiments.
We begin by specifying aspects of the implemen-
tation and the training of the network. We then re-
late the results of the experiments and we inspect
the learnt feature detectors.
5.1 Training
In each of the experiments, the top layer of the
network has a fully connected layer followed by
a softmax non-linearity that predicts the probabil-
ity distribution over classes given the input sen-
tence. The network is trained to minimise the
cross-entropy of the predicted and true distribu-
tions; the objective includes an L
2
regularisation
Classifier Fine-grained (%) Binary (%)
NB 41.0 81.8
BINB 41.9 83.1
SVM 40.7 79.4
RECNTN 45.7 85.4
MAX-TDNN 37.4 77.1
NBOW 42.4 80.5
DCNN 48.5 86.8
Table 1: Accuracy of sentiment prediction in the
movie reviews dataset. The first four results are
reported from Socher et al (2013b). The baselines
NB and BINB are Naive Bayes classifiers with,
respectively, unigram features and unigram and bi-
gram features. SVM is a support vector machine
with unigram and bigram features. RECNTN is a
recursive neural network with a tensor-based fea-
ture function, which relies on external structural
features given by a parse tree and performs best
among the RecNNs.
term over the parameters. The set of parameters
comprises the word embeddings, the filter weights
and the weights from the fully connected layers.
The network is trained with mini-batches by back-
propagation and the gradient-based optimisation is
performed using the Adagrad update rule (Duchi
et al, 2011). Using the well-known convolution
theorem, we can compute fast one-dimensional
linear convolutions at all rows of an input matrix
by using Fast Fourier Transforms. To exploit the
parallelism of the operations, we train the network
on a GPU. A Matlab implementation processes
multiple millions of input sentences per hour on
one GPU, depending primarily on the number of
layers used in the network.
5.2 Sentiment Prediction in Movie Reviews
The first two experiments concern the prediction
of the sentiment of movie reviews in the Stanford
Sentiment Treebank (Socher et al, 2013b). The
output variable is binary in one experiment and
can have five possible outcomes in the other: neg-
ative, somewhat negative, neutral, somewhat posi-
tive, positive. In the binary case, we use the given
splits of 6920 training, 872 development and 1821
test sentences. Likewise, in the fine-grained case,
we use the standard 8544/1101/2210 splits. La-
belled phrases that occur as subparts of the train-
ing sentences are treated as independent training
instances. The size of the vocabulary is 15448.
Table 1 details the results of the experiments.
661
Classifier Features Acc. (%)
HIER
unigram, POS, head chunks 91.0
NE, semantic relations
MAXENT
unigram, bigram, trigram 92.6
POS, chunks, NE, supertags
CCG parser, WordNet
MAXENT
unigram, bigram, trigram 93.6
POS, wh-word, head word
word shape, parser
hypernyms, WordNet
SVM
unigram, POS, wh-word 95.0
head word, parser
hypernyms, WordNet
60 hand-coded rules
MAX-TDNN unsupervised vectors 84.4
NBOW unsupervised vectors 88.2
DCNN unsupervised vectors 93.0
Table 2: Accuracy of six-way question classifica-
tion on the TREC questions dataset. The second
column details the external features used in the
various approaches. The first four results are re-
spectively from Li and Roth (2002), Blunsom et al
(2006), Huang et al (2008) and Silva et al (2011).
In the three neural sentence models?the Max-
TDNN, the NBoW and the DCNN?the word vec-
tors are parameters of the models that are ran-
domly initialised; their dimension d is set to 48.
The Max-TDNN has a filter of width 6 in its nar-
row convolution at the first layer; shorter phrases
are padded with zero vectors. The convolu-
tional layer is followed by a non-linearity, a max-
pooling layer and a softmax classification layer.
The NBoW sums the word vectors and applies a
non-linearity followed by a softmax classification
layer. The adopted non-linearity is the tanh func-
tion. The hyper parameters of the DCNN are as
follows. The binary result is based on a DCNN
that has a wide convolutional layer followed by a
folding layer, a dynamic k-max pooling layer and
a non-linearity; it has a second wide convolutional
layer followed by a folding layer, a k-max pooling
layer and a non-linearity. The width of the convo-
lutional filters is 7 and 5, respectively. The value
of k for the top k-max pooling is 4. The num-
ber of feature maps at the first convolutional layer
is 6; the number of maps at the second convolu-
tional layer is 14. The network is topped by a soft-
max classification layer. The DCNN for the fine-
grained result has the same architecture, but the
filters have size 10 and 7, the top pooling parame-
ter k is 5 and the number of maps is, respectively,
6 and 12. The networks use the tanh non-linear
Classifier Accuracy (%)
SVM 81.6
BINB 82.7
MAXENT 83.0
MAX-TDNN 78.8
NBOW 80.9
DCNN 87.4
Table 3: Accuracy on the Twitter sentiment
dataset. The three non-neural classifiers are based
on unigram and bigram features; the results are re-
ported from (Go et al, 2009).
function. At training time we apply dropout to the
penultimate layer after the last tanh non-linearity
(Hinton et al, 2012).
We see that the DCNN significantly outper-
forms the other neural and non-neural models.
The NBoW performs similarly to the non-neural
n-gram based classifiers. The Max-TDNN per-
forms worse than the NBoW likely due to the ex-
cessive pooling of the max pooling operation; the
latter discards most of the sentiment features of the
words in the input sentence. Besides the RecNN
that uses an external parser to produce structural
features for the model, the other models use n-
gram based or neural features that do not require
external resources or additional annotations. In the
next experiment we compare the performance of
the DCNN with those of methods that use heavily
engineered resources.
5.3 Question Type Classification
As an aid to question answering, a question may
be classified as belonging to one of many question
types. The TREC questions dataset involves six
different question types, e.g. whether the question
is about a location, about a person or about some
numeric information (Li and Roth, 2002). The
training dataset consists of 5452 labelled questions
whereas the test dataset consists of 500 questions.
The results are reported in Tab. 2. The non-
neural approaches use a classifier over a large
number of manually engineered features and
hand-coded resources. For instance, Blunsom et
al. (2006) present a Maximum Entropy model that
relies on 26 sets of syntactic and semantic fea-
tures including unigrams, bigrams, trigrams, POS
tags, named entity tags, structural relations from
a CCG parse and WordNet synsets. We evaluate
the three neural models on this dataset with mostly
the same hyper-parameters as in the binary senti-
662
POSITIVE
lovely	 	 	 	 	 comedic	 	 	 	 	 moments	 and	 	 	 	 several	 	 	 	 	 fine	 	 	 	 	 	 performances
good	 	 	 	 	 	 	 script	 	 	 	 	 	 ,	 	 	 	 	 	 	 good	 	 	 dialogue	 	 	 	 ,	 	 	 	 	 	 	 	 	 funny	 	 	 	 	 	 	 
sustains	 	 	 throughout	 	 is	 	 	 	 	 	 daring	 ,	 	 	 	 	 	 	 	 	 	 	 inventive	 and	 	 	 	 	 	 	 	 	 
well	 	 	 	 	 	 	 written	 	 	 	 	 ,	 	 	 	 	 	 	 nicely	 acted	 	 	 	 	 	 	 and	 	 	 	 	 	 	 beautifully	 
remarkably	 solid	 	 	 	 	 	 	 and	 	 	 	 	 subtly	 satirical	 	 	 tour	 	 	 	 	 	 de	 	 	 	 	 	 	 	 	 	 
NEGATIVE
,	 	 	 	 	 	 	 	 	 	 nonexistent	 plot	 	 	 	 and	 	 	 	 pretentious	 visual	 	 	 	 style	 	 	 	 	 	 	 
it	 	 	 	 	 	 	 	 	 fails	 	 	 	 	 	 	 the	 	 	 	 	 most	 	 	 basic	 	 	 	 	 	 	 test	 	 	 	 	 	 as	 	 	 	 	 	 	 	 	 	 
so	 	 	 	 	 	 	 	 	 stupid	 	 	 	 	 	 ,	 	 	 	 	 	 	 so	 	 	 	 	 ill	 	 	 	 	 	 	 	 	 conceived	 ,	 	 	 	 	 	 	 	 	 	 	 
,	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 	 	 dull	 	 	 	 and	 	 	 	 pretentious	 to	 	 	 	 	 	 	 	 be	 	 	 	 	 	 	 	 	 	 
hood	 	 	 	 	 	 	 rats	 	 	 	 	 	 	 	 butt	 	 	 	 their	 	 ugly	 	 	 	 	 	 	 	 heads	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 	 
'NOT'
n't	 	 	 	 have	 	 	 	 	 any	 	 	 	 	 	 	 	 	 huge	 laughs	 	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 its	 	 	 
no	 	 	 	 	 movement	 ,	 	 	 	 	 	 	 	 	 	 	 no	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 much	 	 
n't	 	 	 	 stop	 	 	 	 	 me	 	 	 	 	 	 	 	 	 	 from	 enjoying	 	 	 	 much	 	 	 	 	 	 	 	 	 of	 	 	 	 
not	 	 	 	 that	 	 	 	 	 kung	 	 	 	 	 	 	 	 pow	 	 is	 	 	 	 	 	 	 	 	 	 n't	 	 	 	 	 	 	 	 	 	 funny	 
not	 	 	 	 a	 	 	 	 	 	 	 	 moment	 	 	 	 	 	 that	 is	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 false	 
'TOO'
,	 	 	 	 	 	 too	 	 	 	 	 	 dull	 	 	 	 	 	 	 	 and	 	 pretentious	 to	 	 	 	 	 	 	 	 	 	 	 be	 	 	 	 	 	 	 	 
either	 too	 	 	 	 	 	 serious	 	 	 	 	 or	 	 	 too	 	 	 	 	 	 	 	 	 lighthearted	 ,	 	 	 	 	 	 	 	 	 
too	 	 	 	 slow	 	 	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 too	 	 long	 	 	 	 	 	 	 	 and	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 
feels	 	 too	 	 	 	 	 	 formulaic	 	 	 and	 	 too	 	 	 	 	 	 	 	 	 familiar	 	 	 	 	 to	 	 	 	 	 	 	 	 
is	 	 	 	 	 too	 	 	 	 	 	 predictable	 and	 	 too	 	 	 	 	 	 	 	 	 self	 	 	 	 	 	 	 	 	 conscious	 	 
Figure 4: Top five 7-grams at four feature detectors in the first layer of the network.
ment experiment of Sect. 5.2. As the dataset is
rather small, we use lower-dimensional word vec-
tors with d = 32 that are initialised with embed-
dings trained in an unsupervised way to predict
contexts of occurrence (Turian et al, 2010). The
DCNN uses a single convolutional layer with fil-
ters of size 8 and 5 feature maps. The difference
between the performance of the DCNN and that of
the other high-performing methods in Tab. 2 is not
significant (p < 0.09). Given that the only labelled
information used to train the network is the train-
ing set itself, it is notable that the network matches
the performance of state-of-the-art classifiers that
rely on large amounts of engineered features and
rules and hand-coded resources.
5.4 Twitter Sentiment Prediction with
Distant Supervision
In our final experiment, we train the models on a
large dataset of tweets, where a tweet is automat-
ically labelled as positive or negative depending
on the emoticon that occurs in it. The training set
consists of 1.6 million tweets with emoticon-based
labels and the test set of about 400 hand-annotated
tweets. We preprocess the tweets minimally fol-
lowing the procedure described in Go et al (2009);
in addition, we also lowercase all the tokens. This
results in a vocabulary of 76643 word types. The
architecture of the DCNN and of the other neural
models is the same as the one used in the binary
experiment of Sect. 5.2. The randomly initialised
word embeddings are increased in length to a di-
mension of d = 60. Table 3 reports the results of
the experiments. We see a significant increase in
the performance of the DCNN with respect to the
non-neural n-gram based classifiers; in the pres-
ence of large amounts of training data these clas-
sifiers constitute particularly strong baselines. We
see that the ability to train a sentiment classifier on
automatically extracted emoticon-based labels ex-
tends to the DCNN and results in highly accurate
performance. The difference in performance be-
tween the DCNN and the NBoW further suggests
that the ability of the DCNN to both capture fea-
tures based on long n-grams and to hierarchically
combine these features is highly beneficial.
5.5 Visualising Feature Detectors
A filter in the DCNN is associated with a feature
detector or neuron that learns during training to
be particularly active when presented with a spe-
cific sequence of input words. In the first layer, the
sequence is a continuous n-gram from the input
sentence; in higher layers, sequences can be made
of multiple separate n-grams. We visualise the
feature detectors in the first layer of the network
trained on the binary sentiment task (Sect. 5.2).
Since the filters have width 7, for each of the 288
feature detectors we rank all 7-grams occurring in
the validation and test sets according to their ac-
tivation of the detector. Figure 5.2 presents the
top five 7-grams for four feature detectors. Be-
sides the expected detectors for positive and nega-
tive sentiment, we find detectors for particles such
as ?not? that negate sentiment and such as ?too?
that potentiate sentiment. We find detectors for
multiple other notable constructs including ?all?,
?or?, ?with...that?, ?as...as?. The feature detectors
learn to recognise not just single n-grams, but pat-
terns within n-grams that have syntactic, semantic
or structural significance.
6 Conclusion
We have described a dynamic convolutional neural
network that uses the dynamic k-max pooling op-
erator as a non-linear subsampling function. The
feature graph induced by the network is able to
capture word relations of varying size. The net-
work achieves high performance on question and
sentiment classification without requiring external
features as provided by parsers or other resources.
Acknowledgements
We thank Nando de Freitas and Yee Whye Teh
for great discussions on the paper. This work was
supported by a Xerox Foundation Award, EPSRC
grant number EP/F042728/1, and EPSRC grant
number EP/K036580/1.
663
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP, pages 1183?1193. ACL.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear mod-
els. In SIGIR ?06: Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 615?616, New York, NY, USA. ACM.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical Foundations for a Com-
positional Distributional Model of Meaning. March.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Interna-
tional Conference on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing - EMNLP ?08,
(October):897.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Felix A. Gers and Jrgen Schmidhuber. 2001. Lstm
recurrent networks learn simple context-free and
context-sensitive languages. IEEE Transactions on
Neural Networks, 12(6):1333?1340.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Edward Grefenstette. 2013. Category-theoretic
quantitative compositional distributional models
of natural language semantics. arXiv preprint
arXiv:1311.1539.
Emiliano Guevara. 2010. Modelling Adjective-Noun
Compositionality by Regression. ESSLLI?10 Work-
shop on Compositionality and Distributional Se-
mantic Models.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Forthcoming.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artif. Intell., 40(1-3):185?234.
Zhiheng Huang, Marcus Thint, and Zengchang Qin.
2008. Question classification using head words and
their hypernyms. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?08, pages 927?936, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Recur-
rent continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing, Seattle, October. As-
sociation for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Recur-
rent Convolutional Neural Networks for Discourse
Compositionality. In Proceedings of the Workshop
on Continuous Vector Space Models and their Com-
positionality, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
Andreas K?uchler and Christoph Goller. 1996. Induc-
tive learning in symbolic domains using structure-
driven recurrent neural networks. In G?unther G?orz
and Steffen H?olldobler, editors, KI, volume 1137 of
Lecture Notes in Computer Science, pages 183?197.
Springer.
Yann LeCun, L?eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278?2324, November.
Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7. Association for Computational Linguis-
tics.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT, pages 234?239.
664
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP, pages 5528?5531. IEEE.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, volume 8.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46:77?105.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Joo Silva, Lusa Coheur, AnaCristina Mendes, and An-
dreas Wichert. 2011. From symbolic to sub-
symbolic information in question classification. Ar-
tificial Intelligence Review, 35(2):137?154.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Grounded Composi-
tional Semantics for Finding and Describing Images
with Sentences. In Transactions of the Association
for Computational Linguistics (TACL).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631?1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
J. Artif. Intell. Res.(JAIR), 44:533?585.
Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-
ton, Kiyohiro Shikano, and Kevin J. Lang. 1990.
Readings in speech recognition. chapter Phoneme
Recognition Using Time-delay Neural Networks,
pages 393?404. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
665
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 212?217,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Resolving Lexical Ambiguity in
Tensor Regression Models of Meaning
Dimitri Kartsaklis
University of Oxford
Department of
Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Nal Kalchbrenner
University of Oxford
Department of
Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
nkalch@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary Univ. of London
School of Electronic Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrnoosh.sadrzadeh@qmul.ac.uk
Abstract
This paper provides a method for improv-
ing tensor-based compositional distribu-
tional models of meaning by the addition
of an explicit disambiguation step prior to
composition. In contrast with previous re-
search where this hypothesis has been suc-
cessfully tested against relatively simple
compositional models, in our work we use
a robust model trained with linear regres-
sion. The results we get in two experi-
ments show the superiority of the prior dis-
ambiguation method and suggest that the
effectiveness of this approach is model-
independent.
1 Introduction
The provision of compositionality in distributional
models of meaning, where a word is represented as
a vector of co-occurrence counts with every other
word in the vocabulary, offers a solution to the
fact that no text corpus, regardless of its size, is
capable of providing reliable co-occurrence statis-
tics for anything but very short text constituents.
By composing the vectors for the words within
a sentence, we are still able to create a vectorial
representation for that sentence that is very useful
in a variety of natural language processing tasks,
such as paraphrase detection, sentiment analysis
or machine translation. Hence, given a sentence
w
1
w
2
. . . w
n
, a compositional distributional model
provides a function f such that:
??
s = f(
??
w
1
,
??
w
2
, . . . ,
??
w
n
) (1)
where
??
w
i
is the distributional vector of the ith
word in the sentence and
??
s the resulting compos-
ite sentential vector.
An interesting question that has attracted the at-
tention of researchers lately refers to the way in
which these models affect ambiguous words; in
other words, given a sentence such as ?a man was
waiting by the bank?, we are interested to know to
what extent a composite vector can appropriately
reflect the intended use of word ?bank? in that con-
text, and how such a vector would differ, for exam-
ple, from the vector of the sentence ?a fisherman
was waiting by the bank?.
Recent experimental evidence (Reddy et al,
2011; Kartsaklis et al, 2013; Kartsaklis and
Sadrzadeh, 2013) suggests that for a number of
compositional models the introduction of a dis-
ambiguation step prior to the actual composi-
tional process results in better composite represen-
tations. In other words, the suggestion is that Eq.
1 should be replaced by:
??
s = f(?(
??
w
1
), ?(
??
w
2
), . . . , ?(
??
w
n
)) (2)
where the purpose of function ? is to return a dis-
ambiguated version of each word vector given the
rest of the context (e.g. all the other words in the
sentence). The composition operation, whatever
that could be, is then applied on these unambigu-
ous representations of the words, instead of the
original distributional vectors.
Until now this idea has been verified on rela-
tively simple compositional functions, usually in-
volving some form of element-wise operation be-
tween the word vectors, such as addition or mul-
tiplication. An exception to this is the work of
Kartsaklis and Sadrzadeh (2013), who apply Eq.
2 on partial tensor-based compositional models.
In a tensor-based model, relational words such
as verbs and adjectives are represented by multi-
linear maps; composition takes place as the ap-
plication of those maps on vectors representing
the arguments (usually nouns). What makes the
models of the above work ?partial? is that the au-
thors used simplified versions of the linear maps,
projected onto spaces of order lower than that re-
quired by the theoretical framework. As a result,
a certain amount of transformational power was
traded off for efficiency.
A potential explanation then for the effective-
ness of the proposed prior disambiguation method
can be sought on the limitations imposed by the
compositional models under test. After all, the
idea of having disambiguation emerge as a direct
212
consequence of the compositional process, with-
out the introduction of any explicit step, seems
more natural and closer to the way the human
mind resolves lexical ambiguities.
The purpose of this paper is to investigate
the hypothesis whether prior disambiguation is
important in a pure tensor-based compositional
model, where no simplifying assumptions have
been made. We create such a model by using lin-
ear regression, and we explain how an explicit dis-
ambiguation step can be introduced to this model
prior to composition. We then proceed by com-
paring the composite vectors produced by this ap-
proach with those produced by the model alone in
a number of experiments. The results show a clear
superiority of the priorly disambiguated models
following Eq. 2, confirming previous research and
suggesting that the reasons behind the success of
this approach are more fundamental than the form
of the compositional function.
2 Composition in distributional models
Compositional distributional models of meaning
vary in sophistication, from simple element-wise
operations between vectors such as addition and
multiplication (Mitchell and Lapata, 2008) to deep
learning techniques based on neural networks
(Socher et al, 2011; Socher et al, 2012; Kalch-
brenner and Blunsom, 2013a). Tensor-based mod-
els, formalized by Coecke et al (2010), comprise
a third class of models lying somewhere in be-
tween these two extremes. Under this setting rela-
tional words such as verbs and adjectives are rep-
resented by multi-linear maps (tensors of various
orders) acting on a number of arguments. An ad-
jective for example is a linear map f : N ? N
(where N is our basic vector space for nouns),
which takes as input a noun and returns a mod-
ified version of it. Since every map of this sort
can be represented by a matrix living in the ten-
sor product space N ? N , we now see that the
meaning of a phrase such as ?red car? is given by
red ?
??
car, where red is an adjective matrix and
? indicates matrix multiplication. The same con-
cept applies for functions of higher order, such as
a transitive verb (a function of two arguments, so
a tensor of order 3). For these cases, matrix mul-
tiplication generalizes to the more generic notion
of tensor contraction. The meaning of a sentence
such as ?kids play games? is computed as:
???
kids
T
? play ?
?????
games (3)
where play here is an order-3 tensor (a ?cube?)
and ? now represents tensor contraction. A con-
cise introduction to compositional distributional
models can be found in (Kartsaklis, 2014).
3 Disambiguation and composition
The idea of separating disambiguation from com-
position first appears in a work of Reddy et al
(2011), where the authors show that the intro-
duction of an explicit disambiguation step prior
to simple element-wise composition is beneficial
for noun-noun compounds. Subsequent work by
Kartsaklis et al (2013) reports very similar find-
ings for verb-object structures, again on additive
and multiplicative models. Finally, in (Kartsaklis
and Sadrzadeh, 2013) these experiments were ex-
tended to include tensor-based models following
the categorical framework of Coecke et al (2010),
where again all ?unambiguous? models present
superior performance compared to their ?ambigu-
ous? versions.
However, in this last work one of the dimen-
sions of the tensors was kept empty (filled in
with zeros). This simplified the calculations but
also weakened the effectiveness of the multi-linear
maps. If, for example, instead of using an order-3
tensor for a transitive verb, one uses some of the
matrix instantiations of Kartsaklis and Sadrzadeh,
Eq. 3 is reduced to one of the following forms:
play  (
???
kids?
?????
games) ,
???
kids (play ?
?????
games)
(
???
kids
T
? play)
?????
games
(4)
where symbol  denotes element-wise multipli-
cation and play is a matrix. Here, the model does
not fully exploit the space provided by the theo-
retical framework (i.e. an order-3 tensor), which
has two disadvantages: firstly, we lose space that
could hold valuable information about the verb in
this case and relational words in general; secondly,
the generally non-commutative tensor contraction
operation is now partly relying on element-wise
multiplication, which is commutative, thus forgets
(part of the) order of composition.
In the next section we will see how to apply lin-
ear regression in order to create full tensors for
verbs and use them for a compositional model that
avoids these pitfalls.
4 Creating tensors for verbs
The essence of any tensor-based compositional
model is the way we choose to create our sentence-
producing maps, i.e. the verbs. In this paper we
adopt a method proposed by Baroni and Zampar-
elli (2010) for building adjective matrices, which
can be generally applied to any relational word.
213
In order to create a matrix for, say, the intransi-
tive verb ?play?, we first collect all instances of
the verb occurring with some subject in the train-
ing corpus, and then we create non-compositional
holistic vectors for these elementary sentences fol-
lowing exactly the same methodology as if they
were words. We now have a dataset with instances
of the form ?
????
subj
i
,
???????
subj
i
play? (e.g. the vector of
?kids? paired with the holistic vector of ?kids play?,
and so on), that can be used to train a linear regres-
sion model in order to produce an appropriate ma-
trix for verb ?play?. The premise of a model like
this is that the multiplication of the verb matrix
with the vector of a new subject will produce a re-
sult that approximates the distributional behaviour
of all these elementary two-word exemplars used
in training.
We present examples and experiments based
on this method, constructing ambiguous and dis-
ambiguated tensors of order 2 (that is, matrices)
for verbs taking one argument. In principle, our
method is directly applicable to tensors of higher
order, following a multi-step process similar to
that of Grefenstette et al (2013) who create order-
3 tensors for transitive verbs using similar means.
Instead of using subject-verb constructs as above
we concentrate on elementary verb phrases of the
form verb-object (e.g. ?play football?, ?admit stu-
dent?), since in general objects comprise stronger
contexts for disambiguating the usage of a verb.
5 Experimental setting
Our basic vector space is trained from the ukWaC
corpus (Ferraresi et al, 2008), originally using as
a basis the 2,000 content words with the highest
frequency (but excluding a list of stop words as
well as the 50 most frequent content words since
they exhibit low information content). We cre-
ated vectors for all content words with at least
100 occurrences in the corpus. As context we
considered a 5-word window from either side of
the target word, while as our weighting scheme
we used local mutual information (i.e. point-wise
mutual information multiplied by raw counts).
This initial semantic space achieved a score of
0.77 Spearman?s ? (and 0.71 Pearson?s r) on the
well-known benchmark dataset of Rubenstein and
Goodenough (1965). In order to reduce the time of
regression training, our vector space was normal-
ized and projected onto a 300-dimensional space
using singular value decomposition (SVD). The
performance of the reduced space on the R&G
dataset was again very satisfying, specifically 0.73
Spearman?s ? and 0.72 Pearson?s r.
In order to create the vector space of the holistic
verb phrase vectors, we first collected all instances
where a verb participating in the experiments ap-
peared at least 100 times in a verb-object relation-
ship with some noun in the corpus. As context of
a verb phrase we considered any content word that
falls into a 5-word window from either side of the
verb or the object. For the 68 verbs participating
in our experiments, this procedure resulted in 22k
verb phrases, a vector space that again was pro-
jected into 300 dimensions using SVD.
Linear regression For each verb we use simple
linear regression with gradient descent directly ap-
plied on matrices X and Y, where the rows of X
correspond to vectors of the nouns that appear as
objects for the given verb and the rows ofY to the
holistic vectors of the corresponding verb phrases.
Our objective function then becomes:
?
W = argmin
W
1
2m
(
?WX
T
?Y
T
?
2
+ ??W?
2
)
(5)
wherem is the number of training examples and ?
a regularization parameter. The matrix W is used
as the tensor for the specific verb.
6 Supervised disambiguation
In our first experiment we test the effectiveness
of a prior disambiguation step for a tensor-based
model in a ?sandbox? using supervised learning.
The goal is to create composite vectors for a num-
ber of elementary verb phrases of the form verb-
object with and without an explicit disambiguation
step, and evaluate which model approximates bet-
ter the holistic vectors of these verb phrases.
The verb phrases of our dataset are based on the
5 ambiguous verbs of Table 1. Each verb has been
combined with two different sets of nouns that ap-
pear in a verb-object relationship with that verb
in the corpus (a total of 343 verb phrases). The
nouns of each set have been manually selected in
order to explicitly represent a different meaning of
the verb. As an example, in the verb ?play? we im-
pose the two distinct meanings of using a musical
instrument and participating in a sport; so the first
Verb Meaning 1 Meaning 2
break violate (56) break (22)
catch capture (28) be on time (21)
play musical instrument (47) sports (29)
admit permit to enter (12) acknowledge (25)
draw attract (64) sketch (39)
Table 1: Ambiguous verbs for the supervised task.
The numbers in parentheses refer to the collected
training examples for each case.
214
set of objects contains nouns such as ?oboe?, ?pi-
ano?, ?guitar?, and so on, while in the second set
we see nouns such as ?football?, ?baseball? etc.
In more detail, the creation of the dataset was
done in the following way: First, all verb entries
with more than one definition in the Oxford Junior
Dictionary (Sansome et al, 2000) were collected
into a list. Next, a linguist (native speaker of En-
glish) annotated the semantic difference between
the definitions of each verb in a scale from 1 (sim-
ilar) to 5 (distinct). Only verbs with definitions
exhibiting completely distinct meanings (marked
with 5) were kept for the next step. For each one
of these verbs, a list was constructed with all the
nouns that appear at least 50 times under a verb-
object relationship in the corpus with the specific
verb. Then, each object in the list was manually
annotated as exclusively belonging to one of the
two senses; so, an object could be selected only if
it was related to a single sense, but not both. For
example, ?attention? was a valid object for the at-
tract sense of verb ?draw?, since it is unrelated to
the sketch sense of that verb. On the other hand,
?car? is not an appropriate object for either sense
of ?draw?, since it could actually appear under both
of them in different contexts. The verbs of Table
1 were the ones with the highest numbers of ex-
emplars per sense, creating a dataset of significant
size for the intended task (each holistic vector is
compared with 343 composite vectors).
We proceed as follows: We apply linear regres-
sion in order to train verb matrices using jointly
the object sets for both meanings of each verb, as
well as separately?so in this latter case we get
two matrices for each verb, one for each sense. For
each verb phrase, we create a composite vector by
matrix-multiplying the verb matrix with the vector
of the specific object. Then we use 4-fold cross
validation to evaluate which version of composite
vectors (the one created by the ambiguous tensors
or the one created by the unambiguous ones) ap-
proximates better the holistic vectors of the verb
phrases in our test set. This is done by comparing
each holistic vector with all the composite ones,
and then evaluating the rank of the correct com-
posite vector within the list of results.
In order to get a proper mixing of objects from
both senses of a verb in training and testing sets,
we set the cross-validation process as follows: We
first split both sets of objects in 4 parts. For each
fold then, our training set is comprised by
3
4
of set
#1 plus
3
4
of set #2, while the test set consists of
the remaining
1
4
of set #1 plus
1
4
of set #2. The
data points of the training set are presented in the
Accuracy MRR Avg Sim
Amb. Dis. Amb. Dis. Amb. Dis.
break 0.19 0.28 0.41 0.50 0.41 0.43
catch 0.35 0.37 0.58 0.61 0.51 0.57
play 0.20 0.28 0.41 0.49 0.60 0.68
admit 0.33 0.43 0.57 0.64 0.41 0.46
draw 0.24 0.29 0.45 0.51 0.40 0.44
Table 2: Results for the supervised task. ?Amb.?
refers to models without the explicit disambigua-
tion step, and ?Dis.? to models with that step.
learning algorithm in random order.
We measure approximation in three different
metrics. The first one, accuracy, is the strictest,
and evaluates in how many cases the composite
vector of a verb phrase is the closest one (the first
one in the result list) to the corresponding holistic
vector. A more relaxed and perhaps more repre-
sentative method is to calculate the mean recipro-
cal rank (MRR), which is given by:
MRR =
1
m
m
?
i=1
1
rank
i
(6)
where m is the number of objects and rank
i
refers
to the rank of the correct composite vector for the
ith object.
Finally, a third way to evaluate the efficiency of
each model is to simply calculate the average co-
sine similarity between every holistic vector and
its corresponding composite vector. The results
are presented in Table 2, reflecting a clear supe-
riority (p < 0.001 for average cosine similarity)
of the prior disambiguation method for every verb
and every metric.
7 Unsupervised disambiguation
In Section 6 we used a controlled procedure to col-
lect genuinely ambiguous verbs and we trained our
models from manually annotated data. In this sec-
tion we briefly outline how the process of creat-
ing tensors for distinct senses of a verb can be au-
tomated, and we test this idea on a generic verb
phrase similarity task.
First, we use unsupervised learning in order to
detect the latent senses of each verb in the corpus,
following a procedure first described by Sch?utze
(1998). For every occurrence of the verb, we cre-
ate a vector representing the surrounding context
by averaging the vectors of every other word in
the same sentence. Then, we apply hierarchical
agglomerative clustering (HAC) in order to cluster
these context vectors, hoping that different groups
of contexts will correspond to the different senses
under which the word has been used in the corpus.
The clustering algorithm uses Ward?s method as
215
inter-cluster measure, and Pearson correlation for
measuring the distance of vectors within a clus-
ter. Since HAC returns a dendrogram embedding
all possible groupings, we measure the quality of
each partitioning by using the variance ratio crite-
rion (Cali?nski and Harabasz, 1974) and we select
the partitioning that achieves the best score (so the
number of senses varies from verb to verb).
The next step is to classify every noun that has
been used as an object with that verb to the most
probable verb sense, and then use these sets of
nouns as before for training tensors for the vari-
ous verb senses. Being equipped with a number of
sense clusters created as above for every verb, the
classification of each object to a relevant sense is
based on the cosine distance of the object vector
from the centroids of the clusters.
1
Every sense
with less than 3 training exemplars is merged to
the dominant sense of the verb. The union of all
object sets is used for training a single unambigu-
ous tensor for the verb. As usual, data points are
presented to learning algorithm in random order.
No objects in our test set are used for training.
We test this system on a verb phase similarity
task introduced in (Mitchell and Lapata, 2010).
The goal is to assess the similarity between pairs
of short verb phrases (verb-object constructs) and
evaluate the results against human annotations.
The dataset consists of 72 verb phrases, paired
in three different ways to form groups of various
degrees of phrase similarity?a total of 108 verb
phrase pairs.
The experiment has the following form: For ev-
ery pair of verb phrases, we construct composite
vectors and then we evaluate their cosine similar-
ity. For the ambiguous regression model, the com-
position is done by matrix-multiplying the am-
biguous verb matrix (learned by the union of all
object sets) with the vector of the noun. For the
disambiguated version, we first detect the most
probable sense of the verb given the noun, again
by comparing the vector of the noun with the
centroids of the verb clusters; then, we matrix-
multiply the corresponding unambiguous tensor
created exclusively from objects that have been
classified as closer to this specific sense of the
verb with the noun. We also test a number
of baselines: the ?verbs-only? model is a non-
compositional baseline where only the two verbs
are compared; ?additive? and ?multiplicative? com-
pose the word vectors of each phrase by applying
simple element-wise operations.
1
In general, our approach is quite close to the multi-
prototype models of Reisinger and Mooney (2010).
Model Spearman?s ?
Verbs-only 0.331
Additive 0.379
Multiplicative 0.301
Linear regression (ambiguous) 0.349
Linear regression (disamb.) 0.399
Holistic verb phrase vectors 0.403
Human agreement 0.550
Table 3: Results for the phrase similarity task. The
difference between the ambiguous and the disam-
biguated version is s.s. with p < 0.001.
The results are presented in Table 3, where
again the version with the prior disambiguation
step shows performance superior to that of the am-
biguous version. There are two interesting obser-
vations that can be made on the basis of Table
3. First of all, the regression model is based on
the assumption that the holistic vectors of the ex-
emplar verb phrases follow an ideal distributional
behaviour that the model aims to approximate as
close as possible. The results of Table 3 confirm
this: using just the holistic vectors of the corre-
sponding verb phrases (no composition is involved
here) returns the best correlation with human an-
notations (0.403), providing a proof that the holis-
tic vectors of the verb phrases are indeed reli-
able representations of each verb phrase?s mean-
ing. Next, observe that the prior disambiguation
model approximates this behaviour very closely
(0.399) on unseen data, with a difference not sta-
tistically significant. This is very important, since
a regression model can only perform as well as its
training dataset alows it; and in our case this is
achieved to a very satisfactory level.
8 Conclusion and future work
This paper adds to existing evidence from previ-
ous research that the introduction of an explicit
disambiguation step before the composition im-
proves the quality of the produced composed rep-
resentations. The use of a robust regression model
rejects the hypothesis that the proposed methodol-
ogy is helpful only for relatively ?weak? composi-
tional approaches. As for future work, an interest-
ing direction would be to see how a prior disam-
biguation step can affect deep learning composi-
tional settings similar to (Socher et al, 2012) and
(Kalchbrenner and Blunsom, 2013b).
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Support
by EPSRC grant EP/F042728/1 is gratefully ac-
knowledged by D. Kartsaklis and M. Sadrzadeh.
216
References
M. Baroni and R. Zamparelli. 2010. Nouns are Vec-
tors, Adjectives are Matrices. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
T. Cali?nski and J. Harabasz. 1974. A Dendrite Method
for Cluster Analysis. Communications in Statistics-
Theory and Methods, 3(1):1?27.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for Distributed Compositional
Model of Meaning. Lambek Festschrift. Linguistic
Analysis, 36:345?384.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
N. Kalchbrenner and P. Blunsom. 2013a. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Compo-
sitionality, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA, October. Association for Computational Lin-
guistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2013.
Separating Disambiguation from Composition in
Distributional Semantics. In Proceedings of 17th
Conference on Computational Natural Language
Learning (CoNLL-2013), Sofia, Bulgaria, August.
Dimitri Kartsaklis. 2014. Compositional operators in
distributional semantics. Springer Science Reviews,
April. DOI: 10.1007/s40362-014-0017-z.
J. Mitchell and M. Lapata. 2008. Vector-based Mod-
els of Semantic Composition. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 705?713.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109?117. Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of
the ACM, 8(10):627?633.
R. Sansome, D. Reid, and A. Spooner. 2000. The Ox-
ford Junior Dictionary. Oxford University Press.
H. Sch?utze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:97?123.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and
C.D. Manning. 2011. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. Advances in Neural Information Processing
Systems, 24.
R. Socher, B. Huval, C. Manning, and Ng. A.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Conference on Empirical
Methods in Natural Language Processing 2012.
217
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119?126,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of sen-
tences, so do sentences combine to form
the meaning of paragraphs, dialogues and
general discourse. We introduce both a
sentence model and a discourse model cor-
responding to the two levels of composi-
tionality. The sentence model adopts con-
volution as the central operation for com-
posing semantic vectors and is based on
a novel hierarchical convolutional neural
network. The discourse model extends the
sentence model and is based on a recur-
rent neural network that is conditioned in
a novel way both on the current sentence
and on the current speaker. The discourse
model is able to capture both the sequen-
tiality of sentences and the interaction be-
tween different speakers. Without feature
engineering or pretraining and with simple
greedy decoding, the discourse model cou-
pled to the sentence model obtains state of
the art performance on a dialogue act clas-
sification experiment.
1 Introduction
There are at least two levels at which the mean-
ing of smaller linguistic units is composed to form
the meaning of larger linguistic units. The first
level is that of sentential compositionality, where
the meaning of words composes to form the mean-
ing of the sentence or utterance that contains them
(Frege, 1892). The second level extends beyond
the first and involves general discourse composi-
tionality, where the meaning of multiple sentences
or utterances composes to form the meaning of
the paragraph, document or dialogue that com-
prises them (Korta and Perry, 2012; Potts, 2011).
The problem of discourse compositionality is the
problem of modelling how the meaning of general
discourse composes from the meaning of the sen-
tences involved and, since the latter in turn stems
from the meaning of the words, how the meaning
of discourse composes from the words themselves.
Tackling the problem of discourse composition-
ality promises to be central to a number of differ-
ent applications. These include sentiment or topic
classification of single sentences within the con-
text of a longer discourse, the recognition of di-
alogue acts within a conversation, the classifica-
tion of a discourse as a whole and the attainment
of general unsupervised or semi-supervised repre-
sentations of a discourse for potential use in di-
alogue tracking and question answering systems
and machine translation, among others.
To this end much work has been done on mod-
elling the meaning of single words by way of se-
mantic vectors (Turney and Pantel, 2010; Col-
lobert and Weston, 2008) and the latter have found
applicability in areas such as information retrieval
(Jones et al, 2006). With regard to modelling
the meaning of sentences and sentential compo-
sitionality, recent proposals have included sim-
ple additive and multiplicative models that do
not take into account sentential features such as
word order or syntactic structure (Mitchell and
Lapata, 2010), matrix-vector based models that
do take into account such features but are lim-
ited to phrases of a specific syntactic type (Ba-
roni and Zamparelli, 2010) and structured mod-
els that fully capture such features (Grefenstette et
al., 2011) and are embedded within a deep neu-
ral architecture (Socher et al, 2012; Hermann and
Blunsom, 2013). It is notable that the additive
and multiplicative models as well as simple, non-
compositional bag of n-grams and word vector av-
eraging models have equalled or outperformed the
structured models at certain phrase similarity (Bla-
coe and Lapata, 2012) and sentiment classifica-
119
tion tasks (Scheible and Schu?tze, 2013; Wang and
Manning, 2012).
With regard to discourse compositionality, most
of the proposals aimed at capturing semantic as-
pects of paragraphs or longer texts have focused
on bag of n-grams or sentence vector averaging
approaches (Wang and Manning, 2012; Socher et
al., 2012). In addition, the recognition of dialogue
acts within dialogues has largely been treated in
non-compositional ways by way of language mod-
els coupled to hidden Markov sequence models
(Stolcke et al, 2000). Principled approaches to
discourse compositionality have largely been un-
explored.
We introduce a novel model for sentential com-
positionality. The composition operation is based
on a hierarchy of one dimensional convolutions.
The convolutions are applied feature-wise, that is
they are applied across each feature of the word
vectors in the sentence. The weights adopted in
each convolution are different for each feature, but
do not depend on the different words being com-
posed. The hierarchy of convolution operations
involves a sequence of convolution kernels of in-
creasing sizes (Fig. 1). This allows for the com-
position operation to be applied to sentences of
any length, while keeping the model at a depth
of roughly ?2l where l is the length of the sen-
tence. The hierarchy of feature-wise convolution
operations followed by sigmoid non-linear acti-
vation functions results in a hierarchical convo-
lutional neural network (HCNN) based on a con-
volutional architecture (LeCun et al, 2001). The
HCNN shares with the structured models the as-
pect that it is sensitive to word order and adopts a
hierarchical architecture, although it is not based
on explicit syntactic structure.
We also introduce a novel model for discourse
compositionality. The discourse model is based
on a recurrent neural network (RNN) architecture
that is a powerful model for sequences (Sutskever
et al, 2011; Mikolov et al, 2010). The model
aims at capturing two central aspects of discourse
and its meaning: the sequentiality of the sentences
or utterances in the discourse and, where applica-
ble, the interactions between the different speak-
ers. The underlying RNN has its recurrent and out-
put weights conditioned on the respective speaker,
while simultaneously taking as input at every turn
the sentence vector for the current sentence gener-
ated through the sentence model (Fig. 2).
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential co positionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.
We here introduce
1 Introduction
2 Compositionality Models
2.1 Sentenc Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were tak n from th formatting instructions
of the International Joint Conference on Artificial
Intelligence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 General Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.
6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.
We here introduce
1 Introduction
2 Compositionality Models
2.1 Sentence Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference n Artificial
Intell gence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 General Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.
6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to f rmat
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox. .uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just s words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.
We here i troduce
1 Introduction
2 Compositionality Models
2.1 Sentence Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and arlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference on Artificial
Intelligence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 General Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Typ sing e-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.
6.1 Electronically-available sources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to f rmat
Recurrent Convolutional Neural Networks for Discourse Compositionality
Nal Kalchbrenner
Department of Computer Science
Oxford University
nkalch@cs.ox.ac.uk
Phil Blunsom
Department of Computer Science
Oxford University
pblunsom@cs.ox.ac.uk
Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so d sentences in turn combin se-
quentially to form the meaning of general
discourse. Discourse ay take the form
of paragraph , soliloqui or conversations
between multiple speakers. The problem
of ross-sentential compositionality is the
problem of modelling how the me ning
of th various forms of discourse arises
from the meaning of the utterances and t
words inv lved.
We here introduce
1 Introduction
2 Compositionality Models
2.1 Sentence Model
2.2 Discourse Model
3
4 Credits
This document has been adapted from the instruc-
tions for earlier ACL proceeding , includ ng those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Cha g and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou N
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and e rlier ACL and
EACL formats. Those versions re written by
sever l people, including John Chen, Henry S.
T ompson and Donald Walker. Additional le-
ments were t ken from the formatting instructions
of the International Joint Conferenc on Artificial
Intelligence.
5 Introduction
Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse
Sentence (n)
Agent (n)
Agent (n  1)
Words (n)
Class (n)
Class (n  1)
k = 2 k = 3 k = 4
6 G neral Instructions
Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors? names and complete addresses, which
must be centered at the top of the first page, and
any full-widt figures or tables (see the guidelines
in Subsectio 6.5). Type single-spaced. Start
all pag s directly under the top margin. S e the
guidelines later regarding formatting the first pag .
The manuscript should be printed si gle-sided and
its len th should not xceed the maximum pag
limit described in Section 8. Do not number the
pages.
6.1 Electronically-availabl resources
ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format
Figur 1: A hierarchical convolutional neural net-
work f r sentential compositionality. The bottom
layer represents a single feature across all the word
vectors in the sentence. The top layer is the value
for that feature in the r ulting sent nc vector.
Lines represent single weights and color coded
lines indicate sharing of weights. The parameter
k indicates the size of the convolution kernel at
the corresponding layer.
We experiment with the discourse model cou-
pled to the sentence model on the task of recog-
nizing dialogue acts of utterances within a conver-
sation. The dataset is given by 1134 transcribed
and annotated telephone conversations amounting
to about 200K utterances from the Switchboard
Dialogue Act Corpus (Calhoun et al, 2010).1 The
model is trained in a supervised setting without
previous pretraining; word vectors are also ran-
domly initialised. The model learns a probability
distribution over the dialogue acts at step i given
the sequence of utterances up to step i, the se-
quence of acts up to the previous step i?1 and the
binary sequence of agents up to the current step
i. Predicting the sequence of dialogue acts is per-
formed in a greedy fashion.2
We proceed as follows. In Sect. 2 we give the
motivation and the definition for the HCNN sen-
tence model. In Sect. 3 we do the same for the
RCNN discourse model. In Sect. 4 we describe
the dialogue act classification experiment and the
training procedure. We also inspect the discourse
vector representations produced by the model. We
conclude in Sect. 5.
1The dataset is available at compprag.
christopherpotts.net/swda.html
2Code and trained model available at nal.co
120
SI
O
i
x
i-1
P(x )
i
i-1
H
Hello
HAL 
do you read me
s
i
Figure 2: Recurrent convolutional neural network
(RCNN) discourse model based on a RNN archi-
tecture. At each step the RCNN takes as input
the current sentence vector si generated through
the HCNN sentence model and the previous label
xi?1 to predict a probability distribution over the
current label P (xi). The recurrent weights Hi?1
are conditioned on the previous agent ai?1 and
the output weights are conditioned on the current
agent ai. Note also the sentence matrix Ms of the
sentence model and the hierarchy of convolutions
applied to each feature that is a row in Ms to pro-
duce the corresponding feature in si.
2 Sentence Model
The general aim of the sentence model is to com-
pute a vector for a sentence s given the sequence
of words in s and a vector for each of the words.
The computation captures certain general consid-
erations regarding sentential compositionality. We
first relate such considerations and we then pro-
ceed to give a definition of the model.
2.1 Sentential compositionality
There are three main aspects of sentential compo-
sitionality that the model aims at capturing. To
relate these, it is useful to note the following basic
property of the model: a sentence s is paired to the
matrix Ms whose columns are given sequentially
by the vectors of the words in s. A row in Ms cor-
responds to the values of the corresponding feature
across all the word vectors. The first layer of the
network in Fig. 1 represents one such row of Ms,
whereas the whole matrix Ms is depicted in Fig.
2. The three considerations are as follows.
First, at the initial stage of the composition,
the value of a feature in the sentence vector is
a function of the values of the same feature in
the word vectors. That is, the m-th value in the
sentence vector of s is a function of the m-th
row of Ms. This aspect is preserved in the ad-
ditive and multiplicative models where the com-
position operations are, respectively, addition +
and component-wise multiplication . The cur-
rent model preserves the aspect up to the compu-
tation of the sentence vector s by adopting one-
dimensional, feature-wise convolution operations.
Subsequently, the discourse model that uses the
sentence vector s includes transformations across
the features of s (the transformation S in Fig. 2).
The second consideration concerns the hierar-
chical aspect of the composition operation. We
take the compositionality of meaning to initially
yield local effects across neighbouring words and
then yield increasingly more global effects across
all the words in the sentence. Composition oper-
ations like those in the structured models that are
guided by the syntactic parse tree of the sentence
capture this trait. The sentence model preserves
this aspect not by way of syntactic structure, but
by adopting convolution kernels of gradually in-
creasing sizes that span an increasing number of
words and ultimately the entire sentence.
The third aspect concerns the dependence of the
composition operation. The operation is taken to
depend on the different features, but not on the dif-
ferent words. Word specific parameters are intro-
duced only by way of the learnt word vectors, but
no word specific operations are learnt. We achieve
this by using a single convolution kernel across a
feature, and by utilizing different convolution ker-
nels for different features. Given these three as-
pects of sentential compositionality, we now pro-
ceed to describe the sentence model in detail.
2.2 Hierarchical Convolutional Neural
Network
The sentence model is taken to be a CNN where
the convolution operation is applied one dimen-
sionally across a single feature and in a hierarchi-
cal manner. To describe it in more detail, we first
recall the convolution operation that is central to
the model. Then we describe how we compute the
sequence of kernel sizes and how we determine the
hierarchy of layers in the network.
121
kk
k
k
m
m
m
m
1
1
2
2
3
3
4
4
(k   m)
1
*
Figure 3: Convolution of a vector m with a kernel
k of size 4.
2.2.1 Kernel and One-dimensional
Convolution
Given a sentence s and its paired matrix Ms, let
m be a feature that is a row in Ms. Before
defining kernels and the convolution operation,
let us consider the underlying operation of local
weighted addition. Let w1, ..., wk be a sequence
of k weights; given the feature m, local weighted
addition over the first k values of m gives:
y = w1m1 + ...+ wkmk (1)
Then, a kernel simply defines the value of k
by specifying the sequence of weights w1, ..., wk
and the one-dimensional convolution applies local
weighted addition with the k weights to each sub-
sequence of values of m.
More precisely, let a one-dimensional kernel k
be a vector of weights and assume |k| ? |m|,
where | ? | is the number of elements in a vec-
tor. Then we define the discrete, valid, one-
dimensional convolution (k ?m) of kernel k and
feature m by:
(k ?m)i :=
k?
j=1
kj ?mk+i?j (2)
where k = |k| and |k ?m| = |m| ? k + 1. Each
value in k ?m is a sum of k values of m weighted
by values in k (Fig. 3). To define the hierarchical
architecture of the model, we need to define a se-
quence of kernel sizes and associated weights. To
this we turn next.
2.2.2 Sequence of Kernel Sizes
Let l be the number of words in the sentence
s. The sequence of kernel sizes ?kli?i?t depends
only on the length of s and itself has length t =
d
?
2le ? 1. It is given recursively by:
kl1 = 2, k
l
i+1 = k
l
i + 1, k
l
t = l ?
t?1?
j=1
(klj ? 1)
(3)
That is, kernel sizes increase by one until the re-
sulting convolved vector is smaller or equal to the
last kernel size; see for example the kernel sizes in
Fig. 1. Note that, for a sentence of length l, the
number of layers in the HCNN including the input
layer will be t + 1 as convolution with the cor-
responding kernel is applied at every layer of the
model. Let us now proceed to define the hierarchy
of layers in the HCNN.
2.2.3 Composition Operation in a HCNN
Given a sentence s, its length l and a sequence
of kernel sizes ?kli?i?t, we may now give the
recursive definition that yields the hierarchy of
one-dimensional convolution operations applied
to each feature f that is a row in Ms. Specifi-
cally, for each feature f , let Kfi be a sequence of
t kernels, where the size of the kernel |Kfi | = kli.
Then we have the hierarchy of matrices and corre-
sponding features as follows:
M1f,: = M
s
f,: (4)
Mi+1f,: = ?( K
f
i ?Mif,: + bif ) (5)
for some non-linear sigmoid function ? and bias
bif , where i ranges over 1, ..., t. In sum, one-
dimensional convolution is applied feature-wise to
each feature of a matrix at a certain layer, where
the kernel weights depend both on the layer and
the feature at hand (Fig. 1). A hierarchy of matri-
ces is thus generated with the top matrix being a
single vector for the sentence.
2.2.4 Multiple merged HCNNs
Optionally one may consider multiple parallel
HCNNs that are merged according to different
strategies either at the top sentence vector layer or
at intermediate layers. The weights in the word
vectors may be tied across different HCNNs. Al-
though potentially useful, multiple merged HC-
NNs are not used in the experiment below.
This concludes the description of the sentence
model. Let us now proceed to the discourse model.
122
Open
the 
pod bay doors HAL
Dave
I'm afraid I can't do thats
S S
i
s
i+1
I
H
O
O
i
i
i+1
P(x )
P(x     )  
i+1
i
x
i-1
I
x
i
Figure 4: Unravelling of a RCNN discourse model to depth d = 2. The recurrent Hi and output Oi
weights are conditioned on the respective agents ai.
3 Discourse Model
The discourse model adapts a RNN architecture
in order to capture central properties of discourse.
We here first describe such properties and then de-
fine the model itself.
3.1 Discourse Compositionality
The meaning of discourse - and of words and utter-
ances within it - is often a result of a rich ensemble
of context, of speakers? intentions and actions and
of other relevant surrounding circumstances (Ko-
rta and Perry, 2012; Potts, 2011). Far from cap-
turing all aspects of discourse meaning, we aim
at capturing in the model at least two of the most
prominent ones: the sequentiality of the utterances
and the interactions between the speakers.
Concerning sequentiality, just the way the
meaning of a sentence generally changes if words
in it are permuted, so does the meaning of a para-
graph or dialogue change if one permutes the sen-
tences or utterances within. The change of mean-
ing is more marked the larger the shift in the order
of the sentences. Especially in tasks where one is
concerned with a specific sentence within the con-
text of the previous discourse, capturing the order
of the sentences preceding the one at hand may be
particularly crucial.
Concerning the speakers? interactions, the
meaning of a speaker?s utterance within a dis-
course is differentially affected by the speaker?s
previous utterances as opposed to other speakers?
previous utterances. Where applicable we aim at
making the computed meaning vectors reflect the
current speaker and the sequence of interactions
with the previous speakers. With these two aims
in mind, let us now proceed to define the model.
3.2 Recurrent Convolutional Neural Network
The discourse model coupled to the sentence
model is based on a RNN architecture with inputs
from a HCNN and with the recurrent and output
weights conditioned on the respective speakers.
We take as given a sequence of sentences or ut-
terances s1, ..., sT , each in turn being a sequence
of words si = yi1...yil , a sequence of labels
x1, ..., xT and a sequence of speakers or agents
a1, ..., aT , in such way that the i-th utterance is
performed by the i-th agent and has label xi. We
denote by si the sentence vector computed by way
of the sentence model for the sentence si. The
RCNN computes probability distributions pi for
the label at step i by iterating the following equa-
tions:
hi = ?( Ixi?1 +Hi?1hi?1 + Ssi + bh) (6)
pi = softmax(Oihi + bo) (7)
where I,Hi,Oi are corresponding weight matri-
ces for each agent ai and softmax(y)k = e
yk
?
j e
yj
returns a probability distribution. Thus pi is taken
to model the following predictive distribution:
pi = P (xi|x<i, s?i, a?i) (8)
123
Dialogue Act Label Example Train (%) Test (%)
Statement And, uh, it?s a legal firm office. 36.9 31.5
Backchannel/Acknowledge Yeah, anything could happen. 18.8 18.2
Opinion I think that would be great. 12.7 17.1
Abandoned/Uninterpretable So, - 7.6 8.6
Agreement/Accept Yes, exactly. 5.5 5.0
Appreciation Wow. 2.3 2.2
Yes?No?Question Is that what you do? 2.3 2.0
Non?Verbal [Laughter], [Throat-clearing] 1.7 1.9
Other labels (34) 12.2 13.5
Total number of utterances 196258 4186
Total number of dialogues 1115 19
Table 1: Most frequent dialogue act labels with examples and frequencies in train and test data.
An RCNN and the unravelling to depth d = 2 are
depicted respectively in Fig. 2 and Fig. 4. With
regards to vector representations of discourse, we
take the hidden layer hi as the vector represent-
ing the discourse up to step i. This concludes the
description of the discourse model. Let us now
consider the experiment.
4 Predicting Dialogue Acts
We experiment with the prediction of dialogue
acts within a conversation. A dialogue act spec-
ifies the pragmatic role of an utterance and helps
identifying the speaker?s intentions (Austin, 1962;
Korta and Perry, 2012). The automated recog-
nition of dialogue acts is crucial for dialogue
state tracking within spoken dialogue systems
(Williams, 2012). We first describe the Switch-
board Dialogue Act (SwDA) corpus (Calhoun et
al., 2010) that serves as the dataset in the experi-
ment. We report on the training procedure and the
results and we make some qualitative observations
regarding the discourse representations produced
by the model.
4.1 SwDA Corpus
The SwDA corpus contains audio recordings and
transcripts of telephone conversations between
multiple speakers that do not know each other and
are given a topic for discussion. For a given utter-
ance we use the transcript of the utterance, the dia-
logue act label and the speaker?s label; no other an-
notations are used in the model. Overall there are
42 distinct dialogue act labels such as Statement
and Opinion (Tab.1). We adopt the same data split
of 1115 train dialogues and 19 test dialogues as
used in (Stolcke et al, 2000).
4.2 Objective Function and Training
We minimise the cross-entropy error of the pre-
dicted and the true distributions and include an
l2 regularisation parameter. The RCNN is trun-
cated to a depth d = 2 so that the prediction of
a dialogue act depends on the previous two utter-
ances, speakers and dialogue acts; adopting depths
> 2 has not yielded improvements in the experi-
ment. The derivatives are efficiently computed by
back-propagation (Rumelhart et al, 1986). The
word vectors are initialised to random vectors of
length 25 and no pretraining procedure is per-
formed. We minimise the objective using L-BFGS
in mini-batch mode; the minimisation converges
smoothly.
4.3 Prediction Method and Results
The prediction of a dialogue act is performed in
a greedy fashion. Given the two previously pre-
dicted acts x?i?1, x?i?2, one chooses the act x?i that
has the maximal probability in the predicted dis-
tribution P (xi). The LM-HMM model of (Stol-
cke et al, 2000) learns a language model for each
dialogue act and a Hidden Markov Model for the
sequence of dialogue acts and it requires all the
utterances in a dialogue in order to predict the dia-
logue act of any one of the utterances. The RCNN
makes the weaker assumption that only the utter-
ances up to utterance i are available to predict the
dialogue act x?i. The accuracy results of the mod-
els are compared in Tab. 3.
4.4 Discourse Vector Representations
We inspect the discourse vector representations
that the model generates. After a dialogue is pro-
cessed, the hidden layer h of the RCNN is taken
124
Center A: Do you repair your own car? A: ? I guess we can start. A: Did you use to live around here?
Dialogue B: I try to, whenever I can. B: Okay. B: Uh, Redwood City.
First NN A: Do you do it every day? A: I think for serial murder ? A: Can you stand up in it?
B: I try to every day. B: Uh-huh. B: Uh, in parts.
Second NN A: Well, do you have any children? A: The USSR ? wouldn?t do it A: [Laughter] Do you have any kids
that you take fishing?
B: I?ve got one. B: Uh-huh. B: Uh, got a stepdaughter.
Third NN A: Do you manage the money? A: It seems to me there needs A: Is our five minutes up?
to be some ground, you know,
some rules ?
B: Well, I, we talk about it. B: Uh-huh. B: Uh, pretty close to it.
Fourth NN A: Um, do you watch it every A: It sounds to me like, uh, A: Do you usually go out, uh,
Sunday? you are doing well. with the children or without them?
B: [Breathing] Uh, when I can. B: My husband?s retired. B: Well, a variety.
Table 2: Short dialogues and nearest neighbours (NN).
Accuracy (%)
RCNN 73.9
LM-HMM trigram 71.0
LM-HMM bigram 70.6
LM-HMM unigram 68.2
Majority baseline 31.5
Random baseline 2.4
Table 3: SwDA dialogue act tagging accuracies.
The LM-HMM results are from (Stolcke et al,
2000). Inter-annotator agreement and theoretical
maximum is 84%.
to be the vector representation for the dialogue
(Sect. 3.2). Table 2 includes three randomly cho-
sen dialogues composed of two utterances each;
for each dialogue the table reports the four near-
est neighbours. As the word vectors and weights
are initialised randomly without pretraining, the
word vectors and the weights are induced during
training only through the dialogue act labels at-
tached to the utterances. The distance between
two word, sentence or discourse vectors reflects
a notion of pragmatic similarity: two words, sen-
tences or discourses are similar if they contribute
in a similar way to the pragmatic role of the utter-
ance signalled by the associated dialogue act. This
is suggested by the examples in Tab. 2, where a
centre dialogue and a nearest neighbour may have
some semantically different components (e.g. ?re-
pair your own car? and ?manage the money?), but
be pragmatically similar and the latter similarity is
captured by the representations. In the examples,
the meaning of the relevant words in the utter-
ances, the speakers? interactions and the sequence
of pragmatic roles are well preserved across the
nearest neighbours.
5 Conclusion
Motivated by the compositionality of meaning
both in sentences and in general discourse, we
have introduced a sentence model based on a novel
convolutional architecture and a discourse model
based on a novel use of recurrent networks. We
have shown that the discourse model together with
the sentence model achieves state of the art results
in a dialogue act classification experiment with-
out feature engineering or pretraining and with
simple greedy decoding of the output sequence.
We have also seen that the discourse model pro-
duces compelling discourse vector representations
that are sensitive to the structure of the discourse
and promise to capture subtle aspects of discourse
comprehension, especially when coupled to fur-
ther semantic data and unsupervised pretraining.
Acknowledgments
We thank Ed Grefenstette and Karl Moritz Her-
mann for great conversations on the matter. The
authors gratefully acknowledge the support of the
Clarendon Fund and the EPSRC.
References
[Austin1962] John L. Austin. 1962. How to do things
with words. Oxford: Clarendon.
[Baroni and Zamparelli2010] Marco Baroni and
Roberto Zamparelli. 2010. Nouns are vectors, ad-
jectives are matrices: Representing adjective-noun
constructions in semantic space. In EMNLP, pages
1183?1193.
[Blacoe and Lapata2012] William Blacoe and Mirella
Lapata. 2012. A comparison of vector-based rep-
resentations for semantic composition. In EMNLP-
CoNLL, pages 546?556.
125
[Calhoun et al2010] Sasha Calhoun, Jean Carletta, Ja-
son M. Brenier, Neil Mayo, Dan Jurafsky, Mark
Steedman, and David Beaver. 2010. The nxt-format
switchboard corpus: a rich resource for investigat-
ing the syntax, semantics, pragmatics and prosody
of dialogue. Language Resources and Evaluation,
44(4):387?419.
[Collobert and Weston2008] R. Collobert and J. We-
ston. 2008. A unified architecture for natural lan-
guage processing: Deep neural networks with mul-
titask learning. In International Conference on Ma-
chine Learning, ICML.
[Frege1892] Gottlob Frege. 1892. U?ber Sinn
und Bedeutung. Zeitschrift fu?r Philosophie und
philosophische Kritik, 100.
[Grefenstette et al2011] Edward Grefenstette,
Mehrnoosh Sadrzadeh, Stephen Clark, Bob
Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.
[Hermann and Blunsom2013] Karl Moritz Hermann
and Phil Blunsom. 2013. The Role of Syntax in
Vector Space Models of Compositional Semantics.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Sofia, Bulgaria, August. Association
for Computational Linguistics. Forthcoming.
[Jones et al2006] Rosie Jones, Benjamin Rey, Omid
Madani, and Wiley Greiner. 2006. Generating
query substitutions. In WWW, pages 387?396.
[Korta and Perry2012] Kepa Korta and John Perry.
2012. Pragmatics. In Edward N. Zalta, editor, The
Stanford Encyclopedia of Philosophy. Winter 2012
edition.
[LeCun et al2001] Y. LeCun, L. Bottou, Y. Bengio, and
P. Haffner. 2001. Gradient-based learning applied
to document recognition. In Intelligent Signal Pro-
cessing, pages 306?351. IEEE Press.
[Mikolov et al2010] Tomas Mikolov, Martin Karafia?t,
Lukas Burget, Jan Cernocky?, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based lan-
guage model. In INTERSPEECH, pages 1045?
1048.
[Mitchell and Lapata2010] Jeff Mitchell and Mirella
Lapata. 2010. Composition in distributional models
of semantics. Cognitive Science, 34(8):1388?1429.
[Potts2011] Christopher Potts. 2011. Pragmatics.
In Ruslan Mitkov, editor, The Oxford Handbook
of Computational Linguistics. Oxford University
Press, 2 edition.
[Rumelhart et al1986] D. E. Rumelhart, G. E. Hinton,
and R. J. Williams. 1986. Learning internal repre-
sentations by error propagation. MIT Press Compu-
tational Models Of Cognition And Perception Series,
page 318362.
[Scheible and Schu?tze2013] Christian Scheible and
Hinrich Schu?tze. 2013. Cutting recursive autoen-
coder trees. CoRR, abs/1301.2811.
[Socher et al2012] Richard Socher, Brody Huval,
Christopher D. Manning, and Andrew Y. Ng. 2012.
Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In Proceedings of the 2012
Conference on Empirical Methods in Natural
Language Processing (EMNLP).
[Stolcke et al2000] Andreas Stolcke, Klaus Ries, Noah
Coccaro, Elizabeth Shriberg, Rebecca A. Bates,
Daniel Jurafsky, Paul Taylor, Rachel Martin,
Carol Van Ess-Dykema, and Marie Meteer. 2000.
Dialog act modeling for automatic tagging and
recognition of conversational speech. Computa-
tional Linguistics, 26(3):339?373.
[Sutskever et al2011] Ilya Sutskever, James Martens,
and Geoffrey E. Hinton. 2011. Generating text with
recurrent neural networks. In ICML, pages 1017?
1024.
[Turney and Pantel2010] Peter D. Turney and Patrick
Pantel. 2010. From frequency to meaning: Vec-
tor space models of semantics. J. Artif. Intell. Res.
(JAIR), 37:141?188.
[Wang and Manning2012] Sida Wang and Christo-
pher D. Manning. 2012. Baselines and bigrams:
Simple, good sentiment and topic classification. In
ACL (2), pages 90?94.
[Williams2012] Jason D. Williams. 2012. A belief
tracking challenge task for spoken dialog systems.
In NAACL-HLT Workshop on Future Directions and
Needs in the Spoken Dialog Community: Tools and
Data, SDCTD ?12, pages 23?24, Stroudsburg, PA,
USA. Association for Computational Linguistics.
126
