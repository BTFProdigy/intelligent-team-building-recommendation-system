Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1522?1533, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Characterizing Stylistic Elements in Syntactic Structure
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Much of the writing styles recognized in
rhetorical and composition theories involve
deep syntactic elements. However, most
previous research for computational sty-
lometric analysis has relied on shallow
lexico-syntactic patterns. Some very re-
cent work has shown that PCFG models
can detect distributional difference in syn-
tactic styles, but without offering much in-
sights into exactly what constitute salient
stylistic elements in sentence structure
characterizing each authorship. In this
paper, we present a comprehensive ex-
ploration of syntactic elements in writing
styles, with particular emphasis on inter-
pretable characterization of stylistic ele-
ments. We present analytic insights with
respect to the authorship attribution task
in two different domains.
1 Introduction
Much of the writing styles recognized in rhetor-
ical and composition theories involve deep syn-
tactic elements in style (e.g., Bain (1887), Kem-
per (1987) Strunk and White (2008)). However,
previous research for automatic authorship at-
tribution and computational stylometric analy-
sis have relied mostly on shallow lexico-syntactic
patterns (e.g., Mendenhall (1887), Mosteller
and Wallace (1984), Stamatatos et al(2001),
Baayen et al(2002), Koppel and Schler (2003),
Zhao and Zobel (2007), Luyckx and Daelemans
(2008)).
Some very recent works have shown that
PCFG models can detect distributional differ-
ence in sentence structure in gender attribution
(Sarawgi et al 2011), authorship attribution
(Raghavan et al 2010), and native language
identification (Wong and Dras, 2011). However,
still very little has been understood exactly what
constitutes salient stylistic elements in sentence
structures that characterize each author. Al-
though the work of Wong and Dras (2011) has
extracted production rules with highest informa-
tion gain, their analysis stops short of providing
insight any deeper than what simple n-gram-
level analysis could also provide.1 One might
even wonder whether PCFG models are hing-
ing mostly on leaf production rules, and whether
there are indeed deep syntactic differences at all.
This paper attempts to answer these questions.
As an example of syntactic stylistic elements
that have been much discussed in rhetorical the-
ories, but have not been analyzed computation-
ally, let us consider two contrasting sentence
styles: loose (cumulative) and periodic:2 a loose
sentence places the main clause at the begin-
ning, and then appends subordinate phrases and
clauses to develop the main message. In con-
trast, a periodic sentence starts with subordi-
nate phrases and clauses, suspending the most
1For instance, missing determiners in English text
written by Chinese speakers, or simple n-gram anomaly
such as frequent use of ?according to? by Chinese speak-
ers (Wong and Dras, 2011).
2Periodic sentences were favored in classical times,
while loose sentences became more popular in the modern
age.
1522
Hobbs Joshi Lin McDon
S?ROOT ? S , CC S PP?PRN ? IN NP NP?S ? NN CD NP?NP ? DT NN POS
NP?PP ? DT NP?PP ? NP PRN SBAR NP?NP ? DT NN NNS WHNP?SBAR ? IN
VP?VP ? TO VP S?ROOT ? PP NP VP . S?ROOT ? SBAR , NP VP . NP?PP ? NP SBAR
PP?PP ? IN S PRN?NP ? -LRB- PP -RRB- NP?PP ? NP : NP SBAR?PP ? WHADVP S
NP?PP ? NP , PP NP?NP ? NNP S?ROOT ? PP , NP VP . SBAR?S ? WHNP S
VP?S ? VBZ ADJP S S?SBAR ? PP NP VP NP?NP ? PDT DT NNS PP?NP ? IN SBAR
VP?SINV ? VBZ S?ROOT ? LST NP VP . NP?VP ? DT NN SBAR SBAR?NP ? WHNP S
VP?S ? VBD S CONJP?NP ? RB RB IN SBAR?S ? WHADVP S SBAR?PP ? SBAR CC SBAR
VP?S ? VBG PP NP?PP ? NP PRN PP PRN?NP ? -LRB- NP -RRB- PP?VP ? IN
ADVP?VP ? RB PP NP?NP ? NP , NP NP?PP ? NN NN S?SBAR ? VP
Table 1: Top 10 most discriminative production rules for each author in the scientific domain.
loose Christopher Columbus finally
reached the shores of San Salvador
after months of uncertainty at
sea, the threat of mutiny, and a
shortage of food and water.
periodic After months of uncertainty at sea,
the threat of mutiny, and a short-
age of food and water, Christopher
Columbus finally reached the shores
of San Salvador.
Table 2: Loose/Periodic sentence with identical set
of words and POS tags
important part to the end. The example in Ta-
ble 2 highlights the difference:
Notice that these two sentences comprise of an
identical set of words and part-of-speech. Hence,
shallow lexico-syntactic analysis will not be able
to catch the pronounced stylistic difference that
is clear to a human reader.
One might wonder whether we could gain in-
teresting insights simply by looking at the most
discriminative production rules in PCFG trees.
To address this question, Table 1 shows the
top ten most discriminative production rules
for authorship attribution for scientific articles,3
ranked by LIBLINEAR (Fan et al 2008).4 Note
that terminal production rules are excluded so
as to focus directly on syntax.
It does provide some insights, but not to a sat-
isfactory degree. For instance, Hobbs seems to
favor inverted declarative sentences (SINV) and
adverbs with prepositions (RB PP). While the
latter can be easily obtained by simple part-of-
3See Section 2 for the description of the dataset.
4We use Berkeley PCFG parser (Petrov and Klein,
2007) for all experiments.
speech analysis, the former requires using parse
trees. We can also observe that none of the
top 10 most discriminative production rules for
Hobbs includes SBAR tag, which represents sub-
ordinate clauses. But examining discriminative
rules alone is limited in providing more compre-
hensive characterization of idiolects.
Can we unveil something more in deep syntac-
tic structure that can characterize the collective
syntactic difference between any two authors?
For instance, what can we say about distribu-
tional difference between loose and periodic sen-
tences discussed earlier for each author? As can
be seen in Table 1, simply enumerating most dis-
criminative rules does not readily answer ques-
tions such as above.
In general, production rules in CFGs do not
directly map to a wide variety of stylistic el-
ements in rhetorical and composition theories.
This is only as expected however, partly because
CFGs are not designed for stylometric analysis
in the first place, and also because some syntac-
tic elements can go beyond the scope of context
free grammars.
As an attempt to reduce this gap between
modern statistical parsers and cognitively recog-
nizable stylistic elements, we explore two com-
plementary approaches:
1. Translating some of the well known stylistic
elements of rhetorical theories into PCFG
analysis (Section 3).
2. Investigating different strategies of analyz-
ing PCFG trees to extract author charac-
teristics that are interesting as well as in-
terpretable (Sections 4 & 5).
1523
Algorithm 1 Sentence Type-1 Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
if S ? Ltop then
if SBAR /? ?(Nr) then
return COMPOUND
else
return COMPLEX-COMPOUND
else
if VP ? Ltop then
if SBAR /? ?(Nr) then
return SIMPLE
else
return COMPLEX
return OTHER
We present analytic insights with respect to
the authorship attribution task in two distinct
domains.
2 Data
For the empirical analysis of authorship attri-
bution, we use two different datasets described
below. Sections 3, 4 & 5 provide the details of
our stylometric analysis.
Scientific Paper We use the ACL Anthol-
ogy Reference Corpus (Bird et al 2008). Since
it is nearly impossible to determine the gold-
standard authorship of a paper written by multi-
ple authors, we select 10 authors who have pub-
lished at least 8 single-authored papers. We in-
clude 8 documents per author, and remove cita-
tions, tables, formulas from the text using sim-
ple heuristics.5
Novels We collect 5 novels from 5 English au-
thors: Charles Dickens, Edward Bulwer-Lytton,
Jane Austen, Thomas Hardy and Walter Scott.
We select the first 3000 sentences from each
novel and group every 50 consecutive sentences
into 60 documents per novel per author.
5Some might question whether the size of the dataset
used here is relatively small in comparison to typical
dataset comprised of thousands of documents in conven-
tional text categorization. We point out that authorship
attribution is fundamentally different from text catego-
rization in that it is often practically impossible to collect
more than several documents for each author. Therefore,
it is desirable that the attribution algorithms to detect
the authors based on very small samples.
Algorithm 2 Sentence Type-II Identification
Input: Parse tree t(Nr) of sentence s
Output: Type of s.
k ? 1
while k ? ? do
if Ltopk 6= VP then
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return PERIODIC
else
if S ? ?(Ltopk ) or SBAR ? ?(L
top
k ) then
return LOOSE
return OTHER
3 Sentence Types
In this section, we examine well-known sentence
types that are recognized in the literature, but
have not been analyzed computationally.
Type-I Identification ? Simple/Complex/
Compound/Complex-Compound: PCFG
trees do not provide this information directly,
hence we must construct an algorithm to derive
it. The key to identifying these sentences is the
existence of dependent and independent clauses.
For the former, we rely on the SBAR tag, while
for the latter, we first define the sequence of
nodes right below the root (e.g., [NP VP .] shown
in the horizontal box in Figure 1). We call this
the top structural level. We then check whether
S (in addition to the root S) appears in this
sequence.
Formally, let Ltop = {Ni} be the set of nodes
in the top structural level, and ? = |Ltop|. Let
t(Nr) be the tree rooted at Nr, and ?(Nr) de-
note the set of nodes in t(Nr). Algorithm 1
shows the procedure to determine the type-I
class of a sentence based on its PCFG tree.6
Type-II Identification ? Loose/Periodic:
A sentence can also be classified as loose or
periodic, and we present Algorithm 2 for this
identification. We perform a mini-evaluation on
20 previously unseen sentences for each type7.
Our algorithm was able to perform type-I iden-
tification on all sentences correctly. In type-II
6Note that Algorithm 1 & 2 rely on the use of Berkeley
parser (Petrov and Klein, 2007).
7These were gathered from several online quizzes
for English learners. E.g., http://grammar.about.com,
http://a4esl.org
1524
Type Hobbs Joshi Lin McDon
simple 40.0 41.7 50.2 27.9
cplex 40.8 40.7 37.6 48.4
cpnd 7.9 5.6 3.9 5.5
cpxnd 8.5 9.2 7.7 15.5
other 2.8 2.8 0.6 2.7
loose 27.6 26.4 26.9 30.8
perio 11.1 11.7 15.2 16.4
other 61.3 61.9 57.9 52.8
Table 3: Sentence Types (%) in scientific data.
identification, it labeled all loose sentences cor-
rectly, and achieved 90% accuracy on periodic
sentences.
Discussion Tables 3 & 4 show the sentence
type distribution in scientific data and novels,
respectively.8 We see that different authors are
characterized by different distribution of sen-
tence types. For instance, in Table 3, Lin is
a prolific user of simple sentences while McDon
prefers employing complex sentences. McDon
also uses complex-compound sentences quite of-
ten (15.5%), more than twice as frequently as
Lin. Notice that all authors use loose sen-
tences much more often than periodic sentences,
a known trend in modern English.
In Table 4, we see the opposite trend among
19th-century novels: with the exception of Jane
Austen, all authors utilize periodic sentences
comparatively more often. We also notice
that complex and complex-compound sentences
abound, as expected from classic literary proses.
Can we determine authorship solely based on the
distribution of sentence types?
We experiment with a SVM classifier using just
6 features (one feature for each sentence type in
Table 3), and we achieve accuracy 36.0% with
the scientific data. Given that a random base-
line would achieve only about 10% accuracy, this
demonstrates that the distribution of sentence
types does characterize an idiolect to some de-
gree.
8Due to space limitation, we present analyses based
on 4 authors from the scientific data.
Type Dickens B-Lyt Austen Hardy Scott
simple 26.0 21.2 23.9 25.6 17.5
cplex 24.4 21.8 24.8 25.6 31.8
cpnd 15.3 15.2 12.6 16.3 11.7
cpxnd 20.8 23.3 31.1 18.9 28.7
other 13.5 18.5 7.6 13.6 10.3
loose 11.5 10.8 17.9 14.5 15.3
perio 19.5 13.6 14.0 16.2 18.0
other 69.0 75.6 68.1 69.3 66.7
Table 4: Sentence Types (%) in Novels
4 Syntactic Elements Based on
Production Rules
In this section, we examine three different as-
pects of syntactic elements based on production
rules.
4.1 Syntactic Variations
We conjecture that the variety of syntactic
structure, which most previous research in com-
putational stylometry has not paid much atten-
tion to, provides an interesting insight into au-
thorship. One way to quantify the degree of syn-
tactic variations is to count the unique produc-
tion rules. In Tables 5, we show the extent of
syntactic variations employed by authors using
the standard deviation ? and the coverage of an
author:
C(a) :=
|R(a)|
| ?a R(a)|
? 100
whereR(a) denotes the set of unique production
rules used by author a, and ?a iterates over all
authors. In order to compare among authors,
we also show these parameters normalized with
respect to the highest value. Our default setting
is to exclude all lexicalized rules in the produc-
tions to focus directly on the syntactic varia-
tions. In our experiments (Section 6), however,
we do augment the rules with (a) ancestor nodes
to capture deeper syntactic structure and (b)
lexical (leaf) nodes.
As hypothesized, these statistics provide us
new insights into the authorship. For instance,
we find that McDon employs a wider variety of
syntactic structure than others, while Lin?s writ-
ing exhibits relatively the least variation. More-
over, comparing Joshi and Hobbs, it is inter-
esting to see the standard deviation differ a lot
1525
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
C 36.0 37.6 32.8 42.6 30.9 28.8 36.2 30.0 24.1
Cnorm 0.84 0.88 0.77 1.0 0.85 0.79 1.0 0.83 0.67
? 51.5 39.2 63.3 44.4 88.3 81.6 98.0 125.3 114.7
?norm 0.81 0.62 1.0 0.7 0.7 0.65 0.78 1.0 0.92
Table 5: Syntactic variations of different authors in the scientific domain.
Hobbs Joshi Lin McDon
# 136 # 142 # 124 # 161
S ? S CC S . S ? ADVP PP NP VP . S ? SBAR NP VP . S ? S NP VP .
S ? CC NP VP . S ? PP NP ADVP VP . FRAG ? NP : S . S ? S : S .
S ? S VP . S ? NP VP S ? NP VP . S ? SBAR VP .
S ? NP NP VP . S ? S S CC S . S ? PP VP . S ? SBAR S CC S .
S ? PP NP VP . S ? ADVP NP VP . S ? NP ADVP VP . S ? NP PP VP .
Table 6: Most discriminative sentence outlines in the scientific data. #N shows the number of unique
sentence outlines of each author.
(51.5 and 39.2), in spite of their C scores being
similar: 36.0% and 37.6%, respectively. This
indicates that Hobbs tends to use a certain sub-
set production rules much more frequently than
Joshi. Lin exhibits the highest standard devia-
tion in spite of having least syntactic variation,
indicating that he uses a much smaller subset of
productions regularly, while ocassionally deviat-
ing to other rules.
Similarly, among novels, Jane Austen?s writ-
ing has the highest amount of variation, while
Walter Scott?s writing style is the least varied.
Even though authors from both datasets display
similar C scores (Table 5), the difference in ? is
noteworthy. The significantly higher linguistic
variation is to be expected in creative writing
of such stature. It is interesting to note that
the authors with highest coverage ? Austen and
Dickens ? have much lower deviation in their
syntactic structure when compared to Hardy
and Scott. This indicates that while Austen and
Dickens consistently employ a wider variety of
sentence structures in their writing, Hardy and
Scott follow a relatively more uniform style with
sporadic forays into diverse syntactic constructs.
4.2 Sentence Outlines
Although the approach of Section 4.1 give us a
better and more general insight into the char-
acteristics of each author, its ability to provide
insight on deep syntactic structure is still lim-
ited, as it covers production rules at all levels of
the tree. We thus shift our focus to the top level
of the trees, e.g., the second level (marked in a
horizontal box) in Tree (1) of Figure 1, which
gives us a better sense of sentence outlines.
Tables 6 and 7 present the most discrimina-
tive sentence outlines of each author in the scien-
tific data and novels, respectively. We find that
McDon is a prolific user of subordinate clauses,
indicating his bias towards using complex sen-
tences. The rule ?S ? SBAR S CC S? shows
his inclination towards complex-compound sen-
tences as well. These inferences are further sup-
ported by the observations in Table 3. Another
observation of possible interest is the tendency
of Joshi and Lin to begin sentences with prepo-
sitional phrases.
In comparing Table 6 and Table 7, notice
the significantly higher presence of complex and
compound-complex structures in the latter9.
The most discriminating sentence outlines for
Jane Austen, for instance, are all indicative of
complex-compound sentences. This is further
supported by Table 4.
5 Syntactic Elements Based on Tree
Topology
In this section, we investigate quantitative tech-
niques to capture stylistic elements in the tree
9The presence of ?FRAG? is not surprising. Inten-
tional use of verbless sentence fragments, known as sce-
sis onomaton, was often employed by authors such as
Dickens and Bulwer-Lytton (Quinn, 1995).
1526
Dickens Bulwer-Lytton Austen Hardy Scott
# 1820 # 1696 # 2137 # 1772 # 1423
SQ ? NNP . SBARQ ? WHNP S . S ? S : CC S . S ? S NP VP . S ? NP PRN VP .
FRAG ? NP . FRAG ? INTJ NP . S ? S CC S : CC S . S ? ADVP NP VP . S ? PP NP VP .
SINV ? NP VP NP . S ? S : S CC S . S ? S : CC S : CC S . S ? FRAG : S . S ? S S : S .
INTJ ? UH . FRAG ? CC NP . S ? S : S : CC S . S ? INTJ NP VP . S ? NP PP VP .
SBARQ ? WHNP SQ . FRAG ? NP ADJP . S ? SBAR S : CC S . S ? NP VP . S ? ADVP PRN NP VP .
Table 7: Most discriminative sentence outlines in the novel data. #N shows the number of unique sentence
outlines of each author.
Metrics Scientific Data Novels
Hobbs Joshi Lin McDon Dickens B-Lyt Austen Hardy Scott
sen-len avg 23.7 26.0 21.0 32.2 24.1 26.7 31.4 21.5 34.1
hT avg 5.8 5.3 5.9 4.8 4.7 5.0 5.4 4.9 5.9
hF avg 2.4 2.1 2.5 1.9 1.9 1.9 2.1 1.9 2.1
wL avg 5.0 4.8 5.5 4.2 4.1 4.4 4.7 3.8 4.9
?H avg 1.2 1.1 1.1 1.0 1.1 1.1 1.3 1.2 1.4
?S avg 1.9 1.8 1.8 1.7 1.0 1.1 1.2 1.0 1.4
Table 8: Tree topology metrics for scientific data and novels.
topology. Figure 1 shows three different parse
trees to accompany our discussion.10 Notice
that sentence (1) is a loose sentence, and sen-
tence (2) is periodic. In general, loose sentences
grow deep and unbalanced, while periodic sen-
tences are relatively more balanced and wider.
For a tree t rooted at NR with a height n, let
T be the set of leaf nodes, and let F be the set
of furcation nodes, and let ?(Ni, Nj) denote the
length of the shortest path from Ni to Nj . In-
spired by the work of Shao (1990), we analyze
tree topology with the following four measure-
ments:
? Leaf height (hT = {hTi , Ni ? T }), where
hTi = ?(Ni, NR) Ni ? T . For instance, the
leaf height of ?free? of Tree (2) in Fig. 1
is 6.
? Furcation height (hF = {hFi , Ni ? F}),
where hFi is the maximum leaf height within
the subtree rooted at Ni. In Figure 1, for
example, the furcation height of the VP in
Tree (2) (marked in triangle) is 3.
? Level width (wL = {wl, 1 ? l ? n}),
where wl = |{Ni : ?(Ni, NR) = l}|. E.g., w4
of Tree (1) in Figure 1 is 6.
10Example sentences are taken from Lin (1997), Joshi
(1992), and Lin (1995).
? Horizontal ?H = {?Hi , Ni ? F} , and
Vertical Imbalance ?S = {?Si , Ni ? F}.
Let C be the set of child nodes of Nk. If
|C| ? 2, then
?Hk =
?
?
?
? 1
n
|C|?
i=1
(hFi ?H)
2
where H = 1|C|
?|C|
i=1 h
F
i . Similarly,
?Sk =
?
?
?
? 1
n
|C|?
i=1
(s(Ni)? S)2
where S = 1|C|
?|C|
i=1 s(Ni) and s(Ni) is the
number of leaf nodes of tree rooted at Ni.
As shown in Figure 1, the imbalance of the
internal node VP in Tree (2) (marked in
triangle) is 0.5 horizontally, and 0.5 verti-
cally.
To give an intuition on the relation between
these measurements and different tree struc-
tures, Table 9 provides the measurements of the
three trees shown in Figure 1.
Note that all three sentences are of similar
length but show different tree structures. Tree
(1) and Tree (2) differ in that Tree (1) is
highly unbalanced and grows deep, while Tree
1527
Figure 1: Parsed trees
Metrics Tree (1) Tree (2) Tree (3)
# of tokens 15 13 13
maxi {hTi } 11 6 6
maxi {wLi } 6 9 9
maxi {?Hi } 4.97 1.6 1.7
maxi {?Si } 4 1.5 4.7
Table 9: Tree Topology Statistics for Figure 1.
(2) is much better balanced and grows shorter
but wider. Comparing Tree (2) and Tree (3),
they have the same max Leaf height, Level
width, and Horizontal Imbalance, but the
latter has bigger Vertical Imbalance, which
quantifies the imbalance in terms of the text
span covered by subtrees.
We provide these topological metrics for au-
thors from both datasets in Table 8.
6 Experiments & Evaluation
In our experiments, we utilize a set of features
motivated by PCFG trees. These consist of sim-
ple production rules and other syntactic features
based on tree-traversals. Table 10 describes
these features with examples from Tree (2), us-
ing the portion marked by the triangle.
These sets of production rules and syntax fea-
tures are used to build SVM classifiers using LI-
BLINEAR (Fan et al 2008), wherein all fea-
ture values are encoded as term-frequencies nor-
malized by document size. We run 5-fold cross-
validation with training and testing split first as
80%/20%, and then as 20%/80%.
We would like to point out that the latter con-
figuration is of high practical importance in au-
thorship attribution, since we may not always
have sufficient training data in realistic situa-
tions, e.g., forensics (Luyckx and Daelemans,
2008).
Lexical tokens provide strong clues by creat-
ing features that are specific to each author: re-
search topics in the scientific data, and proper
nouns such as character names in novels. To
lessen such topical bias, we lemmatize and rank
words according to their frequency (in the entire
dataset), and then consider the top 2,000 words
only. Leaf-node productions with words outside
this set are disregarded.
Our experimental results (Tables 11 & 12)
show that not only do deep syntactic features
perform well on their own, but they also signif-
icantly improve over lexical features. We also
show that adding the style11 features further
improves performance.
1528
Features
pr Rules excluding terminal productions.
E.g., VP ? VBG NP
synv Traversal from a non-leaf node to its grand-
parent (embedded rising).
E.g., VP?S ? PP
synh Left-to-right traversal in the set of all non-
leaf children of a node.
E.g., VBG ? NP (for node VP)
synv+h synv ? synh
syn0 No tree traversal. Feature comprises inte-
rior nodes only.
syn? Union of all edges to child nodes, except
when child is a leaf node.
E.g., {VP ? VBG, VP ? NP}
synl syn? ? { edge to parent node}
style11 The set of 11 extra stylistic features. 6 val-
ues from the distribution of sentence types
(Section 3), and 5 topological metrics (Sec-
tion 5) characterizing the height, width and
imbalance of a tree.
Variations
p?r Each production rule is augmented with the
grandparent node.
? Terminal (leaf) nodes are included.
Table 10: Features and their lexico-syntactic varia-
tions. Illustration: p?r? denotes the set of production
rules pr (including terminal productions) that are
augmented with their grandparent nodes.
To quantify the amount of authorship infor-
mation carried in the set style11, we experi-
ment with a SVM classifier using only 11 fea-
tures (one for each metric), and achieve accu-
racy of 42.0% and 52.0% with scientific data
and novels, respectively. Given that a random-
guess baseline would achieve only 10% and 20%
(resp.), and that the classification is based on
just 11 features, this experiment demonstrates
how effectively the tree topology statistics cap-
ture idiolects. In general, lexicalized features
yield higher performance even after removing
topical words. This is expected since tokens
such as function words play an important role
in determining authorship (e.g., Mosteller and
Wallace (1984), Garcia and Martin (2007), Arg-
amon et al(2007)).
A more important observation, however, is
that even after removing the leaf production
rules, accuracy as high as 93% (scientific) and
92.2% (novels) are obtained using syntactic fea-
Features Scientific Novels
+style11 +style11
style11 20.6 ? 43.1 ?
Unigram 56.9 ? 69.3 ?
synh 53.7 53.7 68.3 67.9
syn0 22.9 31.1 57.8 62.5
syn? 43.4 44.0 63.6 65.7
synl 51.1 51.7 71.3 72.8
synv+h 54.0 55.7 72.0 73.2
syn?h 63.1 64.0 72,1 73.2
syn?0 56.6 56.0 73.1 74.1
syn?? 56.3 57.2 74.0 74.9
syn?l 64.6 65.4 74.9 75.3
syn?v+h 64.0 67.7 74.0 74.7
pr 50.3 53.4 67.0 66.7
p?r 59.1 60.6 69.7 68.7
pr? 63.7 65.1 71.5 73.2
p?r? 66.3 69.4 73.6 74.9
Table 11: Authorship attribution with 20% train-
ing data. Improvement with addition of style11
shown in bold.
tures, which demonstrates that there are syn-
tactic patterns unique to each author. Also no-
tice that using only production rules, we achieve
higher accuracy in novels (90.1%), but the ad-
dition of style11 features yields better results
with scientific data (93.0%).
Using different amounts of training data pro-
vides insight about the influence of lexical clues.
In the scientific dataset, increasing the amount
of training data decreases the average perfor-
mance difference between lexicalized and unlex-
icalized features: 13.5% to 11.6%. In novels,
however, we see the opposite trend: 6.1% in-
creases to 8.1%.
We further observe that with scientific data,
increasing the amount of training data improves
the average performance across all unlexicalized
feature-sets from 50.0% to 82.9%, an improve-
ment of 32.8%. For novels, the corresponding
improvement is small in comparison: 17.0%.
This difference is expected. While authors
such as Dickens or Hardy have their unique writ-
ing styles that a classifier can learn based on few
documents, capturing idiolects in the more rigid
domain of scientific writing is far from obvious
with little training data.
1529
Features Scientific Novels
+style11 +style11
style11 42.0 ? 52.0 ?
Unigram 88.0 ? 92.7 ?
synh 85.0 85.0 87.6 88.9
syn0 40.0 53.0 66.4 72.3
syn? 78.0 82.0 80.3 82.3
synl 85.0 92.0 89.3 92.2
synv+h 89.0 93.0 90.1 91.2
syn?h 93.0 93.0 93.7 93.9
syn?0 92.0 94.0 92.1 93.2
syn?? 93.0 94.0 93.4 94.5
syn?l 93.0 95.0 94.9 95.2
syn?v+h 94.0 96.0 94.7 94.8
pr 85.0 86.0 86.7 86.7
p?r 87.0 89.0 88.2 89.3
pr? 93.0 94.0 92.1 93.2
p?r? 94.0 95.0 94.5 95.1
Table 12: Authorship attribution with 80% train-
ing data.
Turning to lexicalized features, we note that
with more training data, lexical cues perform
better in scientific domain than in novels. With
80% data used for training, the average per-
formance of lexicalized feature-sets with science
data is 94.4%, and slightly lower at 94.3% for
novels. With less training data, however, these
figures are 63.5% and 74.3% respectively.
Finally, we point out that adding the style
features derived from sentence types and tree
topologies almost always improves the perfor-
mance. In scientific data, syn?v+h with style11
features shows the best performance (96%),
while syn?l yields the best results for novels
(95.2%). For unlexicalized features, adding
style11 to synv+h and synl yields respective
improvements of 4.0% and 2.9% in the two
datasets.
7 Related Work
There are several hurdles in authorship attribu-
tion. First and foremost, writing style is ex-
tremely domain-dependent. Much of previous
research has focused on several domains of writ-
ing, such as informal modern writing in blogs
and online messages (Zheng et al 2006), rela-
tively formal contemporary texts such as news
articles (Raghavan et al 2010), or classical lit-
erature like novels and proses (e.g., (Burrows,
2002), (Hoover, 2004)).
The nature of these features have also var-
ied considerably. Character level n-grams have
been used by several researchers; most notably
by Peng et al(2003), by Houvardas and Sta-
matatos (2006) for feature selection, and by Sta-
matatos (2006) in ensemble learning. Keselj et
al. (2003) employed frequency measures on n-
grams for authorship attribution.
Others, such as Zhao and Zobel (2005), Arg-
amon and Levitan (2004), Garcia and Martin
(2007), have used word-level approaches instead,
incorporating the differential use of function
words by authors.
More sophisticated linguistic cues have been
explored as well: parts-of-speech n-grams
(Diederich et al 2003), word-level statistics to-
gether with POS-sequences (Luyckx and Daele-
mans, 2008), syntactic labels from partial pars-
ing (Hirst and Feiguina, 2007), etc. The use
of syntactic features from parse trees in au-
thorship attribution was initiated by Baayen et
al. (1996), and more recently, Raghavan et al
(2010) have directly employed PCFG language
models in this area.
Syntactic features from PCFG parse trees
have also been used for gender attribution
(Sarawgi et al 2011), genre identification (Sta-
matatos et al 2000), native language identifi-
cation (Wong and Dras, 2011) and readability
assessment (Pitler and Nenkova, 2008). The
primary focus of most previous research, how-
ever, was to attain better classification accuracy,
rather than providing linguistic interpretations
of individual authorship and their stylistic ele-
ments.
Our work is the first to attempt authorship
attribution of scientific papers, a contemporary
domain where language is very formal, and the
stylistic variations have limited scope. In ad-
dition to exploring this new domain, we also
present a comparative study expounding the
role of syntactic features for authorship attri-
bution in classical literature. Furthermore, our
work is also the first to utilize tree topological
1530
features (Chan et al 2010) in the context of
stylometric analysis.
8 Conclusion
In this paper, we have presented a comprehen-
sive exploration of syntactic elements in writing
styles, with particular emphasis on interpretable
characterization of stylistic elements, thus dis-
tinguishing our work from other recent work on
syntactic stylometric analysis. Our analytical
study provides novel statistically supported in-
sights into stylistic elements that have not been
computationally analyzed in previous literature.
In the future, we plan to investigate the use of
syntactic feature generators for text categoriza-
tion (e.g., Collins and Duffy (2002), Moschitti
(2008), Pighin and Moschitti (2009)) for stylom-
etry analysis.
Acknowledgments Yejin Choi is partially
supported by the Stony Brook University Office
of the Vice President for Research. We thank
reviewers for many insightful and helpful com-
ments.
References
Shlomo Argamon and Shlomo Levitan. 2004. Mea-
suring the usefulness of function words for author-
ship attribution. Literary and Linguistic Comput-
ing, pages 1?3.
Shlomo Argamon, Casey Whitelaw, Paul Chase,
Sobhan Raj Hota, Navendu Garg, and Shlomo
Levitan. 2007. Stylistic text classification using
functional lexical features: Research articles. J.
Am. Soc. Inf. Sci. Technol., 58(6):802?822.
H. Baayen, H. Van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: Using syntactic an-
notation to enhance authorship attribution. Lit-
erary and Linguistic Computing, 11(3):121.
H. Baayen, H. van Halteren, A. Neijt, and
F. Tweedie. 2002. An experiment in authorship
attribution. In 6th JADT. Citeseer.
A. Bain. 1887. English Composition and Rhetoric:
Intellectual elements of style. D. Appleton and
company.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph,
M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and
Y.F. Tan. 2008. The acl anthology reference
corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proc.
of the 6th International Conference on Language
Resources and Evaluation Conference (LREC08),
pages 1755?1759.
J. Burrows. 2002. Delta: A measure of stylistic dif-
ference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267?287.
Samuel W. K. Chan, Lawrence Y. L. Cheung, and
Mickey W. C. Chong. 2010. Tree topological fea-
tures for unlexicalized parsing. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics: Posters, COLING ?10, pages
117?125, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?02,
pages 263?270, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Diederich, J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with
support vector machines. Applied Intelligence,
19(1):109?123.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Antonion Miranda Garcia and Javier Calle Mar-
tin. 2007. Function words in authorship attribu-
tion studies. Literary and Linguistic Computing,
22(1):49?66.
Graeme Hirst and Olga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of
short texts. Literary and Linguistic Computing,
22(4):405?417.
D. L. Hoover. 2004. Testing burrow?s delta. Literary
and Linguistic Computing, 19(4):453?475.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proc.
of the 12th International Conference on Artificial
Intelligence: Methodology, Systems and Applica-
tions, volume 4183 of LNCS, pages 77?86, Varna,
Bulgaria. Springer.
Aravind K. Joshi. 1992. Statistical language mod-
eling. In Proceedings of a Workshop Held at Har-
riman, New York, February 23-26, 1992. Associa-
tion for Computational Linguistics.
S. Kemper. 1987. Life-span changes in syntactic
complexity. Journal of gerontology, 42(3):323.
Vlado Keselj, Fuchun Peng, Nick Cercone, and
Calvin Thomas. 2003. N-gram-based author pro-
files for authorship attribution. In Proc. of the
1531
Pacific Association for Computational Linguistics,
pages 255?264.
M. Koppel and J. Schler. 2003. Exploiting stylistic
idiosyncrasies for authorship attribution. In Pro-
ceedings of IJCAI, volume 3, pages 69?72. Cite-
seer.
D. Lin. 1995. University of manitoba: descrip-
tion of the pie system used for muc-6. In Pro-
ceedings of the 6th conference on Message under-
standing, pages 113?126. Association for Compu-
tational Linguistics.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 64?71.
Association for Computational Linguistics.
Kim Luyckx and Walter Daelemans. 2008. Author-
ship attribution and verification with many au-
thors and limited data. In COLING ?08, pages
513?520.
T.C. Mendenhall. 1887. The characteristic curves of
composition. Science, ns-9(214S):237?246.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization.
In Proceedings of the 17th ACM conference on In-
formation and knowledge management, CIKM ?08,
pages 253?262, New York, NY, USA. ACM.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Fuchun Peng, Dale Schuurmans, Shaojun Wang, and
Vlado Keselj. 2003. Language independent au-
thorship attribution using character level language
models. In Proceedings of the tenth conference on
European chapter of the Association for Compu-
tational Linguistics - Volume 1, EACL ?03, pages
267?274, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 1 - Volume 1, EMNLP ?09, pages 111?120,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: a unified framework for predicting
text quality. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 186?195, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arthus Quinn. 1995. Figures of Speech: 60 Ways To
Turn A Phrase. Routledge.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using
probabilistic context-free grammars. In Proceed-
ings of the ACL 2010 Conference Short Papers,
pages 38?42, Uppsala, Sweden. Association for
Computational Linguistics.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin
Choi. 2011. Gender attribution: tracing stylo-
metric evidence beyond topic and genre. In Pro-
ceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?11,
pages 78?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
K.T. Shao. 1990. Tree balance. Systematic Biology,
39(3):266.
Efstathios Stamatatos, George Kokkinakis, and
Nikos Fakotakis. 2000. Automatic text catego-
rization in terms of genre and author. Comput.
Linguist., 26(4):471?495.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis.
2001. Computer-based authorship attribution
without lexical measures. Computers and the Hu-
manities, 35(2):193?214.
E. Stamatatos. 2006. Ensemble-based author iden-
tification using character n-grams. ReCALL, page
4146.
W. Strunk and E.B. White. 2008. The elements of
style. Penguin Group USA.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1600?1610, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ying Zhao and Justin Zobel. 2005. Effective
and scalable authorship attribution using func-
tion words. In Proceedings of the Second Asia
conference on Asia Information Retrieval Technol-
ogy, AIRS?05, pages 174?189, Berlin, Heidelberg.
Springer-Verlag.
Y. Zhao and J. Zobel. 2007. Searching with style:
Authorship attribution in classic literature. In
Proceedings of the thirtieth Australasian confer-
ence on Computer science-Volume 62, pages 59?
68. Australian Computer Society, Inc.
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship identi-
fication of online messages: Writing-style features
1532
and classification techniques. J. Am. Soc. Inf. Sci.
Technol., 57(3):378?393.
1533
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1469?1473,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Keystroke Patterns as Prosody in Digital Writings:
A Case Study with Deceptive Reviews and Essays
Ritwik Banerjee Song Feng Jun S. Kang
Computer Science
Stony Brook University
{rbanerjee, songfeng, junkang}
@cs.stonybrook.edu
Yejin Choi
Computer Science & Engineering
University of Washington
yejin@cs.washington.edu
Abstract
In this paper, we explore the use of keyboard
strokes as a means to access the real-time writ-
ing process of online authors, analogously to
prosody in speech analysis, in the context of
deception detection. We show that differences
in keystroke patterns like editing maneuvers
and duration of pauses can help distinguish be-
tween truthful and deceptive writing. Empiri-
cal results show that incorporating keystroke-
based features lead to improved performance
in deception detection in two different do-
mains: online reviews and essays.
1 Introduction
Due to the practical importance of detecting deceit, in-
terest in it is ancient, appearing in papyrus dated back
to 900 B.C. (Trovillo, 1939). In more recent years, sev-
eral studies have shown that the deceiver often exhibits
behavior that belies the content of communication, thus
providing cues of deception to an observer. These in-
clude linguistic (e.g., Newman et al. (2003), Hancock
et al. (2004)) as well as paralinguistic (e.g., Ekman et
al. (1991), DePaulo et al. (2003)) cues. Recognizing
deception, however, remains a hard task for humans,
who perform only marginally better than chance (Bond
and DePaulo, 2006; Ott et al., 2011).
Recent studies suggest that computers can be sur-
prisingly effective in this task, albeit in limited domains
such as product reviews. Prior research has employed
lexico-syntactic patterns (Ott et al., 2011; Feng et al.,
2012) as well as online user behavior (Fei et al., 2013;
Mukherjee et al., 2013). In this paper, we study the
effect of keystroke patterns for deception detection in
digital communications, which might be helpful in un-
derstanding the psychology of deception and help to-
ward trustful online communities. This allows us to in-
vestigate differences in the writing and revisional pro-
cesses of truthful and fake writers. Our work thus
shares intuition with HCI research linking keystroke
analysis to cognitive processes (Vizer et al., 2009; Epp
et al., 2011) and psychology research connecting cog-
nitive differences to deception (Ekman, 2003; Vrij et
al., 2006).
Recent research has shown that lying generally im-
poses a cognitive burden (e.g., McCornack (1997), Vrij
et al. (2006)) which increases in real-time scenar-
ios (Ekman, 2003). Cognitive burden has been known
to produce differences in keytroke features (Vizer et
al., 2009; Epp et al., 2011). Previous research has not,
however, directly investigated any quantitative connec-
tion between keystroke patterns and deceptive writing.
In this paper, we posit that cognitive burdens in
deception may lead to measurable characteristics in
keystroke patterns. Our contributions are as follows:
(1) introducing keystroke logs as an extended linguis-
tic signal capturing the real-time writing process (anal-
ogous to prosody in speech analysis) by measuring the
writing rate, pauses and revision rate. (2) showing
their empirical value in deception detection, (3) provid-
ing novel domain-specific insights into deceptive writ-
ing, and (4) releasing a new corpus of deception writ-
ings in new domains.
1
2 Related Work
Prior research has focused mainly on using keystroke
traits as a behvioral biometric. Forsen et al. (1977)
first demonstrated that users can be distinguished by the
way they type their names. Subsequent work showed
that typing patterns are unique to individuals (Leggett
and Williams, 1988), and can be used for authentica-
tion (Cho et al., 2000; Bergadano et al., 2002) and in-
trusion detection (Killourhy and Maxion, 2009).
Keystroke pauses have been linked to linguistic pat-
terns in discourse (e.g. Matsuhashi (1981), van Hell et
al. (2008)) and regarded as indications of cognitive bur-
den (e.g., Johansson (2009), Zulkifli (2013)). In this pa-
per, we present the first empirical study that quantita-
tively measures the deception cues in real-time writing
process as manifested in keystroke logs.
3 Data Collection
As discussed by Gokhman et al. (2012), the crowd-
sourcing approach to soliciting deceptive content sim-
ulates the real world of online deceptive content cre-
ators. We collected the data via Amazon Mechanical
Turk.
2
Turkers were led to a separate website where
keylogging was enabled, and asked to write truthful
and deceptive texts (? 100 words) on one of three top-
1
Available at http://www3.cs.stonybrook.
edu/
?
junkang/keystroke/
2
https://www.mturk.com/mturk
1469
ArrowKey Del MouseUp
0
5
10
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
Fre
que
ncy
 of e
ditin
g ke
ystr
oke
s
Deceptive Truthful
Figure 1: Number of keystrokes corresponding to the three
types of edit patterns (E
3
): (a) use of arrow keys, (b) deletion
(Delete and Backspace) and (c) text selection with mouse.
ics: restaurant review, gay marriage and gun control.
Each Turker was required to agree to their typing be-
ing logged. Since copy/paste operations defeat our pur-
pose of studying keystrokes in the typing process, they
were disabled. This restriction also acts as a hindrance
to plagiarism. All texts were reviewed manually, and
those not meeting the requirements (due to the being
too short, plagiarized content, etc.) were disregarded.
Writing task design: The task was designed such
that each Turker wrote a pair of texts, one truthful and
one deceptive, on the same topic. For restaurant re-
views, they were asked to write a truthful review of
a restaurant they liked, and a deceptive review of a
restaurant they have never been to or did not like. For
the other two topics ? ?gun control? and ?gay marriage?
? we asked their opinion: support, neutral, or against.
Then, they were asked to write a truthful and a decep-
tive essay articulating, respectively, their actual opin-
ion and its opposite.
3
The tasks further were divided
into two ?flows?: writing the truthful text before the de-
ceptive one, and vice versa. Each Turker was assigned
only one flow, and was not allowed to participate in the
other. After completing this, each Turker was asked to
copy their own typing, i.e., re-type the two texts.
Finally, in order to get an idea of the cognitive bur-
den associated with truthful and deceptive writing, we
asked the Turkers which task was easier for them. Of
the 196 participants, 152 answered ?truthful?, 40 an-
swered ?deceptive? and only 4 opted for ?not sure?.
What are logged: We deployed a keylogger to cap-
ture the mouse and keyboard events in the ?text area?.
The events KeyUp, KeyDown and MouseUp, along with
the keycode and timestamp were logged.
4
For the three
topics restaurant review, gay marriage and gun control
we obtained 1000, 800 and 800 texts, respectively.
In the remainder of this paper, k
dn
and k
up
denote
the KeyDown and KeyUp events for a key k. For any
3
To prevent a change in opinion depending on task avail-
ability, Turkers were redirected to other tasks if their opinion
was neutral, or if we had enough essays of their opinion.
4
Printable (e.g., alphanumeric characters) as well as non-
printable keystrokes like (e.g., ?Backspace?), are logged.
Document Sentence Word Key Press
1.5
2.0
2.5
1.5
2.0
2.5
First?only
First+Second
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
GayMarriage
GunControl
Restaurant
Tim
e ta
ken
 (rel.
 to c
opy t
ask
)
Deceptive Truthful
Figure 2: Average normalized timespan ??(e) for documents,
sentences, words and key presses. The top row considers only
the first text, while the bottom row considers both flows.
event e, its timespan, i.e., the time interval between the
beginning and end of e, is denoted by ?(e).
4 Feature Design
Keystroke logging enables the study of two types of in-
formation that go beyond conventional linguistic anal-
ysis. First, it captures editing processes (e.g., deletions,
insertions made by changing cursor position, etc.).
Second, it reveals the temporal aspect of text generation
(e.g., duration, latency). Our exploration of these fea-
tures and their application in deception detection is mo-
tivated by the similarities between text and speech gen-
eration. Editing patterns, for instance, can be viewed as
attempts to veil incoherence in deceptive writing and
temporal patterns like latency or pause can be treated
as analogous to disfluency.
Different people, of course, have varying typing
skills, and some may type faster than others. In or-
der to control for this variation, we normalize all event
timespans ?(e) with respect to the corresponding event
timespan in the copy task:
?
?(e) = ?(e)/?(e
copy
).
4.1 Editing Patterns
In this work, we treat keys that are used only for edit-
ing as different from others. Text editing is done by
employing a small subset of available keys: deletion
keys (?Backspace? and ?Delete?), arrow keys (?, ?,
? and ?) and by using the mouse for text selection
(i.e., the ?MouseUp? event). The three types of editing
keystrokes are collectively denoted by
E
3
= ?|DEL| , |MSELECT| , |ARROW|?
where
(i) |DEL| = number of deletion keystrokes
(ii) |MSELECT| = number of ?MouseUp? events, and
(iii) |ARROW| = number of arrow keystrokes
The editing differences between truthful and deceptive
writing across all three topics are shown in Fig. 1.
4.2 Temporal Aspects
Each event is logged with a keycode and a timestamp.
In order to study the temporal aspects of digital writ-
ing, we calculate the timespan of different linguistic
1470
Topic Features Flow
First + Second First-only
Restaurants
BoW 73.9 78.8
BoW + T
6
74.3 79.1
BoW + T
6
+ E
3
74.6 80.3
?
Gun Control
(Support)
BoW 86.5 80.0
BoW + T
6
86.8 82.5
?
BoW + T
6
+ E
3
88.0
?
83.5
?
Gun Control
(Oppose)
BoW 88.5 88.0
BoW + T
6
89.8 87.5
BoW + T
6
+ E
3
90.8
?
89.1
Gay Marriage
(Support)
BoW 92.5 92.0
BoW + T
6
93.8 92.5
BoW + T
6
+ E
3
94.3
?
92.0
Gay Marriage
(Oppose)
BoW 84.5 86.5
BoW + T
6
85.0 87.0
BoW + T
6
+ E
3
85.3 86.8
Table 1: SVM classifier performance for truthful vs. de-
ceptive writing. Statistically significant improvements over
the baseline are marked * (p < 0.05) and ? (p < 0.1).
E
3
= ?|DEL| , |MSELECT| , |ARROW|? denotes the editing
keystrokes, and T
6
is the set of normalized timespans of
documents, words (plus preceding keystroke), all keystrokes,
spaces, non-whitespace keystrokes and inter-word intervals:
T
6
= {??(D), ??(k), ??(SP), ??(?SP), ??(?W), ??(k
prv
+ W)}
units such as words, sentences and even entire docu-
ments. Further, we separately inspect the timespans
of different parts of speech, function words and con-
tent words. In addition to event timespans, intervals
between successive events (e.g., inter-word and inter-
sentence pauses) and pauses preceding or succeeding
and event (e.g., time interval before and after a function
word) are measured as well.
5 Experimental Results
This section describes our experimental setup and
presents insights based on the obtained results. All
classification experiments use 5-fold cross validation
with 80/20 division for training and testing. In addition
to experimenting on the entire dataset, we also sepa-
rately analyze the texts written first (of the two texts in
each ?flow?). This additional step is taken in order to
eliminate the possibility of a text being primed by its
preceding text.
Deception cues in keystroke patterns: To empiri-
cally check whether keystroke features can help distin-
guish between truthful and deceptive writing, we de-
sign binary SVM classifiers.
5
Unigrams with tf-idf
encoding is used as the baseline. The average baseline
accuracy across all topics is 82.58% when considering
both texts of a flow, and 83.62% when considering only
the first text of each flow. The better performance in the
latter possibly indicates that the second text of a flow
exhibits some amount of lexical priming with the first.
The high accuracy of the baseline is not surprising.
Previous work by Ott et al. (2011) reported similar per-
5
We use the LIBLINEAR (Fan et al., 2008) package.
??(W) ??(kprv + W)
D > T T > D D > T T > D
our best when one
if get quality other
when well even get
were your on service
it?s fresh by been
quality not me their
dishes my has not
the one also with
i?ve had go friendly
on hat we great
they of had an
we other is our
friendly very at are
has love which really
at service from but
wait great dishes favorite
an really or very
go you re about
is but would will
which been just here
Table 2: Top 20 words in restaurant reviews with greatest
timespan difference between deceptive and truthful writing.
formance of unigram models. The focus of our work
is to explore the completely new feature space of ty-
pographic patterns in deception detection. We draw
motivation from parallels between the text generation
and speech generation processes. Prosodic concepts
such as speed, disfluency and coherence can be real-
ized in typographic behavior by analyzing timestamp
of keystrokes, pauses and editing patterns, respectively.
Based on the differences in the temporal aspects of
keystrokes, we extract six timespan features to improve
this baseline. This set, denoted by T
6
, comprises of
(i)
?
?(D) = timespan of entire document
(ii)
?
?(k
prv
+W) = average timespan of word plus pre-
ceding keystroke
(iii)
?
?(k) = average keystroke timespan
(iv)
?
?(SP) = average timespan of spaces
(v)
?
?(?SP) = average timespan of non whitesp-
ace keystrokes
(vi)
?
?(?W) = average interval between words.
The improvements attained by adding T
6
to the base-
line are shown in Table 1. Adding the edit patterns (E
3
)
(cf. ? 4.1) further improves the performance (with the
exception of two cases) by 0.7?3.5%.
Writing speed, pauses and revisions: To study the
temporal aspect of language units across all topics,
we first consider all texts, and then restrict to only
the first of each ?flow?. The timespan measurements
are presented in Fig. 2, showing the average duration
of typing documents, sentences, words and individual
keystrokes. The timespans are measured as the inter-
val between the first and the last keystrokes. The sen-
tence timespan, for instance, does not include the gap
between a sentence end and the first keystroke marking
the beginning of the next.
The sentence timespans for ?gay marriage? and ?gun
1471
DT+TD
120
130
140
150
160
170
All Words
Function Words
Content Words
Nouns Verbs Adjectives
Adverbs
Tim
esp
an (m
s) Deceptive
Truthful
(a)
DT+TD
350
400
450
500
550
Function Words
Content Words
Nouns Verbs Adjectives
Adverbs
Tim
esp
an (m
s) Deceptive
Truthful
(b)
Figure 3: Event timespans in restaurant reviews: (a) language units, and (b) language units including their preceding k
dn
.
control? are lower in truthful writing, even though the
document timespans are higher. This difference implies
that the writer is spending a longer period of time to
think before commencing the next sentence, but once
started, the actual typing proceeds rapidly.
Apart from restaurant reviews, truthful writers have
typed slower. This may be due to exercising better care
while expressing their honest opinion.
For restaurant reviews, the document, sentence and
word timespans are significantly higher in deceptive
writing. This, however, is not the case for documents
and words in the other two topics. We conjecture that
this is because deception is harder to write for prod-
uct reviews, due to their dependence on factual details.
Gun control and gay marriage, on the other hand, are
topics well discussed in media, and it is possible that
the writers are aware of the arguments that go against
their personal belief. The frequency of revisional oc-
currences (i.e., keys used for editing) shown in Fig. 1,
too, supports the thought that writing fake reviews may
be harder than adopting a fake stance on well-known
issues. Deceptive reviews exhibit a higher number of
revisions than truthful ones, but essays show the oppo-
site trend. Our findings align with previous studies (Ott
et al., 2011) which showed that deception cues are do-
main dependent.
Writing speed variations over word categories:
Next, we investigate whether there is any quantitative
difference in the writing rate over different words with
respect to the deceptive and truthful intent of the author.
In an attempt to understand this, we analyze words
which show the highest timespan difference between
deceptive and truthful writings.
Table 2 presents words in the restaurant review
topic for which deceptive writers took a lot longer
than truthful writers, and vice versa. Some word cat-
egories exhibit common trends across all three top-
ics. Highly subjective words, for instance (e.g., ?love?,
?best?, ?great?) are words over which truthful writers
spent more time.
Deceptive and truthful texts differ in the typing rate
of first- and second-person pronouns. Deceptive re-
views reveal more time spent in using 2
nd
-person pro-
nouns, as shown by ?you? and ?your?. This finding
throws some light on how people perceive text cues.
Toma and Hancock (2012) showed that readers per-
form poorly at deception detection because they rely on
unrelated text cues such as 2
nd
-person pronouns. Our
analysis indicates that people associate the use of 2
nd
-
person pronouns more with deception not only while
reading, but while writing as well.
Deceptive reviews also exhibit longer time spans for
1
st
-person pronouns (e.g., ?we?, ?me?), which have
been known to be useful in deception detection (New-
man et al., 2003; Ott et al., 2011). Newman et al.
(2003) attributed the less frequent usage of 1
st
-person
pronouns to psychological distancing. The longer time
taken by deceptive writers in our data is a possible sign
of increased cognitive burden when the writer is unable
to maintain the psychological distance. Deceptive re-
viewers also paused a lot more around relative clauses,
e.g., ?if?, ?when?, and ?which?.
In essays, however, the difference in timespans of
1
st
-person and 2
nd
-person pronouns as well as the
timespan difference in relative clauses were insignifi-
cant (< 50ms).
A broader picture of the temporal difference in using
different types of words is presented in Fig. 3, which
shows deceptive reviewers spending less time on ad-
verbs as compared to truthful writers, but more time on
nouns, verbs, adjectives, function words and content
words. They also exhibited significantly longer pauses
before nouns, verbs and function words.
6 Conclusion
In this paper, we investigated the use of typographic
style in deception detection and presented distinct tem-
poral and revisional aspects of keystroke patterns that
improve the characterization of deceptive writing. Our
study provides novel empirically supported insights
into the writing and editing processes of truthful and
deceptive writers. It also presents the first application
of keylogger data used to distinguish between true and
fake texts, and opens up a new range of questions to
better understand what affects these different keystroke
patterns and what they exhibit. It also suggests new
possibilities for making use of keystroke information
as an extended linguistic signal to accompany writings.
Acknowledgements
This research is supported in part by gift from Google.
1472
References
Francesco Bergadano, Daniele Gunetti, and Claudia Pi-
cardi. 2002. User Authentication through Keystroke
Dynamics. ACM Transactions on Information and
System Security (TISSEC), 5(4):367?397.
Charles F Bond and Bella M DePaulo. 2006. Accu-
racy of Deception Judgments. Personality and So-
cial Psychology Review, 10(3):214?234.
Sungzoon Cho, Chigeun Han, Dae Hee Han, and
Hyung-Il Kim. 2000. Web-based Keystroke Dy-
namics Identity Verification Using Neural Network.
Journal of Organizationl Computing and Electronic
Commerce, 10(4):295?307.
Bella M DePaulo, James J Lindsay, Brian E Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Harris
Cooper. 2003. Cues to Deception. Psychological
Bulletin, 129(1):74.
Paul Ekman, Maureen O?Sullivan, Wallace V Friesen,
and Klaus R Scherer. 1991. Invited Article: Face,
Voice and Body in Detecting Deceit. Journal of
Nonverbal Behavior, 15(2):125?135.
Paul Ekman. 2003. Darwin, Deception, and Facial Ex-
pression. Annals of the New York Academy of Sci-
ences, 1000(1):205?221.
Clayton Epp, Michael Lippold, and Regan L Mandryk.
2011. Identifying Emotional States Using Keystroke
Dynamics. In Proc. of the SIGCHI Conference on
Human Factors in Computing Systems, pages 715?
724. ACM.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. The Jour-
nal of Machine Learning Research, 9:1871?1874.
Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu,
Malu Castellanos, and Riddhiman Ghosh. 2013.
Exploiting Burstiness in Reviews for Review Spam-
mer Detection. In ICWSM, pages 175?184.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic Stylometry for Deception Detection. In
Proc. 50th Annual Meeting of the ACL, pages 171?
175. ACL.
George E Forsen, Mark R Nelson, and Raymond J
Staron Jr. 1977. Personal Attributes Authentication
Techniques. Technical report, DTIC Document.
Stephanie Gokhman, Jeff Hancock, Poornima Prabhu,
Myle Ott, and Claire Cardie. 2012. In Search of a
Gold Standard in Studies of Deception. In Compu-
tational Approaches to Deception Detection, pages
23?30. ACL.
Jeffrey T Hancock, L Curry, Saurabh Goorha, and
Michael T Woodworth. 2004. Lies in Conversa-
tion: An Examination of Deception Using Auto-
mated Linguistic Analysis. In Annual Conference
of the Cognitive Science Society, volume 26, pages
534?540.
Victoria Johansson. 2009. Developmental Aspects of
Text Production in Writing and Speech. Ph.D. thesis,
Lund University.
Kevin S Killourhy and Roy A Maxion. 2009. Compar-
ing Anomaly-Detection Algorithms for Keystroke
Dynamics. In Dependable Systems & Networks,
2009. DSN?09., pages 125?134. IEEE.
John Leggett and Glen Williams. 1988. Verifying
Identity Via Keystroke Characteristics. Interna-
tional Journal of Man-Machine Studies, 28(1):67?
76.
Ann Matsuhashi. 1981. Pausing and Planning: The
Tempo of Written Discourse Production. Research
in the Teaching of English, pages 113?134.
Steven A McCornack. 1997. The Generation of De-
ceptive Messages: Laying the Groundwork for a Vi-
able Theory of Interpersonal Deception. In John O
Greene, editor, Message Production: Advances in
Communication Theory. Erlbaum, Mahwah, NJ.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Natalie Glance. 2013. What Yelp Fake Review Fil-
ter Might be Doing. In ICSWM, pages 409?418.
Matthew L Newman, James W Pennebaker, Diane S
Berry, and Jane M Richards. 2003. Lying Words:
Predicting Deception from Linguistic Styles. Per-
sonality and Social Psychology Bulletin, 29(5):665?
675.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. 2011. Finding Deceptive Opinion Spam
by Any Stretch of the Imagination. In Proc. 49th
Annual Meeting of the ACL: HLT, pages 309?319.
ACL.
Catalina L Toma and Jeffrey T Hancock. 2012. What
Lies Beneath: The Linguistic Traces of Deception in
Online Dating Profiles. Journal of Communication,
62(1):78?97.
Paul V Trovillo. 1939. A History of Lie Detection.
Journal of Criminal Law and Criminology (1931-
1951), 29:848?881.
Janet G van Hell, Ludo Verhoeven, and Liesbeth M van
Beijsterveldt. 2008. Pause Time Patterns in Writ-
ing Narrative and Expository Texts by Children and
Adults. Discourse Processes, 45(4-5):406?427.
Lisa M Vizer, Lina Zhou, and Andrew Sears. 2009.
Automated Stress Detection Using Keystroke and
Linguistic Features: An Exploratory Study. In-
ternational Journal of Human-Computer Studies,
67(10):870?886.
Aldert Vrij, Ronald Fisher, Samantha Mann, and
Sharon Leal. 2006. Detecting Deception by Ma-
nipulating Cognitive Load. Trends in Cognitive Sci-
ences, 10(4):141?142.
Putri Zulkifli. 2013. Applying Pause Analysis to Ex-
plore Cognitive Processes in the Copying of Sen-
tences by Second Language Users. Ph.D. thesis,
University of Sussex.
1473
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 171?175,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Stylometry for Deception Detection
Song Feng Ritwik Banerjee Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
Abstract
Most previous studies in computerized de-
ception detection have relied only on shal-
low lexico-syntactic patterns. This pa-
per investigates syntactic stylometry for
deception detection, adding a somewhat
unconventional angle to prior literature.
Over four different datasets spanning from
the product review to the essay domain,
we demonstrate that features driven from
Context Free Grammar (CFG) parse trees
consistently improve the detection perfor-
mance over several baselines that are based
only on shallow lexico-syntactic features.
Our results improve the best published re-
sult on the hotel review data (Ott et al,
2011) reaching 91.2% accuracy with 14%
error reduction.
1 Introduction
Previous studies in computerized deception de-
tection have relied only on shallow lexico-
syntactic cues. Most are based on dictionary-
based word counting using LIWC (Pennebaker
et al, 2007) (e.g., Hancock et al (2007), Vrij et
al. (2007)), while some recent ones explored the
use of machine learning techniques using sim-
ple lexico-syntactic patterns, such as n-grams
and part-of-speech (POS) tags (Mihalcea and
Strapparava (2009), Ott et al (2011)). These
previous studies unveil interesting correlations
between certain lexical items or categories with
deception that may not be readily apparent to
human judges. For instance, the work of Ott
et al (2011) in the hotel review domain results
in very insightful observations that deceptive re-
viewers tend to use verbs and personal pronouns
(e.g., ?I?, ?my?) more often, while truthful re-
viewers tend to use more of nouns, adjectives,
prepositions. In parallel to these shallow lexical
patterns, might there be deep syntactic struc-
tures that are lurking in deceptive writing?
This paper investigates syntactic stylometry
for deception detection, adding a somewhat un-
conventional angle to prior literature. Over four
different datasets spanning from the product re-
view domain to the essay domain, we find that
features driven from Context Free Grammar
(CFG) parse trees consistently improve the de-
tection performance over several baselines that
are based only on shallow lexico-syntactic fea-
tures. Our results improve the best published re-
sult on the hotel review data of Ott et al (2011)
reaching 91.2% accuracy with 14% error reduc-
tion. We also achieve substantial improvement
over the essay data of Mihalcea and Strapparava
(2009), obtaining upto 85.0% accuracy.
2 Four Datasets
To explore different types of deceptive writing,
we consider the following four datasets spanning
from the product review to the essay domain:
I. TripAdvisor?Gold: Introduced in Ott et
al. (2011), this dataset contains 400 truthful re-
views obtained from www.tripadviser.com and
400 deceptive reviews gathered using Amazon
Mechanical Turk, evenly distributed across 20
Chicago hotels.
171
TripAdvisor?Gold TripAdvisor?Heuristic
Deceptive Truthful Deceptive Truthful
NP?PP ? DT NNP NNP NNP S?ROOT ? VP . NP?S ? PRP VP?S ? VBZ NP
SBAR?NP ? S NP?NP ? $ CD SBAR?S ? WHADVP S NP?NP ? NNS
NP?VP ? NP SBAR PRN?NP ? LRB NP RRB VP?S ? VBD PP WHNP?SBAR ? WDT
NP?NP ? PRP$ NN NP?NP ? NNS S?SBAR ? NP VP NP?NP ? NP PP PP
NP?S ? DT NNP NNP NNP NP?S ? NN S?ROOT ? PP NP VP . NP?S ? EX
VP?S ? VBG PP NP?PP ? DT NNP VP?S ? VBD S NX?NX ? JJ NN
NP?PP ? PRP$ NN NP?PP ? CD NNS NP?S ? NP CC NP NP?NP ? NP PP
VP?S ? MD ADVP VP NP?NP ? NP PRN NP?S ? PRP$ NN VP?S ? VBZ RB NP
VP?S ? TO VP PRN?NP ? LRB PP RRB NP?PP ? DT NNP PP?NP ? IN NP
ADJP?NP ? RBS JJ NP?NP ? CD NNS NP?PP ? PRP$ NN PP?ADJP ? TO NP
Table 1: Most discriminative rewrite rules (r?): hotel review datasets
Figure 1: Parsed trees
II. TripAdvisor?Heuristic: This dataset
contains 400 truthful and 400 deceptive reviews
harvested from www.tripadviser.com, based
on fake review detection heuristics introduced
in Feng et al (2012).1
III. Yelp: This dataset is our own creation
using www.yelp.com. We collect 400 filtered re-
views and 400 displayed reviews for 35 Italian
restaurants with average ratings in the range of
[3.5, 4.0]. Class labels are based on the meta
data, which tells us whether each review is fil-
tered by Yelp?s automated review filtering sys-
tem or not. We expect that filtered reviews
roughly correspond to deceptive reviews, and
displayed reviews to truthful ones, but not with-
out considerable noise. We only collect 5-star
reviews to avoid unwanted noise from varying
1Specifically, using the notation of Feng et al (2012),
we use data created by Strategy-dist? heuristic, with
HS ,S as deceptive and H ?S , T as truthful.
degree of sentiment.
IV. Essays: Introduced in Mihalcea and
Strapparava (2009), this corpus contains truth-
ful and deceptive essays collected using Amazon
Mechanic Turk for the following three topics:
?Abortion? (100 essays per class), ?Best Friend?
(98 essays per class), and ?Death Penalty? (98
essays per class).
3 Feature Encoding
Words Previous work has shown that bag-of-
words are effective in detecting domain-specific
deception (Ott et al, 2011; Mihalcea and Strap-
parava, 2009). We consider unigram, bigram,
and the union of the two as features.
Shallow Syntax As has been used in many
previous studies in stylometry (e.g., Argamon-
Engelson et al (1998), Zhao and Zobel (2007)),
we utilize part-of-speech (POS) tags to encode
shallow syntactic information. Note that Ott
et al (2011) found that even though POS tags
are effective in detecting fake product reviews,
they are not as effective as words. Therefore, we
strengthen POS features with unigram features.
Deep syntax We experiment with four differ-
ent encodings of production rules based on the
Probabilistic Context Free Grammar (PCFG)
parse trees as follows:
? r: unlexicalized production rules (i.e., all
production rules except for those with ter-
minal nodes), e.g., NP2 ? NP3 SBAR.
? r?: lexicalized production rules (i.e., all
production rules), e.g., PRP ? ?you?.
? r?: unlexicalized production rules combined
with the grandparent node, e.g., NP2 ?VP
172
TripAdvisor Yelp Essay
Gold Heur Abort BstFr Death
unigram 88.4 74.4 59.9 70.0 77.0 67.4
words bigram 85.8 71.5 60.7 71.5 79.5 55.5
uni + bigram 89.6 73.8 60.1 72.0 81.5 65.5
pos(n=1) + unigram 87.4 74.0 62.0 70.0 80.0 66.5
shallow syntax pos(n=2) + unigram 88.6 74.6 59.0 67.0 82.0 66.5
+words pos(n=3) + unigram 88.6 74.6 59.3 67.0 82.0 66.5
r 78.5 65.3 56.9 62 67.5 55.5
deep syntax r? 74.8 65.3 56.5 58.5 65.5 56.0
r? 89.4 74.0 64.0 70.1 77.5 66.0
r?? 90.4 75 63.5 71.0 78 67.5
r + unigram 89.0 74.3 62.3 76.5 82.0 69.0
deep syntax r? + unigram 88.5 74.3 62.5 77.0 81.5 70.5
+words r? + unigram 90.3 75.4 64.3 74.0 85.0 71.5
r?? + unigram 91.2 76.6 62.1 76.0 84.5 71.0
Table 2: Deception Detection Accuracy (%).
1 ? NP3 SBAR.
? r??: lexicalized production rules (i.e., all
production rules) combined with the grand-
parent node, e.g., PRP?NP 4 ? ?you?.
4 Experimental Results
For all classification tasks, we use SVM classi-
fier, 80% of data for training and 20% for test-
ing, with 5-fold cross validation.2 All features
are encoded as tf-idf values. We use Berkeley
PCFG parser (Petrov and Klein, 2007) to parse
sentences. Table 2 presents the classification
performance using various features across four
different datasets introduced earlier.3
4.1 TripAdvisor?Gold
We first discuss the results for the TripAdvisor?
Gold dataset shown in Table 2. As reported in
Ott et al (2011), bag-of-words features achieve
surprisingly high performance, reaching upto
89.6% accuracy. Deep syntactic features, en-
coded as r?? slightly improves this performance,
achieving 90.4% accuracy. When these syntactic
features are combined with unigram features, we
attain the best performance of 91.2% accuracy,
2We use LIBLINEAR (Fan et al, 2008) with L2-
regulization, parameter optimized over the 80% training
data (3 folds for training, 1 fold for testing).
3Numbers in italic are classification results reported
in Ott et al (2011) and Mihalcea and Strapparava (2009).
yielding 14% error reduction over the word-only
features.
Given the power of word-based features, one
might wonder, whether the PCFG driven fea-
tures are being useful only due to their lexi-
cal production rules. To address such doubts,
we include experiments with unlexicalized rules,
r and r?. These features achieve 78.5% and
74.8% accuracy respectively, which are signifi-
cantly higher than that of a random baseline
(?50.0%), confirming statistical differences in
deep syntactic structures. See Section 4.4 for
concrete exemplary rules.
Another question one might have is whether
the performance gain of PCFG features are
mostly from local sequences of POS tags, indi-
rectly encoded in the production rules. Compar-
ing the performance of [shallow syntax+words]
and [deep syntax+words] in Table 2, we find sta-
tistical evidence that deep syntax based features
offer information that are not available in simple
POS sequences.
4.2 TripAdvisor?Heuristic & Yelp
The performance is generally lower than that of
the previous dataset, due to the noisy nature
of these datasets. Nevertheless, we find similar
trends as those seen in the TripAdvisor?Gold
dataset, with respect to the relative performance
differences across different approaches. The sig-
173
TripAdvisor?Gold TripAdvisor?Heur
Decep Truth Decep Truth
VP PRN VP PRN
SBAR QP WHADVP NX
WHADVP S SBAR WHNP
ADVP PRT WHADJP ADJP
CONJP UCP INTJ WHPP
Table 3: Most discriminative phrasal tags in PCFG
parse trees: TripAdvisor data.
nificance of these results comes from the fact
that these two datasets consists of real (fake)
reviews in the wild, rather than manufactured
ones that might invite unwanted signals that
can unexpectedly help with classification accu-
racy. In sum, these results indicate the exis-
tence of the statistical signals hidden in deep
syntax even in real product reviews with noisy
gold standards.
4.3 Essay
Finally in Table 2, the last dataset Essay con-
firms the similar trends again, that the deep syn-
tactic features consistently improve the perfor-
mance over several baselines based only on shal-
low lexico-syntactic features. The final results,
reaching accuracy as high as 85%, substantially
outperform what has been previously reported
in Mihalcea and Strapparava (2009). How ro-
bust are the syntactic cues in the cross topic set-
ting? Table 4 compares the results of Mihalcea
and Strapparava (2009) and ours, demonstrat-
ing that syntactic features achieve substantially
and surprisingly more robust results.
4.4 Discriminative Production Rules
To give more concrete insights, we provide
10 most discriminative unlexicalized production
rules (augmented with the grand parent node)
for each class in Table 1. We order the rules
based on the feature weights assigned by LIB-
LINEAR classifier. Notice that the two produc-
tion rules in bolds ? [SBAR?NP? S] and [NP
?VP? NP SBAR] ? are parts of the parse tree
shown in Figure 1, whose sentence is taken from
an actual fake review. Table 3 shows the most
discriminative phrasal tags in the PCFG parse
training: A & B A & D B & D
testing: DeathPen BestFrn Abortion
M&S 2009 58.7 58.7 62.0
r? 66.8 70.9 69.0
Table 4: Cross topic deception detection accuracy:
Essay data
trees for each class. Interestingly, we find more
frequent use of VP, SBAR (clause introduced
by subordinating conjunction), and WHADVP
in deceptive reviews than truthful reviews.
5 Related Work
Much of the previous work for detecting de-
ceptive product reviews focused on related, but
slightly different problems, e.g., detecting dupli-
cate reviews or review spams (e.g., Jindal and
Liu (2008), Lim et al (2010), Mukherjee et al
(2011), Jindal et al (2010)) due to notable dif-
ficulty in obtaining gold standard labels.4 The
Yelp data we explored in this work shares a sim-
ilar spirit in that gold standard labels are har-
vested from existing meta data, which are not
guaranteed to align well with true hidden la-
bels as to deceptive v.s. truthful reviews. Two
previous work obtained more precise gold stan-
dard labels by hiring Amazon turkers to write
deceptive articles (e.g., Mihalcea and Strappa-
rava (2009), Ott et al (2011)), both of which
have been examined in this study with respect
to their syntactic characteristics. Although we
are not aware of any prior work that dealt
with syntactic cues in deceptive writing directly,
prior work on hedge detection (e.g., Greene and
Resnik (2009), Li et al (2010)) relates to our
findings.
6 Conclusion
We investigated syntactic stylometry for decep-
tion detection, adding a somewhat unconven-
tional angle to previous studies. Experimental
results consistently find statistical evidence of
deep syntactic patterns that are helpful in dis-
criminating deceptive writing.
4It is not possible for a human judge to tell with full
confidence whether a given review is a fake or not.
174
References
S. Argamon-Engelson, M. Koppel, and G. Avneri.
1998. Style-based text categorization: What
newspaper am i reading. In Proc. of the AAAI
Workshop on Text Categorization, pages 1?4.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
S. Feng, L. Xing, Gogar A., and Y. Choi. 2012.
Distributional footprints of deceptive product re-
views. In Proceedings of the 2012 International
AAAI Conference on WebBlogs and Social Media,
June.
S. Greene and P. Resnik. 2009. More than
words: Syntactic packaging and implicit senti-
ment. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 503?511. Asso-
ciation for Computational Linguistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Wood-
worth. 2007. On lying and being lied to: A lin-
guistic analysis of deception in computer-mediated
communication. Discourse Processes, 45(1):1?23.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the international
conference on Web search and web data mining,
WSDM ?08, pages 219?230, New York, NY, USA.
ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010.
Finding unusual review patterns using unexpected
rules. In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 1549?1552.
X. Li, J. Shen, X. Gao, and X. Wang. 2010. Ex-
ploiting rich features for detecting hedges and
their scope. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning?Shared Task, pages 78?83. Association
for Computational Linguistics.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing
Liu, and Hady Wirawan Lauw. 2010. Detecting
product review spammers using rating behaviors.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge manage-
ment, CIKM ?10, pages 939?948, New York, NY,
USA. ACM.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages
309?312. Association for Computational Linguis-
tics.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie S.
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th Interna-
tional Conference on World Wide Web (Compan-
ion Volume), pages 93?94.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 309?319, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development
and psychometric properties of liwc2007. Austin,
TX, LIWC. Net.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007.
Cues to deception and ability to detect lies as a
function of police interview styles. Law and hu-
man behavior, 31(5):499?518.
Ying Zhao and Justin Zobel. 2007. Searching with
style: authorship attribution in classic literature.
In Proceedings of the thirtieth Australasian confer-
ence on Computer science - Volume 62, ACSC ?07,
pages 59?68, Darlinghurst, Australia, Australia.
Australian Computer Society, Inc.
175
