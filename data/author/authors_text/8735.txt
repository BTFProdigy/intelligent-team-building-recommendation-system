Evaluation of a Practical Interlingua 
for Task-Oriented Dialogue 
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi, 
Dorcas Wallace, Taro Watanabe, Monika Woszczyna 
Language Technologies Inst i tute,  Carnegie Mellon Univers i ty  and 
IRST  ITC,  Trento, I taly 
Internet:  l s l?cs ,  cmu. edu 
Abstract 
IF (Interchange Format), the interlingua used by 
the C-STAR consortium, is a speech-act based in- 
terlingua for task-oriented ialogue. IF was de- 
signed as a practical interlingua that could strike 
a balance between expressivity and simplicity. If 
it is too simple, components of meaning will be 
lost and coverage of unseen data will be low. On 
the other hand, if it is too complex, it cannot be 
used with a high degree of consistency by collab- 
orators on different continents. In this paper, we 
suggest methods for evaluating the coverage of IF 
and the consistency with which it was used in the 
C-STAR consortium. 
Introduction 
IF (Interchange Format) is an interlingua used by 
the C-STAR consortium 1 for task-oriented ia- 
logues. Because it is used in five different coun- 
tries for six different languages, it had to achieve 
a careful balance between being expressive hough 
and being simple enough to be used consistently. 
If it was not expressive nough, components of 
meaning would be lost and coverage of unseen data 
would be low. On the other hand, if was not sim- 
ple enough, different system developers would use 
it inconsistently and the wrong meanings would be 
translated. IF is described in our previous papers 
(\[PT98, LGLW98, LLW+\]). 
For this paper, we have proposed methods for 
evaluating the coverage of IF and the degree to 
which it can be used consistently across C-STAR 
sites. Coverage was measured by having human IF 
specialists annotate unseen data. Consistency was 
measured by two means. The first was inter-coder 
agreement among IF specialists at Carnegie Mel- 
lonUniversity and ITC-irst (Centre per la ricerca 
lhttp://www.c-star.org 
18 
scientifica e tecnologica). The second, less direct 
method, was a cross-site nd-to-end evaluation of 
English-to-Italian translation where the English- 
to-IF analysis grammars were written at CMU and 
IF-to-Italian generation was developed at IRST. If 
the English and Italian grammar writers did not 
agree on the meaning of the IF, wrong transla- 
tions will be produced. In this way, the cross-site 
evaluation can be an indirect indicator of whether 
the CMU and IRST IF specialists agreed on the 
meaning of IF representations. For comparison, 
we also present within-site nd-to-end evaluations 
of English-to-German, English-to-Japanese, and 
English-to-IF-to-English, where all of the analysis 
and generation grammars were written at CMU. 
The  In terchange Format  
Because we are working with task-oriented dia- 
logues, adequate rendering of the speech act in the 
target language often overshadows the need for lit- 
eral translation of the words. IF is therefore based 
on domain actions (DAs), which consist of on 
speech acts plus domain-specific concepts. An ex- 
ample of a DA is give-information+price+room 
(giving information about the price of a room). 
DAs are composed from 45 general speech acts 
(e.g., acknowledge, give- information, accept) 
and about 96 domain-specific oncepts (e.g, 
pr ice,  temporal, room, f l ight ,  ava i lab i l i ty ) .  
In addition to the DA, IF representations can con- 
tain arguments such as room-type, dest inat ion,  
and price. There are about 119 argument types. 
In the following example, the DA consists 
of a speaker tag (a: for agent), the speech- 
act give- information,  and two main concepts, 
+price and +room. The DA is followed by a list 
of arguments: room-type= and price=. The ar- 
guments have values that represent-information 
for the type of room double and the cost repre- 
Percent 
Cumulatlve Percent Count 
Coverage 
15.7 15,7 652 
19.8 4.1 172 
28.3 3.4 143 
26.0 2.7 113 
28.0 2.0 85 
30.1 2.0 85 
31,9 1.9 78 
33.7 1.8 75 
35.5 1.8 73 
37.2 1.7 70 
38.8 1.6 66 
40.3 l .S 64 
41.7 1,4 60 
43.2 1.4 60 
44.5 1.3 56 
45.8 1.3 52 
46.9 1.2 48 
48.0 1.1 46 
49.1 1.1 44 
50.1 1.0 42 
NA* ;:; 244 
DA 
acknowledge 
aff i rm 
thank 
introduce-self 
give-lnformation+prtce 
greeting 
give-lnfor marion+tern poral 
give-lnformatlon+numeral 
give-in formation+ pr ice+room 
request-in for matio n+ payment 
give-information + payment 
g ive- inform+features+room 
give-in form -t- availabil ity + room 
accept 
give-information+personal-data 
req-act +reserv+ feat ures+room 
req- verif-give-inforra +numera l  
offer+help 
apologize 
request-inform+personal-data 
no-tag 
Figure 1: Coverage of Top20 DAs and No-tag in 
development data 
sented with the complex argument price= which 
has its own arguments quantity=, currency= and 
per-unit=. This IF representation is neutral be- 
tween sentences that have different verbs, sub- 
jects, and objects uch as A double room costs 150 
dollars a night, The price of  a double room is 150 
dollars a night, and A double room is 150 dollars 
a night. ~ 
AGENT: ''a double room costs $150 a night.'' 
a:give-information+price+room 
( room-type=doub le ,  
price=(quantity=lSO, 
currency=dollar, 
per-unit=night) 
Coverage and D is t r ibut ion  of  
Dia logue  Acts  
In this section, we address the coverage of IF for 
task-oriented dialogues about ravel planning. We 
want to know whether a very simple interlingua 
like IF can have good coverage. We are using a 
rather subjective measure of coverage: IF experts 
hand-tagged unseen data with IF representations 
and counted the percentage ofutterances towhich 
no IF could be assigned. (When they tagged the 
unseen data, they were not told that the IF was 
being tested for coverage. The tagging was done 
for system development purposes.) Our end-to- 
end evaluation described in the following sections 
can be taken as a less subjective measure of cov- 
2When we add anaphora resolution, we will need 
to know whether a verb (cost) or a noun (price) was 
used. This will be an issue our new project, NESPOLEI 
(http://nespole. itc. it/). 
Percent 
Cumulative Percent Count Speech Act 
Coverage 
30.1 80.1 1250 glve-lnformation 
45,8 15.7 655 acknowledge 
57,7 11.9 498 request- lnformation 
62,7 5,0 209 request-verif ication-give-inform 
87.6 4.9 203 request-actlon 
71.7 4.1 172 affirm 
75,1 3.4 143 thank 
77,9 2.7 113 introduce-self 
80.2 2.4 98 offer 
82,4 2.1 89 accept 
84.4 2.0 85 greeting 
85.7 1.3 55 suggest 
66.8 I . I  44 apologize 
87.8 1.0 41 closing 
88.5 0.8 32 negate.give-information 
89.2 0.6 27 delay-action 
89,8 0.6 25 introduce-topic 
90,2 0.5 19 please-wait 
90.6 0.4 15 reject 
91.0 0.4 15 request-suggestlon 
Figure 2: 
data 
Coverage of speech-acts in development 
erage. However, the score of an end-to-end evalu- 
ation encompasses grammar coverage problems as 
well as IF coverage problems. 
The development portion of the coverage x- 
periment proceeded as follows. Over a period of 
two years, a database of travel planning dialogues 
was collected by C-STAR partners in the U.S., 
Italy, and Korea. The dialogues were role-playing 
dialogues between a person pretending to be a 
traveller and a person pretending to be a travel 
agent. For the English and Italian dialogues, the 
traveller and agent were talking face-to-face in the 
same language - -  both speaking English or both 
speaking Italian. The Korean dialogues were also 
role playing dialogues, but one participant was 
speaking Korean and the other was speaking En- 
glish. From these dialogues, only the Korean ut- 
terances are included in the database. Each utter- 
ance in the database is annotated with an English 
translation and an IF representation. Table 1 sum- 
marizes the amount of data in each language. The 
English, Italian, and Korean data was used for IF 
development. 
The development database contains over 4000 
dialogue act units, which are covered by a total of 
about 542 distinct DAs (346 agent DAs and 278 
client DAs). Figures 1 and 2 show the cumulative 
coverage of the top twenty DA's and speech acts 
in the development data. Figure 1 also shows the 
percentage ofno-tag utterances (the ones we de- 
cided not to cover) in the development data. The 
first column shows the percent of the development 
data that is covered cumulatively by the DA's or 
speech acts from the top of the table to the cur- 
rent line. For example, acknowledg e and aff irm 
together account for 19.8 percent of the data. The 
19 
Language(s) Type  of Dialogue Number  of DA Units  
D'evelopment Data: 
English 
Italian 
Korean-English 
Test Data: 
Japanese-English 
monolingual 
monolingual 
biiingual (only 'Korean 
utterances are included) 
bilingual (Japanese and 
English utterances are 
included) 
Table 1: The IF Database 
2698 
1142 
6069 
Percent 
' Cumulat ive Percent Count DA 
Cover~,--= - " 4.6 263 no-tag 
15.6 15.6 ? - 885 acknowledge 
20.2 4,6 260 thank 
23.7 3.5 200 introduce-self 
27.0 3.4 191 affirm 
29.7 2.7 153 apologize 
32.3 2.6 147 greeting 
34.6 2.3 128 closing 
36.3 1.7 98 give- information+personal-data 
38.0 1.7 95 glve-inform ation +t  em poraI 
39.5 1.6 89 give-in formation +price 
41.1 1.5 88 please-wait 
42.5 1.4 82 give-inform+telephone-number 
43.8 1.3 75 g ive- informat ion+features+room 
45.0 I . I  65 request- inform+personal-data 
46.0 1.0 59 give-in for m ?temp oral-.{- arrival 
47.0 1.0 55 accept 
48.0 l.O 55 give-infor m +avai labi l i ty + room 
48.9 1.0 55 give-information+price-broom 
49.8 0.9 50 verify 
50.7 0.9 49 request-in form +tempora l+arr iva l  
Figure 3: Coverage of Top 20 DAs and No-tag in 
test data 
Percent 
Cumulat ive 
Coverage 
25.6 
Percent Count DA 
25.6 1454 give-information 
41.7 16.1 916 acknowledge 
53.6 11.9 677 request- information 
58.2 4.6 260 thank 
62,0 3.7 213 request-verification-give-inform 
65.5 3.5 200 introduce-self 
68.8 3.4 191 a f f i rm 
72.0 3.2 181 request -act ion  
74.8 2.8 159 accept 
77.5 2.7 153 apologize 
80.1 2.6 147 greet ing 
82.4 2.3 130 closing 
84.4 2.1 117 suggest 
86.3 1.8 104 verlfy-give-information 
87.9 1.7 94 offer 
89.5 1.5 88 please-wait 
90.6 I . I  65 negate-glve-lnformation 
91.5 0.9 50 verify 
92.0 0.5 30 negate 
92.5 0.5 . 26 request-aff irmatlon 
Figure 4: Coverage of Top 20 SAs in test data 
second column shows the percent of the develop- 
ment data covered by each DA or speech act. The 
third column shows the number of times each DA 
or speech act occurs in the development data. 
The evaluation portion of the coverage x- 
periment was carried out on 124 dialogues (6069 
dialogue act units) that were collected at ATR, 
Japan. One participant in each dialogue was 
speaking Japanese and the other was speaking En- 
glish. Both Japanese and English utterances are 
included in the data. The 124 Japanese-English 
dialogues were not examined closely by system de- 
velopers during IF development. After the IF de- 
sign was finalized and frozen in Summer 1999, the 
Japanese-English data was tagged with IFs. No 
further IF development took place at this point 
except hat values for arguments were added. For 
example, Miyako could be added as a hotel name, 
but no new speech acts, concepts, or argument 
types could be added. Sentences were tagged as 
no-tag if the IF did not cover them. 
Figures 3 and 4 show the cumulative cover- 
age of the top twenty DAs and speech acts in the 
Japanese-English data, including the percent of 
no-tag sentences. 
Notice that the percentage of no-tag was 
lower in our test data than in our development 
data. This is because the role playing instructions 
for the test data were more restrictive than the 
role playing instructions for the development data. 
Figures 1 and 3 show that slightly more of the test 
data is covered by slightly fewer DAs. 
Cross-Site Reliability of IF 
Representations 
In this section we attempt o measure how reliably 
IF is used by researchers at different sites. Recall 
that one of the design criteria of IF was consis- 
tency of use by researchers who are separated by 
oceans. This criterion limits the complexity of IF. 
Two measures of consistency are used - inter-coder 
agreement and a cross-site nd-to-end evaluation. 
Inter -Coder  Agreement:  Inter-coder agree- 
ment is a direct measure of consistency among 
20 
Percent Agreement 
Speech-act 82.14 
Dialog-act 65.48 
Concept lists 88.00 
Argument lists I 85.79 
Table 2: Inter-coder Agreement between CMU 
and IRST 
C-STAR partners. We used 84 DA units from 
the Japanese-English data described above. The 
84 DA units consisted of some coherent dialogue 
fragments and and some isolated sentences. The 
data was coded at CMU and at IRST. We counted 
agreement on ~he components ofthe IF separately. 
Table 2 shows agreement on speech acts, dialogue 
acts (speech act plus concepts), concepts, and ar- 
guments. The results are reported in Table 2 in 
terms of percent agreement. Further work might 
include some other calculation of agreement such 
as Kappa or precision and recall of the coders 
against each other. Figure 5 shows a fragment of 
a dialogue coded by CMU and IRST. The coders 
disagreed on the IF middle sentence, I'd like a twin 
room please. One coded it as an acceptance of a 
twin room, the other coded it as a preference for 
a twin room. 
Cross-Site Evaluation: As an approximate and 
indirect measure of consistency, we have compared 
intra-site end-to-end evaluation with cross-site 
end-to-end evaluation. An end-to-end evaluation 
includes an analyzer, which maps the source lan- 
guage input into IF and a generator, which maps 
IF into target language sentences. The intra-site 
evaluation was carried out on English-German, 
English-Japanese, and English-IF-English trans- 
lation. The English analyzer and the German, 
Japanese, and English generators were all writ- 
ten at CMU by IF experts who worked closely 
with each other. The cross-site valuation was car- 
ried out on English-Italian translation, involving 
an English analyzer written at CMU and an Ital- 
ian generator written at IRST. The IF experts at 
CMU and IRST were in occasional contact with 
each other by email, and met in person two or 
three times between 1997 and 1999. 
A number of factors contribute to the success 
of an inter-site valuation, just one of which is that 
the sites used IF consistently with each other. An- 
other factor is that the two sites used similar de- 
velopment data and have approximately the same 
coverage. If the inter-site valuation results are 
about as good as the intra-site results, we can con- 
clude that all factors are handled acceptably, in- 
cluding consistency of IF usage. If the inter-site 
results are worse than the intra-site results, con- 
sistency of IF use or some other factor may be 
to blame. Before conducting this evaluation, we 
already knew that there was some degree of cross- 
site consistency in IF usage because we conducted 
successful inter-continental demos with speech 
translation and video conferencing in Summer 
1999. (The demos and some of the press coverage 
are reported on the C-STAR web site.) The de- 
mos included ialogues in English-Italian, English- 
German, English-Japanese, English-Korean, and 
English-French. At a later date, an Italian-Korean 
demo was produced with no additional work, thus 
illustrating the well-cited advantage of an inter- 
lingual approach in a multi-lingual situation. The 
end-to-end evaluation reported here goes beyond 
the demo situation to include data that was un- 
seen by system developers. 
Evaluation Data: The Summer 1999 intra-site 
evaluation was conducted on about 130 utterances 
from a CMU user study. The traveller was played 
by a second time user - -  someone who had partici- 
pated in one previous user study, but had no other 
experience with our MT system. The travel agent 
was played by a system developer. Both people 
were speaking English, but they were in different 
rooms, and their utterances were paraphrased us- 
ing IF. The end-to-end procedure was that (1) an 
English utterance was spoken and decoded by the 
JANUS speech recognizer, (2) the output of the rec- 
ognizer was parsed into an IF representation, and 
(3) a different English utterance (supposedly with 
the same meaning) was generated from the IF rep- 
resentation. The speakers had no other means of 
communication with each other. 
In order to evaluate English-German and 
English-Japanese translation, the IFs of the 130 
test sentences were fed into German and Japanese 
generation components atCMU. The data used in 
the evaluation was unseen by system developers 
at the time of the evaluation. For English-Italian 
translation, the IF representations produced by 
the English analysis component were sent to IRST 
to be generated in Italian. 
Evaluation Scoring: In order to score the eval- 
uation, input and output sentences were compared 
by bilingual people, or monolingual people in the 
case of English-IF-English evaluation. A score of 
ok is assigned if the target language utterance is
comprehensible and no components ofmeaning are 
deleted, added, or" changed by the translation. A
21 
We have singles, and t,ins and also Japanese rooms available on the eleventh. 
CMU a:give-information+availability+room 
(room-type=(single ~ twin ~ japanese_style), time=mdll) 
IRST a:give-in2ormation+availability+room 
(room-type=(single ~ twin & japanese_style), time=mdll) 
I'd like a twin room, please. 
CMU c:accept+features+room (room-typeffitwin) 
IBST c:give-information+preference+features+room (room-type=twin) 
A twin room is fourteen thousand yen. 
CMU a:give-information+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
IRST a:give-in.formation+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
Figure 5: Examples of IF coding from CMU and IRST 
.o  
Method 
1 Recosnition only 
2 Transcription 
3 Recosnition 
4 Transcription 
5 Recognition 
6 Transcription 
7 Recognition 
8 Transcription 
9 Recognition 
10 Transcription 
11 Recognition 
I OutPut Language II OK+Perfect Perfect Grader I No. of Graders 
En$1ish 
English. 
En$1ish 
Japanese 
Japanese 
German 
German 
German 
German 
Italian 
Italian 
78 % 62 % CMU 3 
74 % 54 % CMU 3 
59 ~ 42 % CMU 3 
777 % 59 % CMU 2 
62 % 4,5 % CMU 2 
70 %- ..... s9 % CMU " 
58 % 34 % CMU 2 
67 ~ 43 % IRST 2 
59 % 36 % IRST 2 
73 % 51% IRST .... . .  6 
61% 42 % IRST 6 
Figure 6: Translation Grades for English to English, Japanese, German, and Italian 
score of perfect is assigned if, in addition to the 
previous criteria, the translation is fluent in the 
target language. A score of bad is assigned if the 
target language sentence is incomprehensible or 
some element of meaning has been added, deleted, 
or changed. The evaluation procedure isdescribed 
in detail in \[GLL+96\]. In Figure 6, acceptable is
the sum of per fec t  and ok scores, s 
Figure 6 shows the results of the intra-site 
and inter-site evaluations. The first row grades 
the speech recognition output against a human- 
produced transcript of what was said. This gives 
us a ceiling for how well we could do if trans- 
lation were perfect, given speech recognition er- 
rors. Rows 2 through 7 show the results of the 
intra-site evaluation. All analyzers and genera- 
tors were written at CMU, and the results were 
graded by CMU researchers. (The German re- 
sults are a lower than the English and Japanese 
results because a shorter time was spent on gram- 
mar development.) Rows 8 and 9 report on CMU's 
intra~site valuation of English-German transla~ 
Sin another paper (\[LBL+00\]), we describe a task- 
based evaluation which focuses on success of commu- 
nicative goals and how long it takes to achieve them. 
tion (the same system as in Rows 6 and 7), but 
the results were graded by researchers at IRST. 
Comparing Rows 6 and 7 with Rows 8 and 9, we 
can check that CMU and IRST graders were us- 
ing roughly the same grading criteria: a difference 
of up to ten percent among graders is normal in 
our experience. Rows 10 and 11 show the results 
of the inter-site English-Italian evaluation. The 
CMU English analyzer produced IF representa- 
tions which were sent to IRST and were fed into 
IRST's Italian generator. The results were graded 
by IRST researchers. 
Conclusions drawn from the inter-site valuation: 
Since the inter-site evaluation results are compa- 
rable to the intra-site results, we conclude that re- 
searchers at IRST and CMU are using IF at least 
as consistently as researchers within CMU. 
Future Plans 
In the next phase of C-STAR, we will cover de- 
scriptive sentences (e.g., The castle was built in 
the thirteenth century and someone was impris- 
oned in the tower) as well as task-oriented sen- 
tences. Descriptive sentences will be represented 
22 
in a more traditional frame-based interlingua fo- 
cusing on lexical meaning and grammatical fea- 
tures of the sentences. We are working on disam- 
biguating literal from task-oriented meanings in 
context. For example That's great could be an ac- 
ceptance (like I'll take it) (task oriented) or could 
just express appreciation. Sentences may also con- 
tain a combination of task oriented (e.g., Can you 
tell me) and descriptive (how long the castle has 
been standing) components. 
\[GLL+96\] 
\[LBL+O0\] 
\[LGLW98\] 
Re ferences  
Donna Gates, A. Lavie, L. Levin, 
A. Waibel, M. Gavald~, L. Mayfield, 
M:-Woszczyna, and P. Zhan. End-to- 
End Evaluation in JANUS: A Speech- 
to-Speech Translation System. In Pro- 
ceedings of ECAI-96, Budapest, Hun- 
gary, 1996. 
Lori Levin, Boris Bartlog, Ari- 
adna Font Llitjos, Donna Gates, Alon 
Lavie, Dorcas Wallace, Taro Watan- 
abe, and Monika Woszczyna. Lessons 
Learned from a Task-Based Evaluation 
of Speech-to-Speech MT. In Proceed- 
ings of LREC 2000, Athens, Greece, 
June to appear, 2000. 
Lori Levin, D. Gates, A. Lavie, and 
A. Waibel. An Interlingua Based on 
Domain Actions for Machine Transla- 
tion of Task-Oriented Dialogues. In 
Proceedings of the International Con- 
ference on Spoken Language Process- 
ing (ICSLP'98), Sydney, Australia, 
1998. 
\[LLW +\] 
\[PT98\] 
Lori Levin, A. Lavie, M. Woszczyna, 
D. Gates, M. Gavald~, D. Koll, and 
A. Waibel. The Janus-III Translation 
System. Machine Translation. To ap- 
pear. 
Fabio Pianesi and Lucia Tovena. Us- 
ing the Interchange Format for Encod- 
ing Spoken Dialogue. In Proceedings of
SIG-IL Workshop, 1998. 
23 
 	 
  
	
 		 	 	    	  	
 	
     
 	
  	  		
 
 	
 		
	
Example-based Machine Translation Based on Syntactic Transfer
with Statistical Models
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Eiichiro Sumita
ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, ?Keihanna Science City?
Kyoto, 619-0288, Japan
{kenji.imamura,hideo.okuma,taro.watanabe,eiichiro.sumita}@atr.jp
Abstract
This paper presents example-based machine
translation (MT) based on syntactic trans-
fer, which selects the best translation by us-
ing models of statistical machine translation.
Example-based MT sometimes generates in-
valid translations because it selects similar ex-
amples to the input sentence based only on
source language similarity. The method pro-
posed in this paper selects the best transla-
tion by using a language model and a trans-
lation model in the same manner as statisti-
cal MT, and it can improve MT quality over
that of ?pure? example-based MT. A feature
of this method is that the statistical models
are applied after word re-ordering is achieved
by syntactic transfer. This implies that MT
quality is maintained even when we only ap-
ply a lexicon model as the translation model.
In addition, translation speed is improved by
bottom-up generation, which utilizes the tree
structure that is output from the syntactic
transfer.
1 Introduction
In response to the ongoing expansion of bilingual
corpora, many machine translation (MT) meth-
ods have been proposed that automatically ac-
quire their knowledge or models from the cor-
pora. Recently, two major approaches to such ma-
chine translation have emerged: example-based
machine translation and statistical machine trans-
lation.
Example-based MT (Nagao, 1984) regards a
bilingual corpus as a database and retrieves exam-
ples that are similar to an input sentence. Then,
a translation is generated by modifying the tar-
get part of the examples while referring to trans-
lation dictionaries. Most example-based MT sys-
tems employ phrases or sentences as the unit for
examples, so they can translate while consider-
ing case relations or idiomatic expressions. How-
ever, when some examples conflict during re-
E =
J =
A =
NULL0 show1 me2 the3 one4 in5 the6 window7
uindo1 no2 shinamono3 o4 mise5 telidasai6
7 0 4 0 1 1( )
Figure 1: Example of Word Alignment between
English and Japanese (Watanabe and Sumita,
2003)
trieval, example-based MT selects the best exam-
ple scored by the similarity between the input and
the source part of the example. This implies that
example-based MT does not check whether the
translation of the given input sentence is correct
or not.
On the other hand, statistical MT employing
IBM models (Brown et al, 1993) translates an in-
put sentence by the combination of word transfer
and word re-ordering. Therefore, when it is ap-
plied to a language pair in which the word order is
quite different (e.g., English and Japanese, Figure
1), it becomes difficult to find a globally optimal
solution due to the enormous search space (Watan-
abe and Sumita, 2003).
Statistical MT could generate high-quality
translations if it succeeded in finding a globally
optimal solution. Therefore, the models employed
by statistical MT are superior indicators of the
quality of machine translation. Using this feature,
Akiba et al (2002) achieved selection of the best
translation among those output by multiple MT
engines.
This paper presents an example-based MT
method based on syntactic transfer, which selects
the best translation by using models of statisti-
cal MT. This method is roughly structured using
two modules (Figure 2). One is an example-based
syntactic transfer module. This module constructs
Input Sentence Output Sentence
Example-based
Syntactic Transfer
Thesaurus
Preprocessing Postprocessing
Statistical
Generation
Translation
Dictionary
Transfer
Rules
Translation
Model
Language
Model
Figure 2: Structure of Proposed Method
tree structures of the target language by parsing
and mapping the input sentence while referring to
transfer rules. The other is a statistical generation
module, which selects the best word sequence of
the target language in the same manner as statis-
tical MT. Therefore, this method is sequentially
combined example-based and statistical MT.
The proposed method has the following advan-
tages.
? From the viewpoint of example-based MT, the
quality of machine translation improves by se-
lecting the best translation not only from the
similarity judgment between the input sen-
tence and the source part of the examples but
also from the scoring of translation correctness
represented by the word transfer and word con-
nection.
? From the viewpoint of statistical MT, an ap-
propriate translation can be obtained even if
we use simple models because a global search
is applied after word re-ordering by syntac-
tic transfer. In addition, the search space
becomes smaller because the example-based
transfer generates syntactically correct candi-
dates for the most appropriate translation.
The rest of this paper is organized as follows:
Section 2 describes the example-based syntactic
transfer, Section 3 describes the statistical gen-
eration, Section 4 evaluates an experimental sys-
tem that uses this method, and Section 5 compares
other hybrid methods of example-based and statis-
tical MT.
2 Example-based Syntactic Transfer
The example-based syntactic transfer used in this
paper is a revised version of the Hierarchical
Phrase Alignment-based Translator (HPAT, re-
fer to (Imamura, 2002)). This section gives an
overview with an example of Japanese-to-English
machine translation.
2.1 Transfer Rules
Transfer rules are automatically acquired from
bilingual corpora by using hierarchical phrase
alignment (HPA; (Imamura, 2001)). HPA parses
bilingual sentences and acquires corresponding
syntactic nodes of the source and target sentences.
The transfer rules are created from their node cor-
respondences. Figure 3 shows an example of the
transfer rules. Variables, such as X and Y in Fig-
ure 3, denote non-terminal symbols that corre-
spond between source and target grammar. The
set of transfer rules is regarded as synchronized
context-free grammar.
The difference between this approach and con-
ventional synchronized context-free grammar is
that source examples are added to each transfer
rule. The source example is an instance (i.e., a
headword) of the variables that appeared in the
training corpora. For example, the source exam-
ple of Rule 1 in Figure 3 is obtained from a phrase
pair of the Japanese verb phrase ?furaito (flight)
wo yoyaku-suru (reserve)? and the English verb
phrase ?make a reservation for the flight.?
2.2 Syntactic Transfer Process
When an input sentence is given, the target tree
structure is constructed in the following three
steps.
1. The input sentence is parsed by using the
source grammar of the transfer rules.
2. The nodes in the source tree are mapped to the
target nodes by using transfer rules.
3. If non-terminal symbols remain in the leaves of
the target tree, candidates of translated words
are inserted by referring to the translation dic-
tionary.
An example of the syntactic transfer process is
shown in Figure 4 for the input sentence ?basu
wa 11 ji ni de masu (The bus will leave at 11
o?clock).? There are two points worthy of notice in
this figure. First, nodes in which the word order is
inverted are generated after transfer (cf. VP node
represented by a bold frame). Word re-ordering
is achieved by syntactic transfer. Second, words
No. Source Grammar Target Grammar Source Example
1 VP ? X
PP
Y
VP
? VP ? Y
VP
X
PP
((furaito (flight), yoyaku-suru (reserve)) ..)
2 VP ? Y
VP
X
ADVP
((soko (there), yuku (go)) ..)
3 VP ? Y
BEVP
X
NP
((hashi (bridge), aru (be)) ..)
4 S ? X
NP
wa Y
VP
masu ? S ? X
NP
Y
VP
((kare (he), enso-suru (play)) ..)
5 S ? X
NP
will Y
VP
((basu (bus), tomaru (stop)) ..)
Figure 3: Example of Transfer Rules
bus
bath
go
leave
start
11
NP -> a X3
NP -> the X3
NP -> X3
VP -> Y2 X2
VP -> X5 PP -> at X4
ADVP -> X4
NP -> X6 o?clock
NP -> X6
basu
(bus)
11
deru
(leave)
NP
X3
NP
X6 ji
                 (o?clock)
PP
X4 ni
VP
X5
VP
X2 Y2
S
X1 wa Y1 masu
X1
Y1
Y2 X2
Japanese English
S -> X1 will Y1
Figure 4: Example of Syntactic Transfer Process
(Bold frames are syntactic nodes mentioned in text)
that do not correspond between the source and tar-
get sentences (e.g., the determiner ?a? or ?the?)
are automatically inserted or eliminated by the tar-
get grammar (cf. NP node represented by a bold
frame). Namely, transfer rules work in a manner
similar to the functions of distortion, fertility, and
NULL in IBM models.
2.3 Usage of Source Examples
Example-based transfer utilizes the source exam-
ples for disambiguation of mapping and parsing.
Specifically, the semantic distance (Sumita and
Iida, 1991) is calculated between the source exam-
ples and the headwords of the input sentence, and
the transfer rules that contain the nearest exam-
ple are used to construct the target tree structure.
The semantic distance between words is defined
as the distance from the leaf node to the most spe-
cific common abstraction (MSCA) in a thesaurus
(Ohno and Hamanishi, 1984).
For example, if the input phrase ?ie (home) ni
kaeru (return)? is given, Rules 1 to 3 in Figure 3
are used for the syntactic transfer, and three target
nodes are generated without any disambiguation.
However, when we compare the source examples
with the headword of the variables X (ie) and Y
(kaeru), only Rule 2 is used for the transfer be-
cause the semantic distance of the example (soko
(there), yuku (go)) is the nearest. In the current
implementation, all rules that contain examples of
the same distance are used.
Consequently, example-based transfer achieves
translation while considering case relations or id-
iomatic expressions based on the semantic dis-
tance from the source examples.
3 Statistical Generation
3.1 Translation Model and Language Model
Statistical generation searches for the most ap-
propriate sequence of target words from the tar-
get tree output from the example-based syntactic
transfer. The most appropriate sequence is deter-
mined from the product of the translation model
and the language model in the same manner as sta-
tistical MT. In other words, when F and E denote
the channel target and channel source sequence,
respectively, the output word sequence E? that sat-
isfies the following equation is searched for.
E? = argmax
E
P (E|F )
= argmax
E
P (E)P (F |E). (1)
We only utilize the lexicon model as the trans-
lation model in this paper, similar to the models
proposed by Vogel et al (2003). Namely, when f
and e denote the channel target and channel source
word, respectively, the translation probability is
computed by the following equation.
P (F |E) =
?
j
?
i
t(f
j
|e
i
). (2)
The IBM models include other models, such
as fertility, NULL, and distortion models. As we
described in Section 2.2, the quality of machine
translation is maintained using only the lexicon
model because syntactical correctness is already
preserved by example-based transfer.
For the language model, we utilize a standard
word n-gram model.
3.2 Bottom-up Generation
We can construct word graphs by serializing the
target tree structure, which allows us to select the
best word sequence from the graphs. However,
the tree structure already shares nodes transferred
from the same input sub-sequence. The cost of
calculating probabilities is equivalent if we cal-
culate the probabilities while serializing the tree
structure. We call this method bottom-up genera-
tion in this paper.
Figure 5 shows a partial example of bottom-
up generation when the target tree in Figure 4
is given. For each node, word sub-sequences
and their probabilities (language and translation)
are obtained from child nodes. Then, the new
probabilities of the word sequence combination
are calculated, and the n-best sequences are se-
lected. These n-best sequences and their prob-
abilities are reused to calculate the probabilities
of parent nodes. When the translation probabil-
ity is calculated, the source word sub-sequence is
obtained by tracing transfer mapping, and the ap-
plied translation model is restricted to the source
sub-sequence. In other words, the translation
probability is locally calculated between the cor-
responding phrases.
Set Name Item English Japanese
Training # of Sentences 152,170
# of Words 886,708 1,007,484
Test # of Sentences 510
# of Words 2,973 3,340
Table 1: Corpus Size
When the generation reaches the top node, the
language probability is re-calculated with marks
for start-of-sentence and end-of-sentence, and the
n-best list is re-sorted. As a result, the translation
?The bus will leave at 11 o?clock? is obtained from
the tree of Figure 4.
Bottom-up generation calculates the probabili-
ties of shared nodes only once, so it effectively
uses tree information.
4 Evaluation
In order to evaluate the effect when models of sta-
tistical MT are integrated into example-based MT,
we compared various methods that changed the
statistical generation module.
4.1 Experimental Setting
Bilingual Corpus The corpus used in the fol-
lowing experiments is the Basic Travel Expression
Corpus (Takezawa et al, 2002; Kikui et al, 2003).
This is a collection of Japanese sentences and their
English translations based on expressions that are
usually found in phrasebooks for foreign tourists.
We divided it into subsets for training and testing
as shown in Table 1.
Transfer Rules Transfer rules were acquired
from the training set using hierarchical phrase
alignment, and low-frequency rules that appeared
less than twice were removed. The number of
rules was 24,310.
Translation Model and Language Model We
used a lexicon model of IBM Model 4 learned by
GIZA++ (Och and Ney, 2003) and word bigram
and trigram models learned by CMU-Cambridge
Statistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997).
Compared Methods We compared the follow-
ing four methods.
? Baseline (Example-based Transfer only)
The best translation that had the same seman-
tic distance was randomly selected from the
the bus TM: -0.07LM: -1.94
bus TM: -0.07LM: -0.0
XNP n-best
n-best n-best
will
YVP
leave at 11 o?clock TM: -2.72LM: -4.58
start at 11 o?clock TM: -3.62LM: -4.17
leaves at 11 o?clock TM: -2.72LM: -3.11
YVP
leave TM: -1.88LM: -0.0
start TM: -2.78LM: -0.0
leaves TM: -1.88LM: -0.0
XPP
at 11 o?clock TM: -0.84LM: -2.79
at 11 TM: -4.91LM: -2.26
a bus TM: -0.07LM: -2.11
S
bus will start at 11 o?clock
the bus will leave at 11 o?clock
bus will leave at 11 o?clock TM: -7.13LM: -14.30
TM: -8.03
LM: -13.84
TM: -7.13
LM: -13.54
<s> </s>
Figure 5: Example of Bottom-up Generation
(TM and LM denote log probabilities of the translation and language models, respectively)
tree that was output from the example-based
transfer module. The translation words were
selected in advance as those having the highest
frequency in the training corpus. This is the
baseline for translating a sentence when using
only the example-based transfer.
? Bottom-up
The bottom-up generation selects the best
translation from the outputs of the example-
based transfer. We used a 100-best criterion
in this experiment.
? All Search
For all combinations that can be generated
from the outputs of the example-based trans-
fer, we calculated the translation and language
probabilities and selected the best translation.
Namely, a globally optimal solution was se-
lected when the search space was restricted by
the example-based transfer.
? LM Only
In the same way as All Search, the best trans-
lation was searched for, but only the language
model was used for calculating probabilities.
The purpose of this experiment is to measure
the influence of the translation model.
Evaluation Metrics From the test set, 510 sen-
tences were evaluated by the following automatic
and subjective evaluation metrics. The number
of reference translations for automatic evaluation
was 16 per sentence.
BLEU: Automatic evaluation by BLEU score
(Papineni et al, 2002).
NIST: Automatic evaluation by NIST score
(Doddington, 2002).
mWER: The mean rate by calculating the word
error rates between the MT results and all ref-
erence translations, where the lowest rate is se-
lected.
Subjective Evaluation: Subjective evaluation
by an English native speaker into the four ranks
of A: Perfect, B: Fair, C: Acceptable, and D:
Nonsense.
Automatic Evaluation Subjective Evaluation Translation Speed
Method BLEU NIST mWER A A+B A+B+C Mean Worst
(sec./sent.) (sec.)
Baseline 0.410 9.06 0.423 51.6% 64.3% 70.4% 0.180 10.82
Bottom-up 0.491 9.99 0.366 62.2% 72.5% 80.4% 0.211 5.03
All Search 0.498 10.04 0.353 62.9% 73.1% 80.8% 1.23 171.31
LM Only 0.491 9.11 0.385 57.6% 66.9% 72.0% 1.624 220.69
Table 2: MT Quality and Translation Speed vs. Generation Methods
4.2 Results
Table 2 shows the results of the MT quality and
translation speed among each method.
First, comparing the baseline with the statisti-
cal generations (Bottom-up and All Search), the
MT quality of statistical generation improved in
all evaluation metrics. Accordingly, the models of
statistical MT are effective for improving the MT
quality of example-based MT.
Next, comparing Bottom-up with All Search,
the MT quality of bottom-up generation was
slightly low. Bottom-up generation locally applies
the translation model to a partial tree. In other
words, the probability is calculated without word
alignment linked to the outside of the tree. This re-
sult indicates that the results of bottom-up genera-
tion are not equal to the global optimal solution.
Comparing LM Only with the statistical gener-
ations, the MT quality of ranks A+B+C by subjec-
tive evaluation significantly decreased. This is be-
cause the n-gram language model used here does
not consider output length, and shorter translations
are preferred. Although the language model was
effective to some degree, it could not evaluate the
equivalence of the translation and the input sen-
tence. Therefore, we concluded that the transla-
tion model is necessary for improving MT quality.
Finally, focusing on translation speed, the worst
time for Bottom-up generation was dramatically
faster than that for All Search. Bottom-up gen-
eration effectively uses shared nodes of the target
tree, so it can improve translation speed. There-
fore, bottom-up generation is suitable for tasks
that require real-time processing, such as spoken
dialogue translation.
5 Discussion
We incorporated example-based MT in models
of statistical MT. However, some methods to ob-
tain initial solutions of statistical MT by example-
based MT have already been proposed. For
example, Marcu (2001) proposed a method in
which initial translations are constructed by com-
bining bilingual phrases from translation mem-
ory, which is followed by modifying the transla-
tions by greedy decoding (Germann et al, 2001).
Watanabe and Sumita (2003) proposed a decoding
algorithm in which translations that are similar to
the input sentence are retrieved from bilingual cor-
pora and then modified by greedy decoding.
The difference between our method and these
methods involves whether modification is applied.
Our approach simply selects the best translation
from candidates that are output from example-
based MT. Even though example-based MT can
output appropriate translations to some degree,
our method assumes that the candidates contain
a globally optimal solution. This means that
the upper bound of MT quality is limited by the
example-based transfer, so we have to improve
this stage in order to further improve MT quality.
For instance, example-based MT can be improved
by applying an optimization algorithm that uses
an automatic evaluation of MT quality (Imamura
et al, 2003).
6 Conclusions
This paper demonstrated that example-based MT
can be improved by incorporating it in models of
statistical MT. The example-based MT used in this
paper is based on syntactic transfer, so word re-
ordering is achieved in the transfer module. Us-
ing this feature, the best translation was selected
by using only a lexicon model and an n-gram lan-
guage model. In addition, bottom-up generation
achieved faster translation speed by using the tree
structure of the target sentence.
Acknowledgements
The authors would like to thank Kadokawa Pub-
lishers, who permitted us to use the hierarchy of
Ruigo-shin-jiten.
The research reported here is supported in part
by a contract with the Telecommunications Ad-
vancement Organization of Japan entitled, ?A
study of speech dialogue translation technology
based on a large corpus.?
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro
Sumita. 2002. Using language and transla-
tion models to select the best among outputs
from multiple MT systems. In Proceedings of
COLING-2002, pages 8?14.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the CMU-
Cambridge toolkit. In Proceedings of Eu-
roSpeech 97, pages 2707?2710.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram
co-occurrence statistics. In Proceedings of the
HLT Conference, San Diego, California.
Ulrich Germann, Michael Jahr, Kevin Knight,
Daniel Marcu, and Kenji Yamada. 2001. Fast
decoding and optimal decoding for machine
translation. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 228?235.
Kenji Imamura, Eiichiro Sumita, and Yuji Mat-
sumoto. 2003. Feedback cleaning of machine
translation rules using automatic evaluation. In
Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL 2003), pages 447?454.
Kenji Imamura. 2001. Hierarchical phrase align-
ment harmonized with parsing. In Proceed-
ings of the 6th Natural Language Processing
Pacific Rim Symposium (NLPRS 2001), pages
377?384.
Kenji Imamura. 2002. Application of transla-
tion knowledge acquired by hierarchical phrase
alignment for pattern-based MT. In Proceed-
ings of the 9th Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-2002), pages 74?84.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech translation.
In Proceedings of EuroSpeech 2003, pages
381?384.
Daniel Marcu. 2001. Towards a unified approach
to memory- and statistical-based machine trans-
lation. In Proceedings of 39th Annual Meeting
of the Association for Computational Linguis-
tics, pages 386?393.
Makoto Nagao. 1984. A framework of mechani-
cal translation between Japanese and English by
analogy principle. In Artificial and Human In-
telligence, pages 173?180, Amsterdam: North-
Holland.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Susumu Ohno and Masato Hamanishi. 1984.
Ruigo-Shin-Jiten. Kadokawa, Tokyo. in
Japanese.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 311?318.
Eiichiro Sumita and Hitoshi Iida. 1991. Experi-
ments and prospects of example-based machine
translation. In Proceedings of the 29th ACL,
pages 185?192.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki
Sugaya, Hirofumi Yamamoto, and Seiichi Ya-
mamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel con-
versations in the real world. In Proceedings
of the Third International Conference on Lan-
guage Resources and Evaluation (LREC 2002),
pages 147?152.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia
Tribble, Ashish Venugopal, Bing Zhao, and
Alex Waibel. 2003. The CMU statistical ma-
chine translation system. In Proceedings of the
9th Machine Translation Summit (MT Summit
IX), pages 402?409.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical machine
translation. In Proceedings of Machine Trans-
lation Summit IX, pages 410?417.
Reordering Constraints for Phrase-Based
Statistical Machine Translation
Richard Zens1, Hermann Ney1, Taro Watanabe2 and Eiichiro Sumita2
1Lehrstuhl fu?r Informatik VI 2 Spoken Language Translation Research Laboratories
Computer Science Department ATR
RWTH Aachen University, Germany Kyoto, Japan
{zens,ney}@cs.rwth-aachen.de {watanabe,sumita}@slt.atr.co.jp
Abstract
In statistical machine translation, the gen-
eration of a translation hypothesis is com-
putationally expensive. If arbitrary re-
orderings are permitted, the search prob-
lem is NP-hard. On the other hand,
if we restrict the possible reorderings
in an appropriate way, we obtain a
polynomial-time search algorithm. We in-
vestigate different reordering constraints
for phrase-based statistical machine trans-
lation, namely the IBM constraints and
the ITG constraints. We present effi-
cient dynamic programming algorithms
for both constraints. We evaluate the con-
straints with respect to translation quality
on two Japanese?English tasks. We show
that the reordering constraints improve
translation quality compared to an un-
constrained search that permits arbitrary
phrase reorderings. The ITG constraints
preform best on both tasks and yield sta-
tistically significant improvements com-
pared to the unconstrained search.
1 Introduction
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =f1 . . . fj . . . fJ , which is to be translated into
a target language (?English?) sentence eI1 =e1 . . . ei . . . eI . Among all possible target lan-
guage sentences, we will choose the sentence
with the highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources is known as the source-channel ap-
proach to statistical machine translation
(Brown et al, 1990). It allows an independent
modeling of target language model Pr(eI1) and
translation model Pr(fJ1 |eI1). The target lan-guage model describes the well-formedness of
the target language sentence. The translation
model links the source language sentence to
the target language sentence. It can be fur-
ther decomposed into alignment and lexicon
model. The argmax operation denotes the
search problem, i.e. the generation of the out-
put sentence in the target language. We have
to maximize over all possible target language
sentences.
An alternative to the classical source-
channel approach is the direct modeling of the
posterior probability Pr(eI1|fJ1 ). Using a log-linear model (Och and Ney, 2002), we obtain:
Pr(eI1|fJ1 ) = exp
( M?
m=1
?mhm(eI1, fJ1 )
)
? Z(fJ1 )
Here, Z(fJ1 ) denotes the appropriate normal-ization constant. As a decision rule, we obtain:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
This approach is a generalization of the
source-channel approach. It has the advan-
tage that additional models or feature func-
tions can be easily integrated into the over-
all system. The model scaling factors ?M1 aretrained according to the maximum entropy
principle, e.g. using the GIS algorithm. Al-
ternatively, one can train them with respect
to the final translation quality measured by
some error criterion (Och, 2003).
In this paper, we will investigate the re-
ordering problem for phrase-based translation
approaches. As the word order in source and
target language may differ, the search algo-
rithm has to allow certain reorderings. If arbi-
trary reorderings are allowed, the search prob-
lem is NP-hard (Knight, 1999). To obtain an
efficient search algorithm, we can either re-
strict the possible reorderings or we have to
use an approximation algorithm. Note that in
the latter case we cannot guarantee to find an
optimal solution.
The remaining part of this work is struc-
tured as follows: in the next section, we
will review the baseline translation system,
namely the alignment template approach. Af-
terward, we will describe different reordering
constraints. We will begin with the IBM con-
straints for phrase-based translation. Then,
we will describe constraints based on inver-
sion transduction grammars (ITG). In the fol-
lowing, we will call these the ITG constraints.
In Section 4, we will present results for two
Japanese?English translation tasks.
2 Alignment Template Approach
In this section, we give a brief description of
the translation system, namely the alignment
template approach. The key elements of this
translation approach (Och et al, 1999) are the
alignment templates. These are pairs of source
and target language phrases with an alignment
within the phrases. The alignment templates
are build at the level of word classes. This
improves the generalization capability of the
alignment templates.
We use maximum entropy to train the
model scaling factors (Och and Ney, 2002).
As feature functions we use a phrase transla-
tion model as well as a word translation model.
Additionally, we use two language model fea-
ture functions: a word-based trigram model
and a class-based five-gram model. Further-
more, we use two heuristics, namely the word
penalty and the alignment template penalty.
To model the alignment template reorderings,
we use a feature function that penalizes re-
orderings linear in the jump width.
A dynamic programming beam search al-
gorithm is used to generate the translation
hypothesis with maximum probability. This
search algorithm allows for arbitrary reorder-
ings at the level of alignment templates.
Within the alignment templates, the reorder-
ing is learned in training and kept fix during
the search process. There are no constraints
on the reorderings within the alignment tem-
plates.
This is only a brief description of the align-
ment template approach. For further details,
see (Och et al, 1999; Och and Ney, 2002).
3 Reordering Constraints
Although unconstrained reordering looks per-
fect from a theoretical point of view, we find
that in practice constrained reordering shows
J
uncovered position
covered position
uncovered position for extension
1 j
Figure 1: Illustration of the IBM constraints
with k = 3, i.e. up to three positions may be
skipped.
better performance. The possible advantages
of reordering constraints are:
1. The search problem is simplified. As a
result there are fewer search errors.
2. Unconstrained reordering is only helpful
if we are able to estimate the reorder-
ing probabilities reliably, which is unfor-
tunately not the case.
In this section, we will describe two variants
of reordering constraints. The first constraints
are based on the IBM constraints for single-
word based translation models. The second
constraints are based on ITGs. In the follow-
ing, we will use the term ?phrase? to mean ei-
ther a sequence of words or a sequence of word
classes as used in the alignment templates.
3.1 IBM Constraints
In this section, we describe restrictions on the
phrase reordering in spirit of the IBM con-
straints (Berger et al, 1996).
First, we briefly review the IBM constraints
at the word level. The target sentence is pro-
duced word by word. We keep a coverage vec-
tor to mark the already translated (covered)
source positions. The next target word has to
be the translation of one of the first k uncov-
ered, i.e. not translated, source positions. The
IBM constraints are illustrated in Figure 1.
For further details see e.g. (Tillmann and Ney,
2003).
For the phrase-based translation approach,
we use the same idea. The target sentence is
produced phrase by phrase. Now, we allow
skipping of up to k phrases. If we set k = 0,
we obtain a search that is monotone at the
phrase level as a special case.
The search problem can be solved using dy-
namic programming. We define a auxiliary
function Q(j, S, e). Here, the source position
j is the first unprocessed source position; with
unprocessed, we mean this source position is
neither translated nor skipped. We use the
set S = {(jn, ln)|n = 1, ..., N} to keep track
of the skipped source phrases with lengths ln
and starting positions jn. We show the formu-
lae for a bigram language model and use the
target language word e to keep track of the
language model history. The symbol $ is used
to mark the sentence start and the sentence
end. The extension to higher-order n-gram
language models is straightforward. We use
M to denote the maximum phrase length in
the source language. We obtain the following
dynamic programming equations:
Q(1, ?, $) = 1
Q(j, S, e) = max
{
max
e?,e?
{
max
j?M?j?<j
Q(j?, S, e?) ? p(f j?1j? |e?) ? p(e?|e?),
max
(j?,l)?S?
S=S?\{(j?,l)}
Q(j, S?, e?) ? p(f j?+l?1j? |e?) ? p(e?|e?)
}
,
max
j?M?j?<j
S?:S=S??{(j?,j?j?)}?|S?|<k
Q(j?, S?, e)
}
Q(J + 2, ?, $) = maxe Q(J + 1, ?, e) ? p($|e)
In the recursion step, we have distinguished
three cases: in the first case, we translate the
next source phrase. This is the same expan-
sion that is done in monotone search. In the
second case, we translate a previously skipped
phrase and in the third case we skip a source
phrase. For notational convenience, we have
omitted one constraint in the preceding equa-
tions: the final word of the target phrase e? is
the new language model state e (using a bi-
gram language model).
Now, we analyze the complexity of this al-
gorithm. Let E denote the vocabulary size of
the target language and let E? denote the max-
imum number of phrase translation candidates
for a given source phrase. Then, J ?(J ?M)k ?E
is an upper bound for the size of the Q-table.
Once we have fixed a specific element of this
table, the maximization steps can be done in
O(E ? E? ? (M + k ? 1) + (k ? 1)). There-
fore, the complexity of this algorithm is in
O(J ?(J ?M)k ?E ?(E ?E? ?(M+k?1)+(k?1))).
Assuming k < M , this can be simplified to
O((J ?M)k+1 ?E2 ? E?). As already mentioned,
source positions
ta
rg
et
 p
os
it
io
ns
without inversion with inversion
source positions
ta
rg
et
 p
os
it
io
ns
Figure 2: Illustration of monotone and
inverted concatenation of two consecutive
blocks.
setting k = 0 results in a search algorithm that
is monotone at the phrase level.
3.2 ITG Constraints
In this section, we describe the ITG con-
straints (Wu, 1995; Wu, 1997). Here, we inter-
pret the input sentence as a sequence of blocks.
In the beginning, each alignment template is a
block of its own. Then, the reordering process
can be interpreted as follows: we select two
consecutive blocks and merge them to a single
block by choosing between two options: either
keep the target phrases in monotone order or
invert the order. This idea is illustrated in Fig-
ure 2. The dark boxes represent the two blocks
to be merged. Once two blocks are merged,
they are treated as a single block and they can
be only merged further as a whole. It is not
allowed to merge one of the subblocks again.
3.2.1 Dynamic Programming Algorithm
The ITG constraints allow for a polynomial-
time search algorithm. It is based on the fol-
lowing dynamic programming recursion equa-
tions. During the search a table Qjl,jr,eb,etis constructed. Here, Qjl,jr,eb,et denotes theprobability of the best hypothesis translating
the source words from position jl (left) to po-
sition jr (right) which begins with the target
language word eb (bottom) and ends with the
word et (top). This is illustrated in Figure 3.
The initialization is done with the phrase-
based model described in Section 2. We in-
troduce a new parameter pm (m=? monotone),
which denotes the probability of a monotone
combination of two partial hypotheses. Here,
we formulate the recursion equation for a bi-
gram language model, but of course, the same
method can also be applied for a trigram lan-
jl jr
e b
et
Figure 3: Illustration of the Q-table.
guage model.
Qjl,jr,eb,et =
max
jl?k<jr,
e?,e??
{
Q0jl,jr,eb,et ,
Qjl,k,eb,e? ?Qk+1,jr,e??,et ? p(e??|e?) ? pm,
Qk+1,jr,eb,e? ?Qjl,k,e??,et ? p(e??|e?) ? (1? pm)
}
The resulting algorithm is similar to the CYK-
parsing algorithm. It has a worst-case com-
plexity of O(J3 ?E4). Here, J is the length of
the source sentence and E is the vocabulary
size of the target language.
3.2.2 Beam Search Algorithm
For the ITG constraints a dynamic program-
ming search algorithm exists as described in
the previous section. It would be more prac-
tical with respect to language model recom-
bination to have an algorithm that generates
the target sentence word by word or phrase
by phrase. The idea is to start with the beam
search decoder for unconstrained search and
modify it in such a way that it will produce
only reorderings that do not violate the ITG
constraints. Now, we describe one way to ob-
tain such a decoder. It has been pointed out
in (Zens and Ney, 2003) that the ITG con-
straints can be characterized as follows: a re-
ordering violates the ITG constraints if and
only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as
a subsequence. This means, if we select four
columns and the corresponding rows from the
alignment matrix and we obtain one of the two
patterns illustrated in Figure 4, this reordering
cannot be generated with the ITG constraints.
Now, we have to modify the beam search
decoder such that it cannot produce these two
patterns. We implement this in the follow-
ing way. During the search, we have a cover-
age vector cov of the source sentence available
for each partial hypothesis. A coverage vec-
1
2
3
4
a b c d
1
2
3
4
a b c d
Figure 4: Illustration of the two reordering
patterns that violate the ITG constraints.
tor is a binary vector marking the source sen-
tence words that have already been translated
(covered). Additionally, we know the current
source sentence position jc and a candidate
source sentence position jn to be translated
next.
To avoid the patterns in Figure 4, we have
to constrain the placement of the third phrase,
because once we have placed the first three
phrases we also have determined the position
of the fourth phrase as the remaining uncov-
ered position. Thus, we check the following
constraints:
case a) jn < jc (1)
?jn < j < jc : cov[j] ? cov[j + 1]
case b) jc < jn (2)
?jc < j < jn : cov[j] ? cov[j ? 1]
The constraints in Equations 1 and 2 enforce
the following: imagine, we traverse the cover-
age vector cov from the current position jc to
the position to be translated next jn. Then,
it is not allowed to move from an uncovered
position to a covered one.
Now, we sketch the proof that these con-
straints are equivalent to the ITG constraints.
It is easy to see that the constraint in Equa-
tion 1 avoids the pattern on the left-hand side
in Figure 4. To be precise: after placing the
first two phrases at (b,1) and (d,2), it avoids
the placement of the third phrase at (a,3).
Similarly, the constraint in Equation 2 avoid
the pattern on the right-hand side in Fig-
ure 4. Therefore, if we enforce the constraints
in Equation 1 and Equation 2, we cannot vio-
late the ITG constraints.
We still have to show that we can gener-
ate all the reorderings that do not violate the
ITG constraints. Equivalently, we show that
any reordering that violates the constraints in
Equation 1 or Equation 2 will also violate the
ITG constraints. It is rather easy to see that
any reordering that violates the constraint in
Table 1: Statistics of the BTEC corpus.
Japanese English
train Sentences 152 K
Words 1 044 K 893 K
Vocabulary 17 047 12 020
dev sentences 500
words 3 361 2 858
test sentences 510
words 3 498 ?
Table 2: Statistics of the SLDB corpus.
Japanese English
train Sentences 15 K
Words 201 K 190 K
Vocabulary 4 757 3 663
test sentences 330
words 3 940 ?
Equation 1 will generate the pattern on the
left-hand side in Figure 4. The conditions to
violate Equation 1 are the following: the new
candidate position jn is to the left of the cur-
rent position jc, e.g. positions (a) and (d).
Somewhere in between there has to be an cov-
ered position j whose successor position j + 1
is uncovered, e.g. (b) and (c). Therefore, any
reordering that violates Equation 1 generates
the pattern on the left-hand side in Figure 4,
thus it violates the ITG constraints.
4 Results
4.1 Corpus Statistics
To investigate the effect of reordering con-
straints, we have chosen two Japanese?English
tasks, because the word order in Japanese and
English is rather different. The first task is the
Basic Travel Expression Corpus (BTEC) task
(Takezawa et al, 2002). The corpus statistics
are shown in Table 1. This corpus consists of
phrasebook entries.
The second task is the Spoken Language
DataBase (SLDB) task (Morimoto et al,
1994). This task consists of transcription of
spoken dialogs in the domain of hotel reser-
vation. Here, we use domain-specific training
data in addition to the BTEC corpus. The
corpus statistics of this additional corpus are
shown in Table 2. The development corpus is
the same for both tasks.
4.2 Evaluation Criteria
WER (word error rate). The WER is com-
puted as the minimum number of substitution,
insertion and deletion operations that have to
be performed to convert the generated sen-
tence into the reference sentence.
PER (position-independent word er-
ror rate). A shortcoming of the WER is that
it requires a perfect word order. The word or-
der of an acceptable sentence can be different
from that of the target sentence, so that the
WER measure alone could be misleading. The
PER compares the words in the two sentences
ignoring the word order.
BLEU. This score measures the precision
of unigrams, bigrams, trigrams and fourgrams
with respect to a reference translation with a
penalty for too short sentences (Papineni et
al., 2002). The BLEU score measures accu-
racy, i.e. large BLEU scores are better.
NIST. This score is similar to BLEU. It is
a weighted n-gram precision in combination
with a penalty for too short sentences (Dod-
dington, 2002). The NIST score measures ac-
curacy, i.e. large NIST scores are better.
Note that for each source sentence, we have
as many as 16 references available. We com-
pute all the preceding criteria with respect to
multiple references.
4.3 System Comparison
In Table 3 and Table 4, we show the trans-
lation results for the BTEC task. First, we
observe that the overall quality is rather high
on this task. The average length of the used
alignment templates is about five source words
in all systems. The monotone search (mon)
shows already good performance on short sen-
tences with less than 10 words. We conclude
that for short sentences the reordering is cap-
tured within the alignment templates. On the
other hand, the monotone search degrades for
long sentences with at least 10 words resulting
in a WER of 16.6% for these sentences.
We present the results for various nonmono-
tone search variants: the first one is with the
IBM constraints (skip) as described in Sec-
tion 3.1. We allow for skipping one or two
phrases. Our experiments showed that if we
set the maximum number of phrases to be
skipped to three or more the translation re-
sults are equivalent to the search without any
reordering constraints (free). The results for
the ITG constraints as described in Section 3.2
are also presented.
The unconstrained reorderings improve the
total translation quality down to a WER of
11.5%. We see that especially the long sen-
tences benefit from the reorderings resulting in
an improvement from 16.6% to 13.8%. Com-
paring the results for the free reorderings and
Table 3: Translation performance WER[%]
for the BTEC task (510 sentences). Sentence
lengths: short: < 10 words, long: ? 10 words;
times in milliseconds per sentence.
WER[%]
sentence length
reorder short long all time[ms]
mon 11.4 16.6 12.7 73
skip 1 10.8 13.5 11.4 134
2 10.8 13.4 11.4 169
free 10.8 13.8 11.5 194
ITG 10.6 12.2 11.0 164
Table 4: Translation performance for the
BTEC task (510 sentences).
error rates[%] accuracy measures
reorder WER PER BLEU[%] NIST
mon 12.7 10.6 86.8 14.14
skip 1 11.4 10.1 88.0 14.19
2 11.4 10.1 88.1 14.20
free 11.5 10.0 88.0 14.19
ITG 11.0 9.9 88.2 14.25
the ITG reorderings, we see that the ITG
system always outperforms the unconstrained
system. The improvement on the whole test
set is statistically significant at the 95% level.1
In Table 5 and Table 6, we show the re-
sults for the SLDB task. First, we observe
that the overall quality is lower than for the
BTEC task. The SLDB task is a spoken lan-
guage translation task and the training cor-
pus for spoken language is rather small. This
is also reflected in the average length of the
used alignment templates that is about three
source words compared to about five words for
the BTEC task.
The results on this task are similar to the
results on the BTEC task. Again, the ITG
constraints perform best. Here, the improve-
ment compared to the unconstrained search is
statistically significant at the 99% level. Com-
pared to the monotone search, the BLEU score
for the ITG constraints improves from 54.4%
to 57.1%.
5 Related Work
Recently, phrase-based translation approaches
became more and more popular. Marcu and
Wong (2002) present a joint probability model
for phrase-based translation. In (Koehn et
1The statistical significance test were done for the
WER using boostrap resampling.
Table 5: Translation performance WER[%]
for the SLDB task (330 sentences). Sentence
lengths: short: < 10 words, long: ? 10 words;
times in milliseconds per sentence.
WER[%]
sentence length
reorder short long all time[ms]
mon 32.0 52.6 48.1 911
skip 1 31.9 51.1 46.9 3 175
2 32.0 51.4 47.2 4 549
free 32.0 51.4 47.2 4 993
ITG 31.8 50.9 46.7 4 472
Table 6: Translation performance for the
SLDB task (330 sentences).
error rates[%] accuracy measures
reorder WER PER BLEU[%] NIST
mon 48.1 35.5 54.4 9.45
skip 1 46.9 35.0 56.8 9.71
2 47.2 35.1 57.1 9.74
free 47.2 34.9 57.1 9.75
ITG 46.7 34.6 57.1 9.76
al., 2003), various aspects of phrase-based
systems are compared, e.g. the phrase ex-
traction method, the underlying word align-
ment model, or the maximum phrase length.
In (Vogel, 2003), a phrase-based system is
used that allows reordering within a window
of up to three words. Improvements for a
Chinese?English task are reported compared
to a monotone search.
The ITG constraints were introduced in
(Wu, 1995). The applications were, for in-
stance, the segmentation of Chinese character
sequences into Chinese words and the bracket-
ing of the source sentence into sub-sentential
chunks. Investigations on the IBM constraints
(Berger et al, 1996) for single-word based sta-
tistical machine translation can be found e.g.
in (Tillmann and Ney, 2003). A comparison of
the ITG constraints and the IBM constraints
for single-word based models can be found in
(Zens and Ney, 2003). In this work, we investi-
gated these reordering constraints for phrase-
based statistical machine translation.
6 Conclusions
We have presented different reordering con-
straints for phrase-based statistical machine
translation, namely the IBM constraints and
the ITG constraints, as well as efficient dy-
namic programming algorithms. Transla-
tion results were reported for two Japanese?
English translation tasks. Both type of re-
ordering constraints resulted in improvements
compared to a monotone search. Restrict-
ing the reorderings according to the IBM con-
straints resulted already in a translation qual-
ity similar to an unconstrained search. The
translation results with the ITG constraints
even outperformed the unconstrained search
consistently on all error criteria. The improve-
ments have been found statistically significant.
The ITG constraints showed the best per-
formance on both tasks. Therefore we plan to
further improve this method. Currently, the
probability model for the ITG constraints is
very simple. More sophisticated models, such
as phrase dependent inversion probabilities,
might be promising.
Acknowledgments
This work was partially done at the Spoken
Language Translation Research Laboratories
(SLT) at the Advanced Telecommunication
Research Institute International (ATR), Ky-
oto, Japan. This research was supported in
part by the Telecommunications Advancement
Organization of Japan. This work has been
partially funded by the EU project PF-Star,
IST-2001-37599.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D.
Pietra, J. R. Gillett, A. S. Kehler, and R. L.
Mercer. 1996. Language translation apparatus
and method of using context-based translation
models, United States patent, patent number
5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J.
Della Pietra, F. Jelinek, J. D. Lafferty, R. L.
Mercer, and P. S. Roossin. 1990. A statisti-
cal approach to machine translation. Compu-
tational Linguistics, 16(2):79?85, June.
G. Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proc. ARPA Workshop
on Human Language Technology.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of
the Human Language Technology Conf. (HLT-
NAACL), pages 127?133, Edmonton, Canada,
May/June.
D. Marcu and W. Wong. 2002. A phrase-based,
joint probability model for statistical machine
translation. In Proc. Conf. on Empirical Meth-
ods for Natural Language Processing, pages 133?
139, Philadelphia, PA, July.
T. Morimoto, N. Uratani, T. Takezawa, O. Furuse,
Y. Sobashima, H. Iida, A. Nakamura, Y. Sag-
isaka, N. Higuchi, and Y. Yamazaki. 1994. A
speech and language database for speech trans-
lation research. In Proc. of the 3rd Int. Conf. on
Spoken Language Processing (ICSLP?94), pages
1791?1794, Yokohama, Japan, September.
F. J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statisti-
cal machine translation. In Proc. of the 40th
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302,
Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Im-
proved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 20?28,
University of Maryland, College Park, MD,
June.
F. J. Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of
the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?
167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto,
and S. Yamamoto. 2002. Toward a broad-
coverage bilingual corpus for speech translation
of travel conversations in the real world. In
Proc. of the Third Int. Conf. on Language Re-
sources and Evaluation (LREC), pages 147?152,
Las Palmas, Spain, May.
C. Tillmann and H. Ney. 2003. Word reordering
and a dynamic programming beam search algo-
rithm for statistical machine translation. Com-
putational Linguistics, 29(1):97?133, March.
S. Vogel. 2003. SMT decoder dissected: Word re-
ordering. In Proc. of the Int. Conf. on Natural
Language Processing and Knowledge Engineer-
ing (NLP-KE), pages 561?566, Beijing, China,
October.
D. Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation,
bracketing, and alignment of parallel corpora.
In Proc. of the 14th International Joint Conf.
on Artificial Intelligence (IJCAI), pages 1328?
1334, Montreal, August.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
R. Zens and H. Ney. 2003. A comparative study
on reordering constraints in statistical machine
translation. In Proc. of the 41th Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL), pages 144?151, Sapporo, Japan,
July.
A Unified Approach in Speech-to-Speech Translation: Integrating
Features of Speech Recognition and Machine Translation
Ruiqiang Zhang and Genichiro Kikui and Hirofumi Yamamoto
Taro Watanabe and Frank Soong and Wai Kit Lo
ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang, genichiro.kikui}@atr.jp
Abstract
Based upon a statistically trained speech
translation system, in this study, we try
to combine distinctive features derived from
the two modules: speech recognition and
statistical machine translation, in a log-
linear model. The translation hypotheses
are then rescored and translation perfor-
mance is improved. The standard trans-
lation evaluation metrics, including BLEU,
NIST, multiple reference word error rate
and its position independent counterpart,
were optimized to solve the weights of the
features in the log-linear model. The exper-
imental results have shown significant im-
provement over the baseline IBM model 4
in all automatic translation evaluation met-
rics. The largest was for BLEU, by 7.9%
absolute.
1 Introduction
Current translation systems are typically of a
cascaded structure: speech recognition followed
by machine translation. This structure, while
explicit, lacks some joint optimality in per-
formance since the speech recognition module
and translation module are running rather in-
dependently. Moreover, the translation module
of a speech translation system, a natural off-
spring of text-input based translation system,
usually takes a single-best recognition hypoth-
esis transcribed in text and performs standard
text-based translation. Lots of supplementary
information available from speech recognition,
such as N -best recognition recognition hypothe-
ses, likelihoods of acoustic and language models,
is not well utilized in the translation process.
The information can be effective for improving
translation quality if employed properly.
The supplementary information can be ex-
ploited by a tight coupling of speech recognition
and machine translation (Ney, 1999) or keeping
the cascaded structure unchanged but using an
integration model, log-linear model, to rescore
the translation hypotheses. In this study the
last approach was used due to its explicitness.
In this paper we intended to improve speech
translation by exploiting these information.
Moreover, a number of advanced features from
the machine translation module were also added
in the models. All the features from the speech
recognition and machine translation module
were combined by the log-linear models seam-
lessly.
In order to test our results broadly, we used
four automatic translation evaluation metrics:
BLEU, NIST, multiple word error rate and po-
sition independent word error rate, to measure
the translation improvement.
In the following, in section 2 we introduce the
speech translation system. In section 3, we de-
scribe the optimization algorithm used to find
the weight parameters in the log-linear model.
In section 4 we demonstrate the effectiveness
of our technique in speech translation experi-
ments. In the final two sections we discuss the
results and present our conclusions.
2 Feature-based Log-linear Models
in Speech Translation
The speech translation experimental system
used in this study illustrated in Fig. 1 is a typi-
cal, statistics-based one. It consists of two ma-
jor cascaded components: an automatic speech
recognition (ASR) module and a statistical ma-
chine translation (SMT) module. Additionally,
a third module, ?Rescore?, has been added to the
system and it forms a key component in the sys-
tem. Features derived from ASR and SMT are
combined in this module to rescore translation
candidates.
Without loss of its generality, in this paper
we use Japanese-to-English translation to ex-
plain the generic speech translation process. Let
X denote acoustic observations of a Japanese
X
utterance
recognized
text
target
translation
E
best
translation
J N1
ASR SMT
E
NxK
1
Rescore
Figure 1: Current framework of speech transla-
tion
utterance, typically a sequence of short-time
spectral vectors received at a frame rate of ev-
ery centi-second. It is first recognized as a
Japanese sentence, J . The recognized sentence
is then translated into a corresponding English
sentence, E.
The conversion from X to J is performed in
the ASR module. Based on Bayes? rule, P (J |X)
can be written as
P (J |X) = Pam(X|J)Plm(J)/P (X)
where Pam(X|J) is the acoustic model likeli-
hood of the observations given the recognized
sentence J ; Plm(J), the source language model
probability; and P (X), the probability of all
acoustic observations.
In the experiment we generated a set of N -
best hypotheses, JN1 = {J1, J2, ? ? ? , JN} 1 and
each Ji is determined by
Ji = arg maxJ??i
Pam(X|J)Plm(J)
where ?i is the set of all possible source sen-
tences excluding all higher ranked Jk?s, 1 ? k ?
i ? 1.
The conversion from J to E in Fig. 1 is
the machine translation process. According
to the statistical machine translation formal-
ism (Brown et al, 1993), the translation process
is to search for the best sentence E? such that
E? = arg max
E
P (E|J) = arg max
E
P (J |E)P (E)
where P (J |E) is a translation model charac-
terizing the correspondence between E and J ;
P (E), the English language model probability.
In the IBM model 4, the translation model
P (J |E) is further decomposed into four sub-
models:
? Lexicon Model ? t(j|e): probability of a
word j in the Japanese language being
translated into a word e in the English lan-
guage.
1Hereafter, J1 is called the single-best hypothesis of
speech recognition; JN1 , the N -best hypotheses.
? Fertility model ? n(?|e): probability of
a English language word e generating ?
words.
? Distortion model ? d: probability of distor-
tion, which is decomposed into the distor-
tion probabilities of head words and non-
head words.
? NULL translation model ? p1: a fixed prob-
ability of inserting a NULL word after de-
termining each English word.
In the above we listed seven features: two
from ASR (Pam(X|J), Plm(J)) and five from
SMT (P (E), t(j|e), n(?|e), d, p1).
The third module in Fig. 1 is to rescore trans-
lation hypotheses from SMT by using a feature-
based log-linear model. All translation can-
didates output through the speech recognition
and translation modules are re-evaluated by us-
ing all relevant features and searching for the
best translation candidate of the highest score.
The log-linear model used in our speech trans-
lation process, P (E|X), is
P?(E|X) =
exp(?Mi=1 ?ifi(X,E))?
E? exp(
?M
i=1 ?ifi(X,E?))
? = {?M1 }
(1)
In the Eq. 1, fi(X,E) is the logarithm value
of the i-th feature; ?i is the weight of the i-
th feature. Integrating different features in the
equation results in different models. In the ex-
periments performed in section 4, four different
models will be trained by increasing the number
of features successively to investigate the effect
of different features for improving speech trans-
lation.
In addition to the above seven features, the
following features are also incorporated.
? Part-of-speech language models: English
part-of-speech language models were used.
POS dependence of a translated English
sentence is an effective constraint in prun-
ing English sentence candidates. In our ex-
periments 81 part-of-speech tags and a 5-
gram POS language model were used.
? Length model P (l|E, J): l is the length
(number of words) of a translated English
sentence.
? Jump weight: Jump width for adjacent
cepts in Model 4 (Marcu and Wong, 2002).
? Example matching score: The translated
English sentence is matched with phrase
translation examples. A score is derived
based on the count of matches (Watanabe
and Sumita, 2003).
? Dynamic example matching score: Similar
to the example matching score but phrases
were extracted dynamically from sentence
examples (Watanabe and Sumita, 2003).
Altogether, we used M(=12) different fea-
tures. In section 3, we review Powell?s algo-
rithm (Press et al, 2000) as our tool to opti-
mize model parameters, ?M1 , based on different
objective translation metrics.
3 Parameter Optimization Based
on Translation Metrics
The denominator in Eq. 1 can be ignored since
the normalization is applied equally to every hy-
pothesis. Hence, the choice of the best transla-
tion, E?, out of all possible translations, E, is
independent of the denominator,
E? = arg max
E
M?
i=1
?ilogPi(X,E) (2)
where we write features, fi(X,E), explicitly in
logarithm, logPi(X,E).
The effectiveness of the model in Eq. 2 de-
pends upon the parameter optimization of the
parameter set ?M1 , with respect to some objec-
tively measurable but subjectively relevant met-
rics.
Suppose we have L speech utterances and
for each utterance, we generate N best speech
recognition hypotheses. For each recogni-
tion hypothesis, K English language transla-
tion hypotheses are generated. For the l-th
input speech utterance, there are then Cl =
{El1 , ? ? ? , ElN?K} translations. All L speech ut-
terances generate L?N?K translations in to-
tal.
Our goal is to minimize the translation ?dis-
tortion? between the reference translations, R,
and the translated sentences, E? .
?M1 = optimize D(E? ,R) (3)
where E? = {E?1, ? ? ? , E?L} is a set of translations
of all utterances. The translation E?l of the l-
th utterance is produced by the (Eq. 2), where
E ? Cl.
Let R = {R1, ? ? ? , RL} be the set of transla-
tion references for all utterances. Human trans-
lators paraphrased 16 reference sentences for
each utterance, i.e., Rl contains 16 reference
candidates for the l-th utterance.
D(E? ,R) is a translation ?distortion? or an
objective translation assessment. The following
four metrics were used specifically in this study:
? BLEU (Papineni et al, 2002): A weighted
geometric mean of the n-gram matches be-
tween test and reference sentences multi-
plied by a brevity penalty that penalizes
short translation sentences.
? NIST : An arithmetic mean of the n-gram
matches between test and reference sen-
tences multiplied by a length factor which
again penalizes short translation sentences.
? mWER (Niessen et al, 2000): Multiple ref-
erence word error rate, which computes the
edit distance (minimum number of inser-
tions, deletions, and substitutions) between
test and reference sentences.
? mPER: Multiple reference position inde-
pendent word error rate, which computes
the edit distance without considering the
word order.
The BLEU score and NIST score are calcu-
lated by the tool downloadable 2.
Because the objective function in the model
(Eq. 3) is not smoothed function, we used Pow-
ell?s search method to find a solution. The Pow-
ell?s algorithm used in this work is similar as the
one from (Press et al, 2000) but we modified the
line optimization codes, a subroutine of Powell?s
algorithm, with reference to (Och, 2003).
Finding a global optimum is usually difficult
in a high dimensional vector space. To make
sure that we had found a good local optimum,
we restarted the algorithm by using various ini-
tializations and used the best local optimum as
the final solution.
4 Experiments
4.1 Corpus & System
The data used in this study was the Basic
Travel Expression Corpus (BTEC) (Kikui et al,
2003), consisting of commonly used sentences
listed in travel guidebooks and tour conversa-
tions. The corpus were designed for developing
multiple language speech-to-speech translation
systems. It contains four different languages:
Chinese, Japanese, Korean and English. Only
Japanese-English parallel data was used in this
2http://www.nist.gov/speech/tests/mt/
Table 1: Training, development and test data
from Basic Travel Expression Corpus(BTEC)
Japanese English
Train Sentences 162,318
Words 1,288,767 949,377
Dev. Sentences 510
Words 4015 2983
Test Sentences 508
Words 4112 2951
study. The speech data was recorded by multi-
ple speakers and was used to train the acoustic
models, while the text database was used for
training the language and translation models.
The standard BTEC training corpus, the first
file and the second file from BTEC standard test
corpus #01 were used for training, development
and test respectively. The statistics of corpus is
shown in table 1.
The speech recognition engine used in the ex-
periments was an HMM-based, large vocabu-
lary continuous speech recognizer. The acoustic
HMMs were triphone models with 2,100 states
in total, using 25 dimensional, short-time spec-
trum features. In the first and second pass of
decoding, a multiclass word bigram of a lexicon
of 37,000 words plus 10,000 compound words
was used. A word trigram was used in rescor-
ing the results.
The machine translation system is a graph-
based decoder (Ueffing et al, 2002). The first
pass of the decoder generates a word-graph, a
compact representation of alternative transla-
tion candidates, using a beam search based on
the scores of the lexicon and language mod-
els. In the second pass an A* search traverses
the graph. The edges of the word-graph, or
the phrase translation candidates, are gener-
ated by the list of word translations obtained
from the inverted lexicon model. The phrase
translations extracted from the Viterbi align-
ments of the training corpus also constitute the
edges. Similarly, the edges are also created from
dynamically extracted phrase translations from
the bilingual sentences (Watanabe and Sumita,
2003). The decoder used the IBM Model 4
with a trigram language model and a 5-gram
part-of-speech language model. The training of
IBM model 4 was implemented by the GIZA++
package (Och and Ney, 2003).
4.2 Model Training
In order to quantify translation improvement by
features from speech recognition and machine
translation respectively, we built four log-linear
models by adding features successively. The
four models are:
? Standard translation model(stm): Only
features from the IBM model 4 (M=5) de-
scribed in section 2 were used in the log-
linear models. We did not perform parame-
ter optimization on this model. It is equiv-
alent to setting all the ?M1 to 1. This model
was the standard model used in most sta-
tistical machine translation system. It is
referred to as the baseline model.
? Optimized standard translation models
(ostm): This model consists of the same
features as the previous model ?stm? but
the parameters were optimized by Powell?s
algorithm. We intended to exhibit the ef-
fect of parameter optimization by compar-
ing this model with the baseline ?stm?.
? Optimized enhanced translation models
(oetm): We incorporated additional trans-
lation features described in section 2 to
enrich the model ?ostm?. In this model
the number of the total features, M , is 10.
Model parameters were optimized. We in-
tended to show how much the enhanced
features can improve translation quality.
? Optimized enhanced speech translation
models (oestm): Features from speech
recognition, likelihood scores of acoustic
and language models, were incorporated
additionally into the model ?oetm?. All
the 12 features described in section 2 were
used. Model parameters were optimized.
To optimize ? parameters of the log-linear
models, we used the development data of 510
speech utterances. We adopted an N -best
hypothesis approach (Och, 2003) to train ?.
For each input speech utterance, N?K candi-
date translations were generated, where N is
the number of generated recognition hypothe-
ses and K is the number of translation hypothe-
ses. A vector of dimension M , corresponding to
multiple features used in the translation model,
was generated for each translation candidate.
The Powell?s algorithm was used to optimize
these parameters. We used a large K to ensure
that promising translation candidates were not
Table 2: Comparisons of single-best and N -best
hypotheses of speech recognition performance
in terms of word accuracy, sentence accuracy,
insertion, deletion and substitution error rates
word sent ins del sub
acc(%) acc(%) (%) (%) (%)
single-best 93.5 78.7 2.0 0.8 3.6
N -best 96.1 87.0 1.2 0.3 2.2
pruned out. In the training, we set N=100 and
K=1, 000.
By using different objective translation eval-
uation metrics described in section 3, for each
model we obtained four sets of optimized pa-
rameters with respect to BLEU, NIST, mWER
and mPER metrics, respectively.
4.3 Translation Improvement by
Additional Features
All 508 utterances in the test data were used to
evaluate the models. Similar to processing the
development data, the speech recognizer gen-
erated N -best (N=100) recognition hypothe-
ses for each test speech utterance. Table 2
shows speech recognition results of the test data
set in single-best and N -best hypotheses. We
observed that over 8% sentence accuracy im-
provement was obtained from the single-best to
the N -best recognition hypotheses. The recog-
nized sentences were then translated into corre-
sponding English sentences. 1,000 such trans-
lation candidates were produced for each recog-
nition hypothesis. These candidates were then
rescored by each of the four models with four
sets of optimized parameters obtained in the
training respectively. The candidates with the
best score were chosen.
The best translations generated by a model
were evaluated by the translation assessment
metrics used to optimize the model parameters
in the development. The experimental results
are shown in Table 3.
In the experiments we changed the number
of speech recognition hypotheses, N , to see how
translation performance is changed as N . We
found that the best translation was achieved
when a relatively smaller set of hypotheses,
N=5, was used. Hence, the values in Table 3
were obtained when N was set to 5.
We test each model by employing the single-
best recognition hypothesis translations and
the N -best recognition hypothesis translations.
Table 3: Translation improvement from the
baseline model(stm) to the optimized enhanced
speech translation model(oestm): Models are
optimized using the same metric as shown in
the columns. Numbers are in percentage except
NIST score.
BLEU NIST mWER mPER
Single-best recognition hypothesis translation
stm 54.2 7.5 39.8 34.8
ostm 59.0 8.9 36.2 34.0
oetm 59.2 9.9 34.3 31.5
N -best recognition hypothesis translation
stm 55.5 7.3 39.8 35.4
ostm 61.1 8.8 36.4 33.9
oetm 61.1 10.0 34.0 31.1
oestm 62.1 10.2 33.7 29.4
The single-best translation was from the trans-
lation of the single best hypotheses of the speech
recognition and the N -best hypothesis trans-
lation was from the translations of all the hy-
potheses produced by speech recognition.
In Table 3, we observe that a large improve-
ment is achieved from the baseline model ?stm?
to the final model ?oestm?. The BLEU, NIST,
mWER, mPER scores are improved by 7.9%,
2.7, 6.1%, 5.4% respectively. Note that a high
value of BLEU and NIST score means a good
translation while a worse translation for mWER
and mPER. Consistent performance improve-
ment was achieved in the single-best and N -
best recognition hypotheses translations. We
observed that the improvement were due to the
following reasons:
? Optimization. Models with optimized pa-
rameters yielded a better translation than
the models with unoptimized parameters.
It can be seen by comparing the model
?stm? with the model ?ostm? for both the
single-best and the N -best results.
? N -best recognition hypotheses. In major-
ity of the cells in Table 3, translation per-
formance of the N -best recognition is bet-
ter than of the corresponding single-best
recognition. N -best BLEU score of ?ostm?
improved over the single-best of ?ostm? by
2.1%. However, NIST score is indifferent
to the change. It appears that NIST score
is insensitive to detect slight translation
changes.
Table 4: Translation improvement of incorrectly
recognized utterances from single-best(oetm) to
N -best(oestm)
BLEU NIST mWER mPER
single-best 29.0 6.1 59.7 51.8
N -best 36.3 7.2 54.4 47.9
? Enhanced features. Translation perfor-
mance is improved steadily when more fea-
tures are incorporated into the log-linear
models. Translation performance of model
?oetm? is better than model ?ostm? be-
cause more effective translation features
are used. Model ?oestm? is better than
model ?oetm? due to its enhanced speech
recognition features. It confirms that our
approach to integrate features from speech
recognition and translation features works
very well.
4.4 Recognition Improvement of
Incorrectly Recognized Sentences
In previous experiments we demonstrated that
speech translation performance was improved
by the proposed enhanced speech translation
model ?oestm?. In this section we want to show
that this improvement is because of the signifi-
cant improvement of incorrectly recognized sen-
tences when N -best recognition hypotheses are
used.
We carried out the following experiments.
Only incorrectly recognized sentences were ex-
tracted for translation and re-scored by the
model ?oetm? for the single-best case and the
model ?oestm? for the N -best case. The trans-
lation results are shown in Table 4. Translation
of incorrectly recognized sentences are improved
significantly as shown in the table.
Because we used N -best recognition hypothe-
ses, the log-linear model chose the recogni-
tion hypothesis among the N hypotheses which
yielded the best translation. As a result, speech
recognition could be improved if the higher ac-
curate recognition hypotheses was chosen for
translation. This effect can be observed clearly
if we extracted the chosen recognition hypothe-
ses of incorrectly recognized sentences. Table 5
shows the word accuracy and sentence accuracy
of the recognition hypotheses selected by the
translation module. The sentence accuracy of
incorrectly recognized sentences was improved
by 7.5%. The word accuracy was also improved.
Table 5: Recognition accuracy of incorrectly
recognized utterance improved by N -best hy-
pothesis translation.
word acc. (%) sent. acc. (%)
single-best 74.6 0
N -best BLEU 76.4 7.5
mWER 75.9 6.5
5 Discussions
As regards to integrating speech recognition
with translation, a coupling structure (Ney,
1999) was proposed as a speech translation in-
frastructure that multiplies acoustic probabili-
ties with translation probabilities in a one-step
decoding procedure. But no experimental re-
sults have been given on whether and how this
coupling structure improved speech translation.
(Casacuberta et al, 2002) used a finite-state
transducer where scores from acoustic infor-
mation sources and lexicon translation models
were integrated together. Word pairs of source
and target languages were tied in the decoding
graph. However, this method was only tested
for a pair of similar languages, i.e., Spanish to
English. For translating between languages of
different families where the syntactic structures
can be quite different, like Japanese and En-
glish, rigid tying of word pair still remains to be
shown its effectiveness for translation.
Our approach is rather general, easy to imple-
ment and flexible to expand. In the experiments
we incorporated features from acoustic models
and language models. But this framework is
flexible to include more effective features. In-
deed, the proposed speech translation paradigm
of log-linear models have been shown effective in
many applications (Beyerlein, 1998) (Vergyri,
2000) (Och, 2003).
In order to use speech recognition features,
the N -best speech recognition hypotheses were
needed. Using N -best could bear computing
burden. However, our experiments have shown
a smaller N seems to be adequate to achieve
most of the translation improvement without
significant increasing of computations.
6 Conclusion
In this paper we presented our approach of in-
corporating both speech recognition and ma-
chine translation features into a log-linear
speech translation model to improve speech
translation.
Under this new approach, translation perfor-
mance was significantly improved. The perfor-
mance improvement was confirmed by consis-
tent experimental results and measured by us-
ing various objective translation metrics. In
particular, BLEU score was improved by 7.9%
absolute.
We show that features derived from speech
recognition: likelihood of acoustic and language
models, helped improve speech translation. The
N -best recognition hypotheses are better than
the single-best ones when they are used in trans-
lation. We also show that N -best recogni-
tion hypothesis translation can improve speech
recognition accuracy of incorrectly recognized
sentences.
The success of the experiments owes to the
use of statistical machine translation and log-
linear models so that various of effective fea-
tures can be jointed and balanced to output the
optimal translation results.
Acknowledgments
We would like to thank for assistance from Ei-
ichiro Sumita, Yoshinori Sagisaka, Seiichi Ya-
mamoto and the anonymous reviewers.
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy of Japan entitled ?A study of speech dia-
logue translation technology based on a large
corpus?.
References
Peter Beyerlein. 1998. Discriminative model
combination. In Proc.of ICASSP?1998, vol-
ume 1, pages 481?484.
Peter F. Brown, Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta, Enrique Vidal, and
Juan M. Vilar. 2002. Architectures for
speech-to-speech translation using finite-state
models. In Proc. of speech-to-speech trans-
lation workshop, pages 39?44, Philadelphia,
PA, July.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech transla-
tion. In Proc.of EUROSPEECH?2003, pages
381?384, Geneva.
Daniel Marcu and William Wong. 2002. A
phrase-based, joint probability model for sta-
tistical machine translation. In Proc. of
EMNLP-2002, Philadelphia, PA, July.
Hermann Ney. 1999. Speech translation: Cou-
pling of recognition and translation. In Proc.
of ICASSP?1999, volume 1, pages 517?520,
Phoenix, AR, March.
Sonja Niessen, Franz J. Och, Gregor Leusch,
and Hermann Ney. 2000. An evaluation tool
for machine translation: Fast evaluation for
machine translation research. In Proc.of the
LREC (2000), pages 39?45, Athens, Greece,
May.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proc. of ACL?2003, pages 160?167.
Kishore A. Papineni, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of machine
translation. In Proc. of ACL?2002, pages
311?318, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky,
William T. Vetterling, and Brian P. Flan-
nery. 2000. Numerical Recipes in C++.
Cambridge University Press, Cambridge,
UK.
Nicola Ueffing, Franz Josef Och, and Hermann
Ney. 2002. Generation of word graphs in sta-
tistical machine translation. In Proc. of the
Conference on Empirical Methods for Natu-
ral Language Processing (EMNLP02), pages
156?163, Philadelphia, PA, July.
Dimitra Vergyri. 2000. Use of word level side
information to improve speech recognition. In
Proc. of the IEEE International Conference
on Acoustics, Speech and Signal Processing,
2000.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical ma-
chine translation. In Machine Translation
Summit IX, pages 410?417, New Orleans,
Louisiana.
171
172
173
174
Empirical Study of Utilizing Morph-Syntactic
Information in SMT
Young-Sook Hwang, Taro Watanabe, and Yutaka Sasaki
ATR SLT Research Labs, 2-2-2 Hikaridai Seika-cho,
Soraku-gun Kyoto, 619-0288, Japan
{youngsook.hwang, taro.watanabe, yutaka.sasaki}@atr.jp
Abstract. In this paper, we present an empirical study that utilizes
morph-syntactical information to improve translation quality. With three
kinds of language pairs matched according to morph-syntactical similar-
ity or difference, we investigate the effects of various morpho-syntactical
information, such as base form, part-of-speech, and the relative positional
information of a word in a statistical machine translation framework.
We learn not only translation models but also word-based/class-based
language models by manipulating morphological and relative positional
information. And we integrate the models into a log-linear model. Ex-
periments on multilingual translations showed that such morphological
information as part-of-speech and base form are effective for improving
performance in morphologically rich language pairs and that the relative
positional features in a word group are useful for reordering the local
word orders. Moreover, the use of a class-based n-gram language model
improves performance by alleviating the data sparseness problem in a
word-based language model.
1 Introduction
For decades, many research efforts have contributed to the advance of statisti-
cal machine translation. Such an approach to machine translation has proven
successful in various comparative evaluations. Recently, various works have im-
proved the quality of statistical machine translation systems by using phrase
translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most sta-
tistical machine translation systems still consider surface forms and rarely use
linguistic knowledge about the structure of the languages involved[8]. In this
paper, we address the question of the effectiveness of morpho-syntactic features
such as parts-of-speech, base forms, and relative positions in a chunk or an ag-
glutinated word for improving the quality of statistical machine translations.
Basically, we take a statistical machine translation model based on an IBM
model that consists of a language model and a separate translation model [5]:
eI1 = argmaxeI1Pr(f
J
1 |eI1)Pr(eI1) (1)
The translation model links the source language sentence to the target language
sentence. The target language model describes the well-formedness of the target
language sentence.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 474?485, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Empirical Study of Utilizing Morph-Syntactic Information in SMT 475
One of the main problems in statistical machine translation is to learn the
less ambiguous correspondences between the words in the source and target
languages from the bilingual training data. When translating one source lan-
guage(which may be inflectional or non-inflectional) into the morphologically
rich language such like Japanese or Korean, the bilingual training data can be
exploited better by explicitly taking into account the interdependencies of re-
lated inflected or agglutinated forms. In this study, we represent a word with
its morphological features in both sides of the source and the target language
to learn less ambiguous correspondences between the source and the target lan-
guage words or phrases. In addition, we utilize the relative positional information
of a word in its word group to consider the word order in an agglutinated word
or a chunk.
Another problem is to produce a correct target sentence. To produce more
correct target sentence, we should consider the following problems: word re-
ordering in a language pair with different word order, production of correct
inflected and agglutinated words in an inflectional or agglutinative target lan-
guage. In this study, we tackle the problem with language models. For learning
language model that can treat morphological and word-order problem, we rep-
resent a word with its morphological and positional information. However, a
word-based language model with enriched word is likely to suffer from a severe
data sparseness problem. To alleviate the problem, we interpolate the word-based
language model with a class-based n-gram model.
In the next section, we briefly discuss related works. Then, we describe the
method that utilizes morpho-syntactic information under consideration for im-
proving the quality of translations. Then we report the experimental results with
some analysis and conclude our study.
2 Related Work
Few papers deal with the integration of linguistic information into the process of
statistical machine translation. [8] introduced hierarchical lexicon models includ-
ing base-form and POS information for translation from German into English.
Irrelevant information contained in the German entries for the generation of the
English translation were omitted. They trained the lexicon model using maxi-
mum entropy. [6] enriched English with knowledge to help select the correct full-
form from morphologically richer languages such as Spanish and Catalan. In other
words, they introduced a splicing operation that merged the pronouns/modals
and verbs for treating differences in verbal expressions. To treat the unknown en-
tries in the lexicon resulting from the splicing operation, they trained the lexicon
model using maximum entropy and used linguistic knowledge just in the source
language part and not in the target language. They don?t use any linguistic knowl-
edge in the target language and use full-form words during training.
In addition, [6] and [8] proposed re-ordering operations to make similar word
orders in the source and target language sentences. In other words, for the in-
terrogative phrases with different word order from the declarative sentences,
476 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
they introduced techniques of question inversion and removed unnecessary aux-
iliary verbs. But, such inversion techniques require additional preprocessing with
heuristics.
Unlike them, we investigate methods for utilizing linguistic knowledge in
both of the source and the target language at the morpheme level. To generate
a correct full-form word in a target language, we consider not only both the
surface and base form of a morpheme but also the relative positional informa-
tion in a full-form word. We strongly utilize the combined features in language
modeling. By training alignments and language models with morphological and
positional features at the morpheme-level, the severe data sparseness problem
can be alleviated with the combined linguistic features. And the correspondence
ambiguities between the source and target words can be decreased.
3 Utilization of Morpho-Syntactic Information in SMT
Generally, the probabilistic lexicon resulting from training a translation model
contains all word forms occurring in the training corpus as separate entries,
not taking into account whether they are inflected forms. A language model
is also composed of the words in the training corpus. However, the use of a
full-form word itself may cause severe data sparseness problem, especially rel-
evant for more inflectional/agglutinative languages like Japanese and Korean.
One alternative is to utilize the results of morphological analysis such as base
form, part-of-speech and other information at the morpheme level. We address
the usefulness of morphological information to improve the quality of statistical
machine translation.
3.1 Available Morpho-Syntactic Information
A prerequisite for methods that improve the quality of statistical machine trans-
lation is the availability of various kinds of morphological and syntactic infor-
mation. In this section, we examine the morpho-syntactic information available
from the morphological analyzers of Korean, Japanese, English and Chinese and
describe a method of utilizing the information.
Japanese and Korean are highly inflectional and agglutinative languages, and
in English inflection has only a marginal role; whereas Chinese usually is regarded
as an isolating language since it has almost no inflectional morphology. As the
syntactic role of each word within Japanese and Korean sentences are often
marked, word order in a sentence plays a relatively small role in characterizing
the syntactic function of each word than in English or Chinese sentences. Thus,
Korean and Japanese sentences have a relatively free word order; whereas words
within Chinese and English sentences adhere to a rigid order. The treatment
of inflection, and not word order, plays the most important role in processing
Japanese and Korean, while word order has a central role in Chinese and English.
Figure 1 shows some examples of morphological information by Chinese,
Japanese, English and Korean morphological analyzers and Figure 2 the corre-
spondences among the words. Note that Korean and Japanese are very similar:
Empirical Study of Utilizing Morph-Syntactic Information in SMT 477
Fig. 1. Examples of linguistic information from Chinese, Japanese, English, and Korean
morphological analyzers
Fig. 2. Correspondences among the words in parallel sentences
highly inflected and agglutinated. One difference in Korean from Japanese is
that a Korean sentence consists of spacing units, eojeols,1 while there are no
space in a Japanese sentence. Especially, a spacing unit(i.e., eojeol) in Korean
often becomes a base phrase that contains such syntactic information as subject,
object, and the mood/tense of a verb in a given sentence. The treatment of such
a Korean spacing unit may contribute to the improvement of translation quality
because a morpheme can be represented with its relative positional information
within an eojeol. The relative positional information is obtained by calculating
the distance between the beginning syllable of a given eojeol and the beginning
of each morpheme within the eojeol. The relative positional information is rep-
resented with indexes of the beginning and the ending syllables (See Figure 1).
3.2 Word Representation
A word(i.e. morpheme) is represented by the combination of the information pro-
vided by a morphological analyzer including the surface form, base form, part-of-
speech or other information such as relative position within an eojeol. The word
1 An eojeol is composed of no less than one morpheme by agglutination principle.
478 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Table 1. Word Representation According to Morpho-Syntactic Characteristics (S: sur-
face form, B:base form, P:part-of-speech, L:RelativePosition)
Chinese English Japanese Korean
Morph-Syntactic no inflection Inflectional Inflectional, Inflectional
Characteristics Agglutinative Agglutinative
Spacing Unit
(Word-Order) Rigid Rigid Partial Free Partial Free
Word Representation S?P S?B?P S?B?P S?B?P?L
S?B, S?P S?B, S?P S?B?P, S?B?L, S?P?L
S?B, S?P, S?L
enriched by the combination of morph-syntactic information must alway include
the surface form of a given word for the direct generation of target sentence
without any post-processing. Other different morphological information is com-
bined according to representation models such as surface plus base form (SB),
surface plus part-of-speech (SP), surface plus relative position (SL), and so on.
Table 1 shows the word representation of each language with every possible
morphological information. Yet, we are not limited to only this word represen-
tation, but we have many possibilities of word representation by removing some
morphological information or inserting additional morpho-syntactic information
as mentioned previously. In order to develop the best translation systems, we
select the best word representation models of the source and the target language
through empirical experiments.
The inherent in the original word forms is augmented by a morphological
analyzer. Of course, this results in an enlarged vocabulary while it may provide
useful disambiguation clues. However, since we regard a morpheme as a word
in a corpus(henceforth, we call a morpheme a word), the enlarged vocabulary
does not make more severe data sparseness problem than using the inflected or
agglutinated word. By taking the approch of morpheme-level alignment, we may
obtain more accurate correspondences among words as illustrated in Figure 2.
Moreover, by learning the language model with rich morph-syntactic informa-
tion, we can generate more syntactically fluent and correct sentence.
3.3 Log-Linear Model for Statistical Machine Translation
In order to improve translation quality, we evaluate the translation candidates
by using the relevant features in a log-linear model framework[11]. The log-linear
model used in our statistical translation process, Pr(eI1|fJ1 ), is:
Pr(eI1|f I1 ) =
exp(
?
m ?mhm(e
I
1, f
J
1 , a
J
1 ))
?
e?I1 ,f
I
1 ,a
I
1
exp(
?
m ?mhm(e
?I
1 , f
J
1 , a
J
1 ))
(2)
where hm(eI1, f
J
1 , a
J
1 ) is the logarithm value of the m-th feature; ?m is the weight
of the m-th feature. Integrating different features in the equation results in dif-
ferent models.
Empirical Study of Utilizing Morph-Syntactic Information in SMT 479
The statistical machine translation process in IBM models is as follows; a
given source string fJ1 = f1 ? ? ? fJ is to be translated into eI1 = e1 ? ? ? eI . Accord-
ing to the Bayes? decision rule, we choose the optimal translation for given string
fJ1 that maximizes the product of target language model Pr(e
I
1) and translation
model Pr(fJ1 |eI1)
eI1 = argmaxeI1Pr(f
J
1 |eI1)Pr(eI1) (3)
In IBM model 4, translation model P (fJ1 |eI1) is further decomposed into four
submodels:
? Lexicon Model, t(f |e): probability of word f in the source language being
translated into word e in the target language.
? Fertility model, n(?|e): probability of target language word e generating ?
words.
? Distortion model d: probability of distortion, which is decomposed into the
distortion probabilities of head words and non-head words.
? NULL translation model p1: a fixed probability of inserting a NULL word
after determining each target word.
In addition to the five features (Pr(eI1), t(f |e), n(?|e), d, p1) from IBM model
4, we incorporate the following features into the log-linear translation model:
? Class-based n-gram model Pr(eI1) =
?
i Pr(ei|ci)Pr(ci|c
i?1
1 ): Grouping of
words into C classes is done according to the statistical similarity of their
surroundings. Target word ei is mapped into its class, ci, which is one of C
classes[13].
? Length model Pr(l|eI1, fJi ): l is the length (number of words) of a translated
target sentence.
? Example matching score: The translated target sentence is matched with
phrase translation examples. A score is derived based on the number of
matches [10]. To extract phrase translation examples, we compute the inter-
section of word alignment of both directions and derive the union. Then we
grab the phrase translation pairs that contain at least one intersected word
alignment and some unioned word alignments[1].
Under the framework of log-linear models, we investigate the effects of morpho-
syntactic information with word representation. The overall training and testing
process with morphological and positional information is depicted in Figure 3. In
the training step, we train the word- and class-based language models with var-
ious word representation methods[12]. Also, we make word alignments through
the learning of IBM models by using GIZA++ toolkit[3]: we learn the translation
model toward IBM model 4, initiating translation iterations from IBM model
1 with intermediate HMM model iterations. Then, we extract example phrases
and translation model features from the alignment results.
Then in the test step, we perform morphological anlysis of a given sentence for
word representation corresponding to training corpus representation. We decode
the best translation of a given test sentence by generating word graphs and
searching for the best hypothesis in a log-linear model[7].
480 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Fig. 3. Overview of training and test of statistical machine translation system with
linguistic information
4 Experiments
4.1 Experimental Environments
The corpus for the experiment was extracted from the Basic Travel Expression
Corpus (BTEC), a collection of conversational travel phrases for Chinese, En-
glish, Japanese and Korean[15]. The entire corpus was split into three parts:
152,169 sentences in parallel for training, 10,150 sentences for testing and the
remaining 10,148 sentences for parameter tuning, such as termination criteria
for training iteration and parameter tuning for decoders. For the reconstruction
of each corpus with morphological information, we used in-house morphological
Table 2. Statistics of Basic Travel Expression Corpus
Chinese English Japanese Korean
# of sentences 167,163
# of words(morph) 1,006,838 1,128,151 1,226,774 1,313,407
Vocabulary size(S) 17,472 11,737 19,485 17,600
Vocabulary size(B) 17,472 9172 15,939 15,410
Vocabulary size(SB) 17,472 13,385 20,197 18,259
Vocabulary size(SP) 18,505 13,467 20,118 20,249
Vocabulary size(SBP(L)) 18,505 14,408 20,444 20,369(26,668)
# of singletons(S) 7,137 4,046 8,107 7,045
# of singletons(B) 7,137 3,025 6,497 6,303
# of singletons(SB) 7,137 4,802 9,453 7,262
# of singletons(SP) 7,601 4,693 8,343 7,921
# of singletons(SBP(L)) 7,601 5,140 8,525 7,983(11,319)
Empirical Study of Utilizing Morph-Syntactic Information in SMT 481
Table 3. Perplexities of tri-gram language model trained on the training corpora
with S, SB, SP SBP, SBL, and SBPL morpho-syntactic representation: word-based
3-gram/class-based 5-gram
S SB SP SBP SBL SBPL
Chinese 31.57/24.09 N/S 35.83/26.28 N/A N/A N/A
English 22.35/18.82 22.19/18.54 22.24/18.12 22.08/18.03 N/A N/A
Japanese 17.89/ 13.44 17.92/13.29 17.82/13.13 17.83/13.06 N/A N/A
Korean 15.54/12.42 15.41/12.09 16.04/11.89 16.03/11.88 16.48/12.24 17.13/11.99
analyzers for four languages: Chinese morphological analyzer with 31 parts-of-
speech tags, English morphological analyzer with 34 tags, Japanese morphologi-
cal analyzer with 34 tags, and Korean morphological analyzer with 49 tags. The
accuracies of Chinese, English, Japanese and Korean morphological analyzers in-
cluding segmentation and POS tagging are 95.82% , 99.25%, 98.95%, and 98.5%
respectively. Table 2 summarizes the morph-syntactic statistics of the Chinese,
English, Japanese, and Korean.
For the four languages, word-based and class-based n-gram language models
were trained on the training set by using SRILM toolkit[12]. The perplexity of
each language model is shown in Table 3.
For the four languages, we chose three kinds of language pairs according to
the linguistic characteristics of morphology and word order, Chinese-Korean,
Japanese-Korean, and English-Korean. 42 translation models based on word
representation methods(S, SB, SP, SBP, SBL, SPL,SBPL) were trained by using
GIZA++[3].
4.2 Evaluation
Translation evaluations were carried out on 510 sentences selected randomly
from the test set. The metrics for the evaluations are as follows:
mWER(multi-reference Word Error Rate), which is based on the minimum
edit distance between the target sentence and the sentences in the reference
set [9].
BLEU, which is the ratio of the n-gram for the translation results found in the
reference translations with a penalty for too short sentences [14].
NIST which is a weighted n-gram precision in combination with a penalty for
too short sentences.
For this evaluation, we made 16 multiple references available. We computed all
of the above criteria with respect to these multiple references.
Table 4, 5 and 6 show the evaluation results on three kinds of language pairs.
The effects of morpho-syntactic information and class-based n-gram language
models on multi-lingual machine translation are shown: The combined morpho-
logical information was useful for improving the translation quality in the NIST,
BLEU and mWER evaluations. Moreover, the class-based n-gram language mod-
els were effective in the BLEU and the mWER scores.
482 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Table 4. Evaluation results of Japanese to Korean and Korean to Japaneses transla-
tions(with class-based n-gram/word-based n-gram language model)
J to K K to J
NIST BLEU WER NIST BLEU WER
S 8.46/8.64 0.694/0.682 26.33/26.73 8.21/8.39 0.666/0.649 25.00/25.81
SB 8.05/8.32 0.705/0.695 26.82/26.97 7.67/8.17 0.690/0.672 23.77/24.68
SP 9.15/9.25 0.755/0.747 21.71/22.22 9.02/9.13 0.720/0.703 21.94/23.50
SL 8.37/8.47 0.699/0.667 25.49/27.76 8.48/8.74 0.671/0.629 25.14/27.88
SBL 8.92/9.12 0.748/0.730 22.66/23.36 8.85/8.92 0.712/0.691 21.88/23.37
SBP 8.19/8.57 0.713/0.696 26.17/27.09 8.21/8.39 0.698/0.669 22.94/24.88
SBPL 8.41/8.85 0.772/0.757 22.30/21.74 7.77/7.83 0.626/0.619 25.19/25.57
Table 5. Evaluation results of English to Korean and Korean to English transla-
tions(with class-based n-gram/word-based n-gram language model)
E to K K to E
NIST BLEU WER NIST BLEU WER
S 5.12/5.79 0.353/0.301 51.12/58.52 5.76/6.05 0.300/0.255 52.54/61.23
SB 6.71/6.87 0.533/0.474 39.10/47.18 7.72/8.15 0.482/0.446 37.86/42.71
SP 6.88/7.19 0.552/0.502 37.63/42.34 8.01/8.46 0.512/0.460 35.13/40.91
SL 6.66/6.96 0.546/0.516 38.20/40.67 7.71/8.02 0.484/0.436 36.79/42.88
SPL 6.16/7.01 0.542/0.519 38.21/39.85 7.83/8.22 0.482/0.443 37.52/41.63
SBL 6.52/6.93 0.547/0.504 37.76/42.23 7.64/8.08 0.479/0.439 37.10/42.30
SBP 7.42/7.60 0.612/0.573 32.17/35.96 8.86/9.05 0.551/0.523 33.13/37.07
SBPL 6.29/6.59 0.580/0.561 36.73/38.36 8.08/8.36 0.528/0.515 36.46/38.21
Table 6. Evaluation results of Chinese to Korean and Korean to Chinese transla-
tions(with class-based n-gram/word-based n-gram language model)
C to K K to C
NIST BLEU WER NIST BLEU WER
S 7.62/7.82 0.640/0.606 30.01/32.79 7.85/7.69 0.380/0.365 53.65/58.46
SB 7.73/7.98 0.643/0.632 29.26/30.08 7.68/7.50 0.366/0.349 54.48/60.49
SP 7.71/7.98 0.651/0.643 28.26/28.60 8.00/7.77 0.383/0.362 54.15/58.30
SL 7.64/7.97 0.656/0.635 28.94/30.33 7.84/7.65 0.373/0.350 54.53/58.38
SPL 7.69/7.93 0.665/0.659 28.43/28.88 7.78/7.62 0.373/0.351 56.14/59.54
SBL 7.65/7.94 0.659/0.635 28.76/30.87 7.85/7.64 0.377/0.354 55.01/58.39
SBP 7.81/7.98 0.660/0.643 28.85/29.61 7.94/7.68 0.386/0.360 53.99/58.94
SBPL 7.64/7.90 0.652/0.634 29.54/30.46 7.82/7.66 0.376/0.358 55.64/58.79
In detail, Table 4 shows the effects of the morphological and relative posi-
tional information on Japanese-to-Korean and Korean-to-Japanese translation.
In almost of the evaluation metrics, the SP model in which a word is repre-
sented by a combination of its surface form and part-of-speech showed the best
performance. The SBL model utilizing the base form and relative positional
information only in Korean showed the second best performance. In Korean-
Empirical Study of Utilizing Morph-Syntactic Information in SMT 483
to-Japanese translation, the SBPL model showed the best score in BLEU and
mWER. In this language pair of highly inflectional and agglutinative languages,
the part-of-speech information combined with surface form was the most ef-
fective in improving the performance. The base form and relative positional
information were less effective than part-of-speech. It could be explained in sev-
eral points: Japanese and Korean are very similar languages in the word order
of SOVs and the ambiguities of translation correspondences in both directions
were converged into 1.0 by combining the distinctive morphological information
with the surface form. When refering to the vocabulay size of SP model in Table
2, it makes it more clear. The Japanese-to-Korean translation outperforms the
Korean-to-Japanese. It might be closely related to the language model: the per-
plexity of the Korean language model is lower than Japanese according to our
corpus statistics.
Table 5 shows the performance of the English-to-Korean and Korean-to-
English translation: a pair of highly inflectional and agglutinative language with
partially free word-order and an inflectional language with rigid word order. In
this language pair, the combined word representation models improved the trans-
lation performance into significantly higher BLEU and mWER scores in both
directions. The part-of-speech and the base form information were distinctive
features. When comparing the performance of SP, SB and SL models, part-of-
speech might be more effective than base form or relative positional information,
and the relative positional information in Korean might play a role not only in
controlling word order in the language models but also in discriminating word
correspondences during alignment.
When the target language was Korean, we had higher BLEU scores in all the
morpho-syntactic models but lower NIST scores. In other words, we took advan-
tage of generating more accurate full-form eojeol with positional information,
i.e. local word ordering.
Table 6 shows the performance of the Chinese-to-Korean and Korean-to-
Chinese translation: a pair of a highly inflectional and agglutinative language
with partially free word order and a non-inflectional language with rigid word
order. This language pair is a quite morpho-syntactically different. When a non-
inflectional language is a target language(i.e. Korean-to-Chinese translation), the
performance was the worst compared with other language pairs and directions in
BLEU and mWER. On the other hand, the performance of Chinese-to-Korean
was much better than Korean-to-Chinese, meaning that it is easier to generate
Korean sentence from Chinese the same as in Japanese-to-Korean and English-
to-Korean. In this language pair, we had gradual improvements according to
the use of combined morpho-syntactic information, but there was no significant
difference from the use of only the surface form. There was scant contribution of
Chinese morphological information such as part-of-speech. On the other hand,
we could get some advantageous Korean morpho-syntactic information in the
Chinese-to-Korean translation, i.e., the advantage of language and translation
models using morpho-syntactic information.
484 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
5 Conclusion and Future Works
In this paper, we described an empirical study of utilizing morpho-syntactic
information in a statistical machine translation framework. We empirically in-
vestigated the effects of morphological information with several language pairs:
Japanese and Korean with the same word order and high inflection/
agglutination, English and Korean, a pair of a highly inflecting and agglutinat-
ing language with partial free word order and an inflecting language with rigid
word order, and Chinese-Korean, a pair of a highly inflecting and agglutinating
language with partially free word order and a non-inflectional language with
rigid word order. As the results of experiments, we found that combined mor-
phological information is useful for improving the translation quality in BLEU
and mWER evaluations. According to the language pair and the direction, we
had different combinations of morpho-syntactic information that are the best
for improving the translation quality: SP(surface form and part-of-speech) for
translating J-to-K or K-to-J, SBP(surface form, base form and part-of-speech)
for E-to-K or K-to-E, SPL(surface form, part-of-speech and relative position) for
C-to-K. The utilization of morpho-syntactic information in the target language
was the most effective. Language models based on morpho-syntactic informa-
tion were very effective for performance improvement. The class-based n-gram
models improved the performance with smoothing effects in the statistical lan-
guage model. However, when translating an inflectional language, Korean into
a non-inflectional language, Chinese with quite different word order, we found
very few advantages using morphological information. One of the main reasons
might be the relatively low performance of the Chinese morphological analyzer.
The other might come from the linguistic difference. For the latter case, we need
to adopt approaches to reflect the structural characteristics such like using a
chunker/parser, context-dependent translation modeling.
Acknowledgments
The research reported here was supported in part by a contract with the National
Institute of Information and Communications Technology entitled ?A study of
speech dialogue translation technology based on a large corpus?.
References
1. Koehn P., Och F.J., and Marcu D.: Statistical Phrase-Based Translation, Proc. of
the Human Language Technology Conference(HLT/NAACL) (2003)
2. Och F. J., Tillmann C., Ney H.: Improved alignment models for statistical machine
translation, Proc. of EMNLP/WVLC (1999).
3. Och F.J. and Ney H. Improved Statistical Alignment Models, Proc. of the 38th
Annual Meeting of the Association for Computational Linguistics (2000) pp. 440-
447.
4. Zens R. and Ney H.: Improvements in Phrase-Based Statistical Machine Transla-
tion, Proc. of the Human Language Technology Conference (HLT-NAACL) (2004)
pp. 257-264
Empirical Study of Utilizing Morph-Syntactic Information in SMT 485
5. Brown P. F., Della Pietra S. A., Della Pietra V. J., and Mercer R. L.: The math-
ematics of statistical machine translation: Parameter estimation, Computational
Linguistics, (1993) 19(2):263-311
6. Ueffing N., Ney H.: Using POS Information for Statistical Machine Translation
into Morphologically Rich Languages, In Proc. 10th Conference of the European
Chapter of the Association for Computational Linguistics (EACL), (2003) pp. 347-
354
7. Ueffing N., Och F.J., Ney H.: Generation of Word Graphs in Statistical Machine
Translation In Proc. Conference on Empirical Methods for Natural Language Pro-
cessing, (2002) pp. 156-163
8. Niesen S., Ney H.: Statistical Machine Translation with Scarce Resources using
Morpho-syntactic Information, Computational Linguistics, (2004) 30(2):181-204
9. Niesen S., Och F.J., Leusch G., Ney H: An Evaluation Tool for Machine Transla-
tion: Fast Evaluation for MT Research, Proc. of the 2nd International Conference
on Language Resources and Evaluation, (2000) pp. 39-45
10. Watanabe T. and Sumita E.: Example-based Decoding for Statistical Machine
Translation, Proc. of MT Summit IX (2003) pp. 410?417
11. Och F. J. Och and Ney H.: Discriminative Training and Maximum Entropy Models
for Statistical Machine Translation, Proc. of ACL (2002)
12. Stolcke, A.: SRILM - an extensible language modeling toolkit. In Proc. Intl. Conf.
Spoken Language Processing, (2002) Denver.
13. Brown P. F., Della Pietra V. J. and deSouza P. V. and Lai J. C. and Mercer
R.L.: Class-Based n-gram Models of Natural Language, Computational Linguistics
(1992) 18(4) pp. 467-479
14. Papineni K., Roukos S., Ward T., and Zhu W.-J.: Bleu: a method for automatic
evaluation of machine translation, IBM Research Report,(2001) RC22176.
15. Takezawa T., Sumita E., Sugaya F., Yamamoto H., and Yamamoto S.: Toward a
broad-coverage bilingual corpus for speech translation of travel conversations in
the real world, Proc. of LREC (2002), pp. 147-152.
Chunk-based Statistical Translation
Taro Watanabe?, Eiichiro Sumita? and Hiroshi G. Okuno?
{taro.watanabe, eiichiro.sumita}@atr.co.jp
? ATR Spoken Language Translation ?Department of Intelligence Science
Research Laboratories and Technology
2-2-2 Hikaridai, Keihanna Science City Graduate School of Informatics, Kyoto Uniersity
Kyoto 619-0288 JAPAN Kyoto 606-8501 JAPAN
Abstract
This paper describes an alternative trans-
lation model based on a text chunk un-
der the framework of statistical machine
translation. The translation model sug-
gested here first performs chunking. Then,
each word in a chunk is translated. Fi-
nally, translated chunks are reordered.
Under this scenario of translation model-
ing, we have experimented on a broad-
coverage Japanese-English traveling cor-
pus and achieved improved performance.
1 Introduction
The framework of statistical machine translation for-
mulates the problem of translating a source sentence
in a language J into a target language E as the
maximization problem of the conditional probability
?E = argmaxE P(E|J). The application of the Bayes
Rule resulted in ?E = argmaxE P(E)P(J|E). The for-
mer term P(E) is called a language model, repre-
senting the likelihood of E. The latter term P(J|E)
is called a translation model, representing the gener-
ation probability from E into J.
As an implementation of P(J|E), the word align-
ment based statistical translation (Brown et al,
1993) has been successfully applied to similar lan-
guage pairs, such as French?English and German?
English, but not to drastically different ones, such
as Japanese?English. This failure has been due to
the limited representation by word alignment and
the weak model structure for handling complicated
word correspondence.
This paper provides a chunk-based statistical
translation as an alternative to the word alignment
based statistical translation. The translation process
inside the translation model is structured as follows.
A source sentence is first chunked, and then each
chunk is translated into target language with local
word alignments. Next, translated chunks are re-
ordered to match the target language constraints.
Based on this scenario, the chunk-based statis-
tical translation model is structured with several
components and trained by a variation of the EM-
algorithm. A translation experiment was carried
out with a decoder based on the left-to-right beam
search. It was observed that the translation quality
improved from 46.5% to 52.1% in BLEU score and
from 59.2% to 65.1% in subjective evaluation.
The next section briefly reviews the word align-
ment based statistical machine translation (Brown et
al., 1993). Section 3 discusses an alternative ap-
proach, a chunk-based translation model, ranging
from its structure to training procedure and decod-
ing algorithm. Then, Section 4 provides experimen-
tal results on Japanese-to-English translation in the
traveling domain, followed by discussion.
2 Word Alignment Based Statistical
Translation
Word alignment based statistical translation rep-
resents bilingual correspondence by the notion of
word alignment A, allowing one-to-many generation
from each source word. Figure 1 illustrates an exam-
ple of English and Japanese sentences, E and J, with
sample word alignments. In this example, ?show1?
has generated two words, ?mise5? and ?tekudasai6?.
E = NULL0 show1 me2 the3 one4 in5 the6 window7
J = uindo1 no2 shinamono3 o4 mise5 tekudasai6
A = ( 7 0 4 0 1 1 )
Figure 1: Example of word alignment
Under this word alignment assumption, the transla-
tion model P(J|E) can be further decomposed with-
out approximation.
P(J|E) =
?
A
P(J, A|E)
2.1 IBM Model
During the generation process from E to J, P(J, A|E)
is assumed to be structured with a couple of pro-
cesses, such as insertion, deletion and reorder. A
scenario for the word alignment based translation
model defined by Brown et al (1993), for instance
IBM Model 4, goes as follows (refer to Figure 2).
1. Choose the number of words to generate for
each source word according to the Fertility
Model. For example, ?show? was increased to
2 words, while ?me? was deleted.
2. Insert NULLs at appropriate positions by the
NULL Generation Model. Two NULLs were
inserted after each ?show? in Figure 2.
3. Translate word-by-word for each generated
word by looking up the Lexicon Model. One of
the two ?show? words was translated to ?mise.?
4. Reorder the translated words by referring to the
Distortion Model. The word ?mise? was re-
ordered to the 5th position, and ?uindo? was
reordered to the 1st position. Positioning is de-
termined by the previous word?s alignment to
capture phrasal constraints.
For the meanings of each symbol in each model, re-
fer to Brown et al (1993).
2.2 Problems of Word Alignment Based
Translation Model
The strategy for the word alignment based transla-
tion model is to translate each word by generating
multiple single words (a bag of words) and to deter-
mine the position of each translated word. Although
show1 show show mise uindo1
me2 show NULL no no2
the3 one show tekudasai shinamono3
one4 window NULL o o4
in5 one shinamono mise5
the6 window uindo tekudasai6
window7
n(2|E1)
n(0|E2)
n(0|E3)
...
Fertility
(4
2
)
p4?20 p
2
1
NULL t(J5 |E1)
t(J6 |E1)
t(J3 |E4)
...
Lexicon
d1(1 ?  31 |E4, J1)
d1(3 ?  5+62 |E1, J3)
d1(5 ?  2+42 |NULL, J5)
d
>1(6 ? 5|J6)
Distortion
Figure 2: Word alignment based translation model
P(J, A|E) (IBM Model 4)
this procedure is sufficient to capture the bilingual
correspondence for similar language pairs, some is-
sues remain for drastically different pairs:
Insertion/Deletion Modeling Although deletion
was modeled in the Fertility Model, it merely as-
signs zero to each deleted word without considering
context. Similarly, inserted words are selected by
the Lexical Model parameter and inserted at the po-
sitions determined by a binomial distribution.
This insertion/deletion scheme contributed to the
simplicity of this representation of the translation
processes, allowing a sophisticated application to
run on an enormous bilingual sentence collection.
However, it is apparent that the weak modeling of
those phenomena will lead to inferior performance
for language pairs such as Japanese and English.
Local Alignment Modeling The IBM Model 4
(and 5) simulates phrasal constraints, although there
were implicitly implemented as its Distortion Model
parameters. In addition, the entire reordering is
determined by a collection of local reorderings in-
sufficient to capture the long-distance phrasal con-
straints.
The next section introduces an alternative model-
ing, chunk-based statistical translation, which was
intended to resolve the above two issues.
3 Chunk-based Statistical Translation
Chunk-based statistical translation models the pro-
cess of chunking for both the source and target sen-
tences, E and J,
P(J|E) =
?
J
?
E
P(J,J ,E|E)
where J and E are the chunked sentences for J
and E, respectively, defined as two-dimentional ar-
E = show1 me2 1 the3 one4 2 in5 the6 window7 3
mise5 tekudasai6 shinamono3 o4 uindo1 no2
J = uindo1 no2
1
shinamono3 o4
2
mise5 tekudasai6
3
A = ( 3 2 1 )
A = ( [ 7, 0 ] [ 4, 0 ] [ 1, 1 ] )
Figure 3: Example of chunk-based alignment
rays. For instance, Ji, j represents the jth word of
the ith chunk. The number of chunks for source
and target is assumed to be equal, |J| = |E|,
so that each chunk can convey a unit of meaning
without added/subtracted information. The term
P(J,J ,E|E) is further decomposed by chunk align-
ment A and word alignment for each chunk transla-
tion A.
P(J,J ,E|E) =
?
A
?
A
P(J,J , A,A,E|E)
The notion of alignment A is the same as those found
in the word alignment based translation model,
which assigns a source chunk index for each target
chunk. A is a two-dimensional array which assigns
a source word index for each target word per chunk.
For example, Figure 3 shows two-level alignments
taken from the example in Figure 1. The target
chunk at position 3, J3, ?mise tekudasai? is aligned
to the first position (A3 = 1), and both the words
?mise? and ?tekudasai? are aligned to the first posi-
tion of the source sentence (A3,1 = 1,A3,2 = 1).
3.1 Translation Model Structure
The term P(J,J , A,A,E|E) is further decomposed
with approximation according to the scenario de-
scribed below (refer to Figure 4).
1. Perform chunking for source sentence E by
P(E|E). For instance, chunks of ?show me? and
?the one? were derived. The process is mod-
eled by two steps:
(a) Selection of chunk size (Head Model).
For each word Ei, assign the chunk size
?i using the head model (?i |Ei). A word
with chunk size more than 0 (?i > 0) is
treated as a head word, otherwise a non-
head (refer to the words in bold in Figure
4).
(b) Associate each non-head word to a head
word (Chunk Model). Each non-head
word Ei is associated to a head word Eh by
the probability ?(c(Eh)|h? i, c(Ei)), where
h is the position of a head word and c(E)
is a function to map a word E to its word
class (i.e. POS). For instance, ?the3? is
associated with the head word ?one4? lo-
cated at 4 ? 3 = +1.
2. Select words to be translated with Deletion and
Fertility Model.
(a) Select the number of head words. For each
head word Eh (?h > 0), choose fertility ?h
according to the Fertility Model ?(?h|Eh).
We assume that the head word must be
translated, therefore ?h > 0. In addition,
one of them is selected as a head word at
target position using a uniform distribu-
tion 1/?h.
(b) Delete some non-head words. For
each non-head word Ei (?i = 0),
delete it according to the Deletion Model
?(di|c(Ei), c(Eh)), where Eh is the head
word in the same chunk and di is 1 if Ei
is deleted, otherwise 0.
3. Insert some words. In Figure 4, NULLs were
inserted for two chunks. For each chunk Ei,
select the number of spurious words ??i by In-
sertion Model ?(??i |c(Eh)), where Eh is the head
word of Ei.
4. Translate word-by-word. Each source word Ei,
including spurious words, is translated to Jj ac-
cording to the Lexicon Model, ?(Jj|Ei).
5. Reorder words. Each word in a chunk is
reordered according to the Reorder Model
P(A j|EA j ,J j). The chunk reordering is taken
after the Distortion Model of IBM Model 4,
where the position is determined by the relative
position from the head word,
P(A j|EA j ,J j) =
|A j |
?
k=1
?(k ? h|c(E
AA j ,k ), c(J j,k))
where h is the position of a head word for the
chunk J j. For example, ?no? is positioned ?1
of ?uindo?.
show1 show show show mise mise uindo1
me2 me show show tekudasai tekudasai no2
the3 the NULL o shinamono shinamono3
one4 one one one shinamono o o4
in5 in mise5
the6 the NULL no uindo tekudasai6
window7 window window window uindo no
Chunking Deletion& Fertility Insertion Lexicon Reorder
Chunk
Reorder
Figure 4: Chunk-based translation model. The words in bold are head words.
6. Reorder chunks. All of the chunks are
reordered according to the Chunk Reorder
Model, P(A|E,J). The chunk reordering is
also similar to the Distortion Model, where the
positioning is determined by the relative posi-
tion from the previous alignment
P(A|E,J) =
|J|
?
j=1

( j ? j?|c(EA j?1,h?), c(J j,h))
where j? is the chunk alignment of the the pre-
vious chunk aEA j?1. h and h? are the head word
indices for J j and EA j?1, respectively. Note
that the reordering is dependent on head words.
To summarize, the chunk-based translation model
can be formulated as
P(J|E) =
?
E,J ,A,A
?
i
(?i|Ei)
?
?
i:?i=0
?(c(Ehi)|hi ? i, c(Ei))
?
?
i:?i>0
?(?i|Ei)/?i
?
?
i:?i=0
?(di |c(Ei), c(Ehi))
?
?
i:?i>0
?(??i |c(Ei)) ?
?
j
?
k
?(J j,k|EA j,k )
?
?
j
P(A j|EA j ,J j) ? P(A|E,J)
.
3.2 Characteristics of chunk-based Translation
Model
The main difference to the word alignment based
translation model is the treatment of the bag of
word translations. The word alignment based trans-
lation model generates a bag of words for each
source word, while the chunk-based model con-
structs a set of target words from a set of source
words. The behavior is modeled as a chunking pro-
cedure by first associating words to the head word
of its chunk and then performing chunk-wise trans-
lation/insertion/deletion.
The complicated word alignment is handled by
the determination of word positions in two stages:
translation of chunk and chunk reordering. The for-
mer structures local orderings while the latter con-
stitutes global orderings. In addition, the concept of
head associated with each chunk plays the central
role in constraining different levels of the reordering
by the relative positions from heads.
3.3 Parameter Estimation
The parameter estimation for the chunk-based trans-
lation model relies on the EM-algorithm (Dempster
et al, 1977). Given a large bilingual corpus the
conditional probability of P(J , A,A,E|J, E) =
P(J,J , A,A,E|E)/?
J ,A,A,E P(J,J , A,A,E|E) is
first estimated for each pair of J and E (E-step),
then each model parameters is computed based
on the estimated conditional probability (M-step).
The above procedure is iterated until the set of
parameters converge.
However, this naive algorithm will suffer from se-
vere computational problems. The enumeration of
all possible chunkings J and E together with word
alignment A and chunk alignment A requires a sig-
nificant amount of computation. Therefore, we have
introduced a variation of the Inside-Outside algo-
rithm as seen in (Yamada and Knight, 2001) for E-
step computation. The details of the procedure are
described in Appendix A.
In addition to the computational problem, there
exists a local-maximum problem, where the EM-
Algorithm converges to a maximum solution but
does not guarantee finding the global maximum. In
order to solve this problem and to make the pa-
rameters converge quickly, IBM Model 4 parame-
ters were used as the initial parameters for training.
We directly applied the Lexicon Model and Fertility
Model to the chunk-based translation model but set
other parameters as uniform.
3.4 Decoding
The decoding algorithm employed for this chunk-
based statistical translation is based on the beam
search algorithm for word alignment statistical
translation presented in (Tillmann and Ney, 2000),
which generates outputs in left-to-right order by
consuming input in an arbitrary order.
The decoder consists of two stages:
1. Generate possible output chunks for all possi-
ble input chunks.
2. Generate hypothesized output by consuming
input chunks in arbitrary order and combining
possible output chunks in left-to-right order.
The generation of possible output chunks is es-
timated through an inverted lexicon model and
sequences of inserted strings (Tillmann and Ney,
2000). In addition, an example-based method is
also introduced, which generates candidate chunks
by looking up the viterbi chunking and alignment
from a training corpus.
Since the combination of all possible chunks is
computationally very expensive, we have introduced
the following pruning and scoring strategies.
beam pruning: Since the search space is enor-
mous, we have set up a size threshold to main-
tain partial hypotheses for both of the above
two stages. We also incorporated a threshold
for scoring, which allows partial hypotheses
with a certain score to be processed.
example-based scoring: Input/output chunk pairs
that appeared in a training corpus are ?re-
warded? so that they are more likely kept in
the beam. During the decoding process, when
a pair of chunks appeared in the first stage, the
score is boosted by using this formula in the log
domain,
log Ptm(J|E) + log Plm(E)
Table 1: Basic Travel Expression Corpus
Japanese English
# of sentences 171,894
# of words 1,181,188 1,009,065
vocabulary size 20472 16232
# of singletons 82,06 5,854
3-gram perplexity 23.7 35.8
+ weight ?
?
j
f req(EA j ,J j)
in which Ptm(J|E) and Plm(E) are translation
model and language model probability, respec-
tively1, f req(EA j ,J j) is the frequency for the
pair EA j and J j appearing in the training cor-
pus, and weight is a tuning parameter.
4 Experiments
The corpus for this experiment was extracted from
the Basic Travel Expression Corpus (BTEC), a col-
lection of conversational travel phrases for Japanese
and English (Takezawa et al, 2002) as seen in Ta-
ble 1. The entire corpus was split into three parts:
152,169 sentences for training, 4,846 sentences for
testing, and the remaining 10,148 sentences for pa-
rameter tuning, such as the termination criteria for
the training iteration and the parameter tuning for
decoders.
Three translation systems were tested for compar-
ison:
model4: Word alignment based translation model,
IBM Model 4 with a beam search decoder.
chunk3: Chunk-based translation model, limiting
the maximum allowed chunk size to 3.
model3+: chunk3 with example-based chunk can-
didate generation.
Figure 5 shows some examples of viterbi chunking
and chunk alignment for chunk3.
Translations were carried out on 510 sentences se-
lected randomly from the test set and evaluated ac-
cording to the following criteria with 16 reference
sets.
WER: Word-error-rate, which penalizes the edit distance
against reference translations.
1For simplicity of notation, dependence on other variables
are omitted, such as J .
[ i * have ] [ the * number ] [ of my * passport ]
[ *?????? e] [ *????? ] [? *???? ]
[ i * have ] [ a * stomach ache ] [ please * give me ] [ some * medicine ]
[??? *?? ] [ *?? ] [ *?? ] [ *??? ]
[ * i ] [ * ?d ] [ * like ] [ a * table ] [ * for ] [ * two ] [ by the * window ] [ * if possible ]
[ *???? ] [?? ] [? *?? ] [ *??? ] [? *????? ] [??? *?? ] [ *??? ] [ *???? ]
[ i ? have ] [ a ? reservation ] [ ? for ] [ two ? nights ] [ my ? name is ] [ ? risa kobayashi ]
[? ?? ] [ ?? ] [??? ?? ] [??? ???? ] [? ???? ] [?? ????? ]
Figure 5: Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model.
Chunks are bracketed and the words with ? to the left are head words.
Table 2: Experimental results for Japanese?English
translation
Model WER PER BLEU SE [%]
[%] [%] [%] A A+B A+B+C
model4 43.3 37.2 46.5 59.2 74.1 80.2
chunk3 40.9 36.1 48.4 59.8 73.5 78.8
chunk3+ 38.5 33.7 52.1 65.1 76.3 80.6
PER: Position independent WER, which penalizes without
considering positional disfluencies.
BLEU: BLEU score, which computes the ratio of n-gram for
the translation results found in reference translations (Pa-
pineni et al, 2002).
SE: Subjective evaluation ranks ranging from A to D
(A:Perfect, B:Fair, C:Acceptable and D:Nonsense),
judged by native speakers.
Table 2 summarizes the evaluation of Japanese-to-
English translations, and Figure 6 presents some of
the results by model4 and chunk3+.
As Table 2 indicates, chunk3 performs better than
model4 in terms of the non-subjective evaluations,
although it scores almost equally in subjective eval-
uations. With the help of example-based decoding,
chunk3+ was evaluated as the best among the three
systems.
5 Discussion
The chunk-based translation model was originally
inspired by transfer-based machine translation but
modeled by chunks in order to capture syntax-based
correspondence. However, the structures evolved
into complicated modeling: The translation model
involves many stages, notably chunking and two
kinds of reordering, word-based and chunk-based
alignments. This is directly reflected in parameter
input: ????????????????
reference: is this all the baggage from flight one five two
model4: is this all you baggage for flight one five two
chunk3: is this all the baggage from flight one five two
input: ?????????????????
reference: may i have room service for breakfast please
model4: please give me some room service please
chunk3: i ?d like room service for breakfast
input: ??????????????????????
reference: hello i ?d like to change my reservation for march nineteenth
model4: i ?d like to change my reservation for ninety days be march hello
chunk3: hello i ?d like to change my reservation on march nineteenth
input: ?????????????????
reference: wait a couple of minutes i ?m telephoning now
model4: is this the line is busy now a few minutes
chunk3: i ?m on another phone now please wait a couple of minutes
Figure 6: Translation examples by word alignment
based model and chunk-based model
estimation, where chunk3 took 20 days for 40 iter-
ations, which is roughly the same amount of time
required for training IBM Model 5 with pegging.
The unit of chunk in the statistical machine
translation framework has been extensively dis-
cussed in the literature. Och et al (1999) pro-
posed a translation template approach that com-
putes phrasal mappings from the viterbi align-
ments of a training corpus. Watanabe et al (2002)
used syntax-based phrase alignment to obtain
chunks. Marcu and Wong (2002) argued for a dif-
ferent phrase-based translation modeling that di-
rectly induces a phrase-by-phrase lexicon model
from word-wise data. All of these methods bias
the training and/or decoding with phrase-level ex-
amples obtained by preprocessing a corpus (Och et
al., 1999; Watanabe et al, 2002) or by allowing a
lexicon model to hold phrases (Marcu and Wong,
2002). On the other hand, the chunk-based transla-
tion model holds the knowledge of how to construct
a sequence of chunks from a sequence of words. The
former approach is suitable for inputs with less de-
viation from a training corpus, while the latter ap-
proach will be able to perform well on unseen word
sequences, although chunk-based examples are also
useful for decoding to overcome the limited context
of a n-gram based language model.
Wang (1998) presented a different chunk-based
method by treating the translation model as a phrase-
to-string process. Yamada and Knight (2001) fur-
ther extended the model to a syntax-to-string trans-
lation modeling. Both assume that the source part
of a translation model is structured either with a se-
quence of chunks or with a parse tree, while our
method directly models a string-to-string procedure.
It is clear that the string-to-string modeling with hi-
den chunk-layers is computationally more expensive
than those structure-to-string models. However, the
structure-to-string approaches are already biased by
a monolingual chunking or parsing, which, in turn,
might not be able to uncover the bilingual phrasal or
syntactical constraints often observed in a corpus.
Alshawi et al (2000) also presented a two-level
arranged word ordering and chunk ordering by a hi-
erarchically organized collection of finite state trans-
ducers. The main difference from our work is that
their approach is basically deterministic, while the
chunk-based translation model is non-deterministic.
The former method, of course, performs more ef-
ficient decoding but requires stronger heuristics to
generate a set of transducers. Although the latter
approach demands a large amount of decoding time
and hypothesis space, it can operate on a very broad-
coverage corpus with appropriate translation model-
ing.
Acknowledgments
The research reported here was supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled ?A study of
speech dialogue translation technology based on a
large corpus?.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
A. P. Dempster, N.M. Laird, and D.B.Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
B(39):1?38.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of EMNLP-2002, Philadelphia, PA, July.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of EMNLP/WVLC, Univer-
sity of Maryland, College Park, MD, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proc. of LREC 2002, pages 147?152, Las Palmas, Ca-
nary Islands, Spain, May.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and dp-based search in statistical machine
translation. In Proc. of the COLING 2000, July-
August.
Ye-Yi Wang. 1998. Grammar Inference and Statis-
tical Machine Translation. Ph.D. thesis, School of
Computer Science, Language Technologies Institute,
Carnegie Mellon University.
Taro Watanabe, Kenji Imamura, and Eiichiro Sumita.
2002. Statistical machine translation based on hierar-
chical phrase alignment. In Proc. of TMI 2002, pages
188?198, Keihanna, Japan, March.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL 2001,
Toulouse, France.
Appendix A Inside-Outside Algorithm for
Chunk-based Translation
Model
The basic idea of inside-outside computation is to
separate the whole process into two parts, chunk
translation and chunk reordering. Chunk transla-
tion handles translation of each chunk, while chunk
reordering performs chunking and chunk reoprder-
ing. The inside (backward or beta) probabilities
can be derived, which represent the probability of
source/target paring of chunks and sentences. The
outside (forward or alpha) probabilities can be de-
fined as the probability of a particular source and
target pair appearing at a particular chunking and re-
ordering.
Inside Probability First, given E and J, compute
chunk translation inside probabilities for all the pos-
sible source and target chunks pairing Ei?i and J
j?
j in
which Ei?i is the chunk ranging from index i to i
?
,
?(Ei?i , J j
?
j ) =
?
A?
P(A, J j?j |Ei
?
i )
=
?
A?
?
?
P
?
(A?, J j?j , Ei
?
i )
where P
?
is the probability of a model with asso-
ciated values for corresponding random variables,
such as (?i|Ei) or ?(Jj|Ei), except for the chunk re-
order model 
. A? is a word alignment for the chunks
Ei?i and J
j?
j .
Second, compute the inside probability for sen-
tence pairs E and J by considering all possible
chunkings and chunk alignments.
?(E, J) =
?
E,J:|E|=|J|
?
A
P(A,E,J , J|E)
=
?
E,J:|E|=|J|
?
A
P(A|E,J)
?
j
?(EA j ,J j)
Outside Probability The outside probability for
sentence pairing is always 1.
?(E, J) = 1.0
The outside probabilities for each chunk pair is
?(Ei?i , J j
?
j ) = ?(E, J)
?
E,J:|E|=|J|
?
A
P(A|E,J)
?
?
EAkE
i?
i ,JkJ
j?
j
?(EAk ,Jk)
.
Inside-Outside Computation The combination
of the above inside-outside probabilities yields the
following formulas for the accumulated counts of
pair occurrences.
First, the counts for each model parameter ? with
associated random variables count
?
(?) is
count
?
(?) =
?
<E,J>
?
?(A?,Ei?i ,J
j?
j )
?(Ei?i , J j
?
j )/?(E, J)
?
?
?
?
P
?
?(A?, J j?j , Ei
?
i )
.
Second, the count for chunk reordering with asso-
ciated random variables count


(?) is
count


(?) =
?
<E,J>
?(E, J)/?(E, J)
?
?(A,E,J)
P


(A|E,J)
?
k
?(EAk ,Jk)
.
Approximation Even with the introduction of the
inside-outside parameter estimation paradigm, the
enumeration of all possible chunk pairing and word
alignment requires O(lmk4(k + 1)k) computations,
where l and m are sentence length for E and J, re-
spectively, and k is the maximum allowed number
of words per chunk. In addition, the enumeration
of all possible alignments for all possible chunked
sentences is O(2l2mn!), where n = |J| = |E|.
In order to handle the massive amount of compu-
tational demand, we have applied an approximation
to the inside-outside estimation procedure. First,
the enumeration of word alignment computation for
chunk translations was approximated by a set of
alignments, the viterbi alignment and neighboring
alignment through move/swap operations of partic-
ular word alignments.
Second, the chunk alignment enumeration was
also approximated by a set of chunking and chunk
alignments as follows.
1. Determines the number of chunks per sentence
2. Determine initial chunking and alignment
3. Compute viterbi chunking-alignment via hill-climbing
using the following operators
? Move boundary of chunk
? Swap chunk alignment
? Move head position
4. Compute neighboring chunking-alignment using the
above operators
Bidirectional Decoding for Statistical Machine Translation
Taro WATANABE ?? and Eiichiro SUMITA ?
{taro.watanabe, eiichiro.sumita}@atr.co.jp
? ATR Spoken Language Translation Research Laboratories ? Department of Information Science
2-2-2 Hikaridai Seika-cho, Kyoto University
Soraku-gun, Kyoto 619-0288 JAPAN Sakyo-ku, Kyoto 606-8501, JAPAN
Abstract
This paper describes the right-to-left decoding
method, which translates an input string by gen-
erating in right-to-left direction. In addition, pre-
sented is the bidirectional decoding method, that
can take both of the advantages of left-to-right and
right-to-left decoding method by generating output
in both ways and by merging hypothesized partial
outputs of two directions. The experimental results
on Japanese and English translation showed that
the right-to-left was better for Englith-to-Japanese
translation, while the left-to-right was suitable for
Japanese-to-English translation. It was also ob-
served that the bidirectional method was better for
English-to-Japanese translation.
1 Introduction
The statistical approach to machine translation re-
gards the machine translation problem as the maxi-
mum likelihood solution of a translation target text
given a translation source text. According to the
Bayes Rule, the problem is transformed into the
noisy channel model paradigm, where the transla-
tion is the maximum a posteriori solution of a dis-
tribution for a channel target text given a channel
source text and a prior distribution for the channel
source text (Brown et al, 1993).
Although there exists efficient algorithms to es-
timate the parameters for the statistical machine
translation (SMT), one of the problems of SMT is
the search algorithms for the translation given a se-
quence of words. There exists stack decoding al-
gorithm (Berger et al, 1996), A* search algorithm
(Och et al, 2001; Wang and Waibel, 1997) and
dynamic-programming algorithms (Tillmann and
Ney, 2000; Garcia-Varea and Casacuberta, 2001),
and all translate a given input string word-by-word
and render the translation in left-to-right, with prun-
ing technologies assuming almost linearly aligned
translation source and target texts. The algorithms
proposed above cannot deal with drastically differ-
ent word correspondence, such as Japanese and En-
glish translation, where Japanese is SOV while SVO
in English. Germann et al (2001) suggested greedy
method and integer programming decoding, though
the first method suffer from the similar problem as
described above and the second is impractical for
the real-world application.
This paper presents two decoding methods, one
is the right-to-left decoding based on the left-to-
right beam search algorithm, which generates out-
puts from the end of a sentence. The second one is
the bidirectional decoding method which decodes in
both of the left-to-right and right-to-left directions
and merges the two hypothesized partial sentences
into one. The experimental results of Japanese and
English translation indicated that the right-to-left
decoding was better for English-to-Japanese trans-
lation, while the left-to-right decoding was better
for Japanese-to-English decoding. The above re-
sults could be justified by the structural difference
of Japanese and English, where English takes the
prefix structure that places emphasis at the begin-
ning of a sentence, hence prefers left-to-right de-
coding. On the other hand, Japanese takes postfix
structure, setting attention around the end of a sen-
tence, therefore favors right-to-left decoding. The
bidirectional decoding, which can take both of the
benefits of decoding method, was superior to mono-
directional decoding methods.
The next section briefly describes the SMT fo-
cusing on the IBM Model 4. Then, the Section 3
presents decoding algorithms in three direction, left-
to-right, right-to-left and bi-direction. The Section
4 presents the results of Japanese and English trans-
lation followed by discussions.
2 Statistical Machine Translation
Statistical machine translation regards machine
translation as a process of translating a source lan-
NULL0 could1 you2 recommend3 another4 hotel5
hoka no hoteru o shokaishi teitadake masu ka
a = (4, 4, 5, 0, 3, 1, 1, 0)
Figure 1: An example of alignment for Japanese and English sentences
guage text (f) into a target language text (e) with the
following formula:
e = arg max
e
P(e|f)
The Bayes Rule is applied to the above to derive:
e = arg max
e
P(f|e)P(e)
The translation process is treated as a noisy chan-
nel model, like those used in speech recognition in
which there exists e transcribed as f, and a trans-
lation is to infer the best e from f in terms of
P(f|e)P(e). The former term, P(f|e), is a translation
model representing some correspondence between
bilingual text. The latter, P(e), is the language
model denoting the likelihood of the channel source
text. In addition, a word correspondence model,
called alignment a, is introduced to the translation
model to represent a positional correspondence of
the channel target and source words:
e = arg max
e
?
a
P(f, a|e)P(e)
An example of an alignment is shown in Figure 1,
where the English sentence ?could you recommend
another hotel? is mapped onto the Japanese ?hoka
no hoteru o shokaishi teitadake masu ka?, and both
?hoka? and ?no? are aligned to ?another?, etc. The
NULL symbol at index 0 is also a lexical entry in
which no morpheme is aligned from the channel
target morpheme, such as ?masu? and ?ka? in this
Japanese example.
2.1 IBM Model 4
The IBM Model 4, main focus in this paper, is com-
posed of the following models (see Figure 2):
? Lexical Model ? t( f |e) : Word-for-word trans-
lation model, representing the probability of a
source word f being translated into a target
word e.
? Fertility Model ? n(?|e) : Representing the
probability of a source word e generating ?
words.
? Distortion Model ? d : The probability of dis-
tortion. In Model 4, the model is decomposed
into two sets of parameters:
? d1( j ? c?i|A(ei),B( f j)) : Distortion prob-
ability for head words. The head word
is the first of the target words generated
from a source word a cept, that is the
channel source word with fertility more
than and equal to one. The head word po-
sition j is determined by the word classes
of the previous source word, A(ei), and
target word, B( f j), relative to the centroid
of the previous source word, c?i .
? d>1( j ? j?|B( f j)) : Distortion probabil-
ity for non-head words. The position of
a non-head word j is determined by the
word class and relative to the previous tar-
get word generated from the cept ( j?).
? NULL Translation Model ? p1 : A fixed prob-
ability of inserting a NULL word after deter-
mining each target word f .
For details, refer to Brown et al (1993).
2.2 Search Problem
The search problem of statistical machine trans-
lation is to induce the maximum likely channel
source sequence, e, given f and the model, P(f|e) =
?
a P(f, a|e) and P(e). For the space of a is ex-
tremely large, |a|l+1, where the l is the output length,
an approximation of P(f|e) ' P(f, a|e) is used when
exploring the possible candidates of translation.
This problem is known to be NP-Complete
(Knight, 1999), for the re-ordering property in the
model further complicates the search. One of the
solution is the left-to-right generation of output by
consuming input words in any-order. Under this
constraint, many researchers had contributed algo-
rithms and associated pruning strategies, such as
Berger et al (1996), Och et al (2001), Wang and
Waibel (1997), Tillmann and Ney (2000) Garcia-
Varea and Casacuberta (2001) and Germann et al
(2001), though they all based on almost linearly
Translation Model
Lexical Model
?
t( f j|ei)
Fertility Model
?
n(?i |ei)
Distortion Model
Head ? d1( j ? c?i|A(e?i )B( f j))
Non-Head ? d1>( j ? j?|B( f j))
NULL Translation Model
(m??0
?0
)
pm?2?00 p
?0
1
Figure 2: Translation Model (IBM Model 4)
aligned language pairs, and not suitable for lan-
guage pairs with totally different alignment corre-
spondence, such as Japanese and English.
3 Decoding Algorithms
The decoding methods presented in this paper ex-
plore the partial candidate translation hypotheses
greedily, as presented in Tillmann and Ney (2000)
and Och et al (2001), and operation applied to each
hypothesis is similar to those explained in Berger
et al (1996), Och et al (2001) and Germann et
al. (2001). The algorithm is depicted in Algorithm
1 where C = { jk : k = 1...|C|} represents a set
of input string position 1. The algorithm assumes
two kinds of partial hypotheses2, translated partially
from an input string, one is an open hypothesis that
can be extended by raising the fertility. The other
is a close hypothesis that is to be extended by in-
serting a string e? to the hypothesis. The e? is a se-
quence of output word, consisting of a word with the
fertility more than one (translation of f j) and other
words with zero fertility. The translation of f j can
be computed either by inverse translation table (Och
et al, 2001; Al-Onaizan et al, 1999). The list of
zero fertility words can be obtained from the viterbi
alignment of training corpus (Germann et al, 2001).
The extension operator applied to an open hypothe-
sis (e,C) is:
? align j to ei ? this creates a new hypothesis
by raising the fertility of ei by consuming the
input word f j. The generated hypothesis can
be treated as either closed or open, that means
to stop raising the fertility or raise the fertility
further more.
The operators applied to a close hypothesis are:
1For simplicity, the dependence of alignment, a is omitted.
2There exist a complete hypothesis, that is a candidate of
translation.
Algorithm 1 Beam Decoding Search
input source string: f1 f2... fm
for all cardinality c = 0, 1, ...m ? 1 do
for all (e,C) where |C| = c do
for all j = 1, ...m and j < C do
if (e,C) is open then
align j to ei and keep it open
align j to ei and close it
else
align j to NULL
insert e?, align from j and open it
insert e?, align from j and close it
end if
end for
end for
end for
? align j to NULL ? raise the fertility for the
NULL word.
? insert e?, align from j ? this operator insert a
string e? and align one input word f j to one of
the word in e?. After this operation, the new
hypothesis can be regarded as either open or
closed.
Pruning is inevitable in the process of decoding,
and applied is the beam search pruning, in which the
maximum number of hypotheses to be considered
is limited. In addition, fertility pruning is also in-
troduced which suppress the word with large num-
ber of fertility. The skipping based criteria, such as
introduced by Och et al (2001), is not appropri-
ate for the language pairs with drastically different
alignment, such as Japanese and English, hence was
not considered in this paper. Depending on the out-
put generation direction, the algorithm can generate
either in left-to-right or right-to-left, by alternating
some constraints of insertion of output words.
e1 ... el e?1 ... e?l?
f1 f2 ... f j ... fm
e e?
Figure 3: string insertion operator for left-to-right
decoding method. A string e? was appended after
the partial output string, e, and the last word in e?
was aligned from f j.
e?1 ... e?l? e1 ... el
f1 f2 ... f j ... fm
e? e
Figure 4: string insertion operation for right-to-left
decoding method. A string e? was prepended before
the partial output string, e, and the first word in e?
was aligned from f j.
3.1 Left-to-Right Decoding
The left-to-right decoding enforces the restriction
where the insertion of e? is allowed after the par-
tially generated e, and alignment from the input
word f j is restricted to the end of the word of e?.
Hence, the operator applied to an open hypothesis
raise the fertility for the word at the end of e (refer
to Figure 3).
The language which place emphasis around the
beginning of a sentence, such as English, will be
suitable in this direction, for the Language Model
score P(e) can estimate what should come first.
Hence, the decoder can discriminate a hypothesis
better or not.
3.2 Right-to-Left Decoding
The right-to-left decoding does the reverse of the
left-to-right decoding, in which the insertion of e?
is allowed only before the e and the f j is aligned
to the beginning of the word of e? (see Figure 4).
Therefore, the open hypothesis is extended by rais-
ing the fertility of the beginning of the word of e. In
prepending a string to a partial hypothesis, an align-
ment vector should be reassigned so that the values
can point out correct index.
Again, the right-to-left direction is suitable for
the language which enforces stronger constraints at
the end of sentence, such as Japanese, similar to the
reason mentioned above.
e f 1 ... ei ... eblbef eb
(a) merging two open hy-
potheses
e f 1 ... e f l f e
? eb1 ... eblbef eb
(b) merging two close hypotheses with in-
serted e?
Figure 5: Merging left-to-right and right-to-left
hypotheses (ef and eb) in bidirectional decoding
method. Figure 5(a) merge two open hypotheses,
while Figure 5(b) merge them with inserted zero fer-
tility words.
3.3 Bidirectional Decoding
The bidirectional decoding decode the input words
in both direction, one with left-to-right decoding
method up to the cardinality of dm/2e and right-to-
left direction up to the cardinality of bm/2c, where
m is the input length. Then, the two hypotheses are
merged when both are open and can share the same
output word e, which resulted in raising the fertility
of e. If both of them are closed hypotheses, then
an additional sequence of zero fertility words (or
NULL sequence) are inserted (refer to Figure 5).
3.4 Computational Complexity
The computational complexity for the left-to-right
and right-to-left is the same, O(|E|3m22m), as re-
ported by Tillmann and Ney (2000), in which |E|
is the size of the vocabulary for output sentences 3.
The bidirectional method involves merging of two
hypotheses, hence additional O(
( m
m/2
)
) is required.
3.5 Effects of Decoding Direction
The decoding algorithm generating in left-to-right
direction fills the output sequence from the begin-
ning of a sentence by consuming the input words in
any order and by selecting the corresponding trans-
lation.
Therefore, the languages with prefix structure,
such as English, German or French, can take the
benefits of this direction, because the language
model/translation model can differentiate ?good?
hypotheses to ?bad? hypotheses around the begin-
ning of the output sentences. Therefore, the nar-
rowing the search space by the beam search crite-
3The term |E|3 is the case for trigram language model.
ria (pruning) would not affect the overall quality.
On the other hand, if right-to-left decoding method
were applied to such a language above, the dif-
ference of good hypotheses and bad hypotheses is
small, hence the drop of hypotheses would affect the
quality of translation.
The similar statement can hold for postfix lan-
guages, such as Japanese, where emphasis is placed
around the end of a sentence. For such languages,
right-to-left decoding will be suitable but left-to-
right decoding will degrade the quality of transla-
tion.
The bidirectional decoding is expected to take the
benefits of both of the directions, and will show the
best results in any kind of languages.
4 Experimental Results
The corpus for this experiment consists of 172,481
bilingual sentences of English and Japanese ex-
tracted from a large-scale travel conversation corpus
(Takezawa et al, 2002). The statistics of the corpus
are shown in Table 1. The database was split into
three parts: a training set of 152,183 sentence pairs,
a validation set of 10,148, and a test set of 10,150.
The translation models, both for the Japanese-to-
English (J-E) and English-to-Japanese (E-J) trans-
lation, were trained toward IBM Model 4 on the
training set and cross-validated on validation set to
terminate the iteration by observing perplexity. In
modeling IBM Model 4, POSs were used as word
classes.
From the viterbi alignments of the training cor-
pus, A list of possible insertion of zero fertility
words were extracted with frequency more than 10,
around 1,300 sequences of words for both of the J-
E and E-J translations. The test set consists of 150
Japanese sentences varying by the sentence length
of 6, 8 and 10. The translation was carried out
by three decoding methods:left-to-right, right-to-
left and bidirectional one.
The translation results were evaluated by word-
error-rate (WER) and position independent word-
error-rate (PER) (Watanabe et al, 2002; Och et al,
2001). The WER is the measure by penalizing in-
sertion/deletion/replacement by 1. The PER is the
one similar to WER but ignores the positions, al-
lowing the reordered outputs, hence can estimate the
accuracy for the tranlslation word selection. It has
been also evaluated by subjective evaluation (SE)
with the criteria ranging from A(perfect) to D(non-
Table 1: Statistics on a travel conversation corpus
Japanese English
# of sentences 172,481
# of words 1,186,620 1,005,080
vocabulary size 22,801 15,768
avg. sentence length 6.88 5.83
3-gram perplexity 26.16 36.92
Table 3: Comparison of the three decoders by the
ratio each decoder produced search errors.
J-E E-J
LtoR 11.3 12.0
RtoL 59.3 34.0
Bi 15.3 15.3
sense) 4 (Sumita et al, 1999).
Table 2 summarizes the results of decoding by
left-to-right, right-to-left and bidirectional method
evaluated with WER, PER and SE. Table 3 shows
the ratio of producing search errors, computed by
comparing the translation model and lnguage model
scores for the outputs from three decoding methods.
Sample Japanese-to-English translations performed
by the decoders is presented in Figure 6.
5 Discussions
From Table 2, the left-to-right decoding method per-
formed better than the right-to-left one in Japanese-
to-English translation as expected in Section 3.5.
Furthermore, the bidirectional decoding method
was slightly better than the left-to-right one, for it
could combine the benefits of both directions.
Similar analysis could hold for English-to-
Japanese translation, and the right-to-left decoding
method was slightly superior to the left-to-right one
in terms of WER/PER scores, though the SE score
dropped from 8.7% to 6.7% in C-ranked sentences.
Overall quality measured by the SE rate for ac-
cepted senteces, ranging from A to C, dropped from
68.0% into 66.0%. In addition, the bidirectional
method in English-to-Japanese translation was not
evaluated as high as those in Japanese-to-English
translation: the results were closer to the left-to-
right method. This might be due to the nature of lan-
4The meanings of the symbol are follows: A ? perfect:
no problem in either information or grammar; B ? fair: easy
to understand but some important information is missing or it
is grammatically flawed; C ? acceptable: broken but under-
standable with effort; D ? nonsense: important information
has been translated incorrectly.
Table 2: Summary of results for Japanese-to-English (J-E) and English-to-Japanese (E-J) translations by
left-to-right (LtoR), right-to-left (RtoL) and bidirectional (Bi) decoding methods.
Trans. Alg. WER PER SE
A B C D
J-E LtoR 70.0 64.8 26.7% 23.3% 20.0% 30.0%
RtoL 74.6 66.9 21.3% 24.7% 18.0% 36.0%
Bi 69.9 63.7 27.3% 22.7% 20.7% 29.3%
E-J LtoR 66.2 57.6 49.3% 10.0% 8.7% 32.0%
RtoL 64.0 56.1 49.3% 10.0% 6.7% 34.0%
Bi 66.0 58.0 48.7% 8.0% 10.0% 33.3%
input: suri ni saifu o sura re mashi ta
(i had my pocket picked)
LtoR: here ?s my wallet was stolen
RtoL: here ?s my wallet was stolen
Bi: i had my wallet stolen
input: sumimasen ga terasu no seki ga ii no desu ga
(excuse me but can we have a table on the terrace)
LtoR: excuse me i ?d like a seat on the terrace
RtoL: i ?d prefer excuse me
Bi: i ?d like a seat on the terrace
input: nan ji ni owaru no desu
(what time will it be over)
LtoR: what time should i be at the end
RtoL: it ?s what time will it be over
Bi : at what time is it end
input: nimotsu o ue ni age te morae masu ka
(will you put my luggage on the rack)
LtoR: could you put my baggage here
RtoL: do you have overhead luggage
Bi: could you put my baggage
input: ee ani to imouto ga hitori zutsu i masu
(yes i have a brother and a sister)
LtoR: yes brother and sister there a daughter
RtoL: you ?re yes brother and sister daughter
Bi: yes my daughter is there a brother and sister
Figure 6: Examples of Japanese-to-English translation
guage model employed for this experiment, for the
language model probabilities were assigned based
on the left history, not the right history. It is ex-
pected that the use of the suitable language model
context direction corresponding to a generation di-
rection would assign appropriate probability, hence
would be able to differentiate better hypotheses.
Table 3 indicats that the right-to-left decoding
method produced more errors than other methods
regardless of translaiton directions. This is ex-
plained by the use of the left history language
model, not the right context one, as stated above.
Nevertheless, the search error decreased from 59.3
into 34.0 by alternating the translation direction for
the right-to-left decoding method, which still sup-
ports the use of the correct rendering direction for
translation target language.
6 Conclusion
The decoding methods for statistical machine trans-
lation presented here varies the output directions,
left-to-right, right-to-left and bi-direction, and were
experimented with drastically different language
pairs, English and Japanese. The results indicated
that the left-to-right decoding method was suit-
able for Japanese-to-English translation while the
right-to-left decoding method fit with English-to-
Japanese translation. In addition, the bidirectional
decoding method was superior to mono-directional
decoding method for Japanese-to-English transla-
tion. This suggests that the translation output gen-
eration should match with the underlying linguistic
structure for the output language.
Acknowledgement
The research reported here was supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled, ?A study of
speech dialogue translation technology based on a
large corpus?.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Frantz-
Josef Och, David Purdy, Noah A. Smith, and
David Yarowsky. 1999. Statistical machine
translation final report, jhu workshop 1999, 12.
A. Berger, P. Brown, S. Pietra, V. Pietra, J. Gillett,
A. Kehler, and R. Mercer. 1996. Language trans-
lation apparatus and method of using context-
based translation models. Technical report,
United States Patent, Patent Number 5510981,
April.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Ismael Garcia-Varea and Francisco Casacuberta.
2001. Search algorithms for statistical machine
translation based on dynamic programming and
pruning techniques. In MT Summit VIII, Santiago
de Compostela, Galicia, Spain, september.
Ulrich Germann, Michael Jahr, Kevin Knight,
Daniel Marcu, and Kenji Yamada. 2001. Fast de-
coding and optimal decoding for machine trans-
lation. In Proc. of ACL-01, Toulouse, France.
Kevin Knight. 1999. Decoding complexity in
word-replacement translation models. Computa-
tional Linguistics, 25(4):607?615.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient a* search algorithm for statis-
tical machine translation. In Proc. of the ACL-
2001 Workshop on Data-Driven Machine Trans-
lation, pages 55?62, Toulouse, France, July.
Eiichiro Sumita, Setsuo Yamada, Kazuhide Ya-
mamoto, Michael Paul, Hideki Kashioka, Kai
Ishikawa, and Satoshi Shirai. 1999. Solutions
to problems inherent in spoken-language transla-
tion: The ATR-MATRIX approach. In Machine
Translation Summit VII, pages 229?235.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki
Sugaya, Hirofumi Yamamoto, and Seiichi Ya-
mamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel con-
versations in the real world. In Proc. of LREC
2002, pages 147?152, Las Palmas, Canary Is-
lands, Spain, May.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and dp-based search in statistical ma-
chine translation. In Proc. of the COLING 2000,
July-August.
Ye-Yi Wang and Alex Waibel. 1997. Decoding al-
gorithm in statistical machine translation. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Taro Watanabe, Kenji Imamura, and Eiichiro
Sumita. 2002. Statistical machine translation
based on hierarchical phrase alignment. In Proc.
of TMI 2002, Keihanna, Japan, March.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 764?773, Prague, June 2007. c?2007 Association for Computational Linguistics
Online Large-Margin Training for Statistical Machine Translation
Taro Watanabe Jun Suzuki Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{taro,jun,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
We achieved a state of the art performance
in statistical machine translation by using
a large number of features with an online
large-margin training algorithm. The mil-
lions of parameters were tuned only on a
small development set consisting of less than
1K sentences. Experiments on Arabic-to-
English translation indicated that a model
trained with sparse binary features outper-
formed a conventional SMT system with a
small number of features.
1 Introduction
The recent advances in statistical machine transla-
tion have been achieved by discriminatively train-
ing a small number of real-valued features based ei-
ther on (hierarchical) phrase-based translation (Och
and Ney, 2004; Koehn et al, 2003; Chiang, 2005) or
syntax-based translation (Galley et al, 2006). How-
ever, it does not scale well with a large number of
features of the order of millions.
Tillmann and Zhang (2006), Liang et al (2006)
and Bangalore et al (2006) introduced sparse binary
features for statistical machine translation trained on
a large training corpus. In this framework, the prob-
lem of translation is regarded as a sequential labeling
problem, in the same way as part-of-speech tagging,
chunking or shallow parsing. However, the use of a
large number of features did not provide any signifi-
cant improvements over a conventional small feature
set.
Bangalore et al (2006) trained the lexical choice
model by using Conditional Random Fields (CRF)
realized on a WFST. Their modeling was reduced to
Maximum Entropy Markov Model (MEMM) to han-
dle a large number of features which, in turn, faced
the labeling bias problem (Lafferty et al, 2001).
Tillmann and Zhang (2006) trained their feature set
using an online discriminative algorithm. Since the
decoding is still expensive, their online training ap-
proach is approximated by enlarging a merged k-
best list one-by-one with a 1-best output. Liang
et al (2006) introduced an averaged perceptron al-
gorithm, but employed only 1-best translation. In
Watanabe et al (2006a), binary features were trained
only on a small development set using a variant of
voted perceptron for reranking k-best translations.
Thus, the improvement is merely relative to the
baseline translation system, namely whether or not
there is a good translation in their k-best.
We present a method to estimate a large num-
ber of parameters ? of the order of millions ?
using an online training algorithm. Although it
was intuitively considered to be prone to overfit-
ting, training on a small development set ? less
than 1K sentences ? was sufficient to achieve im-
proved performance. In this method, each train-
ing sentence is decoded and weights are updated at
every iteration (Liang et al, 2006). When updat-
ing model parameters, we employ a memorization-
variant of a local updating strategy (Liang et al,
2006) in which parameters are optimized toward
a set of good translations found in the k-best list
across iterations. The objective function is an ap-
proximated BLEU (Watanabe et al, 2006a) that
scales the loss of a sentence BLEU to a document-
wise loss. The parameters are trained using the
764
Margin Infused Relaxed Algorithm (MIRA) (Cram-
mer et al, 2006). MIRA is successfully employed
in dependency parsing (McDonald et al, 2005) or
the joint-labeling/chunking task (Shimizu and Haas,
2006). Experiments were carried out on an Arabic-
to-English translation task, and we achieved signif-
icant improvements over conventional minimum er-
ror training with a small number of features.
This paper is organized as follows: First, Sec-
tion 2 introduces the framework of statistical ma-
chine translation. As a baseline SMT system, we
use the hierarchical phrase-based translation with
an efficient left-to-right generation (Watanabe et al,
2006b) originally proposed by Chiang (2005). In
Section 3, a set of binary sparse features are defined
including numeric features for our baseline system.
Section 4 introduces an online large-margin training
algorithm using MIRA with our key components.
The experiments are presented in Section 5 followed
by discussion in Section 6.
2 Statistical Machine Translation
We use a log-linear approach (Och, 2003) in which
a foreign language sentence f is translated into an-
other language, for example English, e, by seeking a
maximum solution:
e? = argmax
e
wT ? h( f , e) (1)
where h( f , e) is a large-dimension feature vector. w
is a weight vector that scales the contribution from
each feature. Each feature can take any real value,
such as the log of the n-gram language model to
represent fluency, or a lexicon model to capture the
word or phrase-wise correspondence.
2.1 Hierarchical Phrase-based SMT
Chiang (2005) introduced the hierarchical phrase-
based translation approach, in which non-terminals
are embedded in each phrase. A translation is gener-
ated by hierarchically combining phrases using the
non-terminals. Such a quasi-syntactic structure can
naturally capture the reordering of phrases that is not
directly modeled by a conventional phrase-based ap-
proach (Koehn et al, 2003). The non-terminal em-
bedded phrases are learned from a bilingual corpus
without a linguistically motivated syntactic struc-
ture.
Based on hierarchical phrase-based modeling, we
adopted the left-to-right target generation method
(Watanabe et al, 2006b). This method is able to
generate translations efficiently, first, by simplifying
the grammar so that the target side takes a phrase-
prefixed form, namely a target normalized form.
Second, a translation is generated in a left-to-right
manner, similar to the phrase-based approach using
Earley-style top-down parsing on the source side.
Coupled with the target normalized form, n-gram
language models are efficiently integrated during the
search even with a higher order of n.
2.2 Target Normalized Form
In Chiang (2005), each production rule is restricted
to a rank-2 or binarized form in which each rule con-
tains at most two non-terminals. The target normal-
ized form (Watanabe et al, 2006b) further imposes
a constraint whereby the target side of the aligned
right-hand side is restricted to a Greibach Normal
Form like structure:
X ?
?
?, ?b?,?
?
(2)
where X is a non-terminal, ? is a source side string of
arbitrary terminals and/or non-terminals. ?b? is a cor-
responding target side where ?b is a string of termi-
nals, or a phrase, and ? is a (possibly empty) string
of non-terminals. ? defines one-to-one mapping be-
tween non-terminals in ? and ?. The use of phrase
?b as a prefix maintains the strength of the phrase-
base framework. A contiguous English side with a
(possibly) discontiguous foreign language side pre-
serves phrase-bounded local word reordering. At
the same time, the target normalized framework still
combines phrases hierarchically in a restricted man-
ner.
2.3 Left-to-Right Target Generation
Decoding is performed by parsing on the source side
and by combining the projected target side. We
applied an Earley-style top-down parsing approach
(Wu and Wong, 1998; Watanabe et al, 2006b; Zoll-
mann and Venugopal, 2006). The basic idea is
to perform top-down parsing so that the projected
target side is generated in a left-to-right manner.
The search is guided with a push-down automaton,
which keeps track of the span of uncovered source
765
word positions. Combined with the rest-cost esti-
mation aggregated in a bottom-up way, our decoder
efficiently searches for the most likely translation.
The use of a target normalized form further sim-
plifies the decoding procedure. Since the rule form
does not allow any holes for the target side, the inte-
gration with an n-gram language model is straight-
forward: the prefixed phrases are simply concate-
nated and intersected with n-gram.
3 Features
3.1 Baseline Features
The hierarchical phrase-based translation system
employs standard numeric value features:
? n-gram language model to capture the fluency
of the target side.
? Hierarchical phrase translation probabilities in
both directions, h(?|?b?) and h(?b?|?), estimated
by relative counts, count(?, ?b?).
? Word-based lexically weighted models of
hlex(?|?b?) and hlex(?b?|?) using lexical transla-
tion models.
? Word-based insertion/deletion penalties that
penalize through the low probabilities of the
lexical translation models (Bender et al, 2004).
? Word/hierarchical-phrase length penalties.
? Backtrack-based penalties inspired by the dis-
tortion penalties in phrase-based modeling
(Watanabe et al, 2006b).
3.2 Sparse Features
In addition to the baseline features, a large number
of binary features are integrated in our MT system.
We may use any binary features, such as
h( f , e) =
?
?
?
?
?
?
?
?
?
1 English word ?violate? and Arabic
word ?tnthk? appeared in e and f .
0 otherwise.
The features are designed by considering the decod-
ing efficiency and are based on the word alignment
structure preserved in hierarchical phrase transla-
tion pairs (Zens and Ney, 2006). When hierarchi-
cal phrases are extracted, the word alignment is pre-
served. If multiple word alignments are observed
ei?1 ei ei+1 ei+2 ei+3 ei+4
f j?1 f j f j+1 f j+2 f j+3
Figure 1: An example of sparse features for a phrase
translation.
with the same source and target sides, only the fre-
quently observed word alignment is kept to reduce
the grammar size.
3.2.1 Word Pair Features
Word pair features reflect the word correspon-
dence in a hierarchical phrase. Figure 1 illustrates
an example of sparse features for a phrase trans-
lation pair f j, ..., f j+2 and ei, ..., ei+3 1. From the
word alignment encoded in this phrase, we can ex-
tract word pair features of (ei, f j+1), (ei+2, f j+2) and
(ei+3, f j).
The bigrams of word pairs are also used to
capture the contextual dependency. We assume
that the word pairs follow the target side order-
ing. For instance, we define ((ei?1, f j?1), (ei, f j+1)),
((ei, f j+1), (ei+2, f j+2)) and ((ei+2, f j+2), (ei+3, f j)) in-
dicated by the arrows in Figure 1.
Extracting bigram word pair features following
the target side ordering implies that the correspond-
ing source side is reordered according to the tar-
get side. The reordering of hierarchical phrases is
represented by using contextually dependent word
pairs across their boundaries, as with the feature
((ei?1, f j?1), (ei, f j+1)) in Figure 1.
3.2.2 Insertion Features
The above features are insufficient to capture the
translation because spurious words are sometimes
inserted in the target side. Therefore, insertion fea-
tures are integrated in which no word alignment is
associated in the target. The inserted words are asso-
ciated with all the words in the source sentence, such
as (ei+1, f1), ..., (ei+1, fJ) for the non-aligned word
ei+1 with the source sentence f J1 in Figure 1. In the
1For simplicity, we show an example of phrase translation
pairs, but it is trivial to define the features over hierarchical
phrases.
766
f j?1
f j f j+1
f j+2
f j+3
X 1
X 2
X 3
Figure 2: Example hierarchical features.
same way, we will be able to include deletion fea-
tures where a non-aligned source word is associated
with the target sentence. However, this would lead to
complex decoding in which all the translated words
are memorized for each hypothesis, and thus not in-
tegrated in our feature set.
3.2.3 Target Bigram Features
Target side bigram features are also included to
directly capture the fluency as in the n-gram lan-
guage model (Roark et al, 2004). For instance, bi-
gram features of (ei?1, ei), (ei, ei+1), (ei+1, ei+2)... are
observed in Figure 1.
3.2.4 Hierarchical Features
In addition to the phrase motivated features, we
included features inspired by the hierarchical struc-
ture. Figure 2 shows an example of hierarchical
phrases in the source side, consisting of X 1 ?
?
f j?1X 2 f j+3
?
, X 2 ?
?
f j f j+1X 3
?
and X 3 ?
?
f j+2
?
.
Hierarchical features capture the dependency of
the source words in a parent phrase to the source
words in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),
( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in-
dicated by the arrows in Figure 2. The hierarchical
features are extracted only for those source words
that are aligned with the target side to limit the fea-
ture size.
3.3 Normalization
In order to achieve the generalization capability, the
following normalized tokens are introduced for each
surface form:
? Word class or POS.
? 4-letter prefix and suffix. For instance, the word
Algorithm 1 Online Training Algorithm
Training data: T = {( f t, et)}Tt=1
m-best oracles: O = {}Tt=1
i = 0
1: for n = 1, ..., N do
2: for t = 1, ..., T do
3: Ct ? bestk( f t; wi)
4: Ot ? oraclem(Ot ? Ct; et)
5: wi+1 = update wi using Ct w.r.t. Ot
6: i = i + 1
7: end for
8: end for
9: return
?NT
i=1 w
i
NT
?violate? is normalized to ?viol+? and ?+late?
by taking the prefix and suffix, respectively.
? Digits replaced by a sequence of ?@?. For ex-
ample, the word ?2007/6/27? is represented as
?@@@@/@/@@?.
We consider all possible combination of those to-
ken types. For example, the word pair feature (vi-
olate, tnthk) is normalized and expanded to (viol+,
tnthk), (viol+, tnth+), (violate, tnth+), etc. using the
4-letter prefix token type.
4 Online Large-Margin Training
Algorithm 1 is our generic online training algo-
rithm. The algorithm is slightly different from other
online training algorithms (Tillmann and Zhang,
2006; Liang et al, 2006) in that we keep and up-
date oracle translations, which is a set of good trans-
lations reachable by a decoder according to a met-
ric, i.e. BLEU (Papineni et al, 2002). In line 3,
a k-best list is generated by bestk(?) using the cur-
rent weight vector wi for the training instance of
( f t, et). Each training instance has multiple (or, pos-
sibly one) reference translations et for the source
sentence f t. Using the k-best list, m-best oracle
translations Ot is updated by oraclem(?) for every it-
eration (line 4). Usually, a decoder cannot generate
translations that exactly match the reference transla-
tions due to its beam search pruning and OOV. Thus,
we cannot always assign scores for each reference
translation. Therefore, possible oracle translations
are maintained according to an objective function,
767
i.e. BLEU. Tillmann and Zhang (2006) avoided the
problem by precomputing the oracle translations in
advance. Liang et al (2006) presented a similar up-
dating strategy in which parameters were updated
toward an oracle translation found in Ct, but ignored
potentially better translations discovered in the past
iterations.
New wi+1 is computed using the k-best list Ct with
respect to the oracle translations Ot (line 5). After N
iterations, the algorithm returns an averaged weight
vector to avoid overfitting (line 9). The key to this
online training algorithm is the selection of the up-
dating scheme in line 5.
4.1 Margin Infused Relaxed Algorithm
The Margin Infused Relaxed Algorithm (MIRA)
(Crammer et al, 2006) is an online version of the
large-margin training algorithm for structured clas-
sification (Taskar et al, 2004) that has been suc-
cessfully used for dependency parsing (McDonald et
al., 2005) and joint-labeling/chunking (Shimizu and
Haas, 2006). The basic idea is to keep the norm of
the updates to the weight vector as small as possible,
considering a margin at least as large as the loss of
the incorrect classification.
Line 5 of the weight vector update procedure in
Algorithm 1 is replaced by the solution of:
w?i+1 = argmin
wi+1
||wi+1 ? wi|| + C
?
e?,e?
?(e?, e?)
subject to
si+1( f t, e?) ? si+1( f t, e?) + ?(e?, e?) ? L(e?, e?; et)
?(e?, e?) ? 0
?e? ? Ot,?e? ? Ct (3)
where si( f t, e) =
{
wi
}T ? h( f t, e). ?(?) is a non-
negative slack variable and C ? 0 is a constant to
control the influence to the objective function. A
larger C implies larger updates to the weight vec-
tor. L(?) is a loss function, for instance difference of
BLEU, that measures the difference between e? and
e? according to the reference translations et. In this
update, a margin is created for each correct and in-
correct translation at least as large as the loss of the
incorrect translation. A larger error means a larger
distance between the scores of the correct and incor-
rect translations. Following McDonald et al (2005),
only k-best translations are used to form the margins
in order to reduce the number of constraints in Eq. 3.
In the translation task, multiple translations are ac-
ceptable. Thus, margins for m-oracle translation are
created, which amount to m ? k large-margin con-
straints. In this online training, only active features
constrained by Eq. 3 are kept and updated, unlike
offline training in which all possible features have to
be extracted and selected in advance.
The Lagrange dual form of Eq. 3 is:
max?(?)?0 ?
1
2
||
?
e?,e?
?(e?, e?)
(
h( f t, e?) ? h( f t, e?)
)
||2
+
?
e?,e?
?(e?, e?)L(e?, e?; et)
?
?
e?,e?
?(e?, e?)
(
si( f t, e?) ? si( f t, e?)
)
subject to
?
e?,e?
?(e?, e?) ? C (4)
with the weight vector update:
wi+1 = wi +
?
e?,e?
?(e?, e?)
(
h( f t, e?) ? h( f t, e?)
)
(5)
Equation 4 is solved using a QP-solver, such as a co-
ordinate ascent algorithm, by heuristically selecting
(e?, e?) and by updating ?(?) iteratively:
?(e?, e?) = max (0, ?(e?, e?) + ?(e?, e?)) (6)
?(e?, e?) =
L(e?, e?; et) ?
(
si( f t, e?) ? si( f t, e?)
)
||h( f t, e?) ? h( f t, e?)||2
C is used to clip the amount of updates.
A single oracle with 1-best translation is analyti-
cally solved without a QP-solver and is represented
as the following perceptron-like update (Shimizu
and Haas, 2006):
? = max
?
?
?
?
?
?
?
?
0, min
?
?
?
?
?
?
?
?
C,
L(e?, e?; et) ?
(
si( f t, e?) ? si( f t, e?)
)
||h( f t, e?) ? h( f t, e?)||2
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Intuitively, the update amount is controlled by the
margin and the loss between the correct and incor-
rect translations and by the closeness of two transla-
tions in terms of feature vectors. Indeed, Liang et al
(2006) employed an averaged perceptron algorithm
in which ? value was always set to one. Tillmann
and Zhang (2006) used a different update style based
on a convex loss function:
? = ?L(e?, e?; et) ?max
(
0, 1 ?
(
si( f t, e?) ? si( f t, e?)
))
768
Table 1: Experimental results obtained by varying normalized tokens used with surface form.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
surface form 492K 11.32 54.11 10.57 49.01 10.77 48.05
w/ prefix/suffix 4,204K 12.38 63.87 10.42 48.74 10.58 47.18
w/ word class 2,689K 10.87 49.59 10.63 49.55 10.89 48.79
w/ digits 576K 11.01 50.72 10.66 49.67 10.84 48.39
all token types 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
where ? > 0 is a learning rate for controlling the
convergence.
4.2 Approximated BLEU
We used the BLEU score (Papineni et al, 2002) as
the loss function computed by:
BLEU(E; E) = exp
?
?
?
?
?
?
?
?
1
N
N
?
n=1
log pn(E, E)
?
?
?
?
?
?
?
?
? BP(E, E)
(7)
where pn(?) is the n-gram precision of hypothesized
translations E = {et}Tt=1 given reference translations
E = {et}Tt=1 and BP(?) ? 1 is a brevity penalty. BLEU
is computed for a set of sentences, not for a sin-
gle sentence. Our algorithm requires frequent up-
dates on the weight vector, which implies higher cost
in computing the document-wise BLEU. Tillmann
and Zhang (2006) and Liang et al (2006) solved
the problem by introducing a sentence-wise BLEU.
However, the use of the sentence-wise scoring does
not translate directly into the document-wise score
because of the n-gram precision statistics and the
brevity penalty statistics aggregated for a sentence
set. Thus, we use an approximated BLEU score
that basically computes BLEU for a sentence set, but
accumulates the difference for a particular sentence
(Watanabe et al, 2006a).
The approximated BLEU is computed as follows:
Given oracle translations O for T , we maintain the
best oracle translations OT1 =
{
e?1, ..., e?T
}
. The ap-
proximated BLEU for a hypothesized translation e?
for the training instance ( f t, et) is computed over OT1
except for e?t, which is replaced by e?:
BLEU({e?1, ..., e?t?1, e?, e?t+1, ..., e?T }; E)
The loss computed by the approximated BLEU mea-
sures the document-wise loss of substituting the cor-
rect translation e?t into an incorrect translation e?.
The score can be regarded as a normalization which
scales a sentence-wise score into a document-wise
score.
5 Experiments
We employed our online large-margin training pro-
cedure for an Arabic-to-English translation task.
The training data were extracted from the Ara-
bic/English news/UN bilingual corpora supplied by
LDC. The data amount to nearly 3.8M sentences.
The Arabic part of the bilingual data is tokenized by
isolating Arabic scripts and punctuation marks. The
development set comes from the MT2003 Arabic-
English NIST evaluation test set consisting of 663
sentences in the news domain with four reference
translations. The performance is evaluated by the
news domain MT2004/MT2005 test set consisting
of 707 and 1,056 sentences, respectively.
The hierarchical phrase translation pairs are ex-
tracted in a standard way (Chiang, 2005): First,
the bilingual data are word alignment annotated by
running GIZA++ (Och and Ney, 2003) in two di-
rections. Second, the word alignment is refined
by a grow-diag-final heuristic (Koehn et al, 2003).
Third, phrase translation pairs are extracted together
with hierarchical phrases by considering holes. In
the last step, the hierarchical phrases are constrained
so that they follow the target normalized form con-
straint. A 5-gram language model is trained on the
English side of the bilingual data combined with the
English Gigaword from LDC.
First, the use of normalized token types in Sec-
tion 3.3 is evaluated in Table 1. In this setting, all
the structural features in Section 3.2 are used, but
differentiated by the normalized tokens combined
with surface forms. Our online large-margin train-
ing algorithm performed 50 iterations constrained
769
Table 2: Experimental results obtained by incrementally adding structural features.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
word pairs 11,042K 11.05 51.63 10.43 48.69 10.73 47.72
+ target bigram 11,230K 11.19 53.49 10.40 48.60 10.66 47.47
+ insertion 13,489K 11.21 52.20 10.77 50.33 10.93 48.08
+ hierarchical 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
Table 3: Experimental results for varying k-best and m-oracle translations.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
baseline 10.64 46.47 10.83 49.33 10.90 47.03
1-oracle 1-best 8,735K 11.25 52.63 10.82 50.77 10.93 48.11
1-oracle 10-best 10,480K 11.24 53.45 10.55 49.10 10.82 48.49
10-oracle 1-best 8,416K 10.70 47.63 10.83 48.88 10.76 46.00
10-oracle 10-best 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
sentence-BLEU 14,587K 11.10 51.17 10.82 49.97 10.86 47.04
by 10-oracle and 10-best list. When decoding, a
1000-best list is generated to achieve better oracle
translations. The training took nearly 1 day using 8
cores of Opteron. The translation quality is eval-
uated by case-sensitive NIST (Doddington, 2002)
and BLEU (Papineni et al, 2002)2. The table also
shows the number of active features in which non-
zero values were assigned as weights. The addition
of prefix/suffix tokens greatly increased the number
of active features. The setting severely overfit to the
development data, and therefore resulted in worse
results in open tests. The word class3 with surface
form avoided the overfitting problem. The digit se-
quence normalization provides a similar generaliza-
tion capability despite of the moderate increase in
the active feature size. By including all token types,
we achieved better NIST/BLEU scores for the 2004
and 2005 test sets. This set of experiments indi-
cates that a token normalization is useful especially
trained on a small data.
Second, we used all the normalized token types,
but incrementally added structural features in Ta-
ble 2. Target bigram features account for only the
fluency of the target side without considering the
source/target correspondence. Therefore, the in-
2We used the tool available at http://www.nist.gov/
speech/tests/mt/
3We induced 50 classes each for English and Arabic.
clusion of target bigram features clearly overfit to
the development data. The problem is resolved by
adding insertion features which can take into ac-
count an agreement with the source side that is not
directly captured by word pair features. Hierarchi-
cal features are somewhat effective in the 2005 test
set by considering the dependency structure of the
source side.
Finally, we compared our online training algo-
rithm with sparse features with a baseline system
in Table 3. The baseline hierarchical phrase-based
system is trained using standard max-BLEU training
(MERT) without sparse features (Och, 2003). Table
3 shows the results obtained by varying the m-oracle
and k-best size (k, m = 1, 10) using all structural
features and all token types. We also experimented
sentence-wise BLEU as an objective function con-
strained by 10-oracle and 10-best list. Even the 1-
oracle 1-best configuration achieved significant im-
provements over the baseline system. The use of
a larger k-best list further optimizes to the devel-
opment set, but at the cost of degraded translation
quality in the 2004 test set. The larger m-oracle size
seems to be harmful if coupled with the 1-best list.
As indicated by the reduced active feature size, 1-
best translation seems to be updated toward worse
translations in 10-oracles that are ?close? in terms
of features. We achieved significant improvements
770
Table 4: Two-fold cross validation experiments.
closed test open test
NIST BLEU NIST BLEU
[%] [%]
baseline 10.71 44.79 10.68 44.44
online 11.58 53.42 10.90 47.64
when the k-best list size was also increased. The
use of sentence-wise BLEU as an objective provides
almost no improvement in the 2005 test set, but is
comparable for the 2004 test set.
As observed in three experiments, the 2004/2005
test sets behaved differently, probably because of
the domain mismatch. Thus, we conducted a two-
fold cross validation using the 2003/2004/2005 test
sets to observe the effect of optimization as shown
in Table 44. The MERT baseline system performed
similarly both in closed and open tests. Our on-
line large-margin training with 10-oracle and 10-
best constraints and the approximated BLEU loss
function significantly outperformed the baseline sys-
tem in the open test. The development data is almost
doubled in this setting. The MERT approach seems
to be confused with the slightly larger data and with
the mixed domains from different epochs.
6 Discussion
In this work, the translation model consisting of mil-
lions of features are successfully integrated. In or-
der to avoid poor overfitting, features are limited to
word-based features, but are designed to reflect the
structures inside hierarchical phrases. One of the
benefit of MIRA is its flexibility. We may include
as many constraints as possible, like m-oracle con-
straints in our experiments. Although we described
experiments on the hierarchical phrase-based trans-
lation, the online training algorithm is applicable to
any translation systems, such as phrase-based trans-
lations and syntax-based translations.
Online discriminative training has already been
studied by Tillmann and Zhang (2006) and Liang
et al (2006). In their approach, training was per-
formed on a large corpus using the sparse features of
phrase translation pairs, target n-grams and/or bag-
of-word pairs inside phrases. In Tillmann and Zhang
4We split data by document, not by sentence.
(2006), k-best list generation is approximated by a
step-by-step one-best merging method that separates
the decoding and training steps. The weight vector
update scheme is very similar to MIRA but based
on a convex loss function. Our method directly em-
ploys the k-best list generated by the fast decoding
method (Watanabe et al, 2006b) at every iteration.
One of the benefits is that we avoid the rather expen-
sive cost of merging the k-best list especially when
handling millions of features.
Liang et al (2006) employed an averaged percep-
tron algorithm. They decoded each training instance
and performed a perceptron update to the weight
vector. An incorrect translation was updated toward
an oracle translation found in a k-best list, but dis-
carded potentially better translations in the past iter-
ations.
An experiment has been undertaken using a small
development set together with sparse features for the
reranking of a k-best translation (Watanabe et al,
2006a). They relied on a variant of a voted percep-
tron, and achieved significant improvements. How-
ever, their work was limited to reranking, thus the
improvement was relative to the performance of the
baseline system, whether or not there was a good
translation in a list. In our work, the sparse features
are directly integrated into the DP-based search.
The design of the sparse features was inspired
by Zens and Ney (2006). They exploited the
word alignment structure inside the phrase trans-
lation pairs for discriminatively training a reorder-
ing model in their phrase-based translation. The re-
ordering model simply classifies whether to perform
monotone decoding or not. The trained model is
treated as a single feature function integrated in Eq.
1. Our approach differs in that each sparse feature is
individually integrated in Eq. 1.
7 Conclusion
We exploited a large number of binary features
for statistical machine translation. The model was
trained on a small development set. The optimiza-
tion was carried out by MIRA, which is an online
version of the large-margin training algorithm. Mil-
lions of sparse features are intuitively considered
prone to overfitting, especially when trained on a
small development set. However, our algorithm with
771
millions of features achieved very significant im-
provements over a conventional method with a small
number of features. This result indicates that we
can easily experiment many alternative features even
with a small data set, but we believe that our ap-
proach can scale well to a larger data set for further
improved performance. Future work involves scal-
ing up to larger data and more features.
Acknowledgements
We would like to thank reviewers and our colleagues
for useful comment and discussion.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2006. Sequence classification for machine trans-
lation. In Proc. of Interspeech 2006, pages 1157?
1160, Pittsburgh.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system?. In Proc. of IWSLT 2004, pages 79?84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263?270, Ann Arbor, Michigan, June.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, March.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006, pages 961?968, Sydney, Aus-
tralia, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of COL-
ING/ACL 2006, pages 761?768, Sydney, Australia,
July.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc. of ACL 2005, pages 91?98, Ann Ar-
bor, Michigan, June.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318, Philadelphia, Pennsylvania.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language model-
ing with conditional random fields and the percep-
tron algorithm. In Proc. of ACL 2004, pages 47?54,
Barcelona, Spain, July.
Nobuyuki Shimizu and Andrew Haas. 2006. Exact de-
coding for jointly labeling and chunking sequences.
In Proc. of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 763?770, Sydney, Australia,
July.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. of EMNLP 2004, pages 1?8, Barcelona, Spain,
July.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. of COLING/ACL 2006, pages 721?728, Sydney,
Australia, July.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006a. NTT Statistical Machine Translation
for IWSLT 2006. In Proc. of IWSLT 2006, pages 95?
102, Kyoto, Japan.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006b. Left-to-right target generation for hierarchi-
cal phrase-based translation. In Proc. of COLING/ACL
2006, pages 777?784, Sydney, Australia, July.
772
Dekai Wu and Hongsing Wong. 1998. Machine transla-
tion with a stochastic grammatical channel. In Proc.
of COLING 98, pages 1408?1415, Montreal, Quebec,
Canada.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proc. of WSMT 2006, pages 55?63, New York City,
June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of WSMT 2006, pages 138?141, New York City,
June.
773
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777?784,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Left-to-Right Target Generation for Hierarchical Phrase-based
Translation
Taro Watanabe Hajime Tsukada Hideki Isozaki
2-4, Hikaridai, Seika-cho, Soraku-gun,
Kyoto, JAPAN 619-0237
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
We present a hierarchical phrase-based
statistical machine translation in which a
target sentence is efficiently generated in
left-to-right order. The model is a class
of synchronous-CFG with a Greibach Nor-
mal Form-like structure for the projected
production rule: The paired target-side
of a production rule takes a phrase pre-
fixed form. The decoder for the target-
normalized form is based on an Early-
style top down parser on the source side.
The target-normalized form coupled with
our top down parser implies a left-to-
right generation of translations which en-
ables us a straightforward integration with
ngram language models. Our model was
experimented on a Japanese-to-English
newswire translation task, and showed sta-
tistically significant performance improve-
ments against a phrase-based translation
system.
1 Introduction
In a classical statistical machine translation, a for-
eign language sentence f J1 = f1, f2, ... fJ is trans-
lated into another language, i.e. English, eI1 =
e1, e2, ..., eI by seeking a maximum likely solution
of:
e?I1 = argmax
eI1
Pr(eI1| f J1 ) (1)
= argmax
eI1
Pr( f J1 |eI1)Pr(eI1) (2)
The source channel approach in Equation 2 inde-
pendently decomposes translation knowledge into
a translation model and a language model, respec-
tively (Brown et al, 1993). The former repre-
sents the correspondence between two languages
and the latter contributes to the fluency of English.
In the state of the art statistical machine transla-
tion, the posterior probability Pr(eI1| f J1 ) is directly
maximized using a log-linear combination of fea-
ture functions (Och and Ney, 2002):
e?I1 = argmax
eI1
exp
(
?M
m=1 ?mhm(eI1, f J1 )
)
?
e? I
?
1
exp
(
?M
m=1 ?mhm(e? I
?
1 , f J1 )
) (3)
where hm(eI1, f J1 ) is a feature function, such as
a ngram language model or a translation model.
When decoding, the denominator is dropped since
it depends only on f J1 . Feature function scaling
factors ?m are optimized based on a maximum
likely approach (Och and Ney, 2002) or on a direct
error minimization approach (Och, 2003). This
modeling allows the integration of various fea-
ture functions depending on the scenario of how
a translation is constituted.
A phrase-based translation model is one of the
modern approaches which exploits a phrase, a
contiguous sequence of words, as a unit of transla-
tion (Koehn et al, 2003; Zens and Ney, 2003; Till-
man, 2004). The idea is based on a word-based
source channel modeling of Brown et al (1993):
It assumes that eI1 is segmented into a sequence
of K phrases e?K1 . Each phrase e?k is transformed
into ?fk. The translated phrases are reordered to
form f J1 . One of the benefits of the modeling is
that the phrase translation unit preserves localized
word reordering. However, it cannot hypothesize
a long-distance reordering required for linguisti-
cally divergent language pairs. For instance, when
translating Japanese to English, a Japanese SOV
structure has to be reordered to match with an En-
777
glish SVO structure. Such a sentence-wise move-
ment cannot be realized within the phrase-based
modeling.
Chiang (2005) introduced a hierarchical phrase-
based translation model that combined the
strength of the phrase-based approach and a
synchronous-CFG formalism (Aho and Ullman,
1969): A rewrite system initiated from a start
symbol which synchronously rewrites paired non-
terminals. Their translation model is a binarized
synchronous-CFG, or a rank-2 of synchronous-
CFG, in which the right-hand side of a production
rule contains at most two non-terminals. The form
can be regarded as a phrase translation pair with
at most two holes instantiated with other phrases.
The hierarchically combined phrases provide a
sort of reordering constraints that is not directly
modeled by a phrase-based model.
Rules are induced from a bilingual corpus with-
out linguistic clues first by extracting phrase trans-
lation pairs, and then by generalizing extracted
phrases with holes (Chiang, 2005). Even in a
phrase-based model, the number of phrases ex-
tracted from a bilingual corpus is quadratic to
the length of bilingual sentences. The grammar
size for the hierarchical phrase-based model will
be further exploded, since there exists numerous
combination of inserting holes to each rule. The
spuriously increasing grammar size will be prob-
lematic for decoding without certain heuristics,
such as a length based thresholding.
The integration with a ngram language model
further increases the cost of decoding especially
when incorporating a higher order ngram, such as
5-gram. In the hierarchical phrase-based model
(Chiang, 2005), and an inversion transduction
grammar (ITG) (Wu, 1997), the problem is re-
solved by restricting to a binarized form where at
most two non-terminals are allowed in the right-
hand side. However, Huang et al (2005) reported
that the computational complexity for decoding
amounted to O(J3+3(n?1)) with n-gram even using
a hook technique. The complexity lies in mem-
orizing the ngram?s context for each constituent.
The order of ngram would be a dominant factor
for higher order ngrams.
As an alternative to a binarized form, we
present a target-normalized hierarchical phrase-
based translation model. The model is a class of a
hierarchical phrase-based model, but constrained
so that the English part of the right-hand side
is restricted to a Greibach Normal Form (GNF)-
like structure: A contiguous sequence of termi-
nals, or a phrase, is followed by a string of non-
terminals. The target-normalized form reduces the
number of rules extracted from a bilingual corpus,
but still preserves the strength of the phrase-based
approach. An integration with ngram language
model is straightforward, since the model gener-
ates a translation in left-to-right order. Our de-
coder is based on an Earley-style top down pars-
ing on the foreign language side. The projected
English-side is generated in left-to-right order syn-
chronized with the derivation of the foreign lan-
guage side. The decoder?s implementation is taken
after a decoder for an existing phrase-based model
with a simple modification to account for produc-
tion rules. Experimental results on a Japanese-to-
English newswire translation task showed signif-
icant improvement against a phrase-based model-
ing.
2 Translation Model
A weighted synchronous-CFG is a rewrite system
consisting of production rules whose right-hand
side is paired (Aho and Ullman, 1969):
X ? ??, ?,?? (4)
where X is a non-terminal, ? and ? are strings of
terminals and non-terminals. For notational sim-
plicity, we assume that ? and ? correspond to the
foreign language side and the English side, re-
spectively. ? is a one-to-one correspondence for
the non-terminals appeared in ? and ?. Starting
from an initial non-terminal, each rule rewrites
non-terminals in ? and ? that are associated with
?.
Chiang (2005) proposed a hierarchical phrase-
based translation model, a binary synchronous-
CFG, which restricted the form of production rules
as follows:
? Only two types of non-terminals allowed: S
and X.
? Both of the strings ? and ? must contain at
least one terminal item.
? Rules may have at most two non-terminals
but non-terminals cannot be adjacent for the
foreign language side ?.
The production rules are induced from a bilingual
corpus with the help of word alignments. To al-
leviate a data sparseness problem, glue rules are
778
added that prefer combining hierarchical phrases
in a serial manner:
S ?
?
S 1 X2 , S 1 X2
?
(5)
S ?
?
X 1 , X1
?
(6)
where boxed indices indicate non-terminal?s link-
ages represented in ?.
Our model is based on Chiang (2005)?s frame-
work, but further restricts the form of production
rules so that the aligned right-hand side ? follows
a GNF-like structure:
X ?
?
?, ?b?,?
?
(7)
where ?b is a string of terminals, or a phrase,
and beta is a (possibly empty) string of non-
terminals. The foreign language at right-hand side
? still takes an arbitrary string of terminals and
non-terminals. The use of a phrase ?b as a pre-
fix keeps the strength of the phrase-base frame-
work. A contiguous English side coupled with
a (possibly) discontiguous foreign language side
preserves a phrase-bounded local word reordering.
At the same time, the target-normalized frame-
work still combines phrases hierarchically in a re-
stricted manner.
The target-normalized form can be regarded as
a type of rule in which certain non-terminals are
always instantiated with phrase translation pairs.
Thus, we will be able to reduce the number of rules
induced from a bilingual corpus, which, in turn,
help reducing the decoding complexity.
The contiguous phrase-prefixed form generates
English in left-to-right order. Therefore, a decoder
can easily hypothesize a derivation tree integrated
with a ngram language model even with higher or-
der.
Note that we do not imply arbitrary
synchronous-CFGs are transformed into the
target normalized form. The form simply restricts
the grammar extracted from a bilingual corpus
explained in the next section.
2.1 Rule Extraction
We present an algorithm to extract production
rules from a bilingual corpus. The procedure is
based on those for the hierarchical phrase-based
translation model (Chiang, 2005).
First, a bilingual corpus is annotated with word
alignments using the method of Koehn et al
(2003). Many-to-many word alignments are in-
duced by running a one-to-many word alignment
model, such as GIZA++ (Och and Ney, 2003), in
both directions and by combining the results based
on a heuristic (Koehn et al, 2003).
Second, phrase translation pairs are extracted
from the word alignment corpus (Koehn et al,
2003). The method exhaustively extracts phrase
pairs ( f j+mj , ei+ni ) from a sentence pair ( f J1 , eI1) that
do not violate the word alignment constraints a:
?(i?, j?) ? a : j? ? [ j, j + m], i? ? [i, i + n]
?(i?, j?) ? a : j? ? [ j, j + m], i? < [i, i + n]
?(i?, j?) ? a : j? < [ j, j + m], i? ? [i, i + n]
Third, based on the extracted phrases, production
rules are accumulated by computing the ?holes?
for contiguous phrases (Chiang, 2005):
1. A phrase pair ( ?f , e?) constitutes a rule
X ?
?
?f , e?
?
2. A rule X ? ??, ?? and a phrase pair ( ?f , e?) s.t.
? = ?? ?f??? and ? = e??e?? constitutes a rule
X ?
?
?? X k ?
??, e?? X k ?
?
Following Chiang (2005), we applied constraints
when inducing rules with non-terminals:
? At least one foreign word must be aligned to
an English word.
? Adjacent non-terminals are not allowed for
the foreign language side.
2.2 Phrase-based Rules
The rule extraction procedure described in Section
2.1 is a corpus-based, therefore will be easily suf-
fered from a data sparseness problem. The hier-
archical phrase-based model avoided this problem
by introducing the glue rules 5 and 6 that com-
bined hierarchical phrases sequentially (Chiang,
2005).
We use a different method of generalizing pro-
duction rules. When production rules without non-
terminals are extracted in step 1 of Section 2.1,
X ?
?
?f , e?
?
(8)
then, we also add production rules as follows:
X ?
?
?f X 1 , e? X 1
?
(9)
X ?
?
X 1 ?f , e? X 1
?
(10)
X ?
?
X 1 ?f X 2 , e? X 1 X 2
?
(11)
X ?
?
X 2 ?f X 1 , e? X 1 X 2
?
(12)
779
The international terrorism also is a possible threat in Japan
Reference translation: ?International terrorism is a threat
even to Japan?
(a) Translation by a phrase-based model. (b) A derivation tree representation for Figure 1(a).Indices in
non-terminal X represent the order to perform rewriting.
Figure 1: An example of Japanese-to-English translation by a phrase-based model.
We call them phrase-based rules, since four types
of rules are generalized directly from phrase trans-
lation pairs.
The class of rules roughly corresponds to the re-
ordering constraints used in a phrase-based model
during decoding. Rules 8 and 9 are sufficient to re-
alize a monotone decoding in which phrase trans-
lation pairs are simply combined sequentially.
With rules 10 and 11, the non-terminal X 1 behaves
as a place holder where certain number of foreign
words are skipped. Therefore, those rules real-
ize a window size constraint used in many phrase-
based models (Koehn et al, 2003). The rule 12
further gives an extra freedom for the phrase pair
reordering. The rules 8 through 12 can be in-
terpreted as ITG-constraints where phrase trans-
lation pairs are hierarchically combined either in
a monotonic way or in an inverted manner (Zens
and Ney, 2003; Wu, 1997). Thus, by controlling
what types of phrase-based rules employed in a
grammar, we will be able to simulate a phrase-
based translation model with various constraints.
This reduction is rather natural in that a finite state
transducer, or a phrase-based model, is a subclass
of a synchronous-CFG.
Figure 1(a) shows an example Japanese-to-
English translation by a phrase-based model de-
scribed in Section 5. Using the phrase-based rules,
the translation results is represented as a derivation
tree in Figure 1(b).
3 Decoding
Our decoder is an Earley-style top down parser on
the foreign language side with a beam search strat-
egy. Given an input sentence f J1 , the decoder seeks
for the best English according to Equation 3 us-
ing the feature functions described in Section 4.
The English output sentence is generated in left-
to-right order in accordance with the derivation of
the foreign language side synchronized with the
cardinality of already translated foreign word po-
sitions.
The decoding process is very similar to those
described in (Koehn et al, 2003): It starts from an
initial empty hypothesis. From an existing hypoth-
esis, new hypothesis is generated by consuming
a production rule that covers untranslated foreign
word positions. The score for the newly generated
hypothesis is updated by combining the scores of
feature functions described in Section 4. The En-
glish side of the rule is simply concatenated to
form a new prefix of English sentence. Hypothe-
ses that consumed m foreign words are stored in a
priority queue Qm.
Hypotheses in Qm undergo two types of prun-
ing: A histogram pruning preserves at most M hy-
potheses inQm. A threshold pruning discards a hy-
potheses whose score is below the maximum score
of Qm multiplied with a threshold value ?. Rules
are constrained by their foreign word span of a
non-terminal. For a rule consisting of more than
two non-terminals, we constrained so that at least
one non-terminal should span at most ? words.
The decoder is characterized as a weighted
synchronous-CFG implemented with a push-down
automaton rather a weighted finite state transducer
(Aho and Ullman, 1969). Each hypothesis main-
tains following knowledge:
? A prefix of English sentence. For space ef-
ficiency, the prefix is represented as a word
graph.
? Partial contexts for each feature function.
For instance, to compute a 5-gram language
model feature, we keep the consecutive last
four words of an English prefix.
780
? A stack that keeps track of the uncovered for-
eign word spans. The stack for an initial hy-
pothesis is initialized with span [1, J].
When extending a hypothesis, the associated stack
structure is popped. The popped foreign word
span [ jl, jr] is used to locate the rules for uncov-
ered foreign word positions. We assume that the
decoder accumulates all the applicable rules from
a large database and stores the extracted rules in a
chart structure. The decoder identifies what rules
to consume when extending a hypothesis using the
chart structure. A new hypothesis is created with
an updated stack by pushing foreign non-terminal
spans: For each rule spanning [ jl, jr] at foreign-
side with non-terminal spans of [kl1, kr1], [kl2, kr2], ...,
the non-terminal spans are pushed in the reverse
order of the projected English side. For example,
A rule with foreign word non-terminal spans:
X ?
?
X 2 : [kl2, kr2] ?f X 1 : [kl1, kr1], e? X 1 X 2
?
will update a stack by pushing the foreign word
spans [kl2, kr2] and [kl1, kr1] in order. This ordering
assures that, when popped, the English-side will
be generated in left-to-right order. A hypothesis
with an empty stack implies that the hypothesis
has covered all the foreign words.
Figure 2 illustrates the decoding process for the
derivation tree in Figure 1(b). Starting from the
initial hypothesis of [1, 11], the stack is updated in
accordance with non-terminal?s spans. The span
is popped and the rule with the foreign word pan
[1, 11] is looked up from the chart structure. The
stack structure for the newly created hypothesis is
updated by pushing non-terminal spans [4, 11] and
[1, 2].
Our decoder is based on an in-house devel-
oped phrase-based decoder which uses a bit vec-
tor to represent uncovered foreign word positions
for each hypothesis. We basically replaced the
bit vector structure to the stack structure: Al-
most no modification was required for the word
graph structure and the beam search strategy im-
plemented for a phrase-based modeling. The use
of a stack structure directly models a synchronous-
CFG formalism realized as a push-down automa-
tion, while the bit vector implementation is con-
ceptualized as a finite state transducer. The cost
of decoding with the proposed model is cubic to
foreign language sentence length.
Rules Stack
[1, 11]
X : [1, 11]?
?
X 1 : [1, 2] X 2 : [4, 11], The X 1 X 2
? [1, 2]
[4, 11]
X : [1, 2]?
?
X 1 : [2, 2], international X 1
? [2, 2]
[4, 11]
X : [2, 2]? ? , terrorism? [4, 11]
X : [4, 11]?
?
X 2 : [4, 5] X 1 : [7, 11], also X 1 X 2
? [7, 11]
[4, 5]
X : [7, 11]?
?
X 1 : [7, 9] , is a X 1
? [7, 9]
[4, 5]
X : [7, 9]?
?
X 1 : [9, 9], possible X 1
? [9, 9]
[4, 5]
X : [9, 9]? ? , threat? [4, 5]
X : [4, 5]?
?
X 1 : [4, 4] , in X 1
?
[4, 4]
X : [4, 4]? ? , Japan?
Figure 2: An example decoding process of Fig-
ure 1(b) with a stack to keep track of foreign word
spans.
4 Feature Functions
The decoder for our translation model uses a log-
linear combination of feature functions, or sub-
models, to seek for the maximum likely translation
according to Equation 3. This section describes
the models experimented in Section 5, mainly
consisting of count-based models, lexicon-based
models, a language model, reordering models and
length-based models.
4.1 Count-based Models
Main feature functions h?( f J1 |eI1,D) and
h?(eI1| f J1 ,D) estimate the likelihood of two
sentences f J1 and eI1 over a derivation tree D.
We assume that the production rules in D are
independent of each other:
h?( f J1 |eI1,D) = log
?
??,???D
?(?|?) (13)
?(?|?) is estimated through the relative frequency
on a given bilingual corpus.
?(?|?) = count(?, ?)?
? count(?, ?)
(14)
where count(?) represents the cooccurrence fre-
quency of rules ? and ?.
The relative count-based probabilities for the
phrase-based rules are simply adopted from the
original probabilities of phrase translation pairs.
4.2 Lexicon-based Models
We define lexically weighted feature functions
hw( f J1 |eI1,D) and hw(eI1| f J1 ,D) applying the inde-
pendence assumption of production rules as in
781
Equation 13.
hw( f J1 |eI1,D) = log
?
??,???D
pw(?|?) (15)
The lexical weight pw(?|?) is computed from word
alignments a inside ? and ? (Koehn et al, 2003):
pw(?|?, a) =
|?|
?
i=1
1
|{ j|(i, j) ? a}|
?
?(i, j)?a
t(? j|?i)
(16)
where t(?) is a lexicon model trained from the word
alignment annotated bilingual corpus discussed in
Section 2.1. The alignment a also includes non-
terminal correspondence with t(X k |X k ) = 1. If we
observed multiple alignment instances for ? and ?,
then, we take the maximum of the weights.
pw(?|?) = max
a
pw(?|?, a) (17)
4.3 Language Model
We used mixed-cased n-gram language model. In
case of 5-gram language model, the feature func-
tion is expressed as follows:
hlm(eI1) = log
?
i
pn(ei|ei?4ei?3ei?2ei?1) (18)
4.4 Reordering Models
In order to limit the reorderings, two feature func-
tions are employed based on the backtracking of
rules during the top-down parsing on foreign lan-
guage side.
hh(eI1, f J1 ,D) =
?
Di?back(D)
height(Di) (19)
hw(eI1, f J1 ,D) =
?
Di?back(D)
width(Di) (20)
where back(D) is a set of subtrees backtracked
during the derivation of D, and height(Di) and
width(Di) refer the height and width of subtreeDi,
respectively. In Figure 1(b), for instance, a rule of
X 1 with non-terminals X 2 and X 4 , two rules X 2
and X 3 spanning two terminal symbols should be
backtracked to proceed to X 4 . The rationale is that
positive scaling factors prefer a deeper structure
whereby negative scaling factors prefer a mono-
tonized structure.
4.5 Length-based Models
Three trivial length-based feature functions were
used in our experiment.
hl(eI1) = I (21)
hr(D) = rule(D) (22)
hp(D) = phrase(D) (23)
Table 1: Japanese/English news corpus
Japanese English
train sentence 175,384
dictionary + 1,329,519
words 8,373,478 7,222,726
vocabulary 297,646 397,592
dev. sentence 1,500
words 47,081 39,117
OOV 45 149
test sentence 1,500
words 47,033 38,707
OOV 51 127
Table 2: Phrases/rules extracted from the
Japanese/English bilingual corpus. Figures do not
include phrase-based rules.
# rules/phrases
Phrase 5,433,091
Normalized-2 6,225,630
Normalized-3 6,233,294
Hierarchical 12,824,387
where rule(D) and phrase(D) are the number
of production rules extracted in Section 2.1 and
phrase-based rules generalized in Section 2.2, re-
spectively. The English length feature function
controls the length of output sentence. Two feature
functions based on rule?s counts are hypothesized
to control whether to incorporate a production rule
or a phrase-based rule into D.
5 Experiments
The bilingual corpus used for our experiments was
obtained from an automatically sentence aligned
Japanese/English Yomiuri newspaper corpus con-
sisting of 180K sentence pairs (refer to Table
1) (Utiyama and Isahara, 2003). From one-to-
one aligned sentences, 1,500 sentence pairs were
sampled for a development set and a test set1.
Since the bilingual corpus is rather small, es-
pecially for the newspaper translation domain,
Japanese/English dictionaries consisting of 1.3M
entries were added into a training set to alleviate
an OOV problem2.
Word alignments were annotated by a HMM
translation model (Och and Ney, 2003). After
1Japanese sentences were segmented by MeCab available
from http://mecab.sourceforge.jp.
2The dictionary entries were compiled from JE-
DICT/JNAMEDICT and an in-house developed dictionary.
782
the annotation via Viterbi alignments with refine-
ments, phrases translation pairs and production
rules were extracted (refer to Table 2). We per-
formed the rule extraction using the hierarchi-
cal phrase-based constraint (Hierarchical) and our
proposed target-normalized form with 2 and 3
non-terminals (Normalized-2 and Normalized-3).
Phrase translation pairs were also extracted for
comparison (Phrase). We did not threshold the
extracted phrases or rules by their length. Ta-
ble 2 shows that Normalized-2 extracted slightly
larger number of rules than those for phrase-
based model. Including three non-terminals did
not increase the grammar size. The hierarchical
phrase-based translation model extracts twice as
large as our target-normalized formalism. The
target-normalized form is restrictive in that non-
terminals should be consecutive for the English-
side. This property prohibits spuriously extracted
production rules.
Mixed-casing 3-gram/5-gram language models
were estimated from LDC English GigaWord 2 to-
gether with the 100K English articles of Yomiuri
newspaper that were used neither for development
nor test sets 3.
We run the decoder for the target-normalized
hierarchical phrase-based model consisting of at
most two non-terminals, since adding rules with
three non-terminals did not increase the grammar
size. ITG-constraint simulated phrase-based rules
were also included into our grammar. The foreign
word span size was thresholded so that at least one
non-terminal should span at most 7 words.
Our phrase-based model employed all feature
functions for the hierarchical phrase-based system
with additional feature functions:
? A distortion model that penalizes the re-
ordering of phrases by the number of words
skipped | j ? ( j? + m?) ? 1|, where j is the for-
eign word position for a phrase f j+mj trans-
lated immediately after a phrase for f j?+m?j?
(Koehn et al, 2003).
? Lexicalized reordering models constrain the
reordering of phrases whether to favor mono-
tone, swap or discontinuous positions (Till-
man, 2004).
The phrase-based decoder?s reordering was con-
strained by ITG-constraints with a window size of
3We used SRI ngram language modeling toolkit with lim-
ited vocabulary size.
Table 3: Results for the Japanese-to-English
newswire translation task.
BLEU NIST
[%]
Phrase 3-gram 7.14 3.21
5-gram 7.33 3.19
Normalized-2 3-gram 10.00 4.11
5-gram 10.26 4.20
7.
The translation results are summarized in Table
3. Two systems were contrasted by 3-gram and 5-
gram language models. Results were evaluated by
ngram precision based metrics, BLEU and NIST,
on the casing preserved single reference test set.
Feature function scaling factors for each system
were optimized on BLEU score under the devel-
opment set using a downhill simplex method. The
differences of translation qualities are statistically
significant at the 95% confidence level (Koehn,
2004). Although the figures presented in Table
3 are rather low, we found that Normalized-2 re-
sulted in statistically significant improvement over
Phrase. Figure 3 shows some translation results
from the test set.
6 Conclusion
The target-normalized hierarchical phrase-based
model is based on a more general hierarchical
phrase-based model (Chiang, 2005). The hier-
archically combined phrases can be regarded as
an instance of phrase-based model with a place
holder to constraint reordering. Such reorder-
ing was realized either by an additional constraint
for decoding, such as window constraints, IBM
constraints or ITG-constraints (Zens and Ney,
2003), or by lexicalized reordering feature func-
tions (Tillman, 2004). In the hierarchical phrase-
based model, such reordering is explicitly repre-
sented in each rule.
As experimented in Section 5, the use of the
target-normalized form reduced the grammar size,
but still outperformed a phrase-based system.
Furthermore, the target-normalized form coupled
with our top down parsing on the foreign lan-
guage side allows an easier integration with ngram
language model. A decoder can be implemented
based on a phrase-based model by employing a
stack structure to keep track of untranslated for-
eign word spans.
The target-normalized form can be interpreted
783
Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes .
Phrase: At the same time , it never mistakes that it is necessary to learn lessons from the history of criminal .
Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .
Reference: The ministries will dispatch design and construction experts to China to train local engineers and to
research technology that is appropriate to China?s economic situation .
Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China and
its design methods by exception of study .
Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , construction
design and construction methods of the recipient from .
Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief
measures to the village and the city of Niigata .
Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law .
Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata .
Figure 3: Sample translations from two systems: Phrase and Normalized-2
as a set of rules that reorders the foreign lan-
guage to match with English language sequen-
tially. Collins et al (2005) presented a method
with hand-coded rules. Our method directly learns
such serialization rules from a bilingual corpus
without linguistic clues.
The translation quality presented in Section 5
are rather low due to the limited size of the bilin-
gual corpus, and also because of the linguistic dif-
ference of two languages. As our future work,
we are in the process of experimenting our model
for other languages with rich resources, such as
Chinese and Arabic, as well as similar language
pairs, such as French and English. Additional
feature functions will be also investigated that
were proved successful for phrase-based models
together with feature functions useful for a tree-
based modeling.
Acknowledgement
We would like to thank to our colleagues, espe-
cially to Hideto Kazawa and Jun Suzuki, for useful
discussions on the hierarchical phrase-based trans-
lation.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL 2005, pages 263?270, Ann Arbor, Michigan,
June.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL 2005, pages 531?540,
Ann Arbor, Michigan, June.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with
hooks. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 65?73,
Vancouver, British Columbia, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL
2003, pages 160?167.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proc. of ACL 2003, pages
72?79.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proc. of ACL 2003, pages 144?151.
784
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 341?344,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Succinct N-gram Language Model
Taro Watanabe Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Efficient processing of tera-scale text data
is an important research topic. This pa-
per proposes lossless compression of N -
gram language models based on LOUDS,
a succinct data structure. LOUDS suc-
cinctly represents a trie with M nodes as a
2M + 1 bit string. We compress it further
for the N -gram language model structure.
We also use ?variable length coding? and
?block-wise compression? to compress val-
ues associated with nodes. Experimental
results for three large-scale N -gram com-
pression tasks achieved a significant com-
pression rate without any loss.
1 Introduction
There has been an increase in available N -gram
data and a large amount of web-scaled N -gram
data has been successfully deployed in statistical
machine translation. However, we need either a
machine with hundreds of gigabytes of memory
or a large computer cluster to handle them.
Either pruning (Stolcke, 1998; Church et al,
2007) or lossy randomizing approaches (Talbot
and Brants, 2008) may result in a compact repre-
sentation for the application run-time. However,
the lossy approaches may reduce accuracy, and
tuning is necessary. A lossless approach is obvi-
ously better than a lossy one if other conditions
are the same. In addtion, a lossless approach can
easly combined with pruning. Therefore, lossless
representation of N -gram is a key issue even for
lossy approaches.
Raj and Whittaker (2003) showed a general N -
gram language model structure and introduced a
lossless algorithm that compressed a sorted integer
vector by recursively shifting a certain number of
bits and by emitting index-value inverted vectors.
However, we need more compact representation.
In this work, we propose a succinct way to
represent the N -gram language model structure
based on LOUDS (Jacobson, 1989; Delpratt et
al., 2006). It was first introduced by Jacobson
(1989) and requires only a small space close to
the information-theoretic lower bound. For an M
node ordinal trie, its information-theoretical lower
bound is 2M ? O(lg M) bits (lg(x) = log
2
(x))
1-gram 2-gram 3-gram
probability
back-off
pointer
word idprobabilityback-offpointer
word id
probability
back-off
pointer
Figure 1: Data structure for language model
and LOUDS succinctly represents it by a 2M + 1
bit string. The space is further reduced by consid-
ering the N -gram structure. We also use variable
length coding and block-wise compression to com-
press the values associated with each node, such as
word ids, probabilities or counts.
We experimented with English Web 1T 5-gram
from LDC consisting of 25 GB of gzipped raw
text N -gram counts. By using 8-bit floating point
quantization 1, N -gram language models are com-
pressed into 10 GB, which is comparable to a lossy
representation (Talbot and Brants, 2008).
2 N -gram Language Model
We assume a back-off N -gram language model in
which the conditional probability Pr(w
n
|w
n?1
1
)
for an arbitrary N -gram wn
1
= (w
1
, ..., w
n
) is re-
cursively computed as follows.
?(w
n
1
) if wn
1
exists.
?(w
n?1
1
)Pr(w
n
|w
n?1
2
) if wn?1
1
exists.
Pr(w
n
|w
n?1
2
) otherwise.
?(w
n
1
) and ?(wn
1
) are smoothed probabilities and
back-off coefficients, respectively.
The N -grams are stored in a trie structure as
shown in Figure 1. N -grams of different orders
are stored in different tables and each row corre-
sponds to a particular wn
1
, consisting of a word id
for w
n
, ?(w
n
1
), ?(w
n
1
) and a pointer to the first po-
sition of the succeeding (n + 1)-grams that share
the same prefix wn
1
. The succeeding (n+1)-grams
are stored in a contiguous region and sorted by the
word id of w
n+1
. The boundary of the region is de-
termined by the pointer of the next N -gram in the
1The compact representation of the floating point is out of
the scope of this paper. Therefore, we use the term lossless
even when using floating point quantization.
341
0
1 2 3 4
5 6 7 8 9 10
11 12 13 14 15
(a) Trie structure
node id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
LOUDS bit 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0
(b) Corresponding LOUDS bit string
0 1 2 3
4 5 6 7 8 9
10 11 12 13 14
(c) Trie structure for N -gram
node id 0 1 2 3 4 5 6 7 8 9
bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
LOUDS bit 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0
(d) Corresponding N -gram optimized LOUDS bit string
Figure 2: Optimization of LOUDS bit string for N -gram data
row. When an N -gram is traversed, binary search
is performed N times. If each word id corresponds
to its node position in the unigram table, we can
remove the word ids for the first order.
Our implementation merges across different or-
ders of N -grams, then separates into multiple ta-
bles such as word ids, smoothed probabilities,
back-off coefficients, and pointers. The starting
positions of different orders are memorized to al-
low access to arbitrary orders. To store N -gram
counts, we use three tables for word ids, counts
and pointers. We share the same tables for word
ids and pointers with additional probability and
back-off coefficient tables.
To support distributed computation (Brants et
al., 2007), we further split the N -gram data into
?shards? by hash values of the first bigram. Uni-
gram data are shared across shards for efficiency.
3 Succinct N -gram Structure
The table of pointers described in the previous
section represents a trie. We use a succinct data
structure LOUDS (Jacobson, 1989; Delpratt et al,
2006) for compact representation of the trie.
For an M node ordinal trie, there exist
1
2M+1
(
2M+1
M
)
different tries. Therefore,
its information-theoretical lower bound is
lg
?
1
2M+1
(
2M+1
M
)
?
? 2M ? O(lg M) bits.
LOUDS represents a trie with M nodes as a
2M + O(M) bit string.
The LOUDS bit string is constructed as follows.
Starting from the root node, we traverse a trie in
level order. For each node with d ? 0 children, the
bit string 1d0 is emitted. In addition, 10 is prefixed
to the bit string emitted by an imaginary super-root
node pointing to the root node. Figure 2(a) shows
an example trie structure. The nodes are numbered
in level order, and from left to right. The cor-
responding LOUDS bit string is shown in Figure
2(b). Since the root node 0 has four child nodes,
it emits four 1s followed by 0, which marks the
end of the node. Before the root node, we assume
an imaginary super root node emits 10 for its only
child, i.e., the root node. After the root node, its
first child or node 1 follows. Since (M + 1)0s and
M1s are emitted for a trie with M nodes, LOUDS
occupies 2M + 1 bits.
We define a basic operation on the bit string.
sel
1
(i) returns the position of the i-th 1. We can
also define similar operations over zero bit strings,
sel
0
(i). Given sel
b
, we define two operations for
a node x. parent(x) gives x?s parent node and
firstch(x) gives x?s first child node:
parent(x) = sel
1
(x + 1) ? x ? 1, (1)
firstch(x) = sel
0
(x + 1) ? x. (2)
To test whether a child node exists, we sim-
ply check firstch(x) 6= firstch(x + 1). Sim-
ilarly, the child node range is determined by
[firstch(x),firstch(x + 1)).
3.1 Optimizing N -gram Structure for Space
We propose removing redundant bits from the
baseline LOUDS representation assuming N -
gram structures. Since we do not store any infor-
mation in the root node, we can safely remove the
root so that the imaginary super-root node directly
points to unigram nodes. The node ids are renum-
bered and the first unigram is 0. In this way, 2 bits
are saved.
The N -gram data structure has a fixed depth N
and takes a flat structure. Since the highest or-
der N -grams have no child nodes, they emit 0NN
in the tail of the bit stream, where N
n
stands for
the number of n-grams. By memorizing the start-
ing position of the highest order N -grams, we can
completely remove N
N
bits.
The imaginary super-root emits 1N10 at the be-
ginning of the bit stream. By memorizing the bi-
gram starting position, we can remove the N
1
+ 1
bits.
Finally, parent(x) and firstch(x) are rewritten as
342
integer seq. 52 156 260 364
coding 0x34 0x9c 0x01 0x04 0x01 0x6c
boundary 1 1 0 1 0 1
Figure 3: Example of variable length coding
follows:
parent(x) = sel
1
(x + 1 ?N
1
) + N
1
? x, (3)
firstch(x) = sel
0
(x) + N
1
+ 1 ? x. (4)
Figure 2(c) shows the N -gram optimized trie
structure (N = 3) from Figure 2 with N
1
= 4
and N
3
= 5. The parent of node 8 is found by
sel
1
(8+1?4) = 5 and 5+4?8 = 1. The first child
is located by sel
0
(8) = 16 and 16+4+1?8 = 13.
When accessing the N -gram data structure,
sel
b
(i) operations are used extensively. We use an
auxiliary dictionary structure proposed by Kim et
al. (2005) and Jacobson (1989) that supports an
efficient sel
1
(i) (sel
0
(i)) with the dictionary. We
omit the details due to lack of space.
3.2 Variable Length Coding
The above method compactly represents pointers,
but not associated values, such as word ids or
counts. Raj and Whittaker (2003) proposed in-
teger compression on each range of the word id
sequence that shared the same N -gram prefix.
Here, we introduce a simple but more effec-
tive variable length coding for integer sequences
of word ids and counts. The basic idea comes from
encoding each integer by the smallest number of
required bytes. Specifically, an integer within the
range of 0 to 255 is coded as a 1-byte integer,
the integers within the range of 256 to 65,535 are
stored as 2-byte integers, and so on. We use an ad-
ditional bit vector to indicate the boundary of the
byte sequences. Figure 3 presents an example in-
teger sequence, 52, 156, 260 and 364 with coded
integers in hex decimals with boundary bits.
In spite of the length variability, the system
can directly access a value at index i as bytes
in [sel
1
(i) + 1, sel
1
(i + 1) + 1) by the efficient
sel
1
operation assuming that sel
1
(0) yields ?1.
For example, the value 260 at index 2 in Figure
3 is mapped onto the byte range of [sel
1
(2) +
1, sel
1
(3) + 1) = [2, 4).
3.3 Block-wise Compression
We further compress every 8K-byte data block of
all tables in N -grams by using a generic com-
pression library, zlib, employed in UNIX gzip.
We treat a sequence of 4-byte floats in the prob-
ability table as a byte stream, and compress ev-
ery 8K-byte block. To facilitate random access to
the compressed block, we keep track of the com-
pressed block?s starting offsets. Since the offsets
are in sorted order, we can apply sorted integer
compression (Raj and Whittaker, 2003). Since N -
gram language model access preserves some local-
ity, N -gram with block compression is still practi-
cal enough to be usable in our system.
4 Experiments
We applied the proposed representation to 5-gram
trained by ?English Gigaword 3rd Edition,? ?En-
glish Web 1T 5-gram? from LDC, and ?Japanese
Web 1T 7-gram? from GSK. Since their tendencies
are the same, we only report in this paper the re-
sults on English Web 1T 5-gram, where the size
of the count data in gzipped raw text format is
25GB, the number of N-grams is 3.8G, the vocab-
ulary size is 13.6M words, and the number of the
highest order N-grams is 1.2G.
We implemented an N -gram indexer/estimator
using MPI inspired by the MapReduce imple-
mentation of N -gram language model index-
ing/estimation pipeline (Brants et al, 2007).
Table 1 summarizes the overall results. We
show the initial indexed counts and the final lan-
guage model size by differentiating compression
strategies for the pointers, namely the 4-byte raw
value (Trie), the sorted integer compression (In-
teger) and our succinct representation (Succinct).
The ?block? indicates block compression. For the
sake of implementation simplicity, the sorted in-
teger compression used a fixed 8-bit shift amount,
although the original paper proposed recursively
determined optimum shift amounts (Raj and Whit-
taker, 2003). 8-bit quantization was performed
for probabilities and back-off coefficients using a
simple binning approach (Federico and Cettolo,
2007).
N -gram counts were reduced from 23.59GB
to 10.57GB by our succinct representation with
block compression. N -gram language models of
42.65GB were compressed to 18.37GB. Finally,
the 8-bit quantized N -gram language models are
represented by 9.83GB of space.
Table 2 shows the compression ratio for the
pointer table alone. Block compression employed
on raw 4-byte pointers attained a large reduc-
tion that was almost comparable to sorted inte-
ger compression. Since large pointer value tables
are sorted, even a generic compression algorithm
could achieve better compression. Using our suc-
cinct representation, 2.4 bits are required for each
N -gram. By using the ?flat? trie structure, we
approach closer to its information-theoretic lower
bound beyond the LOUDS baseline. With block
compression, we achieved 1.8 bits per N -gram.
Table 3 shows the effect of variable length
coding and block compression for the word ids,
counts, probabilities and back-off coefficients. Af-
ter variable-length coding, the word id is almost
half its original size. We assign a word id for each
343
w/o block w/ block
Counts Trie 23.59 GB 12.21 GB
Integer 14.59 GB 11.18 GB
Succinct 12.62 GB 10.57 GB
Language Trie 42.65 GB 20.01 GB
model Integer 33.65 GB 18.98 GB
Succinct 31.67 GB 18.37 GB
Quantized Trie 24.73 GB 11.47 GB
language Integer 15.73 GB 10.44 GB
model Succinct 13.75 GB 9.83 GB
Table 1: Summary of N -gram compression
total per N -gram
4-byte Pointer 12.04 GB 27.24 bits
+block compression 2.42 GB 5.48 bits
Sorted Integer 3.04 GB 6.87 bits
+block compression 1.39 GB 3.15 bits
Succinct 1.06 GB 2.40 bits
+block compression 0.78 GB 1.76 bits
Table 2: Compression ratio for pointers
word according to its reverse sorted order of fre-
quency. Therefore, highly frequent words are as-
signed smaller values, which in turn occupies less
space in our variable length coding. With block
compression, we achieved further 1 GB reduction
in space. Since the word id sequence preserves
local ordering for a certain range, even a generic
compression algorithm is effective.
The most frequently observed count in N -gram
data is one. Therefore, we can reduce the space
by the variable length coding. Large compression
rates are achieved for both probabilities and back-
off coefficients.
5 Conclusion
We provided a succinct representation of the N -
gram language model without any loss. Our
method approaches closer to the information-
theoretic lower bound beyond the LOUDS base-
line. Experimental results showed our succinct
representation drastically reduces the space for
the pointers compared to the sorted integer com-
pression approach. Furthermore, the space of
N -grams was significantly reduced by variable
total per N -gram
word id size (4 bytes) 14.09 GB 31.89 bits
+variable length 6.72 GB 15.20 bits
+block compression 5.57 GB 12.60 bits
count size (8 bytes) 28.28 GB 64.00 bits
+variable length 4.85 GB 10.96 bits
+block compression 4.22 GB 9.56 bits
probability size (4 bytes) 14.14 GB 32.00 bits
+block compression 9.55 GB 21.61 bits
8-bit quantization 3.54 GB 8.00 bits
+block compression 2.64 GB 5.97 bits
backoff size (4 bytes) 9.76 GB 22.08 bits
+block compression 2.48 GB 5.61 bits
8-bit quantization 2.44 GB 5.52 bits
+block compression 0.85 GB 1.92 bits
Table 3: Effects of block compression
length coding and block compression. A large
amount of N -gram data is reduced from unin-
dexed gzipped 25 GB text counts to 10 GB of
indexed language models. Our representation is
practical enough though we did not experimen-
tally investigate the runtime efficiency in this pa-
per. The proposed representation enables us to
utilize a web-scaled N -gram in our MT compe-
tition system (Watanabe et al, 2008). Our suc-
cinct representation will encourage new research
on web-scaled N -gram data without requiring a
larger computer cluster or hundreds of gigabytes
of memory.
Acknowledgments
We would like to thank Daisuke Okanohara for his
open source implementation and extensive docu-
mentation of LOUDS, which helped our original
coding.
References
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In Proc. of EMNLP-CoNLL 2007.
K. Church, T. Hart, and J. Gao. 2007. Compressing
trigram language models with Golomb coding. In
Proc. of EMNLP-CoNLL 2007.
O. Delpratt, N. Rahman, and R. Raman. 2006. Engi-
neering the LOUDS succinct tree representation. In
Proc. of the 5th International Workshop on Experi-
mental Algorithms.
M. Federico and M. Cettolo. 2007. Efficient handling
of n-gram language models for statistical machine
translation. In Proc. of the 2nd Workshop on Statis-
tical Machine Translation.
G. Jacobson. 1989. Space-efficient static trees and
graphs. In 30th Annual Symposium on Foundations
of Computer Science, Nov.
D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Ef-
ficient implementation of rank and select functions
for succinct representation. In Proc. of the 5th Inter-
national Workshop on Experimental Algorithms.
B. Raj and E. W. D. Whittaker. 2003. Lossless com-
pression of language model structure and word iden-
tifiers. In Proc. of ICASSP 2003, volume 1.
A. Stolcke. 1998. Entropy-based pruning of backoff
language models. In Proc. of the ARPA Workshop
on Human Language Technology.
D. Talbot and T. Brants. 2008. Randomized language
models via perfect hash functions. In Proc. of ACL-
08: HLT.
T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT
SMT system 2008 at NTCIR-7. In Proc. of the 7th
NTCIR Workshop, pages 420?422.
344
Proceedings of the Workshop on Statistical Machine Translation, pages 122?125,
New York City, June 2006. c?2006 Association for Computational Linguistics
NTT System Description for the WMT2006 Shared Task
Taro Watanabe Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun,
Kyoto, Japan 619-0237
{taro,tsukada,isozaki}@kecl.ntt.co.jp
Abstract
We present two translation systems ex-
perimented for the shared-task of ?Work-
shop on Statistical Machine Translation,?
a phrase-based model and a hierarchical
phrase-based model. The former uses a
phrasal unit for translation, whereas the
latter is conceptualized as a synchronous-
CFG in which phrases are hierarchically
combined using non-terminals. Experi-
ments showed that the hierarchical phrase-
based model performed very comparable
to the phrase-based model. We also report
a phrase/rule extraction technique differ-
entiating tokenization of corpora.
1 Introduction
We contrasted two translation methods for the
Workshop on Statistical Machine Translation
(WMT2006) shared-task. One is a phrase-based
translation in which a phrasal unit is employed
for translation (Koehn et al, 2003). The other is
a hierarchical phrase-based translation in which
translation is realized as a set of paired production
rules (Chiang, 2005). Section 2 discusses those two
models and details extraction algorithms, decoding
algorithms and feature functions.
We also explored three types of corpus pre-
processing in Section 3. As expected, different
tokenization would lead to different word align-
ments which, in turn, resulted in the divergence
of the extracted phrase/rule size. In our method,
phrase/rule translation pairs extracted from three
distinctly word-aligned corpora are aggregated into
one large phrase/rule translation table. The experi-
ments and the final translation results are presented
in Section 4.
2 Translation Models
We used a log-linear approach (Och and Ney,
2002) in which a foreign language sentence f J1 =f1, f2, ... fJ is translated into another language, i.e.
English, eI1 = e1, e2, ..., eI by seeking a maximum
likelihood solution of
e?I1 = argmax
eI1
Pr(eI1| f J1 ) (1)
= argmax
eI1
exp
(
?M
m=1 ?mhm(eI1, f J1 )
)
?
e? I
?
1
exp
(
?M
m=1 ?mhm(e? I
?
1 , f J1 )
)(2)
In this framework, the posterior probability
Pr(eI1| f J1 ) is directly maximized using a log-linear
combination of feature functions hm(eI1, f J1 ), such
as a ngram language model or a translation model.
When decoding, the denominator is dropped since it
depends only on f J1 . Feature function scaling factors
?m are optimized based on a maximum likelihood
approach (Och and Ney, 2002) or on a direct error
minimization approach (Och, 2003). This modeling
allows the integration of various feature functions
depending on the scenario of how a translation is
constituted.
In a phrase-based statistical translation (Koehn
et al, 2003), a bilingual text is decomposed as K
phrase translation pairs (e?1, ?fa?1), (e?2, ?fa?2 ), ...: The in-
put foreign sentence is segmented into phrases ?f K1 ,
122
mapped into corresponding English e?K1 , then, re-
ordered to form the output English sentence accord-
ing to a phrase alignment index mapping a?.
In a hierarchical phrase-based translation (Chi-
ang, 2005), translation is modeled after a weighted
synchronous-CFG consisting of production rules
whose right-hand side is paired (Aho and Ullman,
1969):
X ? ??, ?,??
where X is a non-terminal, ? and ? are strings of ter-
minals and non-terminals. ? is a one-to-one corre-
spondence for the non-terminals appeared in ? and
?. Starting from an initial non-terminal, each rule
rewrites non-terminals in ? and ? that are associated
with ?.
2.1 Phrase/Rule Extraction
The phrase extraction algorithm is based on those
presented by Koehn et al (2003). First, many-
to-many word alignments are induced by running
a one-to-many word alignment model, such as
GIZA++ (Och and Ney, 2003), in both directions
and by combining the results based on a heuristic
(Och and Ney, 2004). Second, phrase translation
pairs are extracted from the word aligned corpus
(Koehn et al, 2003). The method exhaustively ex-
tracts phrase pairs ( f j+mj , ei+ni ) from a sentence pair
( f J1 , eI1) that do not violate the word alignment con-
straints a.
In the hierarchical phrase-based model, produc-
tion rules are accumulated by computing ?holes? for
extracted contiguous phrases (Chiang, 2005):
1. A phrase pair ( ?f , e?) constitutes a rule:
X ?
?
?f , e?
?
2. A rule X ? ??, ?? and a phrase pair ( ?f , e?) s.t.
? = ?? ?f??? and ? = ??e???? constitutes a rule:
X ?
?
?? X k ?
??, ?? X k ?
??
?
2.2 Decoding
The decoder for the phrase-based model is a left-to-
right generation decoder with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words. The decoding process is very
similar to those described in (Koehn et al, 2003):
It starts from an initial empty hypothesis. From an
existing hypothesis, new hypothesis is generated by
consuming a phrase translation pair that covers un-
translated foreign word positions. The score for the
newly generated hypothesis is updated by combin-
ing the scores of feature functions described in Sec-
tion 2.3. The English side of the phrase is simply
concatenated to form a new prefix of English sen-
tence.
In the hierarchical phrase-based model, decoding
is realized as an Earley-style top-down parser on the
foreign language side with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words (Watanabe et al, 2006). The ma-
jor difference to the phrase-based model?s decoder is
the handling of non-terminals, or holes, in each rule.
2.3 Feature Functions
Our phrase-based model uses a standard pharaoh
feature functions listed as follows (Koehn et al,
2003):
? Relative-count based phrase translation proba-
bilities in both directions.
? Lexically weighted feature functions in both di-
rections.
? The supplied trigram language model.
? Distortion model that counts the number of
words skipped.
? The number of words in English-side and the
number of phrases that constitute translation.
For details, please refer to Koehn et al (2003).
In addition, we added three feature functions to
restrict reorderings and to represent globalized in-
sertion/deletion of words:
? Lexicalized reordering feature function scores
whether a phrase translation pair is monotoni-
cally translated or not (Och et al, 2004):
hlex(a?K1 | ?f K1 , e?K1 ) = log
K
?
k=1
pr(?k | ?fa?k , e?k) (3)
where ?k = 1 iff a?k ? a?k?1 = 1 otherwise ?k = 0.
? Deletion feature function penalizes words that
do not constitute a translation according to a
123
Table 1: Number of word alignment by different preprocessings.
de-en es-en fr-en en-de en-es en-fr
lower 17,660,187 17,221,890 16,176,075 17,596,764 17,237,723 16,220,520
stem 17,110,890 16,601,306 15,635,900 17,052,808 16,597,274 15,658,940
prefix4 16,975,398 16,540,767 15,610,319 16,936,710 16,530,810 15,613,755
intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242
union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570
Table 2: Number of phrases extracted from differently preprocessed corpora.
de-en es-en fr-en en-de en-es en-fr
lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497
stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045
prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947
merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840
lexicon model t( f |e) (Bender et al, 2004):
hdel(eI1, f J1 ) =
J
?
j=1
[
max
0?i?I
t( f j|ei) < ?del
]
(4)
The deletion model simply counts the number
of words whose lexicon model probability is
lower than a threshold ?del. Likewise, we also
added an insertion model hins(eI1, f J1 ) that pe-
nalizes the spuriously inserted English words
using a lexicon model t(e| f ).
For the hierarchical phrase-based model, we em-
ployed the same feature set except for the distortion
model and the lexicalized reordering model.
3 Phrase Extraction from Different Word
Alignment
We prepared three kinds of corpora differentiated
by tokenization methods. First, the simplest pre-
processing is lower-casing (lower). Second, corpora
were transformed by a Porter?s algorithm based mul-
tilingual stemmer (stem) 1. Third, mixed-cased cor-
pora were truncated to the prefix of four letters of
each word (prefix4). For each differently tokenized
corpus, we computed word alignments by a HMM
translation model (Och and Ney, 2003) and by a
word alignment refinement heuristic of ?grow-diag-
final? (Koehn et al, 2003). Different preprocessing
yields quite divergent alignment points as illustrated
in Table 1. The table also shows the numbers for
the intersection and union of three alignment anno-
tations.
The (hierarchical) phrase translation pairs are ex-
tracted from three distinctly word aligned corpora.
1We used the Snowball stemmer from http://snowball.
tartarus.org
In this process, each word is recovered into its lower-
cased form. The associated counts are aggregated
to constitute relative count-based feature functions.
Table 2 summarizes the size of phrase tables in-
duced from the corpora. The number of rules ex-
tracted for the hierarchical phrase-based model was
roughly twice as large as those for the phrase-based
model. Fewer word alignments resulted in larger
phrase translation table size as observed in the ?pre-
fix4? corpus. The size is further increased by our
aggregation step (merged).
Different induction/refinement algorithms or pre-
processings of a corpus bias word alignment. We
found that some word alignments were consistent
even with different preprocessings, though we could
not justify whether such alignments would match
against human intuition. If we could trust such
consistently aligned words, reliable (hierarchical)
phrase translation pairs would be extracted, which,
in turn, would result in better estimates for relative
count-based feature functions. At the same time, dif-
ferently biased word alignment annotations suggest
alternative phrase translation pairs that is useful for
increasing the coverage of translations.
4 Results
Table 3 shows the open test translation results on
2005 and 2006 test set (the development-test set and
the final test set) 2. We used the merged (hierar-
chical) phrase tables for decoding. Feature function
scaling factors were optimized on BLEU score us-
ing the supplied development set that is identical to
the 2005?s development set. We observed that our
2We did not differetiated in-domain or out-of-domain for
2006 test set.
124
Table 3: Open test on the 2005/2006 test sets (BLEU [%]).
de-en es-en fr-en en-de en-es en-fr
test2005 Phrase 25.72 30.97 30.97 18.08 30.48 32.14
Rule 25.14 30.11 30.31 17.96 27.96 31.04
2005?s best 24.77 30.95 30.27
test2006 Phrase 23.16 29.90 27.89 15.79 29.54 29.19
Rule 22.74 28.80 27.28 15.99 26.56 27.86
results are very comparable to the last year?s best re-
sults in test2005. Also found that our hierarchical
phrase-based translation (Rule) performed slightly
inferior to the phrase-based translation (Phrase) in
both test sets. The hierarchically combined phrases
seem to be too flexible to represent the relationship
of similar language pairs. Note that our hierarchical
phrase-based model performed better in the English-
to-German translation task. Those language pair re-
quires rather distorted reordering, which could be
represented by hierarchically combined phrases.
We also conducted additional studies on how
differently aligned corpora might affect the trans-
lation quality on Spanish-to-English task for the
2005 test set. Using our phrase-based model,
the BLEU scores for lower/stem/prefix4 were
30.90/30.89/30.76, respectively. The differences of
translation qualities were statistically significant at
the 95% confidence level. Our phrase translation
pairs aggregated from all the differently prepro-
cessed corpora improved the translation quality.
5 Conclusion
We presented two translation models, a phrase-
based model and a hierarchical phrase-based model.
The former performed as well as the last year?s best
system, whereas the latter performed comparable to
our phrase-based model. We are going to experi-
ment new feature functions to restrict the too flexible
reordering represented by our hierarchical phrase-
based model.
We also investigated different word alignment an-
notations, first using lower-cased corpus, second
performing stemming, and third retaining only 4-
letter prefix. Differently preprocessed corpora re-
sulted in quite divergent word alignment. Large
phrase/rule translation tables were accumulated
from three distinctly aligned corpora, which in turn,
increased the translation quality.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system?. In Proc. of IWSLT 2004, pages 79?84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263?270, Ann Arbor, Michigan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL 2002, pages
295?302.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex a
nd Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings, pages
161?168, Boston, Massachusetts, USA, May 2 - May
7.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING-ACL
2006 (to appear), Sydney, Australia, July.
125
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1908?1917, Dublin, Ireland, August 23-29 2014.
Recurrent Neural Network-based Tuple Sequence Model
for Machine Translation
Youzheng Wu, Taro Watanabe, Chiori Hori
National Institute of Information and Communications Technology (NICT), Japan
erzhengcn@gmail.com
{taro.watanabe, chiori.hori}@nict.go.jp
Abstract
In this paper, we propose a recurrent neural network-based tuple sequence model (RNNTSM)
that can help phrase-based translation model overcome the phrasal independence assumption.
Our RNNTSM can potentially capture arbitrary long contextual information during estimating
probabilities of tuples in continuous space. It, however, has severe data sparsity problem due
to the large tuple vocabulary coupled with the limited bilingual training data. To tackle this
problem, we propose two improvements. The first is to factorize bilingual tuples of RNNTSM
into source and target sides, we call factorized RNNTSM. The second is to decompose phrasal
bilingual tuples to word bilingual tuples for providing fine-grained tuple model. Our extensive
experimental results on the IWSLT2012 test sets
1
showed that the proposed approach essentially
improved the translation quality over state-of-the-art phrase-based translation systems (baselines)
and recurrent neural network language models (RNNLMs). Compared with the baselines, the
BLEU scores on English-French and English-German tasks were greatly enhanced by 2.1%-
2.6% and 1.8%-2.1%, respectively.
1 Introduction
The phrase-based translation systems (Koehn et al., 2003) rely on language model and lexicalized re-
ordering model to capture lexical dependencies that span phrase boundaries. Their translation models,
however, do not explicitly model context dependencies between translation units. To address this limi-
tation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to
capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be
effective in helping the phrase-based translation models overcome the phrasal independence assumption
(Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006;
Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual
tuples also known as minimal translation units (MTUs).
Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural
networks with factorizations to model bilingual tuples in a continuous space. Although the authors
reported some gains over the n-gram model in machine translation tasks, these models can only capture
a limited amount of context and remain a kind of n-gram model. In language modeling, experimental
results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent
neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate
in speech recognition even though it is harder to train properly.
Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose re-
current neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translation
system. Our RNNTSMs are capable of modeling long-span context and have better generalization. Com-
pared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contributions can
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The IWSLT workshop aims at translating TED speeches (http://www.ted.com), a collection of public lectures cov-
ering a variety of topics.
1908
be summarized as: (i) our models can be regarded as deep neural network translation models because
they can capture arbitrary-length context potentially, which are proven to estimate more accurate proba-
bilities of bilingual tuples; (ii) we extend the conventional RNNTSM to factorized RNNTSMs that can
significantly overcome the data sparseness problem caused by the large vocabularies of bilingual tuples
by incorporating the factors from the source and the target sides in addition to bilingual tuples; (iii) we
investigate heuristic rules to decompose phrasal bilingual tuples to word bilingual tuples for reducing
the out-of-tuple-vocabulary rate and providing fine-grained tuple sequence model; (iv) we integrate the
proposed models into the state-of-the-art phrase-based translation system (MOSES) as a supplement of
the work in (Son et al., 2012) that is a complete n-gram translation system.
2 Related Work
The n-gram translation model (Marino et al., 2006) is a Markov model over phrasal bilingual tuples and
can improve the phrase-based translation system (Koehn et al., 2003) by providing contextual depen-
dencies between phrase pairs. To further improve the n-gram translation model, Crego and Yvon (2010)
explored factored bilingual n-gram language models. Durrani et al. (2011) proposed a joint sequence
model for the translation and reordering probabilities. Zhang et al. (2013) explored multiple decomposi-
tion structures as well as dynamic bidirectional decomposition. Since neural networks advance the state
of the art in the fields of image processing, acoustic modeling (Seide et al., 2011), language modeling
(Bengio et al., 2003), natural language processing (Collobert et al., 2011; Socher et al., 2013), machine
transliteration (Deselaers et al., 2009), etc, some prior studies have been done on neural network-based
translation models (NNTMs).
One kind of the NNTMs relies on word-to-word alignment information or phrasal bilingual tuples. For
example, Schwenk et al. (2007) investigated feed-forward neural networks to model bilingual tuples in
continuous space. Son et al. (2012) improved this idea by decomposing tuple units, i.e., distinguishing the
source and target sides of the tuple units, to address data sparsity issues. Although the authors reported
some gains over the n-gram model in the BLEU scores on some tasks, these models can only capture
a limited amount of context and remain a kind of n-gram model. In addition, a feed-forward neural
network independent from bilingual tuples was proposed (Schwenk, 2012), which can infer meaningful
translation probabilities for phrase pairs not seen in the training data.
Another kind of the NNTMs do not rely on alignment. Auli et al. (2013) and Kalchbrenner and
Blunsom (2013) proposed joint language and translation model with recurrent neural networks, in which
latent semantic analysis and convolutional sentence model were used to model source-side sentence.
Potentially, they can exploit an unbounded history of both source and target words thanks to recurrent
connections. However, they only modestly observed gains over the recurrent neural network language
model. Previous studies (Wu and Wang, 2007; Yang et al., 2013) showed that the performance of word
alignment (alignment error rate) is nearly 80%. That means explicit word alignment may be more reliable
as a way to represent the corresponding bilingual sentences compared with an implicit compressed vector
representation (Auli et al., 2013).
Our RNNTSM takes the advantages of the above NNTMs, that is, RNN enables our model to cap-
ture long-span contextual information, while tuple sequence model uses word alignment without much
information loss. Furthermore, factorized RNN and word bilingual tuples are proposed to address data
sparsity issue. To the best of our knowledge, few studies have been done on this aspect.
3 Tuple Sequence Model
In tuple sequence model, bilingual tuples are translation units extracted from word-to-word alignment.
They are composed of source phrases and their aligned target phrases that are also known as minimal
translation units (MTUs) and thus cannot be broken down any further without violating the constrains
of the translation rules. This condition results in a unique segmentation of the bilingual sentence pair
given its alignment. In our implementation, GIZA++ with grow-diag-final-and setting is used
to conduct word-to-word alignments in both directions, source-to-target and target-to-source (Och and
1909
DZRUGWRZRUGDOLJQHGUHVXOWIURP*,=$ZLWKJURZGLDJILQDODQG
FRPSRVHUVDQG
RQW ?W? LQWHUURJ?V
DIULFDQ DPHULFDQ PXVLFLDQV
OHV musiciens DIURDP?ULFDL
LQWHUYLHZHGZHUH
HW FRPSRVLWHXUV
X
PXVLFLDQV
OHVmusiciensW
V
EVHTXHQFHRISKUDVDOELOLQJXDOWXSOHV
DIULFDQDPHULFDQ
DIURDP?ULFDL
V
X
W
X
DQG
HW
V
W
X
FRPSRVHUV
FRPSRVLWHXUV
V
W
X
ZHUHLQWHUYLHZHG
RQW?W?LQWHUURJ?V
V
W
DIULFDQ
DIURDP?ULFDL
V
X X X X XProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479?1488,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Third-order Variational Reranking on Packed-Shared Dependency Forests
Katsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Insutitute of Science and Technology
Ikoma, Nara, 630-0192, Japan
?National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
{katsuhiko-h,masayu-a,matsu}@is.naist.jp
taro.watanabe@nict.go.jp
Abstract
We propose a novel forest reranking algorithm
for discriminative dependency parsing based
on a variant of Eisner?s generative model. In
our framework, we define two kinds of gener-
ative model for reranking. One is learned from
training data offline and the other from a for-
est generated by a baseline parser on the fly.
The final prediction in the reranking stage is
performed using linear interpolation of these
models and discriminative model. In order to
efficiently train the model from and decode
on a hypergraph data structure representing a
forest, we apply extended inside/outside and
Viterbi algorithms. Experimental results show
that our proposed forest reranking algorithm
achieves significant improvement when com-
pared with conventional approaches.
1 Introduction
Recently, much of research on statistical parsing
has been focused on k-best (or forest) reranking
(Collins, 2000; Charniak and Johnson, 2005; Huang,
2008). Typically, reranking methods first generate
a list of top-k candidates (or a forest) from a base-
line system, then rerank the candidates with arbi-
trary features that are intractable within the baseline
system. In the reranking framework, the baseline
system is usually modeled with a generative model,
and a discriminative model is used for reranking.
Sangati et al (2009) reversed the usual order of the
two models for dependency parsing by employing
a generative model to rescore the k-best candidates
provided by a discriminative model. They use a vari-
ant of Eisner?s generative model C (Eisner, 1996b;
Eisner, 1996a) for reranking and extend it to capture
higher-order information than Eisner?s second-order
generative model. Their reranking model showed
large improvements in dependency parsing accu-
racy. They reported that the discriminative model is
very effective at filtering out bad candidates, while
the generative model is able to further refine the se-
lection among the few best candidates.
In this paper, we propose a forest generative
reranking algorithm, opposed to Sangati et al
(2009)?s approach which reranks only k-best candi-
dates. Forests usually encode better candidates more
compactly than k-best lists (Huang, 2008). More-
over, our reranking uses not only a generative model
obtained from training data, but also a sentence spe-
cific generative model learned from a forest. In the
reranking stage, we use linearly combined model
of these models. We call this variational rerank-
ing model. The model proposed in this paper is
factored in the third-order structure, therefore, its
non-locality makes it difficult to perform the rerank-
ing with an usual 1-best Viterbi search. To solve
this problem, we also propose a new search algo-
rithm, which is inspired by the third-order dynamic
programming parsing algorithm (Koo and Collins,
2010). This algorithm enables us an exact 1-best
reranking without any approximation. We summa-
rize our contributions in this paper as follows.
? To extend k-best to forest generative reranking.
? We introduce variational reranking which is a
combination approach of generative reranking
and variational decoding (Li et al, 2009).
? To obtain 1-best tree in the reranking stage, we
1479
propose an exact 1-best search algorithm with
the third-order model.
In experiments on English Penn Treebank data,
we show that our proposed methods bring signif-
icant improvement to dependency parsing. More-
over, our variational reranking framework achieves
consistent improvement, compared to conventional
approaches, such as simple k-best and forest-based
generative reranking algorithms.
2 Dependency Parsing
Given an input sentence x ? X , the task of statis-
tical dependency parsing is to predict output depen-
dencies y? for x. The task is usually modeled within a
discriminative framework, defined by the following
equation:
y? = argmax
y?Y
s(x, y)
= argmax
y?Y
?? ? F(y, x) (1)
where Y is the output space, ? is a parameter vector,
and F() is a set of feature functions.
We denote a set of candidates as G(x). By using
G(x), the conditional probability p(y|x) is typically
derived as follows:
p(y|x) = e
??s(x,y)
Z(x) =
e??s(x,y)?
y?G(x) e??s(x,y)
(2)
where s(x, y) is the score function shown in Eq.1
and ? is a scaling factor to adjust the sharpness of
the distribution and Z(x) is a normarization factor.
2.1 Hypergraph Representation
We propose to encode many hypotheses in a com-
pact representation called dependency forest. While
there may be exponentially many dependency trees,
the forest represents them in polynomial space. A
dependency forest (or tree) can be defined as a hy-
pergraph data strucureHG (Tu et al, 2010).
Figure 1 shows an example of a hypergraph for a
dependency tree. A shaded hyperedge e is defined
as the following form:
e : ?(I1,2, girl3,5,with5,8), saw1,8?.
.
.
.top0,8
..
.
..saw1,8:V
.
.
. .I1,2:N .. . . . .
.
..girl3,5:N
..a3,4:D ..
. .
.
..with5,8:P
..
.
..telescope6,8:N
. ..a6,7:D ..
. .
. .
.e . . . . .
. .
Figure 1: An example of dependency tree for a sentence
?I saw a girl with a telescope?.
The node saw1,8 is a head node of e. The nodes, I1,2,
girl3,5 and with5,8, are tail nodes of e. The hyper-
edge e is an incoming edge for saw1,8 and outgoing
edge for each of I1,2, girl3,5 and with5,81.
More formally, HG(x) of a forest is a pair
?V,E?, where V is a set of nodes and E is a set
of hyperedges. Given a length m sentence x =
(w1 . . . wm), each node v ? V is in the form of
wi,j (= (wi . . . wj+1)) which denotes that a word
w dominates the substring from positions i to j. In
our implementation, each word is paired with POS-
tag tag(w). We denote the root node of dependency
tree y as top. Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the head
and tails(e) ? V + are its dependants. For nota-
tional brevity of algorithmic description, we do not
distinguish left and right tails in the definition, but,
our implementation implicitly distinguishes left tails
tailsL(e) and right tails tailsR(e). We define the set
of incoming edges of a node v as IE(v) and the set
of outgoing edges of a node v as OE(v).
3 Forest Reranking
3.1 Generative Model for Reranking
Given a node v in a dependency tree y, the left and
right children are generated as two separate Markov
sequences, each conditioned on ancestral and sibling
information (context). Like a variation of Eisner?s
generative model C (Eisner, 1996b; Eisner, 1996a),
1In Figure 1, according to custom of dependency tree
description, the direction of hyperedge is written as from
head to tail nodes. However, in this paper, ?incoming? and
?outgoing? have the same meanings as those in (Huang, 2006).
1480
Table 1: An event list of tri-sibling model whose event
space is v|h, sib, tsib, dir, extracted from hyperedge e in
Figure 1. EOC is an end symbol of sequence.
event space
I | saw NONE NONE L
EOC | saw I NONE L
girl | saw NONE NONE R
with | saw girl NONE R
EOC | saw with girl R
the probability of our model q is defined as follows:
q(v) =
|tailsL(e)|?
l=1
q(vl|C(vl)) ? q(vl)
?
|tailsR(e)|?
r=1
q(vr|C(vr)) ? q(vr) (3)
where |tailsL(e)| and |tailsR(e)| are the number of
left and right children of v, vl and vr are the left and
right child of position l and r in each side. C(v) is
a context event space of v. We explain the context
event space later in more detail. The probability of
the entire dependency tree y is recursively computed
by q(y(top)) where y(top) denotes a top node of y.
The probability q(v|C(v)) is dependent on a con-
text space C(v) for a node v. We define two kinds of
context spaces. First, we define a tri-sibling model
whose context space consists of the head node, sib-
ling node, tri-sibling node and direction of a node
v:
q1(v|C(v)) = q1(v|h, sib, tsib, dir) (4)
where h, sib and tsib are head, sibling and tri-sibling
node of v, and dir is a direction of v from h. Table
1 shows an example of an event list of the tri-sibling
model, which is extracted from hyperedge e in Fig-
ure 1. EOC indicates the end of the left or right child
sequence. This is factored in a tri-sibling structure
shown in the left side of Figure 2.
Eq.4 is further decomposed into a product of the
form consisting of three terms:
q1(v|h, sib, tsib, dir) (5)
= q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir)
= q1(tag(v)|h, sib, tsib, dir)
?q1(wrd(v)|tag(v), h, sib, tsib, dir)
?q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir)
where tag(v) and wrd(v) are the POS-tag and word
of v and dist(v, h) is the distance between positions
of v and h. The values of dist(v, h) are partitioned
into 4 categories: 1, 2, 3 ? 6, 7 ??.
Second, following Sangati et al (2009), we define
a grandsibling model whose context space consists
of the head node, sibling node, grandparent node and
direction of a node v.
q2(v|C(v)) = q2(v|h, sib, g, dir) (6)
where g is a grandparent node of v. Analogous to
Eq.5, Eq.6 is decomposed into three terms:
q2(v|h, sib, g, dir) (7)
= q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir)
= q2(tag(v)|h, sib, g, dir)
?q2(wrd(v)|tag(v), h, sib, g, dir)
?q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir)
where notations are the same as those in Eq.5 with
the exception of tri-sibling tsib and grandparent g.
This model is factored in a grandsibling structure
shown in the right side of Figure 2.
The direct estimation of tri-sibling and grandsib-
ling models from a corpus suffers from serious data
sparseness issues. To overcome this, Eisner (1996a)
proposed a back-off strategy which reduces the con-
ditioning of a model. We show the reductions list
for each term of two models in Table 2. The usage
of reductions list is identical to Eisner (1996a) and
readers may refer to it for further details.
The final prediction is performed using a log-
linear interpolated model. It interpolates the base-
line discriminative model and two (tri-sibling and
grandsibling) generative models.
y? = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+ log p(y|x)?base (8)
where ? are parameters to adjust the weight of each
term in prediction. These parameters are tuned using
MERT algorithm (Och, 2003) on development data
using a criterion of accuracy maximization. The rea-
son why we chose MERT is that it effectively tunes
dense parameters with a line search algorithm.
1481
Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word,
POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition;
later reductions throw away more and more of this information.
tri-sibling grandsibling
1-st term 2-nd term 3-rd term 1-st term 2-nd term 3-rd term
wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d
wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d
t(h),wt(sib),t(tsib),d ? ? t(h),wt(sib),t(g),d ? ?wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d
t(h),t(sib),t(tsib),d ? ? t(h),t(sib),t(g),d ? ?
..h .tsib .sib .v ..g .h .sib .v
Figure 2: The left side denotes tri-sibling structure and
the right side denotes grandsibling structure.
Table 3: A summarization of the model factorization and
order
first-order McDonald et al (2005)
second-order Eisner (1996a)
(sibling) McDonald et al (2005)
third-order tri-sibling model
(tri-sibling) Model 2 (Koo and Collins, 2010)
third-order grandsibling model (Sangati et al, 2009)
(grandsibling) Model 1 (Koo and Collins, 2010)
3.2 Exact Search Algorithm
Our baseline discriminative model uses first- and
second-order features provided in (McDonald et al,
2005; McDonald and Pereira, 2006). Therefore,
both our tri-sibling model and baseline discrimina-
tive model integrate local features that are factored
in one hyperedge. On the other hand, the grandsib-
ling model has non-local features because the grand-
parent is not factored in one hyperedge. We sum-
marize the order of each model in Table 3. Our
reranking models are generative versions of Koo and
Collins (2010)?s third-order factorization model.
Non-locality of weight function makes it difficult
to perform the search of Eq.8 with an usual exact
Viterbi 1-best algorithm. One solution to resolve
the intractability is an approximate k-best Viterbi
search. For a constituent parser, Huang (2008) ap-
plied cube pruning techniques to forest reranking
with non-local features. Cube pruning is originally
proposed for the decoding of statistical machine
translation (SMT) with an integrated n-gram lan-
guage model (Chiang, 2007). It is an approximate
k-best Viterbi search algorithm using beam search
and lazy computation (Huang and Chiang, 2005).
In the case of a dependency parser, Koo and
Collins (2010) proposed dynamic-programming-
based third-order parsing algorithm, which enumer-
ates all grandparents with an additional loop. Our
hypergraph based search algorithm for Eq.8 share
the same spirit to their third-order parsing algo-
rithm since the grandsibling model is similar to their
model 1 in that it is factored in grandsibling struc-
ture. Algorithm 1 shows the search algorithm. This
is almost the same bottom-up 1-best Viterbi algo-
rithm except an additional loop in line 4. Line 4 ref-
erences outgoing edge e? of node h from a set of out-
going edges OE(h). tails(e) contains a node v, the
sibling node sib and tri-sibling node tsib of v, more-
over, the head of e? (head(e?)) is the grandparent for
v and sib. Thus, in line 5, we can capture tri-sibling
and grandsibling information and compute the cur-
rent inside estimate of Eq.8.
In our actual implementation, each score of com-
ponents in Eq.8 is represented as a cost. This is writ-
ten as a shortest path search algorithm with a tropi-
cal (real) semiring framework (Mohri, 2002; Huang,
2006). Therefore,? denotes the min operater and?
denotes the + operater. The function f is defined as
follows:
f(d(v1, e), . . . , d(v|e|, e))) =
|e|?
i=1
d(vi, e) (9)
where d(vi, e) denotes the current estimate of the
best cost for a pair of node vi and a hyperedge e.? sums the best cost of a pair of a sub span node
and hyperedge e. Each ctsib and cgsib in line 5 and
7 indicates the cost of tri-sibling and grandsibling
1482
Algorithm 1 Exact DP-Search Algorithm(HG(x))
1: for h ? V in bottom-up topological order do
2: for e ? IE(h) do
3: // tails(e) is {v1, . . . , v|e|
}.
4: for e? ? OE(h) do
5: d(h, e?) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e)) ? cgsib(head(e?), h, tails(e))
6: if h == top then
7: d(h) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e))
model. we indicates the cost of hyperedge e com-
puted from a baseline discriminative model. Lines
6-7 denote the calculation of the best cost for a top
node. We do not compute the cost of the grandsib-
ling model when h is top node because top node has
no outgoing edges.
Our baseline k-best second-order parser is imple-
mented using Huang and Chiang (2005)?s algorithm
2 whose time complexity is O(m3+mk log k). Koo
and Collins (2010)?s third-order parser has O(m4)
time complexity and is theoretically slower than our
baseline k-best parser for a long sentence. Our
search algorithm is based on the third-order parsing
algorithm, but, the search space is previously shrank
by a baseline parser?s k-best approximation and a
forest pruning algorithm presented in the next sec-
tion. Therefore, the time efficiency of our reranking
is unimpaired.
3.3 Forest Pruning
Charniak and Johnson (2005) and Huang (2008)
proposed forest pruning algorithms to reduce the
size of a forest. Huang (2008)?s pruning algo-
rithm uses a 1-best Viterbi inside/outside algorithm
to compute an inside probability ?(v) and an out-
side probability ?(v), while Charniak and Johnson
(2005) use the usual inside/outside algorithm.
In our experiments, we use Charniak and Johnson
(2005)?s forest pruning criterion because the varia-
tional model needs traditional inside/outside proba-
bilities for its ML estimation. We prune away all
hyperedges that have score < ? for a threshold ?.
score = ??(e)?(top) . (10)
Following Huang (2008), we also prune away nodes
with all incoming and outgoing hyperedges pruned.
4 Variational Reranking Model
In place of a maximum a posteriori (MAP) decision
based on Eq.2, the minimum Bayes risk (MBR) deci-
sion rule (Titov and Henderson, 2006) is commonly
used and defined as following equation:
y? = argmin
y?G(x)
?
y??G(x)
loss(y, y?)p(y?|x) (11)
where loss(y, y?) represents a loss function2. As an
alternative to the MBR decision rule, Li et al (2009)
proposed a variational decision rule that rescores
candidates with an approximate distribution q? ? Q.
y? = argmax
y?G(x)
q?(y) (12)
where q? minimizes the KL divergence KL(p||q)
q? = argmin
q?Q
KL(p||q)
= argmax
q?Q
?
y?G(x)
p log q (13)
where each p and q represents p(y|x) and q(y). For
SMT systems, q? is modeled by n-gram language
model over output strings. While the decoding based
on q? is an approximation of intractable MAP de-
coding3, it works as a rescoring function for candi-
dates generated from a baseline model. Here, we
propose to apply the variational decision rule to de-
pendency parsing. For dependency parsing, we can
choose to model q? as the tri-sibling and grandsib-
ling generative models in section 3.
2In case of dependency parsing, Titov and Henderson (2006)
proposed that a loss function is simply defined using a depen-
dency attachment score.
3In SMT, a marginalization of all derivations which yield
a paticular translation needs to be carried out for each trans-
lation. This makes the MAP decoding NP-hard in SMT. This
variational approximate framework can be applied to other tasks
collapsing spurious ambiguity, such as latent-variable parsing
(Matsuzaki et al, 2005).
1483
Algorithm 2 DP-ML Estimation(HG(x))
1: run inside and outside algorithm onHG(x)
2: for v ? V do
3: for e ? IE(v) do
4: ctsib = pe ? ?(v)/?(top)
5: for u ? tails(e) do
6: ctsib = ctsib ? ?(u)
7: for e? ? IE(u) do
8: cgsib = pe ? pe? ? ?(v)/?(top)
9: for u? ? tails(e) \ u do
10: cgsib = cgsib ? ?(u?)
11: for u?? ? tails(e?) do
12: cgsib = cgsib ? ?(u??)
13: for u?? ? tails(e?) do
14: c2(u??|C(u??))+ = cgsib
15: c2(C(u??))+ = cgsib
16: for u ? tails(e) do
17: c1(u|C(u))+ = ctsib
18: c1(C(u))+ = ctsib
19: MLE estimate q?1 , q?2 using formula Eq.14
4.1 ML Estimation from a Forest
q?(v|C(v)) is estimated from a forest using a max-
imum likelihood estimation (MLE). The count of
events is no longer an integer count, but an expected
count under p, which is formulated as follows:
q?(v|C(v)) = c(v|C(v))c(C(v))
=
?
y p(y|x)cv|C(v)(y)?
y p(y|x)cC(v)(y)
(14)
where ce(y) is the number of event e in y. The es-
timation of Eq.14 can be efficiently performed on a
hypergraph data structureHG(x) of a forest.
Algorithm 2 shows the estimation algorithm.
First, it runs the inside/outside algorithm onHG(x).
We denote inside weight for a node v as ?(v) and
outside weight as ?(v). For each hyperedge e, we
denote ctsib as the posterior weight for computing
expected count c1 of events in the tri-sibling model
q?1 . Lines 16-18 compute c1 for all events occuring
in a hyperedge e.
The expected count c2 needed for the estimation
of grandsibling model q?2 is extracted in lines 7-15.
c2 for a grandsibling model must be extracted over
two hyperedges e and e? because it needs grandpar-
ent information. Lines 8-12 show the algorithm to
compute the posterior weight cgsib of e and e?, which
 92
 93
 94
 95
 96
 97
 98
 99
 100
 0  200  400  600  800  1000  1200  1400
Un
la
be
le
d 
Ac
cu
ra
cy
the number of hyperedges per sentence
p=0.001
k=20
k=100
"kbest"
"forest"
Figure 3: The relationship between tha data size (the
number of hyperedges) and oracle scores on develop-
ment data: Forests encode candidates with high accuracy
scores more compactly than k-best lists.
is similar to that to compute the posterior weight
of rules of tree substitution grammars used in tree-
based MT systems (Mi and Huang, 2008). Lines
13-15 compute expected counts c2 of events occur-
ing over two hyperedges e and e?. Finally, line 19
estimates q?1 and q?2 using the form in Eq.14.
Li et al (2009) assumes n-gram locality of the
forest to efficiently train the model, namely, the
baseline n-gram model has larger n than that of vari-
ational n-gram model. In our case, grandsibling lo-
cality is not embedded in the forest generated from
the baseline parser. Therefore, we need to reference
incoming hyperedges of tail nodes in line 7.
y? of Eq.12 may be locally appropriate but glob-
ally inadequate because q? only approximates p.
Therefore, we log-linearly combine q? with a global
generative model estimated from the training data
and the baseline discriminative model.
y? = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+
2?
n=1
log q?n(top(y))?
?
n
+ log p(y|x)?base (15)
Algorithm1 is also applicable to the decoding of
Eq.15. Note that this framework is a combination of
variational decoding and generative reranking. We
call this framework variational reranking.
1484
Table 4: The statistics of forests and 20-best lists on de-
velopment data: this shows the average number of hyper-
edges and nodes per sentence and oracle scores.
forest 20-best
pruning threshold ? = 10?3 ?
ave. num of hyperedges 180.67 255.04
ave. num of nodes 135.74 491.42
oracle scores 98.76 96.78
5 Experiments
Experiments are performed on English Penn Tree-
bank data. We split WSJ part of the Treebank into
sections 02-21 for training, sections 22 for develop-
ment, sections 23 for testing. We use Yamada and
Matsumoto (2003)?s head rules to convert phrase
structure to dependency structure. We obtain k-best
lists and forests generated from the baseline discrim-
inative model which has the same feature set as pro-
vided in (McDonald et al, 2005), using the second-
order Eisner algorithms. We use MIRA for training
as it is one of the learning algorithms that achieves
the best performance in dependency parsing. We set
the scaling factor ? = 1.0.
We also train a generative reranking model from
the training data. To reduce the data sparseness
problem, we use the back-off strategy proposed in
(Eisner, 1996a). Parameters ? are trained using
MERT (Och, 2003) and for each sentence in the de-
velopment data, 300-best dependency trees are ex-
tracted from its forest. Our variational reranking
does not need much time to train the model be-
cause the training is performed over not the train-
ing data (39832 sentences) but the development data
(1700 sentences)4. After MERT was performed un-
til the convergence, the variational reranking finally
achieved a 94.5 accuracy score on development data.
5.1 k-best Lists vs. Forests
Figure 3 shows the relationship between the size of
data structure (the number of hyperedges) and accu-
racy scores on development data. Obviously, forests
can encode a large number of potential candidates
more compactly than k-best lists. This means that
4To generate forests, sentences are parsed only once before
the training. MERT is performed over the forests. We can also
apply a more efficient hypergraph MERT algorithm (Kumar et
al., 2009) to the training than a simple MERT algorithm.
for reranking, there is more possibility of selecting
good candidates in forests than k-best lists.
Table 4 shows the statistics of forests and 20-
best lists on development data. This setting, thresh-
old ? = 10?3 for pruning, is also used for testing.
Forests, which have an average of 180.67 hyper-
edges per sentence, achieve oracle score of 98.76,
which is about 1.0% higher than the 96.78 oracle
score of 20-best lists with 255.04 hyperedges per
sentence. Though the size of forests is smaller than
that of k-best lists, the oracle scores of forests are
much higher than those of k-best lists.
5.2 The Performance of Reranking
First, we compare the performance of variational de-
coding with that of MBR decoding. The results are
shown in Table 5. Variational decoding outperforms
MBR decodings. However, compared with base-
line, the gains of variational and MBR decoding are
small. Second, we also compare the performance of
variational reranking with k-best and forest gener-
ative reranking algorithms. Table 6 shows that our
variational reranking framework achieves the high-
est accuracy scores.
Being different from the decoding framework,
reranking achieves significant improvements. This
result is intuitively reasonable because the rerank-
ing model obtained from training data has the ability
to select a globally consistent candidate, while the
variational approximate model obtained from a for-
est only supports selecting a locally consistent can-
didate. On the other hand, the fact that variational
reranking achieves the best results clearly indicates
that the combination of sentence specific generative
model and that obtained from training data is suc-
cessful in selecting both locally and globally appro-
priate candidate from a forest.
Table 7 shows the parsing time (on 2.66GHz
Quad-Core Xeon) of the baseline k-best, generative
reranking and variational reranking parsers (java im-
plemented). The variational reranking parser con-
tains the following procedures.
1. k-best forest creation (baseline)
2. Estimation of variational model
3. Forest pruning
4. Search with the third-order model
Our reranking parser incurred little overhead to the
1485
Table 5: The comparison of the decoding frameworks:
MBR decoding seeks a candidate which has the high-
est accuracy scores over a forest (Kumar et al, 2009).
Variational decoding is performed based on Eq.8.XXXXXXXXXXDecoding
Eval Unlabeled
baseline 91.9
MBR (8-best forest) 91.99
Variational (8-best forest) 92.17
Table 6: The comparison of the reranking frameworks:
Generative means k-best or forest reranking algorithm
based on a generative model estimated from a corpus.
Variational reranking is performed based on Eq.15.XXXXXXXXXXReranking
Eval Unlabeled
Generative (8-best) 92.66
Generative (8-best forest) 92.72
Variational (8-best forest) 92.87
Table 7: The parsing time (CPU second per sentence) and
accuracy score of the baseline k-best, generative rerank-
ing and variational reranking parsers
k baseline generative variational
2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)
4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81)
8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)
16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)
32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)
64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)
Table 8: The comparison of tri-sibling and grandsibling
models: the performance of the grandsibling model out-
performs that of the tri-sibling model.PPPPPPPModel
Eval Unlabeled
tri-sibling 92.63
grandsibling 92.74
baseline parser in terms of runtime. This means that
our reranking parser can parse sentences at reason-
able times.
5.3 The Effects of Third-order Factors and
Error Analysis
From results in section 5.2, our variational rerank-
ing model achieves higher accuracy scores than the
others. To analyze the factors that improve accu-
racy scores, we further investigate whether varia-
tional reranking is performed better with the tri-
sibling or grandsibling model. Table 8 indicates that
grandsibling model achieves a larger gain than that
of tri-sibling model. Table 9 shows the examples
whose accuracy scores improved by the grandsib-
ling model. For example, the dependency relation-
ship from Verb to Noun phrase was corrected by our
proposed model.
On the other hand, many errors remain still in
Table 10: Comparison of our best result (using 16-best
forests) with other best-performing Systems on the whole
section 23
Parser English
McDonald et al (2005) 90.9
McDonald and Pereira (2006) 91.5
Koo et al (2008) standard 92.02
Huang and Sagae (2010) 92.1
Koo and Collins (2010) model1 93.04
Koo and Collins (2010) model2 92.93
this work 92.89
Koo et al (2008) semi-sup 93.16
Suzuki et al (2009) 93.79
our results. In our experiments, 48% of sentences
which contain errors have Prepositionalword errors.
In fact, well-known PP-Attachment is a problem to
be solved for natural language parsers. Other re-
maining errors are caused by symbols such as .,:??().
45% sentences contain such a dependency mistake.
Adding features to solve these problems may poten-
tially improve our parser more.
5.4 Comparison with Other Systems
Table 10 shows the comparison of the performance
of variational reranking (16-best forests) with that of
other systems. Our method outperforms supervised
parsers with second-order features, and achieves
comparable results compared to a parser with third-
order features (Koo and Collins, 2010). We can not
directly compare our method with semi-supervised
parsers such as Koo et al (2008)?s semi-sup and
Suzuki et al (2009), because ours does not use addi-
tional unlabeled data for training. The model trained
from unlabeled data can be easily incorporated into
our reranking framework. We plan to investigate
semi-supervised learning in future work.
1486
Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational
reranking parsers. The underlined portions show the effect of the grandsibling model.
sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .
correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4
baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4
proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4
sent (No.283) Many called it simply a contrast in styles .
correct 2 0 2 6 6 2 6 7 2
baseline 2 0 2 2 6 2 6 7 2
proposed 2 0 2 6 6 2 6 7 2
6 Related Work
Collins (2000) and Charniak and Johnson (2005)
proposed a reranking algorithm for constituent
parsers. Huang (2008) extended it to a forest rerank-
ing algorithm with non-local features. Our frame-
work is for a dependency parser and the decoding in
the reranking stage is done with an exact 1-best dy-
namic programming algorithm. Sangati et al (2009)
proposed a k-best generative reranking algorithm for
dependency parsing. In this paper, we use a similar
generative model, but combined with a variational
model learned on the fly. Moreover, our framework
is applicable to forests, not k-best lists.
Koo and Collins (2010) presented third-order de-
pendency parsing algorithm. Their model 1 is de-
fined by an enclosing grandsibling for each sibling
or grandchild part used in Carreras (2007). Our
grandsibling model is similar to the model 1, but
ours is defined by a generative model. The decod-
ing in the reranking stage is also similar to the pars-
ing algorithm of their model 1. In order to capture
grandsibling factors, our decoding calculates inside
probablities for not the current head node but each
pair of the node and its outgoing edges.
Titov and Henderson (2006) reported that the
MBR approach could be applied to a projective de-
pendency parser. In the field of SMT, for an approx-
imation of MAP decoding, Li et al (2009) proposed
variational decoding and Kumar et al (2009) pre-
sented hypergraph MBR decoding. Our variational
model is inspired by the study of Li et al (2009) and
we apply it to a dependency parser in order to select
better candidates with third-order information. We
also propose an efficient algorithm to estimate the
non-local third-order model structure.
7 Conclusions
In this paper, we propose a novel forest reranking
algorithm for dependency parsing. Our reranking
algorithm is a combination approach of generative
reranking and variational decoding. The search al-
gorithm in the reranking stage can be performed
using dynamic programming algorithm. Our vari-
ational reranking is aimed at selecting a candidate
from a forest, which is correct both in local and
global. Our experimental results show more signif-
icant improvements than conventional approaches,
such as k-best and forest generative reranking.
In the future, we plan to investigate more ap-
propriate generative models for reranking. PP-
Attachment is one of the most difficult problems
for a natural language parser. We plan to exam-
ine to model such a complex structure (granduncle)
(Goldberg and Elhadad, 2010) or higher-order struc-
ture than third-order for reranking which is compu-
tationally expensive for a baseline parser. As we
mentioned in Section 5.4, we also plan to incorpo-
rate semi-supervised learning into our framework,
which may potentially improve our reranking per-
formance.
Acknowledgments
Wewould like to thank GrahamNeubig andMasashi
Shimbo for their helpful comments and to the anony-
mous reviewers for their effort of reviewing our pa-
per and giving valuable comments. This work was
supported in part by Grant-in-Aid for Japan Society
for the Promotion of Science (JSPS) Research Fel-
lowship for Young Scientists.
1487
References
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. the CoNLL-
EMNLP, pages 957?961.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. the 43rd ACL, pages 173?180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201?228.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. the ICML.
J. M. Eisner. 1996a. An empirical comparison of prob-
ability models for dependency grammar. In Technical
Report, pages 1?18.
J. M. Eisner. 1996b. Three new probabilistic models for
dependency parsing: An exploration. In Proc. the 16th
COLING, pages 340?345.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742?750.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. the IWPT, pages 53?64.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the ACL,
pages 1077?1086.
L. Huang. 2006. Dynamic programming al-
gorithms in semiring and hypergraph frame-
works. Qualification Exam Report, pages 1?19.
http://www.cis.upenn.edu/ lhuang3/wpe2/.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. the 46th ACL,
pages 586?594.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1?11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. the ACL,
pages 595?603.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. the 47th ACL, pages 163?171.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
the 47th ACL, pages 593?601.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic cfg with latent annotations. In Proc. the ACL, pages
75?82.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
the 43rd ACL, pages 91?98.
H. Mi and L. Huang. 2008. Forest-based translation rule
extraction. In Proceedings of EMNLP, pages 206?
214.
M. Mohri. 2002. Semiring framework and algorithms
for shortest-distance problems. Automata, Languages
and Combinatorics, 7:321?350.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160?167.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proc. the
11th IWPT, pages 238?241.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proc. the
EMNLP, pages 551?560.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. In Technical Report,
pages 1?9.
Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. De-
pendency forest for statistical machine translation. In
Proc. the 23rd COLING, pages 1092?1100.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195?206.
1488
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 24?36, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Bilingual Lexicon Extraction from Comparable Corpora Using Label
Propagation
Akihiro Tamura and Taro Watanabe and Eiichiro Sumita
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN
{akihiro.tamura,taro.watanabe,eiichiro.sumita}@nict.go.jp
Abstract
This paper proposes a novel method for lex-
icon extraction that extracts translation pairs
from comparable corpora by using graph-
based label propagation. In previous work,
it was established that performance drasti-
cally decreases when the coverage of a seed
lexicon is small. We resolve this problem
by utilizing indirect relations with the bilin-
gual seeds together with direct relations, in
which each word is represented by a distri-
bution of translated seeds. The seed distri-
butions are propagated over a graph repre-
senting relations among words, and transla-
tion pairs are extracted by identifying word
pairs with a high similarity in the seed dis-
tributions. We propose two types of the
graphs: a co-occurrence graph, representing
co-occurrence relations between words, and
a similarity graph, representing context sim-
ilarities between words. Evaluations using
English and Japanese patent comparable cor-
pora show that our proposed graph propaga-
tion method outperforms conventional meth-
ods. Further, the similarity graph achieved im-
proved performance by clustering synonyms
into the same translation.
1 Introduction
Bilingual lexicons are important resources for bilin-
gual tasks such as machine translation (MT) and
cross-language information retrieval (CLIR). There-
fore, the automatic building of bilingual lexicons
from corpora is one of the issues that have attracted
many researchers. As a solution, a number of pre-
vious works proposed extracting bilingual lexicons
from comparable corpora, in which documents were
not direct translations but shared a topic or domain1.
The use of comparable corpora is motivated by the
fact that large parallel corpora are only available for
a few language pairs and for limited domains.
Most of the previous methods are based on as-
sumption (I), that a word and its translation tend to
appear in similar contexts across languages (Rapp,
1999). Based on this assumption, many methods
calculate word similarity using context and then ex-
tract word translation pairs with a high-context sim-
ilarity. We call these methods context-similarity-
based methods. The context similarities are usu-
ally computed using a seed bilingual lexicon (e.g.
a general bilingual dictionary) by mapping contexts
expressed in two different languages into the same
space. In the mapping, information not represented
by the seed lexicon is discarded. Therefore, the
context-similarity-based methods could not find ac-
curate translation pairs if using a small seed lexicon.
Some of the previous methods tried to alleviate
the problem of the limited seed lexicon size (Koehn
and Knight, 2002; Morin and Prochasson, 2011;
Hazem et al2011), while others did not require any
seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et
al., 2008; Ismail and Manandhar, 2010; Daume? III
and Jagarlamudi, 2011). However, they suffer the
problems of high computational cost (Rapp, 1995),
sensitivity to parameters (Hazem et al2011),
low accuracy (Fung, 1995; Ismail and Manandhar,
2010), and ineffectiveness for language pairs with
1Although Vulic? et al2011) regarded document-aligned
texts such as texts on Wikipedia as comparable corpora, we do
not limit comparable corpora to these kinds of texts.
24
different types of characters (Koehn and Knight,
2002; Haghighi et al2008; Daume? III and Jagar-
lamudi, 2011).
In face of the above problems, we propose a novel
method that uses a graph-based label propagation
technique (Zhu and Ghahramani, 2002). The pro-
posed method is based on assumption (II), which is
derived by recursively applying assumption (I) to the
?contexts?: a word and its translation tend to have
similar co-occurrence (direct and indirect) relations
with all bilingual seeds across languages.
Based on assumption (II), we propose a three-
step approach: (1) constructing a graph for each
language with each edge indicating a direct co-
occurrence relation, (2) representing every word as a
seed translation distribution by iteratively propagat-
ing translated seeds in each graph, (3) finding two
words in different languages with a high similarity
with respect to the seed distributions. By propagat-
ing all the seeds on the graph, indirect co-occurrence
relations are also considered when computing bilin-
gual relations, which have been neglected in previ-
ous methods. In addition to the co-occurrence-based
graph construction, we propose a similarity graph,
which also takes into account context similarities be-
tween words.
The main contributions of this paper are as fol-
lows:
? We propose a bilingual lexicon extraction
method that captures co-occurrence relations
with all the seeds, including indirect rela-
tions, using graph-based label propagation.
In our experiments, we confirm that the
proposed method outperforms conventional
context-similarity-based methods (Rapp, 1999;
Andrade et al2010), and works well even if
the coverage of a seed lexicon is low.
? We propose a similarity graph which represents
context similarities between words. In our ex-
periments, we confirm that a similarity graph
is more effective than a co-occurrence-based
graph.
2 Context-Similarity-based Extraction
Method
The bilingual lexicon extraction from comparable
corpora was pioneered in (Rapp, 1995; Fung, 1995).
The popular similarity-based methods consist of the
following steps: modeling contexts, calculating con-
text similarities, and finding translation pairs.
Step 1. Modeling contexts: The context of each
word is generally modeled by a vector where each
dimension corresponds to a context word and each
dimension has a value indicating occurrence cor-
relation. Various definitions for the context have
been used: distance-based context (e.g. in a sen-
tence (Laroche and Langlais, 2010), in a para-
graph (Fung and McKeown, 1997), in a predefined
window (Rapp, 1999; Andrade et al2010)), and
syntactic-based context (e.g. predecessors and suc-
cessors in dependency trees (Garera et al2009),
certain dependency position (Otero and Campos,
2008)). Some treated context words equally re-
gardless of their positions (Fung and Yee, 1998),
while others treated the words separately for each
position (Rapp, 1999). Various correlation mea-
sures have been used: log-likelihood ratio (Rapp,
1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung
and Yee, 1998), pointwise mutual information
(PMI) (Andrade et al2010), context heterogene-
ity (Fung, 1995), etc.
Shao and Ng (2004) represented contexts using
language models. Andrade et al2010) used a
set of words with a positive association as a con-
text. Andrade et al2011a) used dependency re-
lations instead of context words. Ismail and Man-
andhar (2010) used only in-domain words in con-
texts. Pekar et al2006) constructed smoothed con-
text vectors for rare words. Laws et al2010) used
graphs in which vertices correspond to words and
edges indicate three types of syntactic relations such
as adjectival modification.
Step 2. Calculating context similarities: The con-
texts which are expressed in two different languages
are mapped into the same space. Previous methods
generally use a seed bilingual lexicon for this map-
ping. After that, similarities are calculated based
on the mapped context vectors using various mea-
sures: city-block metric (Rapp, 1999), cosine sim-
ilarity (Fung and Yee, 1998), weighted jaccard in-
dex (Hazem et al2011), Jensen-Shannon diver-
gence (Pekar et al2006), the number of overlap-
ping context words (Andrade et al2010), Sim-
Rank (Laws et al2010), euclidean distance (Fung,
1995), etc.
25
Japanese English
0.8
0.6
0.5
0.8
???? ? ????
(piranha)   (Amazon)???? ? ?????
(piranha)    (jungle)???? ? ??
(piranha)   (freshwater)?? ? ?
(freshwater)  (fish)
AssociationQuery ? Context Word
0.8
0.6
0.5
0.8
0.6
0.8
piranha   ? Amazon
piranha   ? jungle
piranha   ? freshwater
anaconda ? Amazon
anaconda ? jungle
freshwater ? fish
AssociationQuery ? Context Word
Seed Lexicon (Japanese ? English) :???? ? Amazon, ????? ? jungle, ? ? fish
Amazon   jungle???? ( 0.8 , 0.6 ) Amazon  junglepiranha    ( 0.8 , 0.6 )
anaconda ( 0.8 , 0.6 )
similarity???? ? piranha    1.0???? ? anaconda 1.0
Japanese English
0.5
0.8
0.6
0.8
0.8
0.6
0.8 0.6
0.5
0.8
?
(fish)
?????
(jungle)
????
(Amazon)
jungle
fish
???? ????? ?
0.5   ,    0.3   ,     0.2
Amazon  jungle  fish
0.55  ,   0.4   ,  0.05
Amazon jungle  fish
0.5 ,      0.3  ,   0.2
Proposed Method
Context-similarity-based Method
????
(piranha)
??
(freshwater)
freshwater
piranha
anaconda
Amazon
Figure 1: An Example of a Previous Method and our Pro-
posed Method
Andrade et al2011b) performed a linear trans-
formation of context vectors in accordance with the
notion that importance varies by context positions.
Gaussier et al2004) mapped context vectors via
latent classes to capture synonymy and polysemy in
a seed lexicon. Fis?er et al2011) and Kaji (2005)
calculated 2-way similarities.
Step 3. Finding translation pairs: A pair of words
is treated as a translation pair when their context
similarity is high. Various clues have been con-
sidered when computing the similarities: concept
class information obtained from a multilingual the-
saurus (De?jean et al2002), co-occurrence models
generated from aligned documents (Prochasson and
Fung, 2011), and transliteration information (Shao
and Ng, 2004).
2.1 Problems from Previous Works
Most of previous methods used a seed bilingual lex-
icon for mapping modeled contexts in two different
languages into the same space. The mapping heav-
ily relies on the entries in a given bilingual lexicon.
Therefore, if the coverage of the seed lexicon is low,
the context vectors become sparser and its discrim-
inative capability becomes lower, leading to extrac-
tion of incorrect translation equivalents.
Consider the example in Figure 1, where a
context-similarity-based method and our proposed
method find translation equivalents of the Japanese
word ????? (piranha)?. There are three con-
text words for the query. However, the informa-
tion on co-occurrence with ??? (freshwater)? dis-
appears after the context vector is mapped, because
the seed lexicon does not include ??? (freshwa-
ter)?. The same thing happens with the English word
?piranha?. As a result, the pair of ????? (pi-
ranha)? and ?anaconda? could be wrongly identified
as a translation pair.
Some previous work focused on the problem
of seed lexicon limitation. Morin and Prochas-
son (2011) complemented the seed lexicon with
bilingual lexicon extracted from parallel sentences.
Koehn and Knight (2002) used identically-spelled
words in two languages as a seed lexicon. However,
the method is not applicable for language pairs with
different types of characters such as English and
Japanese. Hazem et al2011) exploited k-nearest
words for a query, which is very sensitive to the pa-
rameter k.
Some previous work did not require any seed lex-
icon. Rapp (1995) proposed a computationally de-
manding matrix permutation method which maxi-
mizes a similarity between co-occurrence matrices
in two languages. Ismail and Manandhar (2010) in-
troduced a similarity measure between two words in
different languages without requiring any seed lex-
icon. Fung (1995) used context heterogeneity vec-
tors where each dimension is independent on lan-
guage types. However, their performances are worse
than those of conventional methods using a small
seed lexicon. Haghighi et al2008) and Daume?
III and Jagarlamudi (2011) proposed a generative
model based on probabilistic canonical correlation
analysis, where words are represented by context
features and orthographic features2. However, their
experiments showed that orthographic features to be
important for effectiveness, which means low per-
2In Haghighi et al2008) and Daume? III and Jagarla-
mudi (2011), indirect relations with seeds are considered topo-
logically, but our method utilizes degrees of indirect correla-
tions with seeds.
26
formance for language pairs with different character
types.
3 Lexicon Extraction Based on Label
Propagation
As described in Section 2, the performance of previ-
ous work is significantly degraded when used with a
small seed lexicon. This problem could be resolved
by incorporating indirect relations with all the seeds
when identifying translation pairs. For example, in
Figure 1, ????? (piranha)? has some degree of
association with the seed ?? - fish? through ???
(freshwater)? in both the Japanese side and the En-
glish side, although ????? (piranha)? and ??
(fish)? do not co-occur in the same contexts. More-
over, ?anaconda? has very little association with the
seed ?? - fish? in the English side. Therefore,
the indirect relation with the seed ?? - fish? helps
to discriminate from between ?piranha? and ?ana-
conda? and could be an important clue for identify-
ing a correct translation pair.
To utilize indirect relations, we introduce assump-
tion (II): a word and its translation tend to have simi-
lar co-occurrence (direct and indirect) relations with
all bilingual seeds across languages3. Based on as-
sumption (II), we propose to identify a word pair as
a translation pair when its co-occurrence (direct and
indirect) relations with all the seeds are similar.
To obtain co-occurrence relations with all the
seeds, including indirect relations, we focus on a
graph-based label propagation (LP) technique (Zhu
and Ghahramani, 2002). LP transfers labels from
labeled data points to unlabeled data points. In the
process, all vertices have soft labels that can be inter-
preted as label distributions. We apply LP to bilin-
gual lexicon extraction by representing each word as
a vertex in a graph with each edge encoding a direct
co-occurrence relation. Translated seeds are propa-
gated as labels, and seed distributions are obtained
for each word. From the seed distributions, we iden-
tify translation pairs.
In summary, our proposed method consists of
three steps (see Algorithm 1): (1) graph construc-
3Assumption (I) indicates direct co-occurrence relations be-
tween a word and its context words are preserved across differ-
ent languages. Therefore, assumption (II) is derived by recur-
sively applying assumption (I) to the ?context words?.
Algorithm 1 Bilingual Lexicon Extraction
Require: comparable corpora De and Df ,
a seed lexicon S consists of Se and Sf
Ensure: Output translation pairs T
1-1: Ge = {Ee, V e,W e} ? construct-graph(De)
1-2: Gf = {Ef , V f ,W f} ? construct-graph(Df )
2-1: G?e = {Ee, V e,W e, Qe} ? propagate-seed (Ge, Se)
2-2: G?f = {Ef , V f ,W f , Qf} ? propagate-seed (Gf , Sf )
3: T ? extract-translation (Qe, Qf , S)
tion for each language, (2) seed propagation in each
graph, (3) translation pair extraction.
3.1 Graph Construction
We construct a graph representing the association
between words for each language. Each graph is an
undirected graph because the association does not
have direction. The graphs are constructed as fol-
lows:
Step 1. Vertex assignment extracts words from
each corpus, and assigns a vertex to each of the ex-
tracted words. Let V = {v1, ? ? ? , vn} be a set of
vertices.
Step 2. Edge weight calculation calculates associ-
ation strength between two words as the weights of
edges. Let E and W be a set of edges and that of
the weights respectively, and eij ? E links vi and
vj , and wij ? W is the weight of eij . Note that
|E| = |W |.
Step 3. Edge pruning excludes edges whose
weights are lower than threshold, in order to reduce
the computational cost during seed propagations.
We propose two types of graphs that differ in the
association measure used in Step 2: a co-occurrence
graph and a similarity graph4.
3.1.1 Co-occurrence Graph
A co-occurrence graph directly encodes assump-
tion (II). Each edge in the graph indicates correlation
strength between occurrences of two linked words.
An example is shown in Figure 1.
In edge weight calculation, the co-occurrence
frequencies are first computed for each word pair in
the same context, and then the correlation strength is
estimated. There are various definitions of a context
or correlation measures that can be used (e.g. the
4We can combine the association measures used in a co-
occurrence graph and a similarity graph. We will leave this
combination approach for future work.
27
approaches used for modeling contexts in context-
similarity-based methods). In this paper, we use
words in a predefined window (window size is 10
in our experiments) as the context and PMI as the
correlation measure:
wij = PMI(vi, vj) = log
p(vi, vj)
p(vi) ? p(vj)
,
where p(vi) (or p(vj)) is the probability that vi (or
vj) occurs in a context, and p(vi, vj) is the probabil-
ity that vi and vj co-occur within the same context.
We estimate PMI(vi, vj) by the Bayesian method
proposed by Andrade et al010). Then, edges
with a negative association, PMI(vi, vj) ? 0, are
pruned in edge pruning.
3.1.2 Similarity Graph
Co-occurrence graphs are very sensitive to ac-
cidental relation caused by lower frequent co-
occurrence. Thus, we propose a similarity graph
where context similarities are employed as weights
of edges instead of simple co-occurrence-based cor-
relations. Since the context similarities are com-
puted by the global correlation among words which
co-occur, a similarity graph is less subject to acci-
dental co-occurrence. The use of a similarity graph
is inspired by assumption (III): a word and its trans-
lation tend to have similar context similarities with
all bilingual seeds across languages5.
In edge weight calculation, we first construct a
correlation vector representing co-occurrence rela-
tions for each word. The correlation vectors are con-
structed in the same way as the context vectors used
in context-similarity-based methods (see Section 2),
where context words are words in a predefined win-
dow (window size is 4 in our experiment), the as-
sociation measure is PMI, and context words are
treated separately for each position. A correlation
vector for each position is computed separately, then
concatenated into a single vector within the window.
Secondly, we calculate similarities between correla-
tion vectors. There are various similarity measures
that can be used, and cosine similarity is used in this
5This assumption is justified because context similarities are
based on co-occurrence relations that are preserved across dif-
ferent languages.
paper:
wij = Cos(f?i, f?j) =
f?i ? f?j
?f?i??f?j?
,
where f?i (or f?j) is the correlation vector of vi (or
vj). Then, in edge pruning, we preserve the edges
with top 100 weight for each vertex.
3.2 Seed Propagation
LP is a graph-based technique which transfers the
labels from labeled data to unlabeled data in or-
der to infer labels for unlabeled data. This is pri-
marily used when there is scarce labeled data but
abundant unlabeled data. LP has been success-
fully applied in common natural language process-
ing tasks such as word sense disambiguation (Niu
et al2005; Alexandrescu and Kirchhoff, 2007),
multi-class lexicon acquisition (Alexandrescu and
Kirchhoff, 2007), and part-of-speech tagging (Das
and Petrov, 2011). LP iteratively propagates la-
bel information from any vertex to nearby vertices
through weighted edges, and then a label distribu-
tion for each vertex is generated where the weights
of all labels add up to 1.
We adopt LP to obtain relations with all bilingual
seeds including indirect relations by treating each
seed as a label. First, each translated seed is assigned
to a label, and then the labels are propagated in the
graph described in Section 3.1.
The seed distribution for each word is initialized
as follows:
q0i (z) =
?
?
?
1 if vi ? Vs and z = vi
0 if vi ? Vs and z ?= vi
u(z) otherwise
,
where Vs is the set of vertices corresponding to
translated seeds, u is a uniform distribution, qki (i =
1 ? ? ? |V |) is the seed distribution for vi after k prop-
agation, and qki (z) is the weight of a label (i.e., a
translated seed) z in qki .
After initialization, we iteratively propagate the
seeds through weighted edges. In each propagation,
seeds are probabilistically propagated from linked
vertices under the condition that larger edge weights
allow seeds to travel through easier. Thus, the closer
vertices are, the more likely they have similar seed
distributions. In Figure 1, the balloons attached to
28
vertices in the graphs show examples of the seed dis-
tributions generated by propagations. For example,
the English word ?piranha? has the seed distribution
where the weights of the seeds ?Amazon?, ?jungle?,
and ?fish? are 0.5, 0.3, and 0.2, respectively. Specif-
ically, each of seed distributions is updated as fol-
lows:
qmi (z) =
?
?
?
q0i (z) if vi ? Vs
?
vj?N(vi) wij ? q
m?1
j (z)
?
vj?N(vi) wij
otherwise ,
where N(vi) is the set of vertices linking to vi. We
ran this procedure for 10 iterations in our experi-
ments.
3.3 Translation Pair Extraction
After label propagations, we treat a pair of words in
different languages with similar seed distributions as
a translation pair. Seed distribution can be regarded
as a vector where each dimension corresponds to
each translated seed and each dimension has up-
dated weight through label propagations. A sim-
ilarity between seed distributions can therefore be
calculated in the same way as a context-similarity-
based method. In this paper, we use the cosine sim-
ilarity defined by the following:
Cos(qfx , qey) =
?
si?S q
f
x(v
f
i ) ? qey(vei )
?
?
si?S(q
f
x(vfi ))2
?
?
si?S(q
e
y(vei ))2
,
where qfx (or qey) is the seed distribution for a word x
(or y) in the source language (or target language), S
is the seed lexicon whose i-th entry si is a pairing of
a translated seed in the source language vfi and one
in the target language vei .
4 Experiment
4.1 Experiment Data
We used English and Japanese patent documents
published between 1993 and 2005 by the US Patent
& Trademark Office and the Japanese Patent Of-
fice respectively, which were a part of the data used
in the NTCIR-8 patent translation task (Fujii et al
2010). Note that these documents are not aligned.
There are over three million English-Japanese
parallel sentences (e.g. training data, test data, and
Pair Japanese Word English Word
LexS 2,742 2,566 2,326
LexL 28,053 18,587 12,893
Table 1: Size of Seed Lexicons
development data used in the NTCIR-8 patent trans-
lation task, which is called NTCIR parallel data
hereafter) in the patent data. However, a preliminary
examination showed that the NTCIR parallel data
covers less than 3% of all words because there are
a number of technical terms and neologisms. There-
fore, the patent translation task is a task that requires
bilingual lexicon extraction from non-parallel data.
We selected documents belonging to the physics
domain from each monolingual corpus based on In-
ternational Patent Classification (IPC) code6, and
then used them as a comparable corpus in our ex-
periments. As a result, we used 1,479,831 Japanese
documents and 438,227 English documents. The
reason for selecting the physics domain is that this
domain contains the most documents of all the do-
mains.
The Japanese texts were segmented and part-of-
speech tagged by ChaSen7, and the English texts
were tokenized and part-of-speech tagged by Tree-
Tagger (Schmid, 1994). Next, function words were
removed since function words with little seman-
tic information spuriously co-occurred with many
words. As a result, the number of distinct words
in Japanese corpus and English corpus amounted to
1,111,302 and 4,099,8258, respectively.
We employed seed lexicons from two sources:
(1) EDR bilingual dictionary (EDR, 1990), (2)
automatic word alignments generated by running
GIZA++ (Och and Ney, 2003) with the NTCIR par-
allel data consisting of 3,190,654 parallel sentences.
From each source, we extracted pairs of nouns ap-
pearing in our corpus. From (2), we excluded word
pairs where the average of 2-way translation proba-
6SECTION G of IPC code indicates the physics domain.
7http://chasen-legacy.sourceforge.jp/
8The English words contain words in tables or mathematical
formula but the Japanese words do not because the data format
differs between English and Japanese. This is why the number
of English words is larger than that of Japanese words, even
though the number of English documents is smaller than that of
Japanese documents.
29
bilities was lower than 0.5. The pairs from (1) and
(2) amounted to 27,353 and 2,853 respectively, and
the two sets were not exclusive. In order to mea-
sure the impact of seed lexicon size, we prepared
two seed lexicons: LexL, a large seed lexicon that is
a union of all the extracted word pairs, and LexS , a
small seed lexicon that is a union of a random sam-
pling one-tenth of the pairs from (1) and one-tenth
of the pairs from (2). Table 1 shows the size of each
seed lexicon. Note that our seed lexicons include
one-to-many or many-to-one translation pairs.
We randomly selected 1,000 Japanese words as
our test data which were identified as either a noun
or an unknown by ChaSen and were not covered ei-
ther by the EDR bilingual dictionary or by the NT-
CIR parallel data. This is because the purpose of our
method is to complement existing bilingual dictio-
naries or parallel data. Note that the Japanese words
in our test data may not have translation equivalents
in the English side.
4.2 Competing Methods
We evaluated two types of our label propagation
based methods against two baselines. Cooc em-
ploys co-occurrence graphs and Sim uses similarity
graphs when constructing graphs for label propaga-
tion as described in Section 3.
Rapp is a typical context-similarity-based
method described in Section 2 (Rapp, 1999).
Context words are words in a window (window size
is 10) and are treated separately for each position.
Associations with context words are computed
using the log-likelihood ratio (Dunning, 1993). The
similarity measure between context vectors is the
city-block metric.
Andrade is a sophisticated method in context-
similarity-based methods (Andrade et al2010).
Context is a set of words with a positive association
in a window (window size is 10). The association
is calculated using the PMI estimated by a Bayesian
method, and a similarity between contexts is esti-
mated based on the number of overlapping words
(see the original paper for details).
4.3 Experiment Results
Table 2 shows the performance of each method us-
ing LexS or LexL. Hereafter, Method(L) (or
Method(S)) denotes the Method using LexL (or
LexS LexL
Acc1 Acc20 Acc1 Acc20
Rapp 1.5% 3.8% 4.8% 17.6%
Andrade 1.9% 4.2% 5.6% 17.6%
Cooc 3.2% 8.6% 9.2% 28.3%
Sim 4.1% 11.5% 10.8% 30.6%
Table 2: Performance on Bilingual Lexicon Extraction
LexS). We measure the performance on bilingual
lexicon extraction as Top N accuracy (AccN ), which
is the number of test words whose top N translation
candidates contain a correct translation equivalent
over the total number of test words (=1,000). Table
2 shows Top 1 and Top 20 accuracy. We manually9
evaluated whether translation candidates contained a
correct translation equivalent. We did not use recall
because we do not know if the translation equiva-
lents of a test word appear in the corpus.
Table 2 shows that the proposed methods outper-
form the baselines both when using LexS and using
LexL. The improvements are statistically significant
in the sign-test with 1% significance-level. The re-
sults show that capturing the relations with all the
seeds including indirect relations is effective.
The accuracies of the baselines in Table 2 are
worse than the previous reports: 14% Acc1 and 46%
Acc10 (Andrade et al2010), and 72% Acc1 (Rapp,
1999). This is because previous works evalu-
ated only the queries whose translation equivalents
existed in the experiment data, which is not al-
ways true in our experiments. Moreover, previous
works evaluated only high-frequency words: com-
mon nouns (Rapp, 1999) and words with a docu-
ment frequency of at least 50 (Andrade et al2010).
Our test data, on the other hand, includes many low-
frequency words. It is generally true that translation
of high-frequency words is much easier than that of
low frequency words. We discuss the impact of test
word frequencies in detail in Section 5.3.
Table 2 also shows that Sim outperforms Cooc
both when using LexS and using LexL. The im-
provements of Acc20 are statistically significant in
the sign-test with 5% significance-level.
9We could not evaluate using existing dictionaries because
most of the test data are technical terms and neologisms not
included in the dictionaries.
30
Sim(L) (2) Cooc(L) (5) Andrade(L) (181)
1 psychosis polynephropathy disease
2 manic-depression neuroleptic bowel
3 epilepsy iridocyclitis disorder
4 insomnia Tic symptom
5 dementia manic-depression sclerosis
Sim(S) (974) Cooc(S) (1652) Andrade(S) (1747)
1 ulceration dyslinesia bulimia
2 ulcer encephalomyelopathy spasticity
3 naphthol ganglionic Parkinson
4 dementia corticobasal Asymmetric
5 gastritis praecox anorexia
Table 3: Translation Candidates for ??? (manic-
depression)
???
Cooc(L) Andrade(L) Cooc(S) Andrade(S)
1 ??? (0.12) ??? (7.6) ?? (0.016) ?? (5.0)
narcotic narcotic dementia posteriori
2 ??? (0.11) ?? (6.3) ?? (0.014) ?? (3.7)
psychosis old alien,stepchild dementia
3 ??? (0.08) ??? (6.3) ?? (0.012) ?? (3.2)
neurosis psychosis posteriori ulcer
4 ???? (0.05) ???? (5.6) ?? (0.012) ???? (2.9)
hormone bronchitis electropositivity period
5 ??? (0.04) ?? (5.0) ?? (0.011) ?? (2.5)
insomnia posteriori ulcer seriousness
manic-depression
Cooc(L) Andrade(L) Cooc(S) Andrade(S)
1 illness illness ganja galop(0.15) (8.6) (0.012) (7.0)
2 neurosis psychotherapeutics carbanilide madness(0.11) (7.0) (0.011) (5.4)
3 seizure galop paludism libido(0.07) (7.0) (0.011) (5.2)
4 psychosis psychosis resignation vitiligo(0.06) (6.8) (0.010) (4.6)
5 insomnia somnambulism galop dementia(0.04) (6.7) (0.009) (4.3)
Table 4: Seeds with the Highest Weight
5 Discussion
5.1 Effect of Indirect Relations with Seeds
Table 3 shows a list of the top 5 translation can-
didates for the Japanese word ???? (manic-
depression)? for each method, where the ranks of the
correct translations are shown in parentheses next to
method names. Table 4 shows the top 5 translated
seeds which characterize the query, where the val-
ues in parentheses indicate weight. Table 3 shows
that Cooc(L) can find the correct translation equiv-
alent but Andrade(L) cannot. Table 4 shows that
Cooc(L) can utilize more seeds closely tied to the
query (e.g. ???? (neurosis)?, ???? (insom-
nia)?), which did not occur in the context of the
query in the experiment data. The result shows that
indirectly-related seeds are also important clues, and
our proposed method can utilize these.
5.2 Impact of Seed Lexicon Size
Table 2 shows that a reduction of seed lexicon size
degrades performance. This is natural for the base-
line methods because LexS cannot translate most of
context words, which are necessary for word charac-
terization. Consider Andrade(L) and Andrade(S)
in the example in Section 5.1. Table 4 shows that
Andrade(S) uses less relevant seeds with the query,
and has to express the query by seeds with less as-
sociation. For example, ???? (psychosis)? can-
not be used in Andrade(S) because LexS does not
have the seed. Therefore, it is more difficult for
Andrade(S) to find correct translation pairs.
The proposed methods also share the same ten-
dency, although each word is expressed by all the
seeds in the seed lexicon. Consider Cooc(L) and
Cooc(S) in the above example. Table 4 shows that
Cooc(S) expresses the query by a smooth seed dis-
tribution, which is difficult to discriminate from oth-
ers. This is because LexS does not have relevant
seeds for the query. This is why Cooc(S) cannot
find the correct translation equivalent. On the other
hand, Cooc(L) characterizes ????? and ?manic-
depression? by strongly relevant seeds (e.g. ???
? (psychosis)?,???? (neurosis)?), and then finds
the correct translation equivalent.
To examine the robustness-to-seed lexicon size,
we calculated the reduction rate of Acc20 with the
following expression: (Acc20 with LexL ? Acc20
with LexS) / Acc20 with LexL. The reduction rates
of Rapp, Andrade, Cooc, and Sim are 78.4%,
76.1%, 69.6%, and 62.4% respectively. Moreover,
the difference between degradation in Cooc and that
in Andrade is statistically significant in the sign-test
with 1% significance-level. These results indicate
that the proposed methods are more robust to seed
lexicon size than the baselines. This is because the
proposed methods can utilize seeds with indirect re-
lations while the baselines utilize only seeds in the
context.
To verify our claim, we examined the number
of test words which occurred with no seeds in the
context. There were 570 such words in Rapp(S),
387 in Rapp(L), 572 in Andrade(S), and 388 in
Andrade(L). The baselines cannot find their trans-
31
Low Freq. High Freq.
Acc1 Acc20 Acc1 Acc20
Rapp(L) 0.5% 2.4% 7.2% 25.6%
Andrade(L) 0.3% 1.8% 8.6% 26.3%
Cooc(L) 0.8% 4.3% 13.9% 40.7%
Sim(L) 2.2% 6.7% 15.0% 42.0%
Table 5: Comparison between Performance for High and
Low Frequency Words
lation equivalents. Words such as this occur even if
using LexL, and that number increases when LexS
is used. On the other hand, the proposed methods
are able to utilize all the seeds in order to find equiv-
alents for words such as these. Therefore, the pro-
posed methods work well even if the coverage of a
seed lexicon is low.
5.3 Impact of Word Frequencies
Our test data includes many low-frequency words
which are not covered by the EDR bilingual dic-
tionary or the NTCIR parallel data. 624 words ap-
pear in the corpus less than 50 times. Table 5 shows
AccN using LexL for 624 low-frequency words and
376 high-frequency words. Table 5 shows that per-
formance for low-frequency words is much worse
than that for high-frequency words. This is because
translation of high-frequency words utilizes abun-
dant and reliable context information, while the con-
text information for low-frequency words is statis-
tically unreliable. In the proposed methods, edges
linking rare words are sometimes generated based
on accidental co-occurrences, and then unrelated
seed information is transferred through the edges.
Therefore, even our label propagation based meth-
ods, especially for Cooc, could not identify the cor-
rect translation equivalents for rare words. Sim al-
leviated the problem by using a similarity graph in
which edges are generated based on global correla-
tion among words, as indicated by Table 5. Table
5 also suggests that top 20 translation candidates for
high-frequency words have potential to contribute to
bilingual tasks such as MT and CLIR although the
overall performance is still low.
5.4 Effect of Similarity Graphs
We examined AccN for synonyms of translated
seeds in Japanese. The Acc1 and Acc20 of Sim(L)
are 15.6% and 56.3%, respectively, and those of
Cooc(L) are 9.4% and 37.5%, respectively. The
results show that similarity graphs are effective for
clustering synonyms into the same translation equiv-
alents. For example, Sim(L) extracted the correct
translation pair of the English word ?iodine? and
the Japanese word ???????, a synonym of the
translated seed ???? (iodine)? in Japanese. This
is because synonyms tend to be linked in the similar-
ity graph and have similar seed distributions. On the
other hand, in the co-occurrence graph, synonyms
tend to be indirectly linked through mutual context
words, so the seed distributions of the two could be
far away from each other.
There are in particular many loanwords in patent
documents, which are spelled in different ways from
person to person. For example, the loan word for the
English word ?user? is often written as ?????,
but it is sometimes written as ??????, with an
additional prolonged sound mark. Therefore, Sim
is particularly effective for the experiment data.
5.5 Error Analysis
We discuss errors of the proposed methods except
the errors for low-frequency words (see Section
5.3). Our test data includes words whose transla-
tion equivalents inherently cannot be found. The
first of these types are words whose equivalent does
not exist in the English corpus. This is an unavoid-
able problem for methods based on comparable cor-
pora. The second one are words whose English
equivalents are compound words. The Japanese
morphological analyzer tends to group a compound
word into a single word, while the English text an-
alyzer does not perform a collocation of words di-
vided by the delimiter space. For example, the sin-
gle Japanese word ???? is equivalent to ?palm
pattern? or ?palm print?, which is composed of
two words. This case was counted as an error
even though the proposed methods found the word
?palm? as a equivalent of ????.
A main reason of errors other than those above
is word sense ambiguity, which is different in ev-
ery language. For example, the Japanese word ???
32
means ?right? and ?conservatism? in English. The
proposed methods merge different senses by prop-
agating seeds through these polysemous words in
only one language side. This is why translation pairs
could have wrong seed distributions and then the
proposed methods could not identify correct trans-
lation pairs. We will leave this word sense disam-
biguation problem for future work.
6 Related Work
Besides the comparable corpora approach discussed
in Section 2, many alternatives have been proposed
for bilingual lexicon extraction. The first is a method
that finds translation pairs in parallel corpora (Wu
and Xia, 1994; Fung and Church, 1994; Och and
Ney, 2003). However, large parallel corpora are only
available for a few language pairs and for limited
domains. Moreover, even the large parallel corpora
are relatively smaller than comparable corpora.
The second is a method that exploits the Web. Lu
et al2004) extracted translation pairs by mining
web anchor texts and link structures. As an alter-
native, mixed-language web pages are exploited by
first retrieving texts including both source and tar-
get languages from the web by using a search en-
gine or simple rules, and then extracting transla-
tion pairs from the mixed-language texts utilizing
various clues: Zhang and Vines (2004) used co-
occurrence statistics, Cheng et al2004) used co-
occurrences and context similarity information, and
Huang et al2005) used phonetic, semantic and
frequency-distance features. Lin et al2008) pro-
posed a method for extracting parenthetically trans-
lated terms, where a word alignment algorithm is
used for establishing the correspondences between
in-parenthesis and pre-parenthesis words. However,
those methods cannot find translation pairs when
they are not connected with each other through link
structures, or when they do not co-occur in the same
text.
Transliteration is a completely different way for
bilingual lexicon acquisition, in which a word in
one language is converted into another language us-
ing phonetic equivalence (Knight and Graehl, 1998;
Karimi et al2011). Although machine transliter-
ation works particularly well for proper names and
loan words, it cannot be employed for phonetically
dissimilar translations.
All the methods mentioned above may poten-
tially extract translation pairs more precisely than
our comparable corpora approach when their under-
lying assumptions are satisfied. We might improve
the performance of our method by augmenting a
seed lexicon with translation pairs extracted using
the above methods, as experimented with in Section
4, in which additional lexical entries are included
from parallel data.
7 Conclusion
We proposed a novel bilingual lexicon extraction
method using label propagation for alleviating the
limited seed lexicon size problem. The proposed
method captures relations with all the seeds in-
cluding indirect relations by propagating seed in-
formation. Moreover, we proposed using similar-
ity graphs in propagation process in addition to co-
occurrence graphs. Our experiments showed that the
proposed method outperforms conventional context-
similarity-based methods (Rapp, 1999; Andrade et
al., 2010), and the similarity graphs improve the
performance by clustering synonyms into the same
translation.
We are planning to investigate the following open
problems in future work: word sense disambigua-
tion and translation of compound words as described
in (Daille and Morin, 2005; Morin et al2007).
In addition, indirect relations have also been used
in other tasks, such as paraphrase acquisition from
bilingual parallel corpora (Kok and Brockett, 2010).
We will utilize their random walk approach or other
graph-based techniques such as modified adsorp-
tion (Talukdar and Crammer, 2009) for generating
seed distributions. We are also planning an end-to-
end evaluation, for instance, by employing the ex-
tracted bilingual lexicon into an MT system.
Acknowledgments
We thank anonymous reviewers of EMNLP-CoNLL
2012 for helpful suggestions and comments on a first
version of this paper. We also thank anonymous re-
viewers of First Workshop on Multilingual Model-
ing (MM-2012) for useful comments on this work.
33
References
Andrei Alexandrescu and Katrin Kirchhoff. 2007.
Data-Driven Graph Construction for Semi-Supervised
Graph-Based Learning in NLP. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 204?211.
Daniel Andrade, Tetsuya Nasukawa, and Junichi Tsu-
jii. 2010. Robust Measurement and Comparison
of Context Similarity for Finding Translation Pairs.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), pages
19?27.
Daniel Andrade, Takuya Matsuzaki, and Junichi Tsu-
jii. 2011a. Effective Use of Dependency Structure
for Bilingual Lexicon Creation. In Proceedings of
the 12th International Conference on Computational
Linguistics and Intelligent Text Processing (CICLing
2011) - Volume Part II, pages 80?92.
Daniel Andrade, Takuya Matsuzaki, and Junichi Tsujii.
2011b. Learning the Optimal Use of Dependency-
parsing Information for Finding Translations with
Comparable Corpora. In Proceedings of the 4th Work-
shop on Building and Using Comparable Corpora,
pages 10?18.
Pu-Jen Cheng, Jei-Wen Teng, Ruei-Cheng Chen, Jenq-
Haur Wang, Wen-Hsiang Lu, and Lee-Feng Chien.
2004. Translating Unknown Queries with Web Cor-
pora for Cross-Language Information Retrieval. In
Proceedings of the 27th Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 146?153.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002), pages 1?5.
Be?atrice Daille and Emmanuel Morin. 2005. French-
English Terminology Extraction from Comparable
Corpora. In Proceedings of 2nd International Joint
Conference on Natural Language Processing (IJCNLP
2005), pages 707?718.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
600?609.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main Adaptation for Machine Translation by Mining
Unseen Words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT2011),
pages 407?412.
Herve? De?jean, ?Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational linguistics (COLING 2002), pages 1?
7.
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. COMPUTATIONAL
LINGUISTICS, 19(1):61?74.
EDR. 1990. Bilingual Dictionary. In Technical Report
TR-029. Japan Electronic Dictionary Research Insti-
tute, Tokyo.
Darja Fis?er, Nikola Ljubes?ic?, ?Spela Vintar, and Senja Pol-
lak. 2011. Building and using comparable corpora for
domain-specific bilingual lexicon extraction. In Pro-
ceedings of the 4th Workshop on Building and Using
Comparable Corpora, pages 19?26.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of the 8th NTCIR Workshop, pages 371?376.
Pascale Fung and Kenneth Ward Church. 1994. K-
vec: A New Approach for Aligning Parallel Texts.
In Proceedings of the 15th International Conference
on Computational Linguistics (COLING 1994), pages
1096?1102.
Pascale Fung and Kathleen McKeown. 1997. Finding
Terminology Translations from Non-parallel Corpora.
In Proceedings of the 5th Annual Workshop on Very
Large Corpora, pages 192?202.
Pascale Fung and Lo Yuen Yee. 1998. An IR Approach
for Translating New Words from Nonparallel, Compa-
rable Texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics, Volume 1, pages 414?420.
Pascale Fung. 1995. Compiling Bilingual Lexicon
Entries from a Non-Parallel English-Chinese Corpus.
In Proceedings of the 3rd Annual Workshop on Very
Large Corpora, pages 173?183.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving Translation Lexicon In-
duction from Monolingual Corpora via Dependency
Contexts and Part-of-Speech Equivalences. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009), pages
129?137.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve De?jean. 2004. A Geomet-
34
ric View on Bilingual Lexicon Extraction from Com-
parable Corpora. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguistics
(ACL 2004), pages 526?533.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning Bilingual Lexicons
from Monolingual Corpora. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2008): the Human Language
Technology Conference (HLT), pages 771?779.
Amir Hazem, Emmanuel Morin, and Sebastian Pen?a Sal-
darriaga. 2011. Bilingual Lexicon Extraction from
Comparable Corpora as Metasearch. In Proceedings
of the 4th Workshop on Building and Using Compara-
ble Corpora, pages 35?43.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005. Min-
ing Key Phrase Translations from Web Corpora. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing (HLT-EMNLP 2005), pages
483?490.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora us-
ing in-domain terms. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 481?489.
Hiroyuki Kaji. 2005. Extracting Translation Equivalents
from Bilingual Comparable Corpora. IEICE - Trans.
Inf. Syst., E88-D:313?323.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin. 2011.
Machine Transliteration Survey. ACM Computing
Surveys, 43(3):1?46.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics, 24:599?
612.
Philipp Koehn and Kevin Knight. 2002. Learning a
Translation Lexicon from Monolingual Corpora. In
Proceedings of ACL Workshop on Unsupervised Lexi-
cal Acquisition, pages 9?16.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2010), pages
145?153.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), pages
617?625.
Florian Laws, Lukas Michelbacher, Beate Dorow, Chris-
tian Scheible, Ulrich Heid, and Hinrich Schu?tze. 2010.
A Linguistically Grounded Graph Model for Bilingual
Lexicon Extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 614?622.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining Parenthetical Transla-
tions from the Web by Word Alignment. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL 2008): the Human
Language Technology Conference (HLT), pages 994?
1002.
Wen-Hsiang Lu, Lee-Feng Chien, and Hsi-Jian Lee.
2004. Anchor Text Mining for Translation of Web
Queries: A Transitive Translation Approach. ACM
Transactions on Information Systems, 22(2):242?269.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual Lexicon Extraction from Comparable Cor-
pora Enhanced with Parallel Corpora. In Proceedings
of the 4th Workshop on Building and Using Compara-
ble Corpora, pages 27?34.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining -
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics (ACL 2007), pages
664?671.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005.
Word Sense Disambiguation Using Label Propagation
Based Semi-Supervised Learning. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL 2005), pages 395?402.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29:19?51.
Pablo Gamallo Otero and Jose? Ramom Pichel Campos.
2008. Learning Spanish-Galician Translation Equiva-
lents Using a Comparable Corpus and a Bilingual Dic-
tionary. In Proceedings of the 9th International Con-
ference on Computational Linguistics and Intelligent
Text Processing (CICLing 2008), pages 423?433.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding Translations for Low-
Frequency Words in Comparable Corpora. Machine
Translation, 20:247?266.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compara-
ble Documents. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT2011),
pages 1327?1335.
Reinhard Rapp. 1995. Identifying Word Translations in
Non-Parallel Texts. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL 1995), pages 320?322.
35
Reinhard Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German Cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL
1999), pages 519?526.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Li Shao and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING 2004), pages 618?624.
Partha Pratim Talukdar and Koby Crammer. 2009. New
Regularized Algorithms for Transductive Learning. In
Proceedings of the European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML-PKDD 2009), pages
442?457.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying Word Translations from Compara-
ble Corpora Using Latent Topic Models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2011), pages 479?484.
Dekai Wu and Xuanyin Xia. 1994. Learning an English-
Chinese Lexicon from a Parallel Corpus. In Proceed-
ings of the First Conference of the Association for Ma-
chine Translation in the Americas (AMTA 1994), pages
206?213.
Ying Zhang and Phil Vines. 2004. Using the Web for
Automated Translation Extraction in Cross-Language
Information Retrieval. In Proceedings of the 27th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 162?169.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from Labeled and Unlabeled Data with Label Propa-
gation. Technical report, CMU-CALD-02-107.
36
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Locally Training the Log-Linear Model for SMT
Lemao Liu1, Hailong Cao1, Taro Watanabe2, Tiejun Zhao1, Mo Yu1, CongHui Zhu1
1School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu,hailong,tjzhao,yumo,chzhu}@mtlab.hit.edu.cn
taro.watanabe@nict.go.jp
Abstract
In statistical machine translation, minimum
error rate training (MERT) is a standard
method for tuning a single weight with regard
to a given development data. However, due to
the diversity and uneven distribution of source
sentences, there are two problems suffered by
this method. First, its performance is highly
dependent on the choice of a development set,
which may lead to an unstable performance
for testing. Second, translations become in-
consistent at the sentence level since tuning is
performed globally on a document level. In
this paper, we propose a novel local training
method to address these two problems. Un-
like a global training method, such as MERT,
in which a single weight is learned and used
for all the input sentences, we perform training
and testing in one step by learning a sentence-
wise weight for each input sentence. We pro-
pose efficient incremental training methods to
put the local training into practice. In NIST
Chinese-to-English translation tasks, our lo-
cal training method significantly outperforms
MERT with the maximal improvements up to
2.0 BLEU points, meanwhile its efficiency is
comparable to that of the global method.
1 Introduction
Och and Ney (2002) introduced the log-linear model
for statistical machine translation (SMT), in which
translation is considered as the following optimiza-
tion problem:
e?(f ;W ) = arg max
e
P(e|f ;W )
= arg max
e
exp
{
W ? h(f, e)
}
?
e? exp
{
W ? h(f, e?)
}
= arg max
e
{
W ? h(f, e)
}
, (1)
where f and e (e?) are source and target sentences,
respectively. h is a feature vector which is scaled
by a weight W . Parameter estimation is one of
the most important components in SMT, and var-
ious training methods have been proposed to tune
W . Some methods are based on likelihood (Och and
Ney, 2002; Blunsom et al2008), error rate (Och,
2003; Zhao and Chen, 2009; Pauls et al2009; Gal-
ley and Quirk, 2011), margin (Watanabe et al2007;
Chiang et al2008) and ranking (Hopkins and May,
2011), and among which minimum error rate train-
ing (MERT) (Och, 2003) is the most popular one.
All these training methods follow the same
pipeline: they train only a single weight on a given
development set, and then use it to translate all the
sentences in a test set. We call them a global train-
ing method. One of its advantages is that it allows us
to train a single weight offline and thereby it is effi-
cient. However, due to the diversity and uneven dis-
tribution of source sentences(Li et al2010), there
are some shortcomings in this pipeline.
Firstly, on the document level, the performance of
these methods is dependent on the choice of a devel-
opment set, which may potentially lead to an unsta-
ble translation performance for testing. As referred
in our experiment, the BLEU points on NIST08 are
402
 Source  Candidate Translation   
i  
i
f  j  
ij
e  h  score  
1 ? ? ?? ? 1 I am students . <2, 1> 0.5 
  2 I was students . <1,1> 0.2 
2 ?? ?? ? ? 1 week several today ? <1,2> 0.3 
  2 today several weeks . <3,2> 0.1 
 
(a) (b)
2 21 2 222,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 212,0 ( , ) ( , )h f e h f e? ?? ?1 11 1 11, 0 ( , ) ( , )h f e h f e? ?? ?
1 12 1 111,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 21( , ) ( , )h f e h f e?
1 11 1 12( , ) ( , )h f e h f e?
<-2,0>
<-1,0>
<1,0>
<2,0>
0h1h
. .* *
2 21 2 22( , ) ( , )h f e h f e?
1 12 1 11( , ) ( , )h f e h f e?
Figure 1: (a). An Example candidate space of dimensionality two. score is a evaluation metric of e. (b). The non-
linearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011). Since
score of e11 is greater than that of e12, ?1, 0? corresponds to a possitive example denoted as ???, and ??1, 0? corre-
sponds to a negative example denoted as ?*?. Since the transformed classification problem is not linearly separable,
there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can
obtain e11 and e21 with weights: ?1, 1? and ??1, 1?, respectively.
19.04 when the Moses system is tuned on NIST02
by MERT. However, its performance is improved to
21.28 points when tuned on NIST06. The automatic
selection of a development set may partially address
the problem. However it is inefficient since tuning
requires iteratively decoding an entire development
set, which is impractical for an online service.
Secondly, translation becomes inconsistent on the
sentence level (Ma et al2011). Global training
method such as MERT tries to optimize the weight
towards the best performance for the whole set, and
it can not necessarily always obtain good translation
for every sentence in the development set. The rea-
son is that different sentences may need different
optimal weights, and MERT can not find a single
weight to satisfy all of the sentences. Figure 1(a)
shows such an example, in which a development set
contains two sentences f1 and f2 with translations e
and feature vectors h. When we tune examples in
Figure 1(a) by MERT, it can be regarded as a non-
linearly separable classification problem illustrated
in Figure 1(b). Therefore, there exists no single
weightW which simultaneously obtains e11 and e21
as translation for f1 and f2 via Equation (1). How-
ever, we can achieve this with two weights: ?1, 1?
for f1 and ??1, 1? for f2.
In this paper, inspired by KNN-SVM (Zhang et
al., 2006), we propose a local training method,
which trains sentence-wise weights instead of a sin-
gle weight, to address the above two problems.
Compared with global training methods, such as
MERT, in which training and testing are separated,
our method works in an online fashion, in which
training is performed during testing. This online
fashion has an advantage in that it can adapt the
weights for each of the test sentences, by dynam-
ically tuning the weights on translation examples
which are similar to these test sentences. Similar
to the method of development set automatical selec-
tion, the local training method may also suffer the
problem of efficiency. To put it into practice, we
propose incremental training methods which avoid
retraining and iterative decoding on a development
set.
Our local training method has two advantages:
firstly, it significantly outperforms MERT, especially
when test set is different from the development set;
secondly, it improves the translation consistency.
Experiments on NIST Chinese-to-English transla-
tion tasks show that our local training method sig-
nificantly gains over MERT, with the maximum im-
provements up to 2.0 BLEU, and its efficiency is
comparable to that of the global training method.
2 Local Training and Testing
The local training method (Bottou and Vapnik,
1992) is widely employed in computer vision
(Zhang et al2006; Cheng et al2010). Compared
with the global training method which tries to fit
a single weight on the training data, the local one
learns weights based on the local neighborhood in-
formation for each test example. It is superior to
403
the global one when the data sets are not evenly
distributed (Bottou and Vapnik, 1992; Zhang et al
2006).
Algorithm 1 Naive Local Training Method
Input: T = {ti}Ni=1(test set), K (retrieval size),
Dev(development set), D(retrieval data)
Output: Translation results of T
1: for all sentence ti such that 1 ? i ? N do
2: Retrieve the training examples Di with size
K for ti from D according to a similarity;
3: Train a local weight W i based on Dev and
Di;
4: Decode ti with W i;
5: end for
Suppose T be a test set, Dev a development set,
and D a retrieval data. The local training in SMT
is described in the Algorithm 1. For each sentence
ti in test set, training examples Di is retrieved from
D using a similarity measure (line 2), a weight W i
is optimized on Dev and Di (line 3)1, and, finally,
ti is decoded with W i for testing (line 4). At the
end of this algorithm, it returns the translation re-
sults for T . Note that weights are adapted for each
test sentence ti in line 3 by utilizing the translation
examples Di which are similar to ti. Thus, our local
training method can be considered as an adaptation
of translation weights.
Algorithm 1 suffers a problem of training effi-
ciency in line 3. It is impractical to train a weight
W i on Dev and Di from scratch for every sen-
tence, since iteratively decodingDev andDi is time
consuming when we apply MERT. To address this
problem, we propose a novel incremental approach
which is based on a two-phase training.
On the first phase, we use a global training
method, like MERT, to tune a baseline weight on
the development set Dev in an offline manner. On
the second phase, we utilize the retrieved examples
to incrementally tune sentence-wise local weights
based on the baseline weight. This method can
not only consider the common characteristics learnt
from the Dev, but also take into account the knowl-
1Usually, the quality of development set Dev is high, since
it is manually produced with multiple references. This is the
main reason why Dev is used as a part of new development set
to train W i.
edge for each individual sentence learnt from sim-
ilar examples during testing. On the phase of in-
cremental training, we perform decoding only once
for retrieved examples Di, though several rounds of
decoding are possible and potentially better if one
does not seriously care about training speed. Fur-
thermore, instead of on-the-fly decoding, we decode
the retrieval data D offline using the parameter from
our baseline weight and its nbest translation candi-
dates are saved with training examples to increase
the training efficiency.
Algorithm 2 Local Training Method Based on In-
cremental Training
Input: T = {ti}Ni=1 (test set), K (retrieval size),
Dev (development set),
D = {?fs, rs?}s=Ss=1 (retrieval data),
Output: Translation results of T
1: Run global Training (such as MERT) on Dev to
get a baseline weight Wb; // Phase 1
2: Decode each sentence in D to get
D = {?fs, cs, rs?}s=Ss=1 ;
3: for all sentence ti such that 1 ? i ? N do
4: Retrieve K training examples Di =
{?f ij , c
i
j , r
i
j?}
j=K
j=1 for ti from D according to
a similarity;
5: Incrementally train a local weight W i based
on Wb and Di; // Phase 2
6: Decode ti with W i;
7: end for
The two-phase local training algorithm is de-
scribed in Algorithm 2, where cs and rs denote the
translation candidate set and reference set for each
sentence fs in retrieval data, respectively, and K is
the retrieval size. It globally trains a baseline weight
Wb (line 1), and decodes each sentence in retrieval
data D with the weight Wb (line 2). For each sen-
tence ti in test set T , it first retrieves training exam-
ples Di from D (line 4), and then it runs local train-
ing to tune a local weight W i (line 5) and performs
testing with W i for ti (line 6). Please note that the
two-phase training contains global training in line 1
and local training in line 5.
From Algorithm 2, one can see that our method is
effective even if the test set is unknow, for example,
in the scenario of online translation services, since
the global training on development set and decoding
404
on retrieval data can be performed offline.
In the next two sections, we will discuss the de-
tails about the similarity metric in line 4 and the in-
cremental training in line 5 of Algorithm 2.
3 Acquiring Training Examples
In line 4 of Algorithm 2, to retrieve training exam-
ples for the sentence ti , we first need a metric to
retrieve similar translation examples. We assume
that the metric satisfy the property: more similar the
test sentence and translation examples are, the better
translation result one obtains when decoding the test
sentence with the weight trained on the translation
examples.
The metric we consider here is derived from
an example-based machine translation. To retrieve
translation examples for a test sentence, (Watanabe
and Sumita, 2003) defined a metric based on the
combination of edit distance and TF-IDF (Manning
and Schu?tze, 1999) as follows:
dist(f1, f2) = ? ? edit-dist(f1, f2)+
(1? ?)? tf-idf(f1, f2), (2)
where ?(0 ? ? ? 1) is an interpolation weight,
fi(i = 1, 2) is a word sequence and can be also
considered as a document. In this paper, we extract
similar examples from training data. Like example-
based translation in which similar source sentences
have similar translations, we assume that the optimal
translation weights of the similar source sentences
are closer.
4 Incremental Training Based on
Ultraconservative Update
Compared with retraining mode, incremental train-
ing can improve the training efficiency. In the field
of machine learning research, incremental training
has been employed in the work (Cauwenberghs and
Poggio, 2001; Shilton et al2005), but there is lit-
tle work for tuning parameters of statistical machine
translation. The biggest difficulty lies in that the fea-
ture vector of a given training example, i.e. transla-
tion example, is unavailable until actually decoding
the example, since the derivation is a latent variable.
In this section, we will investigate the incremental
training methods in SMT scenario.
Following the notations in Algorithm 2, Wb is
the baseline weight, Di = {?f ij , c
i
j , r
i
j?}
K
j=1 denotes
training examples for ti. For the sake of brevity, we
will drop the index i, Di = {?fj , cj , rj?}Kj=1, in the
rest of this paper. Our goal is to find an optimal
weight, denoted by W i, which is a local weight and
used for decoding the sentence ti. Unlike the global
method which performs tuning on the whole devel-
opment set Dev +Di as in Algorithm 1, W i can be
incrementally learned by optimizing onDi based on
Wb. We employ the idea of ultraconservative update
(Crammer and Singer, 2003; Crammer et al2006)
to propose two incremental methods for local train-
ing in Algorithm 2 as follows.
Ultraconservative update is an efficient way to
consider the trade-off between the progress made on
development set Dev and the progress made on Di.
It desires that the optimal weight W i is not only
close to the baseline weight Wb, but also achieves
the low loss over the retrieved examples Di. The
idea of ultraconservative update can be formalized
as follows:
min
W
{
d(W,Wb) + ? ? Loss(D
i,W )
}
, (3)
where d(W,Wb) is a distance metric over a pair
of weights W and Wb. It penalizes the weights
far away from Wb and it is L2 norm in this paper.
Loss(Di,W ) is a loss function of W defined on Di
and it evaluates the performance of W over Di. ?
is a positive hyperparameter. If Di is more similar
to the test sentence ti, the better performance will be
achieved for the larger ?. In particular, ifDi consists
of only a single sentence ti, the best performance
will be obtained when ? goes to infinity.
4.1 Margin Based Ultraconservative Update
MIRA(Crammer and Singer, 2003; Crammer et al
2006) is a form of ultraconservative update in (3)
whoseLoss is defined as hinge loss based on margin
over the pairwise translation candiates in Di. It tries
to minimize the following quadratic program:
1
2
||W ?Wb||
2+
?
K
K?
j=1
max
1?n?|cj |
(
`jn?W ??h(fj , ejn)
)
with
?h(fj , ejn) = h(fj , ej?)? h(fj , ejn), (4)
405
where h(fj , e) is the feature vector of candidate e,
ejn is a translation member of fj in cj , ej? is the
oracle one in cj , `jn is a loss between ej? and ejn
and it is the same as referred in (Chiang et al2008),
and |cj | denotes the number of members in cj .
Different from (Watanabe et al2007; Chiang
et al2008) employing the MIRA to globally train
SMT, in this paper, we apply MIRA as one of local
training method for SMT and we call it as margin
based ultraconservative update (MBUU for shortly)
to highlight its advantage of incremental training in
line 5 of Algorithm 2.
Further, there is another difference between
MBUU and MIRA in (Watanabe et al2007; Chi-
ang et al2008). MBUU is a batch update mode
which updates the weight with all training examples,
but MIRA is an online one which updates with each
example (Watanabe et al2007) or part of examples
(Chiang et al2008). Therefore, MBUU is more ul-
traconservative.
4.2 Error Rate Based Ultraconservative
Update
Instead of taking into account the margin-based
hinge loss between a pair of translations as the Loss
in (3), we directly optimize the error rate of trans-
lation candidates with respect to their references in
Di. Formally, the objective function of error rate
based ultraconservative update (EBUU) is as fol-
lows:
1
2
?W ?Wb?
2 +
?
K
K?
j=1
Error(rj ; e?(fj ;W )), (5)
where e?(fj ;W ) is defined in Equation (1), and
Error(rj , e) is the sentence-wise minus BLEU (Pa-
pineni et al2002) of a candidate e with respect to
rj .
Due to the existence of L2 norm in objective
function (5), the optimization algorithm MERT can
not be applied for this question since the exact line
search routine does not hold here. Motivated by
(Och, 2003; Smith and Eisner, 2006), we approxi-
mate the Error in (5) by the expected loss, and then
derive the following function:
1
2
?W?Wb?
2+
?
K
K?
j=1
?
e
Error(rj ; e)P?(e|fj ;W ),
(6)
Systems NIST02 NIST05 NIST06 NIST08
Moses 30.39 26.31 25.34 19.07
Moses hier 33.68 26.94 26.28 18.65
In-Hiero 31.24 27.07 26.32 19.03
Table 1: The performance comparison of the baseline In-
Hiero VS Moses and Moses hier.
with
P?(e|fj ;W ) =
exp[?W ? h(fj , e)]
?
e??cj exp[?W ? h(fj , e
?)]
, (7)
where ? > 0 is a real number valued smoother. One
can see that, in the extreme case, for ? ? ?, (6)
converges to (5).
We apply the gradient decent method to minimize
the function (6), as it is smooth with respect to ?.
Since the function (6) is non-convex, the solution
obtained by gradient descent method may depend on
the initial point. In this paper, we set the initial point
as Wb in order to achieve a desirable solution.
5 Experiments and Results
5.1 Setting
We conduct our experiments on the Chinese-to-
English translation task. The training data is FBIS
corpus consisting of about 240k sentence pairs. The
development set is NIST02 evaluation data, and the
test datasets are NIST05, NIST06,and NIST08.
We run GIZA++ (Och and Ney, 2000) on the
training corpus in both directions (Koehn et al
2003) to obtain the word alignment for each sen-
tence pair. We train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus using the SRILM Toolkits (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). In our experiments the translation per-
formances are measured by case-insensitive BLEU4
metric (Papineni et al2002) and we use mteval-
v13a.pl as the evaluation tool. The significance test-
ing is performed by paired bootstrap re-sampling
(Koehn, 2004).
We use an in-house developed hierarchical
phrase-based translation (Chiang, 2005) as our base-
line system, and we denote it as In-Hiero. To ob-
tain satisfactory baseline performance, we tune In-
Hiero system for 5 times using MERT, and then se-
406
Methods Steps Seconds
Global method Decoding 2.0
Local method Retrieval +0.6
Local training +0.3
Table 2: The efficiency of the local training and testing
measured by sentence averaged runtime.
Methods NIST05 NIST06 NIST08
Global MERT 27.07 26.32 19.03
Local MBUU 27.75+ 27.88+ 20.84+
EBUU 27.85+ 27.99+ 21.08+
Table 3: The performance comparison of local train-
ing methods (MBUU and EBUU) and a global method
(MERT). NIST05 is the set used to tune ? for MBUU and
EBUU, and NIST06 and NIST08 are test sets. + means
the local method is significantly better than MERT with
p < 0.05.
lect the best-performing one as our baseline for the
following experiments. As Table 1 indicates, our
baseline In-Hiero is comparable to the phrase-based
MT (Moses) and the hierarchical phrase-based MT
(Moses hier) implemented in Moses, an open source
MT toolkit2 (Koehn et al2007). Both of these sys-
tems are with default setting. All three systems are
trained by MERT with 100 best candidates.
To compare the local training method in Algo-
rithm 2, we use a standard global training method,
MERT, as the baseline training method. We do not
compare with Algorithm 1, in which retraining is
performed for each input sentence, since retraining
for the whole test set is impractical given that each
sentence-wise retraining may take some hours or
even days. Therefore, we just compare Algorithm
2 with MERT.
5.2 Runtime Results
To run the Algorithm 2, we tune the baseline weight
Wb on NIST02 by MERT3. The retrieval data is set
as the training data, i.e. FBIS corpus, and the re-
trieval size is 100. We translate retrieval data with
Wb to obtain their 100 best translation candidates.
We use the simple linear interpolated TF-IDF met-
ric with ? = 0.1 in Section 3 as the retrieval metric.
2See web: http://www.statmt.org
3Wb is exactly the weight of In-Hiero in Table 1.
NIST05 NIST06 NIST08
NIST02 0.665 0.571 0.506
Table 4: The similarity of development and three test
datasets.
For an efficient tuning, the retrieval process is par-
allelized as follows: the examples are assigned to 4
CPUs so that each CPU accepts a query and returns
its top-100 results, then all these top-100 results are
merged into the final top-100 retrieved examples to-
gether with their translation candidates. In our ex-
periments, we employ the two incremental training
methods, i.e. MBUU and EBUU. Both of the hyper-
parameters ? are tuned on NIST05 and set as 0.018
and 0.06 for MBUU and EBUU, respectively. In
the incremental training step, only one CPU is em-
ployed.
Table 2 depicts that testing each sentence with lo-
cal training method takes 2.9 seconds, which is com-
parable to the testing time 2.0 seconds with global
training method4. This shows that the local method
is efficient. Further, compared to the retrieval, the
local training is not the bottleneck. Actually, if we
use LSH technique (Andoni and Indyk, 2008) in re-
trieval process, the local method can be easily scaled
to a larger training data.
5.3 Results and Analysis
Table 3 shows the main results of our local train-
ing methods. The EBUU training method signifi-
cantly outperforms the MERT baseline, and the im-
provement even achieves up to 2.0 BLEU points on
NIST08. We can also see that EBUU and MBUU are
comparable on these three test sets. Both of these
two local training methods achieve significant im-
provements over the MERT baseline, which proves
the effectiveness of our local training method over
global training method.
Although both local methods MBUU and EBUU
achieved improvements on all the datasets, their
gains on NIST06 and NIST08 are significantly
higher than those achieved on NIST05 test dataset.
We conjecture that, the more different a test set and
a development set are, the more potential improvem-
4The runtime excludes the time of tuning and decoding on D
in Algorithm 2, since both of them can be performanced offline.
407
0 . 0 0 0 . 0 2 0 . 0 4 0 . 0 6 0 . 0 8 0 . 1 01 82 02 2
2 42 62 8  
 
 N I S T 0 5 N I S T 0 6 N I S T 0 8BLEU l
Figure 2: The peformance of EBUU for different ? over
all the test datasets. The horizontal axis denotes the val-
ues of ? in function (6), and the vertical one denotes the
BLEU points.
Metthods Dev NIST08
NIST02 19.03
MERT NIST05 20.06
NIST06 21.28
EBUU NIST02 21.08
Table 5: The comparison of MERT with different de-
velopment datasets and local training method based on
EBUU.
nts local training has for the sentences in this test set.
To test our hypothesis, we measured the similarity
between the development set and a test set by the
average value5 of accumulated TF-IDF scores of de-
velopment dataset and each sentence in test datasets.
Table 4 shows that NIST06 and NIST08 are more
different from NIS02 than NIST05, thus, this is po-
tentially the reason why local training is more effec-
tive on NIST06 and NIST08.
As mentioned in Section 1, the global training
methods such as MERT are highly dependent on de-
velopment sets, which can be seen in Table 5. There-
fore, the translation performance will be degraded if
one chooses a development data which is not close
5Instead of using the similarity between two documents de-
velopment and test datasets, we define the similarity as the av-
erage similarity of the development set and the sentences in test
set. The reason is that it reduces its dependency on the number
of sentences in test dataset, which may cause a bias.
Methods Number Percents
MERT 1735 42.3%
EBUU 1606 39.1%
Table 6: The statistics of sentences with 0.0 sentence-
level BLEU points over three test datasets.
to the test data. We can see that, with the help of the
local training, we still gain much even if we selected
an unsatisfactory development data.
As also mentioned in Section 1, the global meth-
ods do not care about the sentence level perfor-
mance. Table 6 depicts that there are 1735 sentences
with zero BLEU points in all the three test datasets
for MERT. Besides obtaining improvements on doc-
ument level as referred in Table 3, the local training
methods can also achieve consistent improvements
on sentence level and thus can improve the users?
experiences.
The hyperparameters ? in both MBUU (4) and
EBUU (6) has an important influence on transla-
tion performance. Figure 2 shows such influence
for EBUU on the test datasets. We can see that, the
performances on all these datasets improve as ? be-
comes closer to 0.06 from 0, and the performance
continues improving when ? passes over 0.06 on
NIST08 test set, where the performance constantly
improves up to 2.6 BLEU points over baseline. As
mentioned in Section 4, if the retrieved examples are
very similar to the test sentence, the better perfor-
mance will be achieved with the larger ?. There-
fore, it is reasonable that the performances improved
when ? increased from 0 to 0.06. Further, the turn-
ing point appearing at 0.06 proves that the ultra-
conservative update is necessary. We can also see
that the performance on NIST08 consistently im-
proves and achieves the maximum gain when ? ar-
rives at 0.1, but those on both NIST05 and NIST06
achieves the best when it arrives at 0.06. This
phenomenon can also be interpreted in Table 4 as
the lowest similarity between the development and
NIST08 datasets.
Generally, the better performance may be
achieved when more examples are retrieved. Actu-
ally, in Table 7 there seems to be little dependency
between the numbers of examples retrieved and the
translation qualities, although they are positively re-
408
Retrieval Size NIST05 NIST06 NIST08
40 27.66 27.81 20.87
70 27.77 27.93 21.08
100 27.85 27.99 21.08
Table 7: The performance comparison by varying re-
trieval size in Algorithm 2 based on EBUU.
Methods NIST05 NIST06 NIST08
MERT 27.07 26.32 19.03
EBUU 27.85 27.99 21.08
Oracle 29.46 29.35 22.09
Table 8: The performance of Oracle of 2-best results
which consist of 1-best resluts of MERT and 1-best
resluts of EBUU.
lated approximately.
Table 8 presents the performance of the oracle
translations selected from the 1-best translation re-
sults of MERT and EBUU. Clearly, there exists more
potential improvement for local training method.
6 Related Work
Several works have proposed discriminative tech-
niques to train log-linear model for SMT. (Och and
Ney, 2002; Blunsom et al2008) used maximum
likelihood estimation to learn weights for MT. (Och,
2003; Moore and Quirk, 2008; Zhao and Chen,
2009; Galley and Quirk, 2011) employed an eval-
uation metric as a loss function and directly opti-
mized it. (Watanabe et al2007; Chiang et al2008;
Hopkins and May, 2011) proposed other optimiza-
tion objectives by introducing a margin-based and
ranking-based indirect loss functions.
All the methods mentioned above train a single
weight for the whole development set, whereas our
local training method learns a weight for each sen-
tence. Further, our translation framework integrates
the training and testing into one unit, instead of treat-
ing them separately. One of the advantages is that it
can adapt the weights for each of the test sentences.
Our method resorts to some translation exam-
ples, which is similar as example-based translation
or translation memory (Watanabe and Sumita, 2003;
He et al2010; Ma et al2011). Instead of using
translation examples to construct translation rules
for enlarging the decoding space, we employed them
to discriminatively learn local weights.
Similar to (Hildebrand et al2005; Lu? et al
2007), our method also employes IR methods to re-
trieve examples for a given test set. Their methods
utilize the retrieved examples to acquire translation
model and can be seen as the adaptation of trans-
lation model. However, ours uses the retrieved ex-
amples to tune the weights and thus can be consid-
ered as the adaptation of tuning. Furthermore, since
ours does not change the translation model which
needs to run GIZA++ and it incrementally trains lo-
cal weights, our method can be applied for online
translation service.
7 Conclusion and Future Work
This paper proposes a novel local training frame-
work for SMT. It has two characteristics, which
are different from global training methods such as
MERT. First, instead of training only one weight for
document level, it trains a single weight for sentence
level. Second, instead of considering the training
and testing as two separate units, we unify the train-
ing and testing into one unit, which can employ the
information of test sentences and perform sentence-
wise local adaptation of weights.
Local training can not only alleviate the prob-
lem of the development data selection, but also re-
duce the risk of sentence-wise bad translation re-
sults, thus consistently improve the translation per-
formance. Experiments show gains up to 2.0 BLEU
points compared with a MERT baseline. With the
help of incremental training methods, the time in-
curred by local training was negligible and the local
training and testing totally took 2.9 seconds for each
sentence.
In the future work, we will further investigate the
local training method, since there are more room for
improvements as observed in our experiments. We
will test our method on other translation models and
larger training data6.
Acknowledgments
We would like to thank Hongfei Jiang and Shujie
Liu for many valuable discussions and thank three
6Intuitionally, when the corpus of translation examples is
larger, the retrieval results in Algorithm 2 are much similar as
the test sentence. Therefore our method may favor this.
409
anonymous reviewers for many valuable comments
and helpful suggestions. This work was supported
by National Natural Science Foundation of China
(61173073,61100093), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207), and the
Fundamental Research Funds for Central Univer-
sites (HIT.NSRIF.2013065).
References
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. Commun. ACM, 51(1):117?122,
January.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Le?on Bottou and Vladimir Vapnik. 1992. Local learning
algorithms. Neural Comput., 4:888?900, November.
G. Cauwenberghs and T. Poggio. 2001. Incremental
and decremental support vector machine learning. In
Advances in Neural Information Processing Systems
(NIPS*2000), volume 13.
Stanley F Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Technical Report TR-10-98. Harvard Univer-
sity.
Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2010. Ef-
ficient algorithm for localized support vector machine.
IEEE Trans. on Knowl. and Data Eng., 22:537?549,
April.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
S. Hildebrand, M. Eck, S. Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of EAMT. Association for Computational
Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.
ACL.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2010. Adaptive development data selection for
log-linear model in statistical machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 662?
670, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
410
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning - a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1239?1248, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics -
Volume 1, COLING ?08, pages 585?592, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 295?302, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427, Singapore, August. Association for
Computational Linguistics.
Alistair Shilton, Marimuthu Palaniswami, Daniel Ralph,
and Ah Chung Tsoi. 2005. Incremental training of
support vector machines. IEEE Transactions on Neu-
ral Networks, 16(1):114?131.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe and Eiichiro Sumita. 2003. Example-
based decoding for statistical machine translation. In
Proc. of MT Summit IX, pages 410?417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hao Zhang, Alexander C. Berg, Michael Maire, and Ji-
tendra Malik. 2006. Svm-knn: Discriminative near-
est neighbor classification for visual category recog-
nition. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ?06, pages 2126?2136,
Washington, DC, USA. IEEE Computer Society.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, NAACL-Short ?09, pages
21?24, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
411
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 843?853, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Inducing a Discriminative Parser to Optimize Machine
Translation Reordering
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
This paper proposes a method for learning
a discriminative parser for machine trans-
lation reordering using only aligned par-
allel text. This is done by treating the
parser?s derivation tree as a latent variable
in a model that is trained to maximize re-
ordering accuracy. We demonstrate that
efficient large-margin training is possible
by showing that two measures of reorder-
ing accuracy can be factored over the parse
tree. Using this model in the pre-ordering
framework results in significant gains in
translation accuracy over standard phrase-
based SMT and previously proposed unsu-
pervised syntax induction methods.
1 Introduction
Finding the appropriate word ordering in the
target language is one of the most difficult prob-
lems for statistical machine translation (SMT),
particularly for language pairs with widely di-
vergent syntax. As a result, there is a large
amount of previous research that handles the
problem of reordering through the use of im-
proved reordering models for phrase-based SMT
(Koehn et al2005), hierarchical phrase-based
translation (Chiang, 2007), syntax-based trans-
lation (Yamada and Knight, 2001), or pre-
ordering (Xia and McCord, 2004).
In particular, systems that use source-
language syntax allow for the handling of long-
distance reordering without large increases in
The first author is now affiliated with the Nara Institute
of Science and Technology.
decoding time. However, these require a good
syntactic parser, which is not available for many
languages. In recent work, DeNero and Uszko-
reit (2011) suggest that unsupervised grammar
induction can be used to create source-sentence
parse structure for use in translation as a part
of a pre-ordering based translation system.
In this work, we present a method for inducing
a parser for SMT by training a discriminative
model to maximize reordering accuracy while
treating the parse tree as a latent variable. As a
learning framework, we use online large-margin
methods to train the model to directly minimize
two measures of reordering accuracy. We pro-
pose a variety of features, and demonstrate that
learning can succeed when no linguistic informa-
tion (POS tags or parse structure) is available in
the source language, but also show that this lin-
guistic information can be simply incorporated
when it is available. Experiments find that the
proposed model improves both reordering and
translation accuracy, leading to average gains
of 1.2 BLEU points on English-Japanese and
Japanese-English translation without linguistic
analysis tools, or up to 1.5 BLEU points when
these tools are incorporated. In addition, we
show that our model is able to effectively max-
imize various measures of reordering accuracy,
and that the reordering measure that we choose
has a direct effect on translation results.
2 Preordering for SMT
Machine translation is defined as transforma-
tion of source sentence F = f1 . . . fJ to target
sentence E = e1 . . . eI . In this paper, we take
843
Figure 1: An example with a source sentence F re-
ordered into target order F ?, and its corresponding
target sentence E. D is one of the BTG derivations
that can produce this ordering.
the pre-ordering approach to machine transla-
tion (Xia and McCord, 2004), which performs
translation as a two step process of reordering
and translation (Figure 1). Reordering first de-
terministically transforms F into F ?, which con-
tains the same words as F but is in the order of
E. Translation then transforms F ? into E using
a method such as phrase-based SMT (Koehn et
al., 2003), which can produce accurate transla-
tions when only local reordering is required.
This general framework has been widely stud-
ied, with the majority of works relying on a
syntactic parser being available in the source
language. Reordering rules are defined over
this parse either through machine learning tech-
niques (Xia and McCord, 2004; Zhang et al
2007; Li et al2007; Genzel, 2010; Dyer and
Resnik, 2010; Khalilov and Sima?an, 2011) or
linguistically motivated manual rules (Collins et
al., 2005; Xu et al2009; Carpuat et al2010;
Isozaki et al2010b). However, as building a
parser for each source language is a resource-
intensive undertaking, there has also been some
interest in developing reordering rules without
the use of a parser (Rottmann and Vogel, 2007;
Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al2011), and we
will follow this thread of research in this paper.
In particular, two methods deserve mention
for being similar to our approach. First, DeNero
and Uszkoreit (2011) learn a reordering model
through a three-step process of bilingual gram-
mar induction, training a monolingual parser
to reproduce the induced trees, and training
a reordering model that selects a reordering
based on this parse structure. In contrast, our
method trains the model in a single step, treat-
ing the parse structure as a latent variable in
a discriminative reordering model. In addition
Tromble and Eisner (2009) and Visweswariah et
al. (2011) present models that use binary clas-
sification to decide whether each pair of words
should be placed in forward or reverse order. In
contrast, our method uses traditional context-
free-grammar models, which allows for simple
parsing and flexible parameterization, including
features such as those that utilize the existence
of a span in the phrase table. Our work is also
unique in that we show that it is possible to di-
rectly optimize several measures of reordering
accuracy, which proves important for achieving
good translations.1
3 Training a Reordering Model with
Latent Derivations
In this section, we provide a basic overview of
the proposed method for learning a reordering
model with latent derivations using online dis-
criminative learning.
3.1 Space of Reorderings
The model we present here is based on the
bracketing transduction grammar (BTG, Wu
(1997)) framework. BTGs represent a binary
tree derivation D over the source sentence F
as shown in Figure 1. Each non-terminal node
can either be a straight (str) or inverted (inv)
production, and terminals (term) span a non-
empty substring f .2
The ordering of the sentence is determined by
the tree structure and the non-terminal labels
str and inv, and can be built bottom-up. Each
subtree represents a source substring f and its
reordered counterpart f ?. For each terminal
node, no reordering occurs and f is equal to f ?.
1The semi-supervised method of Katz-Brown et al
(2011) also optimizes reordering accuracy, but requires
manually annotated parses as seed data.
2In the original BTG framework used in translation,
terminals produce a bilingual substring pair f/e, but as
we are only interested in reordering the source F , we
simplify the model by removing the target substring e.
844
For each non-terminal node spanning f with its
left child spanning f1 and its right child span-
ning f2, if the non-terminal symbol is str, the
reordered strings will be concatenated in order
as f ? = f ?1f ?2, and if the non-terminal symbol is
inv, the reordered strings will be concatenated
in inverted order as f ? = f ?2f ?1.
We define the space of all reorderings that can
be produced by the BTG as F ?, and attempt to
find the best reordering F? ? within this space.3
3.2 Reorderings with Latent
Derivations
In order to find the best reordering F? ? given only
the information in the source side sentence F , we
define a scoring function S(F ?|F ), and choose
the ordering of maximal score:
F? ? = arg max
F ?
S(F ?|F ).
As our model is based on reorderings licensed
by BTG derivations, we also assume that there
is an underlying derivation D that produced F ?.
As we can uniquely determine F ? given F and
D, we can define a scoring function S(D|F ) over
derivations, find the derivation of maximal score
D? = arg max
D
S(D|F )
and use D? to transform F into F ?.
Furthermore, we assume that the score
S(D|F ) is the weighted sum of a number of fea-
ture functions defined over D and F
S(D|F,w) =
?
i
wi?i(D,F )
where ?i is the ith feature function, and wi is
its corresponding weight in weight vector w.
Given this model, we must next consider how
to learn the weights w. As the final goal of our
model is to produce good reorderings F ?, it is
natural to attempt to learn weights that will al-
low us to produce these high-quality reorderings.
3BTGs cannot reproduce all possible reorderings, but
can handle most reorderings occurring in natural trans-
lated text (Haghighi et al2009).
Figure 2: An example of (a) the ranking function
r(fj), (b) loss according to Kendall?s ? , (c) loss ac-
cording to chunk fragmentation.
4 Evaluating Reorderings
Before we explain the learning algorithm, we
must know how to distinguish whether the F ?
produced by the model is good or bad. This
section explains how to calculate oracle reorder-
ings, and assign each F ? a loss and an accuracy
according to how well it reproduces the oracle.
4.1 Calculating Oracle Orderings
In order to calculate reordering quality, we first
define a ranking function r(fj |F,A), which indi-
cates the relative position of source word fj in
the proper target order (Figure 2 (a)). In or-
der to calculate this ranking function, we define
A = a1, . . . ,aJ , where each aj is a set of the in-
dices of the words in E to which fj is aligned.4
Given these alignments, we define an ordering
function aj1 < aj2 that indicates that the in-
dices in aj1 come before the indices in aj2 . For-
mally, we define this function as ?the first index
in aj1 is at most the first index in aj2 , similarly
for the last index, and either the first or last
index in aj1 is less than that of aj2 .?
Given this ordering, we can sort every align-
ment aj , and use its relative position in the sen-
tence to assign a rank to its word r(fj). In
4Null alignments require special treatment. To do so,
we can place unaligned brackets and quotes directly be-
fore and after the spans they surround, and attach all
other unaligned words to the word directly to the right
for head-initial languages (e.g. English), or left for head-
final languages (e.g. Japanese).
845
the case of ties, where neither aj1 < aj2 nor
aj2 < aj1 , both fj1 and fj2 are assigned the
same rank. We can now define measures of re-
ordering accuracy for F ? by how well it arranges
the words in order of ascending rank. It should
be noted that as we allow ties in rank, there
are multiple possible F ? where all words are in
strictly ascending order, which we will call ora-
cle orderings.
4.2 Kendall?s ?
The first measure of reordering accuracy that
we will consider is Kendall?s ? (Kendall, 1938),
a measure of pairwise rank correlation which
has been proposed for evaluating translation re-
ordering accuracy (Isozaki et al2010a; Birch
et al2010) and pre-ordering accuracy (Talbot
et al2011). The fundamental idea behind the
measure lies in comparisons between each pair of
elements f ?j1 and f ?j2 of the reordered sentence,
where j1 < j2. Because j1 < j2, f ?j1 comes before
f ?j2 in the reordered sentence, the ranks should
be r(f ?j1) ? r(f ?j2) in order to produce the cor-
rect ordering.
Based on this criterion, we first define a loss
Lt(F ?) that will be higher for orderings that are
further from the oracle. Specifically, we take the
sum of all pairwise orderings that do not follow
the expected order
Lt(F ?) =
J?1
?
j1=1
J
?
j2=j1+1
?(r(f ?j1) > r(f
?
j2))
where ?(?) is an indicator function that is 1 when
its condition is true, and 0 otherwise. An exam-
ple of this is given in Figure 2 (b).
To calculate an accuracy measure for ordering
F ?, we first calculate the maximum loss for the
sentence, which is equal to the total number of
non-equal rank comparisons in the sentence5
max
F ?
Lt(F ?) =
J?1
?
j1=1
J
?
j2=j1+1
?(r(f ?j1) 6= r(f
?
j2)).
(1)
5The traditional formulation of Kendall?s ? assumes
no ties in rank, and thus the maximum loss can be cal-
culated as J(J ? 1)/2.
Finally, we use this maximum loss to normalize
the actual loss to get an accuracy
At(F ?) = 1?
Lt(F ?)
max
F? ?
Lt(F? ?)
,
which will take a value between 0 (when F ? has
maximal loss), and 1 (when F ? matches one of
the oracle orderings). In Figure 2 (b), Lt(F ?) =
2 and max
F? ?
Lt(F? ?) = 8, so At(F ?) = 0.75.
4.3 Chunk Fragmentation
Another measure that has been used in eval-
uation of translation accuracy (Banerjee and
Lavie, 2005) and pre-ordering accuracy (Talbot
et al2011) is chunk fragmentation. This mea-
sure is based on the number of chunks that the
sentence needs to be broken into to reproduce
the correct ordering, with a motivation that the
number of continuous chunks is equal to the
number of times the reader will have to jump to
a different position in the reordered sentence to
read it in the target order. One way to measure
the number of continuous chunks is considering
whether each word pair f ?j and f ?j+1 is discon-
tinuous (the rank of f ?j+1 is not equal to or one
greater than f ?j)
discont(f ?j , f ?j+1) =
?(r(f ?j) 6= r(f ?j+1) ? r(f ?j) + 1 6= r(f ?j+1))
and sum over all word pairs in the sentence to
create a sentence-based loss
Lc(F ?) =
J?1
?
j=1
discont(f ?j , f ?j+1) (2)
While this is the formulation taken by previ-
ous work, we found that this under-penalizes
bad reorderings of the first and last words of
the sentence, which can contribute to the loss
only once, as opposed to other words which can
contribute to the loss twice. To account for
this, when calculating the chunk fragmentation
score, we additionally add two sentence bound-
ary words f0 and fJ+1 with ranks r(f0) = 0 and
r(fJ+1) = 1 + max
f ?j?F ?
r(f ?j) and redefine the sum-
mation in Equation (2) to consider these words
(e.g. Figure 2 (c)).
846
procedure WeightUpdate(F , A, w)
D ? parse(F,w) . Create parse forest
D? ? argmax
D?D
S(D|F,w) + L(D|F,A)
. Find the model parse
D? ? argmin
D?D
L(D|F,A)? ?S(D|F,w)
. Find the oracle parse
if L(D?|F,A) 6= L(D?|F,A) then
w ? ?(w + ?(?(D?, F )? ?(D?, F )))
. Perform weight update
end if
end procedure
Figure 3: An online update for sentence F , alignment
A, and weight vector w. ? is a very small constant,
and ? and ? are defined by the update strategy.
Similarly to Kendall?s ? , we can also define
an accuracy measure between 0 and 1 using the
maximum loss, which will be at most J + 1,
which corresponds to the total number of com-
parisons made in calculating the loss6
Ac(F ?) = 1?
Lc(F ?)
J + 1
.
In Figure 2 (c), Lc(F ?) = 3 and J + 1 = 6, so
Ac(F ?) = 0.5.
5 Learning a BTG Parser for
Reordering
Now that we have a definition of loss over re-
orderings produced by the model, we have a
clear learning objective: we would like to find
reorderings F ? with low loss. The learning algo-
rithm we use to achieve this goal is motivated
by discriminative training for machine transla-
tion systems (Liang et al2006), and extended
to use large-margin training in an online frame-
work (Watanabe et al2007).
5.1 Learning Algorithm
Learning uses the general framework of large-
margin online structured prediction (Crammer
et al2006), which makes several passes through
the data, finding a derivation with high model
score (the model parse) and a derivation with
6It should be noted that for sentences of length one or
sentences with tied ranks, the maximum loss may be less
than J +1, but for simplicity we use this approximation.
minimal loss (the oracle parse), and updating w
if these two parses diverge (Figure 3).
In order to create both of these parses effi-
ciently, we first create a parse forest encoding a
large number of derivations Di according to the
model scores. Next, we find the model parse D?i,
which is the parse in the forest Di that maxi-
mizes the sum of the model score and the loss
S(Dk|Fk,w)+L(Dk|Fk, Ak). It should be noted
that here we are considering not only the model
score, but also the derivation?s loss. This is
necessary for loss-driven large-margin training
(Crammer et al2006), and follows the basic
intuition that during training, we would like to
make it easier to select negative examples with
large loss, causing these examples to be penal-
ized more often and more heavily.
We also find an oracle parse D?i, which is se-
lected solely to minimize the loss L(Dk|Fk, Ak).
One important difference between the model we
describe here and traditional parsing models is
that the target derivation D?k is a latent variable.
Because many Dk achieve a particular reorder-
ing F ?, many reorderings F ? are able to mini-
mize the loss L(F ?k|Fk, Ak). Thus it is necessary
to choose a single oracle derivation to treat as
the target out of many equally good reorderings.
DeNero and Uszkoreit (2011) resolve this ambi-
guity with four features with empirically tuned
scores before training a monolingual parser and
reordering model. In contrast, we follow previ-
ous work on discriminative learning with latent
variables (Yu and Joachims, 2009), and break
ties within the pool of oracle derivations by se-
lecting the derivation with the largest model
score. From an implementation point of view,
this can be done by finding the derivation that
minimizes L(Dk|Fk, Ak)??S(Dk|Fk,w), where
? is a constant small enough to ensure that the
effect of the loss will always be greater than the
effect of the score.
Finally, if the model parse D?k has a loss that
is greater than that of the oracle parse D?k, we
update the weights to increase the score of the
oracle parse and decrease the score of the model
parse. Any criterion for weight updates may be
used, such as the averaged perceptron (Collins,
2002) and MIRA (Crammer et al2006), but
847
we opted to use Pegasos (Shalev-Shwartz et al
2007) as it allows for the introduction of regu-
larization and relatively stable learning.
To perform this full process, given a source
sentence Fk, alignment Ak, and model weights
w we need to be able to efficiently calculate
scores, calculate losses, and create parse forests
for derivations Dk, the details of which will be
explained in the following sections.
5.2 Scoring Derivation Trees
First, we must consider how to efficiently assign
scores S(D|F,w) to a derivation or forest during
parsing. The most standard and efficient way to
do so is to create local features that can be cal-
culated based only on the information included
in a single node d in the derivation tree. The
score of the whole tree can then be expressed as
the sum of the scores from each node:
S(D|F,w) =
?
d?D
S(d|F,w)
=
?
d?D
?
i
wi?i(d, F ).
Based on this restriction, we define a number of
features that can be used to score the parse tree.
To ease explanation, we represent each node in
the derivation as d = ?s, l, c, c + 1, r?, where s
is the node?s symbol (str, inv, or term), while
l and r are the leftmost and rightmost indices
of the span that d covers. c and c + 1 are the
rightmost index of the left child and leftmost
index of the right child for non-terminal nodes.
All features are intersected with the node la-
bel s, so each feature described below corre-
sponds to three different features (or two for
features applicable to only non-terminal nodes).
? ?lex: Identities of words in positions fl, fr,
fc, fc+1, fl?1, fr+1, flfr, and fcfc+1.
? ?class: Same as ?lex, but with words ab-
stracted to classes. We use the 50 classes
automatically generated by Och (1999)?s
method that are calculated during align-
ment in standard SMT systems.
? ?balance: For non-terminals, features indi-
cating whether the length of the left span
(c? l+1) is lesser than, equal to, or greater
than the length of the right span (r ? c).
? ?table: Features, bucketed by length, that
indicate whether ?fl . . . fr? appears as a
contiguous phrase in the SMT training
data, as well as the log frequency of the
number of times the phrase appears total
and the number of times it appears as a
contiguous phrase (DeNero and Uszkoreit,
2011). Phrase length is limited to 8, and
phrases of frequency one are removed.
? ?pos: Same as ?lex, but with words ab-
stracted to language-dependent POS tags.
? ?cfg: Features indicating the label of the
spans fl . . . fr, fl . . . fc, and fc+1 . . . fr in a
supervised parse tree, and the intersection
of the three labels. When spans do not cor-
respond to a span in the supervised parse
tree, we indicate ?no span? with the label
?X? (Zollmann and Venugopal, 2006).
Most of these features can be calculated from
only a parallel corpus, but ?pos requires a POS
tagger and ?cfg requires a full syntactic parser
in the source language. As it is preferable to
have a method that is applicable in languages
where these tools are not available, we perform
experiments both with and without the features
that require linguistic analysis tools.
5.3 Finding Losses for Derivation Trees
The above features ? and their corresponding
weights w are all that are needed to calculate
scores of derivation trees at test time. However,
during training, it is also necessary to find model
parses according to the loss-augmented scoring
function S(D|F,w)+L(D|F,A) or oracle parses
according to the loss L(D|F,A). As noted by
Taskar et al2003), this is possible if our losses
can be factored in the same way as the feature
space. In this section, we demonstrate that the
loss L(d|F,A) for the evaluation measures we
defined in Section 4 can (mostly) be factored
over nodes in a fashion similar to features.
848
5.3.1 Factoring Kendall?s ?
For Kendall?s ? , in the case of terminal nodes,
Lt(d = ?term, l, r?|F,A) can be calculated by
performing the summation in Equation (1). We
can further define this sum recursively and use
memoization for improved efficiency
Lt(d|F,A) =Lt(?term, l, r ? 1?|F,A)
+
r?1
?
j=l
?(r(fj) > r(fr)). (3)
For non-terminal nodes, we first focus on
straight non-terminals with parent node d =
?str, l, c, c+1, r?, and left and right child nodes
dl = ?sl, l, lc, lc+1, c? and dr = ?sr, c+1, rc, rc+
1, r?. First, we note that the loss for the subtree
rooted at d can be expressed as
Lt(d|F,A) =Lt(dl|F,A) + Lt(dr|F,A)
+
c
?
j1=l
r
?
j2=c+1
?(r(fj1) > r(fj2)).
In other words, the subtree?s total loss can be
factored into the loss of its left subtree, the
loss of its right subtree, and the additional loss
contributed by comparisons between the words
spanning both subtrees. In the case of inverted
terminals, we must simply reverse the compari-
son in the final sum to be ?(r(fj1) < r(fj2)).
5.3.2 Factoring Chunk Fragmentation
Chunk fragmentation loss can be factored in a
similar fashion. First, it is clear that the loss for
the terminal nodes can be calculated efficiently
in a fashion similar to Equation (3). In order to
calculate the loss for non-terminals d, we note
that the summation in Equation (2) can be di-
vided into the sum over the internal bi-grams
in the left and right subtrees, and the bi-gram
spanning the reordered trees
Lc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)
+ discont(f ?c, f ?c+1).
However, unlike Kendall?s ? , this equation re-
lies not on the ranks of fc and fc+1 in the origi-
nal sentence, but on the ranks of f ?c and f ?c+1 in
the reordered sentence. In order to keep track
of these values, it is necessary to augment each
node in the tree to be d = ?s, l, c, c + 1, r, tl, tr?
with two additional values tl and tr that indi-
cate the position of the leftmost and rightmost
words after reordering. Thus, a straight non-
terminal parent d with children dl = ?sl, l, lc, lc+
1, c, tl, tlr? and dr = ?sr, c+1, rc, rc+1, r, trl, tr?
will have loss as follows
Lc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)
+ discont(ftlr, ftrl)
with a similar calculation being possible for in-
verted non-terminals.
5.4 Parsing Derivation Trees
Finally, we must be able to create a parse forest
from which we select model and oracle parses.
As all feature functions factor over single nodes,
it is possible to find the parse tree with the high-
est score in O(J3) time using the CKY algo-
rithm. However, when keeping track of target
positions for calculation of chunk fragmentation
loss, there are a total of O(J5) nodes, an unrea-
sonable burden in terms of time and memory.
To overcome this problem, we note that this set-
ting is nearly identical to translation using syn-
chronous CFGs with an integrated bigram LM,
and thus we can employ cube-pruning to reduce
our search space (Chiang, 2007).
6 Experiments
Our experiments test the reordering and trans-
lation accuracy of translation systems using the
proposed method. As reordering metrics, we use
Kendall?s ? and chunk fragmentation (Talbot et
al., 2011) comparing the system F ? and oracle
F ? calculated with manually created alignments.
As translation metrics, we use BLEU (Papineni
et al2002), as well as RIBES (Isozaki et al
2010a), which is similar to Kendall?s ? , but eval-
uated on the target sentence E instead of the re-
ordered sentence F ?. All scores are the average
of three training runs to control for randomness
in training (Clark et al2011).
For translation, we use Moses (Koehn et al
2007) with lexicalized reordering (Koehn et al
2005) in all experiments. We test three types
849
en-ja ja-en
Chunk ? BLEU RIBES Chunk ? BLEU RIBES
orig 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36
3-step 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42
3-step+?pos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65
3-step+?cfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93
lader 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
lader+?pos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24
lader+?cfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12
Table 2: Reordering (chunk, ?) and translation (BLEU, RIBES) results for each system. Bold numbers
indicate no significant difference from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004).
sent. word (ja) word (en)
RM-train 602 14.5k 14.3k
RM-test 555 11.2k 10.4k
TM/LM 329k 6.08M 5.91M
Tune 1166 26.8k 24.3k
Test 1160 28.5k 26.7k
Table 1: The number of sentences and words for
training and testing the reordering model (RM),
translation model (TM), and language model (LM).
of pre-ordering: original order with F ? ? F
(orig), pre-orderings learned using the 3-step
process of DeNero and Uszkoreit (2011) (3-
step), and the proposed model with latent
derivations (lader).7 Except when stated oth-
erwise, lader was trained to minimize chunk
fragmentation loss with a cube pruning stack
pop limit of 50, and the regularization constant
of 10?3 (chosen through cross-validation).
We test our systems on Japanese-English and
English-Japanese translation using data from
the Kyoto Free Translation Task (Neubig, 2011).
We use the training set for training translation
and language models, the development set for
weight tuning, and the test set for testing (Table
1). We use the designated development and test
sets of manually created alignments as training
data for the reordering models, removing sen-
tences of more than 60 words.
As default features for lader and the mono-
lingual parsing and reordering models in 3-step,
we use all the features described in Section 5.2
7Available open-source: http://phontron.com/lader
except ?pos and ?cfg. In addition, we test sys-
tems with ?pos and ?cfg added. For English,
we use the Stanford parser (Klein and Manning,
2003) for both POS tagging and CFG parsing.
For Japanese, we use the KyTea tagger (Neu-
big et al2011) for POS tagging,8 and the EDA
word-based dependency parser (Flannery et al
2011) with simple manual head-rules to convert
a dependency parse to a CFG parse.
6.1 Effect of Pre-ordering
Table 2 shows reordering and translation results
for orig, 3-step, and lader. It can be seen
that the proposed lader outperforms the base-
lines in both reordering and translation.9 There
are a number of reasons why lader outper-
forms 3-step. First, the pipeline of 3-step
suffers from error propogation, with errors in
monolingual parsing and reordering resulting
in low overall accuracy.10 Second, as Section
5.1 describes, lader breaks ties between ora-
cle parses based on model score, allowing easy-
to-reproduce model parses to be chosen dur-
ing training. In fact, lader generally found
trees that followed from syntactic constituency,
while 3-step more often used terminal nodes
8In addition, following the example of Sudoh et al
(2011a)?s reordering rules, we lexicalize all particles.
9It should be noted that our results for 3-step are
significantly worse than those of DeNero and Uszkoreit
(2011). Likely reasons include a 20x difference in training
data size, the fact that we are using naturally translated
text as opposed to text translated specifically to create
word alignments, or differences in implementation.
10When using oracle parses, chunk accuracy was up to
81%, showing that parsing errors are highly detrimental.
850
en-ja ja-en
Chunk ? BLEU RIBES Chunk ? BLEU RIBES
Lc 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
Lt 70.37 79.57 22.57 69.47 72.51 78.93 18.52 66.26
Lc + Lt 72.55 80.58 22.89 70.34 74.44 79.82 19.21 66.48
Table 3: Results for systems trained to optimize chunk fragmentation (Lc) or Kendall?s ? (Lt).
that spanned constituent boundaries (as long as
the phrase frequency was high). Finally, as Sec-
tion 6.2 shows in detail, the ability of lader to
maximize reordering accuracy directly allows for
improved reordering and translation results.
It can also be seen that incorporating POS
tags or parse trees improves accuracy of both
lader and 3-step, particularly for English-
Japanese, where syntax has proven useful for
pre-ordering, and less so for Japanese-English,
where syntactic pre-ordering has been less suc-
cessful (Sudoh et al2011b).
We also tested Moses?s implementation of hi-
erarchical phrase-based SMT (Chiang, 2007),
which achieved BLEU scores of 23.21 and 19.30
for English-Japanese and Japanese-English re-
spectively, approximately matching lader in
accuracy, but with a significant decrease in de-
coding speed. Further, when pre-ordering with
lader and hierarchical phrase-based SMT were
combined, BLEU scores rose to 23.29 and 19.69,
indicating that the two techniques can be com-
bined for further accuracy improvements.
6.2 Effect of Training Loss
Table 3 shows results when one of three losses is
optimized during training: chunk fragmentation
(Lc), Kendall?s ? (Lt), or the linear interpola-
tion of the two with weights chosen so that both
losses contribute equally (Lt + Lc). In general,
training successfully maximizes the criterion it is
trained on, and Lt +Lc achieves good results on
both measures. We also find that Lc and Lc+Lt
achieve the best translation results, which is
in concert with Talbot et al2011), who find
chunk fragmentation is better correlated with
translation accuracy than Kendall?s ? . This is
an important result, as methods such as that
of Tromble and Eisner (2009) optimize pairwise
en-ja ja-en
BLEU/RIBES BLEU/RIBES
orig 21.87 68.25 18.34 65.36
man-602 23.11 69.86 19.54 66.93
auto-602 22.39 69.19 18.58 66.07
auto-10k 22.53 69.68 18.79 66.89
Table 4: Results based on data size, and whether
manual or automatic alignments are used in training.
word comparisons equivalent to Lt, which may
not be optimal for translation.
6.3 Effect of Automatic Alignments
Table 4 shows the difference between using man-
ual and automatic alignments in the training of
lader. lader is able to improve over the orig
baseline in all cases, but when equal numbers
of manual and automatic alignments are used,
the reorderer trained on manual alignments is
significantly better. However, as the number of
automatic alignments is increased, accuracy im-
proves, approaching that of the system trained
on a smaller number of manual alignments.
7 Conclusion
We presented a method for learning a discrim-
inative parser to maximize reordering accuracy
for machine translation. Future work includes
application to other language pairs, develop-
ment of more sophisticated features, investiga-
tion of probabilistic approaches to inference, and
incorporation of the learned trees directly in
tree-to-string translation.
Acknowledgments
We thank Isao Goto, Tetsuo Kiso, and anony-
mous reviewers for their helpful comments, and
Daniel Flannery for helping to run his parser.
851
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation
with improved correlation with human judgments.
In Proc. ACL Workshop.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, 24(1):15?26.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving arabic-to-english statistical ma-
chine translation by reordering post-verbal sub-
jects for alignment. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proc. ACL, pages
176?181.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
John DeNero and Jakob Uszkoreit. 2011. Induc-
ing sentence structure from parallel corpora for
reordering. In Proc. EMNLP.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT-
NAACL.
Daniel Flannery, Yusuke Miyao, Graham Neubig,
and Shinsuke Mori. 2011. Training dependency
parsers from partially annotated corpora. In Proc.
IJCNLP, pages 776?784, Chiang Mai, Thailand,
November.
Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale machine
translation. In Proc. COLING.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proc. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, and Hajime Tsukada. 2010a. Auto-
matic evaluation of translation quality for distant
language pairs. In Proc. EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,
and Kevin Duh. 2010b. Head finalization: A
simple reordering rule for sov languages. In Proc.
WMT and MetricsMATR.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proc. EMNLP, pages 183?192.
Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika, 30(1/2):81?93.
Maxim Khalilov and Khalil Sima?an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proc. IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. ACL, pages
423?430.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT, pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation eval-
uation. In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc.
EMNLP.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. ACL.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Proc.
ACL, pages 761?768.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. ACL,
pages 529?533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proc. EACL.
852
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
COLING, pages 311?318.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
pos-based distortion model. In Proc. of TMI-2007.
Shai Shalev-Shwartz, Yoram Singer, and Nathan
Srebro. 2007. Pegasos: Primal estimated sub-
gradient solver for SVM. In Proc. ICML, pages
807?814.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2011a. NTT-
UT statistical machine translation in NTCIR-9
PatentMT. In Proc. NTCIR.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. 2011b. Post-
ordering in statistical machine translation. In
Proc. MT Summit.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering. In Proc. WMT.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin Markov networks. Proc. NIPS,
16.
Roy Tromble and Jason Eisner. 2009. Learning lin-
ear ordering problems for better translation. In
Proc. EMNLP.
Karthik Visweswariah, Rajakrishnan Rajkumar,
Ankur Gandhe, Ananthakrishnan Ramanathan,
and Jiri Navratil. 2011. A word reordering
model for improved machine translation. In Proc.
EMNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proc.
EMNLP, pages 764?773.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3).
Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proc. COLING.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables.
In Proc. ICML, pages 1169?1176.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statis-
tical machine translation. In Proc. SSST.
Andreas Zollmann and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proc. WMT, pages 138?141.
853
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153?158,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Word Alignment Using Frequency Constraint in Posterior
Regularized EM
Hidetaka Kamigaito
1,2
, Taro Watanabe
2
, Hiroya Takamura
1
, Manabu Okumura
1
1
Tokyo Institute of Technology, Precision and Intelligence Laboratory
4259 Nagatsuta-cho Midori-ku Yokohama, Japan
2
National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
Generative word alignment models, such
as IBM Models, are restricted to one-
to-many alignment, and cannot explicitly
represent many-to-many relationships in
a bilingual text. The problem is par-
tially solved either by introducing heuris-
tics or by agreement constraints such that
two directional word alignments agree
with each other. In this paper, we fo-
cus on the posterior regularization frame-
work (Ganchev et al., 2010) that can force
two directional word alignment models
to agree with each other during train-
ing, and propose new constraints that can
take into account the difference between
function words and content words. Ex-
perimental results on French-to-English
and Japanese-to-English alignment tasks
show statistically significant gains over the
previous posterior regularization baseline.
We also observed gains in Japanese-to-
English translation tasks, which prove the
effectiveness of our methods under gram-
matically different language pairs.
1 Introduction
Word alignment is an important component in sta-
tistical machine translation (SMT). For instance
phrase-based SMT (Koehn et al., 2003) is based
on the concept of phrase pairs that are automat-
ically extracted from bilingual data and rely on
word alignment annotation. Similarly, the model
for hierarchical phrase-based SMT is built from
exhaustively extracted phrases that are, in turn,
heavily reliant on word alignment.
The Generative word alignment models, such as
the IBM Models (Brown et al., 1993) and HMM
(Vogel et al., 1996), are popular methods for au-
tomatically aligning bilingual texts, but are re-
stricted to represent one-to-many correspondence
of each word. To resolve this weakness, vari-
ous symmetrization methods are proposed. Och
and Ney (2003) and Koehn et al. (2003) propose
various heuristic methods to combine two direc-
tional models to represent many-to-many relation-
ships. As an alternative to heuristic methods, fil-
tering methods employ a threshold to control the
trade-off between precision and recall based on
a score estimated from the posterior probabili-
ties from two directional models. Matusov et al.
(2004) proposed arithmetic means of two mod-
els as a score for the filtering, whereas Liang et
al. (2006) reported better results using geometric
means. The joint training method (Liang et al.,
2006) enforces agreement between two directional
models. Posterior regularization (Ganchev et al.,
2010) is an alternative agreement method which
directly encodes agreement during training. DeN-
ero and Macherey (2011) and Chang et al. (2014)
also enforce agreement during decoding.
However, these agreement models do not take
into account the difference in language pairs,
which is crucial for linguistically different lan-
guage pairs, such as Japanese and English: al-
though content words may be aligned with each
other by introducing some agreement constraints,
function words are difficult to align.
We focus on the posterior regularization frame-
work and improve upon the previous work by
proposing new constraint functions that take into
account the difference in languages in terms of
content words and function words. In particular,
we differentiate between content words and func-
tion words by frequency in bilingual data, follow-
ing Setiawan et al. (2007).
Experimental results show that the proposed
methods achieved better alignment qualities on the
French-English Hansard data and the Japanese-
English Kyoto free translation task (KFTT) mea-
sured by AER and F-measure. In translation eval-
uations, we achieved statistically significant gains
153
in BLEU scores in the NTCIR10.
2 Statistical word alignment with
posterior regularization framework
Given a bilingual sentence x = (x
s
,x
t
) where x
s
andx
t
denote a source and target sentence, respec-
tively, the bilingual sentence is aligned by a many-
to-many alignment of y. We represent posterior
probabilities from two directional word alignment
models as
??
p
?
(
??
y |x) and
??
p
?
(
??
y |x) with each ar-
row indicating a particular direction, and use ? to
denote the parameters of the models. For instance,
??
y is a subset of y for the alignment from x
s
to
x
t
under the model of p(x
t
,
??
y |x
s
). In the case of
IBM Model 1, the model is represented as follows:
p(xt,??y |xs) =
?
i
1
|xs|+ 1
p
t
(x
t
i
|xs??
y
i
). (1)
where we define the index of x
t
, x
s
as i, j(1 ?
i ? |x
t
|, 1 ? j ? |x
s
|) and the posterior probabil-
ity for the word pair (x
t
i
, x
s
j
) is defined as follows:
??
p (i, j|x) =
p
t
(x
t
i
|x
s
j
)
?
j
?
p
t
(x
t
i
|x
s
j
?
)
. (2)
Herein, we assume that the posterior probabil-
ity for wrong directional alignment is zero (i.e.,
??
p (
??
y |x) = 0).
1
Given the two directional mod-
els, Ganchev et al. defined a symmetric feature for
each target/source position pair, i, j as follows:
?
i,j
(x,y) =
?
?
?
+1 (
??y ? y) ? (??y
i
= j),
?1 (
??y ? y) ? (??y
j
= i),
0 otherwise.
(3)
The feature assigns 1 for the subset of word align-
ment for
??
y , but assigns ?1 for
??
y . As a result,
if a word pair i, j is aligned with equal posterior
probabilities in two directions, the expectation of
the feature value will be zero. Ganchev et al. de-
fined a joint model that combines two directional
models using arithmetic means:
p
?
(y|x) =
1
2
??
p
?
(y|x) +
1
2
??
p
?
(y|x). (4)
Under the posterior regularization framework, we
instead use q that is derived by maximizing the fol-
lowing posterior probability parametrized by ? for
each bilingual data x as follows (Ganchev et al.,
2010):
q?(y|x) =
??
p
?
(
??y |x) +??p
?
(
??y |x)
2
(5)
?
exp{?? ? ?(x,y)}
Z
1
No alignment is represented by alignment into a special
token ?null?.
=
??
q (
??y |x)
Z??
q
??
p
?
(x) +
??
q (
??y |x)
Z??
q
??
p
?
(x)
2Z
,
Z =
1
2
(
Z
??
q
??
p
?
+
Z
??
q
??
p
?
),
??
q (
??y |x) =
1
Z
??
q
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
Z
??
q
=
?
??
y
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
??
q (
??y |x) =
1
Z
??
q
??
p
?
(
??y ,x)exp{?? ? ?(x, y)},
Z
??
q
=
?
??
y
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
such that E
q? [?i,j(x,y)] = 0. In the E-step of
EM-algorithm, we employ q? instead of p? to ac-
cumulate fractional counts for its use in the M-
step. ? is efficiently estimated by the gradient as-
cent for each bilingual sentence x. Note that pos-
terior regularization is performed during parame-
ter estimation, and not during testing.
3 Posterior Regularization with
Frequency Constraint
The symmetric constraint method represented in
Equation (3) assumes a strong one-to-one rela-
tion for any word, and does not take into account
the divergence in language pairs. For linguisti-
cally different language pairs, such as Japanese-
English, content words may be easily aligned one-
to-one, but function words are not always aligned
together. In addition, Japanese is a pro-drop lan-
guage which can easily violate the symmetric con-
straint when proper nouns in the English side have
to be aligned with a ?null? word. In addition, low
frequency words may cause unreliable estimates
for adjusting the weighing parameters ?.
In order to solve the problem, we improve
Ganchev?s symmetric constraint so that it can con-
sider the difference between content words and
function words in each language. In particular, we
follow the frequency-based idea of Setiawan et al.
(2007) that discriminates content words and func-
tion words by their frequencies. We propose con-
straint features that take into account the differ-
ence between content words and function words,
determined by a frequency threshold.
3.1 Mismatching constraint
First, we propose a mismatching constraint that
penalizes word alignment between content words
and function words by decreasing the correspond-
ing posterior probabilities.
154
The constraint is represented as f2c (function to
content) constraint:
?
f2c
i,j
(x,y) = (6)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+1 (
??y ? y) ? (??y
i
= j) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
j
= i) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
i
= j) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) < 0),
?1 (
??y ? y) ? (??y
j
= i) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) < 0).
where ?
i,j
(x,y) =
??
p
?
(i, j|x) ?
??
p
?
(i, j|x) is
the difference in the posterior probabilities be-
tween the source-to-target and the target-to-source
alignment. C
s
and C
t
represent content words in
the source sentence and target sentence, respec-
tively. Similarly, F
s
and F
t
are function words
in the source and target sentence, respectively. In-
tuitively, when there exists a mismatch in content
word and function word for a word pair (i, j), the
constraint function returns a non-zero value for
the model with the highest posterior probability.
When coupled with the constraint such that the ex-
pectation of the feature value is zero, the constraint
function decreases the posterior probability of the
highest direction and discourages agreement with
each other.
Note that when this constraint is not fired, we
fall back to the constraint function in Equation (3)
for each word pair.
3.2 Matching constraint
In contrast to the mismatching constraint, our
second constraint function rewards alignment for
function to function word matching, namely f2f.
The f2f constraint function is defined as follows:
?
f2f
i,j
(x,y) = (7)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+1 (
??y ? y) ? (??y
i
= j)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
j
= i)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
i
= j)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) < 0),
?1 (
??y ? y) ? (??y
j
= i)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) < 0).
This constraint function returns a non-zero value
for a word pair (i, j) when they are function
words. As a result, the pair of function words
are encouraged to agree with each other, but not
other pairs. The content to content word matching
function c2c can be defined similarly by replac-
ing F
s
and F
t
by C
s
and C
t
, respectively. Like-
wise, the function to content word matching func-
tion f2c is defined by considering the matching
of content words and function words in two lan-
guages. As noted in the mismatch function, when
no constraint is fired, we fall back to Eq (3) for
each word pair.
4 Experiment
4.1 Experimental Setup
The data sets used in our experiments are the
French-English Hansard Corpus, and two data sets
for Japanese-English tasks: the Kyoto free trans-
lation task (KFTT) and NTCIR10. The Hansard
Corpus consists of parallel texts drawn from of-
ficial records of the proceedings of the Canadian
Parliament. The KFTT (Neubig, 2011) is derived
from Japanese Wikipedia articles related to Ky-
oto, which is professionally translated into En-
glish. NTCIR10 comes from patent data employed
in a machine translation shared task (Goto et al.,
2013). The statistics of these data are presented in
Table 1.
Sentences of over 40 words on both source and
target sides are removed for training alignment
models. We used a word alignment toolkit ci-
cada
2
for training the IBM Model 4 with our
proposed methods. Training is bootstrapped from
IBM Model 1, followed by HMM and IBM Model
4. When generating the final bidirectional word
alignment, we use a grow-diag-final heuristic for
the Japanese-English tasks and an intersection
heuristic in the French-English task, judged by
preliminary studies.
Following Bisazza and Federico (2012), we
automatically decide the threshold for word fre-
quency to discriminate between content words and
function words. Specifically, the threshold is de-
termined by the ratio of highly frequent words.
The threshold th is the maximum frequency that
satisfies the following equation:
?
w?(freq(w)>th)
freq(w)
?
w?all
freq(w)
> r. (8)
Here, we empirically set r = 0.5 by preliminary
studies. This method is based on the intuition that
content words and function words exist in a docu-
ment at a constant rate.
4.2 Word alignment evaluation
We measure the impact of our proposed meth-
ods on the quality of word alignment measured
2
https://github.com/tarowatanabe/cicada
155
Table 1: The statistics of the data sets
hansard kftt NTCIR10
French English Japanese English Japanese English
train sentence 1.13M 329.88K 2.02M
word 23.3M 19.8M 6.08M 5.91M 53.4M 49.4M
vocabulary 78.1K 57.3K 114K 138K 114K 183K
dev sentence 1.17K 2K
word 26.8K 24.3K 73K 67.3K
vocabulary 4.51K 4.78K 4.38K 5.04K
test WA sentence 447 582
word 7.76K 7.02K 14.4K 12.6K
vocabulary 1,92K 1.69K 2.57K 2.65K
TR sentence 1.16K 8.6K
word 28.5K 26.7K 334K 310K
vocabulary 4.91K 4.57K 10.4K 12.7K
Figure 1: Precision Recall graph in Hansard
French-English
Figure 2: Precision Recall graph in KFTT
Figure 3: AER in Hansard French-English Figure 4: AER in KFTT
156
Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
KFTT Hansard (French-English)
method precision recall AER F precision recall AER F
symmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822
f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844
c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804
f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848
by AER and F-measure (Och and Ney, 2003).
Since there exists no distinction for sure-possible
alignments in the KFTT data, we use only sure
alignment for our evaluation, both for the French-
English and the Japanese-English tasks. Table 2
summarizes our results.
The baseline method is symmetric constraint
(Ganchev et al., 2010) shown in Table 2. The num-
bers in bold and in italics indicate the best score
and the second best score, respectively. The dif-
ferences between f2f,f2c and baseline in KFTT are
statistically significant at p < 0.05 using the sign-
test, but in hansard corpus, there exist no signifi-
cant differences between the baseline and the pro-
posed methods. In terms of F-measure, it is clear
that the f2f method is the most effective method
in KFTT, and both f2f and f2c methods exceed the
original posterior regularized model of Ganchev et
al. (2010).
We also compared these methods with filtering
methods (Liang et al., 2006), in addition to heuris-
tic methods. We plot precision/recall curves and
AER by varying the threshold between 0.1 and
0.9 with 0.1 increments. From Figures, it can be
seen that our proposed methods are superior to
the baseline in terms of both precision-recall and
AER.
4.3 Translation evaluation
Next, we performed a translation evaluation, mea-
sured by BLEU (Papineni et al., 2002). We
compared the grow-diag-final and filtering method
(Liang et al., 2006) for creating phrase tables.
The threshold for the filtering factor was set to
0.1 which was the best setting in the word align-
ment experiment in section 4.2 under KFTT. From
the English side of the training data, we trained a
word using the 5-gram model with SRILM (Stol-
cke and others, 2002). ?Moses? toolkit was used
as a decoder (Koehn et al., 2007) and the model
parameters were tuned by k-best MIRA (Cherry
and Foster, 2012). In order to avoid tuning insta-
bility, we evaluated the average of five runs (Hop-
kins and May, 2011). The results are summarized
Table 3: Results of translation evaluation
KFTT NTCIR10
GDF Filtered GDF Filtered
symmetric 19.06 19.28 28.3 29.71
f2f 19.15 19.17 28.36 29.74
c2c 19.26 19.02 28.36 29.92
f2c 18.91 19.20 28.36 29.67
in Table 3. Our proposed methods achieved large
gains in NTCIR10 task with the filtered method,
but observed no gain in the KFTT with the filtered
method. In NTCIR10 task with GDF, the gain in
BLEU was smaller than that of KFTT. We cal-
culate p-values and the difference between sym-
metric and c2c (the most effective proposed con-
straint) are lower than 0.05 in kftt with GDF and
NTCIR10 with filtered method. There seems to
be no clear tendency in the improved alignment
qualities and the translation qualities, as shown in
numerous previous studies (Ganchev et al., 2008).
5 Conclusion
In this paper, we proposed new constraint func-
tions under the posterior regularization frame-
work. Our constraint functions introduce a
fine-grained agreement constraint considering the
frequency of words, a assuming that the high
frequency words correspond to function words
whereas the less frequent words may be treated
as content words, based on the previous work of
Setiawan et al. (2007). Experiments on word
alignment tasks showed better alignment quali-
ties measured by F-measure and AER on both the
Hansard task and KFTT. We also observed large
gain in BLEU, 0.2 on average, when compared
with the previous posterior regularization method
under NTCIR10 task.
As our future work, we will investigate more
precise methods for deciding function words and
content words for better alignment and translation
qualities.
157
References
Arianna Bisazza and Marcello Federico. 2012. Cutting
the long tail: Hybrid language models for translation
style adaptation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 439?448. Associ-
ation for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Yin-Wen Chang, Alexander M. Rush, John DeNero,
and Michael Collins. 2014. A constrained viterbi
relaxation for bidirectional word alignment. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1481?1490, Baltimore, Maryland,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 420?429, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001?2049.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-Lingual Information Access, NTCIR-10.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. In Proceedings of COLING 2004, pages 219?
225, Geneva, Switzerland, August 23?27.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th annual meeting on associ-
ation for computational linguistics, pages 712?719.
Association for Computational Linguistics.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
158
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 165?171,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Syntax-Augmented Machine Translation using Syntax-Label Clustering
Hideya Mino, Taro Watanabe and Eiichiro Sumita
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
{hideya.mino, taro.watanabe, eiichiro.sumita}@nict.go.jp
Abstract
Recently, syntactic information has helped
significantly to improve statistical ma-
chine translation. However, the use of syn-
tactic information may have a negative im-
pact on the speed of translation because of
the large number of rules, especially when
syntax labels are projected from a parser in
syntax-augmented machine translation. In
this paper, we propose a syntax-label clus-
tering method that uses an exchange algo-
rithm in which syntax labels are clustered
together to reduce the number of rules.
The proposed method achieves clustering
by directly maximizing the likelihood of
synchronous rules, whereas previous work
considered only the similarity of proba-
bilistic distributions of labels. We tested
the proposed method on Japanese-English
and Chinese-English translation tasks and
found order-of-magnitude higher cluster-
ing speeds for reducing labels and gains
in translation quality compared with pre-
vious clustering method.
1 Introduction
In recent years, statistical machine translation
(SMT) models that use syntactic information have
received significant research attention. These
models use syntactic information on the source
side (Liu et al., 2006; Mylonakis and Sima?an,
2011), the target side (Galley et al., 2006; Huang
and Knight, 2006) or both sides (Chiang, 2010;
Hanneman and Lavie, 2013) produce syntactically
correct translations. Zollmann and Venugopal
(2006) proposed syntax-augmented MT (SAMT),
which is a MT system that uses syntax labels of a
parser. The SAMT grammar directly encodes syn-
tactic information into the synchronous context-
free grammar (SCFG) of Hiero (Chiang, 2007),
which relies on two nonterminal labels. One prob-
lem in adding syntax labels to Hiero-style rules
is that only partial phrases are assigned labels.
It is common practice to extend labels by us-
ing the idea of combinatory categorial grammar
(CCG) (Steedman, 2000) on the problem. Al-
though this extended syntactical information may
improve the coverage of rules and syntactic cor-
rectness in translation, the increased grammar size
causes serious speed and data-sparseness prob-
lems. To address these problems, Hanneman and
Lavie (2013) coarsen syntactic labels using the
similarity of the probabilistic distributions of la-
bels in synchronous rules and showed that perfor-
mance improved.
In the present work, we follow the idea of label-
set coarsening and propose a new method to group
syntax labels. First, as an optimization criterion,
we use the logarithm of the likelihood of syn-
chronous rules instead of the similarity of prob-
abilistic distributions of syntax labels. Second,
we use exchange clustering (Uszkoreit and Brants,
2008), which is faster than the agglomerative-
clustering algorithm used in the previous work.
We tested our proposed method on Japanese-
English and Chinese-English translation tasks and
observed gains comparable to those of previous
work with similar reductions in grammar size.
2 Syntax-Augmented Machine
Translation
SAMT is an instance of SCFG G, which can be
formally defined as
G = (N , S, T
?
, T
?
,R)
where N is a set of nonterminals, S ? N is a
start label, T
?
and T
?
are the source- and target-
side terminals, andR is a set of synchronous rules.
Each synchronous rule in R takes the form
X ? ??, ?,??
165
where X ? N is a nonterminal, ? ? (N ? T
?
)?
is a sequence of nonterminals or source-side ter-
minals, and ? ? (N ? T
?
)? is a sequence of
nonterminals or target-side terminals. The num-
ber #NT (?) of nonterminals in ? is equal to
the number #NT (?) of nonterminals in ?, and
?: {1, ...,#NT (?)} ? {1, ...,#NT (?)} is a
one-to-one mapping from nonterminals in ? to
nonterminals in ?. For each synchronous rule, a
nonnegative real-value weight w(X ? ??, ?,??)
is assigned and the sum of the weights of all rules
sharing the same left-hand side in a grammar is
unity.
Hierarchical phrase-based SMT (Hiero) (Chi-
ang, 2007) translates by using synchronous rules
that only have two nonterminal labelsX and S but
have no linguistic information. SAMT augments
the Hiero-style rules with syntax labels from a
parser and extends these labels based on CCG.
Although the use of extended syntax labels may
increase the coverage of rules and improve the
potential for syntactically correct translations, the
growth of the nonterminal symbols significantly
affects the speed of decoding and causes a serious
data-sparseness problem.
To address these problems, Hanneman and
Lavie (2013) proposed a label-collapsing algo-
rithm, in which syntax labels are clustered by us-
ing the similarity of the probabilistic distributions
of clustered labels in synchronous rules. First,
Hanneman and Lavie defined the label-alignment
distribution as
P (s|t) =
#(s, t)
#(t)
(1)
where N
?
and N
?
are the source- and target-side
nonterminals in synchronous rules, s ? N
?
and
t ? N
?
are syntax labels from the source and tar-
get sides, #(s, t) denotes the number of left-hand-
side label pairs, and #(t) denotes the number of
target-side labels. Second, for each target-side la-
bel pair (t
i
, t
j
), we calculate the total distance d of
the absolute differences in the likelihood of labels
that are aligned to a source-side label s:
d(t
i
, t
j
) =
?
s?N
?
|P (s|t
i
)? P (s|t
j
)| (2)
Next, the closest syntax-label pair of
?
t and
?
t
?
is
combined into a new single label. The agglomera-
tive clustering is applied iteratively until the num-
ber of the syntax labels reaches a given value.
The clustering of Hanneman and Lavie proved
successful in decreasing the grammar size and pro-
viding a statistically significant improvement in
translation quality. However, their method relies
on an agglomerative clustering with a worst-case
time complexity of O(|N |
2
log |N |). Also, clus-
tering based on label distributions does not al-
ways imply higher-quality rules, because it does
not consider the interactions of the nonterminals
on the left-hand side and the right-hand side in
each synchronous rule.
3 Syntax-Label Clustering
As an alternative to using the similarity of proba-
bilistic distributions as a criterion for syntax-label
clustering, we propose a clustering method based
on the maximum likelihood of the synchronous
rules in a training data D. We uses the idea
of maximizing the Bayesian posterior probability
P (M |D) of the overall model structure M given
data D (Stolcke and Omohundro, 1994). While
their goal is to maximize the posterior
P (M |D) ? P (M)P (D|M) (3)
we omit the prior term P (M) and directly max-
imize the P (D|M). A model M is a clustering
structure
1
. The synchronous rule in the data D
for SAMT with target-side syntax labels is repre-
sented as
X ? ?a
1
Y
(1)
a
2
Z
(2)
a
3
, b
1
Y
(1)
b
2
Z
(2)
b
3
? (4)
where a
1
, a
2
, a
3
and b
1
, b
2
, b
3
are the source- and
target-side terminals, respectively X , Y , Z are
nonterminal syntax labels, and the superscript
number indicates alignment between the source-
and target-side nonterminals. Using Equation (4)
we maximize the posterior probability P (D|M)
which we define as the probability of right-hand
side given the syntax label X of the left-hand side
rule in the training data as follows:
?
X???,?,???D
logPr(??, ?,??|X) (5)
For the sake of simplicity, we assume that the
generative probability for each rule does not de-
pend on the existence of terminal symbols and that
the reordering in the target side may be ignored.
Therefore, Equation (5) simplifies to
?
X??a
1
Y
(1)
a
2
Z
(2)
a
3
,b
1
Y
(1)
b
2
Z
(2)
b
3
?
log p(Y, Z|X) (6)
1
P (M) is reflected by the number of clusters.
166
3.1 Optimization Criterion
The generative probability in each rule of the form
of Equation (6) can be approximated by clustering
nonterminal symbols as follows:
p(Y, Z|X) ? p(Y |c(Y )) ? p(Z|c(Z))
?p(c(Y ), c(Z)|c(X)) (7)
where we map a syntax label X to its equivalence
cluster c(X). This can be regarded as the cluster-
ing criterion usually used in a class-based n-gram
language model (Brown et al., 1992). If each label
on the right-hand side of a synchronous rule (4) is
independent of each other, we can factor the joint
model as follows:
p(Y, Z|X) ? p(Y |c(Y )) ? p(Z|c(Z))
?p(c(Y )|c(X))?p(c(Z)|c(X)) (8)
We introduce the predictive idea of Uszkoreit and
Brants (2008) to Equation (8), which doesn?t con-
dition on the clustered label c(X), but directly on
the syntax label X:
p(Y, Z|X) ? p(Y |c(Y )) ? p(Z|c(Z))
?p(c(Y )|X) ? p(c(Z)|X) (9)
The objective in Equation (9) is represented using
the frequency in the training data as
N(Y )
N(c(Y ))
?
N(X, c(Y ))
N(X)
?
N(Z)
N(c(Z))
?
N(X, c(Z))
N(X)
(10)
where N(X) and N(c(X)) denote the frequency
2
of X and c(X), and N(X,K) denotes the fre-
quency of cluster K in the right-hand side of a
synchronous rule whose left-hand side syntax la-
bel is X . By replacing the rule probabilities in
Equation (9) with Equation (10) and plugging the
result into Equation (6), our objective becomes
F (C) =
?
Y ?N
N(Y ) ? log
N(Y )
N(c(Y ))
+
?
X?N ,K?C
N(X,K) ? log
N(X,K)
N(X)
=
?
Y ?N
N(Y ) ? logN(Y )
?
?
Y ?N
N(Y ) ? logN(c(Y ))
+
?
X?N ,K?C
N(X,K) ? logN(X,K)
?
?
X?N ,K?C
N(X,K) ? logN(X)(11)
2
We use a fractional count (Chiang, 2007) which adds up
to one as a frequency.
start with the initial mapping (labelX ? c(X))
compute objective function F (C)
for each labelX do
remove labelX from c(X)
for each clusterK do
move labelX tentatively to clusterK
compute F (C) for this exchange
move labelX to cluster with maximum F (C)
do until the cluster mapping does not change
Table 1: Outline of syntax-label clustering method
where C denotes all clusters and N denotes all
syntax labels. For Equation (11), the last summa-
tion is equivalent to the sum of the occurrences
of all syntax labels, and canceled out by the first
summation. K in the third summation consid-
ers clusters in a synchronous rule whose left-hand
side label is X , and we let ch(X) denote a set
of those clusters. The second summation equals
?
K?C
N(K) ? logN(K). As a result, Equation
(11) simplifies to
F (C) =
?
X?N ,K?ch(X)
N(X,K) ? logN(X,K)
?
?
K?C
N(K) ? logN(K) (12)
3.2 Exchange Clustering
We used an exchange clustering algorithm
(Uszkoreit and Brants, 2008) which was proven
to be very efficient in word clustering with a vo-
cabulary of over 1 million words. The exchange
clustering for words begins with the initial cluster-
ing of words and greedily exchanges words from
one cluster to another such that an optimization
criterion is maximized after the move. While ag-
glomerative clustering requires recalculation for
all pair-wise distances between words, exchange
clustering only demands computing the difference
of the objective for the word pair involved in a par-
ticular movement. We applied this exchange clus-
tering to syntax-label clustering. Table 1 shows
the outline. For initial clustering, we partitioned
all the syntax labels into clusters according to the
frequency of syntax labels in synchronous rules. If
remove and move are as computationally inten-
sive as computing the change in F (C) in Equation
(12), then the time complexity of remove and
move is O(K) (Martin et al., 1998), where K is
the number of clusters. Since the remove proce-
dure is called once for each label and, for a given
label, the move procedure is called K ? 1 times
167
Data Lang Training Development Test
sent src-tokens tgt-tokens sent tgt-tokens sent tgt-tokens
IWSLT07 J to E 40 K 483 K 369 K 500 7.4 K 489 3.7 K
FBIS C to E 302 K 2.7 M 3.4 M 1,664 47 K 919 30 K
NIST08 1 M 15 M 17 M
Table 2: Data sets: The ?sent? column indicates the number of sentences. The ?src-tokens? and ?tgt-
tokens? columns indicate the number of words in the source- and the target-side sentences.
to find the maximum F (C), the worst-time com-
plexity for one iteration of the syntax-label clus-
tering is O(|N |K
2
). The exchange procedure is
continued until the cluster mapping is stable or the
number of iterations reaches a threshold value of
100.
4 Experiments
4.1 Data
We conducted experiments on Japanese-English
(ja-en) and Chinese-English (zh-en) translation
tasks. The ja-en data comes from IWSLT07
(Fordyce, 2007) in a spoken travel domain. The
tuning set has seven English references and the test
set has six English references. For zh-en data we
prepared two kind of data. The one is extracted
from FBIS
3
, which is a collection of news arti-
cles. The other is 1 M sentences extracted ron-
domly from NIST Open MT 2008 task (NIST08).
We use the NIST Open MT 2006 for tuning and
the MT 2003 for testing. The tuning and test sets
have four English references. Table 2 shows the
details for each corpus. Each corpus is tokenized,
put in lower-case, and sentences with over 40 to-
kens on either side are removed from the training
data. We use KyTea (Neubig et al., 2011) to to-
kenize the Japanese data and Stanford Word Seg-
menter (Tseng et al., 2005) to tokenize the Chinese
data. We parse the English data with the Berkeley
parser (Petrov and Klein, 2007).
4.2 Experiment design
We did experiments with the SAMT (Zollmann
and Venugopal, 2006) model with the Moses
(Koehn et al., 2007). For the SAMT model, we
conducted experiments with two label sets. One
is extracted from the phrase structure parses and
the other is extended with CCG
4
. We applied the
proposed method (+clustering) and the baseline
method (+coarsening), which uses the Hanneman
3
LDC2003E14
4
Using the relax-parse with option SAMT 4 for IWSLT07
and FBIS and SAMT 2 for NIST08 in the Moses
Label set Label Rule F(C) SD
parse 63 0.3 K - -
CCG 3,147 4.2 M - -
+ coarsening 80 2.4 M -3.8 e+08 249
+ clustering 80 3.8 M -7.2 e+07 73
Table 3: SAMT grammars on ja-en experiments
Label set Label Rule F(C) SD
FBIS
parse 70 2.1 M - -
CCG 5,460 60 M - -
+ coarsening 80 32 M -1.5 e+10 526
+ clustering 80 38 M -7.9 e+09 154
NIST08
parse 70 12 M - -
CCG 7,328 120 M - -
+ clustering 80 100 M -2.6 e+10 218
Table 4: SAMT grammars on zh-en experiments
label-collapsing algorithm described in Section 2,
for syntax-label clustering to the SAMT models
with CCG. The number of clusters for each clus-
tering was set to 80. The language models were
built using SRILM Toolkits (Stolcke, 2002). The
language model with the IWSLT07 is a 5-gram
model trained on the training data, and the lan-
guage model with the FBIS and NIST08 is a 5-
gram model trained on the Xinhua portion of En-
glish GigaWord. For word alignments, we used
MGIZA++ (Gao and Vogel, 2008). To tune the
weights for BLEU (Papineni et al., 2002), we used
the n-best batch MIRA (Cherry and Foster, 2012).
5 Results and analysis
Tables 3 and 4 present the details of SAMT gram-
mars with each label set learned by the exper-
iments using the IWSLT07 (ja-en), FBIS and
NIST08 (zh-en), which include the number of syn-
tax labels and synchronous rules, the values of the
objective (F (C)), and the standard deviation (SD)
of the number of labels assigned to each cluster.
For NIST08 we applied only the + clustering be-
cause the + coarsening needs a huge amount of
computation time. Table 5 shows the differences
between the BLEU score and the rule number for
168
each cluster number when using the IWSLT07
dataset.
Since the +clustering maximizes the likelihood
of synchronous rules, it can introduce appropriate
rules adapted to training data given a fixed number
of clusters. For each experiment, SAMT gram-
mars with the +clustering have a greater number
of rules than with the +coarsening and, as shown
in Table 5, the number of synchronous rules with
+clustering increase with the number of clusters.
For +clustering with eight clusters and +coars-
ening with 80 clusters, which have almost 2.4M
rules, the BLEU score of +clustering with eight
clusters is higher. Also, the SD of the number
of labels, which indicates the balance of the num-
ber of labels among clusters, with +clustering is
smaller than with +coarsening. These results sug-
gest that +clustering maintain a large-scale varia-
tion of synchronous rules for high performance by
balancing the number of labels in each cluster.
The number of synchronous rules grows as you
progress from +coarsening to +clustering and fi-
nally to raw label with CCG. To confirm the ef-
fect of the number of rules, we measured the de-
coding time per sentence for translating the test
set by taking the average of ten runs with FBIS
corpus. +coarsening takes 0.14 s and +clustering
takes 0.16 s while raw label with CCG takes 0.37s.
Thus the increase in the number of synchronous
rules adversely affects the decoding speed.
Table 6 presents the results for the experiments
5
using ja-en and zh-en with the BLEU metric.
SAMT with parse have the lowest BLEU scores.
It appears that the linguistic information of the
raw syntax labels of the phrase structure parses
is not enough to improve the translation perfor-
mance. Hiero has the higher BLEU score than
SAMT with CCG on zh-en. This is likely due to
the low accuracy of the parses, on which SAMT
relies while Hiero doesn?t. SAMT with + clus-
tering have the higher BLEU score than raw label
with CCG. For SAMT with CCG using IWSLT07
and FBIS, though the statistical significance tests
were not significant when p < 0.05, +clustering
have the higher BLEU scores than +coarsening.
For these results, the performance of +clustering
is comparable to that of +coarsening. For the
complexity of both clustering algorithm, though it
is difficult to evaluate directly because the speed
5
As another baseline, we also used Phrase-based SMT
(Koehn et al., 2003) and Hiero (Chiang, 2007).
+clustering +coarsening
Cluster 80 40 8 4 80
BLEU 50.21 49.49 49.96 50.25 49.54
Rule 3.8 M 3.5 M 2.4 M 2.2 M 2.4 M
Table 5: BLEU score and rule number for each
cluster number using IWSLT07
ja-en zh-en
Model parse CCG parse CCG parse CCG
SAMT 42.58 48.77 23.66 26.97 24.67 27.28
+coarsening - 49.54 - 27.12 - -
+clustering - 50.21 - 27.47 - 27.29
Hiero 48.91 28.31 27.62
PB-SMT 49.14 26.88 26.71
Table 6: BLEU scores on each experiments
depends on how each algorithm is implemented,
+clustering is an order of magnitude faster than
+coarsening. For the clustering experiment that
groups 5460 raw labels with CCG into 80 clus-
ters using FBIS corpus, +coarsening takes about
1 week whereas +clustering takes about 10 min-
utes.
6 Conclusion
In this paper, we propose syntax-label clustering
for SAMT, which uses syntax-label information to
generate syntactically correct translations. One of
the problems of SAMT is the large grammar size
when a CCG-style extended label set is used in the
grammar, which make decoding slower. We clus-
ter syntax labels with a very fast exchange algo-
rithm in which the generative probabilities of syn-
chronous rules are maximized. We demonstrate
the effectiveness of the proposed method by us-
ing it to translate Japanese-English and Chinese-
English tasks and measuring the decoding speed,
the accuracy and the clustering speed. Future work
involves improving the optimization criterion. We
expect to make a new objective that includes the
terminal symbols and the reordering of nontermi-
nal symbols that were ignored in this work. An-
other interesting direction is to determine the ap-
propriate number of clusters for each corpus and
the initialization method for clustering.
Acknowledgments
We thank the anonymous reviewers for their sug-
gestions and helpful comments on the early ver-
sion of this paper.
169
References
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montr?eal, Canada, June. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201?228,
June.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1443?1452, Uppsala, Sweden, July.
Association for Computational Linguistics.
Cameron Shaw Fordyce. 2007. Overview of the 4th
international workshop on spoken language transla-
tion iwslt 2007 evaluation campaign. In In Proceed-
ings of IWSLT 2007, pages 1?12, Trento, Italy, Oc-
tober.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961?968, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 288?297, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
Bryant Huang and Kevin Knight. 2006. Relabeling
syntax trees to improve syntax-based machine trans-
lation quality. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 240?247, New York City, USA,
June. Association for Computational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In In
Proceedings of HLT-NAACL, pages 48?54, Edmon-
ton, Canada, May/July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 19?37.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguis-
tic annotations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
642?652, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 529?533, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Mark Steedman. 2000. The syntactic process, vol-
ume 27. MIT Press.
Andreas Stolcke and Stephen Omohundro. 1994. In-
ducing probabilistic grammars by bayesian model
170
merging. In R. C. Carrasco and J. Oncina, editors,
Grammatical Inference and Applications (ICGI-94),
pages 106?118. Berlin, Heidelberg.
Andreas Stolcke. 2002. Srilm an extensible language
modeling toolkit. In In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901?904.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 168?171. Jeju Island, Korea.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings of ACL-08: HLT, pages 755?762, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
171
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253?262,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Optimized Online Rank Learning for Machine Translation
Taro Watanabe
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 JAPAN
{taro.watanabe}@nict.go.jp
Abstract
We present an online learning algorithm for
statistical machine translation (SMT) based on
stochastic gradient descent (SGD). Under the
online setting of rank learning, a corpus-wise
loss has to be approximated by a batch lo-
cal loss when optimizing for evaluation mea-
sures that cannot be linearly decomposed into
a sentence-wise loss, such as BLEU. We pro-
pose a variant of SGD with a larger batch size
in which the parameter update in each iteration
is further optimized by a passive-aggressive
algorithm. Learning is efficiently parallelized
and line search is performed in each round
when merging parameters across parallel jobs.
Experiments on the NIST Chinese-to-English
Open MT task indicate significantly better
translation results.
1 Introduction
The advancement of statistical machine translation
(SMT) relies on efficient tuning of several or many
parameters in a model. One of the standards for such
tuning is minimum error rate training (MERT) (Och,
2003), which directly minimize the loss of transla-
tion evaluation measures, i.e. BLEU (Papineni et al,
2002). MERT has been successfully used in prac-
tical applications, although, it is known to be un-
stable (Clark et al, 2011). To overcome this insta-
bility, it requires multiple runs from random start-
ing points and directions (Moore and Quirk, 2008),
or a computationally expensive procedure by linear
programming and combinatorial optimization (Gal-
ley and Quirk, 2011).
Many alternative methods have been proposed
based on the algorithms in machine learning, such as
averaged perceptron (Liang et al, 2006), maximum
entropy (Och and Ney, 2002; Blunsom et al, 2008),
Margin Infused Relaxed Algorithm (MIRA) (Watan-
abe et al, 2007; Chiang et al, 2008b), or pairwise
rank optimization (PRO) (Hopkins and May, 2011).
They primarily differ in the mode of training; on-
line or MERT-like batch, and in their objectives;
max-margin (Taskar et al, 2004), conditional log-
likelihood (or softmax loss) (Berger et al, 1996),
risk (Smith and Eisner, 2006; Li and Eisner, 2009),
or ranking (Herbrich et al, 1999).
We present an online learning algorithm based
on stochastic gradient descent (SGD) with a larger
batch size (Shalev-Shwartz et al, 2007). Like Hop-
kins and May (2011), we optimize ranking in n-
best lists, but learn parameters in an online fash-
ion. As proposed by Haddow et al (2011), BLEU
is approximately computed in the local batch, since
BLEU is not linearly decomposed into a sentence-
wise score (Chiang et al, 2008a), and optimization
for sentence-BLEU does not always achieve opti-
mal parameters for corpus-BLEU. Setting the larger
batch size implies the more accurate corpus-BLEU,
but at the cost of slower convergence of SGD. There-
fore, we propose an optimized update method in-
spired by the passive-aggressive algorithm (Cram-
mer et al, 2006), in which each parameter update is
further rescaled considering the tradeoff between the
amount of updates to the parameters and the ranking
loss. Learning is efficiently parallelized by splitting
training data among shards and by merging parame-
ters in each round (McDonald et al, 2010). Instead
253
of simple averaging, we perform an additional line
search step to find the optimal merging across paral-
lel jobs.
Experiments were carried out on the NIST 2008
Chinese-to-English OpenMT task. We found signif-
icant gains over traditional MERT and other tuning
algorithms, such as MIRA and PRO.
2 Statistical Machine Translation
SMT can be formulated as a maximization problem
of finding the most likely translation e given an input
sentence f using a set of parameters ? (Brown et al,
1993)
e? = argmax
e
p(e|f ; ?). (1)
Under this maximization setting, we assume that
p(?) is represented by a linear combination of fea-
ture functions h(f, e) which are scaled by a set of
parameters w (Och and Ney, 2002)
e? = argmax
e
w?h(f, e). (2)
Each element ofh(?) is a feature function which cap-
tures different aspects of translations, for instance,
log of n-gram language model probability, the num-
ber of translated words or log of phrasal probability.
In this paper, we concentrate on the problem of
learning w, which is referred to as tuning. One of
the standard methods for parameter tuning is mini-
mum error rate training (Och, 2003) (MERT) which
directly minimizes the task loss ?(?), i.e. negative
BLEU (Papineni et al, 2002), given training data
D = {(f1, e1), ..., (fN , eN )}, sets of paired source
sentence f i and its reference translations ei
w? = argmin
w
?(
{
argmax
e
w?h(f i, e)
}N
i=1
,
{
ei
}N
i=1).
(3)
The objective in Equation 3 is discontinuous and
non-convex, and it requires decoding of all the train-
ing data given w. Therefore, MERT relies on a
derivative-free unconstrained optimization method,
such as Powell?s method, which repeatedly chooses
one direction to optimize using a line search pro-
cedure as in Algorithm 1. Expensive decoding is
approximated by an n-best merging technique in
which decoding is carried out in each epoch of it-
erations t and the maximization in Eq. 3 is approxi-
Algorithm 1 MERT
1: Initialize w1
2: for t = 1, ..., T do ? Or, until convergence
3: Generate n-bests using wt
4: Learn new wt+1 by Powell?s method
5: end for
6: return wT+1
mated by search over the n-bests merged across iter-
ations. The merged n-bests are also used in the line
search procedure to efficiently draw the error surface
for efficient computation of the outer minimization
of Eq. 3.
3 Online Rank Learning
3.1 Rank Learning
Instead of the direct task loss minimization of Eq.
3, we would like to find w by solving the L2-
regularized constrained minimization problem
argmin
w
?
2
?w?22 + ?(w;D) (4)
where ? > 0 is a hyperparameter controlling the fit-
ness to the data. The loss function ?(?) we consider
here is inspired by a pairwise ranking method (Hop-
kins and May, 2011) in which pairs of correct trans-
lation and incorrect translation are sampled from n-
bests and suffer a hinge loss
1
M(w;D)
?
(f,e)?D
?
e?,e?
max
{
0, 1?w??(f, e?, e?)
}
(5)
where
e? ? NBEST(w; f) \ ORACLE(w; f, e)
e? ? ORACLE(w; f, e)
?(f, e?, e?) = h(f, e?)? h(f, e?).
NBEST(?) is the n-best translations of f generated
with the parameter w, and ORACLE(?) is a set of
oracle translations chosen among NBEST(?). Note
that each e? (and e?) implicitly represents a deriva-
tion consisting of a tuple (e?, ?), where ? is a latent
structure, i.e. phrases in a phrase-based SMT, but we
omit ? for brevity. M(?) is a normalization constant
which is equal to the number of paired loss terms
?(f, e?, e?) in Equation 5. Since it is impossible
254
to enumerate all possible translations, we follow the
convention of approximating the domain of transla-
tion by n-bests. Unlike Hopkins and May (2011),
we do not randomly sample from all the pairs in the
n-best translations, but extract pairs by selecting one
oracle translation and one other translation in the n-
bests other than those in ORACLE(?). Oracle trans-
lations are selected by minimizing the task loss,
?(
{
e? ? NBEST(w; f i)
}N
i=1 ,
{
ei
}N
i=1)
i.e. negative BLEU, with respect to a set of ref-
erence translations e. In order to compute oracles
with corpus-BLEU, we apply a greedy search strat-
egy over n-bests (Venugopal, 2005). Equation 5 can
be easily interpreted as a constant loss ?1? for choos-
ing a wrong translation under current parameters w,
which is in contrast with the direct task-loss used in
max-margin approach to structured output learning
(Taskar et al, 2004).
As an alternative, we would also consider a soft-
max loss (Collins and Koo, 2005) represented by
1
N
?
(f,e)?D
? log ZO(w; f, e)
ZN(w; f)
(6)
where
ZO(w; f, e) =
?
e??ORACLE(w;f,e) exp(w?f(f, e?))
ZN(w; f) =
?
e??NBEST(w;f) exp(w?f(f, e?)).
Equation 6 is a log-linear model used in common
NLP tasks such as tagging, chunking and named en-
tity recognition, but differ slightly in that multiple
correct translations are discriminated from the oth-
ers (Charniak and Johnson, 2005).
3.2 Online Approximation
Hopkins and May (2011) applied a MERT-like pro-
cedure in Alg. 1 in which Equation 4 was solved
to obtain new parameters in each iteration. Here,
we employ stochastic gradient descent (SGD) meth-
ods as presented in Algorithm 2 motivated by Pega-
sos (Shalev-Shwartz et al, 2007). In each iteration,
we randomly permute D and choose a set of batches
Bt = {bt1, ..., btK} with each btj consisting of N/K
training data. For each batch b in Bt, we generate
n-bests from the source sentences in b and compute
oracle translations from the newly created n-bests
Algorithm 2 Stochastic Gradient Descent
1: k = 1,w1 ? 0
2: for t = 1, ..., T do
3: Choose Bt = {bt1, ..., btK} from D
4: for b ? Bt do
5: Compute n-bests and oracles of b
6: Set learning rate ?k
7: wk+ 12 ? wk ? ?k?(wk; b)
? Our proposed algorithm solve Eq. 12 or 16
8: wk+1 ? min
{
1, 1/
?
?
?wk+12
?2
}
wk+ 12
9: k ? k + 1
10: end for
11: end for
12: return wk
(line 5) using a batch local corpus-BLEU (Haddow
et al, 2011). Then, we optimize an approximated
objective function
argmin
w
?
2
?w?22 + ?(w; b) (7)
by replacing D with b in the objective of Eq. 4. The
parameters wk are updated by the sub-gradient of
Equation 7, ?(wk; b), scaled by the learning rate
?k (line 7). We use an exponential decayed learn-
ing rate ?k = ?0?k/K , which converges very fast in
practice (Tsuruoka et al, 2009)1. The sub-gradient
of Eq.7 with the hinge loss of Eq. 5 is
?wk ?
1
M(wk; b)
?
(f,e)?b
?
e?,e?
?(f, e?, e?) (8)
such that
1?w?k ?(f, e?, e?) > 0. (9)
We found that the normalization term by M(?) was
very slow in convergence, thus, instead, we used
M ?(w; b), which was the number of paired loss
terms satisfied the constraints in Equation 9. In the
case of the softmax loss objective of Eq. 6, the sub-
gradient is
?wk ?
1
|b|
?
(f,e)?b
?
?w
L(w; f, e)
?
?
?
?
w=wk
(10)
1We set ? = 0.85 and ?0 = 0.2 which converged well in
our preliminary experiments.
255
where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)).
After the parameter update, wk+ 12 is projected
within the L2-norm ball (Shalev-Shwartz et al,
2007).
Setting smaller batch size implies frequent up-
dates to the parameters and a faster convergence.
However, as briefly mentioned in Haddow et al
(2011), setting batch size to a smaller value, such as
|b| = 1, does not work well in practice, since BLEU
is devised for a corpus based evaluation, not for an
individual sentence-wise evaluation, and it is not lin-
early decomposed into a sentence-wise score (Chi-
ang et al, 2008a). Thus, the smaller batch size may
also imply less accurate batch-local corpus-BLEU
and incorrect oracle translation selections, which
may lead to incorrect sub-gradient estimations or
slower convergence. In the next section we propose
an optimized parameter update which works well
when setting a smaller batch size is impractical due
to its task loss setting.
4 Optimized Online Rank Learning
4.1 Optimized Parameter Update
In line 7 of Algorithm 2, parameters are updated by
the sub-gradient of each training instance in a batch
b. When the sub-gradient in Equation 8 is employed,
the update procedure can be rearranged as
wk+ 12 ? (1???k)wk+
?
(f,e)?b,e?,e?
?k
M(wk; b)
?(f, e?, e?)
(11)
in which each individual loss term?(?) is scaled uni-
formly by a constant ?k/M(?).
Instead of the uniform scaling, we propose to up-
date the parameters in two steps: First, we suffer the
sub-gradient from the L2 regularization
wk+ 14 ? (1? ??k)wk.
Second, we solve the following problem
argmin
w
1
2
?w?wk+ 14 ?
2
2+?k
?
(f,e)?b,e?,e?
?f,e?,e? (12)
such that
w??(f, e?, e?) ? 1? ?f,e?,e?
?f,e?,e? ? 0.
The problem is inspired by the passive-aggressive
algorithm (Crammer et al, 2006) in which new pa-
rameters are derived through the tradeoff between
the amount of updates to the parameters and the
margin-based loss. Note that the objective in MIRA
is represented by
argmin
w
?
2
?w ? wk?22 +
?
(f,e)?b,e?,e?
?f,e?,e? (13)
If we treat wk+ 14 as our previous parameters and set
? = 1/?k, they are very similar. Unlike MIRA, the
learning rate ?k is directly used as a tradeoff param-
eter which decays as training proceeds, and the sub-
gradient of the global L2 regularization term is also
combined in the problem through wk+ 14 .
The Lagrangian dual of Equation 12 is
argmin
?e?,e?
1
2
?
?
(f,e)?b,e?,e?
?e?,e??(f, e?, e?)?22
?
?
(f,e)?b,e?,e?
?e?,e?
{
1?w?k+ 14?(f, e
?, e?)
}
(14)
subject to
?
(f,e)?b,e?,e?
?e?,e? ? ?k.
We used a dual coordinate descent algorithm (Hsieh
et al, 2008)2 to efficiently solve the quadratic pro-
gram (QP) in Equation 14, leading to an update
wk+ 12 ? wk+ 14 +
?
(f,e)?b,e?,e?
?e?,e??(f, e?, e?). (15)
When compared with Equation 11, the update pro-
cedure in Equation 15 rescales the contribution from
each sub-gradient through the Lagrange multipliers
?e?,e? . Note that if we set ?e?,e? = ?k/M(?), we sat-
isfy the constraints in Eq. 14, and recover the update
in Eq. 11.
In the same manner as Eq. 12, we derive an opti-
mized update procedure for the softmax loss, which
replaces the update with Equation 10, by solving the
2Specifically, each parameter is bound constrained 0 ? ? ?
?k but is not summation constrained
?
? ? ?k. Thus, we re-
normalize ? after optimization.
256
following problem
argmin
w
1
2
?w ? wk+ 14 ?
2
2 + ?k
?
(f,e)?b
?f (16)
such that
w??(wk; f, e) ? ?L(wk; f, e)? ?f
?f ? 0
in which ?(w?; f, e) = ??wL(w; f, e)
?
?
w=w? . Equa-
tion 16 can be interpreted as a cutting-plane approx-
imation for the objective of Eq. 7, in which the orig-
inal objective of Eq. 7 with the softmax loss in Eq.
6 is approximated by |b| linear constraints derived
from the sub-gradients at pointwk (Teo et al, 2010).
Eq. 16 is efficiently solved by its Lagrange dual,
leading to an update
wk+ 12 ? wk+ 14 +
?
(f,e)?b
?f?(wk; f, e) (17)
subject to
?
(f,e)?b ?f ? ?k. Similar to Eq. 15, the
parameter update by?(?) is rescaled by its Lagrange
multipliers ?f in place of the uniform scale of 1/|b|
in the sub-gradient of Eq. 10.
4.2 Line Search for Parameter Mixing
For faster training, we employ an efficient paral-
lel training strategy proposed by McDonald et al
(2010). The training data D is split into S disjoint
shards, {D1, ..., DS}. Each shard learns its own pa-
rameters in each single epoch t and performs param-
eter mixing by averaging parameters across shards.
We propose an optimized parallel training in Al-
gorithm 3 which performs better mixing with re-
spect to the task loss, i.e. negative BLEU. In line
5, wt+
1
2 is computed by averaging wt+1,s from all
the shards after local training using their own data
Ds. Then, the new parameters wt+1 are obtained by
linearly interpolating with the parameters from the
previous epoch wt. The linear interpolation weight
? is efficiently computed by a line search proce-
dure which directly minimizes the negative corpus-
BLEU. The procedure is exactly the same as the line
search strategy employed in MERT using wt as our
starting point with the direction wt+
1
2 ? wt. The
idea of using the line search procedure is to find the
optimum parameters under corpus-BLEU without a
Algorithm 3 Distributed training with line search
1: w1 ? 0
2: for t = 1, ..., T do
3: wt,s ? wt ? Distribute parameters
4: Each shard learns wt+1,s using Ds
? Line 3?10 in Alg. 2
5: wt+
1
2 ? 1/S
?
s wt+1,s ? Mixing
6: wt+1 ? (1? ?)wt + ?wt+
1
2 ? Line search
7: end for
8: return wT+1
batch-local approximation. Unlike MERT, however,
we do not memorize nor merge all the n-bests gener-
ated across iterations, but keep only n-bests in each
iteration for faster training and for memory saving.
Thus, the optimum ? obtained by the line search may
be suboptimal in terms of the training objective, but
potentially better than averaging for minimizing the
final task loss.
5 Experiments
Experiments were carried out on the NIST 2008
Chinese-to-English Open MT task. The training
data consists of nearly 5.6 million bilingual sen-
tences and additional monolingual data, English
Gigaword, for 5-gram language model estimation.
MT02 and MT06 were used as our tuning and devel-
opment testing, and MT08 as our final testing with
all data consisting of four reference translations.
We use an in-house developed hypergraph-based
toolkit for training and decoding with synchronous-
CFGs (SCFG) for hierarchical phrase-bassed SMT
(Chiang, 2007). The system employs 14 features,
consisting of standard Hiero-style features (Chiang,
2007), and a set of indicator features, such as the
number of synchronous-rules in a derivation. Two
5-gram language models are also included, one from
the English-side of bitexts and the other from En-
glish Gigaword, with features counting the number
of out-of-vocabulary words in each model (Dyer et
al., 2011). For faster experiments, we precomputed
translation forests inspired by Xiao et al (2011). In-
stead of generating forests from bitexts in each it-
eration, we construct and save translation forests by
intersecting the source side of SCFG with input sen-
tences and by keeping the target side of the inter-
257
sected rules. n-bests are generated from the pre-
computed forests on the fly using the forest rescor-
ing framework (Huang and Chiang, 2007) with ad-
ditional non-local features, such as 5-gram language
models.
We compared four algorithms, MERT, PRO,
MIRA and our proposed online settings, online rank
optimization (ORO). Note that ORO without our op-
timization methods in Section 4 is essentially the
same as Pegasos, but differs in that we employ the
algorithm for ranking structured outputs with var-
ied objectives, hinge loss or softmax loss3. MERT
learns parameters from forests (Kumar et al, 2009)
with 4 restarts and 8 random directions in each it-
eration. We experimented on a variant of PRO4, in
which the objective in Eq. 4 with the hinge loss of
Eq. 5 was solved in each iteration in line 4 of Alg. 1
using an off-the-shelf solver5. Our MIRA solves the
problem in Equation 13 in line 7 of Alg. 2. For a sys-
tematic comparison, we used our exhaustive oracle
translation selection method in Section 3 for PRO,
MIRA and ORO. For each learning algorithm, we
ran 30 iterations and generated duplicate removed
1,000-best translations in each iteration. The hyper-
parameter ? for PRO and ORO was set to 10?5, se-
lected from among {10?3, 10?4, 10?5}, and 102 for
MIRA, chosen from {10, 102, 103} by preliminary
testing on MT06. Both decoding and learning are
parallelized and run on 8 cores. Each online learn-
ing took roughly 12 hours, and PRO took one day. It
took roughly 3 days for MERT with 20 iterations.
Translation results are measured by case sensitive
BLEU.
Table 1 presents our main results. Among the pa-
rameters from multiple iterations, we report the out-
puts that performed the best on MT06. With Moses
(Koehn et al, 2007), we achieved 30.36 and 23.64
BLEU for MT06 and MT08, respectively. We de-
note the ?O-? prefix for the optimized parameter up-
dates discussed in Section 4.1, and the ?-L? suffix
3The other major difference is the use of a simpler learning
rate, 1?k , which was very slow in our preliminary studies.
4Hopkins and May (2011) minimized logistic loss sampled
from the merged n-bests, and sentence-BLEU was used for de-
termining ranks.
5We used liblinear (Fan et al, 2008) at http://www.
csie.ntu.edu.tw/?cjlin/liblinear with the solver
type of 3.
MT06 MT08
MERT 31.45? 24.13?
PRO 31.76? 24.43?
MIRA-L 31.42? 24.15?
ORO-Lhinge 29.76 21.96
O-ORO-Lhinge 32.06 24.95
ORO-Lsoftmax 30.77 23.07
O-ORO-Lsoftmax 31.16? 23.20
Table 1: Translation results by BLEU. Results with-
out significant differences from the MERT baseline
are marked ?. The numbers in boldface are signif-
icantly better than the MERT baseline (both mea-
sured by the bootstrap resampling (Koehn, 2004)
with p > 0.05).
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
BL
EU
iteration
MIRA-L MT02MT08ORO-L MT02MT08O-ORO-L MT02MT08
Figure 1: Learning curves for three algorithms,
MIRA-L, ORO-Lhinge and O-ORO-Lhinge.
for parameter mixing by line search as described in
Section 4.2. The batch size was set to 16 for MIRA
and ORO. In general, our PRO and MIRA settings
achieved the results very comparable to MERT. The
hinge-loss and softmax objective OROs were lower
than those of the three baselines. The softmax ob-
jective with the optimized update (O-ORO-Lsoftmax)
performed better than the non-optimized version,
but it was still lower than our baselines. In the case
of the hinge-loss objective with the optimized update
(O-ORO-Lhinge), the gain in MT08 was significant,
and achieved the best BLEU.
Figure 1 presents the learning curves for three al-
gorithms MIRA-L, ORO-Lhinge and O-ORO-Lhinge,
in which the performance is measured by BLEU
258
MT06 MT08
MIRA 30.95 23.06
MIRA-L 31.42? 24.15?
OROhinge 29.09 21.93
ORO-Lhinge 29.76 21.96
OROsoftmax 30.80 23.06
ORO-Lsoftmax 30.77 23.07
O-OROhinge 31.15? 23.20
O-ORO-Lhinge 32.06 24.95
O-OROsoftmax 31.40? 23.93?
O-ORO-Lsoftmax 31.16? 23.20
Table 2: Parameter mixing by line search.
on the training data (MT02) and on the test data
(MT08). MIRA-L quickly converges and is slightly
unstable in the test set, while ORO-Lhinge is very sta-
ble and slow to converge, but with low performance
on the training and test data. The stable learning
curve in ORO-Lhinge is probably influenced by our
learning rate parameter ?0 = 0.2, which will be
investigated in future work. O-ORO-Lhinge is less
stable in several iterations, but steadily improves its
BLEU. The behavior is justified by our optimized
update procedure, in which the learning rate ?k is
used as a tradeoff parameter. Thus, it tries a very
aggressive update at the early stage of training, but
eventually becomes conservative in updating param-
eters.
Next, we compare the effect of line search for pa-
rameter mixing in Table 2. Line search was very
effective for MIRA and O-OROhinge, but less effec-
tive for the others. Since the line search procedure
directly minimizes a task loss, not objectives, this
may hurt the performance for the softmax objective,
where the margins between the correct and incorrect
translations are softly penalized.
Finally, Table 3 shows the effect of batch size se-
lected from {1, 4, 8, 16}. There seems to be no clear
trends in MIRA, and we achieved BLEU score of
24.58 by setting the batch size to 8. Clearly, set-
ting smaller batch size is better for ORO, but it is
the reverse for the optimized variants of both the
hinge and softmax objectives. Figure 2 compares
ORO-Lhinge and O-ORO-Lhinge on MT02 with dif-
ferent batch size settings. ORO-Lhinge converges
faster when the batch size is smaller and fine tun-
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
BL
EU
iteration
ORO-L batch-16batch-8batch-4O-ORO-L batch-16batch-8batch-4
Figure 2: Learning curves on MT02 for ORO-Lhinge
and O-ORO-Lhinge with different batch size.
ing of the learning rate parameter will be required
for a larger batch size. As discussed in Section 3,
the smaller batch size means frequent updates to pa-
rameters and a faster convergence, but potentially
leads to a poor performance since the corpus-BLEU
is approximately computed in a local batch. Our op-
timized update algorithms address the problem by
adjusting the tradeoff between the amount of up-
date to parameters and the loss, and perform better
for larger batch sizes with a more accurate corpus-
BLEU.
6 Related Work
Our work is largely inspired by pairwise rank op-
timization (Hopkins and May, 2011), but runs in
an online fashion similar to (Watanabe et al, 2007;
Chiang et al, 2008b). Major differences come from
the corpus-BLEU computation used to select oracle
translations. Instead of the sentence-BLEU used by
Hopkins andMay (2011) or the corpus-BLEU statis-
tics accumulated from previous translations gener-
ated by different parameters (Watanabe et al, 2007;
Chiang et al, 2008b), we used a simple batch lo-
cal corpus-BLEU (Haddow et al, 2011) in the same
way as an online approximation to the objectives.
An alternative is the use of a Taylor series approxi-
mation (Smith and Eisner, 2006; Rosti et al, 2011),
which was not investigated in this paper.
Training is performed by SGD with a parame-
ter projection method (Shalev-Shwartz et al, 2007).
Slower training incurred by the larger batch size
259
MT06 MT08
batch size 1 4 8 16 1 4 8 16
MIRA-L 31.28? 31.53? 31.63? 31.42? 23.46 23.97? 24.58 24.15?
ORO-Lhinge 31.32? 30.69 29.61 29.76 23.63 23.12 22.07 21.96
O-ORO-Lhinge 31.44? 31.54? 31.35? 32.06 23.72 24.02? 24.28? 24.95
ORO-Lsoftmax 25.10 31.66? 31.31? 30.77 19.27 23.59 23.50 23.07
O-ORO-Lsoftmax 31.15? 31.17? 30.90 31.16? 23.62 23.31 23.03 23.20
Table 3: Translation results with varied batch size.
for more accurate corpus-BLEU is addressed by
optimally scaling parameter updates in the spirit
of a passive-aggressive algorithm (Crammer et al,
2006). The derived algorithm is very similar to
MIRA, but differs in that the learning rate is em-
ployed as a hyperparameter for controlling the fit-
ness to training data which decays when training
proceeds. The non-uniform sub-gradient based up-
date is also employed in an exponentiated gradient
(EG) algorithm (Kivinen and Warmuth, 1997; Kivi-
nen andWarmuth, 2001) in which parameter updates
are maximum-likely estimated using an exponen-
tially combined sub-gradients. In contrast, our ap-
proach relies on an ultraconservative update which
tradeoff between the amount of updates performed
to the parameters and the progress made for the ob-
jectives by solving a QP subproblem.
Unlike a complex parallelization by Chiang et
al. (2008b), in which support vectors are asyn-
chronously exchanged among parallel jobs, train-
ing is efficiently and easily carried out by distribut-
ing training data among shards and by mixing pa-
rameters in each iteration (McDonald et al, 2010).
Rather than simple averaging, new parameters are
derived by linearly interpolating with the previously
mixed parameters, and its weight is determined by
the line search algorithm employed in (Och, 2003).
7 Conclusion
We proposed a variant of an online learning al-
gorithm inspired by a batch learning algorithm of
(Hopkins and May, 2011). Training is performed by
SGDwith a parameter projection (Shalev-Shwartz et
al., 2007) using a larger batch size for a more accu-
rate batch local corpus-BLEU estimation. Parameter
updates in each iteration is further optimized using
an idea from a passive-aggressive algorithm (Cram-
mer et al, 2006). Learning is efficiently parallelized
(McDonald et al, 2010) and the locally learned pa-
rameters are mixed by an additional line search step.
Experiments indicate that better performance was
achieved by our optimized updates and by the more
sophisticated parameter mixing.
In future work, we would like to investigate other
objectives with a more direct task loss, such as max-
margin (Taskar et al, 2004), risk (Smith and Eisner,
2006) or softmax-loss (Gimpel and Smith, 2010),
and different regularizers, such as L1-norm for a
sparse solution. Instead of n-best approximations,
we may directly employ forests for a better con-
ditional log-likelihood estimation (Li and Eisner,
2009). We would also like to explore other mix-
ing strategies for parallel training which can directly
minimize the training objectives like those proposed
for a cutting-plane algorithm (Franc and Sonnen-
burg, 2008).
Acknowledgments
We would like to thank anonymous reviewers and
our colleagues for helpful comments and discussion.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22:39?71, March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08: HLT, pages
200?208, Columbus, Ohio, June.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19:263?311, June.
260
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL 2005, pages 173?180, Ann Arbor,
Michigan, June.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient al-
gorithms. In Proc. of EMNLP 2008, pages 610?619,
Honolulu, Hawaii, October.
David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP 2008,
pages 224?233, Honolulu, Hawaii, October.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. of ACL 2011, pages 176?181, Portland,
Oregon, USA, June.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25?70, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, March.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The cmu-ark german-english
translation system. In Proc. of SMT 2011, pages 337?
343, Edinburgh, Scotland, July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874, June.
Vojte?ch Franc and Soeren Sonnenburg. 2008. Optimized
cutting plane algorithm for support vector machines.
In Proc. of ICML ?08, pages 320?327, Helsinki, Fin-
land.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proc. of EMNLP
2011, pages 38?49, Edinburgh, Scotland, UK., July.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin crfs: Training log-linear models with cost
functions. In Proc. of NAACL-HLT 2010, pages 733?
736, Los Angeles, California, June.
Barry Haddow, Abhishek Arun, and Philipp Koehn.
2011. Samplerank training for phrase-based machine
translation. In Proc. of SMT 2011, pages 261?271, Ed-
inburgh, Scotland, July.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support vector learning for ordinal regression.
In In Proc. of International Conference on Artificial
Neural Networks, pages 97?102.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proc. of EMNLP 2011, pages 1352?1362, Ed-
inburgh, Scotland, UK., July.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In Proc. of
ICML ?08, pages 408?415, Helsinki, Finland.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language models.
In Proc. of ACL 2007, pages 144?151, Prague, Czech
Republic, June.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132(1):1?
63, January.
J. Kivinen and M. K. Warmuth. 2001. Relative
loss bounds for multidimensional regression problems.
Machine Learning, 45(3):301?329, December.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Procc of
ACL 2007, pages 177?180, Prague, Czech Republic,
June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proc. of ACL-IJCNLP
2009, pages 163?171, Suntec, Singapore, August.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009, pages 40?51, Singapore, August.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of COL-
ING/ACL 2006, pages 761?768, Sydney, Australia,
July.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proc. of NAACL-HLT 2010, pages 456?
464, Los Angeles, California, June.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proc. of COLING 2008, pages
585?592, Manchester, UK, August.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
261
tical machine translation. In Proc. of ACL 2002, pages
295?302, Philadelphia, July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. of ACL 2002,
pages 311?318, Philadelphia, Pennsylvania, USA,
July.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected bleu training for
graphs: Bbn system description for wmt11 system
combination task. In Proc. of SMT 2011, pages 159?
165, Edinburgh, Scotland, July.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proc. of ICML ?07, pages 807?814, Cor-
valis, Oregon.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In Proc.
of COLING/ACL 2006, pages 787?794, Sydney, Aus-
tralia, July.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. of EMNLP 2004, pages 1?8, Barcelona, Spain,
July.
Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, and
Quoc V. Le. 2010. Bundle methods for regularized
risk minimization. Journal of Machine Learning Re-
search, 11:311?365, March.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proc. of ACL-IJCNLP 2009, pages 477?
485, Suntec, Singapore, August.
Ashish Venugopal. 2005. Considerations in maximum
mutual information and minimum classification error
training for statistical machine translation. In Proc. of
EAMT-05, page 3031.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007, pages 764?773, Prague, Czech Republic, June.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In Proc. of EMNLP
2011, pages 880?888, Edinburgh, Scotland, UK., July.
262
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632?641,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Unsupervised Model for Joint Phrase Alignment and Extraction
Graham Neubig1,2 Taro Watanabe2, Eiichiro Sumita2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We present an unsupervised model for joint
phrase alignment and extraction using non-
parametric Bayesian methods and inversion
transduction grammars (ITGs). The key con-
tribution is that phrases of many granulari-
ties are included directly in the model through
the use of a novel formulation that memorizes
phrases generated not only by terminal, but
also non-terminal symbols. This allows for
a completely probabilistic model that is able
to create a phrase table that achieves com-
petitive accuracy on phrase-based machine
translation tasks directly from unaligned sen-
tence pairs. Experiments on several language
pairs demonstrate that the proposed model
matches the accuracy of traditional two-step
word alignment/phrase extraction approach
while reducing the phrase table to a fraction
of the original size.
1 Introduction
The training of translation models for phrase-
based statistical machine translation (SMT) systems
(Koehn et al, 2003) takes unaligned bilingual train-
ing data as input, and outputs a scored table of
phrase pairs. This phrase table is traditionally gen-
erated by going through a pipeline of two steps, first
generating word (or minimal phrase) alignments,
then extracting a phrase table that is consistent with
these alignments.
However, as DeNero and Klein (2010) note, this
two step approach results in word alignments that
are not optimal for the final task of generating
phrase tables that are used in translation. As a so-
lution to this, they proposed a supervised discrimi-
native model that performs joint word alignment and
phrase extraction, and found that joint estimation of
word alignments and extraction sets improves both
word alignment accuracy and translation results.
In this paper, we propose the first unsuper-
vised approach to joint alignment and extraction of
phrases at multiple granularities. This is achieved
by constructing a generative model that includes
phrases at many levels of granularity, from minimal
phrases all the way up to full sentences. The model
is similar to previously proposed phrase alignment
models based on inversion transduction grammars
(ITGs) (Cherry and Lin, 2007; Zhang et al, 2008;
Blunsom et al, 2009), with one important change:
ITG symbols and phrase pairs are generated in
the opposite order. In traditional ITG models, the
branches of a biparse tree are generated from a non-
terminal distribution, and each leaf is generated by
a word or phrase pair distribution. As a result, only
minimal phrases are directly included in the model,
while larger phrases must be generated by heuris-
tic extraction methods. In the proposed model, at
each branch in the tree, we first attempt to gener-
ate a phrase pair from the phrase pair distribution,
falling back to ITG-based divide and conquer strat-
egy to generate phrase pairs that do not exist (or are
given low probability) in the phrase distribution.
We combine this model with the Bayesian non-
parametric Pitman-Yor process (Pitman and Yor,
1997; Teh, 2006), realizing ITG-based divide and
conquer through a novel formulation where the
Pitman-Yor process uses two copies of itself as a
632
base measure. As a result of this modeling strategy,
phrases of multiple granularities are generated, and
thus memorized, by the Pitman-Yor process. This
makes it possible to directly use probabilities of the
phrase model as a replacement for the phrase table
generated by heuristic extraction techniques.
Using this model, we perform machine transla-
tion experiments over four language pairs. We ob-
serve that the proposed joint phrase alignment and
extraction approach is able to meet or exceed results
attained by a combination of GIZA++ and heuristic
phrase extraction with significantly smaller phrase
table size. We also find that it achieves superior
BLEU scores over previously proposed ITG-based
phrase alignment approaches.
2 A Probabilistic Model for Phrase Table
Extraction
The problem of SMT can be defined as finding the
most probable target sentence e for the source sen-
tence f given a parallel training corpus ?E ,F?
e? = argmax
e
P (e|f , ?E ,F?).
We assume that there is a hidden set of parameters
? learned from the training data, and that e is condi-
tionally independent from the training corpus given
?. We take a Bayesian approach, integrating over all
possible values of the hidden parameters:
P (e|f , ?E ,F?) =
?
?
P (e|f , ?)P (?|?E ,F?). (1)
If ? takes the form of a scored phrase table, we
can use traditional methods for phrase-based SMT to
find P (e|f , ?) and concentrate on creating a model
for P (?|?E ,F?). We decompose this posterior prob-
ability using Bayes law into the corpus likelihood
and parameter prior probabilities
P (?|?E ,F?) ? P (?E ,F?|?)P (?).
In Section 3 we describe an existing method, and
in Section 4 we describe our proposed method for
modeling these two probabilities.
3 Flat ITG Model
There has been a significant amount of work in
many-to-many alignment techniques (Marcu and
Wong (2002), DeNero et al (2008), inter alia), and
in particular a number of recent works (Cherry and
Lin, 2007; Zhang et al, 2008; Blunsom et al, 2009)
have used the formalism of inversion transduction
grammars (ITGs) (Wu, 1997) to learn phrase align-
ments. By slightly limit reordering of words, ITGs
make it possible to exactly calculate probabilities
of phrasal alignments in polynomial time, which is
a computationally hard problem when arbitrary re-
ordering is allowed (DeNero and Klein, 2008).
The traditional flat ITG generative probabil-
ity for a particular phrase (or sentence) pair
Pflat(?e, f?; ?x, ?t) is parameterized by a phrase ta-
ble ?t and a symbol distribution ?x. We use the fol-
lowing generative story as a representative of the flat
ITG model.
1. Generate symbol x from the multinomial distri-
bution Px(x; ?x). x can take the values TERM,
REG, or INV.
2. According to the x take the following actions.
(a) If x = TERM, generate a phrase pair from
the phrase table Pt(?e, f?; ?t).
(b) If x = REG, a regular ITG rule, gener-
ate phrase pairs ?e1, f1? and ?e2, f2? from
Pflat, and concatenate them into a single
phrase pair ?e1e2, f1f2?.
(c) If x = INV, an inverted ITG rule, follows
the same process as (b), but concatenate
f1 and f2 in reverse order ?e1e2, f2f1?.
By taking the product of Pflat over every sentence
in the corpus, we are able to calculate the likelihood
P (?E ,F?|?) =
?
?e,f???E,F?
Pflat(?e, f?; ?).
We will refer to this model as FLAT.
3.1 Bayesian Modeling
While the previous formulation can be used as-is in
maximum likelihood training, this leads to a degen-
erate solution where every sentence is memorized as
a single phrase pair. Zhang et al (2008) and others
propose dealing with this problem by putting a prior
probability P (?x, ?t) on the parameters.
633
We assign ?x a Dirichlet prior1, and assign the
phrase table parameters ?t a prior using the Pitman-
Yor process (Pitman and Yor, 1997; Teh, 2006),
which is a generalization of the Dirichlet process
prior used in previous research. It is expressed as
?t ?PY (d, s, Pbase) (2)
where d is the discount parameter, s is the strength
parameter, and Pbase is the base measure. The dis-
count d is subtracted from observed counts, and
when it is given a large value (close to one), less
frequent phrase pairs will be given lower relative
probability than more common phrase pairs. The
strength s controls the overall sparseness of the dis-
tribution, and when it is given a small value the dis-
tribution will be sparse. Pbase is the prior probability
of generating a particular phrase pair, which we de-
scribe in more detail in the following section.
Non-parametric priors are well suited for mod-
eling the phrase distribution because every time a
phrase is generated by the model, it is ?memorized?
and given higher probability. Because of this, com-
mon phrase pairs are more likely to be re-used (the
rich-get-richer effect), which results in the induc-
tion of phrase tables with fewer, but more helpful
phrases. It is important to note that only phrases
generated by Pt are actually memorized and given
higher probability by the model. In FLAT, only min-
imal phrases generated after Px outputs the terminal
symbol TERM are generated from Pt, and thus only
minimal phrases are memorized by the model.
While the Dirichlet process is simply the Pitman-
Yor process with d = 0, it has been shown that the
discount parameter allows for more effective mod-
eling of the long-tailed distributions that are often
found in natural language (Teh, 2006). We con-
firmed in preliminary experiments (using the data
described in Section 7) that the Pitman-Yor process
with automatically adjusted parameters results in su-
perior alignment results, outperforming the sparse
Dirichlet process priors used in previous research2.
The average gain across all data sets was approxi-
mately 0.8 BLEU points.
1The value of ? had little effect on the results, so we arbi-
trarily set ? = 1.
2We put weak priors on s (Gamma(? = 2, ? = 1)) and
d (Beta(? = 2, ? = 2)) for the Pitman-Yor process, and set
? = 1?10 for the Dirichlet process.
3.2 Base Measure
Pbase in Equation (2) indicates the prior probability
of phrase pairs according to the model. By choosing
this probability appropriately, we can incorporate
prior knowledge of what phrases tend to be aligned
to each other. We calculate Pbase by first choosing
whether to generate an unaligned phrase pair (where
|e| = 0 or |f | = 0) according to a fixed probabil-
ity pu3, then generating from Pba for aligned phrase
pairs, or Pbu for unaligned phrase pairs.
For Pba, we adopt a base measure similar to that
used by DeNero et al (2008):
Pba(?e, f?) =M0(?e, f?)Ppois(|e|;?)Ppois(|f |;?)
M0(?e, f?) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?. As long phrases lead to spar-
sity, we set ? to a relatively small value to allow
us to bias against overly long phrases4. Pm1 is the
word-based Model 1 (Brown et al, 1993) probabil-
ity of one phrase given the other, which incorporates
word-based alignment information as prior knowl-
edge in the phrase translation probability. We take
the geometric mean5of the Model 1 probabilities in
both directions to encourage alignments that are sup-
ported by both models (Liang et al, 2006). It should
be noted that while Model 1 probabilities are used,
they are only soft constraints, compared with the
hard constraint of choosing a single word alignment
used in most previous phrase extraction approaches.
For Pbu, if g is the non-null phrase in e and f , we
calculate the probability as follows:
Pbu(?e, f?) = Puni(g)Ppois(|g|;?)/2.
Note that Pbu is divided by 2 as the probability is
considering null alignments in both directions.
4 Hierarchical ITG Model
While in FLAT only minimal phrases were memo-
rized by the model, as DeNero et al (2008) note
3We choose 10?2, 10?3, or 10?10 based on which value
gave the best accuracy on the development set.
4We tune ? to 1, 0.1, or 0.01 based on which value gives the
best performance on the development set.
5The probabilities of the geometric mean do not add to one,
but we found empirically that even when left unnormalized, this
provided much better results than the using the arithmetic mean,
which is more theoretically correct.
634
and we confirm in the experiments in Section 7, us-
ing only minimal phrases leads to inferior transla-
tion results for phrase-based SMT. Because of this,
previous research has combined FLAT with heuris-
tic phrase extraction, which exhaustively combines
all adjacent phrases permitted by the word align-
ments (Och et al, 1999). We propose an alterna-
tive, fully statistical approach that directly models
phrases at multiple granularities, which we will refer
to as HIER. By doing so, we are able to do away with
heuristic phrase extraction, creating a fully proba-
bilistic model for phrase probabilities that still yields
competitive results.
Similarly to FLAT, HIER assigns a probability
Phier(?e, f?; ?x, ?t) to phrase pairs, and is parame-
terized by a phrase table ?t and a symbol distribu-
tion ?x. The main difference from the generative
story of the traditional ITG model is that symbols
and phrase pairs are generated in the opposite order.
While FLAT first generates branches of the derivation
tree using Px, then generates leaves using the phrase
distribution Pt, HIER first attempts to generate the
full sentence as a single phrase from Pt, then falls
back to ITG-style derivations to cope with sparsity.
We allow for this within the Bayesian ITG context
by defining a new base measure Pdac (?divide-and-
conquer?) to replace Pbase in Equation (2), resulting
in the following distribution for ?t.
?t ? PY (d, s, Pdac) (3)
Pdac essentially breaks the generation of a sin-
gle longer phrase into two generations of shorter
phrases, allowing even phrase pairs for which
c(?e, f?) = 0 to be given some probability. The
generative process of Pdac, similar to that of Pflat
from the previous section, is as follows:
1. Generate symbol x from Px(x; ?x). x can take
the values BASE, REG, or INV.
2. According to x take the following actions.
(a) If x = BASE, generate a new phrase pair
directly from Pbase of Section 3.2.
(b) If x = REG, generate ?e1, f1? and ?e2, f2?
from Phier, and concatenate them into a
single phrase pair ?e1e2, f1f2?.
Figure 1: A word alignment (a), and its derivations ac-
cording to FLAT (b), and HIER (c). Solid and dotted lines
indicate minimal and non-minimal pairs respectively, and
phrases are written under their corresponding instance of
Pt. The pair hate/cou?te is generated from Pbase.
(c) If x = INV, follow the same process as
(b), but concatenate f1 and f2 in reverse
order ?e1e2, f2f1?.
A comparison of derivation trees for FLAT and
HIER is shown in Figure 1. As previously de-
scribed, FLAT first generates from the symbol dis-
tribution Px, then from the phrase distribution Pt,
while HIER generates directly from Pt, which falls
back to divide-and-conquer based on Px when nec-
essary. It can be seen that while Pt in FLAT only gen-
erates minimal phrases, Pt in HIER generates (and
thus memorizes) phrases at all levels of granularity.
4.1 Length-based Parameter Tuning
There are still two problems with HIER, one theo-
retical, and one practical. Theoretically, HIER con-
tains itself as its base measure, and stochastic pro-
cess models that include themselves as base mea-
sures are deficient, as noted in Cohen et al (2010).
Practically, while the Pitman-Yor process in HIER
shares the parameters s and d over all phrase pairs in
the model, long phrase pairs are much more sparse
635
Figure 2: Learned discount values by phrase pair length.
than short phrase pairs, and thus it is desirable to
appropriately adjust the parameters of Equation (2)
according to phrase pair length.
In order to solve these problems, we reformulate
the model so that each phrase length l = |f |+|e| has
its own phrase parameters ?t,l and symbol parame-
ters ?x,l, which are given separate priors:
?t,l ? PY (s, d, Pdac,l)
?x,l ? Dirichlet(?)
We will call this model HLEN.
The generative story is largely similar to HIER
with a few minor changes. When we generate a sen-
tence, we first choose its length l according to a uni-
form distribution over all possible sentence lengths
l ? Uniform(1, L),
where L is the size |e| + |f | of the longest sentence
in the corpus. We then generate a phrase pair from
the probability Pt,l(?e, f?) for length l. The base
measure for HLEN is identical to that of HIER, with
one minor change: when we fall back to two shorter
phrases, we choose the length of the left phrase from
ll ? Uniform(1, l ? 1), set the length of the right
phrase to lr = l?ll, and generate the smaller phrases
from Pt,ll and Pt,lr respectively.
It can be seen that phrases at each length are gen-
erated from different distributions, and thus the pa-
rameters for the Pitman-Yor process will be differ-
ent for each distribution. Further, as ll and lr must
be smaller than l, Pt,l no longer contains itself as a
base measure, and is thus not deficient.
An example of the actual discount values learned
in one of the experiments described in Section 7
is shown in Figure 2. It can be seen that, as ex-
pected, the discounts for short phrases are lower than
those of long phrases. In particular, phrase pairs of
length up to six (for example, |e| = 3, |f | = 3) are
given discounts of nearly zero while larger phrases
are more heavily discounted. We conjecture that this
is related to the observation by Koehn et al (2003)
that using phrases where max(|e|, |f |) ? 3 cause
significant improvements in BLEU score, while us-
ing larger phrases results in diminishing returns.
4.2 Implementation
Previous research has used a variety of sampling
methods to learn Bayesian phrase based alignment
models (DeNero et al, 2008; Blunsom et al, 2009;
Blunsom and Cohn, 2010). All of these techniques
are applicable to the proposed model, but we choose
to apply the sentence-based blocked sampling of
Blunsom and Cohn (2010), which has desirable con-
vergence properties compared to sampling single
alignments. As exhaustive sampling is too slow for
practical purpose, we adopt the beam search algo-
rithm of Saers et al (2009), and use a probability
beam, trimming spans where the probability is at
least 1010 times smaller than that of the best hypoth-
esis in the bucket.
One important implementation detail that is dif-
ferent from previous models is the management of
phrase counts. As a phrase pair ta may have been
generated from two smaller component phrases tb
and tc, when a sample containing ta is removed from
the distribution, it may also be necessary to decre-
ment the counts of tb and tc as well. The Chinese
Restaurant Process representation of Pt (Teh, 2006)
lends itself to a natural and easily implementable so-
lution to this problem. For each table representing a
phrase pair ta, we maintain not only the number of
customers sitting at the table, but also the identities
of phrases tb and tc that were originally used when
generating the table. When the count of the table
ta is reduced to zero and the table is removed, the
counts of tb and tc are also decremented.
5 Phrase Extraction
In this section, we describe both traditional heuris-
tic phrase extraction, and the proposed model-based
extraction method.
636
Figure 3: The phrase, block, and word alignments used
in heuristic phrase extraction.
5.1 Heuristic Phrase Extraction
The traditional method for heuristic phrase extrac-
tion from word alignments exhaustively enumerates
all phrases up to a certain length consistent with the
alignment (Och et al, 1999). Five features are used
in the phrase table: the conditional phrase proba-
bilities in both directions estimated using maximum
likelihood Pml(f |e) and Pml(e|f), lexical weight-
ing probabilities (Koehn et al, 2003), and a fixed
penalty for each phrase. We will call this heuristic
extraction from word alignments HEUR-W. These
word alignments can be acquired through the stan-
dard GIZA++ training regimen.
We use the combination of our ITG-based align-
ment with traditional heuristic phrase extraction as
a second baseline. An example of these alignments
is shown in Figure 3. In model HEUR-P, minimal
phrases generated from Pt are treated as aligned, and
we perform phrase extraction on these alignments.
However, as the proposed models tend to align rel-
atively large phrases, we also use two other tech-
niques to create smaller alignment chunks that pre-
vent sparsity. We perform regular sampling of the
trees, but if we reach a minimal phrase generated
from Pt, we continue traveling down the tree un-
til we reach either a one-to-many alignment, which
we will call HEUR-B as it creates alignments simi-
lar to the block ITG, or an at-most-one alignment,
which we will call HEUR-W as it generates word
alignments. It should be noted that forcing align-
ments smaller than the model suggests is only used
for generating alignments for use in heuristic extrac-
tion, and does not affect the training process.
5.2 Model-Based Phrase Extraction
We also propose a method for phrase table ex-
traction that directly utilizes the phrase probabil-
ities Pt(?e, f?). Similarly to the heuristic phrase
tables, we use conditional probabilities Pt(f |e)
and Pt(e|f), lexical weighting probabilities, and a
phrase penalty. Here, instead of using maximum
likelihood, we calculate conditional probabilities di-
rectly from Pt probabilities:
Pt(f |e) = Pt(?e, f?)/
?
{f? :c(?e,f??)?1}
Pt(?e, f??)
Pt(e|f) = Pt(?e, f?)/
?
{e?:c(?e?,f?)?1}
Pt(?e?, f?).
To limit phrase table size, we include only phrase
pairs that are aligned at least once in the sample.
We also include two more features: the phrase
pair joint probability Pt(?e, f?), and the average
posterior probability of each span that generated
?e, f? as computed by the inside-outside algorithm
during training. We use the span probability as it
gives a hint about the reliability of the phrase pair. It
will be high for common phrase pairs that are gen-
erated directly from the model, and also for phrases
that, while not directly included in the model, are
composed of two high probability child phrases.
It should be noted that while for FLAT and HIER Pt
can be used directly, as HLEN learns separate models
for each length, we must combine these probabilities
into a single value. We do this by setting
Pt(?e, f?) = Pt,l(?e, f?)c(l)/
L
?
l?=1
c(l?)
for every phrase pair, where l = |e|+ |f | and c(l) is
the number of phrases of length l in the sample.
We call this model-based extraction method MOD.
5.3 Sample Combination
As has been noted in previous works, (Koehn et al,
2003; DeNero et al, 2006) exhaustive phrase extrac-
tion tends to out-perform approaches that use syn-
tax or generative models to limit phrase boundaries.
DeNero et al (2006) state that this is because gen-
erative models choose only a single phrase segmen-
tation, and thus throw away many good phrase pairs
that are in conflict with this segmentation.
Luckily, in the Bayesian framework it is simple to
overcome this problem by combining phrase tables
637
from multiple samples. This is equivalent to approx-
imating the integral over various parameter configu-
rations in Equation (1). In MOD, we do this by taking
the average of the joint probability and span prob-
ability features, and re-calculating the conditional
probabilities from the averaged joint probabilities.
6 Related Work
In addition to the previously mentioned phrase
alignment techniques, there has also been a signif-
icant body of work on phrase extraction (Moore and
Quirk (2007), Johnson et al (2007a), inter alia).
DeNero and Klein (2010) presented the first work
on joint phrase alignment and extraction at multiple
levels. While they take a supervised approach based
on discriminative methods, we present a fully unsu-
pervised generative model.
A generative probabilistic model where longer
units are built through the binary combination of
shorter units was proposed by deMarcken (1996) for
monolingual word segmentation using the minimum
description length (MDL) framework. Our work dif-
fers in that it uses Bayesian techniques instead of
MDL, and works on two languages, not one.
Adaptor grammars, models in which non-
terminals memorize subtrees that lie below them,
have been used for word segmentation or other
monolingual tasks (Johnson et al, 2007b). The pro-
posed method could be thought of as synchronous
adaptor grammars over two languages. However,
adaptor grammars have generally been used to spec-
ify only two or a few levels as in the FLAT model in
this paper, as opposed to recursive models such as
HIER or many-leveled models such as HLEN. One
exception is the variational inference method for
adaptor grammars presented by Cohen et al (2010)
that is applicable to recursive grammars such as
HIER. We plan to examine variational inference for
the proposed models in future work.
7 Experimental Evaluation
We evaluate the proposed method on translation
tasks from four languages, French, German, Span-
ish, and Japanese, into English.
de-en es-en fr-en ja-en
TM (en) 1.80M 1.62M 1.35M 2.38M
TM (other) 1.85M 1.82M 1.56M 2.78M
LM (en) 52.7M 52.7M 52.7M 44.7M
Tune (en ) 49.8k 49.8k 49.8k 68.9k
Tune (other) 47.2k 52.6k 55.4k 80.4k
Test (en) 65.6k 65.6k 65.6k 40.4k
Test (other) 62.7k 68.1k 72.6k 48.7k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
7.1 Experimental Setup
The data for French, German, and Spanish are from
the 2010 Workshop on Statistical Machine Transla-
tion (Callison-Burch et al, 2010). We use the news
commentary corpus for training the TM, and the
news commentary and Europarl corpora for training
the LM. For Japanese, we use data from the NTCIR
patent translation task (Fujii et al, 2008). We use
the first 100k sentences of the parallel corpus for the
TM, and the whole parallel corpus for the LM. De-
tails of both corpora can be found in Table 1. Cor-
pora are tokenized, lower-cased, and sentences of
over 40 words on either side are removed for TM
training. For both tasks, we perform weight tuning
and testing on specified development and test sets.
We compare the accuracy of our proposed method
of joint phrase alignment and extraction using the
FLAT, HIER and HLEN models, with a baseline of
using word alignments from GIZA++ and heuris-
tic phrase extraction. Decoding is performed using
Moses (Koehn and others, 2007) using the phrase
tables learned by each method under consideration,
as well as standard bidirectional lexical reordering
probabilities (Koehn et al, 2005). Maximum phrase
length is limited to 7 in all models, and for the LM
we use an interpolated Kneser-Ney 5-gram model.
For GIZA++, we use the standard training reg-
imen up to Model 4, and combine alignments
with grow-diag-final-and. For the proposed
models, we train for 100 iterations, and use the final
sample acquired at the end of the training process for
our experiments using a single sample6. In addition,
6For most models, while likelihood continued to increase
gradually for all 100 iterations, BLEU score gains plateaued af-
ter 5-10 iterations, likely due to the strong prior information
638
de-en es-en fr-en ja-en
Align Extract # Samp. BLEU Size BLEU Size BLEU Size BLEU Size
GIZA++ HEUR-W 1 16.62 4.91M 22.00 4.30M 21.35 4.01M 23.20 4.22M
FLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7k
HIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723k
HLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820k
HIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21M
HLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70M
Table 2: BLEU score and phrase table size by alignment method, extraction method, and samples combined. Bold
numbers are not significantly different from the best result according to the sign test (p < 0.05) (Collins et al, 2005).
we also try averaging the phrase tables from the last
ten samples as described in Section 5.3.
7.2 Experimental Results
The results for these experiments can be found in Ta-
ble 2. From these results we can see that when using
a single sample, the combination of using HIER and
model probabilities achieves results approximately
equal to GIZA++ and heuristic phrase extraction.
This is the first reported result in which an unsu-
pervised phrase alignment model has built a phrase
table directly from model probabilities and achieved
results that compare to heuristic phrase extraction. It
can also be seen that the phrase table created by the
proposed method is approximately 5 times smaller
than that obtained by the traditional pipeline.
In addition, HIER significantly outperforms FLAT
when using the model probabilities. This confirms
that phrase tables containing only minimal phrases
are not able to achieve results that compete with
phrase tables that use multiple granularities.
Somewhat surprisingly, HLEN consistently
slightly underperforms HIER. This indicates
potential gains to be provided by length-based
parameter tuning were outweighed by losses due
to the increased complexity of the model. In
particular, we believe the necessity to combine
probabilities from multiple Pt,l models into a single
phrase table may have resulted in a distortion of the
phrase probabilities. In addition, the assumption
that phrase lengths are generated from a uniform
distribution is likely too strong, and further gains
provided by Pbase. As iterations took 1.3 hours on a single
processor, good translation results can be achieved in approxi-
mately 13 hours, which could further reduced using distributed
sampling (Newman et al, 2009; Blunsom et al, 2009).
FLAT HIER
MOD 17.97 117k 21.50 751k
HEUR-W 21.52 5.65M 21.68 5.39M
HEUR-B 21.45 4.93M 21.41 2.61M
HEUR-P 21.56 4.88M 21.47 1.62M
Table 3: Translation results and phrase table size for var-
ious phrase extraction techniques (French-English).
could likely be achieved by more accurate modeling
of phrase lengths. We leave further adjustments to
the HLEN model to future work.
It can also be seen that combining phrase tables
from multiple samples improved the BLEU score
for HLEN, but not for HIER. This suggests that for
HIER, most of the useful phrase pairs discovered by
the model are included in every iteration, and the in-
creased recall obtained by combining multiple sam-
ples does not consistently outweigh the increased
confusion caused by the larger phrase table.
We also evaluated the effectiveness of model-
based phrase extraction compared to heuristic phrase
extraction. Using the alignments from HIER, we cre-
ated phrase tables using model probabilities (MOD),
and heuristic extraction on words (HEUR-W), blocks
(HEUR-B), and minimal phrases (HEUR-P) as de-
scribed in Section 5. The results of these ex-
periments are shown in Table 3. It can be seen
that model-based phrase extraction using HIER out-
performs or insignificantly underperforms heuris-
tic phrase extraction over all experimental settings,
while keeping the phrase table to a fraction of the
size of most heuristic extraction methods.
Finally, we varied the size of the parallel corpus
for the Japanese-English task from 50k to 400k sen-
639
Figure 4: The effect of corpus size on the accuracy (a) and
phrase table size (b) for each method (Japanese-English).
tences and measured the effect of corpus size on
translation accuracy. From the results in Figure 4
(a), it can be seen that at all corpus sizes, the re-
sults from all three methods are comparable, with
insignificant differences between GIZA++ and HIER
at all levels, and HLEN lagging slightly behind HIER.
Figure 4 (b) shows the size of the phrase table in-
duced by each method over the various corpus sizes.
It can be seen that the tables created by GIZA++ are
significantly larger at all corpus sizes, with the dif-
ference being particularly pronounced at larger cor-
pus sizes.
8 Conclusion
In this paper, we presented a novel approach to joint
phrase alignment and extraction through a hierar-
chical model using non-parametric Bayesian meth-
ods and inversion transduction grammars. Machine
translation systems using phrase tables learned di-
rectly by the proposed model were able to achieve
accuracy competitive with the traditional pipeline of
word alignment and heuristic phrase extraction, the
first such result for an unsupervised model.
For future work, we plan to refine HLEN to use
a more appropriate model of phrase length than
the uniform distribution, particularly by attempting
to bias against phrase pairs where one of the two
phrases is much longer than the other. In addition,
we will test probabilities learned using the proposed
model with an ITG-based decoder. We will also ex-
amine the applicability of the proposed model in the
context of hierarchical phrases (Chiang, 2007), or
in alignment using syntactic structure (Galley et al,
2006). It is also worth examining the plausibility
of variational inference as proposed by Cohen et al
(2010) in the alignment context.
Acknowledgments
This work was performed while the first author
was supported by the JSPS Research Fellowship for
Young Scientists.
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proceed-
ings of the Human Language Technology: The 11th
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association for Computa-
tional Linguistics, pages 782?790.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint 5th Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL Workshop on Syntax and
Structure in Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars. In
Proceedings of the Human Language Technology: The
640
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 564?572.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 531?540.
Carl de Marcken. 1996. Unsupervised Language Acqui-
sition. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 25?28.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1453?1463.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of the 1st Workshop
on Statistical Machine Translation, pages 31?38.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 314?323.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of the 7th NTCIR Workshop Meeting on Evaluation of
Information Access Technologies, pages 389?400.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics, pages 961?968.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007a. Improving translation quality
by discarding most of the phrasetable. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
Advances in Neural Information Processing Systems,
19:641.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference (HLT-
NAACL), pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 104?111.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. pages 133?139.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of
the 2nd Workshop on Statistical Machine Translation,
pages 112?119.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 4th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 20?28.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the The 11th International Workshop
on Parsing Technologies.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 97?105.
641
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249?1257,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Machine Translation System Combination by Confusion Forest
Taro Watanabe and Eiichiro Sumita
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, 619-0289 JAPAN
{taro.watanabe,eiichiro.sumita}@nict.go.jp
Abstract
The state-of-the-art system combination
method for machine translation (MT) is
based on confusion networks constructed
by aligning hypotheses with regard to word
similarities. We introduce a novel system
combination framework in which hypotheses
are encoded as a confusion forest, a packed
forest representing alternative trees. The
forest is generated using syntactic consensus
among parsed hypotheses: First, MT outputs
are parsed. Second, a context free grammar is
learned by extracting a set of rules that con-
stitute the parse trees. Third, a packed forest
is generated starting from the root symbol of
the extracted grammar through non-terminal
rewriting. The new hypothesis is produced
by searching the best derivation in the forest.
Experimental results on the WMT10 system
combination shared task yield comparable
performance to the conventional confusion
network based method with smaller space.
1 Introduction
System combination techniques take the advantages
of consensus among multiple systems and have been
widely used in fields, such as speech recognition
(Fiscus, 1997; Mangu et al, 2000) or parsing (Hen-
derson and Brill, 1999). One of the state-of-the-art
system combination methods for MT is based on
confusion networks, which are compact graph-based
structures representing multiple hypotheses (Banga-
lore et al, 2001).
Confusion networks are constructed based on
string similarity information. First, one skeleton or
backbone sentence is selected. Then, other hypothe-
ses are aligned against the skeleton, forming a lattice
with each arc representing alternative word candi-
dates. The alignment method is either model-based
(Matusov et al, 2006; He et al, 2008) in which a
statistical word aligner is used to compute hypothe-
sis alignment, or edit-based (Jayaraman and Lavie,
2005; Sim et al, 2007) in which alignment is mea-
sured by an evaluation metric, such as translation er-
ror rate (TER) (Snover et al, 2006). The new trans-
lation hypothesis is generated by selecting the best
path through the network.
We present a novel method for system combina-
tion which exploits the syntactic similarity of system
outputs. Instead of constructing a string-based con-
fusion network, we generate a packed forest (Billot
and Lang, 1989; Mi et al, 2008) which encodes ex-
ponentially many parse trees in a polynomial space.
The packed forest, or confusion forest, is constructed
by merging the MT outputs with regard to their
syntactic consensus. We employ a grammar-based
method to generate the confusion forest: First, sys-
tem outputs are parsed. Second, a set of rules are
extracted from the parse trees. Third, a packed for-
est is generated using a variant of Earley?s algorithm
(Earley, 1970) starting from the unique root symbol.
New hypotheses are selected by searching the best
derivation in the forest. The grammar, a set of rules,
is limited to those found in the parse trees. Spuri-
ous ambiguity during the generation step is further
reduced by encoding the tree local contextual infor-
mation in each non-terminal symbol, such as parent
and sibling labels, using the state representation in
Earley?s algorithm.
1249
Experiments were carried out for the system
combination task of the fifth workshop on sta-
tistical machine translation (WMT10) in four di-
rections, {Czech, French, German, Spanish}-to-
English (Callison-Burch et al, 2010), and we found
comparable performance to the conventional con-
fusion network based system combination in two
language pairs, and statistically significant improve-
ments in the others.
First, we will review the state-of-the-art method
which is a system combination framework based on
confusion networks (?2). Then, we will introduce
a novel system combination method based on con-
fusion forest (?3) and present related work in con-
sensus translations (?4). Experiments are presented
in Section 5 followed by discussion and our conclu-
sion.
2 Combination by Confusion Network
The system combination framework based on confu-
sion network starts from computing pairwise align-
ment between hypotheses by taking one hypothe-
sis as a reference. Matusov et al (2006) employs
a model based approach in which a statistical word
aligner, such as GIZA++ (Och and Ney, 2003), is
used to align the hypotheses. Sim et al (2007) in-
troduced TER (Snover et al, 2006) to measure the
edit-based alignment.
Then, one hypothesis is selected, for example by
employing a minimum Bayes risk criterion (Sim et
al., 2007), as a skeleton, or a backbone, which serves
as a building block for aligning the rest of the hy-
potheses. Other hypotheses are aligned against the
skeleton using the pairwise alignment. Figure 1(b)
illustrates an example of a confusion network con-
structed from the four hypotheses in Figure 1(a), as-
suming the first hypothesis is selected as our skele-
ton. The network consists of several arcs, each of
which represents an alternative word at that position,
including the empty symbol, ?.
This pairwise alignment strategy is prone to spu-
rious insertions and repetitions due to alignment er-
rors such as in Figure 1(a) in which ?green? in the
third hypothesis is aligned with ?forest? in the skele-
ton. Rosti et al (2008) introduces an incremental
method so that hypotheses are aligned incremen-
tally to the growing confusion network, not only the
.
.
..* ..I ..saw ..the . ..forest . .
. ..I ..walked ..the ..blue ..forest . .
. ..I ..saw ..the . ..green ..trees .
. . . ..the . ..forest ..was ..found
(a) Pairwise alignment using the first starred hypothesis as a
skeleton.
. . . . . . . .
.I
.?
.saw
.?
.walked
.the
.blue
.?
.forest
.green
.trees
.?
.was
.found
.?
(b) Confusion network from (a)
. . . . . . . .
.I
.?
.saw
.?
.walked
.the
.blue
.green
.forest
.trees
.was
.?
.found
.?
(c) Incrementally constructed confusion network
Figure 1: An example confusion network construc-
tion
skeleton hypothesis. In our example, ?green trees?
is aligned with ?blue forest? in Figure 1(c).
The confusion network construction is largely in-
fluenced by the skeleton selection, which determines
the global word reordering of a new hypothesis. For
example, the last hypothesis in Figure 1(a) has a pas-
sive voice grammatical construction while the others
are active voice. This large grammatical difference
may produce a longer sentence with spuriously in-
serted words, as in ?I saw the blue trees was found?
in Figure 1(c). Rosti et al (2007b) partially re-
solved the problem by constructing a large network
in which each hypothesis was treated as a skeleton
and the multiple networks were merged into a single
network.
3 Combination by Confusion Forest
The confusion network approach to system com-
bination encodes multiple hypotheses into a com-
pact lattice structure by using word-level consensus.
Likewise, we propose to encode multiple hypothe-
ses into a confusion forest, which is a packed forest
which represents multiple parse trees in a polyno-
mial space (Billot and Lang, 1989; Mi et al, 2008)
Syntactic consensus is realized by sharing tree frag-
1250
..
.PRP
. ..I .
..NP@1
..DT
..the .
..NN
..forest
.
..VBD@3
. ..was
.
..VP@4
..VBN
..found
.
..VBD@2.1
..walked . ..saw . ..NP
@2.2
.
..DT
..the .
..JJ
.. .blue . ..green
.
..NN
..forest . ..trees
.
..DT@2.2.1
. ..the .
..NN@2.2.2
. ..forest
. ..VP
@2
. ..S
@?
Figure 2: An example packed forest representing hy-
potheses in Figure 1(a).
ments among parse trees. The forest is represented
as a hypergraph which is exploited in parsing (Klein
and Manning, 2001; Huang and Chiang, 2005) and
machine translation (Chiang, 2007; Huang and Chi-
ang, 2007).
More formally, a hypergraph is a pair ?V,E?
where V is the set of nodes and E is the set of hy-
peredges. Each node in V is represented as X@p
where X ? N is a non-terminal symbol and p
is an address (Shieber et al, 1995) that encapsu-
lates each node id relative to its parent. The root
node is given the address ? and the address of the
first child of node p is given p.1. Each hyperedge
e ? E is represented as a pair ?head(e), tails(e)?
where head(e) ? V is a head node and tails(e) ?
V ? is a list of tail nodes, corresponding to the
left-hand side and the right-hand side of an in-
stance of a rule in a CFG, respectively. Figure 2
presents an example packed forest for the parsed
hypotheses in Figure 1(a). For example, VP@2
has two hyperedges, ?VP@2,
(
VBD@3,VP@4
)
? and
?VP@2,
(
VBD@2.1,NP@2.2
)
?, leading to different
derivations where the former takes the grammatical
construction in passive voice while the latter in ac-
tive voice.
Given system outputs, we employ the following
grammar based approach for constructing a confu-
sion forest: First, MT outputs are parsed. Second,
Initialization:
[TOP ? ?S, 0] : 1?
Scan:
[X ? ? ? x?, h] : u
[X ? ?x ? ?, h] : u
Predict:
[X ? ? ? Y?, h]
[Y ? ??, h + 1] : u
Y u? ? ? G, h < H
Complete:
[X ? ? ? Y?, h] : u [Y ? ??, h + 1] : v
[X ? ?Y ? ?, h] : u? v
Goal:
[TOP ? S?, 0]
Figure 3: The deductive system for Earley?s genera-
tion algorithm
a grammar is learned by treating each hyperedge as
an instance of a CFG rule. Third, a forest is gen-
erated from the unique root symbol of the extracted
grammar through non-terminal rewriting.
3.1 Forest Generation
Given the extracted grammar, we apply a variant of
Earley?s algorithm (Earley, 1970) which can gener-
ate strings in a left-to-right manner from the unique
root symbol, TOP. Figure 3 presents the deductive
inference rules (Goodman, 1999) for our generation
algorithm. We use capital letters X ? N to denote
non-terminals and x ? T for terminals. Lowercase
Greek letters ?, ? and ? are strings of terminals and
non-terminals (T ? N )?. u and v are weights asso-
ciated with each item.
The major difference compared to Earley?s pars-
ing algorithm is that we ignore the terminal span in-
formation each non-terminal covers and keep track
of the height of derivations by h. The scanning
step will always succeed by moving the dot to the
right. Combined with the prediction and completion
steps, our algorithm may potentially generate a spu-
riously deep forest. Thus, the height of the forest is
constrained in the prediction step not to exceed H ,
which is empirically set to 1.5 times the maximum
1251
height of the parsed system outputs.
3.2 Tree Annotation
The grammar compiled from the parsed trees is lo-
cal in that it can represent a finite number of sen-
tences translated from a specific input sentence. Al-
though its coverage is limited, our generation algo-
rithm may yield a spuriously large forest. As a way
to reduce spurious ambiguities, we relabel the non-
terminal symbols assigned to each parse tree before
extracting rules.
Here, we replace each non-terminal symbol by
the state representation of Earley?s algorithm corre-
sponding to the sequence of prediction steps starting
from TOP. Figure 4(a) presents an example parse
tree with each symbol replaced by the Earley?s state
in Figure 4(b). For example, the label for VBD is
replaced by ?S + NP : ?VP + ?VBD : NP which
corresponds to the prediction steps of TOP ? ?S,
S ? NP ? VP and VP ? ?VBD NP. The context
represented in the Earley?s state is further limited by
the vertical and horizontal Markovization (Klein and
Manning, 2003). We define the vertical order v in
which the label is limited to memorize only v pre-
vious prediction steps. For instance, setting v = 1
yields NP : ?VP + ?VBD : NP in our example.
Likewise, we introduce the horizontal order h which
limits the number of sibling labels memorized on the
left and the right of the dotted label. Limiting h = 1
implies that each deductive step is encoded with at
most three symbols.
No limits in the horizontal and vertical
Markovization orders implies memorizing of
all the deductions and yields a confusion forest
representing the union of parse trees through the
grammar collection and the generation processes.
More relaxed horizontal orders allow more reorder-
ing of subtrees in a confusion forest by discarding
the sibling context in each prediction step. Like-
wise, constraining the vertical order generates a
deeper forest by ignoring the sequence of symbols
leading to a particular node.
3.3 Forest Rescoring
From the packed forest F , new k-best derivations
are extracted from all possible derivations D by
efficient forest-based algorithms for k-best parsing
(Huang and Chiang, 2005). We use a linear combi-
.
.
.S
.
.
.NP
..PRP
. ..I
.
..VP
.
.
.VBD
. ..saw
.
..NP
..DT
..the .
..NN
..forest
(a) A parse tree for ?I saw the forest?
.
.
.?S
.
.
. ?S+ ? NP : VP
.
.
?S
+ ? NP : VP
+ ? PRP
.
.
.I
.
.
. ?S+NP : ?VP
.
.
.
?S
+NP : ?VP
+ ? VBD : NP
.
.
.saw
.
.
.
?S
+NP : ?VP
+VBD : ?NP
.
.
?S
+NP : ?VP
+VBD : ?NP
+ ? DT : NN
.
.
.the .
.
.
?S
+NP : ?VP
+VBD : ?NP
+DT : ?NN
.
.
.forest
(b) Earley?s state annotated tree for (a). The sub-labels in bold-
face indicate the original labels.
Figure 4: Label annotation by Earley?s alsogirhtm
state
nation of features as our objective function to seek
for the best derivation d?:
d? = argmax
d?D
w? ? h(d, F ) (1)
where h(d, F ) is a set of feature functions scaled
by weight vector w. We use cube-pruning (Chiang,
2007; Huang and Chiang, 2007) to approximately
intersect with non-local features, such as n-gram
language models. Then, k-best derivations are ex-
tracted from the rescored forest using algorithm 3 of
Huang and Chiang (2005).
4 Related Work
Consensus translations have been extensively stud-
ied with many granularities. One of the simplest
forms is a sentence-based combination in which
hypotheses are simply reranked without merging
(Nomoto, 2004). Frederking and Nirenburg (1994)
1252
proposed a phrasal combination by merging hy-
potheses in a chart structure, while others depended
on confusion networks, or similar structures, as a
building block for merging hypotheses at the word
level (Bangalore et al, 2001; Matusov et al, 2006;
He et al, 2008; Jayaraman and Lavie, 2005; Sim
et al, 2007). Our work is the first to explicitly ex-
ploit syntactic similarity for system combination by
merging hypotheses into a syntactic packed forest.
The confusion forest approach may suffer from pars-
ing errors such as the confusion network construc-
tion influenced by alignment errors. Even with pars-
ing errors, we can still take a tree fragment-level
consensus as long as a parser is consistent in that
similar syntactic mistakes would be made for simi-
lar hypotheses.
Rosti et al (2007a) describe a re-generation ap-
proach to consensus translation in which a phrasal
translation table is constructed from the MT outputs
aligned with an input source sentence. New transla-
tions are generated by decoding the source sentence
again using the newly extracted phrase table. Our
grammar-based approach can be regarded as a re-
generation approach in which an off-the-shelf mono-
lingual parser, instead of a word aligner, is used to
annotate syntactic information to each hypothesis,
then, a new translation is generated from the merged
forest, not from the input source sentence through
decoding. In terms of generation, our approach is
an instance of statistical generation (Langkilde and
Knight, 1998; Langkilde, 2000). Instead of gener-
ating forests from semantic representations (Langk-
ilde, 2000), we generate forests from a CFG encod-
ing the consensus among parsed hypotheses.
Liu et al (2009) present joint decoding in which
a translation forest is constructed from two distinct
MT systems, tree-to-string and string-to-string, by
merging forest outputs. Their merging method is ei-
ther translation-level in which no new translation is
generated, or derivation-level in that the rules shar-
ing the same left-hand-side are used in both sys-
tems. While our work is similar in that a new forest
is constructed by sharing rules among systems, al-
though their work involves no consensus translation
and requires structures internal to each system such
as model combinations (DeNero et al, 2010).
cz-en de-en es-en fr-en
# of systems 6 16 8 14
avg. words tune 10.6K 10.9K 10.9K 11.0K
test 50.5K 52.1K 52.1K 52.4K
sentences tune 455
test 2,034
Table 1: WMT10 system combination tuning/testing
data
5 Experiments
5.1 Setup
We ran our experiments for the WMT10 sys-
tem combination task usinge four language pairs,
{Czech, French, German, Spanish}-to-English
(Callison-Burch et al, 2010). The data is summa-
rized in Table 1. The system outputs are retok-
enized to match the Penn-treebank standard, parsed
by the Stanford Parser (Klein and Manning, 2003),
and lower-cased.
We implemented our confusion forest sys-
tem combination using an in-house developed
hypergraph-based toolkit cicada which is motivated
by generic weighted logic programming (Lopez,
2009), originally developed for a synchronous-CFG
based machine translation system (Chiang, 2007).
Input to our system is a collection of hypergraphs,
a set of parsed hypotheses, from which rules are ex-
tracted and a new forest is generated as described
in Section 3. Our baseline, also implemented in ci-
cada, is a confusion network-based system combi-
nation method (?2) which incrementally aligns hy-
potheses to the growing network using TER (Rosti
et al, 2008) and merges multiple networks into a
large single network. After performing epsilon re-
moval, the network is transformed into a forest by
parsing with monotone rules of S ? X, S ? S X
and X ? x. k-best translations are extracted from
the forest using the forest-based algorithms in Sec-
tion 3.3.
5.2 Features
The feature weight vector w in Equation 1 is tuned
by MERT over hypergraphs (Kumar et al, 2009).
We use three lower-cased 5-gram language mod-
1253
els hilm(d): English Gigaword Fourth edition1, the
English side of French-English 109 corpus and the
news commentary English data2. The count based
features ht(d) and he(d) count the number of ter-
minals and the number of hyperedges in d, respec-
tively. We employ M confidence measures hms (d)
for M systems, which basically count the number of
rules used in d originally extracted from mth system
hypothesis (Rosti et al, 2007a).
Following Macherey and Och (2007), BLEU (Pa-
pineni et al, 2002) correlations are also incorporated
in our system combination. GivenM system outputs
e1...eM , M BLEU scores are computed for d using
each of the system outputs em as a reference
hmb (d) = BP (e, em) ? exp
(
1
4
4
?
n=1
log ?n(e, em)
)
where e = yield(d) is a terminal yield of d, BP (?)
and ?n(?) respectively denote brevity penalty and
n-gram precision. Here, we use approximated un-
clipped n-gram counts (Dreyer et al, 2007) for com-
puting ?n(?) with a compact state representation (Li
and Khudanpur, 2009).
Our baseline confusion network system has an ad-
ditional penalty feature, hp(m), which is the total
edits required to construct a confusion network us-
ing themth system hypothesis as a skeleton, normal-
ized by the number of nodes in the network (Rosti et
al., 2007b).
5.3 Results
Table 2 compares our confusion forest approach
(CF) with different orders, a confusion network
(CN) and max/min systems measured by BLEU (Pa-
pineni et al, 2002). We vary the horizontal orders,
h = 1, 2,? with vertical orders of v = 3, 4,?.
Systems without statistically significant differences
from the best result (p < 0.05) are indicated by bold
face. Setting v = ? and h = ? achieves compa-
rable performance to CN. Our best results in three
languages come from setting v = ? and h = 2,
which favors little reordering of phrasal structures.
In general, lower horizontal and vertical order leads
to lower BLEU.
1LDC catalog No. LDC2009T13
2Those data are available from http://www.statmt.
org/wmt10/.
language cz-en de-en es-en fr-en
system min 14.09 15.62 21.79 16.79
max 23.44 24.10 29.97 29.17
CN 23.70 24.09 30.45 29.15
CFv=?,h=? 24.13 24.18 30.41 29.57
CFv=?,h=2 24.14 24.58 30.52 28.84
CFv=?,h=1 24.01 23.91 30.46 29.32
CFv=4,h=? 23.93 23.57 29.88 28.71
CFv=4,h=2 23.82 22.68 29.92 28.83
CFv=4,h=1 23.77 21.42 30.10 28.32
CFv=3,h=? 23.38 23.34 29.81 27.34
CFv=3,h=2 23.30 23.95 30.02 28.19
CFv=3,h=1 23.23 21.43 29.27 26.53
Table 2: Translation results in lower-case BLEU.
CN for confusion network and CF for confusion
forest with different vertical (v) and horizontal (h)
Markovization order.
language cz-en de-en es-en fr-en
rerank 29.40 32.32 36.83 36.59
CN 38.52 34.97 47.65 46.37
CFv=?,h=? 30.51 34.07 38.69 38.94
CFv=?,h=2 30.61 34.25 38.87 39.10
CFv=?,h=1 31.09 34.65 39.27 39.51
CFv=4,h=? 30.86 34.19 39.17 39.39
CFv=4,h=2 30.96 34.32 39.35 39.57
CFv=4,h=1 31.44 34.62 39.69 39.90
CFv=3,h=? 31.03 34.30 39.29 39.57
CFv=3,h=2 31.25 34.97 39.61 40.00
CFv=3,h=1 31.55 34.60 39.72 39.97
Table 3: Oracle lower-case BLEU
Table 3 presents oracle BLEU achievable by each
combination method. The gains achievable by the
CF over simple reranking are small, at most 2-3
points, indicating that small variations are encoded
in confusion forests. We also observed that a lower
horizontal and vertical order leads to better BLEU
potentials. As briefly pointed out in Section 3.2,
the higher horizontal and vertical order implies more
faithfulness to the original parse trees. Introducing
new tree fragments to confusion forests leads to new
phrasal translations with enlarged forests, as pre-
sented in Table 4, measured by the average number
1254
lang cz-en de-en es-en fr-en
CN 2,222.68 47,231.20 2,932.24 11,969.40
lattice 1,723.91 41,403.90 2,330.04 10,119.10
CFv=? 230.08 540.03 262.30 386.79
CFv=4 254.45 651.10 302.01 477.51
CFv=3 286.01 802.79 349.21 575.17
Table 4: Hypegraph size measured by the average
number of hyperedges (h = 1 for CF). ?lattice? is
the average number of edges in the original CN.
of hyperedges3. The larger potentials do not imply
better translations, probably due to the larger search
space with increased search errors. We also conjec-
ture that syntactic variations were not captured by
the n-gram like string-based features in Section 5.2,
therefore resulting in BLEU loss, which will be in-
vestigated in future work.
In contrast, CN has more potential for generat-
ing better translations, with the exception of the
German-to-English direction, with scores that are
usually 10 points better than simple sentence-wise
reranking. The low potential in German should be
interpreted in the light of the extremely large confu-
sion network in Table 4. We postulate that the di-
vergence in German hypotheses yields wrong align-
ments, and therefore amounts to larger networks
with incorrect hypotheses. Table 4 also shows that
CN produces a forest that is an order of magnitude
larger than those created by CFs. Although we can-
not directly relate the runtime and the number of
hyperedges in CN and CFs, since the shape of the
forests are different, CN requires more space to en-
code the hypotheses than those by CFs.
Table 5 compares the average length of the min-
imum/maximum hypothesis that each method can
produce. CN may generate shorter hypotheses,
whereby CF prefers longer hypotheses as we de-
crease the vertical order. Large divergence is also
observed for German, such as for hypergraph size.
6 Conclusion
We presented a confusion forest based method for
system combination in which system outputs are
merged into a packed forest using their syntactic
3We measure the hypergraph size before intersecting with
non-local features, like n-gram language models.
language cz-en de-en es-en fr-en
system avg. 24.84 25.62 25.63 25.75
CN min 11.09 3.39 12.27 7.94
max 33.69 40.65 33.22 36.27
CFv=? min 15.97 10.88 17.67 16.62
max 35.20 47.20 35.28 37.94
CFv=4 min 15.52 10.58 17.02 15.85
max 37.11 53.67 38.56 42.64
CFv=3 min 15.15 10.34 16.54 15.30
max 39.88 68.45 42.85 49.55
Table 5: Average min/max hypothesis length pro-
ducible by each method (h = 1 for CF).
similarity. The forest construction is treated as a
generation from a CFG compiled from the parsed
outputs. Our experiments indicate comparable per-
formance to a strong confusion network baseline
with smaller space, and statistically significant gains
in some language pairs.
To our knowledge, this is the first work to directly
introduce syntactic consensus to system combina-
tion by encoding multiple system outputs into a sin-
gle forest structure. We believe that the confusion
forest based approach to system combination has
future exploration potential. For instance, we did
not employ syntactic features in Section 5.2 which
would be helpful in discriminating hypotheses in
larger forests. We would also like to analyze the
trade-offs, if any, between parsing errors and confu-
sion forest constructions by controlling the parsing
qualities. As an alternative to the grammar-based
forest generation, we are investigating an edit dis-
tance measure for tree alignment, such as tree edit
distance (Bille, 2005) which basically computes in-
sertion/deletion/replacement of nodes in trees.
Acknowledgments
We would like to thank anonymous reviewers and
our colleagues for helpful comments and discussion.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceedings
of Automatic Speech Recognition and Understanding
(ASRU), 2001, pages 351 ? 354.
1255
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theor. Comput. Sci., 337:217?239,
June.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association for
Computational Linguistics, pages 143?151, Vancou-
ver, British Columbia, Canada, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Revised August
2010.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine trans-
lation. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
975?983, Los Angeles, California, June.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for smt us-
ing efficient bleu oracle computation. In Proceedings
of SSST, NAACL-HLT 2007 / AMTA Workshop on Syn-
tax and Structure in Statistical Translation, pages 103?
110, Rochester, New York, April.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13:94?102, February.
J.G. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting error
reduction (rover). In Proceedings of Automatic Speech
Recognition and Understanding (ASRU), 1997, pages
347 ?354, December.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the fourth
conference on Applied natural language processing,
pages 95?100, Morristown, NJ, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25:573?605, December.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-HMM-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 98?107, Honolulu, Hawaii,
October.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the Fourth Conference on
Empirical Methods in Natural Language Processing,
pages 187?194.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 53?64, Van-
couver, British Columbia, October.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proceedings of the ACL 2005 on In-
teractive poster and demonstration sessions, ACL ?05,
pages 101?104, Morristown, NJ, USA.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the Seventh In-
ternational Workshop on Parsing Technologies (IWPT-
2001), pages 123?134.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
- Volume 1, ACL-36, pages 704?710, Morristown, NJ,
USA.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 170?177, San Francisco, CA,
USA.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extrac-
tion of oracle-best translations from hypergraphs. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 9?12, Boul-
der, Colorado, June.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
1256
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 576?584, Suntec, Singapore, Au-
gust.
Adam Lopez. 2009. Translation as weighted deduction.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 532?
540, Athens, Greece, March.
Wolfgang Macherey and Franz J. Och. 2007. An empir-
ical study on computing consensus translations from
multiple machine translation systems. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
986?995, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. Computer Speech & Language, 14(4):373 ?
400.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 33?40.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June.
Tadashi Nomoto. 2004. Multi-engine machine transla-
tion with voted language model. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 494?501,
Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007a. Combining outputs from multiple machine
translation systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 228?
235, Rochester, New York, April.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319, Prague, Czech
Republic, June.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186, Columbus, Ohio,
June.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1?2):3?36, July?August.
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi, and P.C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
Proceedings of Acoustics, Speech and Signal Process-
ing (ICASSP), 2007, volume 4, pages IV?105 ?IV?
108, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
1257
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Machine Translation without Words through Substring Alignment
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
In this paper, we demonstrate that accu-
rate machine translation is possible without
the concept of ?words,? treating MT as a
problem of transformation between character
strings. We achieve this result by applying
phrasal inversion transduction grammar align-
ment techniques to character strings to train
a character-based translation model, and us-
ing this in the phrase-based MT framework.
We also propose a look-ahead parsing algo-
rithm and substring-informed prior probabil-
ities to achieve more effective and efficient
alignment. In an evaluation, we demonstrate
that character-based translation can achieve
results that compare to word-based systems
while effectively translating unknown and un-
common words over several language pairs.
1 Introduction
Traditionally, the task of statistical machine trans-
lation (SMT) is defined as translating a source sen-
tence fJ1 = {f1, . . . , fJ} to a target sentence eI1 =
{e1, . . ., eI}, where each element of fJ1 and eI1 is
assumed to be a word in the source and target lan-
guages. However, the definition of a ?word? is of-
ten problematic. The most obvious example of this
lies in languages that do not separate words with
white space such as Chinese, Japanese, or Thai, in
which the choice of a segmentation standard has
a large effect on translation accuracy (Chang et
al., 2008). Even for languages with explicit word
The first author is now affiliated with the Nara Institute of Sci-
ence and Technology.
boundaries, all machine translation systems perform
at least some precursory form of tokenization, split-
ting punctuation and words to prevent the sparsity
that would occur if punctuated and non-punctuated
words were treated as different entities. Sparsity
also manifests itself in other forms, including the
large vocabularies produced by morphological pro-
ductivity, word compounding, numbers, and proper
names. A myriad of methods have been proposed
to handle each of these phenomena individually,
including morphological analysis, stemming, com-
pound breaking, number regularization, optimizing
word segmentation, and transliteration, which we
outline in more detail in Section 2.
These difficulties occur because we are translat-
ing sequences of words as our basic unit. On the
other hand, Vilar et al (2007) examine the possibil-
ity of instead treating each sentence as sequences of
characters to be translated. This method is attrac-
tive, as it is theoretically able to handle all sparsity
phenomena in a single unified framework, but has
only been shown feasible between similar language
pairs such as Spanish-Catalan (Vilar et al, 2007),
Swedish-Norwegian (Tiedemann, 2009), and Thai-
Lao (Sornlertlamvanich et al, 2008), which have
a strong co-occurrence between single characters.
As Vilar et al (2007) state and we confirm, accu-
rate translations cannot be achieved when applying
traditional translation techniques to character-based
translation for less similar language pairs.
In this paper, we propose improvements to the
alignment process tailored to character-based ma-
chine translation, and demonstrate that it is, in fact,
possible to achieve translation accuracies that ap-
165
proach those of traditional word-based systems us-
ing only character strings. We draw upon recent
advances in many-to-many alignment, which allows
for the automatic choice of the length of units to
be aligned. As these units may be at the charac-
ter, subword, word, or multi-word phrase level, we
conjecture that this will allow for better character
alignments than one-to-many alignment techniques,
and will allow for better translation of uncommon
words than traditional word-based models by break-
ing down words into their component parts.
We also propose two improvements to the many-
to-many alignment method of Neubig et al (2011).
One barrier to applying many-to-many alignment
models to character strings is training cost. In the
inversion transduction grammar (ITG) framework
(Wu, 1997), which is widely used in many-to-many
alignment, search is cumbersome for longer sen-
tences, a problem that is further exacerbated when
using characters instead of words as the basic unit.
As a step towards overcoming this difficulty, we in-
crease the efficiency of the beam-search technique of
Saers et al (2009) by augmenting it with look-ahead
probabilities in the spirit of A* search. Secondly,
we describe a method to seed the search process us-
ing counts of all substring pairs in the corpus to bias
the phrase alignment model. We do this by defining
prior probabilities based on these substring counts
within the Bayesian phrasal ITG framework.
An evaluation on four language pairs with differ-
ing morphological properties shows that for distant
language pairs, character-based SMT can achieve
translation accuracy comparable to word-based sys-
tems. In addition, we perform ablation studies,
showing that these results were not possible with-
out the proposed enhancements to the model. Fi-
nally, we perform a qualitative analysis, which finds
that character-based translation can handle unseg-
mented text, conjugation, and proper names in a uni-
fied framework with no additional processing.
2 Related Work on Data Sparsity in SMT
As traditional SMT systems treat all words as single
tokens without considering their internal structure,
major problems of data sparsity occur for less fre-
quent tokens. In fact, it has been shown that there
is a direct negative correlation between vocabulary
size (and thus sparsity) of a language and transla-
tion accuracy (Koehn, 2005). Sparsity causes trou-
ble for alignment models, both in the form of incor-
rectly aligned uncommon words, and in the form of
garbage collection, where uncommon words in one
language are incorrectly aligned to large segments
of the sentence in the other language (Och and Ney,
2003). Unknown words are also a problem during
the translation process, and the default approach is
to map them as-is into the target sentence.
This is a major problem in agglutinative lan-
guages such as Finnish or compounding languages
such as German. Previous works have attempted to
handle morphology, decompounding and regulariza-
tion through lemmatization, morphological analysis,
or unsupervised techniques (Nie?en and Ney, 2000;
Brown, 2002; Lee, 2004; Goldwater and McClosky,
2005; Talbot and Osborne, 2006; Mermer and Ak?n,
2010; Macherey et al, 2011). It has also been noted
that it is more difficult to translate into morpho-
logically rich languages, and methods for modeling
target-side morphology have attracted interest in re-
cent years (Bojar, 2007; Subotin, 2011).
Another source of data sparsity that occurs in all
languages is proper names, which have been handled
by using cognates or transliteration to improve trans-
lation (Knight and Graehl, 1998; Kondrak et al,
2003; Finch and Sumita, 2007), and more sophisti-
cated methods for named entity translation that com-
bine translation and transliteration have also been
proposed (Al-Onaizan and Knight, 2002).
Choosing word units is also essential for creat-
ing good translation results for languages that do
not explicitly mark word boundaries, such as Chi-
nese, Japanese, and Thai. A number of works have
dealt with this word segmentation problem in trans-
lation, mainly focusing on Chinese-to-English trans-
lation (Bai et al, 2008; Chang et al, 2008; Zhang et
al., 2008b; Chung and Gildea, 2009; Nguyen et al,
2010), although these works generally assume that a
word segmentation exists in one language (English)
and attempt to optimize the word segmentation in
the other language (Chinese).
We have enumerated these related works to
demonstrate the myriad of data sparsity problems
and proposed solutions. Character-based transla-
tion has the potential to handle all of the phenom-
ena in the previously mentioned research in a single
166
unified framework, requiring no language specific
tools such as morphological analyzers or word seg-
menters. However, while the approach is attractive
conceptually, previous research has only been shown
effective for closely related language pairs (Vilar et
al., 2007; Tiedemann, 2009; Sornlertlamvanich et
al., 2008). In this work, we propose effective align-
ment techniques that allow character-based transla-
tion to achieve accurate translation results for both
close and distant language pairs.
3 Alignment Methods
SMT systems are generally constructed from a par-
allel corpus consisting of target language sentences
E and source language sentences F . The first step
of training is to find alignments A for the words in
each sentence pair.
We represent our target and source sentences as
eI1 and fJ1 . ei and fj represent single elements of
the target and source sentences respectively. These
may be words in word-based alignment models or
single characters in character-based alignment mod-
els.1 We define our alignment as aK1 , where each
element is a span ak = ?s, t, u, v? indicating that the
target string es, . . . , et and source string fu, . . . , fv
are aligned to each-other.
3.1 One-to-Many Alignment
The most well-known and widely-used models for
bitext alignment are for one-to-many alignment, in-
cluding the IBM models (Brown et al, 1993) and
HMM alignment model (Vogel et al, 1996). These
models are by nature directional, attempting to find
the alignments that maximize the conditional prob-
ability of the target sentence P (eI1|fJ1 ,aK1 ). For
computational reasons, the IBM models are re-
stricted to aligning each word on the target side to
a single word on the source side. In the formal-
ism presented above, this means that each ei must
be included in at most one span, and for each span
u = v. Traditionally, these models are run in both
directions and combined using heuristics to create
many-to-many alignments (Koehn et al, 2003).
However, in order for one-to-many alignment
methods to be effective, each fj must contain
1Some previous work has also performed alignment using
morphological analyzers to normalize or split the sentence into
morpheme streams (Corston-Oliver and Gamon, 2004).
enough information to allow for effective alignment
with its corresponding elements in eI1. While this is
often the case in word-based models, for character-
based models this assumption breaks down, as there
is often no clear correspondence between characters.
3.2 Many-to-Many Alignment
On the other hand, in recent years, there have been
advances in many-to-many alignment techniques
that are able to align multi-element chunks on both
sides of the translation (Marcu and Wong, 2002;
DeNero et al, 2008; Blunsom et al, 2009; Neu-
big et al, 2011). Many-to-many methods can be ex-
pected to achieve superior results on character-based
alignment, as the aligner can use information about
substrings, which may correspond to letters, mor-
phemes, words, or short phrases.
Here, we focus on the model presented by Neu-
big et al (2011), which uses Bayesian inference in
the phrasal inversion transduction grammar (ITG,
Wu (1997)) framework. ITGs are a variety of syn-
chronous context free grammar (SCFG) that allows
for many-to-many alignment to be achieved in poly-
nomial time through the process of biparsing, which
we explain more in the following section. Phrasal
ITGs are ITGs that allow for non-terminals that can
emit phrase pairs with multiple elements on both
the source and target sides. It should be noted
that there are other many-to-many alignment meth-
ods that have been used for simultaneously discov-
ering morphological boundaries over multiple lan-
guages (Snyder and Barzilay, 2008; Naradowsky
and Toutanova, 2011), but these have generally been
applied to single words or short phrases, and it is not
immediately clear that they will scale to aligning full
sentences.
4 Look-Ahead Biparsing
In this work, we experiment with the alignment
method of Neubig et al (2011), which can achieve
competitive accuracy with a much smaller phrase ta-
ble than traditional methods. This is important in
the character-based translation context, as we would
like to use phrases that contain large numbers of
characters without creating a phrase table so large
that it cannot be used in actual decoding. In this
framework, training is performed using sentence-
167
Figure 1: (a) A chart with inside probabilities in boxes
and forward/backward probabilities marking the sur-
rounding arrows. (b) Spans with corresponding look-
aheads added, and the minimum probability underlined.
Lightly and darkly shaded spans will be trimmed when
the beam is log(P ) ? ?3 and log(P ) ? ?6 respectively.
wise block sampling, acquiring a sample for each
sentence by first performing bottom-up biparsing to
create a chart of probabilities, then performing top-
down sampling of a new tree based on the probabil-
ities in this chart.
An example of a chart used in this parsing can
be found in Figure 1 (a). Within each cell of the
chart spanning ets and fvu is an ?inside? probabil-
ity I(as,t,u,v). This probability is the combination
of the generative probability of each phrase pair
Pt(ets,fvu) as well as the sum the probabilities over
all shorter spans in straight and inverted order2
I(as,t,u,v) = Pt(ets, fvu)
+
?
s?S?t
?
u?U?v
Px(str)I(as,S,u,U )I(aS,t,U,v)
+
?
s?S?t
?
u?U?v
Px(inv)I(as,S,U,v)I(aS,t,u,U )
where Px(str) and Px(inv) are the probability of
straight and inverted ITG productions.
While the exact calculation of these probabilities
can be performed in O(n6) time, where n is the
2Pt can be specified according to Bayesian statistics as de-
scribed by Neubig et al (2011).
length of the sentence, this is impractical for all but
the shortest sentences. Thus it is necessary to use
methods to reduce the search space such as beam-
search based chart parsing (Saers et al, 2009) or
slice sampling (Blunsom and Cohn, 2010).3
In this section we propose the use of a look-ahead
probability to increase the efficiency of this chart
parsing. Taking the example of Saers et al (2009),
spans are pushed onto a different queue based on
their size, and queues are processed in ascending or-
der of size. Agendas can further be trimmed based
on a histogram beam (Saers et al, 2009) or probabil-
ity beam (Neubig et al, 2011) compared to the best
hypothesis a?. In other words, we have a queue dis-
cipline based on the inside probability, and all spans
ak where I(ak) < cI(a?) are pruned. c is a constant
describing the width of the beam, and a smaller con-
stant probability will indicate a wider beam.
This method is insensitive to the existence of
competing hypotheses when performing pruning.
Figure 1 (a) provides an example of why it is unwise
to ignore competing hypotheses during beam prun-
ing. Particularly, the alignment ?les/1960s? com-
petes with the high-probability alignment ?les/the,?
so intuitively should be a good candidate for prun-
ing. However its probability is only slightly higher
than ?anne?es/1960s,? which has no competing hy-
potheses and thus should not be trimmed.
In order to take into account competing hypothe-
ses, we can use for our queue discipline not only the
inside probability I(ak), but also the outside proba-
bility O(ak), the probability of generating all spans
other than ak, as in A* search for CFGs (Klein and
Manning, 2003), and tic-tac-toe pruning for word-
based ITGs (Zhang and Gildea, 2005). As the cal-
culation of the actual outside probability O(ak) is
just as expensive as parsing itself, it is necessary to
approximate this with heuristic function O? that can
be calculated efficiently.
Here we propose a heuristic function that is de-
signed specifically for phrasal ITGs and is com-
putable with worst-case complexity of n2, compared
with the n3 amortized time of the tic-tac-toe pruning
3Applying beam-search before sampling will sample from
an improper distribution, although Metropolis-in-Gibbs sam-
pling (Johnson et al, 2007) can be used to compensate. How-
ever, we found that this had no significant effect on results, so
we omit the Metropolis-in-Gibbs step for experiments.
168
algorithm described by (Zhang et al, 2008a). Dur-
ing the calculation of the phrase generation proba-
bilities Pt, we save the best inside probability I? for
each monolingual span.
I?e (s, t) = max
{a?=?s?,t?,u?,v??;s?=s,t?=t}
Pt(a?)
I?f (u, v) = max
{a?=?s?,t?,u?,v??;u?=u,v?=v}
Pt(a?)
For each language independently, we calculate for-
ward probabilities ? and backward probabilities ?.
For example, ?e(s) is the maximum probability of
the span (0, s) of e that can be created by concate-
nating together consecutive values of I?e :
?e(s) = max
{S1,...,Sx}
I?e (0, S1)I?e (S1, S2) . . . I?e (Sx, s).
Backwards probabilities and probabilities over f can
be defined similarly. These probabilities are calcu-
lated for e and f independently, and can be calcu-
lated in n2 time by processing each ? in ascending
order, and each ? in descending order in a fashion
similar to that of the forward-backward algorithm.
Finally, for any span, we define the outside heuristic
as the minimum of the two independent look-ahead
probabilities over each language
O?(as,t,u,v) = min(?e(s) ? ?e(t), ?f (u) ? ?f (v)).
Looking again at Figure 1 (b), it can be seen
that the relative probability difference between the
highest probability span ?les/the? and the spans
?anne?es/1960s? and ?60/1960s? decreases, allowing
for tighter beam pruning without losing these good
hypotheses. In contrast, the relative probability of
?les/1960s? remains low as it is in conflict with a
high-probability alignment, allowing it to be dis-
carded.
5 Substring Prior Probabilities
While the Bayesian phrasal ITG framework uses
the previously mentioned phrase distribution Pt dur-
ing search, it also allows for definition of a phrase
pair prior probability Pprior(ets,fvu), which can ef-
ficiently seed the search process with a bias towards
phrase pairs that satisfy certain properties. In this
section, we overview an existing method used to cal-
culate these prior probabilities, and also propose a
new way to calculate priors based on substring co-
occurrence statistics.
5.1 Word-based Priors
Previous research on many-to-many translation has
used IBM model 1 probabilities to bias phrasal
alignments so that phrases whose member words are
good translations are also aligned. As a representa-
tive of this existing method, we adopt a base mea-
sure similar to that used by DeNero et al (2008):
Pm1(e,f) =M0(e,f)Ppois(|e|;?)Ppois(|f |;?)
M0(e,f) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?, which we set to 0.01. Pm1 is the
word-based (or character-based) Model 1 probabil-
ity, which can be efficiently calculated using the dy-
namic programming algorithm described by Brown
et al (1993). However, for reasons previously stated
in Section 3, these methods are less satisfactory
when performing character-based alignment, as the
amount of information contained in a character does
not allow for proper alignment.
5.2 Substring Co-occurrence Priors
Instead, we propose a method for using raw sub-
string co-occurrence statistics to bias alignments to-
wards substrings that often co-occur in the entire
training corpus. This is similar to the method of
Cromieres (2006), but instead of using these co-
occurrence statistics as a heuristic alignment crite-
rion, we incorporate them as a prior probability in
a statistical model that can take into account mutual
exclusivity of overlapping substrings in a sentence.
We define this prior probability using three counts
over substrings c(e), c(f), and c(e,f). c(e) and
c(f) count the total number of sentences in which
the substrings e and f occur respectively. c(e,f) is
a count of the total number of sentences in which the
substring e occurs on the target side, and f occurs
on the source side. We perform the calculation of
these statistics using enhanced suffix arrays, a data
structure that can efficiently calculate all substrings
in a corpus (Abouelhoda et al, 2004).4
While suffix arrays allow for efficient calculation
of these statistics, storing all co-occurrence counts
c(e,f) is an unrealistic memory burden for larger
4Using the open-source implementation esaxx http://
code.google.com/p/esaxx/
169
corpora. In order to reduce the amount of mem-
ory used, we discount every count by a constant d,
which we set to 5. This has a dual effect of reducing
the amount of memory needed to hold co-occurrence
counts by removing values for which c(e,f) < d, as
well as preventing over-fitting of the training data. In
addition, we heuristically prune values for which the
conditional probabilities P (e|f) or P (f |e) are less
than some fixed value, which we set to 0.1 for the
reported experiments.
To determine how to combine c(e), c(f), and
c(e,f) into prior probabilities, we performed pre-
liminary experiments testing methods proposed by
previous research including plain co-occurrence
counts, the Dice coefficient, and ?-squared statistics
(Cromieres, 2006), as well as a newmethod of defin-
ing substring pair probabilities to be proportional to
bidirectional conditional probabilities
Pcooc(e,f) = Pcooc(e|f)Pcooc(f |e)/Z
=
(
c(e,f) ? d
c(f) ? d
)(
c(e,f) ? d
c(e) ? d
)
/Z
for all substring pairs where c(e,f) > d and where
Z is a normalization term equal to
Z =
?
{e,f ;c(e,f)>d}
Pcooc(e|f)Pcooc(f |e).
The experiments showed that the bidirectional con-
ditional probability method gave significantly better
results than all other methods, so we adopt this for
the remainder of our experiments.
It should be noted that as we are using discount-
ing, many substring pairs will be given zero proba-
bility according to Pcooc. As the prior is only sup-
posed to bias the model towards good solutions and
not explicitly rule out any possibilities, we linearly
interpolate the co-occurrence probability with the
one-to-many Model 1 probability, which will give
at least some probability mass to all substring pairs
Pprior(e,f) = ?Pcooc(e,f) + (1 ? ?)Pm1(e,f).
We put a Dirichlet prior (? = 1) on the interpolation
coefficient ? and learn it during training.
6 Experiments
In order to test the effectiveness of character-based
translation, we performed experiments over a variety
of language pairs and experimental settings.
de-en fi-en fr-en ja-en
TM (en) 2.80M 3.10M 2.77M 2.13M
TM (other) 2.56M 2.23M 3.05M 2.34M
LM (en) 16.0M 15.5M 13.8M 11.5M
LM (other) 15.3M 11.3M 15.6M 11.9M
Tune (en) 58.7k 58.7k 58.7k 30.8k
Tune (other) 55.1k 42.0k 67.3k 34.4k
Test (en) 58.0k 58.0k 58.0k 26.6k
Test (other) 54.3k 41.4k 66.2k 28.5k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
6.1 Experimental Setup
We use a combination of four languages with En-
glish, using freely available data. We selected
French-English, German-English, Finnish-English
data from EuroParl (Koehn, 2005), with develop-
ment and test sets designated for the 2005 ACL
shared task on machine translation.5 We also did
experiments with Japanese-English Wikipedia arti-
cles from the Kyoto Free Translation Task (Neu-
big, 2011) using the designated training and tuning
sets, and reporting results on the test set. These lan-
guages were chosen as they have a variety of inter-
esting characteristics. French has some inflection,
but among the test languages has the strongest one-
to-one correspondence with English, and is gener-
ally considered easy to translate. German has many
compound words, which must be broken apart to
translate properly into English. Finnish is an ag-
glutinative language with extremely rich morphol-
ogy, resulting in long words and the largest vocab-
ulary of the languages in EuroParl. Japanese does
not have any clear word boundaries, and uses logo-
graphic characters, which contain more information
than phonetic characters.
With regards to data preparation, the EuroParl
data was pre-tokenized, so we simply used the to-
kenized data as-is for the training and evaluation of
all models. For word-based translation in the Kyoto
task, training was performed using the provided tok-
enization scripts. For character-based translation, no
tokenization was performed, using the original text
for both training and decoding. For both tasks, we
selected as training data all sentences for which both
5http://statmt.org/wpt05/mt-shared-task
170
de-en fi-en fr-en ja-en
GIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70
ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89
GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34
ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58
en-de en-fi en-fr en-ja
GIZA-word 17.94 / 62.71 / 37.88 13.22 / 58.50 / 27.03 32.19 / 69.20 / 52.39 20.79 / 27.01 / 38.41
ITG-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34
GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67
ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71
Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal
ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ-
ence from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004).
source and target were 100 characters or less,6 the
total size of which is shown in Table 1. In character-
based translation, white spaces between words were
treated as any other character and not given any spe-
cial treatment. Evaluation was performed on tok-
enized and lower-cased data.
For alignment, we use the GIZA++ implementa-
tion of one-to-many alignment7 and the pialign im-
plementation of the phrasal ITG models8 modified
with the proposed improvements. For GIZA++, we
used the default settings for word-based alignment,
but used the HMM model for character-based align-
ment to allow for alignment of longer sentences.
For pialign, default settings were used except for
character-based ITG alignment, which used a prob-
ability beam of 10?4 instead 10?10.9 For decoding,
we use the Moses decoder,10 using the default set-
tings except for the stack size, which we set to 1000
instead of 200. Minimum error rate training was per-
formed to maximize word-based BLEU score for all
systems.11 For language models, word-based trans-
lation uses a word 5-gram model, and character-
based translation uses a character 12-gram model,
both smoothed using interpolated Kneser-Ney.
6100 characters is an average of 18.8 English words
7http://code.google.com/p/giza-pp/
8http://phontron.com/pialign/
9Improvement by using a beam larger than 10?4 was
marginal, especially with co-occurrence prior probabilities.
10http://statmt.org/moses/
11We chose this set-up to minimize the effect of tuning crite-
rion on our experiments, although it does indicate that we must
have access to tokenized data for the development set.
6.2 Quantitative Evaluation
Table 2 presents a quantitative analysis of the trans-
lation results for each of the proposed methods. As
previous research has shown that it is more diffi-
cult to translate into morphologically rich languages
than into English (Koehn, 2005), we perform exper-
iments translating in both directions for all language
pairs. We evaluate translation quality using BLEU
score (Papineni et al, 2002), both on the word and
character level (with n = 4), as well as METEOR
(Denkowski and Lavie, 2011) on the word level.
It can be seen that character-based translation
with all of the proposed alignment improvements
greatly exceeds character-based translation using
one-to-many alignment, confirming that substring-
based information is necessary for accurate align-
ments. When compared with word-based trans-
lation, character-based translation achieves better,
comparable, or inferior results on character-based
BLEU, comparable or inferior results on METEOR,
and inferior results on word-based BLEU. The dif-
ferences between the evaluation metrics are due to
the fact that character-based translation often gets
words mostly correct other than one or two letters.
These are given partial credit by character-based
BLEU (and to a lesser extent METEOR), but marked
entirely wrong by word-based BLEU.
Interestingly, for translation into English,
character-based translation achieves higher ac-
curacy compared to word-based translation on
Japanese and Finnish input, followed by German,
171
fi-en ja-en
ITG-word 2.851 2.085
ITG-char 2.826 2.154
Table 3: Human evaluation scores (0-5 scale).
Ref: directive on equality
Source Unk. Word: tasa-arvodirektiivi
(13/26) Char: equality directive
Ref: yoshiwara-juku station
Target Unk. Word: yoshiwara no eki
(5/26) Char: yoshiwara-juku station
Ref: world health organisation
Uncommon Word: world health
(5/26) Char: world health organisation
Table 4: The major gains of character-based translation,
unknown, hyphenated, and uncommon words.
and finally French. This confirms that character-
based translation is performing well on languages
that have long words or ambiguous boundaries, and
less well on language pairs with relatively strong
one-to-one correspondence between words.
6.3 Qualitative Evaluation
In addition, we performed a subjective evaluation of
Japanese-English and Finnish-English translations.
Two raters evaluated 100 sentences each, assigning
a score of 0-5 based on how well the translation con-
veys the information contained in the reference. We
focus on shorter sentences of 8-16 English words to
ease rating and interpretation. Table 3 shows that
the results are comparable, with no significant dif-
ference in average scores for either language pair.
Table 4 shows a breakdown of the sentences for
which character-based translation received a score
of at 2+ points more than word-based. It can be seen
that character-based translation is properly handling
sparsity phenomena. On the other hand, word-based
translation was generally stronger with reordering
and lexical choice of more common words.
6.4 Effect of Alignment Method
In this section, we compare the translation accura-
cies for character-based translation using the phrasal
ITG model with and without the proposed improve-
ments of substring co-occurrence priors and look-
ahead parsing as described in Sections 4 and 5.2.
fi-en en-fi ja-en en-ja
ITG +cooc +look 28.94 25.31 24.58 35.71
ITG +cooc -look 28.51 24.24 24.32 35.74
ITG -cooc +look 28.65 24.49 24.36 35.05
ITG -cooc -look 27.45 23.30 23.57 34.50
Table 5: METEOR scores for alignment with and without
look-ahead and co-occurrence priors.
Figure 5 shows METEOR scores12 for experi-
ments translating Japanese and Finnish. It can be
seen that the co-occurrence prior gives gains in all
cases, indicating that substring statistics are effec-
tively seeding the ITG aligner. The introduced look-
ahead probabilities improve accuracy significantly
when substring co-occurrence counts are not used,
and slightly when co-occurrence counts are used.
More importantly, they allow for more aggressive
beam pruning, increasing sampling speed from 1.3
sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6
sent/s for Japanese.
7 Conclusion and Future Directions
This paper demonstrated that character-based trans-
lation can act as a unified framework for handling
difficult problems in translation: morphology, com-
pound words, transliteration, and segmentation.
One future challenge includes scaling training up
to longer sentences, which can likely be achieved
through methods such as the heuristic span prun-
ing of Haghighi et al (2009) or sentence splitting
of Vilar et al (2007). Monolingual data could also
be used to improve estimates of our substring-based
prior. In addition, error analysis showed that word-
based translation performed better than character-
based translation on reordering and lexical choice,
indicating that improved decoding (or pre-ordering)
and language modeling tailored to character-based
translation will likely greatly improve accuracy. Fi-
nally, we plan to explore the middle ground between
word-based and character based translation, allow-
ing for the flexibility of character-based translation,
while using word boundary information to increase
efficiency and accuracy.
12Similar results were found for character and word-based
BLEU, but are omitted for lack of space.
172
References
Mohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohle-
busch. 2004. Replacing suffix trees with enhanced
suffix arrays. Journal of Discrete Algorithms, 2(1).
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. ACL.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proc. IJCNLP.
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proc.
HLT-NAACL, pages 238?241.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proc. ACL.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proc. WMT.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proc. TMI.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
WMT.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004. Nor-
malizing German and English inflectional morphology
to improve statistical word alignment. Machine Trans-
lation: From Real Users to Research.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proc. COL-
ING/ACL 2006 Student Research Workshop.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Proc.
WMT.
Andrew Finch and Eiichiro Sumita. 2007. Phrase-based
machine transliteration. In Proc. TCAST.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proc. EMNLP.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proc. ACL.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proc. NAACL.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: fast exact Viterbi parse selection. In Proc. HLT.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. HLT,
pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proc. HLT.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. HLT.
Klaus Macherey, Andrew Dai, David Talbot, Ashok
Popat, and Franz Och. 2011. Language-independent
compound splitting with morphological operations. In
Proc. ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsu-
pervised search for the optimal segmentation for sta-
tistical machine translation. In Proc. ACL Student Re-
search Workshop.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proc. ACL.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL, pages 632?641, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation task.
http://www.phontron.com/kftt.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. COLING.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Proc. COL-
ING.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. COLING.
173
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proc. IWPT, pages 29?32.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. Proc. ACL.
Virach Sornlertlamvanich, Chumpol Mokarat, and Hi-
toshi Isahara. 2008. Thai-lao machine translation
based on phoneme transfer. In Proc. 14th Annual
Meeting of the Association for Natural Language Pro-
cessing.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
David Talbot and Miles Osborne. 2006. Modelling lexi-
cal redundancy for machine translation. In Proc. ACL.
Jo?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proc. 13th Annual
Conference of the European Association for Machine
Translation.
David Vilar, Jan-T. Peter, and Hermann Ney. 2007. Can
we translate letters. In Proc. WMT.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proc. ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of
non-compositional phrases with synchronous parsing.
Proc. ACL.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proc. WMT.
174
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 657?665,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-driven Transition-based Parsing with Top-down Prediction
Katsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Institute of Science and Technology
Ikoma, Nara, 630-0192, Japan
?National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
?National Institute for Japanese Language and Linguistics
Tachikawa, Tokyo, 190-8561, Japan
katsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jp
masayu-a@ninjal.ac.jp, matsu@is.naist.jp
Abstract
This paper presents a novel top-down head-
driven parsing algorithm for data-driven pro-
jective dependency analysis. This algorithm
handles global structures, such as clause and
coordination, better than shift-reduce or other
bottom-up algorithms. Experiments on the
English Penn Treebank data and the Chinese
CoNLL-06 data show that the proposed algo-
rithm achieves comparable results with other
data-driven dependency parsing algorithms.
1 Introduction
Transition-based parsing algorithms, such as shift-
reduce algorithms (Nivre, 2004; Zhang and Clark,
2008), are widely used for dependency analysis be-
cause of the efficiency and comparatively good per-
formance. However, these parsers have one major
problem that they can handle only local information.
Isozaki et al (2004) pointed out that the drawbacks
of shift-reduce parser could be resolved by incorpo-
rating top-down information such as root finding.
This work presents an O(n2) top-down head-
driven transition-based parsing algorithm which can
parse complex structures that are not trivial for shift-
reduce parsers. The deductive system is very similar
to Earley parsing (Earley, 1970). The Earley predic-
tion is tied to a particular grammar rule, but the pro-
posed algorithm is data-driven, following the current
trends of dependency parsing (Nivre, 2006; McDon-
ald and Pereira, 2006; Koo et al, 2010). To do the
prediction without any grammar rules, we introduce
a weighted prediction that is to predict lower nodes
from higher nodes with a statistical model.
To improve parsing flexibility in deterministic
parsing, our top-down parser uses beam search al-
gorithm with dynamic programming (Huang and
Sagae, 2010). The complexity becomes O(n2 ? b)
where b is the beam size. To reduce prediction er-
rors, we propose a lookahead technique based on a
FIRST function, inspired by the LL(1) parser (Aho
and Ullman, 1972). Experimental results show that
the proposed top-down parser achieves competitive
results with other data-driven parsing algorithms.
2 Definition of Dependency Graph
A dependency graph is defined as follows.
Definition 2.1 (Dependency Graph) Given an in-
put sentence W = n0 . . .nn where n0 is a spe-
cial root node $, a directed graph is defined as
GW = (VW , AW ) where VW = {0, 1, . . . , n} is a
set of (indices of) nodes and AW ? VW ? VW is a
set of directed arcs. The set of arcs is a set of pairs
(x, y) where x is a head and y is a dependent of x.
x ?? l denotes a path from x to l. A directed graph
GW = (VW , AW ) is well-formed if and only if:
? There is no node x such that (x, 0) ? AW .
? If (x, y) ? AW then there is no node x? such
that (x?, y) ? AW and x? ?= x.
? There is no subset of arcs {(x0, x1), (x1, x2),
. . . , (xl?1, xl)} ? AW such that x0 = xl.
These conditions are refered to ROOT, SINGLE-
HEAD, and ACYCLICITY, and we call an well-
formed directed graph as a dependency graph.
Definition 2.2 (PROJECTIVITY) A dependency
graph GW = (VW , AW ) is projective if and only if,
657
input: W = n0 . . .nn
axiom(p0): 0 : ?1, 0, n + 1,n0? : ?
predx:
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, h, sd?1|...|s0|nk? : {p}
?k : i ? k < h
predy:
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, j, sd?1|...|s0|nk? : {p}
?k : i ? k < j ? h < i
scan:
? : ?i, h, j, sd|...|s0? : pi
? + 1 : ?i + 1, h, j, sd|...|s0? : pi
i = h
comp:
state q
? ?? ?
: ? , h?, j?, s?d|...|s?0? : pi?
state p
? ?? ?
? : ?i, h, j, sd|...|s0? : pi
? + 1 : ?i, h?, j?, s?d|...|s?1|s?0ys0? : pi?
q ? pi, h < i
goal: 3n : ?n + 1, 0, n + 1, s0? : ?
Figure 1: The non-weighted deductive system of top-down dependency parsing algorithm: means ?take anything?.
for every arc (x, y) ? AW and node l in x < l < y
or y < l < x, there is a path x ?? l or y ?? l.
The proposed algorithm in this paper is for projec-
tive dependency graphs. If a projective dependency
graph is connected, we call it a dependency tree,
and if not, a dependency forest.
3 Top-down Parsing Algorithm
Our proposed algorithm is a transition-based algo-
rithm, which uses stack and queue data structures.
This algorithm formally uses the following state:
? : ?i, h, j, S? : pi
where ? is a step size, S is a stack of trees sd|...|s0
where s0 is a top tree and d is a window size for
feature extraction, i is an index of node on the top
of the input node queue, h is an index of root node
of s0, j is an index to indicate the right limit (j ?
1 inclusive) of predy, and pi is a set of pointers to
predictor states, which are states just before putting
the node in h onto stack S. In the deterministic case,
pi is a singleton set except for the initial state.
This algorithm has four actions, predictx(predx),
predicty(predy), scan and complete(comp). The
deductive system of the top-down algorithm is
shown in Figure 1. The initial state p0 is a state ini-
tialized by an artificial root node n0. This algorithm
applies one action to each state selected from appli-
cable actions in each step. Each of three kinds of
actions, pred, scan, and comp, occurs n times, and
this system takes 3n steps for a complete analysis.
Action predx puts nk onto stack S selected from
the input queue in the range, i ? k < h, which is
to the left of the root nh in the stack top. Similarly,
action predy puts a node nk onto stack S selected
from the input queue in the range, h < i ? k < j,
which is to the right of the root nh in the stack top.
The node ni on the top of the queue is scanned if it
is equal to the root node nh in the stack top. Action
comp creates a directed arc (h?, h) from the root h?
of s?0 on a predictor state q to the root h of s0 on a
current state p if h < i 1.
The precondition i < h of action predx means
that the input nodes in i ? k < h have not been
predicted yet. Predx, scan and predy do not con-
flict with each other since their preconditions i < h,
i = h and h < i do not hold at the same time.
However, this algorithm faces a predy-comp con-
flict because both actions share the same precondi-
tion h < i, which means that the input nodes in
1 ? k ? h have been predicted and scanned. This
1In a single root tree, the special root symbol $0 has exactly
one child node. Therefore, we do not apply comp action to a
state if its condition satisfies s1.h = n0 ? ? ?= 3n? 1.
658
step state stack queue action state information
0 p0 $0 I1 saw2 a3 girl4 ? ?1, 0, 5? : ?
1 p1 $0|saw2 I1 saw2 a3 girl4 predy ?1, 2, 5? : {p0}
2 p2 saw2|I1 I1 saw2 a3 girl4 predx ?1, 1, 2? : {p1}
3 p3 saw2|I1 saw2 a3 girl4 scan ?2, 1, 2? : {p1}
4 p4 $0|I1xsaw2 saw2 a3 girl4 comp ?2, 2, 5? : {p0}
5 p5 $0|I1xsaw2 a3 girl4 scan ?3, 2, 5? : {p0}
6 p6 I1xsaw2|girl4 a3 girl4 predy ?3, 4, 5? : {p5}
7 p7 girl4|a3 a3 girl4 predx ?3, 3, 4? : {p6}
8 p8 girl4|a3 girl4 scan ?4, 3, 4? : {p6}
9 p9 I1xsaw2|a3xgirl4 girl4 comp ?4, 4, 5? : {p5}
10 p10 I1xsaw2|a3xgirl4 scan ?5, 4, 5? : {p5}
11 p11 $0|I1xsaw2ygirl4 comp ?5, 2, 5? : {p0}
12 p12 $0ysaw2 comp ?5, 0, 5? : ?
Figure 2: Stages of the top-down deterministic parsing process for a sentence ?I saw a girl?. We follow a convention
and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example,
we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1.
parser constructs left and right children of a head
node in a left-to-right direction by scanning the head
node prior to its right children. Figure 2 shows an
example for parsing a sentence ?I saw a girl?.
4 Correctness
To prove the correctness of the system in Figure
1 for the projective dependency graph, we use the
proof strategy of (Nivre, 2008a). The correct deduc-
tive system is both sound and complete.
Theorem 4.1 The deductive system in Figure 1 is
correct for the class of dependency forest.
Proof 4.1 To show soundness, we show that Gp0 =
(VW , ?), which is a directed graph defined by the
axiom, is well-formed and projective, and that every
transition preserves this property.
? ROOT: The node 0 is a root in Gp0 , and the
node 0 is on the top of stack of p0. The two pred
actions put a word onto the top of stack, and
predict an arc from root or its descendant to
the child. The comp actions add the predicted
arcs which include no arc of (x, 0).
? SINGLE-HEAD: Gp0 is single-head. A node
y is no longer in stack and queue after a comp
action creates an arc (x, y). The node y cannot
make any arc (x?, y) after the removal.
? ACYCLICITY: Gp0 is acyclic. A cycle is cre-
ated only if an arc (x, y) is added when there
is a directed path y ?? x. The node x is no
longer in stack and queue when the directed
path y ?? x was made by adding an arc (l, x).
There is no chance to add the arc (x, y) on the
directed path y ?? x.
? PROJECTIVITY: Gp0 is projective. Projec-
tivity is violated by adding an arc (x, y) when
there is a node l in x < l < y or y < l < x
with the path to or from the outside of the span
x and y. When predy creates an arc relation
from x to y, the node y cannot be scanned be-
fore all nodes l in x < l < y are scanned and
completed. When predx creates an arc rela-
tion from x to y, the node y cannot be scanned
before all nodes k in k < y are scanned and
completed, and the node x cannot be scanned
before all nodes l in y < l < x are scanned
and completed. In those processes, the node l
in x < l < y or y < l < x does not make a
path to or from the outside of the span x and y,
and a path x ?? l or y ?? l is created. 2
To show completeness, we show that for any sen-
tence W , and dependency forest GW = (VW , AW ),
there is a transition sequence C0,m such that Gpm =
GW by an inductive method.
? If |W | = 1, the projective dependency graph
for W is GW = ({0}, ?) and Gp0 = GW .
? Assume that the claim holds for sentences with
length less or equal to t, and assume that
|W | = t + 1 and GW = (VW , AW ). The sub-
graph GW ? is defined as (VW ? t, A?t) where
659
..
.s2 h
.
.. . .
. .
.
.
.s1 h
.. . .
.. . . .
..s1.l
.. . . .
.. . .
.. . . .
..s1.r
. .. . .
.
.
.s0 h
.. . .
.. . . .
..s0.l
.. . . .
.. . .
.. . . .
..s0.r
. .. . .
Figure 3: Feature window of trees on stack S: The win-
dow size d is set to 2. Each x.h, x.l and x.r denotes root,
left and right child nodes of a stack element x.
A?t = AW ?{(x, y)|x = t? y = t}. If GW is
a dependency forest, then GW ? is also a depen-
dency forest. It is obvious that there is a transi-
tion sequence for constructing GW except arcs
which have a node t as a head or a dependent2.
There is a state pq = q : ?i, x, t + 1? :
for i and x (0 ? x < i < t + 1). When
x is the head of t, predy to t creates a state
pq+1 = q + 1 : ?i, t, t + 1? : {pq}. At least one
node y in i ? y < t becomes the dependent of
t by predx and there is a transition sequence
for constructing a tree rooted by y. After con-
structing a subtree rooted by t and spaned from
i to t, t is scaned, and then comp creates an
arc from x to t. It is obvious that the remaining
transition sequence exists. Therefore, we can
construct a transition sequence C0,m such that
Gpm = GW . 2
The deductive sysmtem in Figure 1 is both sound and
complete. Therefore, it is correct. 2
5 Weighted Parsing Model
5.1 Stack-based Model
The proposed algorithm employs a stack-based
model for scoring hypothesis. The cost of the model
is defined as follows:
cs(i, h, j, S) = ?s ? fs,act(i, h, j, S) (1)
where ?s is a weight vector, fs is a feature function,
and act is one of the applicable actions to a state ? :
?i, h, j, S? : pi. We use a set of feature templates of
(Huang and Sagae, 2010) for the model. As shown
in Figure 3, left children s0.l and s1.l of trees on
2This transition sequence is defined for GW ? , but it is pos-
sible to be regarded as the definition for GW as long as the
transition sequence is indifferent from the node t.
Algorithm 1 Top-down Parsing with Beam Search
1: input W = n0, . . . ,nn
2: start? ?1, 0, n + 1,n0?
3: buf [0]? {start}
4: for ?? 1 . . . 3n do
5: hypo? {}
6: for each state in buf [?? 1] do
7: for act?applicableAct(state) do
8: newstates?actor(act, state)
9: addAll newstates to hypo
10: add top b states to buf [?] from hypo
11: return best candidate from buf [3n]
stack for extracting features are different from those
of Huang and Sagae (2010) because in our parser the
left children are generated from left to right.
As mentioned in Section 1, we apply beam search
and Huang and Sagae (2010)?s DP techniques to
our top-down parser. Algorithm 1 shows the our
beam search algorithm in which top most b states
are preserved in a buffer buf [?] in each step. In
line 10 of Algorithm 1, equivalent states in the step
? are merged following the idea of DP. Two states
?i, h, j, S? and ?i?, h?, j?, S?? in the step ? are equiv-
alent, notated ?i, h, j, S? ? ?i?, h?, j?, S??, iff
fs,act(i, h, j, S) = fs,act(i?, h?, j?, S?). (2)
When two equivalent predicted states are merged,
their predictor states in pi get combined. For fur-
ther details about this technique, readers may refer
to (Huang and Sagae, 2010).
5.2 Weighted Prediction
The step 0 in Figure 2 shows an example of predic-
tion for a head node ?$0?, where the node ?saw2? is
selected as its child node. To select a probable child
node, we define a statistical model for the prediction.
In this paper, we integrate the cost from a graph-
based model (McDonald and Pereira, 2006) which
directly models dependency links. The cost of the
1st-order model is defined as the relation between a
child node c and a head node h:
cp(h, c) = ?p ? fp(h, c) (3)
where ?p is a weight vector and fp is a features func-
tion. Using the cost cp, the top-down parser selects
a probable child node in each prediction step.
When we apply beam search to the top-down
parser, then we no longer use ? but ? on predx and
660
..
.h
..l1 . .. . . . ..ll . ..r1 . .. . . . ..rm
Figure 4: An example of tree structure: Each h, l and r
denotes head, left and right child nodes.
predy in Figure 1. Therefore, the parser may predict
many nodes as an appropriate child from a single
state, causing many predicted states. This may cause
the beam buffer to be filled only with the states, and
these may exclude other states, such as scanned or
completed states. Thus, we limit the number of pre-
dicted states from a single state by prediction size
implicitly in line 10 of Algorithm 1.
To improve the prediction accuracy, we introduce
a more sophisticated model. The cost of the sibling
2nd-order model is defined as the relationship be-
tween c, h and a sibling node sib:
cp(h, sib, c) = ?p ? fp(h, sib, c). (4)
The 1st- and sibling 2nd-order models are the same
as McDonald and Pereira (2006)?s definitions, ex-
cept the cost factors of the sibling 2nd-order model.
The cost factors for a tree structure in Figure 4 are
defined as follows:
cp(h,?, l1) +
l?1
?
y=1
cp(h, ly, ly+1)
+cp(h,?, r1) +
m?1
?
y=1
cp(h, ry, ry+1).
This is different from McDonald and Pereira (2006)
in that the cost factors for left children are calcu-
lated from left to right, while those in McDonald and
Pereira (2006)?s definition are calculated from right
to left. This is because our top-down parser gener-
ates left children from left to right. Note that the
cost of weighted prediction model in this section is
incrementally calculated by using only the informa-
tion on the current state, thus the condition of state
merge in Equation 2 remains unchanged.
5.3 Weighted Deductive System
We extend deductive system to a weighted one, and
introduce forward cost and inside cost (Stolcke,
1995; Huang and Sagae, 2010). The forward cost is
the total cost of a sequence from an initial state to the
end state. The inside cost is the cost of a top tree s0
in stack S. We define these costs using a combina-
tion of stack-based model and weighted prediction
model. The forward and inside costs of the combi-
nation model are as follows:
{
cfw = cfws + cfwp
cin = cins + cinp
(5)
where cfws and cins are a forward cost and an inside
cost for stack-based model, and cfwp and cinp are a for-
ward cost and an inside cost for weighted prediction
model. We add the following tuple of costs to a state:
(cfws , cins , cfwp , cinp ).
For each action, we define how to efficiently cal-
culate the forward and inside costs3 , following Stol-
cke (1995) and Huang and Sagae (2010)?s works. In
either case of predx or predy,
(cfws , , cfwp , )
(cfws + ?, 0, cfwp + cp(s0.h,nk), 0)
where
? =
{
?s ? fs,predx(i, h, j, S) if predx
?s ? fs,predy(i, h, j, S) if predy
(6)
In the case of scan,
(cfws , cins , cfwp , cinp )
(cfws + ?, cins + ?, cfwp , cinp )
where
? = ?s ? fs,scan(i, h, j, S). (7)
In the case of comp,
(c?fws , c?
in
s , c?
fw
p , c?
in
p ) (cfws , cins , cfwp , cinp )
(c?fws + cins + ?, c?ins + cins + ?,
c?fwp + cinp + cp(s?0.h, s0.h),
c?inp + cinp + cp(s?0.h, s0.h))
where
? = ?s ? fs,comp(i, h, j, S) + ?s ? fs,pred ( , h?, j?, S?).
(8)
3For brevity, we present the formula not by 2nd-order model
as equation 4 but a 1st-order one for weighted prediction.
661
Pred takes either predx or predy. Beam search is
performed based on the following linear order for
the two states p and p? at the same step, which have
(cfw, cin) and (c?fw, c?in) respectively:
p ? p? iff cfw < c?fw or cfw = c?fw ? cin < c?in. (9)
We prioritize the forward cost over the inside cost
since forward cost pertains to longer action sequence
and is better suited to evaluate hypothesis states than
inside cost (Nederhof, 2003).
5.4 FIRST Function for Lookahead
Top-down backtrack parser usually reduces back-
tracking by precomputing the set FIRST(?) (Aho and
Ullman, 1972). We define the set FIRST(?) for our
top-down dependency parser:
FIRST(t?) = {ld.t|ld ? lmdescendant(Tree, t?)
Tree ? Corpus} (10)
where t? is a POS-tag, Tree is a correct depen-
dency tree which exists in Corpus, a function
lmdescendant(Tree, t?) returns the set of the leftmost
descendant node ld of each nodes in Tree whose
POS-tag is t?, and ld.t denotes a POS-tag of ld.
Though our parser does not backtrack, it looks ahead
when selecting possible child nodes at the prediction
step by using the function FIRST. In case of predx:
?k : i ? k < h ? ni.t ? FIRST(nk.t)
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, h, sd?1|...|s0|nk? : {p}
where ni.t is a POS-tag of the node ni on the top of
the queue, and nk.t is a POS-tag in kth position of
an input nodes. The case for predy is the same. If
there are no nodes which satisfy the condition, our
top-down parser creates new states for all nodes, and
pushes them into hypo in line 9 of Algorithm 1.
6 Time Complexity
Our proposed top-down algorithm has three kinds
of actions which are scan, comp and predict. Each
scan and comp actions occurs n times when parsing
a sentence with the length n. Predict action also oc-
curs n times in which a child node is selected from
a node sequence in the input queue. Thus, the algo-
rithm takes the following times for prediction:
n + (n? 1) + ? ? ? + 1 =
n
?
i
i = n(n + 1)2 . (11)
As n2 for prediction is the most dominant factor, the
time complexity of the algorithm is O(n2) and that
of the algorithm with beam search is O(n2 ? b).
7 Related Work
Alshawi (1996) proposed head automaton which
recognizes an input sentence top-down. Eisner
and Satta (1999) showed that there is a cubic-time
parsing algorithm on the formalism of the head
automaton grammars, which are equivalently con-
verted into split-head bilexical context-free gram-
mars (SBCFGs) (McAllester, 1999; Johnson, 2007).
Although our proposed algorithm does not employ
the formalism of SBCFGs, it creates left children
before right children, implying that it does not have
spurious ambiguities as well as parsing algorithms
on the SBCFGs. Head-corner parsing algorithm
(Kay, 1989) creates dependency tree top-down, and
in this our algorithm has similar spirit to it.
Yamada and Matsumoto (2003) applied a shift-
reduce algorithm to dependency analysis, which is
known as arc-standard transition-based algorithm
(Nivre, 2004). Nivre (2003) proposed another
transition-based algorithm, known as arc-eager al-
gorithm. The arc-eager algorithm processes right-
dependent top-down, but this does not involve the
prediction of lower nodes from higher nodes. There-
fore, the arc-eager algorithm is a totally bottom-up
algorithm. Zhang and Clark (2008) proposed a com-
bination approach of the transition-based algorithm
with graph-based algorithm (McDonald and Pereira,
2006), which is the same as our combination model
of stack-based and prediction models.
8 Experiments
Experiments were performed on the English Penn
Treebank data and the Chinese CoNLL-06 data. For
the English data, we split WSJ part of it into sections
02-21 for training, section 22 for development and
section 23 for testing. We used Yamada and Mat-
sumoto (2003)?s head rules to convert phrase struc-
ture to dependency structure. For the Chinese data,
662
time accuracy complete root
McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 ?
Koo10 (Koo and Collins, 2010) ? 93.04 ? ?
Hayashi11 (Hayashi et al, 2011) 0.3 92.89 ? ?
2nd-MST? 0.13 92.3 43.7 96.0
Goldberg10 (Goldberg and Elhadad, 2010) ? 89.7 37.5 91.5
Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) ? 91.3 41.7 ?
Zhang08 (Sh beam 64) ? 91.4 41.8 ?
Zhang08 (Sh+Graph beam 64) ? 92.1 45.4 ?
Huang10 (beam+DP) 0.04 92.1 ? ?
Huang10? (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1
Zhang11 (beam 64) (Zhang and Nivre, 2011) ? 93.07 49.59 ?
top-down? (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2
top-down? (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6
Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled
attachment score, complete is a sentence complete rate, and root is a correct root rate. ? indicates our experiments.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70
pa
rs
in
g 
tim
e 
(cp
u s
ec
)
length of input sentence
"shift-reduce"
"2nd-mst"
"top-down"
Figure 5: Scatter plot of parsing time against sentence
length, comparing with top-down, 2nd-MST and shift-
reduce parsers (beam size: 8, pred size: 5)
we used the information of words and fine-grained
POS-tags for features. We also implemented and ex-
perimented Huang and Sagae (2010)?s arc-standard
shift-reduce parser. For the 2nd-order Eisner-Satta
algorithm, we used MSTParser (McDonald, 2012).
We used an early update version of averaged per-
ceptron algorithm (Collins and Roark, 2004) for
training of shift-reduce and top-down parsers. A
set of feature templates in (Huang and Sagae, 2010)
were used for the stack-based model, and a set of
feature templates in (McDonald and Pereira, 2006)
were used for the 2nd-order prediction model. The
weighted prediction and stack-based models of top-
down parser were jointly trained.
8.1 Results for English Data
During training, we fixed the prediction size and
beam size to 5 and 16, respectively, judged by pre-
accuracy complete root
oracle (sh+mst) 94.3 52.3 97.7
oracle (top+sh) 94.2 51.7 97.6
oracle (top+mst) 93.8 50.7 97.1
oracle (top+sh+mst) 94.9 55.3 98.1
Table 2: Oracle score, choosing the highest accuracy
parse for each sentence on test data from results of top-
down (beam 8, pred 5) and shift-reduce (beam 8) and
MST(2nd) parsers in Table 1.
accuracy complete root
top-down (beam:8, pred:5) 90.9 80.4 93.0
shift-reduce (beam:8) 90.8 77.6 93.5
2nd-MST 91.4 79.3 94.2
oracle (sh+mst) 94.0 85.1 95.9
oracle (top+sh) 93.8 84.0 95.6
oracle (top+mst) 93.6 84.2 95.3
oracle (top+sh+mst) 94.7 86.5 96.3
Table 3: Results for Chinese Data (CoNLL-06)
liminary experiments on development data. After
25 iterations of perceptron training, we achieved
92.94 unlabeled accuracy for top-down parser with
the FIRST function and 93.01 unlabeled accuracy
for shift-reduce parser on development data by set-
ting the beam size to 8 for both parsers and the pre-
diction size to 5 in top-down parser. These trained
models were used for the following testing.
We compared top-down parsing algorithm with
other data-driven parsing algorithms in Table 1.
Top-down parser achieved comparable unlabeled ac-
curacy with others, and outperformed them on the
sentence complete rate. On the other hand, top-
down parser was less accurate than shift-reduce
663
No.717 Little Lily , as Ms. Cunningham calls7 herself in the book , really was14 n?t ordinary .
shift-reduce 2 7 2 2 6 4 14 7 7 11 9 7 14 0 14 14 14
2nd-MST 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
top-down 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
correct 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
No.127 resin , used to make garbage bags , milk jugs , housewares , toys and meat packaging25 , among other items .
shift-reduce 25 9 9 13 11 15 13 25 18 25 25 25 25 25 25 25 7 25 25 29 27 4
2nd-MST 29 9 9 13 11 15 13 29 18 29 29 29 29 25 25 25 29 25 25 29 7 4
top-down 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
correct 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
Table 4: Two examples on which top-down parser is superior to two bottom-up parsers: In correct analysis, the boxed
portion is the head of the underlined portion. Bottom-up parsers often mistake to capture the relation.
parser on the correct root measure. In step 0, top-
down parser predicts a child node, a root node of
a complete tree, using little syntactic information,
which may lead to errors in the root node selection.
Therefore, we think that it is important to seek more
suitable features for the prediction in future work.
Figure 5 presents the parsing time against sen-
tence length. Our proposed top-down parser is the-
oretically slower than shift-reduce parser and Fig-
ure 5 empirically indicates the trends. The domi-
nant factor comes from the score calculation, and
we will leave it for future work. Table 2 shows
the oracle score for test data, which is the score
of the highest accuracy parse selected for each sen-
tence from results of several parsers. This indicates
that the parses produced by each parser are differ-
ent from each other. However, the gains obtained by
the combination of top-down and 2nd-MST parsers
are smaller than other combinations. This is because
top-down parser uses the same features as 2nd-MST
parser, and these are more effective than those of
stack-based model. It is worth noting that as shown
in Figure 5, our O(n2?b) (b = 8) top-down parser is
much faster than O(n3) Eisner-Satta CKY parsing.
8.2 Results for Chinese Data (CoNLL-06)
We also experimented on the Chinese data. Fol-
lowing English experiments, shift-reduce parser was
trained by setting beam size to 16, and top-down
parser was trained with the beam size and the predic-
tion size to 16 and 5, respectively. Table 3 shows the
results on the Chinese test data when setting beam
size to 8 for both parsers and prediction size to 5 in
top-down parser. The trends of the results are almost
the same as those of the English results.
8.3 Analysis of Results
Table 4 shows two interesting results, on which top-
down parser is superior to either shift-reduce parser
or 2nd-MST parser. The sentence No.717 contains
an adverbial clause structure between the subject
and the main verb. Top-down parser is able to han-
dle the long-distance dependency while shift-reudce
parser cannot correctly analyze it. The effectiveness
on the clause structures implies that our head-driven
parser may handle non-projective structures well,
which are introduced by Johansonn?s head rule (Jo-
hansson and Nugues, 2007). The sentence No.127
contains a coordination structure, which it is diffi-
cult for bottom-up parsers to handle, but, top-down
parser handles it well because its top-down predic-
tion globally captures the coordination.
9 Conclusion
This paper presents a novel head-driven parsing al-
gorithm and empirically shows that it is as practi-
cal as other dependency parsing algorithms. Our
head-driven parser has potential for handling non-
projective structures better than other non-projective
dependency algorithms (McDonald et al, 2005; At-
tardi, 2006; Nivre, 2008b; Koo et al, 2010). We are
in the process of extending our head-driven parser
for non-projective structures as our future work.
Acknowledgments
We would like to thank Kevin Duh for his helpful
comments and to the anonymous reviewers for giv-
ing valuable comments.
664
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation and Compiling, volume 1: Parsing.
Prentice-Hall.
H. Alshawi. 1996. Head automata for speech translation.
In Proc. the ICSLP.
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proc. the 10th
CoNLL, pages 166?170.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. the 42nd ACL.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94?102.
J. M. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. the 37th ACL, pages 457?464.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742?750.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2011. The third-order variational rerank-
ing on packed-shared dependency forests. In Proc.
EMNLP, pages 1479?1488.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the 48th
ACL, pages 1077?1086.
H. Isozaki, H. Kazawa, and T. Hirao. 2004. A determin-
istic word dependency analyzer enhanced with prefer-
ence learning. In Proc. the 21st COLING, pages 275?
281.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. NODALIDA.
M. Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proc. the 45th ACL, pages 168?
175.
M. Kay. 1989. Head driven parsing. In Proc. the IWPT.
K. Kitagawa and K. Tanaka-Ishii. 2010. Tree-based de-
terministic dependency parsing ? an application to
nivre?s method ?. In Proc. the 48th ACL 2010 Short
Papers, pages 189?193, July.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1?11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. EMNLP, pages
1288?1298.
D. McAllester. 1999. A reformulation of eisner and
satta?s cubic time parser for split head automata gram-
mars. http://ttic.uchicago.edu/ dmcallester/.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP, pages 523?
530.
R. McDonald. 2012. Minimum spanning tree parser.
http://www.seas.upenn.edu/ strctlrn/MSTParser.
M.-J. Nederhof. 2003. Weighted deductive parsing
and knuth?s algorithm. Computational Linguistics,
29:135?143.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. the IWPT, pages 149?160.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proc. the ACL Workshop Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
J. Nivre. 2008a. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34:513?553.
J. Nivre. 2008b. Sorting out dependency parsing. In
Proc. the CoTAL, pages 16?27.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In
Proc. EMNLP, pages 562?571.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
the 49th ACL, pages 188?193.
665
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791?801,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Additive Neural Networks for Statistical Machine Translation
Lemao Liu1, Taro Watanabe2, Eiichiro Sumita2, Tiejun Zhao1
1School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology (NICT)
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu | tjzhao}@mtlab.hit.edu.cn
{taro.watanabe | eiichiro.sumita}@nict.go.jp
Abstract
Most statistical machine translation
(SMT) systems are modeled using a log-
linear framework. Although the log-linear
model achieves success in SMT, it still
suffers from some limitations: (1) the
features are required to be linear with
respect to the model itself; (2) features
cannot be further interpreted to reach
their potential. A neural network is
a reasonable method to address these
pitfalls. However, modeling SMT with a
neural network is not trivial, especially
when taking the decoding efficiency
into consideration. In this paper, we
propose a variant of a neural network, i.e.
additive neural networks, for SMT to go
beyond the log-linear translation model.
In addition, word embedding is employed
as the input to the neural network, which
encodes each word as a feature vector.
Our model outperforms the log-linear
translation models with/without embed-
ding features on Chinese-to-English and
Japanese-to-English translation tasks.
1 Introduction
Recently, great progress has been achieved in
SMT, especially since Och and Ney (2002) pro-
posed the log-linear model: almost all the state-
of-the-art SMT systems are based on the log-linear
model. Its most important advantage is that arbi-
trary features can be added to the model. Thus,
it casts complex translation between a pair of lan-
guages as feature engineering, which facilitates re-
search and development for SMT.
Regardless of how successful the log-linear
model is in SMT, it still has some shortcomings.
This joint work was done while the first author visited
NICT.
On the one hand, features are required to be lin-
ear with respect to the objective of the translation
model (Nguyen et al, 2007), but it is not guaran-
teed that the potential features be linear with the
model. This induces modeling inadequacy (Duh
and Kirchhoff, 2008), in which the translation per-
formance may not improve, or may even decrease,
after one integrates additional features into the
model. On the other hand, it cannot deeply in-
terpret its surface features, and thus can not ef-
ficiently develop the potential of these features.
What may happen is that a feature p does initially
not improve the translation performance, but after
a nonlinear operation, e.g. log(p), it does. The
reason is not because this feature is useless but the
model does not efficiently interpret and represent
it. Situations such as this confuse explanations for
feature designing, since it is unclear whether such
a feature contributes to a translation or not.
A neural network (Bishop, 1995) is a reason-
able method to overcome the above shortcomings.
However, it should take constraints, e.g. the de-
coding efficiency, into account in SMT. Decod-
ing in SMT is considered as the expansion of
translation states and it is handled by a heuris-
tic search (Koehn, 2004a). In the search pro-
cedure, frequent computation of the model score
is needed for the search heuristic function, which
will be challenged by the decoding efficiency for
the neural network based translation model. Fur-
ther, decoding with non-local (or state-dependent)
features, such as a language model, is also a prob-
lem. Actually, even for the (log-) linear model,
efficient decoding with the language model is not
trivial (Chiang, 2007).
In this paper, we propose a variant of neural net-
works, i.e. additive neural networks (see Section
3 for details), for SMT. It consists of two com-
ponents: a linear component which captures non-
local (or state dependent) features and a non-linear
component (i.e., neural nework) which encodes lo-
791
XProceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802?810,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Hierarchical Phrase Table Combination for Machine Translation
Conghui Zhu1 Taro Watanabe2 Eiichiro Sumita2 Tiejun Zhao1
1School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{chzhu,tjzhao}@mtlab.hit.edu.cn
{taro.watanabe,Sumita}@nict.go.jp
Abstract
Typical statistical machine translation sys-
tems are batch trained with a given train-
ing data and their performances are large-
ly influenced by the amount of data. With
the growth of the available data across
different domains, it is computationally
demanding to perform batch training ev-
ery time when new data comes. In face
of the problem, we propose an efficient
phrase table combination method. In par-
ticular, we train a Bayesian phrasal inver-
sion transduction grammars for each do-
main separately. The learned phrase ta-
bles are hierarchically combined as if they
are drawn from a hierarchical Pitman-Yor
process. The performance measured by
BLEU is at least as comparable to the tra-
ditional batch training method. Further-
more, each phrase table is trained sepa-
rately in each domain, and while compu-
tational overhead is significantly reduced
by training them in parallel.
1 Introduction
Statistical machine translation (SMT) system-
s usually achieve ?crowd-sourced? improvements
with batch training. Phrase pair extraction, the
key step to discover translation knowledge, heav-
ily relies on the scale of training data. Typi-
cally, the more parallel corpora used, the more
phrase pairs and more accurate parameters will
be learned, which can obviously be beneficial to
improving translation performances. Today, more
parallel sentences are drawn from divergent do-
mains, and the size keeps growing. Consequent-
ly, how to effectively use those data and improve
translation performance becomes a challenging is-
sue.
This joint work was done while the first author visited
NICT.
Batch retraining is not acceptable for this case,
since it demands serious computational overhead
when training on a large data set, and it requires
us to re-train every time new training data is avail-
able. Even if we can handle the large computation
cost, improvement is not guaranteed every time we
perform batch tuning on the newly updated train-
ing data obtained from divergent domains. Tradi-
tional domain adaption methods for SMT are also
not adequate in this scenario. Most of them have
been proposed in order to make translation sys-
tems perform better for resource-scarce domain-
s when most training data comes from resource-
rich domains, and ignore performance on a more
generic domain without domain bias (Wang et al,
2012). As an alternative, incremental learning
may resolve the gap by incrementally adding da-
ta sentence-by-sentence into the training data. S-
ince SMT systems trend to employ very large scale
training data for translation knowledge extraction,
updating several sentence pairs each time will be
annihilated in the existing corpus.
This paper proposes a new phrase table combi-
nation method. First, phrase pairs are extracted
from each domain without interfering with oth-
er domains. In particular, we employ the non-
parametric Bayesian phrasal inversion transduc-
tion grammar (ITG) of Neubig et al (2011) to per-
form phrase table extraction. Second, extracted
phrase tables are combined as if they are drawn
from a hierarchical Pitman-Yor process, in which
the phrase tables represented as tables in the Chi-
nese restaurant process (CRP) are hierarchically
chained by treating each of the previously learned
phrase tables as prior to the current one. Thus, we
can easily update the chain of phrase tables by ap-
pending the newly extracted phrase table and by
treating the chain of the previous ones as its prior.
Experiment results indicate that our method can
achieve better translation performance when there
exists a large divergence in domains, and can
802
achieve at least comparable results to batch train-
ing methods, with a significantly less computa-
tional overhead.
The rest of the paper is organized as follows.
In Section 2, we introduce related work. In sec-
tion 3, we briefly describe the translation mod-
el with phrasal ITGs and Pitman-Yor process. In
section 4, we explain our hierarchical combination
approach and give experiment results in section 5.
We conclude the paper in the last section.
2 Related Work
Bilingual phrases are cornerstones for phrase-
based SMT systems (Och and Ney, 2004; Koehn
et al, 2003; Chiang, 2005) and existing translation
systems often get ?crowd-sourced? improvements
(Levenberg et al, 2010). A number of approaches
have been proposed to make use of the full poten-
tial of the available parallel sentences from vari-
ous domains, such as domain adaptation and in-
cremental learning for SMT.
The translation model and language model
are primary components in SMT. Previous work
proved successful in the use of large-scale data for
language models from diverse domains (Brants et
al., 2007; Schwenk and Koehn, 2008). Alterna-
tively, the language model is incrementally up-
dated by using a succinct data structure with a
interpolation technique (Levenberg and Osborne,
2009; Levenberg et al, 2011).
In the case of the previous work on translation
modeling, mixed methods have been investigat-
ed for domain adaptation in SMT by adding do-
main information as additional labels to the orig-
inal phrase table (Foster and Kuhn, 2007). Un-
der this framework, the training data is first di-
vided into several parts, and phase pairs are ex-
tracted with some sub-domain features. Then al-
l the phrase pairs and features are tuned together
with different weights during decoding. As a way
to choose the right domain for the domain adap-
tion, a classifier-based method and a feature-based
method have been proposed. Classification-based
methods must at least add an explicit label to indi-
cate which domain the current phrase pair comes
from. This is traditionally done with an automat-
ic domain classifier, and each input sentence is
classified into its corresponding domain (Xu et al,
2007). As an alternative to the classification-based
approach, Wang et al (2012) employed a feature-
based approach, in which phrase pairs are enriched
by a feature set to potentially reflect the domain in-
formation. The similarity calculated by a informa-
tion retrieval system between the training subset
and the test set is used as a feature for each paral-
lel sentence (Lu et al, 2007). Monolingual topic
information is taken as a new feature for a domain
adaptive translation model and tuned on the devel-
opment set (Su et al, 2012). Regardless of under-
lying methods, either classifier-based or feature-
based method, the performance of current domain
adaptive phrase extraction methods is more sensi-
tive to the development set selection. Usually the
domain similar to a given development data is usu-
ally assigned higher weights.
Incremental learning in which new parallel sen-
tences are incrementally updated to the training
data is employed for SMT. Compared to tradi-
tional frequent batch oriented methods, an online
EM algorithm and active learning are applied to
phrase pair extraction and achieves almost compa-
rable translation performance with less computa-
tional overhead (Levenberg et al, 2010; Gonza?lez-
Rubio et al, 2011). However, their methods usu-
ally require numbers of hyperparameters, such as
mini-batch size, step size, or human judgment to
determine the quality of phrases, and still rely on a
heuristic phrase extraction method in each phrase
table update.
3 Phrase Pair Extraction with
Unsupervised Phrasal ITGs
Recently, phrase alignment with ITGs (Cherry
and Lin, 2007; Zhang et al, 2008; Blunsom et
al., 2008) and parameter estimation with Gibb-
s sampling (DeNero and Klein, 2008; Blunsom
and Cohn, 2010) are popular. Here, we em-
ploy a method proposed by Neubig et al (2011),
which uses parametric Bayesian inference with the
phrasal ITGs (Wu, 1997). It can achieve com-
parable translation accuracy with a much small-
er phrase table than the traditional GIZA++ and
heuristic phrase extraction methods. It has al-
so been proved successful in adjusting the phrase
length granularity by applying character-based
SMT with more sophisticated inference (Neubig
et al, 2012).
ITG is a synchronous grammar formalism
which analyzes bilingual text by introducing in-
verted rules, and each ITG derivation corresponds
to the alignment of a sentence pair (Wu, 1997).
Translation probabilities of ITG phrasal align-
803
ments can be estimated in polynomial time by s-
lightly limiting word reordering (DeNero and K-
lein, 2008).
More formally, P (?e, f?; ?x, ?t
) are the proba-
bility of phrase pairs ?e, f?, which is parameter-
ized by a phrase pair distribution ?t and a symbol
distribution ?x. ?x is a Dirichlet prior, and ?t is es-
timated with the Pitman-Yor process (Pitman and
Yor, 1997; Teh, 2006), which is expressed as
?t ? PY
(
d, s, Pdac
) (1)
where d is the discount parameter, s is the strength
parameter, and , and Pdac is a prior probability
which acts as a fallback probability when a phrase
pair is not in the model.
Under this model, the probability for a phrase
pair found in a bilingual corpus ?E,F ? can be rep-
resented by the following equation using the Chi-
nese restaurant process (Teh, 2006):
P
(
?ei, fi?; ?E,F ?
)
= 1C + s(ci ? d? ti)+
1
C + s(s+ d? T )? Pdac(?ei, fi?) (2)
where
1. ci and ti are the customer and table count of
the ith phrase pair ?ei, fi? found in a bilingual
corpus ?E,F ?;
2. C and T are the total customer and table count
in corpus ?E,F ?;
3. d and s are the discount and strengthen hyper-
parameters.
The prior probability Pdac is recursively defined
by breaking a longer phrase pair into two through
the recursive ITG?s generative story as follows
(Neubig et al, 2011):
1. Generate symbol x from Px(x; ?x) with three
possible values: Base, REG, or INV .
2. Depending on the value of x take the following
actions.
a. If x = Base, generate a new phrase pair
directly from Pbase.
b. If x = REG, generate ?e1, f1? and
?e2, f2? from P
(
?e, f?; ?x, ?t
), and con-
catenate them into a single phrase pair
?e1e2, f1f2?.
Figure 1: A word alignment (a), and its hierarchi-
cal derivation (b).
c. If x = INV , follow a similar process as b,
but concatenate f1 and f2 in reverse order
?e1e2, f2f1?.
Note that the Pdac is recursively defined through
the binary branched P , which in turns employs
Pdac as a prior probability. Pbase is a base measure
defined as a combination of the IBM Models in t-
wo directions and the unigram language models in
both sides. Inference is carried out by a heuristic
beam search based block sampling with an effi-
cient look ahead for a faster convergence (Neubig
et al, 2012).
Compared to GIZA++ with heuristic phrase ex-
traction, the Bayesian phrasal ITG can achieve
competitive accuracy under a smaller phrase ta-
ble size. Further, the fallback model can incor-
porate phrases of all granularity by following the
ITG?s recursive definition. Figure 1 (b) illustrates
an example of the phrasal ITG derivation for word
alignment in Figure 1 (a) in which a bilingual sen-
tence pair is recursively divided into two through
the recursively defined generative story.
4 Hierarchical Phrase Table
Combination
We propose a new phrase table combination
method, in which individually learned phrase ta-
ble are hierarchically chained through a hierarchi-
cal Pitman-Yor process.
Firstly, we assume that the whole train-
ing data ?E,F ? can be split into J domains,
{?E1, F 1?, . . . , ?EJ , F J?}. Then phrase pairs are
804
Figure 2: A hierarchical phrase table combination (a), and a basic unit of a Chinese restaurant process
with K tables and N customers.
extracted from each domain j (1 ? j ? J) sepa-
rately with the method introduced in Section 3. In
traditional domain adaptation approaches, phrase
pairs are extracted together with their probabili-
ties and/or frequencies so that the extracted phrase
pairs are merged uniformly or after scaling.
In this work, we extract the table counts for each
phrase pair under the Chinese restaurant process
given in Section 3. In Figure 2 (b), a CRP is illus-
trated which has K tables and N customers with
each chair representing a customer. Meanwhile
there are two parameters, discount and strength for
each domain similar to the ones in Equation (1).
Our proposed hierarchical phrase table combi-
nation can be formally expressed as following:
?1 ? PY (d1, s1, P 2)
? ? ? ? ? ?
?j ? PY (dj , sj , P j+1)
? ? ? ? ? ?
?J ? PY
(
dJ , sJ , P Jbase
) (3)
Here the (j + 1)th layer hierarchical Pitman-Yor
process is employed as a base measure for the
jth layer hierarchical Pitman-Yor process. The
hierarchical chain is terminated by the base mea-
sure from the J th domain P Jbase. The hierarchi-
cal structure is illustrated in Figure 2 (a) in which
the solid lines implies a fall back using the ta-
ble counts from the subsequent domains, and the
dotted lines means the final fallback to the base
measure P Jbase. When we query a probability of
a phrase pair ?e, f?, we first query the probabil-
ity of the first layer P 1(?e, f?). If ?e, f? is not
in the model, we will fallback to the next level of
P 2(?e, f?). This process continues until we reach
the Jth base measure of P J(?e, f?). Each fallback
can be viewed as a translation knowledge integra-
tion process between subsequent domains.
For example in Figure 2 (a), the ith phrase pair
?ei, fi? appears only in the domain 1 and domain
2, so its translation probability can be calculated
by substituting Equation (3) with Equation (2):
P
(
?ei, fi?; ?E,F ?
)
= 1C1 + s1 (c
1
i ? d1 ? t1i )
+ s
1 + d1 ? T 1
(C1 + s1)? (C2 + s2)(c
2
i ? d2 ? t2i )
+
J?
j=1
(sj + dj ? T j
Cj + sj
)
? P Jbase(?ei, fi?) (4)
where the superscript indicates the domain for the
corresponding counts, i.e. cji for the customer
count in the jth domain. The first term in Equa-
tion (4) is the phrase probability from the first do-
main, and the second one comes from the second
domain, but weighted by the fallback weight of the
1st domain. Since ?ei, fi? does not appear in the
rest of the layers, the last term is taken from al-
l the fallback weight from the second layer to the
J th layer with the final P Jbase. All the parameter-
s ?j and hyperparameters dj and sj , are obtained
by learning on the jth domain. Returning the hy-
perparameters again when cascading another do-
main may improve the performance of the combi-
nation weight, but we will leave it for future work.
The hierarchical process can be viewed as an in-
stance of adapted integration of translation knowl-
edge from each sub-domain.
805
Algorithm 1 Translation Probabilities Estima-
tion
Input: cji , tji , P jbase, Cj , T j , dj and sjOutput: The translation probabilities for each
pair
1: for all phrase pair ?ei, fi? do
2: Initialize the P (?ei, fi?) = 0 and wi = 1
3: for all domain ?Ej , Fj? such that 1 6 j 6
J ? 1 do
4: if ?ei, fi? ? ?Ej , Fj? then
5: P (?ei, fi?) += wi ? (Cji ? dj ?
tji )/(Cj + sj)
6: end if
7: wi = wi ? (sj + dj ? T j)/(Cj + sj)
8: end for
9: P (?ei, fi?) += wi? (CJi ?dJ ? tJi + (sJ +
dJ ? T J)? P Jbase(?ei, fi?))/(CJ + sJ)
10: end for
Our approach has several advantages. First,
each phrase pair extraction can concentrate on a s-
mall portion of domain-specific data without inter-
fering with other domains. Since no tuning stage
is involved in the hierarchical combination, we can
easily include a new phrase table from a new do-
main by simply chaining them together. Second,
phrase pair phrase extraction in each domain is
completely independent, so it is easy to parallelize
in a situation where the training data is too large
to fit into a small amount of memory. Finally, new
domains can be integrated incrementally. When
we encounter a new domain, and if a phrase pair is
completely new in terms of the model, the phrase
pair is simply appended to the current model, and
computed without the fallback probabilities, since
otherwise, the phrase pair would be boosted by the
fallback probabilities. Pitman-Yor process is also
employed in n-gram language models which are
hierarchically represented through the hierarchi-
cal Pitman-Yor process with switch priors to in-
tegrate different domains in all the levels (Wood
and Teh, 2009). Our work incrementally combines
the models from different domains by directly em-
ploying the hierarchical process through the base
measures.
5 Experiment
We evaluate the proposed approach on the
Chinese-to-English translation task with three data
sets with different scales.
Data set Corpus #sent. pairs
IWSLT HIT 52, 603
BTEC 19, 975
Domain 1 47, 993
Domain 2 30, 272
FBIS Domain 3 49, 509
Domain 4 38, 228
Domain 5 55, 913
News 221, 915
News 95, 593
LDC Magazine 98, 335
Magazine 254, 488
Finance 86, 112
Table 1: The sentence pairs used in each data set.
5.1 Experiment Setup
The first data set comes from the IWSLT2012
OLYMPICS task consisting of two training sets:
the HIT corpus, which is closely related to the Bei-
jing 2008 Olympic Games, and the BTEC corpus,
which is a multilingual speech corpus containing
tourism-related sentences. The second data set,
the FBIS corpus, is a collection of news articles
and does not have domain information itself, so a
Latent Dirichlet Allocation (LDA) tool, PLDA1,
is used to divide the whole corpus into 5 different
sub-domains according to the concatenation of the
source side and target side as a single sentence (Li-
u et al, 2011). The third data set is composed of 5
corpora2 from LDC with various domains, includ-
ing news, magazine, and finance. The details are
shown in Table 1.
In order to evaluate our approach, four phrase
pair extraction methods are performed:
1. GIZA-linear: Phase pairs are extracted in each
domain by GIZA++ (Och and Ney, 2003) and
the ?grow-diag-final-and? method with a max-
imum length 7. The phrase tables from vari-
ous domains are linearly combined by averag-
ing the feature values.
2. Pialign-linear: Similar to GIZA-linear, but we
employed the phrasal ITG method described in
Section 3 using the pialign toolkit 3 (Neubig et
1http://code.google.com/p/plda/
2In particular, they come from LDC catalog number:
LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47,
LDC2006E26, in this order.
3http://www.phontron.com/pialign/
806
Methods IWSLT FBIS LDCBLEU Size BLEU Size BLEU Size
GIZA-linear 19.222 1,200,877 29.342 15,369,028 30.67 77,927,347
Pialign-linear 19.534 876,059 29.858 7,235,342 31.12 28,877,149
GIZA-batch 19.616 1,185,255 31.38 13,737,258 32.06 63,606,056
Pialign-batch 19.506 841,931 31.104 6,459,200
Pialign-adaptive 19.624 841,931 30.926 6,459,200
Hier-combin 20.32 876,059 31.29 7,235,342 32.03 28,877,149
Table 2: BLEU scores and phrase table size by alignment method and probabilities estimation method.
Pialign was run with five samples. Because of computational overhead, the baseline Pialign-batch and
Pialign-adaptive were not run on the largest data set.
al., 2011). Extracted phrase pairs are linearly
combined by averaging the feature values.
3. GIZA-batch: Instead of splitting into each do-
main, the data set is merged as a single corpus
and then a heuristic GZA-based phrase extrac-
tion is performed, similar as GIZA-linear.
4. Pialign-batch: Similar to the GIZA-batch, a s-
ingle model is estimated from a single, merged
corpus. Since pialign cannot handle large data,
we did not experiment on the largest LDC data
set.
5. Pialign-adaptive: Alignment and phrase pairs
extraction are same to Pialign-batch, while
translation probabilities are estimated by the
adaptive method with monolingual topic in-
formation (Su et al, 2012). The method es-
tablished the relationship between the out-of-
domain bilingual corpus and in-domain mono-
lingual corpora via topic distribution to esti-
mate the translation probability.
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|tf , f?) ? P (tf |f?)
(5)
where ?(e?|tf , f?) is the probability of translating f?
into e? given the source-side topic f? , P (tf |f?) is
the phrase-topic distribution of f.
The method we proposed is named Hier-
combin. It extracts phrase pairs in the same way as
the Pialign-linear. In the phrase table combination
process, the translation probability of each phrase
pair is estimated by the Hier-combin and the other
features are also linearly combined by averaging
the feature values. Pialign is used with default pa-
rameters. The parameter ?samps? is set to 5, which
indicates 5 samples are generated for a sentence
pair.
The IWSLT data consists of roughly 2, 000 sen-
tences and 3, 000 sentences each from the HIT and
BTEC for development purposes, and the test da-
ta consists of 1, 000 sentences. For the FBIS and
LDC task, we used NIST MT 2002 and 2004 for
development and testing purposes, consisting of
878 and 1, 788 sentences respectively. We em-
ploy Moses, an open-source toolkit for our exper-
iment (Koehn et al, 2007). SRILM Toolkit (Stol-
cke, 2002) is employed to train 4-gram language
models on the Xinhua portion of Gigaword cor-
pus, while for the IWLST2012 data set, only its
training set is used. We use batch-MIRA (Cher-
ry and Foster, 2012) to tune the weight for each
feature and translation quality is evaluated by the
case-insensitive BLEU-4 metric (Papineni et al,
2002). The BLEU scores reported in this paper
are the average of 5 independent runs of indepen-
dent batch-MIRA weight training, as suggested by
(Clark et al, 2011).
5.2 Result and Analysis
5.2.1 Performances of various extraction
methods
We carry out a series of experiments to evaluate
translation performance. The results are listed in
Table 2. Our method significantly outperforms the
baseline Pialign-linear. Except for the translation
probabilities, the phrase pairs of two methods are
exactly same, so the number of phrase pairs are
equal in the two methods. Further more, the per-
formance of the baseline Pialign-adaptive is also
higher than the baseline Pialign-linear?s and lower
than ours. This proves that the adaptive method
807
Methods Task Time(minute)
Batch Retraining 536.9
Hierarchical Parallel Extraction 122.55
Combination Integrating 1.5
Total 124.05
Table 3: Minutes used for alignment and phase
pair extraction in the FBIS data set.
with monolingual topic information is useful in
the tasks, but our approach with the hierarchical
Pitman-Yor process can estimate more accurate
translation probabilities based on all the data from
various domains.
Compared with the GIZA-batch, our approach
achieves competitive performance with a much s-
maller phrase table. The number of phase pairs
generated by our method is only 73.9%, 52.7%,
and 45.4% of the GIZA-batch?s respectively. In
the IWLST2012 data set, there is a huge difference
gap between the HIT corpus and the BTEC corpus,
and our method gains 0.814 BLEU improvement.
While the FBIS data set is artificially divided and
no clear human assigned differences among sub-
domains, our method loses 0.09 BLEU.
In the framework we proposed, phrase pairs are
extracted from each domain completely indepen-
dent of each other, so those tasks can be executed
on different machines, at different times, and of
course in parallel when we assume that the do-
mains are not incrementally added in the train-
ing data. The runtime of our approach and the
batch-based ITGs sampling method in the FBIS
data set is listed in Table 3 measured on a 2.7 GHz
E5-2680 CPU and 128 Gigabyte memory. When
comparing the hier-combin with the pialign-batch,
the BLEU scores are a little higher while the time
spent for training is much lower, almost one quar-
ter of the pialign-batch.
Even the performance of the pialign-linear is
better than the Baseline GIZA-linear?s, which
means that phrase pair extraction with hierarchi-
cal phrasal ITGs and sampling is more suitable
for domain adaptation tasks than the combination
GIZA++ and a heuristic method.
Generally, the hierarchical combination method
exploits the nature of a hierarchical Pitman-Yor
process and gains the advantage of its smoothing
effect, and our approach can incrementally gener-
ate a succinct phrase table based on all the data
from various domains with more accurate prob-
abilities. Traditional SMT phrase pair extraction
is batch-based, while our method has no obvious
shortcomings in translation accuracy, not to men-
tion efficiency.
5.2.2 Effect of Integration Order
Here, we evaluate whether our hierarchical com-
bination is sensitive to the order of the domains
when forming a hierarchical structure. Through
Equation (3), in our experiments, we chained the
domains in the order listed in Table 1, which is
in almost chronological order. Table 4 shows the
BLEU scores for the three data sets, in which the
order of combining phrase tables from each do-
main is alternated in the ascending and descending
of the similarity to the test data. The similarity be-
tween the data from each domain and the test data
is calculated using the perplexity measure with 5-
gram language model. The model learned from
the domain more similar to the test data is placed
in the front so that it can largely influence the
parameter computation with less backoff effects.
There is a big difference between the two opposite
order in IWSLT 2012 data set, in which more than
one point of decline in BLEU score when taking
the BTEC corpus as the first layer. Note that the
perplexity of BTEC was 344.589 while that of HIT
was 107.788. The result may indicate that our hi-
erarchical phrase combination method is sensitive
to the integration order when the training data is
small and there exists large gap in the similarity.
However, if most domains are similar (FBIS data
set) or if there are enough parallel sentence pairs
(NIST data set) in each domain, then the transla-
tion performances are almost similar even with the
opposite integrating orders.
IWSLT FBIS LDC
Descending 20.154 30.491 31.268
Ascending 19.066 30.388 31.254
Difference 1.088 0.103 0.014
Table 4: BLEU scores for the hierarchical model
with different integrating orders. Here Pialign was
run without multi-samples.
6 Conclusion and Future Work
In this paper, we present a novel hierarchical
phrase table combination method for SMT, which
can exploit more of the potential from all of da-
ta coming from various fields and generate a suc-
808
cinct phrase table with more accurate translation
probabilities. The method assumes that a com-
bined model is derived from a hierarchical Pitman-
Yor process with each prior learned separately in
each domain, and achieves BLEU scores competi-
tive with traditional batch-based ones. Meanwhile,
the framework has natural characteristics for par-
allel and incremental phrase pair extraction. The
experiment results on three different data sets in-
dicate the effectiveness of our approach.
In future work, we will also introduce incre-
mental learning for phase pair extraction inside a
domain, which means using the current translation
probabilities already obtained as the base measure
of sampling parameters for the upcoming domain.
Furthermore, we will investigate any tradeoffs be-
tween the accuracy of the probability estimation
and the coverage of phrase pairs.
Acknowledgments
We would like to thank our colleagues in both
HIT and NICT for insightful discussions, and
three anonymous reviewers for many invaluable
comments and suggestions to improve our paper.
This work is supported by National Natural Sci-
ence Foundation of China (61100093, 61173073,
61073130, 61272384), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207).
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238?241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 858?867, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada, June. Association for
Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion
transduction grammar for joint phrasal translation
modeling. In Proceedings of SSST, NAACL-HLT
2007/AMTA Workshop on Syntax and Structure in
Statistical Translation, pages 17?24.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2008. The complexi-
ty of phrase alignment problems. In Proceedings of
ACL-08: HLT, Short Papers, pages 25?28, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 128?135.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Martinez, and
Francisco Casacuberta. 2011. Fast incremental ac-
tive learning for statistical machine translation. A-
VANCES EN INTELIGENCIA ARTIFICIAL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 45?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756?764. Association for Compu-
tational Linguistics.
809
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
177?186, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
Technology (TIST), 2(3):1?18.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 343?350, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistic-
s: Human Language Technologies, pages 632?641,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Graham Neubig, Taro Watanabe, Shinsuke Mori, and
Tatsuya Kawahara. 2012. Machine translation with-
out words through substring alignment. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 165?174, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In International Joint Conference on
Natural Language Processing, pages 661?668.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, X-
iaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 459?468.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985?992. Association for Computa-
tional Linguistics.
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In Proceedings of the Conference of the Association
for Machine translation, Americas.
F. Wood and Y. W. Teh. 2009. A hierarchical non-
parametric Bayesian approach to statistical language
model domain adaptation. In Proceedings of the In-
ternational Conference on Artificial Intelligence and
Statistics, volume 12.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann
Ney. 2007. Domain dependent statistical machine
translation. In Proceedings of the MT Summit XI.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
810
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841?851,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Part-of-Speech Induction in Dependency Trees for Statistical Machine
Translation
Akihiro Tamura?,?, Taro Watanabe?, Eiichiro Sumita?,
Hiroya Takamura?, Manabu Okumura?
? National Institute of Information and Communications Technology
{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp
? Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura, oku}@pi.titech.ac.jp
Abstract
This paper proposes a nonparametric
Bayesian method for inducing Part-of-
Speech (POS) tags in dependency trees
to improve the performance of statistical
machine translation (SMT). In particular,
we extend the monolingual infinite tree
model (Finkel et al, 2007) to a bilin-
gual scenario: each hidden state (POS tag)
of a source-side dependency tree emits a
source word together with its aligned tar-
get word, either jointly (joint model), or
independently (independent model). Eval-
uations of Japanese-to-English translation
on the NTCIR-9 data show that our in-
duced Japanese POS tags for dependency
trees improve the performance of a forest-
to-string SMT system. Our independent
model gains over 1 point in BLEU by re-
solving the sparseness problem introduced
in the joint model.
1 Introduction
In recent years, syntax-based SMT has made
promising progress by employing either depen-
dency parsing (Lin, 2004; Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008; Mi and Liu,
2010) or constituency parsing (Huang et al, 2006;
Liu et al, 2006; Galley et al, 2006; Mi and Huang,
2008; Zhang et al, 2008; Cohn and Blunsom,
2009; Liu et al, 2009; Mi and Liu, 2010; Zhang
et al, 2011) on the source side, the target side,
or both. However, dependency parsing, which
is a popular choice for Japanese, can incorporate
only shallow syntactic information, i.e., POS tags,
compared with the richer syntactic phrasal cate-
gories in constituency parsing. Moreover, exist-
ing POS tagsets might not be optimal for SMT
because they are constructed without considering
the language in the other side. Consider the ex-
amples in Figure 1. The Japanese noun ???? in
? ? ?? ?? ? ??
??? ? ??????? ? ???? ??
You can not use the Internet  .
I  pay  usage fees  .
noun particle particlenoun noun verb auxiliary verb
noun particle noun noun verbparticle
[Example 1]
[Example 2]
Japanese POS:
Japanese POS:
Figure 1: Examples of Existing Japanese POS
Tags and Dependency Structures
Example 1 corresponds to the English verb ?use?,
while that in Example 2 corresponds to the English
noun ?usage?. Thus, Japanese nouns act like verbs
in English in one situation, and nouns in English
in another. If we could discriminate POS tags for
two cases, we might improve the performance of a
Japanese-to-English SMT system.
In the face of the above situations, this pa-
per proposes an unsupervised method for inducing
POS tags for SMT, and aims to improve the perfor-
mance of syntax-based SMT by utilizing the in-
duced POS tagset. The proposed method is based
on the infinite tree model proposed by Finkel et
al. (2007), which is a nonparametric Bayesian
method for inducing POS tags from syntactic de-
pendency structures. In this model, hidden states
represent POS tags, the observations they generate
represent the words themselves, and tree structures
represent syntactic dependencies between pairs of
POS tags.
The proposed method builds on this model by
incorporating the aligned words in the other lan-
guage into the observations. We investigate two
types of models: (i) a joint model and (ii) an in-
dependent model. In the joint model, each hid-
den state jointly emits both a source word and its
aligned target word as an observation. The in-
dependent model separately emits words in two
languages from hidden states. By inferring POS
841
tags based on bilingual observations, both mod-
els can induce POS tags by incorporating infor-
mation from the other language. Consider, for ex-
ample, inducing a POS tag for the Japanese word ?
??? in Figure 1. Under a monolingual induction
method (e.g., the infinite tree model), the ????
in Example 1 and 2 would both be assigned the
same POS tag since they share the same observa-
tion. However, our models would assign separate
tags for the two different instances since the ??
?? in Example 1 and Example 2 could be disam-
biguated by encoding the target-side information,
either ?use? or ?usage?, in the observations.
Inference is efficiently carried out by beam sam-
pling (Gael et al, 2008), which combines slice
sampling and dynamic programming. Experi-
ments are carried out on the NTCIR-9 Japanese-
to-English task using a binarized forest-to-string
SMT system with dependency trees as its source
side. Our bilingually-induced tagset signifi-
cantly outperforms the original tagset and the
monolingually-induced tagset. Further, our inde-
pendent model achieves a more than 1 point gain
in BLEU, which resolves the sparseness problem
introduced by the bi-word observations.
2 Related Work
A number of unsupervised methods have been
proposed for inducing POS tags. Early methods
have the problem that the number of possible POS
tags must be provided preliminarily. This limita-
tion has been overcome by automatically adjust-
ing the number of possible POS tags using non-
parametric Bayesian methods (Finkel et al, 2007;
Gael et al, 2009; Blunsom and Cohn, 2011; Sirts
and Aluma?e, 2012). Gael et al (2009) applied
infinite HMM (iHMM) (Beal et al, 2001; Teh
et al, 2006), a nonparametric version of HMM,
to POS induction. Blunsom and Cohn (2011)
used a hierarchical Pitman-Yor process prior to the
transition and emission distribution for sophisti-
cated smoothing. Sirts and Aluma?e (2012) built a
model that combines POS induction and morpho-
logical segmentation into a single learning prob-
lem. Finkel et al (2007) proposed the infinite
tree model, which represents recursive branching
structures over infinite hidden states and induces
POS tags from syntactic dependency structures. In
the following, we overview the infinite tree model,
which is the basis of our proposed model. In par-
ticular, we will describe the independent children
H ?k
?k? z1
z2 z3
x1 x2 x3k=1,?,C
Hk
k
~
),...,(Dirichlet~|? ???pi
Figure 2: A Graphical Representation of the Finite
Tree Model
model (Finkel et al, 2007), where children are
dependent only on their parents, used in our pro-
posed model1.
2.1 Finite Tree Model
We first review the finite tree model, which can
be graphically represented in Figure 2. Let
Tt denote the tree whose root node is t. A
node t has a hidden state zt (the POS tag)
and an observation xt (the word). The prob-
ability of a tree Tt, pT (Tt), is recursively de-
fined: pT (Tt) = p(xt|zt)
?
t??c(t)
p(zt? |zt)pT (Tt?),
where c(t) is the set of the children of t.
Let each hidden state variable have C possible
values indexed by k. For each state k, there is
a parameter ?k which parameterizes the observa-
tion distribution for that state: xt|zt ? F (?zt). ?k
is distributed according to a prior distribution H:
?k ? H .
Transitions between states are governed by
Markov dynamics parameterized by pi, where
?ij = p(zc(t) = j|zt = i) and pik are the transition
probabilities from the parent?s state k. pik is dis-
tributed according to a Dirichlet distribution with
parameter ?: pik|? ? Dirichlet(?, . . . , ?). The
hidden state of each child zt? is distributed accord-
ing to a multinomial distributionpizt specific to the
parent?s state zt: zt? |zt ? Multinomial(pizt).
2.2 Infinite Tree Model
In the infinite tree model, the number of possible
hidden states is potentially infinite. The infinite
model is formed by extending the finite tree model
using a hierarchical Dirichlet process (HDP) (Teh
et al, 2006). The reason for using an HDP rather
1Finkel et al (2007) originally proposed three types of
models: besides the independent children model, the simul-
taneous children model and the markov children model. Al-
though we could apply the other two models, we leave this
for future work.
842
H ?k
?k?0 z1
z2 z3
x1 x2 x3?
? ?
Hk
k
~
),(DP~,|
)(GEM~|
00? ????pi
???
Figure 3: A Graphical Representation of the Infi-
nite Tree Model
than a simple Dirichlet process (DP)2 (Ferguson,
1973) is that we have to introduce coupling across
transitions from different parent?s states. A similar
measure was adopted in iHMM (Beal et al, 2001).
HDP is a set of DPs coupled through a shared
random base measure which is itself drawn from
a DP: each Gk ? DP(?0, G0) with a shared base
measure G0, and G0 ? DP(?,H) with a global
base measure H . From the viewpoint of the stick-
breaking construction3 (Sethuraman, 1994), the
HDP is interpreted as follows: G0 =
??
k?=1
?k???k?
and Gk =
??
k?=1
?kk???k? , where ? ? GEM(?),
pik ? DP(?0,?), and ?k? ? H .
We regard each Gk as two coindexed distribu-
tions: pik, a distribution over the transition prob-
abilities from the parent?s state k, and ?k? , an ob-
servation distribution for the state k?. Then, the
infinite tree model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt).
Figure 3 shows the graphical representation of the
infinite tree model. The primary difference be-
2DP is a measure on measures. It has two parameters, a
scaling parameter ? and a base measure H: DP (?,H).
3Sethuraman (1994) showed a definition of a measure
G ? DP(?0, G0). First, infinite sequences of i.i.d variables
(??k)?k=1 and (?k)?k=1 are generated: ??k|?0 ? Beta(1, ?0),
?k ? G0. Then, G is defined as: ?k = ??k
?k?1
l=1 (1 ? ??l),
G = ??k=1 ?k??k . If pi is defined by this process, then we
write pi ? GEM(?0).
H ?k
?k?0
?
? ? z1
z2 z3
z4 z5 z6
???
+pay? ??????+fees? ???+usage???+I? ???
Figure 4: An Example of the Joint Model
tween Figure 2 and Figure 3 is whether the number
of copies of the state is finite or not.
3 Bilingual Infinite Tree Model
We propose a bilingual variant of the infinite tree
model, the bilingual infinite tree model, which uti-
lizes information from the other language. Specifi-
cally, the proposed model introduces bilingual ob-
servations by embedding the aligned target words
in the source-side dependency trees. This paper
proposes two types of models that differ in their
processes for generating observations: the joint
model and the independent model.
3.1 Joint Model
The joint model is a simple application of the in-
finite tree model under a bilingual scenario. The
model is formally defined in the same way as in
Section 2.2 and is graphically represented simi-
larly to Figure 3. The only difference from the
infinite tree model is the instances of observations
(xt). Observations in the joint model are the com-
bination of source words and their aligned target
words4, while observations in the monolingual in-
finite tree model represent only source words. For
each source word, all the aligned target words are
copied and sorted in alphabetical order, and then
concatenated into a single observation. Therefore,
a single target word may be emitted multiple times
if the target word is aligned with multiple source
words. Likewise, there may be target words which
may not be emitted by our model, if the target
words are not aligned.
Figure 4 shows the process of generating Exam-
ple 2 in Figure 1 through the joint model, where
aligned words are jointly emitted as observations.
In Figure 4, the POS tag of ???? (z5) generates
4When no target words are aligned, we simply add a
NULL target word.
843
H ?k
?k?0
? ? z1
z2 z3
z4 z5
H? ?'k
?? pay
I?
???
??
? NONE NONEusage
fees z6
?
'~',~
),(DP~,|
)(GEM~|
00 HH kk
k ?? ????pi
???
Figure 5: A Graphical Representation of the Inde-
pendent Model
the string ???+usage? as the observation (x5).
Similarly, the POS tag of ???? in Example 1
would generate the string ???+use?. Hence, this
model can assign different POS tags to the two dif-
ferent instances of the word ????, based on the
different observation distributions in inference.
3.2 Independent Model
The joint model is prone to a data sparseness prob-
lem, since each observation is a combination of a
source word and its aligned target word. Thus, we
propose an independent model, where each hidden
state generates a source word and its aligned target
word separately. For the aligned target side, we in-
troduce an observation variable x?t for each zt and
a parameter ??k for each state k, which parame-
terizes a distinct distribution over the observations
x?t for that state. ??k is distributed according to a
prior distribution H ?. Specifically, the indepen-
dent model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H, ??k ? H ?,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt), x?t|zt ? F ?(??zt).
When multiple target words are aligned to a single
source word, each aligned word is generated sepa-
rately from observation distribution parameterized
by ??k.
Figure 5 graphs the process of generating Ex-
ample 2 in Figure 1 using the independent model.
x?t and ??k are introduced for aligned target words.
The state of ???? (z5) generates the Japanese
word ???? as x5 and the English word ?usage?
as x?5. Due to this factorization, the independent
model is less subject to the sparseness problem.
3.3 Introduction of Other Factors
We assumed the surface form of aligned target
words as additional observations in previous sec-
tions. Here, we introduce additional factors, i.e.,
the POS of aligned target words, in the observa-
tions. Note that POSs of target words are assigned
by a POS tagger in the target language and are not
inferred in the proposed model.
First, we can simply replace surface forms of
target words with their POSs to overcome the
sparseness problem. Second, we can incorporate
both information from the target language as ob-
servations. In the joint model, two pieces of in-
formation are concatenated into a single observa-
tion. In the independent model, we introduce ob-
servation variables (e.g., x?t and x??t ) and parame-
ters (e.g., ??k and ???k) for each information. Specif-
ically, x?t and ??k are introduced for the surface
form of aligned words, and x??t and ???k for the POS
of aligned words. Consider, for example, Example
1 in Figure 1. The POS tag of ???? generates the
string ???+use+verb? as the observation in the
joint model, while it generates ????, ?use?, and
?verb? independently in the independent model.
3.4 POS Refinement
We have assumed a completely unsupervised way
of inducing POS tags in dependency trees. An-
other realistic scenario is to refine the existing POS
tags (Finkel et al, 2007; Liang et al, 2007) so
that each refined sub-POS tag may reflect the in-
formation from the aligned words while preserv-
ing the handcrafted distinction from original POS
tagset. Major difference is that we introduce sep-
arate transition probabilities pisk and observation
distributions (?sk, ?
?s
k ) for each existing POS tag s.
Then, each node t is constrained to follow the dis-
tributions indicated by the initially assigned POS
tag st, and we use the pair (st, zt) as a state repre-
sentation.
3.5 Inference
In inference, we find the state set that maximizes
the posterior probability of state transitions given
observations (i.e., P (z1:n|x1:n)). However, we
cannot evaluate the probability for all possible
states because the number of states is infinite.
Finkel et al (2007) presented a sampling algo-
rithm for the infinite tree model, which is based on
the Gibbs sampling in the direct assignment rep-
resentation for iHMM (Teh et al, 2006). In the
844
Gibbs sampling, individual hidden state variables
are resampled conditioned on all other variables.
Unfortunately, its convergence is slow in HMM
settings because sequential data is likely to have
a strong correlation between hidden states (Gael
et al, 2008).
We present an inference procedure based on
beam sampling (Gael et al, 2008) for the joint
model and the independent model. Beam sam-
pling limits the number of possible state transi-
tions for each node to a finite number using slice
sampling (Neal, 2003), and then efficiently sam-
ples whole hidden state transitions using dynamic
programming. Beam sampling does not suffer
from slow convergence as in Gibbs sampling by
sampling the whole state variables at once. In ad-
dition, Gael et al (2008) showed that beam sam-
pling is more robust to initialization and hyperpa-
rameter choice than Gibbs sampling.
Specifically, we introduce an auxiliary variable
ut for each node in a dependency tree to limit
the number of possible transitions. Our procedure
alternates between sampling each of the follow-
ing variables: the auxiliary variables u, the state
assignments z, the transition probabilities pi, the
shared DP parameters ?, and the hyperparameters
?0 and ?. We can parallelize procedures in sam-
pling u and z because the slice sampling for u and
the dynamic programing for z are independent for
each sentence. See Gael el al. (2009) for details.
The only difference between inferences in the
joint model and the independent model is in com-
puting the posterior probability of state transi-
tions given observations (e.g., p(z1:n|x1:n) and
p(z1:n|x1:n, x?1:n)) in sampling z. In the follow-
ing, we describe each sampling stage. See Teh et
al., (2006) for details of sampling pi, ?, ?0 and ?.
Sampling u:
Each ut is sampled from the uniform distribu-
tion on [0, ?zd(t)zt ], where d(t) is the parent of
t: ut ? Uniform(0, ?zd(t)zt). Note that ut is a
positive number, since each transition probability
?zd(t)zt is larger than zero.
Sampling z:
Possible values k of zt are divided into the two
sets using ut: a finite set with ?zd(t)k > ut and
an infinite set with ?zd(t)k ? ut. The beam
sampling considers only the former set. Owing
to the truncation of the latter set, we can compute
the posterior probability of a state zt given ob-
servations for all t (t = 1, . . . , T ) using dynamic
programming as follows:
In the joint model, p(zt|x?(t), u?(t)) ?
p(xt|zt) ?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), u?(d(t))),
and in the independent model,
p(zt|x?(t), x??(t), u?(t)) ? p(xt|zt) ? p(x?t|zt)
?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), x??(d(t)), u?(d(t))),
where x?(t) (or u?(t)) denotes the set of xt (or ut)
on the path from the root node to the node t in a
tree.
In our experiments, we assume that F (?k)
is Multinomial(?k) and H is Dirichlet(?, . . . , ?),
which is the same in Finkel et al (2007). Un-
der this assumption, the posterior probability of an
observation is as follows: p(xt|zt) =
n?xtk + ?
n??k + N?
,
where n?xk is the number of observations x with
state k, n??k is the number of hidden states whose
values are k, and N is the total number of observa-
tions x. Similarly, p(x?t|zt) =
n?x?tk + ?
?
n??k + N ???
, where
N ? is the total number of observations x?.
When the posterior probability of a state zt
given observations for all t can be computed,
we first sample the state of each leaf node and
then perform backtrack sampling for every other
zt where the zt is sampled given the sample
for zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ?
p(zt|x?(t), u?(t))
?
t??c(t) p(zt? |zt, ut?).
Sampling pi:
We introduce a count variable nij ? n,
which is the number of observations with
state j whose parent?s state is i. Then,
we sample pi using the Dirichlet distri-
bution: (?k1, . . . , ?kK ,
??
k?=K+1 ?kk?) ?
Dirichlet(nk1 + ?0?1, . . . , nkK +
?0?K , ?0
??
k?=K+1 ?k?), where K is the
number of distinct states in z.
Sampling ?:
We introduce a set of auxiliary variables m, where
mij ? m is the number of elements of pij
corresponding to ?i. The conditional distribu-
tion of each variable is p(mij = m|z,?, ?0) ?
S(nij ,m)(?0?j)m, where S(n,m) are unsigned
Stirling numbers of the first kind5.
5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0,
S(n,m) = 0 for m > n, and S(n + 1,m) = S(n,m ?
1) + nS(n,m) for others.
845
The parameters ? are sampled using the Dirich-
let distribution: (?1, . . . , ?K ,
??
k?=K+1 ?k?) ?
Dirichlet(m?1, . . . ,m?K , ?), where m?k =?K
k?=1 mk?k.
Sampling ?0:
?0 is parameterized by a gamma hyperprior
with hyperparameters ?a and ?b. We introduce
two types of auxiliary variables for each state
(k = 1, . . . ,K), wk ? [0, 1] and vk ? {0, 1}.
The conditional distribution of each wk is
p(wk|?0) ? w?0k (1?wk)n?k?1 and that of each vk
is p(vk|?0) ? (
n?k
?0
)
vk
, where n?k =
?K
k?=1 nk?k.
The conditional distribution of ?0 given wk
and vk (k = 1, . . . ,K) is p(?0|w,v) ?
??a?1+m..?
?K
k=1 vk
0 e??0(?b?
?K
k=1 logwk), where
m?? =
?K
k?=1
?K
k??=1 mk?k?? .
Sampling ?:
? is parameterized by a gamma hyperprior with
hyperparameters ?a and ?b. We introduce an
auxiliary variable ?, whose conditional distribu-
tion is p(?|?) ? ??(1 ? ?)m???1. The con-
ditional distribution of ? given ? is p(?|?) ?
??a?1+Ke??(?b?log?).
4 Experiment
We tested our proposed models under the
NTCIR-9 Japanese-to-English patent translation
task (Goto et al, 2011), consisting of approxi-
mately 3.2 million bilingual sentences. Both the
development data and the test data consist of 2,000
sentences. We also used the NTCIR-7 develop-
ment data consisting of 2,741 sentences for devel-
opment testing purposes.
4.1 Experimental Setup
We evaluated our bilingual infinite tree model
for POS induction using an in-house developed
syntax-based forest-to-string SMT system. In
the training process, the following steps are per-
formed sequentially: preprocessing, inducing a
POS tagset for a source language, training a POS
tagger and a dependency parser, and training a
forest-to-string MT model.
Step 1. Preprocessing
We used the first 10,000 Japanese-English sen-
tence pairs in the NTCIR-9 training data for in-
ducing a POS tagset for Japanese6. The Japanese
sentences were segmented using MeCab7, and the
English sentences were tokenized and POS tagged
using TreeTagger (Schmid, 1994), where 43 and
58 types of POS tags are included in the Japanese
sentences and the English sentences, respectively.
The Japanese POS tags come from the second-
level POS tags in the IPA POS tagset (Asahara and
Matsumoto, 2003) and the English POS tags are
derived from the Penn Treebank. Note that the
Japanese POS tags are used for initialization of
hidden states and the English POS tags are used
as observations emitted by hidden states.
Word-by-word alignments for the sentence
pairs are produced by first running GIZA++ (Och
and Ney, 2003) in both directions and then com-
bining the alignments using the ?grow-diag-final-
and? heuristic (Koehn et al, 2003). Note that we
ran GIZA++ on all of the NTCIR-9 training data
in order to obtain better alignements.
The Japanese sentences are parsed using
CaboCha (Kudo and Matsumoto, 2002), which
generates dependency structures using a phrasal
unit called a bunsetsu8, rather than a word unit as
in English or Chinese dependency parsing. Since
we focus on the word-level POS induction, each
bunsetsu-based dependency tree is converted into
its corresponding word-based dependency tree us-
ing the following heuristic9: first, the last func-
tion word inside each bunsetsu is identified as
the head word10; then, the remaining words are
treated as dependents of the head word in the same
bunsetsu; finally, a bunsetsu-based dependency
structure is transformed to a word-based depen-
dency structure by preserving the head/modifier
relationships of the determined head words.
Step 2. POS Induction
A POS tag for each word in the Japanese sentences
is inferred by our bilingual infinite tree model, ei-
6Due to the high computational cost, we did not use all
the NTCIR-9 training data. We leave scaling up to a larger
dataset for future work.
7http://mecab.googlecode.com/svn/
trunk/mecab/doc/index.html
8A bunsetsu is the smallest meaningful sequence con-
sisting of a content word and accompanying function words
(e.g., a noun and a particle).
9We could use other word-based dependency trees such
as trees by the infinite PCFG model (Liang et al, 2007)
and syntactic-head or semantic-head dependency trees in
Nakazawa and Kurohashi (2012), although it is not our major
focus. We leave this for future work.
10If no function words exist in a bunsetsu, the last content
word is treated as the head word.
846
ther jointly (Joint) or independently (Ind). We
also performed monolingual induction of Finkel et
al. (2007) for comparison (Mono). In each model,
a sequence of sampling u, z, pi, ?, ?0, and ? is
repeated 10,000 times. In sampling ?0 and ?, hy-
perparameters ?a, ?b, ?a, and ?b are set to 2, 1,
1, and 1, respectively, which is the same setting in
Gael et al (2008). In sampling z, parameters ?, ??,
. . ., are set to 0.01. In the experiments, three types
of factors for the aligned English words are com-
pared: surface forms (?s?), POS tags (?P?), and the
combination of both (?s+P?). Further, two types of
inference frameworks are compared: induction
(IND) and refinement (REF ). In both frame-
works, each hidden state zt is first initialized to
the POS tags assigned by MeCab (the IPA POS
tagset), and then each state is updated through
the inference procedure described in Section 3.5.
Note that in REF , the sampling distribution over
zt is constrained to include only states that are a
refinement of the initially assigned POS tag.
Step 3. Training a POS Tagger and a
Dependency Parser
In this step, we train a Japanese dependency parser
from the 10,000 Japanese dependency trees with
the induced POS tags which are derived from Step
2. We employed a transition-based dependency
parser which can jointly learn POS tagging and
dependency parsing (Hatori et al, 2011) under an
incremental framework11. Note that the learned
parser can identify dependencies between words
and attach an induced POS tag for each word.
Step 4. Training a Forest-to-String MT
In this step, we train a forest-to-string MT model
based on the learned dependency parser in Step 3.
We use an in-house developed hypergraph-based
toolkit, cicada, for training and decoding with a
tree-to-string model, which has been successfully
employed in our previous work for system com-
bination (Watanabe and Sumita, 2011) and online
learning (Watanabe, 2012). All the Japanese and
English sentences in the NTCIR-9 training data
are segmented in the same way as in Step 1, and
then each Japanese sentence is parsed by the de-
pendency parser learned in Step 3, which simul-
taneously assigns induced POS tags and word de-
pendencies. Finally, a forest-to-string MT model
is learned with Zhang et al, (2011), which ex-
tracts translation rules by a forest-based variant of
11http://triplet.cc/software/corbit/
IND REF
BS 27.54
Mono 27.66 26.83
Joint[s] 28.00 28.00
Joint[P] 26.36 26.72
Joint[s+P] 27.99 27.82
Ind[s] 28.00 27.93
Ind[P] 28.11 28.63
Ind[s+P] 28.13 28.62
Table 1: Performance on Japanese-to-English
Translation Measured by BLEU (%)
the GHKM algorithm (Mi and Huang, 2008) af-
ter each parse tree is restructured into a binarized
packed forest. Parameters are tuned on the devel-
opment data using xBLEU (Rosti et al, 2011) as
an objective and L-BFGS (Liu and Nocedal, 1989)
as an optimization toolkit, since it is stable and less
prone to randomness, unlike MERT (Och, 2003)
or PRO (Hopkins and May, 2011). The develop-
ment test data is used to set up hyperparameters,
i.e., to terminate tuning iterations.
When translating Japanese sentences, a parse
tree for each sentence is constructed in the same
way as described earlier in this step, and then the
parse trees are translated into English sentences
using the learned forest-to-string MT model.
4.2 Experimental Results
Table 1 shows the performance for the test data
measured by case sensitive BLEU (Papineni et
al., 2002). We also present the performance of
our baseline forest-to-string MT system (BS) us-
ing the original IPA POS tags. In Table 1, num-
bers in bold indicate that the systems outperform
the baselines, BS and Mono. Under the Moses
phrase-based SMT system (Koehn et al, 2007)
with the default settings, we achieved a 26.80%
BLEU score.
Table 1 shows that the proposed systems outper-
form the baseline Mono. The differences between
the performance of Ind[s+P] and Mono are statis-
tically significant in the bootstrap method (Koehn,
2004), with a 1% significance level both in IND
and REF . The results indicate that integrating the
aligned target-side information in POS induction
makes inferred tagsets more suitable for SMT.
Table 1 also shows that the independent model
is more effective for SMT than the joint model.
This means that sparseness is a severe problem in
847
Model IND REF
Joint[s+P] 164 620
Ind[s+P] 102 517
IPA POS tags 42
Table 2: The Number of POS Tags
POS induction when jointly encoding bilingual in-
formation into observations. Additionally, all the
systems using the independent model outperform
BS. The improvements are statistically significant
in the bootstrap method (Koehn, 2004), with a 1%
significance level. The results show that the pro-
posed models can generate more favorable POS
tagsets for SMT than an existing POS tagset.
In Table 1, REF s are at least comparable to, or
better than, INDs except for Mono. This shows
that REF achieves better performance by preserv-
ing the clues from the original POS tagset. How-
ever, REF may suffer sever overfitting problem
for Mono since no bilingual information was in-
corporated. Further, when the full-level IPA POS
tags12 were used in BS, the system achieved a
27.49% BLEU score, which is worse than the re-
sult using the second-level IPA POS tags. This
means that manual refinement without bilingual
information may also cause an overfitting problem
in MT.
5 Discussion
5.1 Comparison to the IPA POS Tagset
Table 2 shows the number of the IPA POS tags
used in the experiments and the POS tags induced
by the proposed models. This table shows that
each induced tagset contains more POS tags than
the IPA POS tagset. In the experimental data,
some of Japanese verbs correspond to genuine En-
glish verbs, some are nominalized, and others cor-
respond to English past participle verbs or present
participle verbs which modify other words. Re-
spective examples are ?I use a card.?, ?Using the
index is faster.?, and ?I explain using an exam-
ple.?, where all the underlined words correspond
to the same Japanese word, ????, whose IPA
POS tag is a verb. Ind[s+P] in REF generated
the POS tagset where the three types are assigned
to separate POS groups.
The Japanese particle ??? is sometimes at-
tached to nouns to give them adverb roles. For
12377 types of full-level IPA POS tags were included in our
experimental data.
Tagging Dependency
IND REF IND REF
Original 90.37 93.62
Mono 90.75 88.04 91.77 91.51
Joint[s] 89.08 86.73 91.55 91.14
Joint[P] 80.54 79.98 91.06 91.29
Joint[s+P] 87.56 84.92 91.31 91.10
Ind[s] 87.62 84.33 92.06 92.58
Ind[P] 90.21 88.50 92.85 93.03
Ind[s+P] 89.57 86.12 92.96 92.78
Table 3: Tagging and Dependency Accuracy (%)
example, ??? (mutual) ??? is translated as
the adverb ?mutually? in English. Other times,
it is attached to words to make them the objects
of verbs. For example, ?? (he) ??????
(give)? is translated as ?give him?. The POS tags
by Ind[s+P] in REF discriminated the two types.
These examples show that the proposed mod-
els can disambiguate POS tags that have different
functions in English, whereas the IPA POS tagset
treats them jointly. Thus, such discrimination im-
proves the performance of a forest-to-string SMT.
5.2 Impact of Tagging and Dependency
Accuracy
The performance of our methods depends not only
on the quality of the induced tag sets but also on
the performance of the dependency parser learned
in Step 3 of Section 4.1. We cannot directly eval-
uate the tagging accuracy of the parser trained
through Step 3 because we do not have any data
with induced POS tags other than the 10,000-
sentence data gained through Step 2. Thus we split
the 10,000 data into the first 9,000 data for train-
ing and the remaining 1,000 for testing, and then
a dependency parser was learned in the same way
as in Step 3.
Table 3 shows the results. Original is the per-
formance of the parser learned from the training
data with the original POS tagset. Note that the de-
pendency accuracies are measured on the automat-
ically parsed dependency trees, not on the syntac-
tically correct gold standard trees. Thus Original
achieved the best dependency accuracy.
In Table 3, the performance for our bilingually-
induced POSs, Joint and Ind, are lower than
Original and Mono. It seems performing pars-
ing and tagging with the bilingually-induced POS
tagset is too difficult when only monolingual in-
848
formation is available to the parser. However, our
bilingually-induced POSs, except for Joint[P ],
with the lower accuracies are more effective for
SMT than the monolingually-induced POSs and
the original POSs, as indicated in Table 1. The
tagging accuracies for Joint[P ] both in IND and
REF are significantly lower than the others, while
the dependency accuracies do not differ signifi-
cantly. The lower tagging accuracies may directly
reflect the lower translation qualities for Joint[P ]
in Table 1.
6 Conclusion
We proposed a novel method for inducing POS
tags for SMT. The proposed method is a non-
parametric Bayesian method, which infers hidden
states (i.e., POS tags) based on observations repre-
senting not only source words themselves but also
aligned target words. Our experiments showed
that a more favorable POS tagset can be induced
by integrating aligned information, and further-
more, the POS tagset generated by the proposed
method is more effective for SMT than an existing
POS tagset (the IPA POS tagset).
Even though we employed word alignment
from GIZA++ with potential errors, large gains
were achieved using our proposed method. We
would like to investigate the influence of align-
ment errors in the future. In addition, we are plan-
ning to prove the effectiveness of our proposed
method for language pairs other than Japanese-to-
English. We are also planning to introduce our
proposed method to other syntax-based SMT, such
as a string-to-tree SMT and a tree-to-tree SMT.
Acknowledgments
We thank Isao Goto for helpful discussions and
anonymous reviewers for valuable comments. We
also thank Jun Hatori for helping us to apply his
software, Corbit, to our induced POS tagsets.
References
Masayuki Asahara and Yuji Matsumoto. 2003.
IPADIC User Manual. Technical report, Japan.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2001. The Infinite Hidden Markov Model.
In Advances in Neural Information Processing Sys-
tems, pages 577?584.
Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of
Speech Induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 865?874.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
Model of Syntax-Directed Tree to String Grammar
Induction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 352?361.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 541?548.
Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The Infinite Tree. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 272?279.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam Sampling for
the Infinite Hidden Markov Model. In Proceedings
of the 25th International Conference on Machine
Learning, pages 1088?1095.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsuper-
vised PoS tagging. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, pages 678?687.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961?968.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559?578.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental Joint POS Tag-
ging and Dependency Parsing in Chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-Directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop on
849
Computationally Hard Problemsand Joint Inference
in Speech and Language Processing, pages 1?8.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning, pages 63?69.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The Infinite PCFG using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 688?697.
Dekang Lin. 2004. A Path-based Transfer Model for
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 625?630.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improv-
ing Tree-to-Tree Translation with Packed Forests.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, pages 558?566.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 206?214.
Haitao Mi and Qun Liu. 2010. Constituency to De-
pendency Translation with Forests. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, pages 1433?1442.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by Bilingual Generation and Monolin-
gual Derivation. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1963?1978.
Radford M. Neal. 2003. Slice Sampling. Annals of
Statistics, 31:705?767.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Conference of the Association for Computa-
tional Linguistics, pages 271?279.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 159?165.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49.
Jayaram Sethuraman. 1994. A Constructive Definition
of Dirichlet Priors. Statistica Sinica, 4(2):639?650.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 577?
585.
Kairit Sirts and Tanel Aluma?e. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 407?416.
850
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
Processes. Journal of the American Statistical Asso-
ciation, 101(476):1566?1581.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
Translation System Combination by Confusion For-
est. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1249?1257.
Taro Watanabe. 2012. Optimized Online Rank Learn-
ing for Machine Translation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 253?262.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 559?
567.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized Forest to String Translation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 19?24.
851
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470?1480,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Recurrent Neural Networks for Word Alignment Model
Akihiro Tamura
?
, Taro Watanabe, Eiichiro Sumita
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
a-tamura@ah.jp.nec.com,
{taro.watanabe, eiichiro.sumita}@nict.go.jp
Abstract
This study proposes a word alignment
model based on a recurrent neural net-
work (RNN), in which an unlimited
alignment history is represented by re-
currently connected hidden layers. We
perform unsupervised learning using
noise-contrastive estimation (Gutmann
and Hyv?arinen, 2010; Mnih and Teh,
2012), which utilizes artificially generated
negative samples. Our alignment model is
directional, similar to the generative IBM
models (Brown et al, 1993). To overcome
this limitation, we encourage agreement
between the two directional models by
introducing a penalty function that en-
sures word embedding consistency across
two directional models during training.
The RNN-based model outperforms
the feed-forward neural network-based
model (Yang et al, 2013) as well as the
IBM Model 4 under Japanese-English
and French-English word alignment
tasks, and achieves comparable transla-
tion performance to those baselines for
Japanese-English and Chinese-English
translation tasks.
1 Introduction
Automatic word alignment is an important task for
statistical machine translation. The most classical
approaches are the probabilistic IBM models 1-5
(Brown et al, 1993) and the HMM model (Vogel
et al, 1996). Various studies have extended those
models. Yang et al (2013) adapted the Context-
Dependent Deep Neural Network for HMM (CD-
DNN-HMM) (Dahl et al, 2012), a type of feed-
forward neural network (FFNN)-based model, to
?
The first author is now affiliated with Knowledge
Discovery Research Laboratories, NEC Corporation, Nara,
Japan.
the HMM alignment model and achieved state-of-
the-art performance. However, the FFNN-based
model assumes a first-order Markov dependence
for alignments.
Recurrent neural network (RNN)-based models
have recently demonstrated state-of-the-art per-
formance that outperformed FFNN-based models
for various tasks (Mikolov et al, 2010; Mikolov
and Zweig, 2012; Auli et al, 2013; Kalchbrenner
and Blunsom, 2013; Sundermeyer et al, 2013).
An RNN has a hidden layer with recurrent con-
nections that propagates its own previous signals.
Through the recurrent architecture, RNN-based
models have the inherent property of modeling
long-span dependencies, e.g., long contexts, in in-
put data. We assume that this property would fit
with a word alignment task, and we propose an
RNN-based word alignment model. Our model
can maintain and arbitrarily integrate an alignment
history, e.g., bilingual context, which is longer
than the FFNN-based model.
The NN-based alignment models are super-
vised models. Unfortunately, it is usually dif-
ficult to prepare word-by-word aligned bilingual
data. Yang et al (2013) trained their model from
word alignments produced by traditional unsuper-
vised probabilistic models. However, with this
approach, errors induced by probabilistic mod-
els are learned as correct alignments; thus, gen-
eralization capabilities are limited. To solve this
problem, we apply noise-contrastive estimation
(NCE) (Gutmann and Hyv?arinen, 2010; Mnih
and Teh, 2012) for unsupervised training of our
RNN-based model without gold standard align-
ments or pseudo-oracle alignments. NCE artifi-
cially generates bilingual sentences through sam-
plings as pseudo-negative samples, and then trains
the model such that the scores of the original bilin-
gual sentences are higher than those of the sam-
pled bilingual sentences.
Our RNN-based alignment model has a direc-
1470
tion, such as other alignment models, i.e., from f
(source language) to e (target language) and from
e to f . It has been proven that the limitation may
be overcome by encouraging two directional mod-
els to agree by training them concurrently (Ma-
tusov et al, 2004; Liang et al, 2006; Grac?a et al,
2008; Ganchev et al, 2008). The motivation for
this stems from the fact that model and generaliza-
tion errors by the two models differ, and the mod-
els must complement each other. Based on this
motivation, our directional models are also simul-
taneously trained. Specifically, our training en-
courages word embeddings to be consistent across
alignment directions by introducing a penalty term
that expresses the difference between embedding
of words into an objective function. This con-
straint prevents each model from overfitting to a
particular direction and leads to global optimiza-
tion across alignment directions.
This paper presents evaluations of Japanese-
English and French-English word alignment tasks
and Japanese-to-English and Chinese-to-English
translation tasks. The results illustrate that our
RNN-based model outperforms the FFNN-based
model (up to +0.0792 F1-measure) and the IBM
Model 4 (up to +0.0703 F1-measure) for the word
alignment tasks. For the translation tasks, our
model achieves up to 0.74% gain in BLEU as com-
pared to the FFNN-based model, which matches
the translation qualities of the IBM Model 4.
2 Related Work
Various word alignment models have been pro-
posed. These models are roughly clustered into
two groups: generative models, such as those pro-
posed by Brown et al (1993), Vogel et al (1996),
and Och and Ney (2003), and discriminative mod-
els, such as those proposed by Taskar et al (2005),
Moore (2005), and Blunsom and Cohn (2006).
2.1 Generative Alignment Model
Given a source language sentence f
J
1
= f
1
, ..., f
J
and a target language sentence e
I
1
= e
1
, ..., e
I
,
f
J
1
is generated by e
I
1
via the alignment a
J
1
=
a
1
, ..., a
J
. Each a
j
is a hidden variable indicat-
ing that the source word f
j
is aligned to the target
word e
a
j
. Usually, a ?null? word e
0
is added to
the target language sentence and a
J
1
may contain
a
j
= 0, which indicates that f
j
is not aligned to
any target word. The probability of generating the
sentence f
J
1
from e
I
1
is defined as
p(f
J
1
|e
I
1
) =
?
a
J
1
p(f
J
1
, a
J
1
|e
I
1
). (1)
The IBM Models 1 and 2 and the HMM model
decompose it into an alignment probability p
a
and
a lexical translation probability p
t
as
p(f
J
1
, a
J
1
|e
I
1
) =
J
?
j=1
p
a
(a
j
|a
j?1
, j)p
t
(f
j
|e
a
j
). (2)
The three models differ in their definition of align-
ment probability. For example, the HMM model
uses an alignment probability with a first-order
Markov property: p
a
(a
j
|a
j
? a
j?1
). In addition,
the IBM models 3-5 are extensions of these, which
consider the fertility and distortion of each trans-
lated word.
These models are trained using the expectation-
maximization algorithm (Dempster et al, 1977)
from bilingual sentences without word-level align-
ments (unlabeled training data). Given a specific
model, the best alignment (Viterbi alignment) of
the sentence pair (f
J
1
, e
I
1
) can be found as
a?
J
1
= argmax
a
J
1
p(f
J
1
, a
J
1
|e
I
1
). (3)
For example, the HMM model identifies the
Viterbi alignment using the Viterbi algorithm.
2.2 FFNN-based Alignment Model
As an instance of discriminative models, we de-
scribe an FFNN-based word alignment model
(Yang et al, 2013), which is our baseline. An
FFNN learns a hierarchy of nonlinear features
that can automatically capture complex statisti-
cal patterns in input data. Recently, FFNNs have
been applied successfully to several tasks, such as
speech recognition (Dahl et al, 2012), statistical
machine translation (Le et al, 2012; Vaswani et
al., 2013), and other popular natural language pro-
cessing tasks (Collobert and Weston, 2008; Col-
lobert et al, 2011).
Yang et al (2013) have adapted a type of FFNN,
i.e., CD-DNN-HMM (Dahl et al, 2012), to the
HMM alignment model. Specifically, the lexical
translation and alignment probability in Eq. 2 are
computed using FFNNs as
s
NN
(a
J
1
|f
J
1
, e
I
1
) =
J
?
j=1
t
a
(a
j
? a
j?1
|c(e
a
j?1
))
?t
lex
(f
j
, e
a
j
|c(f
j
), c(e
a
j
)), (4)
1471
Lookup
Layer
Hidden Layer
Output Layer
Input fj-1 e
L L L L L L 
htanh(H? +BH)
O? +BO
aj-1
t   ( ,    |      ,      )fj eaj ef j-1j+1lex
z0
z1
fj fj+1 eaj eaj+1
z0
z1
aj-1
aj+1
Figure 1: FFNN-based model for computing a lex-
ical translation score of (f
j
, e
a
j
)
where t
a
and t
lex
are an alignment score and a lex-
ical translation score, respectively, s
NN
is a score
of alignments a
J
1
, and ?c(a word w)? denotes a
context of word w. Note that the model uses non-
probabilistic scores rather than probabilities be-
cause normalization over all words is computa-
tionally expensive. The model finds the Viterbi
alignment using the Viterbi algorithm, similar to
the classic HMM model. Note that alignments
in the FFNN-based model are also governed by
first-order Markov dynamics because an align-
ment score depends on the previous alignment
a
j?1
.
Figure 1 shows the network structure with one
hidden layer for computing a lexical translation
probability t
lex
(f
j
, e
a
j
|c(f
j
), c(e
a
j
)). The model
consists of a lookup layer, a hidden layer, and an
output layer, which have weight matrices. The
model receives a source and target word with their
contexts as inputs, which are words in a prede-
fined window (the window size is three in Fig-
ure 1). First, the lookup layer converts each in-
put word into its word embedding by looking up
its corresponding column in the embedding ma-
trix (L), and then concatenates them. Let V
f
(or
V
e
) be a set of source words (or target words) and
M be a predetermined embedding length. L is a
M ? (|V
f
|+ |V
e
|) matrix
1
. Word embeddings are
dense, low dimensional, and real-valued vectors
that can capture syntactic and semantic properties
of the words (Bengio et al, 2003). The concate-
nation (z
0
) is then fed to the hidden layer to cap-
ture nonlinear relations. Finally, the output layer
receives the output of the hidden layer (z
1
) and
computes a lexical translation score.
1
We add a special token ?unk? to handle unknown words
and ?null? to handle null alignments to V
f
and V
e
The computations in the hidden and output layer
are as follows
2
:
z
1
= f(H ? z
0
+ B
H
), (5)
t
lex
= O ? z
1
+ B
O
, (6)
where H , B
H
, O, and B
O
are |z
1
| ? |z
0
|, |z
1
| ? 1,
1?|z
1
|, and 1?1 matrices, respectively, and f(x)
is an activation function. Following Yang et al
(2013), a ?hard? version of the hyperbolic tangent,
htanh(x)
3
, is used as f(x) in this study.
The alignment model based on an FFNN is
formed in the same manner as the lexical trans-
lation model. Each model is optimized by mini-
mizing the following ranking loss with a margin
using stochastic gradient descent (SGD)
4
, where
gradients are computed by the back-propagation
algorithm (Rumelhart et al, 1986):
loss(?) =
?
(f ,e)?T
max{0, 1? s
?
(a
+
|f , e)
+s
?
(a
?
|f ,e)}, (7)
where ? denotes the weights of layers in the
model, T is a set of training data, a
+
is the gold
standard alignment, a
?
is the incorrect alignment
with the highest score under ?, and s
?
denotes the
score defined by Eq. 4 as computed by the model
under ?.
3 RNN-based Alignment Model
This section proposes an RNN-based alignment
model, which computes a score for alignments a
J
1
using an RNN:
s
NN
(a
J
1
|f
J
1
, e
I
1
) =
J
?
j=1
t
RNN
(a
j
|a
j?1
1
, f
j
, e
a
j
), (8)
where t
RNN
is the score of an alignment a
j
. The
prediction of the j-th alignment a
j
depends on all
preceding alignments a
j?1
1
. Note that the pro-
posed model also uses nonprobabilistic scores,
similar to the FFNN-based model.
The RNN-based model is illustrated in Figure
2. The model consists of a lookup layer, a hid-
den layer, and an output layer, which have weight
2
Consecutive l hidden layers can be used: z
l
= f(H
l
?
z
l?1
+ B
H
l
). For simplicity, this paper describes the model
with 1 hidden layer.
3
htanh(x) = ?1 for x < ?1, htanh(x) = 1 for x > 1,
and htanh(x) = x for others.
4
In our experiments, we used a mini-batch SGD instead
of a plain SGD.
1472
O? +BO
htanh(H? +R? +BH )
t    ( |    ,    ,    )
Lookup Layer
Hidden 
Layer
Output Layer
Input
L L
d
aj fjRNN eajj-1a1
fj eaj
yj
yj-1
yj
d
xj
xj dyj-1
Figure 2: RNN-based alignment model
matrices L, {H
d
, R
d
, B
d
H
}, and {O,B
O
}, respec-
tively. Each matrix in the hidden layer (H
d
, R
d
,
and B
d
H
) depends on alignment, where d denotes
the jump distance from a
j?1
to a
j
: d = a
j
?
a
j?1
. In our experiments, we merge distances
that are greater than 8 and less than -8 into the
special ??8? and ??-8? distances, respectively.
Specifically, the hidden layer has weight matrices
{H
??8
, H
?7
, ? ? ? , H
7
, H
?8
, R
??8
, R
?7
, ? ? ? ,
R
7
, R
?8
, B
??8
H
, B
?7
H
, ? ? ? , B
7
H
, B
?8
H
} and com-
putes y
j
using the corresponding matrices of the
jump distance d.
The Viterbi alignment is determined using the
Viterbi algorithm, similar to the FFNN-based
model, where the model is sequentially applied
from f
1
to f
J
5
. When computing the score of the
alignment between f
j
and e
a
j
, the two words are
input to the lookup layer. In the lookup layer, each
of these words is converted to its word embedding,
and then the concatenation of the two embeddings
(x
j
) is fed to the hidden layer in the same manner
as the FFNN-based model. Next, the hidden layer
receives the output of the lookup layer (x
j
) and
that of the previous hidden layer (y
j?1
). The hid-
den layer then computes and outputs the nonlinear
relations between them. Note that the weight ma-
trices used in this computation are embodied by
the specific jump distance d. The output of the hid-
den layer (y
j
) is copied and fed to the output layer
and the next hidden layer. Finally, the output layer
computes the score of a
j
(t
RNN
(a
j
|a
j?1
1
, f
j
, e
a
j
))
from the output of the hidden layer (y
j
). Note that
the FFNN-based model consists of two compo-
5
Strictly speaking, we cannot apply the dynamic pro-
gramming forward-backward algorithm (i.e., the Viterbi al-
gorithm) due to the long alignment history of y
i
. Thus, the
Viterbi alignment is computed approximately using heuristic
beam search.
nents: one is for lexical translation and the other
is for alignment. The proposed RNN produces a
single score that is constructed in the hidden layer
by employing the distance-dependent weight ma-
trices.
Specifically, the computations in the hidden and
output layer are as follows:
y
j
= f(H
d
? x
j
+ R
d
? y
j?1
+ B
d
H
), (9)
t
RNN
= O ? y
j
+ B
O
, (10)
where H
d
, R
d
, B
d
H
, O, and B
O
are |y
j
| ? |x
j
|,
|y
j
| ? |y
j?1
|, |y
j
| ? 1, 1 ? |y
j
|, and 1 ? 1 matri-
ces, respectively. Note that |y
j?1
| = |y
j
|. f(x) is
an activation function, which is a hard hyperbolic
tangent, i.e., htanh(x), in this study.
As described above, the RNN-based model has
a hidden layer with recurrent connections. Under
the recurrence, the proposed model compactly en-
codes the entire history of previous alignments in
the hidden layer configuration y
i
. Therefore, the
proposed model can find alignments by taking ad-
vantage of the long alignment history, while the
FFNN-based model considers only the last align-
ment.
4 Training
During training, we optimize the weight matrices
of each layer (i.e., L, H
d
, R
d
, B
d
H
, O, and B
O
)
following a given objective using a mini-batch
SGD with batch size D, which converges faster
than a plain SGD (D = 1). Gradients are com-
puted by the back-propagation through time algo-
rithm (Rumelhart et al, 1986), which unfolds the
network in time (j) and computes gradients over
time steps. In addition, an l2 regularization term
is added to the objective to prevent the model from
overfitting the training data.
The RNN-based model can be trained by a
supervised approach, similar to the FFNN-based
model, where training proceeds based on the rank-
ing loss defined by Eq. 7 (Section 2.2). However,
this approach requires gold standard alignments.
To overcome this drawback, we propose an un-
supervised method using NCE, which learns from
unlabeled training data.
4.1 Unsupervised Learning
Dyer et al (2011) presented an unsupervised
alignment model based on contrastive estimation
(CE) (Smith and Eisner, 2005). CE seeks to dis-
criminate observed data from its neighborhood,
1473
which can be viewed as pseudo-negative samples.
Dyer et al (2011) regarded all possible align-
ments of the bilingual sentences, which are given
as training data (T ), and those of the full transla-
tion search space (?) as the observed data and its
neighborhood, respectively.
We introduce this idea to a ranking loss with
margin as
loss(?) = max
{
0, 1?
?
(f+,e+)?T
E
?
[s
?
(a|f
+
, e
+
)]
+
?
(f+,e?)??
E
?
[s
?
(a|f
+
, e
?
)]
}
, (11)
where ? is a set of all possible alignments given
(f , e), E
?
[s
?
] is the expected value of the scores
s
?
on ?, e
+
denotes a target language sentence in
the training data, and e
?
denotes a pseudo-target
language sentence. The first expectation term is
for the observed data, and the second is for the
neighborhood.
However, the computation for ? is prohibitively
expensive. To reduce computation, we employ
NCE, which uses randomly sampled sentences
from all target language sentences in ? as e
?
, and
calculate the expected values by a beam search
with beam width W to truncate alignments with
low scores. In our experiments, we set W to 100.
In addition, the above criterion is converted to an
online fashion as
loss(?) =
?
f+?T
max
{
0, 1? E
GEN
[s
?
(a|f
+
, e
+
)]
+
1
N
?
e?
E
GEN
[s
?
(a|f
+
, e
?
)]
}
, (12)
where e
+
is a target language sentence aligned to
f
+
in the training data, i.e., (f
+
, e
+
) ? T , e
?
is
a randomly sampled pseudo-target language sen-
tence with length |e
+
|, and N denotes the num-
ber of pseudo-target language sentences per source
sentence f
+
. Note that |e
+
| = |e
?
|. GEN is a
subset of all possible word alignments ?, which is
generated by beam search.
In a simple implementation, each e
?
is gener-
ated by repeating a random sampling from a set of
target words (V
e
) |e
+
| times and lining them up
sequentially. To employ more discriminative neg-
ative samples, our implementation samples each
word of e
?
from a set of the target words that co-
occur with f
i
? f
+
whose probability is above a
threshold C under the IBM Model 1 incorporating
l
0
prior (Vaswani et al, 2012). The IBM Model
1 with l
0
prior is convenient for reducing transla-
tion candidates because it generates more sparse
alignments than the standard IBM Model 1.
4.2 Agreement Constraints
Both of the FFNN-based and RNN-based models
are based on the HMM alignment model, and they
are therefore asymmetric, i.e., they can represent
one-to-many relations from the target side. Asym-
metric models are usually trained in each align-
ment direction. The model proposed by Yang et
al. (2013) is no exception. However, it has been
demonstrated that encouraging directional mod-
els to agree improves alignment performance (Ma-
tusov et al, 2004; Liang et al, 2006; Grac?a et al,
2008; Ganchev et al, 2008).
Inspired by their work, we introduce an agree-
ment constraint to our learning. The constraint
concretely enforces agreement in word embed-
dings of both directions. The proposed method
trains two directional models concurrently based
on the following objective by incorporating a
penalty term that expresses the difference between
word embeddings:
argmin
?
FE
{
loss(?
FE
) + ???
L
EF
? ?
L
FE
?
}
, (13)
argmin
?
EF
{
loss(?
EF
) + ???
L
FE
? ?
L
EF
?
}
, (14)
where ?
FE
(or ?
EF
) denotes the weights of lay-
ers in a source-to-target (or target-to-source) align-
ment model, ?
L
denotes weights of a lookup layer,
i.e., word embeddings, and ? is a parameter that
controls the strength of the agreement constraint.
??? indicates the norm of ?. 2-norm is used in our
experiments. Equations 13 and 14 can be applied
to both supervised and unsupervised approaches.
Equations 7 and 12 are substituted into loss(?)
in supervised and unsupervised learning, respec-
tively. The proposed constraint penalizes overfit-
ting to a particular direction and enables two di-
rectional models to optimize across alignment di-
rections globally.
Our unsupervised learning procedure is summa-
rized in Algorithm 1. In Algorithm 1, line 2 ran-
domly samples D bilingual sentences (f
+
, e
+
)
D
from training data T . Lines 3-1 and 3-2 gener-
ate N pseudo-negative samples for each f
+
and
e
+
based on the translation candidates of f
+
and
e
+
found by the IBM Model 1 with l
0
prior,
1474
Algorithm 1 Training Algorithm
Input: ?
1
FE
, ?
1
EF
, training data T , MaxIter,
batch size D, N , C, IBM1, W , ?
1: for all t such that 1 ? t ?MaxIter do
2: {(f
+
, e
+
)
D
}?sample(D,T )
3-1: {(f
+
, {e
?
}
N
)
D
}?neg
e
({(f
+
, e
+
)
D
}, N,C, IBM1)
3-2: {(e
+
, {f
?
}
N
)
D
}?neg
f
({(f
+
, e
+
)
D
}, N,C, IBM1)
4-1: ?
t+1
FE
?update((f
+
, e
+
, {e
?
}
N
)
D
, ?
t
FE
, ?
t
EF
,W, ?)
4-2: ?
t+1
EF
?update((e
+
, f
+
, {f
?
}
N
)
D
, ?
t
EF
, ?
t
FE
,W, ?)
5: end for
Output: ?
MaxIter+1
EF
, ?
MaxIter+1
FE
Train Dev Test
BTEC 9 K 0 960
Hansards 1.1 M 37 447
FBIS
NIST03
240 K 878
919
NIST04 1,597
IWSLT 40 K 2,501 489
NTCIR 3.2 M 2,000 2,000
Table 1: Size of experimental datasets
IBM1 (Section 4.1). Lines 4-1 and 4-2 update the
weights in each layer following a given objective
(Sections 4.1 and 4.2). Note that ?
t
FE
and ?
t
EF
are
concurrently updated in each iteration, and ?
t
EF
(or ?
t
FE
) is employed to enforce agreement be-
tween word embeddings when updating ?
t
FE
(or
?
t
EF
).
5 Experiment
5.1 Experimental Data
We evaluated the alignment performance of the
proposed models with two tasks: Japanese-
English word alignment with the Basic Travel
Expression Corpus (BTEC) (Takezawa et al,
2002) and French-English word alignment with
the Hansard dataset (Hansards) from the 2003
NAACL shared task (Mihalcea and Pedersen,
2003). In addition, we evaluated the end-to-end
translation performance of three tasks: a Chinese-
to-English translation task with the FBIS corpus
(FBIS), the IWSLT 2007 Japanese-to-English
translation task (IWSLT ) (Fordyce, 2007), and
the NTCIR-9 Japanese-to-English patent transla-
tion task (NTCIR) (Goto et al, 2011)
6
.
Table 1 shows the sizes of our experimental
datasets. Note that the development data was
not used in the alignment tasks, i.e., BTEC
6
We did not evaluate the translation performance on the
Hansards data because the development data is very small
and performance is unreliable.
and Hansards, because the hyperparameters of
the alignment models were set by preliminary
small-scale experiments. The BTEC data is
the first 9,960 sentence pairs in the training data
for IWSLT , which were annotated with word
alignment (Goh et al, 2010). We split these
pairs into the first 9,000 for training data and
the remaining 960 as test data. All the data in
BTEC is word-aligned, and the training data in
Hansards is unlabeled data. In FBIS, we used
the NIST02 evaluation data as the development
data, and the NIST03 and 04 evaluation data as
test data (NIST03 and NIST04).
5.2 Comparing Methods
We evaluated the proposed RNN-based alignment
models against two baselines: the IBM Model
4 and the FFNN-based model with one hidden
layer. The IBM Model 4 was trained by pre-
viously presented model sequence schemes (Och
and Ney, 2003): 1
5
H
5
3
5
4
5
, i.e., five iterations of
the IBM Model 1 followed by five iterations of the
HMM Model, etc., which is the default setting for
GIZA++ (IBM4). For the FFNN-based model,
we set the word embedding length M to 30, the
number of units of a hidden layer |z
1
| to 100, and
the window size of contexts to 5. Hence, |z
0
| is
300 (30?5?2). Following Yang et al (2013), the
FFNN-based model was trained by the supervised
approach described in Section 2.2 (FFNN
s
).
For the RNN-based models, we set M to 30
and the number of units of each recurrent hid-
den layer |y
j
| to 100. Thus, |x
j
| is 60 (30 ? 2).
The number of units of each layer of the FFNN-
based and RNN-based models and M were set
through preliminary experiments. To demonstrate
the effectiveness of the proposed learning meth-
ods, we evaluated four types of RNN-based mod-
els: RNN
s
, RNN
s+c
, RNN
u
, and RNN
u+c
,
where ?s/u? denotes a supervised/unsupervised
model and ?+c? indicates that the agreement con-
straint was used.
In training all the models except IBM4, the
weights of each layer were initialized first. For
the weights of a lookup layer L, we preliminarily
trained word embeddings for the source and target
language from each side of the training data. We
then set the word embeddings to L to avoid falling
into local minima. Other weights were randomly
initialized to [?0.1, 0.1]. For the pretraining, we
1475
Alignment BTEC Hansards
IBM4 0.4859 0.9029
FFNN
s
(I) 0.4770 0.9020
RNN
s
(I) 0.5053
+
0.9068
RNN
s+c
(I) 0.5174
+
0.9202
+
RNN
u
0.5307
+
0.9037
RNN
u+c
0.5562
+
0.9275
+
FFNN
s
(R) 0.8224 -
RNN
s
(R) 0.8798
+
-
RNN
s+c
(R) 0.8921
+
-
Table 2: Word alignment performance (F1-
measure)
used the RNNLM Toolkit
7
(Mikolov et al, 2010)
with the default options. We mapped all words
that occurred less than five times to the special to-
ken ?unk?. Next, each weight was optimized us-
ing the mini-batch SGD, where batch size D was
100, learning rate was 0.01, and an l
2
regulariza-
tion parameter was 0.1. The training stopped after
50 epochs. The other parameters were set as fol-
lows: W , N and C in the unsupervised learning
were 100, 50, and 0.001, respectively, and ? for
the agreement constraint was 0.1.
In the translation tasks, we used the Moses
phrase-based SMT systems (Koehn et al, 2007).
All Japanese and Chinese sentences were seg-
mented by ChaSen
8
and the Stanford Chinese seg-
menter
9
, respectively. In the training, long sen-
tences with over 40 words were filtered out. Using
the SRILM Toolkits (Stolcke, 2002) with modified
Kneser-Ney smoothing, we trained a 5-gram lan-
guage model on the English side of each training
data for IWSLT and NTCIR, and a 5-gram lan-
guage model on the Xinhua portion of the English
Gigaword corpus for FBIS. The SMT weighting
parameters were tuned by MERT (Och, 2003) in
the development data.
5.3 Word Alignment Results
Table 2 shows the alignment performance by
the F1-measure. Hereafter, MODEL(R) and
MODEL(I) denote the MODEL trained from
gold standard alignments and word alignments
found by the IBM Model 4, respectively. In
Hansards, all models were trained from ran-
7
http://www.fit.vutbr.cz/
?
imikolov/
rnnlm/
8
http://chasen-legacy.sourceforge.jp/
9
http://nlp.stanford.edu/software/
segmenter.shtml
domly sampled 100 K data
10
. We evaluated
the word alignments produced by first applying
each model in both directions and then combin-
ing the alignments using the ?grow-diag-final-
and? heuristic (Koehn et al, 2003). The signif-
icance test on word alignment performance was
performed by the sign test with a 5% significance
level. ?+? in Table 2 indicates that the compar-
isons are significant over corresponding baselines,
IBM4 and FFNN
s
(R/I).
In Table 2, RNN
u+c
, which includes all our
proposals, i.e., the RNN-based model, the unsu-
pervised learning, and the agreement constraint,
achieves the best performance for both BTEC
and Hansards. The differences from the base-
lines are statistically significant.
Table 2 shows that RNN
s
(R/I) outperforms
FFNN
s
(R/I), which is statistically significant
in BTEC. These results demonstrate that captur-
ing the long alignment history in the RNN-based
model improves the alignment performance. We
discuss the difference of the RNN-based model?s
effectiveness between language pairs in Section
6.1. Table 2 also shows that RNN
s+c
(R/I) and
RNN
u+c
achieve significantly better performance
than RNN
s
(R/I) and RNN
u
in both tasks, re-
spectively. This indicates that the proposed agree-
ment constraint is effective in training better mod-
els in both the supervised and unsupervised ap-
proaches.
In BTEC, RNN
u
and RNN
u+c
significantly
outperform RNN
s
(I) and RNN
s+c
(I), respec-
tively. The performance of these models is com-
parable with Hansards. This indicates that our
unsupervised learning benefits our models because
the supervised models are adversely affected by
errors in the automatically generated training data.
This is especially true when the quality of training
data, i.e., the performance of IBM4, is low.
5.4 Machine Translation Results
Table 3 shows the translation performance by the
case sensitive BLEU4 metric
11
(Papineni et al,
2002). Table 3 presents the average BLEU of three
different MERT runs. In NTCIR and FBIS,
each alignment model was trained from the ran-
10
Due to high computational cost, we did not use all the
training data. Scaling up to larger datasets will be addressed
in future work.
11
We used mteval-v13a.pl as the evaluation tool
(http://www.itl.nist.gov/iad/mig/tests/
mt/2009/).
1476
Alignment IWSLT NTCIR
FBIS
NIST03 NIST04
IBM4
all
46.47
27.91 25.90 28.34
IBM4 27.25 25.41 27.65
FFNN
s
(I) 46.38 27.05 25.45 27.61
RNN
s
(I) 46.43 27.24 25.47 27.56
RNN
s+c
(I) 46.51 27.12 25.55 27.73
RNN
u
47.05
?
27.79
?
25.76
?
27.91
?
RNN
u+c
46.97
?
27.76
?
25.84
?
28.20
?
Table 3: Translation performance (BLEU4(%))
domly sampled 100 K data, and then a translation
model was trained from all the training data that
was word-aligned by the alignment model. In ad-
dition, for a detailed comparison, we evaluated the
SMT system where the IBM Model 4 was trained
from all the training data (IBM4
all
). The sig-
nificance test on translation performance was per-
formed by the bootstrap method (Koehn, 2004)
with a 5% significance level. ?*? in Table 3 in-
dicates that the comparisons are significant over
both baselines, i.e., IBM4 and FFNN
s
(I).
Table 3 also shows that better word align-
ment does not always result in better translation,
which has been discussed previously (Yang et al,
2013). However, RNN
u
and RNN
u+c
outper-
form FFNN
s
(I) and IBM4 in all tasks. These
results indicate that our proposals contribute to im-
proving translation performance
12
. In addition,
Table 3 shows that these proposed models are
comparable to IBM4
all
in NTCIR and FBIS
even though the proposed models are trained from
only a small part of the training data.
6 Discussion
6.1 Effectiveness of RNN-based Alignment
Model
Figure 3 shows word alignment examples from
FFNN
s
and RNN
s
, where solid squares indi-
cate the gold standard alignments. Figure 3 (a)
shows that RRN
s
adequately identifies compli-
cated alignments with long distances compared
to FFNN
s
(e.g., jaggy alignments of ?have you
been learning? in Fig 3 (a)) because RNN
s
cap-
tures alignment paths based on long alignment his-
tory, which can be viewed as phrase-level align-
ments, while FFNN
s
employs only the last align-
ment.
In French-English word alignment, the most
12
We also confirmed the effectiveness of our models on the
NIST05 and NTCIR-10 evaluation data.
How
long
have
you
been
learning
English
?
??
?
? ??? ????? ?? ?????? ?? ?? ? ?
?
?
? ?
? ?
?
?
?
?
(a) Japanese-English Alignment
they
also
have
a
role
to
play
in
food
chain
.
the
eux aus
si
ont un r?le ? jou
erdan
s
la cha
?ne
alim
ent
aire
.
(b) French-English Alignment
?
?
?
?
?
?
?
?
?
?
?
?
? : FFNN  (R)s
: RNN  (R)s
? : FFNN  (I)s
: RNN  (I)s
Figure 3: Word alignment examples
Alignment 40 K 9 K 1 K
IBM4 0.5467 0.4859 0.4128
RNN
u+c
0.6004 0.5562 0.4842
RNN
s+c
(R) - 0.8921 0.6063
Table 4: Word alignment performance on BTEC
with various sized training data
valuable clues are located locally because English
and French have similar word orders and their
alignment has more one-to-one mappings than
Japanese-English word alignment (Figure 3). Fig-
ure 3 (b) shows that both RRN
s
and FFNN
s
work for such simpler alignments. Therefore,
the RNN-based model has less effect on French-
English word alignment than Japanese-English
word alignment, as indicated in Table 2.
6.2 Impact of Training Data Size
Table 4 shows the alignment performance on
BTEC with various training data sizes, i.e., train-
ing data for IWSLT (40 K), training data for
BTEC (9 K), and the randomly sampled 1 K
data from the BTEC training data. Note that
RNN
s+c
(R) cannot be trained from the 40 K data
because the 40 K data does not have gold standard
1477
Alignment BTEC Hansards
FFNN
s
(I) 0.4770 0.9020
FFNN
s+c
(I) 0.4854
+
0.9085
+
FFNN
u
0.5105
+
0.9026
FFNN
u+c
0.5313
+
0.9144
+
FFNN
s
(R) 0.8224 -
FFNN
s+c
(R) 0.8367
+
-
Table 5: Word alignment performance of various
FFNN-based models (F1-measure)
word alignments.
Table 4 demonstrates that the proposed RNN-
based model outperforms IBM4 trained from the
unlabeled 40 K data by employing either the 1
K labeled data or the 9 K unlabeled data, which
is less than 25% of the training data for IBM4.
Consequently, the SMT system using RNN
u+c
trained from a small part of training data can
achieve comparable performance to that using
IBM4 trained from all training data, which is
shown in Table 3.
6.3 Effectiveness of Unsupervised
Learning/Agreement Constraints
The proposed unsupervised learning and agree-
ment constraints can be applied to any NN-based
alignment model. Table 5 shows the alignment
performance of the FFNN-based models trained
by our supervised/unsupervised approaches (s/u)
with and without our agreement constraints. In
Table 5, ?+c? denotes that the agreement con-
straint was used, and ?+? indicates that the
comparison with its corresponding baseline, i.e.,
FFNN
s
(I/R), is significant in the sign test with a
5% significance level.
Table 5 shows that FFNN
s+c
(R/I) and
FFNN
u+c
achieve significantly better perfor-
mance than FFNN
s
(R/I) and FFNN
u
, respec-
tively, in both BTEC and Hansards. In addi-
tion, FFNN
u
and FFNN
u+c
significantly out-
perform FFNN
s
(I) and FFNN
s+c
(I), respec-
tively, in BTEC. The performance of these mod-
els is comparable in Hansards. These results
indicate that the proposed unsupervised learning
and agreement constraint benefit the FFNN-based
model, similar to the RNN-based model.
7 Conclusion
We have proposed a word alignment model based
on an RNN, which captures long alignment his-
tory through recurrent architectures. Furthermore,
we proposed an unsupervised method for training
our model using NCE and introduced an agree-
ment constraint that encourages word embeddings
to be consistent across alignment directions. Our
experiments have shown that the proposed model
outperforms the FFNN-based model (Yang et al,
2013) for word alignment and machine translation,
and that the agreement constraint improves align-
ment performance.
In future, we plan to employ contexts composed
of surrounding words (e.g., c(f
j
) or c(e
a
j
) in the
FFNN-based model) in our model, even though
our model implicitly encodes such contexts in the
alignment history. We also plan to enrich each
hidden layer in our model with multiple layers
following the success of Yang et al (2013), in
which multiple hidden layers improved the perfor-
mance of the FFNN-based model. In addition, we
would like to prove the effectiveness of the pro-
posed method for other datasets.
Acknowledgments
We thank the anonymous reviewers for their help-
ful suggestions and valuable comments on the first
version of this paper.
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 65?72.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning. In
1478
Proceedings of the 25th International Conference on
Machine Learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-Dependent Pre-trained Deep Neu-
ral Networks for Large Vocabulary Speech Recog-
nition. Audio, Speech, and Language Processing,
IEEE Transactions on, 20(1):30?42.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1?38.
Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised Word Alignment with
Arbitrary Features. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, pages 409?419.
Cameron S. Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the
4th International Workshop on Spoken Languaeg
Translation, pages 1?12.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better Alignments = Better Translations? In
Proceedings of the 46th Annual Conference of the
Association for Computational Linguistics: Human
Language Technologies, pages 986?993.
Chooi-Ling Goh, TaroWatanabe, Hirofumi Yamamoto,
and Eiichiro Sumita. 2010. Constraining a Gen-
erative Word Alignment Model with Discriminative
Output. IEICE Transactions, 93-D(7):1976?1983.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559?578.
Jo?ao V. Grac?a, Kuzman Ganchev, and Ben Taskar.
2008. Expectation Maximization and Posterior
Constraints. In Advances in Neural Information
Processing Systems 20, pages 569?576.
Michael Gutmann and Aapo Hyv?arinen. 2010. Noise-
Contrastive Estimation: A New Estimation Principle
for Unnormalized Statistical Models. In Proceed-
ings of the 13st International Conference on Artifi-
cial Intelligence and Statistics, pages 297?304.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700?1709.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388?395.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 39?48.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of the Main
Conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric Word Alignments for Statistical
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 219?225.
Rada Mihalcea and Ted Pedersen. 2003. An Evalua-
tion Exercise for Word Alignment. In Proceedings
of the HLT-NAACL 2003 Workshop on Building and
Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 1?10.
Tomas Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Lan-
guage Model. In Proceedings of the 4th IEEE Work-
shop on Spoken Language Technology, pages 234?
239.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proceedings of 11th Annual Conference of the Inter-
national Speech Communication Association, pages
1045?1048.
Andriy Mnih and Yee Whye Teh. 2012. A Fast and
Simple Algorithm for Training Neural Probabilistic
Language Models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751?1758.
1479
Robert C. Moore. 2005. A Discriminative Framework
for Bilingual Word Alignment. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 81?88.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Learning Internal Representations by Error
Propagation. In D. E. Rumelhart and J. L. McClel-
land, editors, Parallel Distributed Processing, pages
318?362. MIT Press.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 354?362.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, pages 8430?8434.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-
aya, Hirofumi Yamamoto, and Seiichi Yamamoto.
2002. Toward a Broad-coverage Bilingual Corpus
for Speech Translation of Travel Conversations in
the Real World. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Eval-
uation, pages 147?152.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach to
Word Alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?80.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller Alignment Models for Better Trans-
lations: Unsupervised Word Alignment with the l
0
-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 311?319.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-Scale
Neural Language Models Improves Translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1387?1392.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 836?841.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word Alignment Modeling with Con-
text Dependent Deep Neural Network. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 166?175.
1480
